[{"id": "20120327243", "patent_code": "10375534", "patent_name": "Video transmission and sharing over ultra-low bitrate wireless\n     communication channel", "year": "2019", "inventor_and_country_data": " Inventors: \nRezvani; Behrooz (San Ramon, CA)  ", "description": "<BR><BR>FIELD OF THE INVENTION\nAt least one embodiment of the present invention pertains to video compression and distribution application, and more particularly, to a video compression application and system for compressing and transmitting video based on shared information\nand its content over an ultra-low bit rate channel\n<BR><BR>BACKGROUND\nA digital video sequence can contain a very large amount of data.  In order to transfer a video efficiently using current technology, a large transmission bandwidth is needed.  However, wireless data transmission bandwidth is a limited and\nsometimes expensive resource.  Consequently, it is desirable to use compression techniques to encode the video using fewer bits than the original video contains.  The compressed video should effectively reduce the bandwidth required to transmit the video\nvia networks.\nIt is desirable to compress a video in a highly efficient way so that the video can be transmitted through an ultra-low bit channel, such as a SMS channel.  Short message service (\"SMS\"), sometimes called \"texting,\" is one of the most popular\nperson-to-person messaging technologies in use today.  SMS functionality is widely available in almost all modern mobile phones.  However, SMS has a very limited capacity to transmit information; each SMS message has a fixed length of 140 bytes or 160\ncharacters.  Multimedia messaging service (\"MMS\") is another possible way to send messages that include multimedia content.  However, MMS messaging cannot utilize existing SMS infrastructure; so it costs more than SMS messaging.  There is no mechanism\ntoday to effectively send a video message with ultra-low bandwidth on wireless channel, particularly on very low bandwidth channels such as an SMS channel.\nFor speech compression, a large body of work in the past four decades has been done analyzing various concepts in speech compression.  In a typical voice compression technique for wireless communications such as ITU G.723, FS MELP, a voice\nrecord is analyzed for its correlation property in the acoustical sense.  The speech compression programs are typically based on waveform coders, such as Code Excited Linear Prediction (\"CELP\") algorithm.  While a number of approaches in the past have\nresulted in very low bit rates 200-300 bps, the voice quality has been compromised.  The mean opinion score (\"MOS\") factor of compressed voice record typically is about 2, wherein the MOS provides a numerical indication of the perceived quality from the\nusers' perspective of the voice record after compression.  It is desirable to have a method to compress and transmit the voice record in a very low bit rates while maintaining a high voice quality, with an MOS factor between 4 or 5.\n<BR><BR>SUMMARY\nThe technology introduced here includes a \"cognitively aware\" technique to transmit video over an ultra-low bandwidth channel, such as an SMS channel.  By \"ultra-low\" bandwidth, what is meant is a channel that supports a data rate of no more\nthan 6,500 bits per second (bps) A \"cognitively aware\" method, in the context of this description, is a method that takes into consideration the levels of importance and interest of different spatial regions depicted in a video sequence, to a real or\nhypothetical human viewer.  The use of \"cognitive awareness\" in this way allows key portions of a video sequence to be identified and sent over a communication link while less important portions of the video sequence are not transmitted, or transmitted\nwith lower fidelity.  The technique further provides for synthesizing, at a receiving device, a high-quality approximation (called a \"copy\" herein) of the original video sequence, based on the relatively small amount of information that is transferred\nover the link.  This is done while still providing an appropriate amount of resolution for a high quality video synthesis, while keeping the number of bits per frame low enough to permit use of an ultra-low bitrate channel (such as SMS) to transmit the\nvideo.\nIn one embodiment, an ultra-compact video file is generated based on an ultra-low bandwidth video with data rate less than 6500 bps.  The ultra-compact video file may be embedded in an e-mail; and the email may be sent asynchronously at a later\ntime over wireless links such as mobile or satellite channels to at least one receiver.  Such ultra-compact files allow for large distribution of video content and help preventing wireless networks from congestion or network outage.  The ultra-compact\nfile may not include the original video sequence but carry specific data for video regeneration or synthesis.\nIn certain embodiments, a set of parameters are developed from a video region of a high interest, wherein the parameters represent a mapping function of a database to model the video region.  The set of parameters is transmitted over the\nultra-low bandwidth channel to a remote device, wherein the remote device also has access to an instance of the database.  The remote device synthesizes the video by using the mapping function of the database, which is represented by the transmitted set\nof parameters\nThe technology further includes a method for segmenting, modeling, transmitting and synthesizing a video.  The method comprising capturing a video sequence at a mobile communication device; segmenting each frame of the video sequence into a\nplurality of regions; evaluating each of the plurality of regions based on the database and recognizing a cognitively visually important region of the plurality of regions based on a result of said evaluating; developing a data sequence based on the\ncognitively visually important region in the frames of the video sequence; sending the data sequence representative of the video sequence from the mobile communication device to a remote device over a wireless communication link in an ultra low bit rate\nchannel; receiving the data sequence at the remote device; and synthesizing the video sequence based on the data sequence and a database mutually available to both the mobile communication device and the remote device.  The database may include a visual\ncharacteristics model of a person.  The data sequence may represent a time series of mapping functions of the database; the mapping functions model visual characteristics of a person.\nOther aspects of the technology introduced here will be apparent from the accompanying figures and from the detailed description which follows. <BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nThese and other objects, features and characteristics of the present invention will become more apparent to those skilled in the art from a study of the following detailed description in conjunction with the appended claims and drawings, all of\nwhich form a part of this specification.  In the drawings:\nFIG. 1 illustrates an example of a frame of a video message containing regions of various interest levels;\nFIG. 2 illustrates an example of a process of a transmitter segmenting, modeling, and transmitting a video;\nFIG. 3 illustrates an example of a process of a receiver synthesizing a video;\nFIG. 4 illustrates an example of a process of a transmitter selecting database for modeling a region of interest of a video;\nFIG. 5 illustrates an example of a process of computing eigenvectors according to a principle component analysis (PCA) method;\nFIG. 6 illustrates an example of a process of encoding and synthesizing a region of a video based on basis eigenvectors in a database;\nFIG. 7 illustrates an example of a server segmenting and modeling a video;\nFIG. 8 illustrates an example of a server translating mapping function parameters between different versions of databases;\nFIG. 9 is a block diagram of a voice modeling algorithm;\nFIG. 10 is a block diagram of an automatic speech recognition (ASR) method; and\nFIG. 11 is a block diagram of a processing system that can be used to implement a transmitter, receiver or server implementing the techniques described herein.\n<BR><BR>DETAILED DESCRIPTION\nReferences in this specification to \"an embodiment,\" \"one embodiment,\" or the like, mean that the particular feature, structure, or characteristic being described is included in at least one embodiment of the present invention.  Occurrences of\nsuch phrases in this specification do not necessarily all refer to the same embodiment.\nA cognitively aware method for segmenting, modeling, transmitting and synthesizing video is described herein, wherein the video has one or more regions in which a viewer has some level of interest.  The viewer may be a real or hypothetical human\nviewer.  In this disclosure, a \"cognitively aware\" method is a method that automatically segments frames of a video into regions of various levels of importance or interest to a real or hypothetical human viewer and where the viewer may have some a\npriori knowledge about the video.  The cognitively aware method further provides an appropriate amount of resolution for a high quality video synthesis, while keeping the number of bits per frame low enough to permit use of an ultra-low bitrate channel\n(such as SMS) to transmit the video to a receiver.  Accordingly, an important aspect of the technique introduced here is recognition that it may not be necessary to encode or transmit regions that are likely to be of low interest to a human viewer.  The\nvideo frames are partitioned into regions of varying degree of interest where different methods are used for compression for different regions.  In regions of high interest, low bitrate is achieved using precise knowledge of the nature of the region.  In\nlower interest regions lower bitrate may be achieved by accepting lower resolution for these regions.  Accordingly, bandwidth can be conserved by segmentation of frames into regions and sending data suitable for these regions over the communication\nchannel.  The receiver receives this data and synthesizes an instance of the video at the receiver side with the quality of synthesis for a segment in a frame being dependent on interest level of the segment.\nVideo chatting and video messaging are applications that can benefit from low bandwidth video encoding and transmitting.  In one example, a video message contains image content that depicts an individual's head and shoulders while the individual\nis talking in front of a camera.  The video sequence may be captured by a camera of a mobile communication device, such as a mobile phone.\nFIG. 1 illustrates an example of a frame of the video message.  In the frame 100 of the video message, the region of high interest level (\"RoHI\") (to a human viewer) is the face 102 of the individual 101, particularly the lips and the eyes of\nthe individual.  A region of high interest level means that the region has cognitive visual importance from a real or hypothetical human viewer's perspective.  A region of medium interest level (\"RoMI\") includes the hair 103, neck 104 and shoulders 105\nof the individual 101.  A region of low interest level (\"RoLI\") includes the rest of the video frame, such as the background 106.  A region of a high interest level is a \"cognitively visually important\" region, i.e. a region having visual importance from\na real or hypothetical human viewer's perspective.  This segmentation is an example of segmentation of video frames into regions of varying interest.\nThe cognitively aware method utilizes a cognitive dictionary which is an adaptive learning repository of databases and codebooks that contain audio and visual information about a person or a group of people and their interactions with an\nenvironment.  The audio and visual information is represented by elementary functional models associated with statistical, semantic, situational markers.  These statistical, semantic, situational markers provide contextually aware information of the\nvideo in a compact form for transmission.  The database may include one or more visual characteristics models of human.\nThe regions of various interest levels of a video are encoded (compressed) using different techniques described in the following paragraphs.  The video message example illustrated in FIG. 1 is referred to only for illustration purposes when\nexplaining the techniques.  A person having ordinary skill in the art will readily be able to apply the techniques to encode videos in scenarios different from the example.  After the regions of various interest levels of the video are encoded at a\ntransmitter side, the encoded data sequence is transmitted to a receiver side.  The data sequence may be transmitted over a wireless communication link in an ultra-low bit rate channel, such as an SMS channel.  The regions are synthesized and \"stitched\ntogether\" to synthesize an instance of the video at the receiver side.  The transmitter, receiver, or both can be mobile communication devices, such as smartphones.  Alternatively, the technique can be employed by other types of devices, such that\nneither the transmitter nor the receiver is necessarily a mobile device.  For example, the technique could be implemented between two personal computers (PCs), or between a PC and a mobile device, and/or other types of devices capable of transmitting or\nreceiving video.  Any of these devices can be a transmitter or a receiver.  The wireless transmission link could be any wireless medium and protocols including, but not limited to, mobile phone networks, terrestrial TVs, satellite channels, AM and FM\nradios, and amateur radios.\nOnce the high interest region is identified in an initial frame, an initial guess for the mapping function for the first frame of the video is used in order to initiate an iterative algorithm to find the best mapping function.  The mapping\nfunction is explained in detail in the following paragraphs.\nFIG. 2 illustrates an example of a process of a transmitting user device (\"transmitter\") segmenting, modeling, and transmitting a video.  The transmitter captures or otherwise inputs a video at step 201.  The first frame of video is extracted\n(202) for initial approximation of a region of high interest level (RoHI).  A classifier, such as a discriminative classifier, can be used to solve the initial approximation and identify the RoHI in the first frame (203).  Thus, a region having cognitive\nvisual importance from a real or hypothetical human viewer's perspective identifying, is identified by a processor.  The databases (\"OBs\") are searched in order to find an individual specific database that matches the identified RoHI (204).  If an\nindividual specific database is matched, the database is used to extract the location of the RoHI and develop the mapping function (\"MF\") for the RoHI in the first frame (205).  If there is no matched individual specific database, the location of the\nRoHI is extracted using a general database (206).  Then a cascade of specific databases is examined to select a best database to model the RoHI (207), which is disclosed in details in following paragraphs.  The selected database is used to extract the\nlocation of the RoHI and develop the mapping function for the RoHI in the first frame (208).  The mapping function of the second frame is developed by using the mapping function of the first frame as a starting approximation of an interactive\noptimization.  Similarly, a mapping function of each frame of the video is developed by using the mapping function of the previous frame as a starting approximation (209).  Then, since the location of the RoHI is identified, The RoMI and RoLI of the\nvideo are located by using the RoHI as anchor (210).  The compression of RoMI and RoLI can be implemented using any standard video compression methods.  For example, in steps 211-213, the RoMI and RoLI are compressed by vector quantization (\"VQ\") method,\nwhich is disclosed in detail in following paragraphs.  Optionally, an error concealment library can be utilized to further fine-tune the modeling of the video (213).  The mapping functions are transmitted along the compressed RoMI and RoLI video data\n(214).  An indication of which database is used is also transmitted to instruct the receiver synthesizing the video using the appropriate database.  The low bitrate is achieved by the low bandwidth and high fidelity in high interest regions using a\npriori knowledge of the nature of the regions.  The low bitrate is further achieved by a priori knowledge about the regions of lower interests to a human viewer that the regions of lower interest does not need to maintain high fidelities.\nFIG. 3 illustrates an example of a process of a receiving user device (\"receiver\") synthesizing a video.  At step of 301, the receiver receives the transmitted mapping functions, and optionally, the compressed RoMI and RoLI video data.  The\nreceiver is also notified of the appropriate database for synthesizing the video.  The RoMI and RoLI of the video are decompressed from the transmitted compressed data (302).  In some embodiment, there are no received data for RoMI or RoLI; these regions\nmay be reconstructed based on certain assumption by the receiver.  RoHI is synthesized by calculating the mapping function using the appropriate database (303).  All regions are combined to synthesize the frames of the video (304).  If any indices of the\nerror concealment library are received (305), the indexed content from error concealment library is applied on the frames of the video (306).  The boundary between RoHI and regions of lower interests are blurred to alleviate a \"separation\" effect (307). \nAt 308, the video is synthesized in the receiver.\nThe cognitively aware method of adaptively compressing a video described herein is disclosed in details in following multiple sections of this document:\n1.  Identifying and Segmenting Regions of Interests\n2.  Modeling and Synthesis of Regions of High Interest Levels 2.1.  Selection of Database 2.2.  Methods for Modeling High Interest Level Regions 2.3.  Error Concealment Library 2.4.  Further Bitrate Reduction for Non-Real Time Video Transmission\n3.  Modeling and Synthesis of Regions of Medium and Lower Interest Levels 3.1.  Special Treatment of Low Interest Level Regions 3.2.  Special Treatment of Medium Interest Level Regions\n4.  Database Synchronization between Transmitter and Receivers\n5.  Quality of Service (QoS) Selection: Target Bitrate vs.  Choice of Modeling Method\n6.  Regenerating Video by Stitching of Regions\n7.  Voice modeling and synthesizing\n8.  Implementation of transmitters, receivers and servers.\n1.  Identifying and Segmenting Regions of Interests\nTo segment regions of various interest levels, an initial approximation of a region of a high interest level is employed.  A discriminative classifier can be trained to solve this initial approximation.  An example of a discriminative classifier\nis the Viola Jones AdaBoost classifier for identifying regions of face and facial features, such as eyes and nose, in an image.  Similar approaches are discussed in articles including: \"Game theory, on-line prediction and boosting,\" Proceedings of the\nNinth Annual Conference on Computational Learning Theory, pages 325-332, 1996, Y. Freund and R. E. Schapire; and \"Rapid object detection using a boosted cascade of simple features,\" Conference on Computer Vision and Pattern Recognition (CVPR), 2001, P.\nViola and M. Jones; all of which are incorporated herein by reference in their entireties.\nOnce the high interest region is identified in an initial frame, an initial guess for the mapping function for the first frame of the video is required in order to initiate an iterative algorithm to find the best mapping function.  The mapping\nfunction is explained in detail in the following section.  For example, if a principle component analysis (\"PCA\") method is used, some initial guess for the coefficient of the eigenvectors (also referred to as eigen-functions) is required.  For the\nsubsequent frames of the video, mapping function for the previous frame may be utilized as an initial guess of the mapping function for the current frame.  Through the iterative process of modeling the high interest level region by mapping functions, the\nhigh interest region is accurately tracked throughout the length of the video.  This knowledge of the evolving boundary of the high interest region is a basis for segmenting the video images into regions with various interest levels.\nIdentification of boundaries between regions of medium and low interest levels is usually done on a case by case basis.  It depends on the relationship between the high, medium and low interest regions.  In the video message example as\nillustrated in FIG. 1, the medium interest level region is defined as hair 103, neck 104 and shoulders 105.  Since the method to model the high interest region can accurately locate the face in each frame, the medium interest regions can be located by\ntheir relationship to the face.  The rest of the video frame belongs to the low interest level region.\n2.  Modeling and Synthesis of Regions of High Interest Levels\n2.1.  Selection of Database\nModeling of regions of high interest levels can be done using prior knowledge of what the region represents to viewers.  In regard to the above video message example, human viewers easily recognize a face.  If the face belongs to an individual\nwho is a celebrity or a person known by the viewer, the face may be recognized immediately by the viewer.\nIn one embodiment, therefore, to model an individual who has been identified, a database containing images of only this individual is utilized.  If the individual in video is not known, a database containing images of different types of faces is\nutilized to model the face of the individual.  Furthermore, if certain information about the individual, such as age range, sex and/or ethnicity, is known, the database may be optimized to contain images of types of faces consistent with the information.\nA region of high interest level is usually relatively complex, and the object in the region, for example a face, typically is not rigid.  Furthermore, the appearance of the object can vary greatly under different illumination conditions. \nTherefore, it is useful to provide different databases for different views of the object in the region.  For example, there may be a database for a front view of a face and databases for the face turned to the left and right by 45 or 90 degrees.  The\ndatabases preferably have sufficient commonly shared data to facilitate modeling transitions between views of the object.  When there are multiple databases specialized for different views, the motions of the face can be modeled with mapping functions\nhaving fewer parameters.  Since the parameters are sent from transmitter to receiver for synthesizing high interest region, fewer parameters are needed so that less bandwidth is required for the transmission.  When the transmitter transitions the\nmodeling from using one database to another database, the receiver can be notified by receiving a pre-determined code from the transmitter indicating the usage of the new database.\nOnce an initial approximation of the high interest region is made, as discussed in previous section, the next step is to determine which database to use for computing the corresponding mapping function.  In some instances, such as the video\nmessage example, the transmitter and the receiver both may be aware that the nature of the high interest region is the face.  If the transmitter is a mobile phone device capable of transmitting a video message, the high interest region is generally the\nface of the user of the mobile phone.  A database that corresponds the identity of the face can be used to compute the mapping function of face region of the video on the transmitter, such as a mobile phone; an instance of the same database is used to\nsynthesize an instance of the face region of the video on the receiver.\nIn some other instances, the nature or identity of the high interest level may not be known.  The appropriate database in such instances can be chosen by comparing the high interest region against multiple databases.  For example in the case of\na face, the regional location of the face can be found by methods such as the ones described above and a mapping function for synthesizing the face based on a general face database is used.  Such a general face database may contain images of the faces of\nmany individuals.  This technique, therefore, can pinpoint the boundaries and main regions of the face.  However, a general face database will not necessarily yield the best face synthesis results.\nIn another embodiment, for each database, a mapping function of the face is computed based on each database and an image of the face is synthesized based on each mapping function and the corresponding database.  The fidelity of the synthesized\nface can be calculated as the sum of pixel value differences between the original face and the synthesized face, where a lower sum corresponds to a higher fidelity.  A specialized database that yields the best fidelity of the synthesized region of the\nimage can be chosen.  For example, the face synthesis is tested against databases with different age ranges.  If one database with certain age range yields better fidelity than databases with other age ranges, further specialized databases with different\nethnicities in that age range may be tested for face synthesis, and so on.  Finally, a specialized database that yields a high fidelity of the synthesized face is chosen to model the high interest region, i.e. the face.\nFIG. 4 illustrates an example of a process of a transmitter selecting database for modeling the RoHI of a video.  In the example disclosed in this paragraph, the database is selected by the transmitter.  In some other embodiments, as disclosed\nin following paragraphs, the database may be selected by a server.  At the step of 401, the location of the RoHI is identified by a method, such as AdaBoost classifier mentioned in previous paragraphs.  At 402, for each database specific for a value of\nfirst criterion, a synthesis fidelity of the RoHI is computed.  For example, the first criterion may be gender, and the value of the first criterion (gender) can be male or female.  The synthesis fidelity (\"fidelity\") of the RoHI can be calculated by as\nthe sum of pixel value differences between the original RoHI and the synthesized RoHI, where a lower sum corresponds to a higher fidelity.  At 403, an optimal value of first criterion for the best synthesis fidelity is determined.  For example, the\noptimal value of first criterion may be female because the database specific for female yields better fidelity than the database specific for male.  Then, at 404, for each database specific for the optimal value of first criterion and for a value of\nsecond criterion, a synthesis fidelity of the RoHI is computed.  For example, the optimal value of first criterion may be female; the second criterion may be age range.  Accordingly, each of databases specific for female in age ranges of 20-30, 30-40,\n40-50 and 50-60 may be used to compute the synthesis fidelity.  At 405, an optimal value of second criterion for the best synthesis fidelity is determined.  For example, the optimal value of second criterion may be age range of 20-30 if the database\nspecific for female in age range of 20-30 yields better fidelity than databases specific for female in other age ranges.  Then, at 406, for each database specific for the optimal value of first criterion, the optimal value of second criterion, and for a\nvalue of third criterion, a synthesis fidelity of the RoHI is computed.  For example, the optimal value of first criterion may be female; the optimal value of second criterion may be age range of 20-30; the third criterion may be ethnicity.  Accordingly,\neach of databases specific for female in age ranges of 20-30, and with an ethnicity of Asian, Caucasian or African, may be used to compute the synthesis fidelity.  At 407, an optimal value of third criterion for the best synthesis fidelity is determined. For example, the optimal value of third criterion may be Asian if the database specific for Asian female in age range of 20-30 yields better fidelity than databases specific for female in other age ranges with other ethnicities.  Therefore, a database\nspecific for the optimal values of first, second and third criteria is selected for modeling the RoHI of the video.  For example, a database specific for Asian female in age range of 20-30 may be selected.  The sample process in FIG. 3 is for\nillustration purpose only.  An ordinary skilled person in the art will readily apply the principle of the process to select a database using other criteria.  The number of criteria may be greater or fewer.  Different criteria may be applied in different\nsituations\nFurthermore, the databases can be updated and supplemented when more videos are received and processed.  For example, as more videos of a particular user are captured by the transmitter, a more specialized database for the user can be\nconstructed.\n2.2.  Methods for Modeling High Interest Level Regions\nThe databases in at least some embodiments are shared between the transmitter and the receiver.  The general idea of compressing the high interest region is to model an instance of the region as a time series of mapping functions for an\nappropriate database for the region.  Using the above video message example, an instance of the face of an individual of a known age range, sex and ethnicity can be synthesized using a time series of mapping functions for a database of faces matching\nthis age range, sex and ethnicity.  For each frame, the transmitter computes a mapping function for the appropriate database, wherein each mapping function is represented as a set of parameters.  The transmitter sends a time series of sets of parameters\nrepresenting the time series of the mapping functions over to the receiver.  A time series of sets of parameters is a sequence of the parameter sets, wherein the parameter sets correspond to the frames of the video at successive times spaced by time\nintervals.  The receiver then uses these mapping functions, which are represented by the parameters, to synthesize an instance of the face.  The synthesis of the high interest level region is feasible because the receiver also has access to an instance\nof the same database.\nThe highly efficient compression of the high interest level region results from two facts.  First, the number of bits for the parameters needed to be transmitted to represent the mapping functions is much smaller than the bit size of the video\ndata of the region compressed by conventional methods such as JPEG or MPEG.  Second, the databases are specialized to model a known region of interest, such as a face of an individual.  Therefore, the number of the parameters required to synthesize the\nregion is low.\nIn certain embodiments, the mapping functions are parameterized.  For example, the parameters can be coefficients for a set of basis functions that are stored in a dictionary.  The receiver receives the parameters from the transmitter and\nrecreates the mapping functions to synthesize the high interest region, based on the dictionary available to the receiver.\nIn one example, the database contains images that are representative of the high interest region.  An instance of a high interest region of a frame can be synthesized by a combination of images in the database.  One simple example of a mapping\nfunction is a function that synthesizes the region of interest as a linear combination of the database images.  Thus, the mapping function in this case can be represented by the linear coefficients from the linear combination.  This simple example is\nprovided for illustrative purpose only.  More efficient compression methods are disclosed in the embodiments in following paragraphs.\nIn one embodiment, a method called principle component analysis (\"PCA\") is utilized for modeling the RoHI.  In the embodiment disclosed in this paragraph, the method is executed within the transmitter, such as a smartphone or a PC.  In some\nother embodiments, the method may be executed within a server.  FIG. 5 illustrates an example of a process of computing eigenvectors according to the PCA method.  A plurality of training images is supplied to the transmitter either from a training video,\nor from a database (501).  The training video may be a video of an individual for the purpose of building a database specific for the individual or it may be a pre-existing video created for another purpose.  The images can be individual frames of video,\nfor example.  A mean image (also referred to as average image, or mean vector) is calculated from the images (502).  The mean image is then subtracted from the individual images to produce a plurality of subtracted images (503).  These subtracted images\nhave a mean image of zero.  At 504, the autocorrelation matrix of these zero mean images is computed.  A matrix of the training data is formed where each row of the matrix represents one training image.  The product of this matrix with its transpose is\nthe autocorrelation matrix.  The eigenvectors of the autocorrelation matrix are the basis functions, while the magnitude of the corresponding eigenvalue is an indication of the importance of the eigenvectors.  Because the region of interest has\nconstraints on its pixels, e.g. the region represents the face of a person of known age, sex, and/or ethnicity; the mean image plus a linear combination of a few chosen eigenvectors (referred to as basis eigenvectors) from these eigenvectors are\nsufficient to represent an instance of region of interest with great accuracy (506).  In one embodiment, eigenvectors with the largest associated eigenvalues are chosen as basis eigenvectors, since an eigenvalue represents how far a corresponding\neigenvector is from the mean image.\nThe \"training\" of the basis eigenvectors can be carried out offline and is independent of the encoding process.  For example, a training video can be captured and the frames in the training video utilized as images to generate the basis\neigenvectors in the manner described above.  In one embodiment, the basis eigenvectors are included in a database specific for an individual, for encoding videos from the individual in the future.\nFIG. 6 illustrates an example of a process of a transmitter encoding a region in a video sequence and a receiver synthesizing the region in the video sequence, based on the basis eigenvectors in the database.  A high interest level region in\neach frame of a video is projected to the basis eigenvectors (601).  The region is then mapped as a linear function of the basis eigenvectors.  In this case, the projection coefficients are computed as the parameters representing the mapping function\n(602), which is a linear mapping function in this embodiment.  The parameters, i.e. the projection coefficients, are transmitted to a remote device or a receiver (603).  The receiver also has access to an instance of the same database containing the same\nset of basis eigenvectors.  At 604, the receiver synthesizes the high interest level region by calculating a linear combination of the basis eigenvectors in the database, wherein each of the projection coefficients is the linear coefficient for the\ncorresponding basis eigenvector.\nIn a related embodiment, an enhancement of this method is to model the shape and the texture of the high interest region separately.  This leads to an even smaller number of mapping functions for the modeling.  Similar approaches are discussed\nin articles including: \"On Lines and Planes of Closest Fit to Systems of Points in Space,\" Philosophical Magazine 2 (6): 559-572, 1901, by K. Pearson; \"Statistical Models of Appearance for Computer Vision,\" Technical Report, University of Manchester, 125\npages.  2004, by T. F. Cootes, C. J. Taylor; \"Active appearance models,\" Proc.  European Conf.  Computer Vision, 2:484-489, 1998, by T. Cootes, G. Edwards, and C. Taylor; and \"A morphable model for the synthesis of 3d faces,\" Proceedings of the 26th\nannual conference on Computer graphics and interactive techniques, pages 187-194.  ACM Press/Addison-Wesley Publishing Co., 1999, by V, Slant and T. Vetter; all of which are incorporated herein by reference in their entireties.\nImages in a database can be thought of as points in a high-dimensional space.  Each color component of a pixel in the image represents one axis of the high-dimensional space.  The dimensionality of this space is thus the number of pixels times\nthe number of colors (three) used to construct the image.  For an image of a 640.times.480 resolution, the dimensionality of the space is 640.times.480.times.3=921,600.  In the PCA method described above, the assumption is that because of the particular\nstructure of the region of interest, all instances of the region of interest lie on a linear subspace having a dimensionality much lower than 921,600, or at least the subspace is very close to a linear subspace.  Although the assumption leads to\npractically useful results, points representing instances of the region of interest do not necessarily lie in a linear subspace.  Furthermore, points lying in a linear subspace are not necessarily the most efficient representation of instances of the\nregion of interest, in terms of bitrate.\nA manifold method can take advantage of the observation of the preceding paragraph for even better compression.  Using knowledge of the structure of the images, it is possible to locate the region of interest on a manifold in the space, wherein\nthe manifold has much lower dimensionality than the dimensionality of the space.  Therefore, the region of interest is represented by points on the manifold, and requires fewer mapping functions for the database to achieve the representation.  Similar\napproach is discussed in the article of \"Nonlinear component analysis as a kernel eigenvalue problem,\" Neural Computation 10 (5), 1299-1399, 1998, B. SchJolkopf, A. J. Smola and K, MJuller, which is incorporated herein by reference in its entirety.\nIn some embodiments mentioned above, a video is segmented and modeled by the transmitter.  In some other embodiments, the video may also be segmented and modeled by a server.  For example, a video compressed by a know method such as H.264 is\nsent to the server: the video at the server is then modeled by the server and sent to a large number of receivers through ultra-low bandwidth channels.  FIG. 7 illustrates an example of a server segmenting and modeling a video.  A transmitter 701\nacquires (i.e., captures or otherwise receives inputs) a video of an individual and sends the raw video 721 to a server 702.  The raw video 721 may be generated and encoded in transmitter 701 using a standard video format supported by the hardware and\nsoftware of the transmitter 701.  The server 702 receives the raw video 721 and models the video based on a database specific to the individual.  Mapping functions 723 are developed based on any methods disclosed in other embodiments.  The mapping\nfunctions 723 are sent back to the transmitter 701.  The transmitter 701 synthesizes the video using the mapping functions, and calculates the synthesis fidelity of the mapping function in a similar way as disclosed in FIG. 4 and corresponding\nparagraphs.  If the fidelity is determined to be acceptable by transmitter 701, transmitter 701 sends an approval message 724 to the server 702.  Upon receiving approval message 724, the server 702 sends out (multicast) the mapping functions to one or\nmore receivers.  In some embodiments, the receivers may have different versions of the database.  Then the server translates the mapping functions accordingly for different versions of databases, and sends out to corresponding receivers.  The versions of\ndatabases and the translation are discussed in details in FIG. 8 and corresponding paragraphs.  The mapping functions 725 may be transmitted directly to a plurality of receivers 705-709.  Receivers 705-709 may have different operating systems, such as\niOS, Android, or Windows.  The mapping functions 725 may be transmitted through an intermediate private channel 710 and relayed to a plurality of receivers 711-714.  An intermediate private channel 710 may be a communication channel provided by an online\nsocial media service, such as Facebook or Twitter.  Comparing to sending out the raw video to all receivers, the disclosed method achieves a great saving in bandwidth.\n2.3.  Error Concealment Library\nTypically the chosen database is adequate for the transmitter to model the region of high interest level.  But sometimes it may be desired to have additional information, to further reduce the difference between the original region in the video\nand the synthesized instance of the region.  The transmitter and the receiver can share an error concealment library containing some image patches, or sets of image transformation operations, or a combination thereof.  The difference between the original\nregion in the video and the synthesized instance of the region can be further reduced by finding a patch or an image transformation operation that matches the difference.  Then an index to the patch or the image transformation operation can be sent over\nto the receiver for a better synthesis of the region.  Because the databases are constantly being updated, eventually the databases will be updated to be capable of modeling the difference, and the error concealment library will be needed to a lesser\ndegree.\nIn one embodiment, the error concealment library is utilized to model an individual's facial hair changes.  The existing database might not contain any images of the individual wearing a mustache.  A new video of the individual having a mustache\nneeds to be modeled, transmitted and synthesized.  Image shows a synthesized instance of a frame of the video using only the existing database.  An index to an image patch of a mustache from the error concealment library can be transmitted along with\nother mapping function parameters.  The receiver receives the index and adding a mustache to the video.  Video shows a synthesized instance of the video using both the existing database and the error concealment library.  Eventually the database is\ntrained to contain images of the individual wearing the mustache, such that the library is no longer needed for correcting the mustache difference.\nIn another embodiment, an image transformation operation from the error concealment library can be utilized.  An image transformation operation can operate on a predetermined group of pixels, for example a 10.times.20 pixel region.  The library\nstorage size of the operation might be comparable to an image patch.  For an instance, an image transformation operation might do a linear or nonlinear filtering that is pre-determined and indexed.  The operation, along with all contents of the error\nconcealment library, is assumed to be available to both the transmitter and the receiver.\n2.4.  Further Bitrate Reduction for Non-Real Time Video Transmission\nThe above description relates to transmission of video in real time or near real time.  If the video does not need to be transmitted in real time, further techniques can be utilized to compress the data to be transmitted for a faithful synthesis\nof the region of interest.  Thus, the transmission bitrate is further reduced when the video is compressed and transmitted in non-real time.  The term \"non-real time\" in this disclosure herein means that there is certain amount of time delay, between the\ntime when a frame the video is captured or transmitted and the time when the frame of the video is synthesized and available for display at the receiver, which is large enough to be noticeable by human, such as 20 seconds.\nIn one embodiment, the mapping function does not change substantially for a number of consecutive frames of the input video sequence.  The receiver can be instructed to use the same set of parameters to represent the mapping function for the\ncorresponding multiple consecutive frames.\nIn another embodiment, mapping functions for multiple consecutive frames can be interpolated linearly or nonlinearly based on the parameters of mapping functions for the frames immediately before and after the consecutive frames.  The sets of\nparameters for the consecutive frames need not to be transmitted to the receiver.  Instead, the receiver only needs to be instructed to interpolate the mapping functions between the frames immediately before and after the consecutive frames.\nIn yet another embodiment, the parameters of mapping functions for some frames can be approximated by combinations of parameters for other frames.  Instead of transmitting the parameters, the receiver is instructed to calculate the parameters\nfor certain frames based on the parameters for other frames.\nIn still another embodiment, the subjective quality of the video is not sensitive to a small variation of parameters for mapping functions.  Therefore, a small number of bits are sufficient to represent the parameters for the mapping functions.\n3.  Modeling and Synthesis of Regions of Medium and Lower Interest Levels\nUnlike the regions of high interest levels, there is usually little knowledge in the databases about the regions of medium and low interest levels.  In the video message example mentioned above, regions of medium interest levels might include\nhair and shoulders.  There may be some knowledge about such a region, such as hair style and color of clothes.  But this knowledge is not very dependable since hairstyle can change and clothes can be different.  There may be no a priori knowledge at all\nabout the regions of low interest levels.  On the other hand, these regions are of lower interest by assumption.  These regions do not need to be rendered as accurately as the high interest region.  Still these regions need to be realistic enough for the\nuser at the receiver end to be satisfied by the overall video quality; and the synthesized instances of these regions need to be non-obtrusive so that the user at the receiver end is not distracted.\nIn one embodiment, the regions of medium or low interest levels might be objects of which the appearance is irrelevant to the message.  For example, the background could be a tree, a flower, or items such as a flag.  If the processor in the\ntransmitter recognizes the object, and find a index to object in the object server directory, it could simple provide an index reference to the object in the frame.  Typical these objects are not moving in the consequent frames.\nIn one embodiment, the compression of the medium and low interest regions can be done by a vector quantization (\"VQ\") method.  Similar approach is discussed in the article of \"An Algorithm for Vector Quantizer Design,\" IEEE Transactions on\nCommunications, pp.  702-710, January 1980, Y. Linde, A. Buzo, and R. M. Gray, which is incorporated herein by reference in its entirety.  Each of the regions are further segmented into n.times.m rectangular pixel sections, wherein n and m may be\ndifferent for each region depending on the interest level.\nThese n.times.m sections are denoted as tiles.  Each image has a given number of tiles K. Assuming the video has T frames in total, there are T.times.K tiles in the video.  For each frame, a number of adjacent tiles are chosen as center tiles. \nFor example, 16 or 32 center tiles may be used.  Each the in each frame of the video is replaced by a center tile closest to the tile.  The center tiles are small images which can be further compressed by representing them in Fourier space via the\ndiscrete cosine transform (\"DCT\") and discarding indices for the small high-frequency components.  The important DCT values are sent to the receiver to reconstruct the tiles.  Compression is achieved by sending the indices of tiles to the receiver.  In\none embodiment, these indices can be further compressed, in both space and time domains, with methods such as run length encoding (\"RLE\") and entropy encoding.  The RLE and entropy method is efficient because of the small number of center tiles used, as\nunderstood by a person having ordinary skill in the art.\n3.1.  Special Treatment of Low Interest Level Regions\nIn the low interest level region, the tiles are not being updated often in time.  The user will see a synthesized video with the low interest region substantially unchanged in time.  This is hardly noticeable since the user likely has little\ninterest in the region.  Movement in the low interest level region can actually distract the user's attention from the regions of higher interest levels.  Therefore, the fixed background, i.e. the low interest level region, is desirable.  In one\nembodiment, there is no data transmitted for the low interest region.  The receiver synthesizes the video by filling in the low interest region in each frame with a predetermined background image that is available to the receiver.\n3.2.  Special Treatment of Medium Interest Level Regions\nTypically, the medium interest regions are not fixed in time.  For example the hair moves along with the face.  In these regions, further compression is achieved by tracking the evolution of the indices of the center tiles in time.  It is\npossible some of the center tiles will stay constant over a time period.  In the video message example, a portion of clothing in the shoulder area may stay unchanged over a time period.  The observation of the region of the frame staying unchanged can be\ncommunicated to the receiver.  The receiver treats the indicated center tile in the same way as the center tiles in the low interest level region.\nSome center tiles in the medium interest level region need to be updated in time.  For these center tiles, motion estimation can be carried out for interpolation.  Instead of sending indices of a center tile to the receiver at every frame, the\nreceiver may only receive indices for every N'th frame and interpolate the intermediate frames.  The value of N depends on the speed of motion.  The faster the movement, the smaller N has to be to give satisfying results.  For example, for a rapid flick\nof a head, the center tiles for the region around the head need to be updated frequently.\n4.  Database Synchronization between Transmitter and Receivers\nIn one embodiment, a centralized server system (i.e., one or more server computers) is used to create and/or maintain all of the above-described databases and to track the versions of the databases.  The databases can be distributed to the\nclients, i.e. the transmitters and receivers, on an as-needed basis.  In the video message example, all databases for the individual specific, age specific, ethnicity specific faces are stored in the server with a version number noted.\nWhen a transmitter sends parameters of a mapping function to one or more receivers, the transmitter also sends the identification and version number(s) of the database to the server.  For example, the receiver may communicate to the server that\nit has chosen to use age 30 to 35, female database and the transmitter has versions 3.0, 4.4 and 5.2 of this database.  The server checks with receivers to see what version of the database the receivers have.\nThe receivers respond to the server with the version numbers of the versions of this database that are available to the receivers.  In one embodiment, an option is to select the lowest version commonly available to all involved clients.  In\nanother embodiment, if most of the receivers have a newer version of this database which is also available to the transmitter, the transmitter is instructed to use the newer version.  Some receivers may not have the newer version of this database or may\nnot have any version of this database.  The server in such cases sends the newer version of the database to the receiver which lacks the newer version.\nIn yet another embodiment, one receiver does not have the newer version of the database.  The server therefore translates the parameters of the mapping function based on the newer version, to parameters of a mapping function based on an older\nversion of the database that is available to the receiver.  FIG. 8 illustrates an example of the server translating the mapping function parameters between different versions of databases.  For example, the transmitter 801 sends out the parameters based\non a database of particular version, say version 5.2; while the receiver 803 only has the database of another version, version 5.1, and the receiver 804 only has the database of yet another version, version 4.3.  The server 802 synthesizes the video\nregion using version 5.2 database; and then computes mapping function parameters of the synthesized video region based on version 5.1 database, as well as parameters based on 4.3 database.  The server 802 then transmits the translated mapping function\nparameters (version 5.1) to the receiver 803, and transmits the parameters (version 4.3) to the receiver 804.  The receiver 803 receives the parameters (version 5.1) and is able to synthesize the video region using the appropriate available database of\nversion 5.1; and the receiver 804 receives the parameters (version 4.3) and is able to synthesize the video region using the appropriate available database of version 4.3\nIn still another embodiment, instead of translating the mapping function parameters for the receivers, the server sends the newer version of the database to the receivers.  For example, server 802 may send the database 5.2 to receivers 803 and\n804, so that receivers 803 and 804 are able to synthesize the video region directly from mapping function parameters based on version 5.2.\nIn yet still another embodiment, there is no server operating between the transmitter and the receiver as an intermediary.  The transmitter communicates with the receiver and determines the types of the databases and versions commonly available\nto both sides.  The transmitter encodes the region of interest using a commonly available database available to both transmitter and receiver; the receiver then synthesizes the region using an instance of the same database.\nIn still yet another embodiment, the transmitter does not have the compression capability or wireless access.  The video sequence is delivered to central server.  In turn, the server compresses the video sequence and transmits the information to\nall the intended receivers in a compressed form.\n5.  Quality of Service (QoS) Selection: Target Bitrate vs.  Choice of Modeling Method\nThe above sections have disclosed how regions of various interest levels are encoded in different ways.  For each of these regions, there is a tradeoff of quality versus bitrate.  For example, for the high interest level region, there is a\ntradeoff between the video quality and the number of parameters used to represent the mapping function.  The greater the number of parameters transmitted to the receiver per frame, the more accurate the mapping functions for representing the region will\nbe, but at the cost of a higher bitrate.  Similarly, for the medium and low interest level regions, in order to synthesize more accurate regions, the method can use a greater number of tiles, smaller dimension of the tiles, and/or a greater rate of the\nindex updates per frame.  Accordingly, the bitrate of the data transmission will increase.\nWhen a session for video transmission is established between a transmitter and one or more receivers, a target quality of service PoS) will be negotiated.  The QoS establishes a total bitrate budget which poses a constraint on the video quality. Techniques described above will in turn establish separated bitrate budgets for regions of high, medium and low interest levels.  The regions of high interest levels have top priority, in terms of allocating bitrate budget.  In the video message example,\nif the total bitrate budget is very low, the face still needs to be rendered accurately.  The low interest level region can be represented as just a single color uniform background to save the quota of the bitrate budget.  In another example, higher\ntotal bitrates may be allowed after the negotiation between the transmitter and receivers, where separate bitrate budgets are assigned to each region.\n6.  Synthesizing Video by Stitching of Regions.\nBecause of the different treatments of the regions of different interest levels, the high interest region will have better resolution than the medium and low interest regions.  A raw combination of the regions will result in a video with an\nunacceptable quality.  The high interest level region may therefore appear \"separated\" from the rest of the video image.  It is possible to blur certain selected sections of the video image to alleviate this \"separation\" effect.  However, it is not\ndesirable to blur the entire high interest level region, which defeats the purpose of a high resolution synthesis of the region at low bitrate.\nBecause of the nature of modeling the high interest level region, the boundary of the region is known precisely.  Instead of blurring the entire high interest level region, only small sections on the boundary of the high interest region may be\nblurred using standard image processing techniques known by persons of ordinary skill in the art.  This treatment results in natural looking video while the high resolution of the high interest level region is not sacrificed.\nIn one embodiment, low and medium interest level regions are represented by relatively large tiles.  In order to avoid the blocky appearance in these regions, blurring is carried out on the boundaries of the tiles in these regions as well.  This\nblurring further directs the viewer's attention to the high interest level region.\n7.  Voice Modeling and Synthesizing\nThe goal of the voice modeling technique introduced here is to compress and transmit high quality voice information for remote synthesis at a low bitrate.  In one embodiment, for a known speaker, the data rate of the transmitted voice sequence\nis about from 200 to 500 bits per second (\"bps\").  The MOS of the transmitted voice sequence is at least 4, where a maximum score of MOS is 5, so that voice features of the speakers such as accent, pronunciation, or reconcilability can be accurately\nmodeled and synthesized.  Accordingly, the voice sequence can be transmitted over the ultra-low bandwidth channel as part of the video.  The voice sequence may be speech, singing, or a combination thereof.  In one embodiment, the voice sequence is\nextracted from the video and analyzed for its correlation property in an acoustical sense.\nIn one embodiment, several assumptions are made.  First, the message length is longer than 20 seconds.  Second, there is an adaptive self-learning codebook present at the transmitter and the receiver side.  Third, there is certain information\nabout the speaker available in the codebook.\nIn order to achieve a massive compression while maintaining a high quality of sound, a plurality of dictionaries containing information for a speaker from multiple perspectives are collected and are made available to both transmitter and\nreceiver, such as:\n1.  Dictionary of acoustic waveform models and features vectors such as cepstral patterns, or mel-frequency cepstral coefficients (\"MFCC\");\n2.  Dictionary of sounds and phonemes;\n3.  Dictionary of words, the dictionary may contain emotional context tags associated with the words;\n4.  Dictionary of frequently used sentences and/or phrases, typically from 1 second to 10 second long;\n5.  Dictionary of features of singing voices and models;\n6.  Dictionary of Visual speech model such as lips models and positions, and other facial features; and\n7.  Dictionary of hierarchical maps and Bayesian Networks relating the above 6 dictionaries.\nCepstral patterns are frequency domain patterns obtained by taking the Fourier transform of the voice record.  For purposes of representing human voice features, the spectrum is usually first transformed using the mel scale, the resulting\ncepstral coefficients are called mel-frequency cepstral coefficients (MFCC).\nThe seventh dictionary contains Bayesian networks and hierarchical maps that connect and correlate the entries of each dictionary.  For example, a recognized word, through an automatic speech recognition method, may match an entry in the third\ndictionary, and may correlate to the lip model in the sixth dictionary, based on the conditional dependencies in the Bayesian networks.  The Bayesian networks are pre-mapped.\nThe entries of the dictionaries are utilized for voice modeling based on the hierarchical maps.  It is preferred to use high level dictionaries.  For example, the sentences and words dictionary is used in preference to the acoustical waveform\ndictionary.  Using high level entries enables higher compression efficiency while maintaining a high quality of the voice.\nDepending on the time delay sensitivity of a transmission scenario, a time windows may be available for analysis and compression.  In one embodiment, up to 30 seconds of delay is allowed and available for correlation window analysis.  Methods\nsuch as factor analysis, linear and nonlinear regression analysis may be used to obtain maximum correlation properties of the waveform and the associated synthesis models that are adaptive in nature and have different properties.\nFIG. 9 is a block diagram of a voice modeling algorithm to be executed by a transmitter or a server, depending on which device is carrying out the transmission.  The architecture is based on adaptive machine learning and mutual information\navailable for both transmitter and receiver.  Depending on the time delay window, a waveform vocal encoder (\"vocoder\") 902 is utilized to extract correlation information for short time scales (less than 25 msec), for medium time scales (less than 450\nmsec), for long time scales (less than 30 sec).  Using regression analysis, correlation information in the longest possible time period is factored to model the voice.  For example, linear prediction filters can be used to model the correlation in short\ntime scales of less than 25 msec.  The best model or combinations of models are used to model the current voice, and appropriate coefficients for model or models are transmitted.  In some scenarios greater time delay is allowed in the given scenario, and\ncorrelation information from this longer time period can be found and represented by indices to the dictionaries.  Phonetic vocoder 903 contains tagging information corresponding to the phonetic sounds.  The higher level phonetic models are language\ndependent models to model the voice sequence.  Even higher models can be used for modeling, such as the speaker dependent text-to-speech (TTS) 904 using automatic speech recognition (ASR), with visual information tagging extracted form the video and\nemotional speech information.\nAll data represented by different levels of models is synthesized within the transmitter for comparison with the original voice record 901.  The waveforms of the voice sequence are segmented into sections.  An error minimizer 906 runs error\nanalysis for different models of the voice sequence as a function of time.  A maximum likelihood waveform selector 905 then determines one or more models best suited for modeling each section of the voice sequence, based on calculations from an error\nminimizer 906.  In some cases, frequently used sentences and words 908 can be utilized to model some sections of the voice sequence.  A segmentation and reassembly unit 907 reassembles the sections of the voice sequence based on the information of\noptimal models for each section from the maximum likelihood waveform selector 905.  The reassembled data corresponding to different models for reach section is transmitted to the receiver to represent the voice sequence.\nIn one embodiment, the above dictionaries at the beginning typically have fairly small entries.  Over time, through frequent usage and interaction with core server, the dictionaries will learn and be updated with more content.  In one\nembodiment, for dictionary 3 and 4 of each user, eventually the estimated entries will be more than 1000 words and over 100 sentences respectively.\nIn one embodiment, each dictionary for a speaker has three codebooks:\n1.  Basis Codebook (\"BC\").  Basis codebook is pre-calculated and shared by all communication clients (transmitters and receivers) and servers during service registration.  The basis codebook is synchronized before communication begins.  These\nspeech and/or sound corpora are generally language, age, gender and accent dependent.  A corpus is a large and structured set of texts.  The basis codebook for each person is fairly large in size.\n2.  Similarity Driven Generative Codebook (\"SDGC\").  SDGC is created and optimized for each user on the transmitter.  A copy of SDGC also resides on the server.  In one embodiment, a receiver may not have direct access to the latest version of\nSDGC.  SDGC is generated based on partial information available from the user.  An estimate of the full size codebook is algorithmically generated.  Methods such as Gaussian processes and maximum likelihood techniques will be used to fill in the content\nof the SDGC as a placeholder.  As more information becomes available, the estimates will be removed and will be replaced with actual information.  SDGC is being updated frequently when more voice information is received from the speaker.\n3.  Real-time Codebook (\"RC\") is where a new codebook generated from the voice sequence data in real-time and is used to compensate for the missing information which did not exist in existing BC and SDGC.  RC is generated on-the-fly to provide\nsupplemental models for catching further differences between the original voice sequence and the sequence modeled by BC and SDGC.  In one embodiment, the content of RC will be updated to the SDGC after the session.  In another embodiment, the RC will be\ndiscarded after the session.\nThe combination of these codebooks will achieve a high compression of the voice sequence by maximizing the amount of mutual information between the transmitter and the receiver.  Accumulation and update of mutual information is an iterative\nprocess that will be built over time through transactions between transmitters and receivers.\nThe codebook is initially generated by collecting a massive dataset from various speakers, for example more than of 1000 different speakers.  The data may be collected from various public sources such as radio broadcasts, movies, and other\nrecordings of various types.  Off-line calculations are carried out by use of K-mean methods with multiple different dimensions.  A subset of the codebook is selected as the basis codebook set; so that through linear or nonlinear regression methods,\nother codebooks can be generated.  The generated codebook may be a close approximation of a personalized codebook for an individual.\nIn one embodiment, several assumptions are made.  First, the message length is longer than 20 seconds.  Second, there is an adaptive self-learning codebook present at the transmitter and the receiver side.  Third, there is certain information\nabout the speaker available in the codebook.\nThe acoustical information includes all formant frequencies, such as F0, F1-F6.  The acoustical information further includes various transition parameters relevant to changes in formant frequencies.  Acoustic features of voice may be broken into\ntwo parts because they show different autocorrelation property.  Voiced speech is produced by exciting the vocal tract with quasi-periodic glottal air pulses generated by the vibrating vocal chords.  The frequency of the periodic pulses is referred to as\nthe fundamental frequency or pitch.  Unvoiced speech is produced by forcing air through a constriction in the vocal tract.  Typically, the unvoiced part is a white noise like signal with a broad spectrum and a low energy.  The voiced part is highly\nperiodic with well-behaved auto-correlation properties.\nBasic waveform voice encoders Vocoders\") can be utilized to synthesize speech with a short-term correlation window(less than 25 msec).  More compression can be achieved if the correlation window is extended.  Medium-term correlation (less than\n450 msec) and long-term correlation (less than 30 sec) are chosen for finding better VQ models and more correlation properties Phonetic voice encoders can be utilized to generate phonetic formant frequency information about short-term correlation (less\nthan 25 msec), medium-term correlation (less than 450 msec) and long-term correlation (less than 30 sec).\nAt the transmitter, automatic speech recognition (\"ASR\") method is also applied to the voice sequence to generate a text sequence.  As illustrated in FIG. 10, the ASR method extracts the MFCC features as MFCC coefficients (1002) from the voice\nsequence 1001.  The MFCC coefficients are fed to a Hidden Markov Model (\"HMM\") engine (1003) to determine the text (1004).  These coefficients are extracted based on the context and/or emotion tagging Information about pitch frequency F0, time\nstatistics, frequency statistics, time and frequency joint statistics, is extracted from the ASR module.  Excitation coefficients for speech synthesis are obtained and may be modified from a joint decision between available sets of stored sound vectors. \nThe generated excitation coefficients are fed to source models and filters modules to synthesize a sound wave as an instance of synthesized voice sequence from the text sequence with context/emotional tagging,\nAt the receiver, coded speech is fed to a vocoder to play back the speech feeding information about excitation codes and source filter models that were used at the transmitter based on available mutual information shared by both the transmitter\nand the receiver.  In the case when only text is received by the receiver a speaker dependent text-to-speech (\"TTS\") method is used to synthesize an instance of the voice sequence by using the text sequence, the visual information tags, the emotional\ninformation, waveform torment frequency information and phonetic formant frequency information.\nApproaches similar to methods disclosed in this section are discussed in articles including: \"Quantization of LPC parameters\", Speech Coding and Synthesis, Amsterdam, Elsvier Science B. V., 1995, K. K. Paliwal, W. B. Kleijn; \"Speech coding, a\ntutorial review\", Proceedings of IEEE, October 1994, A. S. Spanias; \"Speech coding algorithms: Foundation and Evolution of Standardized Coders\", John Wiley and Sons, 2003, W. C. Chu; \"Linear inter-frame dependencies for very low bit-rate speech coding,\"\nSpeech Communication 34 (2001) 333-349, J. M. Lopez-Sole, et al; \"A tutorial on Hidden Markov Models and Selected Applications in Speech Recognition,\" Proceedings of IEEE, February 1989, L. R. Rabiner; \"Vowel Spectra, Vowel Spaces, and Vowel\nIdentification,\" Journal of Acoustical Society of America, Vol 48, 1970, W. Klein, et al; all of which are incorporated herein by reference in their entireties.\n8.  Implementation of Transmitters, Receivers and Servers.\nThe transmitters and receivers disclosed in the embodiments may be any devices that have network communication abilities and processing powers to model or synthesize the video.  The server may be any computer or group of computers that each has\nnetwork communication abilities and storage capability to store the databases.  The transmitter, receiver, and server may be devices such as mobile phones (e.g., \"smartphones\"), PDAs, portable computers, desktop computers, netbooks, tablets, network\nservers, storage servers.\nFIG. 11 is a block diagram of a processing system that can be used to implement any of the techniques described above, as a transmitter or a receiver or both.  Note that in certain embodiments, at least some of the components illustrated in FIG.\n11 may be distributed between two or more physically separate but connected computing platforms or boxes.  The processing can represent a conventional server-class computer, PC, mobile communication device (e.g., smartphone), or any other known or\nconventional processing/communication device.\nThe processing system 1101 shown in FIG. 11 includes one or more processors 1110, i.e. a central processing unit (CPU), memory 1120, at least one communication device 1140 such as an Ethernet adapter and/or wireless communication subsystem\n(e.g., cellular, Wi-Fi, Bluetooth or the like), and one or more I/O devices 1170, 1180, all coupled to each other through an interconnect 1190.\nThe processor(s) 1110 control(s) the operation of the computer system 1101 and may be or include one or more programmable general-purpose or special-purpose microprocessors, microcontrollers, application specific integrated circuits (ASICs),\nprogrammable logic devices (PLDs), or a combination of such devices.  The interconnect 1190 can include one or more buses, direct connections and/or other types of physical connections, and may include various bridges, controllers and/or adapters such as\nare well-known in the art.  The interconnect 1190 further may include a \"system bus\", which may be connected through one or more adapters to one or more expansion buses, such as a form of Peripheral Component Interconnect (PCI) bus, HyperTransport or\nindustry standard architecture (ISA) bus, small computer system interface (SCSI) bus, universal serial bus (USB), or Institute of Electrical and Electronics Engineers (IEEE) standard 1394 bus (sometimes referred to as \"Firewire\").\nThe memory 1120 may be or include one or more memory devices of one or more types, such as read-only memory (ROM), random access memory (RAM); flash memory; disk drives; etc. The network adapter 1140 is a device suitable for enabling the\nprocessing system 1101 to communicate data with a remote processing system over a communication link, and may be, for example, a conventional telephone modem, a wireless modem, a Digital Subscriber Line (DSL) modem, a cable modem, a radio transceiver, a\nsatellite transceiver, an Ethernet adapter, or the like.  The I/O devices 1170, 1180 may include, for example, one or more devices such as: a pointing device such as a mouse, trackball, joystick, touchpad, or the like; a keyboard; a microphone with\nspeech recognition interface; audio speakers; a display device; etc. Note, however, that such I/O devices may be unnecessary in a system that operates exclusively as a server and provides no direct user interface, as is the case with the server in at\nleast some embodiments.  Other variations upon the illustrated set of components can be implemented in a manner consistent with the invention.\nSoftware and/or firmware 1130 to program the processor(s) 1110 to carry out actions described above may be stored in memory 1120.  In certain embodiments, such software or firmware may be initially provided to the computer system 1101 by\ndownloading it from a remote system through the computer system 1101 (e.g., via network adapter 1140).\nThe techniques introduced above can be implemented by, for example, programmable circuitry (e.g., one or more microprocessors) programmed with software and/or firmware, or entirely in special-purpose hardwired circuitry, or in a combination of\nsuch forms.  Special-purpose hardwired circuitry may be in the form of, for example, one or more application-specific integrated circuits (ASICs), programmable logic devices (PLDs), field-programmable gate arrays (FPGAs), etc.\nSoftware or firmware for use in implementing the techniques introduced here may be stored on a machine-readable storage medium and may be executed by one or more general-purpose or special-purpose programmable microprocessors.  A\n\"machine-readable storage medium\", as the term is used herein, includes any mechanism that can store information in a form accessible by a machine (a machine may be, for example, a computer, network device, cellular phone, personal digital assistant\n(PDA), manufacturing tool, any device with one or more processors, etc.).  For example, a machine-accessible storage medium includes recordable/non-recordable media (e.g., read-only memory (ROM); random access memory (RAM); magnetic disk storage media;\noptical storage media; flash memory devices; etc.), etc.\nThe term \"logic\", as used herein, can include, for example, programmable circuitry programmed with specific software and/or firmware, special-purpose hardwired circuitry, or a combination thereof.\nIn one embodiment, a method is introduced.  The method comprises: capturing a video sequence at a mobile communication device; sending a data sequence representative of the video sequence from the mobile communication device to a remote device\nover a wireless communication link in an ultra low bit rate channel; receiving the data sequence at the remote device; and synthesizing the video sequence based on the data sequence and a database mutually available to both the mobile communication\ndevice and the remote device.\nIn a related embodiment, the database comprises a visual characteristics model of human.  In another related embodiment, the database further comprises steps of building a database having a visual characteristics model of human based on a\nplurality of images; and transmitting the database to the mobile communication device and the remote device.  In another related embodiment, the method further comprises a step of identifying, by the mobile communication device, a cognitively visually\nimportant region in the frames of the video sequence as a region having cognitive visual importance from a real or hypothetical human viewer's perspective.  In another related embodiment, the method further comprises a step of developing the data\nsequence based on the cognitively visually important region in the frames of the video sequence, the data sequence representing a time series of mapping functions of a database, the mapping functions modeling visual characteristics of a person.  In\nanother related embodiment, the step of identifying comprises: segmenting each frame of the video sequence into a plurality of regions; and evaluating each of the plurality of regions based on the database and recognizing a cognitively visual important\nregion of the plurality of regions based on a result of said evaluating.  In another related embodiment, the data sequence represents a time series of mapping functions of the database, and the mapping functions model visual characteristics of a person. \nIn another related embodiment, the database comprises a visual characteristics model of human, and the mapping functions models the visual characteristics of the person based on the visual characteristics model in the database.  In another related\nembodiment, the step of synthesizing comprises synthesizing the video sequence by calculating the time series of mapping functions based the database\nIn another embodiment, there introduced another method.  The method comprises: capturing an audio sequence of a person; recognizing a set of texts, an associated set of emotions, and an associated set of formant frequencies from the audio\nsequence; updating a codebook with the set of texts, the associated set of emotions, and the associated set of formant frequencies; generating a time-stamped set of indices of the set of texts, the associated set of emotions; and the associated set of\nformant frequencies to the codebook; transmitting the time-stamped set of indices to a remote device; and reconstructing an instance of the audio sequence based on the time-stamped set of indices and the codebook.\nIn another embodiment, there introduced another method.  The method comprises: creating a dictionary having a basis codebook set, a similarity driven generative codebook set and a real-time adaptive codebook set, each codebook set containing one\nor more codebooks, each codebook including indices to sound vectors, associated formant frequencies, words and phrases; updating the basis codebook set by analyzing audio sequences from a plurality of persons; updating the similarity driven generative\ncodebook set to model a person's audio characters; capturing an audio sequence of the person; updating the real-time codebook set by analyzing the audio sequence; generating a set of parameters based on the dictionary; transmitting the set of parameters\nto a remote device; and regenerating the audio sequence based on the set of parameters and the dictionary.\nIn another embodiment, there introduced another method.  The method comprises: modeling a voice sequence of a person by generating indices to a plurality of voice models store in a plurality of dictionaries; segmenting the voice sequence into a\nplurality of sections; choosing one or more optimal models for each section of the plurality of sections; reassembling a data sequence representing the voice sequence by combining the indices to the optimal models for each section of the plurality of\nsections; and transmitting the data sequence to a remote device.\nIn a related embodiment, the plurality of dictionaries includes a dictionary of acoustic waveform models.  In another related embodiment, the plurality of dictionaries includes a dictionary of sounds and phonemes.  In another related embodiment,\nthe plurality of dictionaries includes a dictionary of words.  In another related embodiment, the dictionary of words further includes emotional context tags.  In another related embodiment, the plurality of dictionaries includes a dictionary of\nsentences and phrases.  In another related embodiment, the plurality of dictionaries includes a dictionary of singing voice models.  In another related embodiment, the plurality of dictionaries includes a dictionary of visual speech models.  In another\nrelated embodiment, the plurality of dictionaries includes a dictionary of hierarchical maps and Bayesian Networks relating to entries of other dictionaries of the plurality of dictionaries.\nIn another related embodiment, each of the plurality of dictionaries includes a basis codebook.  In another related embodiment, each of the plurality of dictionaries includes similarity driven generative codebook.  In another related embodiment,\nthe method further comprises a step of generating a real-time codebook to compensate a difference between the voice sequence and a synthesized instance of the voice sequence based on the plurality of voice models.  In another related embodiment, the\nindices to the acoustic waveform models are generated by computing correlation information in at least a portion of the voice sequence.  In another related embodiment, the indices to the sounds and phonemes are generated by computing correlation\ninformation in at least a portion of the voice sequence.  In another related embodiment, the indices to the words of the dictionary are generated by an automatic speech recognition method.  In another related embodiment, the step of choosing comprises\nchoosing one or more optimal models for each section of the plurality of sections using a maximum likelihood waveform selector.\nIn another embodiment, there described a method.  The method comprises: receiving, at a receiving device, a data sequence representative of a video sequence sent from a sending device via a wireless communication link; and synthesizing the video\nsequence at the receiving device based on the data sequence and mutual information available to both the sending device and the receiving device.\nIn a related embodiment, the mutual information comprises a visual characteristics model of human.  In another related embodiment, the mutual information is in a database.  In another related embodiment, the data sequence represents a time\nseries of mapping functions of a database, and the mapping functions model visual characteristics of a person.  In another related embodiment, the database comprises a visual characteristics model of human, and the mapping functions models the visual\ncharacteristics of the person based on the visual characteristics model in the database.  In another related embodiment, the step of synthesizing comprises synthesizing the video sequence by calculating the time series of mapping functions based the\ndatabase.  In another related embodiment, the data sequence is a time-series of sets of parameters, and each set of parameters represents one mapping function of the time-series of mapping functions.  In another related embodiment, the time-series of\nsets of parameters are a time series of sets of projection coefficients, and the time series of the set of projection coefficients is generated by projecting a cognitively important region of each frame of the video sequence on a set of the basis\neigenvectors in a dictionary.  In another related embodiment, the step of synthesizing comprises synthesizing the video sequence by calculating a linear combination of the basis eigenvectors with the projection coefficients.\nIn another embodiment, there described a method.  The method comprises: building a database having a visual characteristics model of one or more objects, the one or more objects including a person based on a plurality of images; and transmitting\nthe database to at least one remote device.\nIn a related embodiment, the visual characteristics model comprises a plurality of eigenimages, and each of the plurality of eigenimages represents a variation of the visual characteristics model.  In another related embodiment, the plurality of\neigenimages is generated based on the plurality of images.  In another related embodiment, the plurality of eigenimages are a set of basis eigenvectors, and each of the plurality of basis eigenvectors represents a direction in which images differ from a\nmean image of the plurality of images.  In another related embodiment, the plurality of images is extracted from a video from a person.  In another related embodiment, the step of transmitting comprises transmitting the dictionary to at least one video\ntransmitting device and at least one remote device.  In another related embodiment, the plurality of basis eigenvectors are calculated from an autocorrelation matrix, the autocorrelation matrix is constructed using the plurality of images.\nIn addition to the above mentioned examples, various other modifications and alterations of the invention may be made without departing from the invention.  Accordingly, the above disclosure is not to be considered as limiting and the appended\nclaims are to be interpreted as encompassing the true spirit and the entire scope of the invention.", "application_number": "13334726", "abstract": " Techniques for transmitting and sharing a video sequence over an\n     ultra-low bandwidth channel, such as a short message service (SMS)\n     channel, are disclosed herein. A video is segmented into regions of\n     various interest levels. A set of parameters is developed from a video\n     region of a high interest, wherein the parameters represent a mapping\n     function of a database to model the video region. The set of parameters\n     is transmitted over the ultra-low bandwidth channel to a remote device,\n     wherein the remote device also has access to an instance of the database.\n     The remote device synthesizes the video by using the mapping function of\n     the database, which is represented by the transmitted set of parameters.\n", "citations": ["5946419", "6250928", "6366885", "6449595", "6539354", "6735566", "6813607", "6826530", "6970820", "7248677", "7298256", "7636662", "7664645", "8224652", "20010016008", "20010026631", "20020034319", "20020120450", "20030018475", "20040218827", "20050203743", "20070082700", "20070254684", "20090252481", "20100217817", "20100232384", "20100296571", "20100302254", "20110050878", "20110115799", "20110234825", "20130124206", "20140064578"], "related": ["61426441", "61483571", "61544123"]}, {"id": "20140187242", "patent_code": "10375629", "patent_name": "Service preferences for multiple-carrier-enabled devices", "year": "2019", "inventor_and_country_data": " Inventors: \nZhang; Hongliang (Samammish, WA)  ", "description": "<BR><BR>BACKGROUND\nTelecommunication devices have evolved from mobile replacements for the telephone to all-in-one communication, media, and productivity solutions.  In addition to voice calling, telecommunication devices now support video and song playback,\ncalendaring, and a variety of features requiring communication over a packet-based network.  Such features include web browsing, video streaming, video chat, and many others.  To support such packet-based communications, network operators have enhanced\ntheir circuit-based telecommunication network offerings by building and offering packet-based telecommunication networks, such as Long Term Evolution (LTE) and Evolved High-Speed Packet Access (HSPA+) networks.  In addition to packet-based\ntelecommunication network services, telecommunication devices are now also typically equipped to engage in packet-based communications through wireless data networks, such as WiFi networks, WiMax networks, or Bluetooth networks, or through infrared\ntechnology.\nWith the rapid spread of wireless data networks and packet-based telecommunication networks, telecommunication devices typically have multiple network connectivities to select from.  For example, a telecommunication device could download an\naudio file via a packet-based telecommunication network offered by the network operator tethered to that telecommunication device or via a WiFi network.  Typically, selection of a network connectivity is made based on a universal preference (e.g., always\nuse WiFi when available).  Such selection techniques often provide suboptimal results, however, and do not take full advantage of the continuously improving packet-based communication infrastructure available to telecommunication devices.  Further, each\nnetwork operator has advantages and disadvantages; some may offer great coverage, but at a high price, while others may offer suboptimal coverage at a lower price.  Also, a given network operator may have better coverage in some locations than in others. Telecommunication devices may not be able to take advantage of these varying benefits, however, as they are often tethered to a single network operator. <BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nThe detailed description is set forth with reference to the accompanying figures.  In the figures, the left-most digit(s) of a reference number identifies the figure in which the reference number first appears.  The use of the same reference\nnumbers in different figures indicates similar or identical items or features.\nFIG. 1 illustrates an example environment in which a service broker generates service preferences based on input from network operators and provides the service preferences to a telecommunication device to enable the telecommunication device to\nintelligently select network connectivities to use for transmission of network packets.\nFIG. 2 illustrates an example environment in which a network operator generates service preferences and provides the service preferences to a telecommunication device to enable the telecommunication device to intelligently select network\nconnectivities to use for transmission of network packets.\nFIG. 3 illustrates an example telecommunication device packet routing module for intelligently selecting network connectivities to use for transmission of network packets.\nFIG. 4 illustrates a plurality of packet-based networks supporting various packet routing scenarios.\nFIG. 5 illustrates an example computing device configured with a service preference module for generating service preferences and providing the service preferences to a telecommunication device.\nFIG. 6 illustrates an example telecommunication device configured with a packet routing module and service preferences for selecting network connectivities to use for transmission of network packets.\nFIG. 7 illustrates an example process of a network operator for generating service preferences for a telecommunication device and providing the service preferences to the telecommunication device to enable the telecommunication device to\nintelligently select network connectivities to use for transmission of network packets.\nFIG. 8 illustrates an example process of a service broker for generating service preferences for a telecommunication device based on input from network operators and providing the service preferences to the telecommunication device to enable the\ntelecommunication device to intelligently select network connectivities to use for transmission of network packets.\nFIG. 9 illustrates an example process of a management service for generating service preferences for a telecommunication device and providing the service preferences to the telecommunication device to manage intelligent selection by the\ntelecommunication device of network connectivities to use for transmission of network packets.\n<BR><BR>DETAILED DESCRIPTION\nThis disclosure describes, in part, techniques for generating service preferences for a telecommunication device.  Those service preferences are then provided to the telecommunication device to enable the telecommunication device to select a\nnetwork connectivity from a plurality of network connectivities based at least in part on the service preferences.  The network connectivities are respectively associated with a plurality of network operators.  The service preferences may be generated\nand provided by any of a network operator, a service broker, or a management service.\nTo intelligently select among a plurality of available network connectivities, a telecommunication device may be equipped with a packet routing module to select a connectivity and route network packets via that connectivity.  The\ntelecommunication device may also be equipped with an open subscriber identity module (SIM) which includes authentication information for a plurality of network operators and thus enables selection of available network connectivities associated with\nthose network operators.  The user of the telecommunication device may receive the open SIM from a service broker in exchange for subscribing to services of that service broker.  Alternatively, the user may receive the open SIM from a network operator,\nfrom a management service, or the telecommunication device may be built to include an open SIM.  The packet routing module may be part of a platform of the telecommunication device or may be received from the service broker, network operator, or\nmanagement service.\nIn various embodiments, the telecommunication device intelligently selects among the available network connectivities based on service preferences.  Service preferences may prioritize at least one of connection performance, cost, power\nconsumption, bandwidth, packet delivery efficiency, security level, coverage, and user experience.  Service preferences may also prioritize between different network operators associated with the available network connectivities.  In some embodiments,\nthe service preferences may be an aggregation of service preferences of the different network operators that is generated by a service broker or may be service preferences received from a single network operator.  Further, the service preferences may\ninclude different fees respectively associated with different levels of usage of a specific network connectivity associated with a specific network operator.  The service preferences may also be associated with a location of the telecommunication device\nand may be updated as that location changes.  The telecommunication device may utilize the service preferences alone or in conjunction with network connectivity metrics or packet attributes in selecting a network connectivity.\nIn some embodiments, the service preferences may be received from a management service and may include a policy which is enforced by the telecommunication device.  The policy may dictate which network connectivity is selected and may prevent the\nuser of the telecommunication device from providing input which changes the network connectivity selected by the telecommunication device.  For example, the telecommunication device may be provided to the user by the user's employer, and the employer may\nsubscribe to the management service to manage network usage on its telecommunication devices.\nIn further embodiments, as mentioned above, the service preferences may be generated by a service broker.  The service broker may receive service policies, indications of network conditions, usage histories, or service fees from a plurality of\nnetwork operators and may generate service preferences based on those service policies, indications of network conditions, usage histories, or service fees.  In some embodiments, the service policies, indications of network conditions, usage histories,\nor service fees may be provided as service preferences of the network operators and the service broker may aggregate those service preferences.  The user of the telecommunication device may subscribe to services of the service broker, and in return the\nservice broker may provide service preferences to the telecommunication device.  The service broker may also provide either or both of the open SIM or the packet routing module, as mentioned above.  The service preferences may be provided automatically\nor responsive to a user request.\nIn various embodiments, the service preferences may be generated by a network operator and provided by that network operator to the telecommunication device.  The user of the telecommunication device may have a non-exclusive service contract or\na service contract without a set-term with the network operator.  The network operator may prioritize network connectivities associated with its network, may offer different fees depending on the usage by the telecommunication device of that network,\netc. The service preferences may be provided automatically or responsive to a user request.\nThe telecommunication device may further enable the user of the telecommunication device to update service preferences through a preferences user interface.  Alternatively or additionally, the user may update the service preferences through a\nwebsite.  In some embodiments, the changes may trigger an update to service preferences by a service broker or network operator, which may then provide updated service preferences to the telecommunication device.  In further embodiments, the service\nbroker or network operator may monitor network usage at the location of the telecommunication device and provide updated service preferences based on that network usage.\n<BR><BR>Example Environments\nFIG. 1 illustrates an example environment in which a service broker generates service preferences based on input from network operators and provides the service preferences to a telecommunication device to enable the telecommunication device to\nintelligently select network connectivities to use for transmission of network packets.  As illustrated, a service broker 102 may generate service preferences 104 based on input from a plurality of network operators 106 and may provide the service\npreferences 104 to a telecommunication device 108.  The telecommunication device 108 may be associated with a user 110 who has subscribed 112 to services of the service broker 102.  The service broker 102 may provide 114 an open SIM 116 to the user 110,\nunless the telecommunication device 108 is already provisioned with an open SIM 116.  The telecommunication device 108 may also be provisioned with a packet routing module 118 and transceivers 120.  The telecommunication device 108 may utilize the\nservice preferences 104, open SIM 116, packet routing module 118, and transceivers 120 to select among connectivities for a first network 122, a second network 124, and a third network 126.  In some embodiments, the user 110 or an entity associated with\nthe user 110 may subscribe to services of a management entity 128, which may provide service preferences 130 in addition to or in place of the service preferences 104.\nIn various embodiments, each of the service broker 102, the network operators 106, and the management entity 128 may be associated with one or more computing devices.  Such computing devices may each be any of a server or server farm, multiple,\ndistributed server farms, a mainframe, a work station, a personal computer (PC), a laptop computer, a tablet computer, an embedded system, or any other sort of device or devices.  In some embodiments, the computing devices may form a cloud computing\ndevice.  In further embodiments, the computing devices may include one or more virtual machines.  An example computing device is illustrated in FIG. 5 and described below with reference to that figure.\nIn various embodiments, the telecommunications device 108 may be any sort of device capable of engaging in packet-based connections and of having multiple wireless network connectivities.  For example the telecommunications device 108 may be any\nof a smart phone, a tablet computer, a personal digital assistant (PDA), a personal computer (PC), a laptop computer, a media center, a work station, etc. An example telecommunications device 108 is illustrated in FIG. 6 and described below with\nreference to that figure.\nIn some embodiments, the service broker 102 may be an entity which provides service preferences 104 to telecommunication devices 108 of users 110 who are subscribed 112 to services of the service broker 102.  If the telecommunication device 108\nis not provisioned with either or both of an open SIM 116 or a packet routing module 118, the service broker 102 may provide the open SIM 116 to the user 110 to install in the telecommunication device 108 or may provide the packet routing module 118 to\nthe telecommunication device 108.  Also, if the user 110 does not have a telecommunication device 108 when subscribing 112, the service broker 102 may provide the user 110 with the telecommunication device 108 (e.g., either through lease or sale).  The\nservice broker 102 may also receive input from a plurality of network operators 106, such as service policies, indications of network conditions, usage histories, or service fees from the network operators 106.  The service broker 102 then generates the\nservice preferences 104 based at least in part on that network operator input.  The service broker 102 may have contractual arrangements with some or all of the network operators 106.  Also or instead, the service broker 102 may be an affiliated entity\n(e.g., a subsidiary) of a network operator 106.  The service broker 102 may deal with network operators 106 on charging, policy, and billing and remotely update charging policy for telecommunication device 108.\nIn various embodiments, the network operators 106 may each be any sort of network operators, such as telecommunication service providers that operate a telecommunication infrastructure, including access networks and a core network, to provide\ntelecommunication services such as voice calling, video calling, messaging, email, and data (e.g., streaming video and audio or web browsing).  The network operators 106 may offer these services as part of service plans subscribed to by telecommunication\ndevice users or may allow the services to be purchased incrementally (e.g., per packet, per communication session, per connection, etc.).\nFor example, each of the first network 122, the second network 124, and the third network 126 may be associated with a different network operator 106.  Alternatively, two of the first network 122, the second network 124, and the third network\n126 may be associated with a single network operator 106, and the other with another network operator 106.  Each network operator 106 may charge different fees for usage of its network 122, 124, or 126, which may vary based on the user 110, the\ntelecommunication device 108, the location of service, the utilization of services at that location, or the type of service (e.g., voice, video, data, etc.).  Each network operator 106 may provide information to a service broker 102, such as service\npolicies, indications of network conditions, usage histories, or service fees.  In some embodiments, each network operator 106 generates service preferences to be suggested to a telecommunication device 108, and may provide the service preferences to the\nservice broker 102.  Such service preferences, may, in embodiments, convey the service policies, indications of network conditions, usage histories, or service fees to the service broker 102.\nUpon receiving service preferences or service policies, indications of network conditions, usage histories, or service fees from a plurality of network operators 106, the service broker 102 generates the service preferences 104.  Additionally or\ninstead, the service broker 102 may generate the service preferences 104 based input received from the user 110 or based on observation of behavior or preferences of the user 110.  For instance, as part of the subscription 112 or later, the user 110 may\nindicate that she prefers to use the \"lowest cost\" network, and the service broker 102 may generate the service preferences 104 based at least in part on those user preferences.  In some embodiments, the service broker 102 may generate the service\npreferences 104 for the telecommunication device 108 when it first registers.  In other embodiments, the service broker 102 may generate and provide the service preferences 104 periodically.  In embodiments in which the service broker 102 received\nservice preferences from network operators 106, generating the service preferences 104 may comprise aggregating the service preferences of the network operators.\nIn various embodiments, the service preferences 104 may prioritize at least one of connection performance, cost, power consumption, bandwidth, packet delivery efficiency, security level, coverage, and user experience.  Service preferences 104\nmay also prioritize between different network operators 106 associated with the available network connectivities.  Further, the service preferences 104 may include different fees respectively associated with different levels of usage of a specific\nnetwork connectivity associated with a specific network operator 106.  The service preferences 104 may also be associated with a location of the telecommunication device 108 and may be updated as that location changes.  The telecommunication device 108\nmay utilize the service preferences 104 alone or in conjunction with network connectivity metrics or packet attributes in selecting a network connectivity.\nIn some embodiments, the service preferences 104 may be any of \"best network performance\" (which may be a function of factors such as packet delay, packet loss, and jitter), a specific QoS, \"lowest cost\", \"lowest power consumption with the\nminimum bandwidth\", \"largest available bandwidth\" given specific performance requirements, \"best packet delivering efficiency\" (e.g., by grouping/sorting network packets), \"best overall metrics\", \"most secure connection\", or some combination of these\ncriteria.\nAs mentioned, a user 110 may subscribe 112 to services of the service broker 102 and may receive, in return, a telecommunication device 108 or, if the user 110 has a telecommunication device 108, the service broker 102 may provide 114 the user\n110 with an open SIM 116.  In yet other embodiments, the user 110 may have a telecommunication device 108 provisioned with an open SIM 116, and the service broker 102 may simply active service in response to the subscription 112.  In further embodiments,\nsubscribing 112 to the service may include indicating preferences or limitations, such as \"best performance\" or \"do not spend more than $100 per month\" which may be used by the service broker 102 in generating service preferences 104.\nIn various embodiments, an open SIM 116 is a SIM that is not tethered to any specific network operator 106.  Typically, a SIM is tethered to a specific network operator, providing information (e.g., authentication information) to its network\noperator that utilizes in enabling services and connectivity for the telecommunication device that includes the SIM.  The open SIM 116 provides authentication information to each or any of a plurality of network operators 106 or to a specific subset of\nnetwork operators 106, enabling the telecommunication device 108 to switch between different network connectivities of different network operators 106.  An open SIM 116 may be implemented with multiple SIMs combined, each from one network operator.  It\nmay also be implemented as virtual SIM or soft SIM.\nThe packet routing module 118 is described below in detail with respect to FIG. 3.  The packet routing module 118 may form part of the platform of the telecommunication device 108 or may be an application downloaded from the service broker 102\nor another source.\nIn some embodiments, the telecommunication device 108 includes one or more wireless transceivers 120, such as a wireless transceiver for each network connectivity or a wireless transceiver capable of communicating with multiple networks.  The\ntelecommunication device 108 may also have a single wireless transceiver 120 with multiple soft radios, such as a soft radio for each network connectivity.  To increase throughput, the wireless transceivers 120 may utilize multiple-input/multiple-output\n(MIMO) technology.  The wireless transceivers 120 may be any sort of wireless transceivers capable of engaging in wireless, radio frequency (RF) communication.  The wireless transceivers 120 may also include other wireless modems, such as a modem for\nengaging in WiFi, WiMax, Bluetooth, or infrared communication.\nOnce the telecommunication device 108 is associated with a subscription 112, has an open SIM 116, packet routing module 118, wireless transceivers 120, and has received service preferences 104, the telecommunication device 108 may select among\navailable network connectivities associated with one or more of the first network 122, the second network 124, and the third network 126.  The packet routing module 118 of the telecommunication device 108 utilizes at least the service preferences 104\nand, optionally, connectivity metrics or packet attributes in selecting a network connectivity for a packet or flow of packets.  Such use of the service preferences 104 in selecting network connectivities is described in greater detail further herein.\nIn various embodiments, the networks 122-126 may represent one packet-based access network, such as a WiFi network, an LTE network, an HSPA+ network, a Bluetooth network, or a network associated with infrared technology.  If one of the networks\n122-126 is a packet-based cellular network, such as a LTE network or an HSPA+ network, that network 122, 124, or 126 may include a base station transceiver, base station controller, a node B, or an eNode B. Such a network 122, 124, or 126 may provide\nconnectivity to a core network of a network operator 106.  In some embodiments, each of the networks 122-126 that is a packet-based cellular network may be associated with a network operator 106.  Multiple ones of the networks 122-126 may be associated\nwith a same network operator 106, or each of the networks 122-126 may be associated with a different network operator 106.  Networks 122-126 that are non-cellular data networks, such as WiFi networks, may include an access point device for sending and\nreceiving wireless transmissions.  These access devices may in turn be associated with an Internet service provider that provides connectivity to the Internet.  For example, the first network 122 may be a WiFi network, the second network 124 may be an\nLTE network, and the third network 126 may be an HSPA+ network.\nIn some embodiments, a user 110 may wish to update the received service preferences 104.  For example, the service preferences 104 may prioritize \"best QoS\" over \"lowest cost,\" and the user may wish to alter the order of those preferences.  The\nuser 110 may enter or update service preferences 104 through some sort of user interface, such as a graphic user interface (GUI), physical input control, or voice input control.  The telecommunication device 108 may then translate the user-provided or\nuser-updated preferences to service preferences 104 based on some rules or models.  In some embodiments, rather than entering the user-provided or user-updated preferences through an interface or control of the telecommunication device 108, the user 110\nof the telecommunication device 108 may enter the preferences through a web site.  Also, in further embodiments, the telecommunication device 108 may provide a GUI that shows costs and performance associated with different service preferences 104 (e.g.\ndecrease in cost per increase in delay measured in milliseconds, or increase in throughput per increase in power consumption).  Such a GUI may enable a user 110 to express preferences that result in more satisfactory service preferences.  Also, if the\nuser-provided or user-updated preferences will result in changes service fees or other service parameters from a network operator 106, the GUI may warn the user 110 of this and ask the user 110 to confirm that he or she wishes to proceed.\nThe service preferences 104 may be dynamically updated at any time.  For example, a user 110 may be unhappy with the quality of the video played on the telecommunication devices 108 and may actuate a soft key or physical button, or may provide a\nspoken command (e.g., \"increase quality\").  This new user-provided preference may be translated into an updated service preference 104 (e.g., better performance or improved QoS) that is used going forward in selecting network connectivities for network\npackets.\nIn some embodiments, the telecommunication device 108 may monitor the usage by the user 110 of voice, video, and data services provided via networks 122-126 and update the service preferences 104 based on the monitoring.  Such monitoring and\nupdating may involve machine learning techniques which may progressively improve the service preferences 104 to match preferences of the user 110.\nIn various embodiments, the service broker 102 may update the service preferences 104, either periodically or in response to an event, such as updated input from a network operator 106 or service preference updates made by the user 110.  As\nmentioned above, a user 110 may specify a new service preference 104 or modify one or more of the service preferences 104 provided by the service broker 102.  The service broker 102 may be notified of the new or modified service preferences 104 by the\ntelecommunication device 108 and may, in response, update the service preferences 104.  For example, the service preferences 104 provided to the telecommunication device 108 may have prioritized one network operator 106 over another, and the user 110 may\nhave modified the service preferences 104 to reverse that priority.  In response to the user modification, the service broker 102 may generate updated service preferences 104 that reflect different service fees for the network operators 106.  The service\nbroker 102 may then provide the updated service preferences 104 to the telecommunication device 102.\nThe service broker may also receive new input from one or more of the network operators 106 from time to time.  For example, a network operator 106 may update its service fees based on the network usage at a specific location and may provide\nindicia of that update to the service broker 102.  The service broker 102 may then generate updated service preferences 104 and provide those updated service preferences 104 to the telecommunication device 102.\nIn some embodiments, the service broker 102 or a network operator 106 may monitor the usage by the user 110 of voice, video, and data services provided via one or more of the networks 122-126 and update the service preferences 104 based on the\nmonitoring.  Such monitoring and updating may involve machine learning techniques which may progressively improve the service preferences 104 to match preferences of the user 110.\nIn various embodiments, the management entity 128 may provide remote device management services, controlling which network connectivity that the telecommunication device 108 selects.  In such embodiments, the user 110 may be an employee of an\nentity that subscribes to the management services offered by the management entity 128.  The management entity 128 may control the network connectivity selection so that the selection is in line with preferences of the employer/service subscriber rather\nthan the user 110.  To control the network connectivity selection, the management entity 128 generates and provides service preferences 130 which include a policy that determines which network connectivity is selected.  The management entity 128 may\ngenerate service preferences 130 in a manner similar to that described above for the service broker 102, taking into account input from the network operators 106.  The user 110 may also not be able to modify the service preferences 130, and if the\ntelecommunication device 104 receives both service preferences 104 and service preferences 130, the service preferences 130 may override the service preferences 104.  In other embodiments, the telecommunication device 108 may only receive service\npreferences 130, not service preferences 104, and the user 110 may not have a subscription with the service broker 102.\nFIG. 2 illustrates an example environment in which a network operator generates service preferences and provides the service preferences to a telecommunication device to enable the telecommunication device to intelligently select network\nconnectivities to use for transmission of network packets.  As illustrated, a network operator 202 may generate service preferences 204 and provide those service preferences 204 directly to the telecommunication device 108, without any service broker 102\nbeing involved.\nIn various embodiments, the network operator 202 may be one of the network operators 106 described above, but may generate, update, and provide its own service preferences 204 to the telecommunication device 108.  The user 110 may have a service\narrangement with the network operator 202.  For example, the user 110 may have purchased the telecommunication device 108 from the network operator 202, subscribed to a non-exclusive or non-set-term service contract with the network operator 202, etc.\nThe network operator 202 may also provide the open SIM 116 to the user 110, if the telecommunication device 108 is not provisioned with an open SIM 116, and may provide the packet routing module 118.\nThe service preferences 204 may be the same as the service preferences 104, except that service preferences 204 are generated by a network operator 202 rather than a service broker 102.  Also, the service preferences 204 may reward the user 110\nfor utilizing the network 122, 124, or 126 of the network operator 202, providing differing service fees based on different levels of usage of the network 122, 124, or 126 of the network operator 202.\n<BR><BR>Example Routing Module\nFIG. 3 illustrates an example telecommunication device packet routing module for intelligently selecting network connectivities to use for transmission of network packets.  As illustrated, a telecommunications device 108 may be configured with a\npacket routing module 118 to route network packets 302a-302c from telecommunication device applications 304a-304c.  To transmit at least one of the network packets 302a-302c, the packet routing module 118 selects a network connectivity 306 of a plurality\nof network connectivities 306x-306z respectively associated with a plurality of networks 122-126.  A network selection module 308 of the packet routing module 118 selects the network connectivity 306 based on an evaluation of service preferences 310,\nconnectivity metrics 312 associated with the network connectivities 306x-306z, and network packet attributes 314 associated with the network packet(s) 302 that is/are to be transmitted.  After selecting the network connectivity 306, a transmission module\n316 of the packet routing module 118 then utilizes the selected network connectivity 306 to transmit the network packet(s) 302.\nIn some embodiments, the packet routing module 118 receives the network packets 302a-302c from the applications 304a-304c and stores the network packets 302a-302c in a queue or stack associated with the packet routing module 118.  The network\npackets 302a-302c may be any sort of Internet Protocol (IP) packets or other sort of network packets.  The applications 304a-304c may be any sort of telecommunication device applications which send and receive network packets.  For example, the\napplication 304a may be a web browser and network packet 302a may be a request for web content.  Application 304b may be a video call client and network packet 302b may be a packet associated with a video call.  Application 304c may be client for a\nmulti-player online game and network packet 302c may be a packet associated with a game-play session.  These examples are provided only for the sake of illustration; applications 304a-304c may be or include many other different types of applications.\nUpon storing the network packets 302a-302c in the queue or stack (or as they are stored), the packet routing module 118 may collect network packet attributes 314 of the network packets 302a-302c and perform quality of service (QoS) marking of\nthe network packets 302a-302c.  The collected network packet attributes 314 may include any of the network protocols of the network packets 302a-302c, such as the transmission control protocol (TCP), the user datagram protocol (UDP), or the real-time\ntransport protocol (RTP), QoS profiles, packet size, destination address, bandwidth demand, class of services, or security level.\nIn various embodiments, before, during, and after receiving the network packets 302a-302c (e.g., on a substantially continuous basis), the packet routing module 118 determines the network connectivities 306x-306z available to the\ntelecommunication device 108 as well as connectivity metrics 312 for those network connectivities 306x-306z.  The network connectivities 306x-306z may be connectivities to a variety of different packet-based networks 122-126, such as WiFi networks, LTE\nnetwork, or HSPA+ networks.  The available network connectivities 306x-306z may change based on movement of the telecommunication device 108 to a different location or may even change at a given location based on factors such as signal strength or\ncongestion of a network 122-126.  The connectivity metrics 312 collected for each network connectivity 306x-306z may include: a network protocol used for the associated network 122-126, such as TCP, UDP, or RTP, the availability of the associated network\n122-126 at the current location and time, performance of a transmission across the associated network 122-126 (delay, jitter, packet-loss rate), power consumption associated with use of the associated network 122-126, a bandwidth profile for the\nassociated network 122-126, connection cost for use of the associated network 122-126, or security level.\nThe networks 122-126 are described in detail with respect to FIGS. 1 and 2.  As shown in FIG. 3, the network connectivity 306x may be a network connectivity associated with the first network 122, the network connectivity 306y may be a network\nconnectivity associated with the second network 124, and the network connectivity 306z may be a network connectivity associated with the third network 126.\nIn various embodiments, the packet routing module 118 may include both a network selection module 308 and transmission module 316 to select a network connectivity 306 and to transmit network packet(s) 302 using the network connectivity 306.  The\nnetwork selection module 308 selects a network connectivity 306 for each received network packet 302 and may do so substantially when that network packet 302 is received or at a later time.  Each selection is based at least on service preferences 310,\nand possibly also on connectivity metrics 312, and attributes 314 of the network packet 302 for which the network connectivity 306 is being selected.  The connectivity metrics 312 may vary from moment to moment and from location to location, and the\nattributes 314 may be different for each network packet 302.  Even when there is no substantial change in connectivity metrics 312 or attributes 314 between the receiving of two network packets 302, the service preferences 310 may result in the network\nselection module 308 selecting different network connectivities 306.\nIn some embodiments, the service preferences 310 may include any one or more of service preferences 104, service preferences 130, service preferences 204, or user-specified service preferences or service preference updates.  Such service\npreferences 310 are described in greater detail above.\nIn various embodiments, the network selection module 308 determines a score for each network connectivity 306.  The score may reflect the degree to which the connectivity metrics 312 for the network connectivity 306 meet the service preferences\n310 given the attributes 314 of the network packet 302 to be transmitted.  The network selection module 308 may then select the network connectivity 306 with the highest score to use in transmitting the network packet 302.  In some embodiments, the\nnetwork selection module 308 may select a specific network connectivity 306 as a default routing path based on the service preferences 310.\nOnce the network selection module 308 has selected the network connectivity 306 to use for transmitting the network packet 302, the network selection module 308 invokes the transmission module 316.  The transmission module 316 may establish a\nconnection with the network 122, 124, or 126 associated with the selected network connectivity 306, unless one is already established, and transmit the network packet 302 to the network 122, 124, or 126 over the connection.\n<BR><BR>Example Routing Scenarios\nFIG. 4 illustrates a plurality of packet-based networks supporting various packet routing scenarios.  As illustrated, the telecommunication device 108 has network connectivities associated with a WiFi network 402, an LTE network 404, an LTE\nnetwork 406, and an HSPA+ network 408.  The LTE network 404 is operated by a first network operator 410, and the LTE network 406 and HSPA+ network 408 are operated by a second network operator 412.  The telecommunication device 108 may transmit one or\nmore network packets to the packet destination 414 over one or more of the networks 402-408.  Also, as illustrated, the telecommunication device 108 may select one network connectivity, such as the network connectivity associated with the HSPA+ network\n408, for downlink communications 416 and another network connectivity, such as the network connectivity associated with the WiFi network 402, for uplink communications 418.\nIn various embodiments, the networks 402-408 may be examples of the networks 122-126 illustrated in FIGS. 1-3 and described herein with reference to those figures.  The first network operator 410 and second network operator 412 may be examples\nof network operators 106 or the network operator 202, which are described above in greater detail.\nThe packet destination 414 may be any sort of device or devices identifiable by a network address.  Such a device or devices may include another telecommunication device, a server, a network or cloud, or any sort of computing device.  The\nnetwork address of the packet destination 414 is specified by the network packet that is to be transmitted to the packet destination 414.\nIn a first routing scenario, the telecommunication device 108 has multiple network connectivities respectively associated with multiple, different network operators, including the first network operator 410 and the second network operator 412. \nIn such a scenario, the user of the telecommunication device 108 may not be tethered to a service plan of any specific network operator, or may be subject to a non-exclusive service plan.  The telecommunication device 108 may utilize service preferences,\nsuch as service preferences 104, 130, or 310, and, optionally, evaluate the connectivity metrics associated with the network connectivities in selecting a network connectivity.  For example, a first network connectivity associated with an LTE network 404\noperated by the first network operator 410 may have a lower cost than a second network connectivity associated with an LTE network 406 operated by the second network operator 412.  If the service preferences specify \"use lowest cost,\" the\ntelecommunication device 108 may select the first network connectivity associated with the LTE network 404, establish a connection to the LTE network 404 (or use an established connection) and transmit a network packet to the packet destination 414 over\nthe LTE network 404.  After transmitting the network packet, there may be a change to the service preferences, connectivity metrics, or packet attributes.  For example, the second network operator 412 may lower its cost, and that lower cost may be\nreflected in updated service preferences or connectivity metrics.  If the service preferences still specify \"use lowest cost,\" the telecommunication device 108 may then select the second network connectivity associated with the LTE network 406 for\ntransmission of further network packets.\nIn a second scenario, network packets associated with a single logical connection (e.g., a video call) may be transmitted first over one network (e.g., LTE network 406) and then another (e.g., HSPA+ network 408) based on updated service\npreferences or varying metrics/attributes, without interruption to the logical connection.  For example, the attributes of the network packets and connectivity metrics may stay largely the same, but the telecommunication device 108 may receive updated\nservice preferences, or the user may modify the service preferences.  The user may modify the service preferences, for instance, because of suboptimal quality of experience.  In this example, the update may result in service preferences that require a\nhigher quality of service (QoS).  This higher QoS may, in turn, result in the telecommunication device 108 selecting a second network (e.g., HSPA+ network 408) in place of a previously used, first network (e.g., LTE network 406).\nIn a third routing scenario, different networks 402-408 may be associated with different network topologies.  These different network topologies may result in significantly different network latencies.  For example, a telecommunication device\n108 may have a first network connectivity associated with a WiFi network 402 and a second network connectivity associated with an LTE network 404.  The packet destination 414 may be geographically proximate to the telecommunication device 108, and the\nnetwork topology of the LTE network 404 may require all network traffic to be sent to a node of the first network operator 410 a significant distance from the telecommunication device 108.  When using the LTE network 404, then, this far distance must be\ntraversed twice to send the network packet to its destination 414.  When using the WiFi network 402, only a short distance need be traversed.  To detect such differences in network topologies, the telecommunication device 108 may send messages to the\ndestination 414 using each of the networks 402 and 404, receive responses, and calculate round trip times.  This may be performed by the telecommunication device 108, for example, using a ping utility.  These round trip times may then be evaluated in\nlight of the service preferences to select a network connectivity.  If the service preferences specify \"select lowest network latency,\" the telecommunication device 108 may select the WiFi network 402.\nIn some embodiments, the calculation of network latencies need not be repeated for each packet; rather, the telecommunication device 108 need only calculate the network latencies once for a logical connection, a session, a flow, or for packets\nassociated with a same destination 414 that are transmitted within a time window of each other.\nIn a fourth scenario, the telecommunication device 108 may select different network connectivities for uplink communications 418 and downlink communications 416.  For example the telecommunication device 108 may select a network connectivity\nassociated with the HSPA+ network 408 for downlink communications 416 and a network connectivity associated with the WiFi network 402 for uplink communications 418.  The telecommunication device 108 may then transmit network packets using the WiFi\nnetwork 402 and receive network packets using the HSPA+ network 408.\nIn a fifth routing scenario, telecommunication device 108 may sort or combine for transmission multiple network packets to better achieve the service preferences.  This may involve grouping network packets by size, destination, or QoS profile\nand transmitting the grouped networked packets using a same one of the networks 402-408 or concurrently over multiple ones of the networks 402-408.  Also or instead, the sorting may involve prioritizing some network packets and delaying others.\nIn a sixth routing scenario, the service preferences may specify \"maximum throughput\" and the telecommunication device 108 may, as a result, select all available network connectivities for transmission of packets to the packet destination 414. \nIn such a scenario, the telecommunication device 108 may connect to each of the networks 402-408 and may simultaneously transmit packets over all of the networks 402-408.\nThe above scenarios are described for the sake of illustration and do not limit the routing scenarios possible given the packet routing techniques described herein.\n<BR><BR>Example Devices\nFIG. 5 illustrates an example computing device configured with a service preference module for generating service preferences and providing the service preferences to a telecommunication device.  As illustrated, a computing device 502 comprises\na system memory 504 storing service preferences module 506, policies 508, network conditions 510, usage histories 512, and subscriber information 514.  Also, the computing device 502 includes processor(s) 516, a removable storage 518 and non-removable\nstorage 520, input device(s) 522, output device(s) 524, and network interface(s) 526.  The computing device 502 may be associated with any of the service broker 102, the management entity 128, or the network operator 202.\nIn various embodiments, system memory 504 is volatile (such as RAM), non-volatile (such as ROM, flash memory, etc.) or some combination of the two.  The service preferences module 506, policies 508, network conditions 510, usage histories 512,\nand subscriber information 514 stored in the system memory 504 may comprise methods, threads, processes, applications or any other sort of executable instructions.  The service preferences module 506, policies 508, network conditions 510, usage histories\n512, and subscriber information 514 may also include files and databases.\nThe service preferences module 506 comprises any logic capable of generating, updating, and providing service preferences in the manner described in greater detail further herein.  Also, further description of the policies 508, network\nconditions 510, usage histories 512, and subscriber information 514 is provided above.\nIn some embodiments, the processor(s) 516 is a central processing unit (CPU), a graphics processing unit (GPU), or both CPU and GPU, or other processing unit or component known in the art.\nComputing device 502 also includes additional data storage devices (removable and/or non-removable) such as, for example, magnetic disks, optical disks, or tape.  Such additional storage is illustrated in FIG. 5 by removable storage 518 and\nnon-removable storage 520.  Tangible computer-readable media may include volatile and nonvolatile, removable and non-removable media implemented in any method or technology for storage of information, such as computer readable instructions, data\nstructures, program modules, or other data.  System memory 504, removable storage 518 and non-removable storage 520 are all examples of computer-readable storage media.  Computer-readable storage media include, but are not limited to, RAM, ROM, EEPROM,\nflash memory or other memory technology, CD-ROM, digital versatile disks (DVD) or other optical storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium which can be used to store the\ndesired information and which can be accessed by the computing device 502.  Any such tangible computer-readable media may be part of the computing device 502.\nComputing device 502 also has input device(s) 522, such as a keypad, a cursor control, a touch-sensitive display, voice input device, etc., and output device(s) 524 such as a display, speakers, etc. These devices are well known in the art and\nneed not be discussed at length here.\nComputing device 502 further includes network interface(s) 526 for wired and/or wireless communication with other computing devices over one or more networks, such as any of networks 122-124, public networks, private networks, or the Internet. \nSuch network interfaces are well known in the art and need not be discussed at length here.\nFIG. 6 illustrates an example telecommunication device configured with a packet routing module and service preferences for selecting network connectivities to use for transmission of network packets.  As illustrated, telecommunication device 108\ncomprises a system memory 602 storing a packet routing module 118, service preferences 604, a preference interface 606, packet queues 608, and applications 610.  Also, the telecommunication device 108 includes processor(s) 612, a removable storage 614\nand non-removable storage 616, input device(s) 618, output device(s) 620, transceivers 120, and an open SIM 116.\nIn various embodiments, system memory 602 is volatile (such as RAM), non-volatile (such as ROM, flash memory, etc.) or some combination of the two.  The packet routing module 118, service preferences 604, preference interface 606, packet queues\n608, and applications 610 stored in the system memory 602 may comprise methods, threads, processes, applications or any other sort of executable instructions.  The packet routing module 118, service preferences 604, preference interface 606, packet\nqueues 608, and applications 610 may also include files and databases.  Further description of the packet routing module 118, examples of service preferences 604 (e.g., service preferences 104, service preferences 130, service preferences 204, and\nservice preferences 310), and applications 610 is provided above.\nIn further embodiments, the preference interface 606 may be any sort of GUI, physical control, or voice control through which a user may enter preferences used to modify or request an update to the service preferences 604.  Such preference\ninterfaces 606 are described in further detail herein when describing examples of service preferences 604 (e.g., service preferences 104, service preferences 130, service preferences 204, and service preferences 310).\nIn various embodiments, the packet queues 608 may include one or more queues or stacks associated with the packet routing module 118.  Such packet queues 608 may be used to store the network packets 302 pending selection of a network\nconnectivity 306 for each network packet 302.  Packet queues 608 may also include queues or stacks associated with wireless transceivers 120 or with network protocols used in transmitting the network packets 302 any one or more of network(s) 122-126.\nIn some embodiments, the processor(s) 612 is a central processing unit (CPU), a graphics processing unit (GPU), or both CPU and GPU, or other processing unit or component known in the art.\nTelecommunication device 108 also includes additional data storage devices (removable and/or non-removable) such as, for example, magnetic disks, optical disks, or tape.  Such additional storage is illustrated in FIG. 6 by removable storage 614\nand non-removable storage 616.  Tangible computer-readable media may include volatile and nonvolatile, removable and non-removable media implemented in any method or technology for storage of information, such as computer readable instructions, data\nstructures, program modules, or other data.  System memory 602, removable storage 614 and non-removable storage 616 are all examples of computer-readable storage media.  Computer-readable storage media include, but are not limited to, RAM, ROM, EEPROM,\nflash memory or other memory technology, CD-ROM, digital versatile disks (DVD) or other optical storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium which can be used to store the\ndesired information and which can be accessed by the telecommunication device 108.  Any such tangible computer-readable media may be part of the telecommunication device 108.\nTelecommunication device 108 also has input device(s) 618, such as a keypad, a cursor control, a touch-sensitive display, voice input device, etc., and output device(s) 620 such as a display, speakers, etc. These devices are well known in the\nart and need not be discussed at length here.\nWireless transceivers 120 and open SIM 116 are described above in detail with respect to FIG. 1.\n<BR><BR>Example Processes\nFIGS. 7-9 illustrate example processes.  These processes are illustrated as logical flow graphs, each operation of which represents a sequence of operations that can be implemented in hardware, software, or a combination thereof.  In the context\nof software, the operations represent computer-executable instructions stored on one or more computer-readable storage media that, when executed by one or more processors, perform the recited operations.  Generally, computer-executable instructions\ninclude routines, programs, objects, components, data structures, and the like that perform particular functions or implement particular abstract data types.  The order in which the operations are described is not intended to be construed as a\nlimitation, and any number of the described operations can be combined in any order and/or in parallel to implement the processes.\nFIG. 7 illustrates an example process of a network operator for generating service preferences for a telecommunication device and providing the service preferences to the telecommunication device to enable the telecommunication device to\nintelligently select network connectivities to use for transmission of network packets.  The process includes, at 702, receiving, by a network operator, a location of a telecommunication device.  The telecommunication device may be associated with a user\nwho does not have a set-term service contract with the network operator.\nAt 704, the network operator generates service preferences for the telecommunication device.  At 706, the generating is based at least in part on one or more network conditions, a network operator policy, or a network usage history.  At 708, the\ngenerating is based at least in part on the received location of the telecommunication device.  The service preferences may include different fees respectively associated with different levels of usage of a network connectivity associated with the\nnetwork operator.  Also or instead, the service preferences may include preferences between the network operator and one of the other network operators or preferences between at least two of the other network operators.  Further, the service preferences\nmay prioritize at least one of connection performance, cost, power consumption, bandwidth, packet delivery efficiency, security level, coverage, and user experience.\nAt 710, the network operator provides the service preferences to the telecommunication device to enable the telecommunication device to select a network connectivity from a plurality of network connectivities based at least in part on the\nservice preferences.  The plurality of network connectivities may be respectively associated with the network operator and one or more other network operators.  At 712, the service preferences are provided responsive to request from the telecommunication\ndevice or are provided automatically.\nAt 714, the network operator, generates updated service preferences.  At 716, generating the updated service preferences comprises generating the updated service preferences based at least in part on network usage at a location of the\ntelecommunication device.  At 718, generating the updated service preferences comprises generating the updated service preferences based at least in part on a user request or a user change to the provided service preferences.\nAt 720, the network operator provides the updated service preferences to the telecommunication device.\nFIG. 8 illustrates an example process of a service broker for generating service preferences for a telecommunication device based on input from network operators and providing the service preferences to the telecommunication device to enable the\ntelecommunication device to intelligently select network connectivities to use for transmission of network packets.  The process includes, at 802, providing, by a service broker, an open SIM to a user of a telecommunication device.  At 804, the service\nbroker may also provide a packet routing module to the telecommunication device.  The service broker enables access to the network connectivities of a plurality of network operators.\nAt 806, the service broker receives at least one of service policies, indications of network conditions, usage histories, or service fees from the plurality of network operators.  At 808, the service broker receives the service policies, the\nindications of network conditions, the usage histories, or the service fees as service preferences specified by each of the plurality of network operators.\nAt 810, the service broker generates service preferences for the telecommunication device based at least in part on the service policies, the indications of network conditions, the usage histories, or the service fees.  The generating may\ncomprise aggregating the service preferences of the network operators.  The service preferences may include preferences between at least two of the network operators.  Also, the service preferences may prioritize at least one of connection performance,\ncost, power consumption, bandwidth, packet delivery efficiency, security level, coverage, and user experience.\nAt 812, the service broker provides the service preferences to the telecommunication device to enable the telecommunication device to select a network connectivity from a plurality of network connectivities based at least in part on the service\npreferences.  The plurality of network connectivities may be respectively associated with the plurality of network operators.  At 814, the service preferences are provided responsive to a request from the telecommunication device or are provided\nautomatically.\nFIG. 9 illustrates an example process of a management service for generating service preferences for a telecommunication device and providing the service preferences to the telecommunication device to manage intelligent selection by the\ntelecommunication device of network connectivities to use for transmission of network packets.  The process includes, at 902, receiving from a subscriber, by a management service, a subscription for management of the telecommunication device through\nservice preferences.  At 904, the management service identifies a telecommunication device associated with the subscriber.\nAt 906, the management service generates service preferences for the telecommunication device based at least in part on a policy of the subscriber.  At 908, the service preferences include a policy to be enforced by a packet routing module of\nthe telecommunication device.\nAt 910, the management service provides the service preferences to the telecommunication device to enable the telecommunication device to select a network connectivity from a plurality of network connectivities based at least in part on the\nservice preferences.  The plurality of network connectivities may be respectively associated with a plurality of network operators.\n<BR><BR>CONCLUSION\nAlthough the subject matter has been described in language specific to structural features and/or methodological acts, it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific\nfeatures or acts described.  Rather, the specific features and acts are disclosed as exemplary forms of implementing the claims.", "application_number": "14137813", "abstract": " Techniques are described herein for generating service preferences for a\n     telecommunication device. Those service preferences are then provided to\n     the telecommunication device to enable the telecommunication device to\n     select a network connectivity from a plurality of network connectivities\n     based at least in part on the service preferences. The network\n     connectivities are respectively associated with a plurality of network\n     operators. The service preferences may be generated and provided by any\n     of a network operator, a service broker, or a management service.\n", "citations": ["7792516", "7925212", "20030043773", "20040053632", "20040092285", "20040165597", "20050070280", "20050164650", "20060268712", "20070019670", "20070136473", "20070286092", "20080080411", "20080253304", "20080313351", "20090005048", "20090245496", "20090324481", "20090325581", "20100095065", "20110130140", "20110131338", "20110319071", "20110320588", "20120163180", "20120166622", "20120208496", "20120224528", "20120236739", "20120238287", "20120270504", "20120322505", "20130157711", "20130324087", "20140003358", "20140086177", "20140172946", "20140185519", "20150373634", "20170180245"], "related": ["13732111", "61911047"]}, {"id": "20150070585", "patent_code": "10375343", "patent_name": "System and method for managing streaming services", "year": "2019", "inventor_and_country_data": " Inventors: \nSharif-Ahmadi; Seyed M. (Richmond, CA), Armani; Sam (Richmond, CA)  ", "description": "This application claims the benefit of Canadian patent application numbers 2,773,342\nand 2,791,935, which are hereby incorporated by reference.\n<BR><BR>TECHNICAL FIELD\nThe present disclosure is directed at a system and method for managing content delivery services, and more particularly for managing such services amongst a plurality of user devices though a common gateway.\n<BR><BR>BACKGROUND\nThe growth in data transmitted over wireless and cable networks has been driving consolidation of broadband operators, and creating a highly competitive global broadband service provision market.  \"Operator\" refers to an operator of a high-speed\nnetwork that offers data transfer services over one or more types of networks (e.g.: mobile networks and other wireless networks, wired networks, including telephone, cable satellite, and mobile networks, or a combination of these).  \"Broadband service\"\nincludes content delivered over a network; the network may be wired, wireless, cable, DSL, or other digital communication systems, or any combination thereof.\nOperators are evolving and becoming broadband service providers (referred to herein as \"service providers\").  Triple or quad operators means operators that provide three or four services, such as residential Internet service, residential phone\nservice, residential television, and mobile data services, which may in turn, include voice, Internet, and television.  Data usage over both wireless, DSL and cable networks around the world is growing.  Service providers may grow revenue by offering\nusers a variety of services, and by reducing the cost of the network operation centers (\"NOCs\") and the cost of data links.  Today, service providers are offering more services and applications, requiring larger NOCs, which in turn translates to higher\noperating costs for the service providers and therefore lower average revenue per user (\"ARPU\").  Service providers are looking for solutions that allow them to run smaller NOCs, offer services suited to users' profiles and the type of device that the\nuser is using at the time (e.g.: television (\"TV\"), laptop computer, desktop computer, pads, eReaders, or smartphone).\nToday, service providers can often identify where users are located, but it is difficult to know what type of content users want based on their context.  \"Context\" includes information about the nature of the user and content, including:\ninformation that refers to the user's location; the type of device the user is using to access the network; the level of connectivity access the user has; the user state, which includes information such as whether the user is connected to the network\nusing a mobile or wired connection; whether the user is accessing the network from home or while traveling (e.g.: on vacation or on a business trip); to which networks the user is connected; and the user's interests and history.\nUser demand for accessing media-rich content in real time is growing; concurrently, the types of devices that are used to view such media-rich content are also growing.  Broadband networks resulted from the migration of multiple networks, each\nhaving different characteristics and operating as silo networks wherein devices, applications, and networks are tightly coupled.  Typical broadband networks are designed based on the assumption that user devices used to access content are dumb terminals\nwith little ability to run rich multimedia applications.  Typical broadband networks were also designed under the assumption that traffic growth will remain linear, which has been an underestimate.\nConventional solutions to satisfying user demands for delivery of rich multimedia content in real-time has centered on a \"core-centric\" approach in which a centralized server resides within a NOC for each different type of network; an exemplary\n\"core-centric\" network 100 is depicted in FIG. 1.  This means that if a service provider is operating broadband networks to deliver TV content, residential Internet content and cellular phone content to specific user devices, then such service providers\nwould use three different NOCs, TV NOC 250, Internet NOC 350 and cellular NOC 450, respectively.  Each is directed to a particular type of user device, which usually do not share networks or content, for example, the user's laptop computer cannot use the\ncellular phone's cellular network to access content.  Likewise the cellular phone cannot access the cable network.\nIn such a model, a service provider uses software and servers that offer functionality such as determining the automatic bit rate (\"ABR\"), providing content inspections, and providing personalization engines that are installed within each NOC. \nChallenges arise when employing these approaches and serving large numbers, e.g. millions, of users; shortcomings associated with these approaches include scalability issues, accuracy issues, and synchronization of collected information.  As depicted in\nFIG. 1 a triple and quad player operator is managing multiple NOCs 250, 350 and 450, and within each NOC is run the same set of services including video transcoding and transrating engines, content reformatting, caching and proxy services.\nThree changes are simultaneously occurring with respect to broadband networks.  The first relates to digital content.  The amount of content available on the Internet is overwhelming for end users, even very technically savvy users, and the\namount of content is still growing exponentially.  This includes on-demand digital video streaming, television services such as video on demand (\"VoD\"), subscription video on demand (\"SVoD\") and pay-per-view (\"PPV\").  Also included in this market is\non-line video advertising, Internet protocol television (\"IPTV\") and mobile TV, as well as television provided through cable, digital cable set top boxes, and satellite.  References to \"television\" or \"TV\" herein shall refer to any of the above listed\nstreaming video services.\nThe second relates to the effect of a new generation of users and their needs.  These new users (often referred to as \"Millennials\"), represent the most populous generation that has ever lived on this planet.  They tend to be technology-centric,\nand both dependent on and aware of technology.  On average, each spends over $100 per week on technology-oriented products and services and directly influences over 80% of the spending in the home.  This is the generation that wants the right information\nsuited to their needs and context, delivered in the least amount of time.  This group of users are socially connected through their mobile phone, laptop and desktop computers, and are the driving force behind enabling connectivity through TV.  This\ngeneration wants to have personalized content; namely, content that is available on their own terms rather than on the terms of service providers and operators.  This is the generation that does not want to be bound to a particular location or device to\naccess specific content.  They like to be able to watch TV content on any device and location within and outside of the home and not just on a TV display.  They also want to be able to access Internet content and/or social networking services such as\nTwitter on their TV display while watching TV.  They are not only content consumers but also content generators and distributors.\nThe third relates to advancements in technology, and particularly advancements in customer premises equipment (\"CPE\").  A household often no longer just has a single TV display and a PC but may have multiple laptops and PCs, along with TV\ndisplays and mobile devices, such as smart phones, cellular phones, video game devices, net books, electronic reading tools (\"eReaders\"), pads, and portable music and video players, that are used in or outside of their homes (collectively, referred to\nherein as \"user devices\").  Additionally, users often have access to other user devices such as home residential gateways, set-top-boxes, routers, Wi-Fi access points and other networking equipment, and the use and availability of such equipment is\ngrowing rapidly.  These changes mean that content is no longer created, controlled and distributed by a specific organization, such as service providers, but instead content can be produced by anyone within a network and either pushed to, or requested\nby, anyone within the network.  Such content includes place shifted video content, multimedia streams, and personal digital content.  Therefore, a centralized approach wherein content is always produced by a selected entity and then distributed to\nsubscribers will no longer be effective due to variation between user interests, and because many users wish to play a more active role in generating content.  Additionally, to centralize all content being created for distribution also will not be\neffective due to the heavy network traffic that would result from distributing such content.\nShortcomings associated with the prior art include: 1.  Pre-formatted content storage any types of device Users' content preferences are very different, when, for example, they are outside of home, than when they are in front of their big TV\nscreen.  Deciding on storing the type of content for possible access from outside of home is therefore difficult.  Also keeping multiple formats suitable for all potential user devices that may be used to experience the content is not efficient.  Typical\nuser devices renew and update data displays very quickly and therefore repeatedly mining data and archiving different formats for possible access by different user devices is cumbersome, costly and inefficient.  2.  Content Inspection The data traffic\npath for the user is not always the same.  The purpose of content inspection is to identify the user's interests.  However, the user's interest is best determined based on user habits over a period of time and not solely based on a snapshot of the user's\ncurrent content usage.  Distributing content inspection over multiple network nodes provides a snapshot of the user's current usage based only on the current traffic flow.  Storing all traffic flows, determining the user identity, synchronizing the\ninformation for a post content inspection process in order to inspect usage over a longer period of time requires large amounts of storage, and extensive processing time.  This is highly costly and inefficient.  Conventional means allow the type of\ncontent the user is looking at any given moment in time to be determined, rather than a long term view.  Consequently, the real preferences of the user under different conditions cannot be predicted accurately.\nIn the prior art, a CPE only facilitates providing CPE specific and vertical functions, e.g. a CPE that only provides Set-Top-Box functionality or a CPE that provides Internet connectivity functionality.  Furthermore, in conventional methods the\nCPE has no role in content distribution functionality among subscribers while they are at home or outside using different methods of connectivity and in delivering content concurrently to different devices and/or users in parallel.\nAccordingly, there exists a need for a method and system that improves on the deficiencies of the prior art.\n<BR><BR>SUMMARY OF THE INVENTION\nThe system and method according to the invention includes a gateway (also referred to herein as a \"serving node\" or \"debox\"), to which user devices are registered and which serves as an intermediary between such registered user devices and\ncontent.\nThe system and method according to the invention includes a gateway having a control manager and a streaming server.\nThe control manager is responsible for registration of user devices with the box, including information about the capabilities and limitations of such device, and the users with whom the device is associated.\nThe control manager maintains usage logs for each user device.  These logs can be used to improve Quality of Service (QoS) by determining when errors or poor performance occurs and taking steps to improve the quality (for example by increasing\nthe buffer for streaming video).\nThe control manager acts as a message broker between user devices when one is used to interact with or access content on the other.\nThe control manager also manages the TV tuners included as part of the gateway.  Such tuners, and associated EPG information, are set to the appropriate channel and output to the appropriate user device by sending a request to the streaming\nserver.\nThe streaming server handles several duties.  It opens the port to the TV tuner, and handles the Electronic Program Guide (EPG), and modifies the format of content to the type of user device requesting the signal.\nThe streaming server also sets and monitors the quality of signal based on the size of the display.  The streaming server controls the size of the buffer needed based on the limitations of the device, its processor, the size of the display and\nthe signal.  Past user experiences with the device may be taken into account.\nThe streaming server thus can adapt to learned limitations of a user device to improve experience and can handle resolution changes and needs\nWhen a user device is receiving Internet content, the streaming server can send a link to the user device to the content and play no other role.  Alternatively, if the user device is local to the streaming server, the gateway can receive the\ncontent and adapt it for the user device.\nThe streaming server uses a transcoder to convert content from one format to another.  The transcoder provides both a coding and decoding process.  It receives input, decodes it, and codes it again as needed by the user device.\nThe gateway is able to authenticate users and user devices.  Limits on use may be in place (for example limits on a user's ability to access certain web sites or channels).  A single sign on (SSO) and backend gateway system may be used.\nThe authentication process can be used for local filtering so as not to display certain scenes displaying objectionable material (such as violence/sex) based on ratings of content (which may be scene by scene or a larger piece of content, like a\nshow or movie).  This can also be used to handle parental controls such as limit use to certain hours of the day or an overall limit on usage per day.\nOn use of the system is to take advantage of the connection between the user device and the gateway, which occurs even if the user device is distant from the gateway (e.g. outside of Wi-Fi range).  This allows a user device to receive adds based\non past users experiences, like browsing history, not available when logging in directly to a distant network.\nA method of determining a closest node for a user device is provided, including: providing a gateway having a server for receiving a request for content from a mobile user device at a remote location; authenticating the user and user device as\nassociated with the gateway; requesting the mobile device type and IP address from the mobile device; determining a geographic location of the mobile device; determining a proximity routing table for the mobile device; and determining a proximity\nneighbour information table for the mobile device.  The gateway further accesses a database associated with the user device to determine patterns of the user thereof.\nThe method of claim 2 wherein the patterns are provided to an advertisement server, and advertisements are transmitted to the mobile device based on the patterns.\nA method of displaying content on a television display is provided, including: determining a plurality of zones on said television display; displaying a video stream on a first zone; displaying means for user input on a second zone; and\ndisplaying content from the Internet on a third zone.  The content from the Internet may be from a social networking web site.\nA system for controlling a television with a mobile phone is provided, including: a gateway in communication with the television and the mobile phone; an application on the mobile phone displaying the functions of a remote control; on input from\na user of the mobile phone, the input is communicated to the gateway, the gateway communicating the input to the television.  The display on the mobile phone may be a d-pad.\nA method of controlling a television with a mobile phone is provided, including: providing a gateway in communication with the television and the mobile phone; providing an application on the mobile phone, the application displaying the\nfunctions of a remote control on a touchscreen of the phone; on receipt of input on the mobile phone, communicating the input to the gateway; on receipt of the input from the gateway communicating the input to the television.\nA method of determining usage patterns is provided, including; providing a gateway through which a plurality of users with a plurality of user devices can process content; when one of said users process content on one of said user devices,\ndetermining a particular user and a particular user device; the gateway gathering information about the content; analyzing the pattern of content processed by the particular user.\nA method of obtaining feedback about a video stream is provided, including: the video stream provided to a user device, through a gateway, said user device and an associated user registered with said gateway, including biographic information\nabout said user; at the conclusion of said video stream, providing a questionnaire to said user, answerable on said user device; Said gateway retrieving answers to said questionnaire, and returning said answers and biographic information to a server. \nThe video stream may be a pilot episode of television.\nA method of remotely managing a gateway in a residential home is provided, including: a service provider providing the gateway to the home; the gateway generating functionality reports to the service provider; the service provider providing\ntechnical support to the gateway based on the functionality reports.\nAn earlier embodiment of the gateway is described in PCT application no. PCT/CA2010/001536, which is hereby incorporated by reference. <BR><BR>BRIEF DESCRIPTION OF THE FIGURES\nFIG. 1 is a block diagram illustrating a prior art network services a number of user devices with both wireless and wired (landline) connections.\nFIG. 2 is a block diagram showing the gateway in communication with a variety of user devices at a variety of homes.\nFIG. 3 is a block diagram showing the gateway acting as a hub in communication with a number of user devices.\nFIG. 4 is a block diagram showing selected components of a gateway according to the invention.\nFIG. 5 is a workflow drawing showing the proximity detection process according to the invention.\nFIG. 6 is a drawing showing the split screen function according to the invention.\nFIG. 7 is a drawing showing an alternate split screen function according to the invention.\nFIG. 8 is a drawing showing a split screen function according to the invention for use on a mobile device.\nFIGS. 9a and 9b is a drawing showing embodiments of a remote control app according to the invention.\nFIG. 10 is a drawing showing the embodiment of FIG. 9a, detailing the area of functionality.\nFIGS. 11a to 11d are drawings showing different states and features of an embodiment of the remote control app according to the invention\nFIG. 12 shows the data analysis steps used in user characterization.\nFIG. 13 illustrates the filter application applied to a preprocessor.\nFIG. 14 is a block diagram showing the control manager of an embodiment of the gateway according to the invention.\nFIG. 15 shows a high level view of the communication within gateway 150 and the service provider data center.\nFIG. 16 shows the high-level software architecture of the gateway.\nFIG. 17 shows the basic workflow of the Gateway.\nFIG. 18 shows an overview of an embodiment of Gateway services, and the interaction with other applications.\nFIG. 19 shows a block diagram of an embodiment of an rStreamer service.\nFIG. 20 is a flow chart showing the content inspection process.\n<BR><BR>DETAILED DESCRIPTION\n<BR><BR>1.  Introduction\nThe embodiments described herein are directed at creating a network (or \"micro-cloud\") of service nodes, referred to as gateways 150, with each gateway being a configured CPE for servicing a home or other residential unit (such as a dorm room or\napartment), as seen in FIG. 2.  Each gateway acts as one node within a content distribution network (\"CDN\" 15), and is used to forward content to one or more user devices 180 that are registered to use that particular gateway 150.  The CDN 15 formed by\nthe gateways 150 and by the user devices 180 registered to each gateway 150 constitute a local area network.  Gateways 150 are able to communicate with each other throughout the network, either directly, or through server 325.\nAs seen in FIG. 3, gateway 150 may have the base functionality of a conventional set top box and is also the service provider's controlled node that can run applications for users and user devices 180 registered at the premises served by gateway\n150.  For example, as shown in FIG. 3, gateway 150 is connected to TV 180t, mobile phone 180p (which may be a smart phone), laptop 1801 and personal computer 180c.  Gateway 150 may be in communication with a set top box 180s, itself connected to a user\ndevice 180, such as TV 180t, that functions as a slave unit.  Gateway 150, depending on the user device's connectivity (for example, if the user device can connect to the CDN 15 through a wireless connection that is faster than the user's current wired\nconnection, gateway 150 will transmit data through the wireless connection), retrieves content for the user devices 180 and provides usage information to the operators.  In FIG. 3, wired connections are shown in solid lines and wireless in broken lines. \nThis architecture eliminates the need to run costly NOCs, and allows users to access data of many types, such as multimedia messaging, wireless VoIP, streaming video, video telephony, corporate applications, email, and wireless gaming.\n<BR><BR>2.  Gateway Components\nAs shown in FIG. 4, each gateway 150 includes processor 155 that executes software, including a variety of software modules, some of which are described below.  The processor 155 is coupled to memory 156 in the form of both permanent (flash\nmemory or hard disk storage) and volatile stores (random access memory).  Database 165 is typically stored in memory 156.  The operating system of the gateway 150 and the various modules described below are stored in the permanent memory storage such\nthat gateway 150 can be powered on and off without having its software erased.  During execution, parts or all of the software stored in the permanent memory store of the gateway 150 are copied into the volatile store where it is executed by the\nprocessor.  Gateway 150 also includes network communication modules 157, such as Wi-Fi port 157w, a DLNA port 157dl, Bluetooth port 157b, USB port 157u (there may be several USB ports 157u), cable modem port 157c, femtocell port 157f, and Ethernet port\n157e (and may include several Ethernet ports), DSL modem port 157d, speaker 158, and control interface 159 (with an IR interface, and optionally, selectable buttons for use by users), that are in communication with processor 155 and that are used to send\nand receive content.  A WiMax port may also be included.  Gateway 150 also has video and audio inputs and outputs 152, and may have several types of video and audio inputs and outputs (e.g. Svideo, composite, component, HDMI, optical, etc.).  Gateway 150\nalso has a power supply 153, and is typically plugged into a nearby outlet to receive power, and a plurality of TV-tuners 154.\nTV-tuners 154 accept RF band cable or antenna TV analog or digital input.  Preferably four or more TV-tuners 154 are present and they support VOD and EPG (meaning QPSK demodulation is required).  TV-tuners 154 may support satellite TV.\nEthernet port 157e enables a standard interface connection to the home IP LAN for media content sharing between gateway 150 and user devices 180.  Ethernet port 157e operates as a router and preferably at least four (10 M/100 M) ports are\navailable.  The Ethernet ports 157e providing routing, NAT, DHCP and DNS forwarding and support all IP device connections, such as network printers and VoIP phones.\nVideo formats supported by gateway 150 include Svideo, composite, component, HDMI, MPEG-1, MPEG-2, MPEG-4 (H264, DivX, Xvid and Nero Digital), Sorenson (used by fly and quicktime), WMV (Windows Media Video), VC1 (WMV 9 based, Blue Ray, HD-DVD),\nRealVideo, DivX, Xvid, FFmpeg and 3ivx (a different implement of mpeg4 part 2).\nAudio formats supported by gateway 150 should include 7.1 Dolby Digital (A/52, AC3); stereo, DTS Coherent Acoustics (DTS, Digital Theatre System Coherent Acoustics); MP1, MP2, MP3; AAC (MPRG-2 Part 7 and MPEG-4 Part 3) and Linear Pulse Code\nModulation (LPCM, generally only described as PCM)\nWi-Fi port 157w should support 802.11b/802.11g and other features such as security (WEP, WPA, and WPA2).  Gateway 150 will serve as a Wi-Fi access point.  A web interface should be provided to configure the Wi-Fi including set-up, maintenance\nand trouble shooting.\nBluetooth port 157b may support Bluetooth 2.0 and keyboard and mouse features.  Human Interface Device (HID) may also be supported as could be a mobile phone connection or headsets.\nUSB ports 157u enables gateway 150 to host a connection to user devices 180 or peripheral devices.  USB 2.0 may be supported and at least two USB ports 157u should be available.  USB ports can be used to support devices such as digital cameras,\ndigital camcorders, USB Hard drives, USB Flash memory, portable Blue Ray or DVD drives, GPS systems and the like.\nKeyboard and mouse connections may be supported via USB ports or Bluetooth.\nGateway 150 may be provided in a form fitting case (not shown) to fit on a typical TV shelf, and may be sized comparably to other and video components such as a Blu-ray player or amplifier.  The front panel of the case should include a reset\nbutton (optionally reset button may also be, or alternatively be, a software function).  The front panel should also include switches such as On/Off status; Recording/Replay status; Current Channel Number; any pre-set alerts, the current time, and other\ndisplays (as digits or icons) and an IR window for receiving input from remote controls.  The ports and connectors noted previously may be placed on the back panel of the case.\nIn operation gateway 150 will typically have a power input of about 100-240 V AC 50/60 Hz, and a power consumption of about 20 W. Gateway 150 should be operable in temperatures from 32 to 105.degree.  F. (0 to 40.degree.  C.) during normal\noperation.\n<BR><BR>3.  The Software\nThe software within gateway 150 should have the capability of receiving TV input and regular Internet content in all supporting protocol levels, provide regular VoIP services, and support connectivity functions including Ethernet (including when\nthe connectivity is initiated from cellular access), WLAN, DLNA and FemtoCell.\nBasic software services refer to a set of software services that are fundamental building blocks for the rest of the software services.  The basic software services are a set of software engines which would either provide a service and/or the\ninformation necessary for enabling delivery of other applications.  For example, streaming TV content and providing transcoding to a user mobile device is a service, whilst profiling a customer in terms of their interest in content, and usage patterns,\nis a function enabling a personalized advertisement insertion service.  The basic software services are described as software \"engines\", which are described below.\nThe Activation Engine is responsible for activate certain services that gateway 150 is authorized to serve to users.  Such services include: a. TV.fwdarw.Enabled/Disabled Channels and bundles that users are authorized to view b.\nInternet.fwdarw.Enabled/Disabled c. VoIP.fwdarw.Enabled/Disabled d. Network connectivity.fwdarw.Enabled/Disabled (means for a remote reset, configuration and troubleshooting)\nThe activation engine 200 is responsible for registering users, devices and services to which users have subscribed.  The input parameters for activating gateway 150 may be through a set of web interfaces.  This interface should follow the\nexisting operator's activation flow.  A master interface defines the activation fields and account setup.  The activation engine runs on the gateway while the activation field operates and runs by first the customer service and then by user device 180\nafter the activation is enabled, for example for activities such as device registration and/or even activities such as parental control for a user device 180.\nUsers may have more than one gateway 150 per household, for example to take advantage of more hard disk space or TV tuners.  In such a case, the gateways will function in a master-slave arrangement.\nAn Authentication, Authorization and Accounting Engine (AAA-E) is responsible for providing the authentication and authorizations of the gateway 150 registered users based on their subscription.  The authentication process must follow the\nbroadband operator's authorization process.  This typically includes combining device_ID (an identifier associated with a user device), username, password, IMEI number or other information.  The user must be distinguished from the user device 180 that is\nbeing used.  For example, a registered wireless data card may be used by two different users at the gateway 150 site.  While the card is the same, or even the laptop is the same, the users may be different.  Therefore a combination of wireless network\ncard, device id, and username and password is required to identify and distinguish the users and therefore their preference and their subscription to a service.  The accounting engine is also responsible for some of the billing and audit services.  It\nalso keeps track of service usage based on the subscription.  The subscription could be based on any combination; examples include: accessing TV only; Internet only; limited number of user device access; limited access means; Ethernet, WI-Fi, FemtoCell,\nand/or cellular along with other service subscriptions.  Any service provider accounting function may be included in Authentication, Authorization and Accounting Engine 210.\nA User Characteristics Catalog Engine (UCC-E) tracks information about registered users of user devices 180.  Personalization is a process by which the content pushed to one user could completely be different from that pushed to another user. \nIn order to be able to push personalized content to a user, gateway 150 must be able to characterize users' behavior.  Characterization of users could start from user segmentation, which means that the user population is subdivided, into more or less\nhomogeneous, mutually exclusive subsets of users who share common user profile characteristics enabling the possibility of providing them with a more personalized content.  The ultimate objective of the User Characteristics Catalog engine is to be able\nto characterize the user in terms of static and dynamic attributes of the user.  Static attributes are those that do not change rapidly, for example gender, age, marital status and income status.  Dynamic attributes are those which change more rapidly\nand which relate to the sensitivity of each user toward their surrounding social events such as politics, fashion, news and the user's location.  The latter is a characteristic with a substantial impact on users' preference, as for example the content a\nuser is interested in when the user is mobile using a small mobile user device is very different than the content that a user is interested in when they are using a larger screen user device.\nThe UCC-E should work using a definition dictionary which could be updated, extended and modified as needed.  For example, if a user was being characterized based on two parameters, a third parameter could be added or replaced by one or both\nparameters and the characterization should be continued based on the new definition dictionary.  The UCC-E should incorporate an artificial intelligent model whereby it continues learning about users' behavior and usage patterns over time, and therefore\nbe able to produce much more accurate characterization of the user.\nThe User Tracking Engine (UT-E) contains the information about the active user session's user device 180, identity, time, date, activity and duration, context, location, and the network connectivity and the proximity to gateway 150, which is\nprovided by routing measurement functionality in the UT-E to determine the optimum path for data transfer and closest content access.  This function may be performed with a client agent on the user device 180 or other means of measurements using lower\nprotocols to avoid client software on the device.  The UT-E also maintains information about accessed content, including TV channels, PVR use, VoD, internet sites, and even the type of content (e.g. video, text and/or mixed).  The UCC-E uses this\ninformation as a set of parameters in order to characterize the users.  The history of the user is used by the UCC-E. User tracking may be configurable, for example, user device ON/OFF; TV activities only; Internet activities only; or video activities\nonly.\nThe Content Personalization Engine (CP-E) uses the UCC-E and the UT-E. The CP-E uses an automatic intelligent search engine that finds matched content for the user.  The CP-E may be a software engine within gateway 150 or may be an external\nservice engine provided by the service provider.\nAn internal CP-E resides within gateway 150 and interfaces with both the UCC-E and the UT-E. The CP-E automatically searches the web and content producers and distributers on behalf of each registered user of gateway 150.  The CP-E indexes the\ncontent and has the content prepared for the user.\nAn external CP-E may be a large personalization server on the service provider network for content distribution, for example from advertising companies with relationship with the operator.  This external CP-E receives information from the UCC-E\nand UT-E (through common and single interfaces).  The CP-E then matches the content, in this case advertisements, and pushes it to gateway 150.\nIn a combined model, the internal CP-E could be considered as an agent of an external CP-E. In this case the internal CP-E supports the external CP-E interface.  The internal CP-E receives information from the UCC-E and UT-E and communicates\nwith the external CP-E for receiving the matched content.  The external CP-E may locate matching content from the other gateway 150 users in different households that share the same interest and are within the same social community authorized by the\nusers.  The external CP-E could be a global service that interfaces with many content distributers for finding matched content.\nThe decision making rule of the internal CP-E should be based on a configurable set of rules which could be updated and modified remotely.\nAn Intelligent Search Engine (IS-E) is an automatic search engine which fetches the users requested information and searches and compiles the information on behalf of the user.  For example, if a user is traveling to Las Vegas and inputs its\ntrip itinerary to the IS-E, the IS-E will find the location of restaurants, car rental agencies, and places to visit based on the proximity, interest and time line of the user.  The IS-E collaborates with the ICT-E (described below) to transcode and\narrange the content to meet the user's profile.\nA TV Transcoding Engine (TVT-E) is responsible for transcoding a stream to the user based on characteristics of the user collected by the UT-E. The TVT-E is therefore one of the users of the UT-E and also interfaces with the TV receiver.  The\nTVT-E must transcode and rate shape the stream to meet the user's particular user device 180, access network type, speed and proximity to gateway 150 (both in distance and/or number of hubs).  The TVT-E is able to stream the live received TV feed per\nchannel to each user device 180, and both a PVR stream and VoD stream.  The latter may require the appropriate subscriber package.  As an example if a user has purchased a VoD service, the user may only be allowed to watch the service on any of his/her\nuser devices 180 for a period of 24 hrs, or on just on a single device at a time.  If the user may watch a portion of the movie on one user device 180 and another portion on a second user device 180 then the TVT-E tracks the used content and the content\nusage policy and subscription.  A similar process may be used for PVR content.\nFor the purpose of digital right management, there may be client software on mobile phone and computer user devices which remove content proportionally as the user plays the content in case of PVR-d and VoD content which is being viewed.  The\npolicy of DRM is an additional layer over the top of all engines which decides on the sharing mechanism of digitally protected content.  Note that for the DRM purposes it may be useful to have software installed on the client side.\nThe Internet Content Transcoding Engine (ICT-E) is a service provider as is the TVT-E, but the ICT-E transcodes internet traffic.  The ICT-E also uses the UT-E and provides the content transcoding to the user based on their current user device,\nsoftware running on the user device, network type, speed, location and proximity.\nThe TV/Internet Content Mix Engine (TVIC-E) is responsible for mixing Internet based content with TV content.  The TVIC-E fetches and receives the content which needs to be displayed on TV as the user watches the TV.  This engine has a GUI\nengine that communicates with the ICT-E and the TVT-E and the TVIC-E has the task to select the format of the content for display based on the user device and preferences.\nThe Content Sharing Engine (CS-E) is responsible for sharing user content amongst other users.  The CS-E uses the UT-E to determine the current user's profile and if necessary uses the ICT-E for transcoding the content.\nThe Master Security Engine (MS-E) is responsible for security related to gateway 150, including intrusion detection, authenticating users, authenticating any access and transactions with outside sources, and encryption/decryption of streams.\nAs shown in FIG. 5, Proximity Engine (P-E) 320 is responsible for measuring the proximity of a mobile user device 180 (designated as \"mobile client\" 185) to gateway 150.  This includes the number of hubs to the user device.  P-E 320 is also\nresponsible for identifying the best and shortest path to the mobile user device.  P-E 320 will also use the geographic location of the user device which is sent to gateway 150.  Optionally, client software may be installed on the user device to\ncommunicate with P-E 320.\nAs shown in FIG. 5, the user device and user credentials are first authenticated by Global Access Portal server 325.  GAP server 325 is responsible for authentication of any gateway 150 local or remote users, and determines the user's registered\nlocation.  Once authenticated P-E 325 requests the mobile client 185's IP address and type of user device 180.  With that information P-E 320 determines the geographic information, a proximity routing table and a proximity neighbour information table. \nThe proximity neighbor information is a table of information related to a specific gateway 150 and any associated user.  This table determine the closest node (or source of information) to provide the content for the requesting mobile client 185 (or\ngateway 150) with the best possible quality of services.\nThe transcoding of TV/Video content varies depending on the particular user device 180, taking into account of the user devices' physical and performance constraints such as screen size, CPU, memory size, and audio/video codecs.  Furthermore,\nvideo stream content is transrated and rate shaped in light of the user device 180, access network type, speed and proximity to Gateway 150 (how far and/or how many hubs to the user).\nTranscoding is used when gateway 150 reformats content during TV broadcasting, TV Streaming, TV peer to peer, TV tuner sharing to other user devices via Ethernet, Wi-Fi or Bluetooth.  It is also used when content is being transferred from\ngateway 150 to user devices.  For example, if video content is transferred from gateway 150 to mobile phone, gateway 150 first transcodes and then transfers.  The transcoding and transrating process should be invisible to a user.\nGateway 150 provides an interface that allows users to transcode and save their contents in different formats; making it cross-device transferable.  The interface will be called from each user account.  It can be started and stopped at any time\nbased on availability of Gateway 150 resources.  It can convert the content and save it on Gateway 150 hard drive for further transfer.\nGateway 150 also reformats Internet content for the user device, by using content inspection, understanding the HTML content, downloading the content, and converting it to the appropriate format.  Gateway 150 can combine content for the TV\ndisplay and Internet, by fetching and receiving internet content, transcoding the content to the most suitable format for TV display, and mixing up with TV content; and displaying both on TV.  A split screen may be used for multiple purposes e.g.\nwidgets, advertising, menus, keyboard, game, or Internet browsing.\nUser owned content may be shared between user devices.  There are web portal service providers and social networking service providers which allows users to create accounts and share their own contents with other users within their network. \nThis service should support the interface to the operator's selected provider of such service for the content access.  Also user tracking information to determine the current user's access profile should be considered and transcoding completed if\nnecessary.\nGateway 150 allows content sharing and saving amongst registered user devices 180, such as TV displays, laptops, PCs, mobile phone devices and gaming consoles.  Gateway 150 can act as a storage device to save content.  Gateway 150 also allows\nfile transfer between user devices via Ethernet.  WI-Fi or Bluetooth connections.  For example, files may be transferred from a PC to gateway 150 or from gateway 150 to a PC; or to or from a mobile phone to gateway 150, etc. Files may be uploaded or\ndownloaded directly to gateway 150 from the Internet.\nContent files may be transferred between gateway 150 and USB devices.  For example, gateway 150 can be used to store personal digital media contents from digital cameras or iPods (image or MP3 files) onto gateway 150's hard drive.  Gateway 150\ncan then be used to organize the content, for example, by managing images, creating playlists and modifying properties, such as the format, of the saved content.\nGateway 150 may use auto detection with certain user devices.  In this case, the user will be prompted to provide certain parameters such as user account, location and content management parameters, and then the transfer begins.\nAlternatively, the user can specify when files are to be transferred.  In this case, the user selects files to import and is shown a set of icons appears representing different types of content.  The user selects the appropriate icon, is then\nasked to connect the user device.  The user connects the device, sets the parameters for the transfer and the transfer begins.\nA user is able to view, modify and play saved media content on the hard drive of gateway 150.  Options could include creating a slideshow, creating a playlist, managing mages, playing a music library of a user device connected to gateway 150,\nmodifying properties of the content, or organizing the content.\nGateway 150 has several ports, such as USB port 157u, that allows connection of external hard drive and an on-screen interface facilitating the file transfer.  The hard drive can be connected and called by any user.  It can be used at any time\nand is user based.  The interface allows the user to store allowed contents.  The external hard drive may be auto partitioned based on number of users of gateway 150.  Users can also modify properties of the external drive; organize the contents thereof;\nand use applications to create photo albums, music libraries, and video libraries.\nGateway 150 may also allow connection of an external home theatre audio system or an external DVD player.\nGateway 150 also may allow connection of user devices 180 that are digital media sources, such as cameras, camcorders, iPods etc. An on-screen interface should be available to facilitate managing the connected user device 180.  The user can\ntransfer allowed content, play a music library of the connected user device, modify properties of the content on the connected user device, and organize content on the connected user device.\nGateway 150 has an internal Wi-Fi LAN router and port 157w, and provides an on-screen interface for basic and advance configuration.  It can be configured by gateway 150 administrators and can be used at any time.\nGateway 150 has a pre-set interface that allows insertion of account information and instant connection to satellite radio providers such as XM or Sirius Radio.  The user inputs an account setting and gateway 150 connects to radio account\ncenter, allows channel changing and can transfer the signal to other registered and capable user devices 180 connected to gateway 150.\nGateway 150 can display an on-screen full keyboard on a user device.  It can be called by any user and used at any time.  It allows users to imitate typing action and type text commands for gateway 150.  It can be used for searching, and can be\ntoggled between a full keyboard and a phone pad keyboard.\nAdapted versions of the user interface for a PC or a mobile phone can be used through gateway 150 and display on different user devices 180.  The interface can be called by any user and used at any time.\nGateway 150 may be able to allow users to store content on extended online storage provided by the operator.  Such storage access can be connected and called by any user and used at any time.  It requires an Internet connection and can use a FTP\naccount or HTML or Flash GUI.\nGateway 150 user interface engine (UI-E), which partially feeds UCC-E, determines the proper zoning of a screen 600 by type of request, type of content, availability of information associated to the request/content and the eligibility of user\nregarding to view the different level of the information.  CP-E also is involved in this process as is a media object database containing information related to the object, object owner and also the type of request which is initiated by the user.  Based\non this information, UI-E determines how to split screen 600 into several zones.\nAs shown in FIGS. 6, 7 and 8, gateway 150 may displays into several zones to view TV content, Internet browser and/or some widgets on display 600 at the same time.  UI-E can split display 600 into 2, 3, 4 or more zones.  Each zone may be\nassigned to different user.  Each zone may individually access a menu and receive commands.  UI-E can automatically fetch info regarding the in progress content from Internet and preview it in the browser inside the assigned zone and can enable users to\naccess other viewers' feedback and ratings regarding any targeted or in progress content in an assigned zone.\nFIG. 6 shows a TV display 600 split into four zones.  Zone A displays information generated by gateway 150.  Zone B shows a live TV stream.  Zone C shows a keyboard for allowing the user to enter data.  Zone D shows information about the user. \nZone E shows some advertisements to the user provided by gateway 150.\nFIG. 7 shows a TV display 600 split into three zones.  The current content display (which may be television or another source) is displayed as background using the entire display 600.  A second zone displays advertising and information about the\ncurrent content.  A third zone provides a menu and system service information.\nFIG. 8 shows a mobile user device, such as a smart phone, with display 600 split unto several zones.  In this case Zone A displays a video stream, zone B user information, zone C system services menu, zone D, an EPG display and zone E system\nservices within the EPG zone D display.\nGateway 150 may be used for accessing personal messages, including emails, text messages, and allows users to twit and stream access to Twitter accounts.  Similarly, gateway 150 can be used for accessing social networking profiles and allows\nusers to access their preferred social networking accounts and display them on registered user devices 180.\nGateway 150 can also be used for sharing content, as users can share their digital content, such as digital images, videos or recorded TV, with other users, including users of different gateway 150 devices.\nGateway 150 also has several other software services available to users, including email services (serving registered user accounts); virus protection (servicing registered user devices); Internet favorite synchronization; and mobile keyboard\nmapping to TV which allows users to use their mobile devices (iPhone soft key or windows mobile hard key) in same fashion as if they had a full keyboard in front of TV, i.e. the mobile user device is used as the keyboard for accessing Internet on the TV\ndisplay.\nGateway 150 can interact with a mobile user device 180 that uses a remote control app to control a television or other display.  The use of the remote control app 900 allows uses to take advantage of the touch screen functionality of most smart\nphones.\nThe remote control app 900 allows users to use a gesture action (in addition to using the more traditional Left, Right, Up, and Down buttons (known as a \"D-Pad\") 910 to navigate TV menus and screens as shown in FIGS. 9a and 9b.  The display of\nthe smart phone appears similar to that of a conventional remote control.  The input area for a touchscreen display 920 using the remote control app is shown in FIG. 10 which allows for gesture functionality.  Similar to any regular physical remote\ncontrol, the remote control app 900 has control buttons 930 for audio and video content, and in addition user can type on any certain data entry fields 940 on the TV screen or mobile screen.\nFIGS. 11a through 11d show different displays presentable on the smart phone using mobile app 900 for different purposes.  FIG. 11 a shows the D-Pad; FIG. 11b a traditional audio/video control layout; FIG. 11c a numerical keypad; and FIG. 11d a\nkeyboard.  Remote control app 900 can use the native capability of the mobile devices with a touch screen as a data entry device.  The user can use the native keyboard of the mobile device to type text or any other type of data entry purpose (e.g. chat,\nsharing comments or etc.).\nRemote control app 900 can recognize its locations and determine if the user is sitting in front of TV, or is at home close to TV or not.  It will disable when user leaves the home is far from the TV.  This functionality prevents remote user app\n900 from interfering with a user who seats in front of TV and interacts with the TV using its regular remote control.\nGateway 150 can also provide Peer-to-Peer services, and interface with a Webcam to allow users to use the webcam for video capture, monitoring and video chatting and remote control services, and can allow users to use their mobile phone as a\nremote control for gateway 150 through a browser interface.\nGateway 150 includes software to perform a number of maintenance tasks, including: Auto Defragmentation, for periodic automatic disk check and defragmentation; Backup and Resource Center, available online or included with gateway 150 to\nauto-backup vital user settings; Interactive Help Center, which is an interactive online library of resources to help users find their way around features and issues; and Remote Assistance, which allows users to contact a customer care representative via\non-screen interface.\nUser Content Characterization\nThe UCC-E operating in gateway 150 provides the advantage of characterizing users on the edge of the network for all users at single premise.  Users at a single premise often have overlapping interests and that similarity assists in analyzing\neach user more accurately.  This means that more accurate information can be provided to the operators about the users which allow the operators to take ownership of users most valuable information from an advertising perspective; their characteristics,\nwhich would generate more revenue from the advertising industry.\nUser characterization is the analysis of a user's activities on Gateway 150 and determination of a user pattern so as to identify the content that he/she is interested in. It characterizes a user's interest and preference in his/her daily life\nthrough his/her activities using Gateway 150.  Characterization is based on current condition and also historical data.  The characterization model must continue to learn about users' behavior and usage patterns over time and therefore be able to produce\nmore accurate characterization for the users.  Characterization will be done on per user account basis\nThe UCC-E is one of the software engines within the middleware running on gateway 150.  Gateway 150 handles the entire user's traffic flow through whether a user's mobile phone is connected through a cellular access point, or the user is at\nhome, or is on an Internet cloud.  Therefore gateway 150 can characterize a user over time and \"learn\" about each registered user at a premise gradually.  To accomplish this characterization, gateway 150 must be aware of the context of the user.  Context\nincludes any information that can be used to characterize the situation of a user.  As an example the UCC-E may characterize a user based on his/her location, the time, type of device, access network and a typical activities within that context (state). \nThe context (state) of a user defines the type of activities.\nThe UCC-E builds a tree, which describes the user's context.  The CP-E uses these characteristics to find the content, which is most usable and is of interest to the user.  To match the content the CP-E must distinguish the management, updates\nand matching of the content to reusable contents such as news, movie and advertisements.  Both the UCC-E and CP-E are highly configurable in terms of algorithms, thresholds and weights.\nThe UCC-E determines the characteristics based on the current condition, and it also has to take into consideration the history of the user.  Therefore both current condition and history are input to the UCC-E. Some social variables also impact\nthe user characteristics.  For example, a holiday event for two users within the same neighborhood could have different importance and effects on them.  At the same time while someone may normally not be interested in politics, but an event may\ntemporarily change this, for example the 2008 US election for Canadian residents.  This means that the periodic social events, such as holidays, and instant social events, such as election, are also inputs to the UCC-E.\nCharacterizing a user includes defining the content description which users use, such as title, keyword, category, time and location; the user description, such as user preferences and history; the user context description, such as time,\nlocation, activity, device profile, active network access profile; and the user description extension, such as gender, age and activities.\nOver time, the UCC-E can detect a typical user session from an atypical user session.  One way to detect the atypical session is to use Mahalanobis distance statistics in the user session space.  Detecting the outliers (atypical) is important\nand valuable for cleaning the noisy user session history and avoids characterization based on random or false information.  The user history is taken into account since if a session is typical it is happening in a regular and periodic model and an\natypical session is no longer atypical but is a typical session should it have this pattern within a certain time, which is then characterized as such by the UCC-E. The UCC-E should also detect a page request as a user action or a system or automatically\ngenerated web action, which categorizes content description as that which a user has viewed (pushed, or pulled).\nThe UCC-E includes a Characterizer Module which tracks a user's browsing behavior down to individual mouse clicks such that advertisers, through gateway 150, can personalize their advertised content/products.  This module should distinguish and\nanalyze user data and the usage of such data.\nThe UCC-E also includes a Data Analyzer module which distinguishes the types of data that user is accessing.  As an example the data could be categorized in the following categories: Content: This is the real data that the site was designed to\ndeliver and convey to the users.  This type of data is consists (usually) of the text and graphics.  Structure: This is the data that describes the arrangement of the information within the page.  Intra-page structure information includes the arrangement\nof various HTML or XML tags with a given page.  This can be represented as a tree structure, where the \"html\" tag becomes the root of the tree.  Inter-page structure information is hyper-links connecting one page to another.  State: This is the data that\ndefines the frequency at which the data changes, e.g. a breaking news headline vs.  the full story of the headline Usage: This is the data that describes the pattern of usage of content, such as IP addresses, Page references, and Date and time of access\nUser location User device User access network\nSince the data is being analyzed on gateway 150, the performance of the information transfer and content value could be collected at the same time, such as the time which user spend on each page or even viewing the content.  Note that \"D-Box\" in\nthis acts like a proxy since it is the point of contact for all users within the premise.  Higher performance could be realized by ability to predict the future page requests correctly and accurately.\nUCC-E 220 uses a data processor for preprocessing, pattern discovery and pattern analysis.  As shown in FIG. 12, in order to discover patterns, the data retrieved needs to be processed.  Preprocessor 410 converts the content, structure, state\nand usage information into a data abstraction that is used for pattern discovery 415.\nTracked activity for the UCC-E can include the time, the location (home, office, Wi-Fi spot, etc.), the type of user devices used, the network connectivity type (Wi-Fi, Bluetooth, Cellular, etc.), the activities and duration (TV, Internet,\nVideo, content sharing, etc.), and the context, which includes TV content, such as TV channels, PPV, VOD, program category, or Internet, such as website visited and keyword monitoring.  Gateway 150 can detect if a TV user device is on or off and can\ndistinguish multiple users using the same device, for example if several users are watching different portions of a split TV screen.\nUsage preprocessing is used to distinguish user sessions from server sessions.  Therefore first the user is distinguished from: another user that uses the same device; the same user that is using a different browser; and the same user that is\nusing a different device and network.\nThe next step is then to identify the user's usage by obtaining a request that was generated from the user, or automatically generated, as a link within a page.  Table 1 is an example of such usage.\nTABLE-US-00001 TABLE 1 Rec Device Net Method/URL/ Size Action Server Browser # Dev_IP Time Type Type Protocol Statu (e) by IP Type 1 127.0.1.26 [dd/mm/YY: MotoQ 3G \"GET A.html 200 4000 -- 239.150.24.67 Java/3/2(Moto I) T:M:S:ms] M230 HTTP/10\" 2\n127.0.1.26 [dd/mm/YY: MotoQ 3G \"GET B.html 200 3050 A.html 239.0.24.67 Java/3/2(Moto I) T:M:S:ms] M230 HTTP/10\" 3 127.0.1.26 [dd/mm/YY: MotoQ 3G \"GET C.html 200 5020 A.html 239.1.24.66 Java/3/2(Moto I) T:M:S:ms] M230 HTTP/10\" 4 127.0.1.26 [dd/mm/YY:\nMotoQ 3G \"GET D.html 200 3080 -- 239.150.24.67 Java/3/2(Moto I) T:M:S:ms] M230 HTTP/10\"\nContent preprocessing classifies the content to different categories.  The result of the content classifications is used to filter the input or the output to the pattern discovery 415.  The page view could also be used to filter the sessions\nbefore or after pattern discovery.  As an example once a pattern is discovered, then the classified material of the page view could be used to focus the result to a certain subject or class of product.  FIG. 12 illustrates the filter application applied\nto preprocessor 410.\nThe preprocessor 410 thus classifies page views.  The page views could be classified based on topics and/or intended use, such as blogs, news, social networking, academia, corporate, personal, shopping, non-profit and others.  Page views convey\nthis information through text, graphic, and multimedia.  The information on the page views must first be converted into a quantifiable format.  Some version of a vector space model could be utilized to accomplish this.  Keywords or text descriptions can\nbe substituted for graphics or multimedia.\nFor the static page views, HTML/XML is parsed and reformatted based on an algorithm to break page content to categories suitable for preprocessing.  When breaking down dynamic page views, first the dynamic sections are distinguished from the\nstatic sections.  If the page is highly dynamic then it is broken down into sections based on the state of dynamic sections, e.g. a section could by highly dynamic while another one is semi-dynamic.  An example of such content is when viewing a news\ndescription page the reading pane is static while the area around it is dynamically changing.  Within the dynamic section some sections are highly dynamic and are automatically changing frequently while some sections are changing only if the page is\nrefreshed by the reader.  In fact these sections changing on refresh are the sections that are relying on user characteristics in order to push the personalized information.  A given set of server sessions may only access a fraction of the page views\npossible for a large dynamic site.  Also the content may be revised on a regular basis.  The content of each page view to be preprocessed should be assembled, either by an HTTP request from a crawler, or a combination of template, script, and database\naccess.  If only the portions of page views that are accessed are preprocessed, the output of any classification or clustering algorithms may be skewed.\nThe structure of a site is created by the hypertext links between page views.  The structure of a site and its preprocessing could follow the same model as content preprocessing described above.  Once again there should be a different model on\ndynamic and static pages.\nThere are main parts in pattern discovery.  The first part is based on the algorithms and methods used for the discovery.  These algorithms could be based on a single or a combination of statistical analytics, pattern recognition, data mining,\nand machine learning.\nStatistical techniques are used to gather information about the user visiting a site.  This includes the frequency at which the user visits the site, the mean value for the time the user spends on the site, the average length of navigation path\n(from where user follow the content linked from one to another page) and others.  These tools include statistical information such as the most frequently accessed pages, average view time of a page or average length of a path through other links or\nidentifying invalid URLs.\nAssociation rule generation can be used to relate pages that are most often referenced together in a single server session.  For example, if a user first visits a sporting equipment site and later visits an electronic equipment site then the\nassociation rule may reveal that the user should be interested in electronic sporting equipment.\nRelation of the pages that user browses may reveal the user's interest in content which assist in clustering and associating and grouping the content based on user interest.  Based on this information, dynamic links could be created to request\non behalf of the user, content based on the user interest.\nAs discussed previously, once the content is discovered and classified in different categories, the result can be used to categorize the user.  The classifier module will learn over time how to classify the user using AI algorithms such as\ndecision tree classifiers, naive Bayesian classifiers, k-nearest neighbor classifiers, Support Vector Machine or others.\nThe last step is Pattern analysis, which filters the irrelevant rules or patterns that was discovered through the discovery phase by using the raw content as above figure shows.  The filters and rules could also be through configuration\nparameters or rule and/or policy engine.  The most common form of pattern analysis consists of a knowledge query mechanism such as SQL.  Another method is to load usage data into a data cube in order to perform OLAP operations.  The content and structure\ninformation can be used to filter out patterns containing pages of a certain usage type, content type, or pages that match a certain hyperlink structure.\nGateway Features\nGateway 150 has the capability of receiving TV from IP, Cable and Satellite and is integrated with DSL modem capabilities, Wi-Fi and optional wireless 3G network connectivity.  The integration of TV, DSL and Wi-Fi reduces the customer premise\ninstallation time and prevents the need for multiple trips to the customer premise for different installations.\nThe middleware of gateway 150 is based on a component software architecture that provides a series of gateway functionalities, additional to standard STB and Modem features, and allows gateway 150 to function as an ASG (Application Serving\nGateway).  With this solution, users of gateway 150 become subscribers of the ASG.  User devices 180; such as TVs, mobile phones, laptops and PCs will all be registered to the ASG through an online portal during the gateway 150 activation process in a\nvery simple process.  To the end user, gateway 150 appears as an integrated Set Top Box that reduces their need for extra equipment and wiring requirements while delivering additional features and benefits that are part of the service differentiation\nofferings.\nThe system does not require installation of a client application on any of the user devices 180 (although in an alternative embodiment it may use such a client application).  All traffic is handled by Gateway 150 through an AI (Artificial\nIntelligent) engine that can distinguish and manage each user's traffic, under varying connectivity situations (device, network type, location, etc.).  The approach distributes the CPU and memory requirements for millions of users out to the subscribers'\npremises, similar to a subnet, enabling faster and more accurate data processing.\nGateway 150 is able to receive live TV broadcast from (cable and IPTV) service provider including free channels and encrypted channels based on subscriber package.  Gateway 150 understands the encryption algorithm used by service providers for\nsuch encrypted channels.  Gateway 150 can also support PPV/VOD services from the service provider and can follow the subscription package to support the services and can differentiate PPV/VOD from a regular TV broadcast.\nGateway 150 can provide TV streaming to other user devices e.g. laptops, mobile phones.  Gateway 150 is able to stream a live received TV feed per channel based on permission level to other users.  Gateway 150 can transcode TV streams\ndifferently and automatically based on the user device receiving the stream, taking into account the user devices' physical and performance constraints such as screen size, CPU, memory size, and supported audio/video codecs.  Gateway 150 has a\ntransrating dynamic speed control to suit the network bandwidth during streaming.\nGateway 150 allows user watching a TV stream to resume playing from a previous stop point while streaming to user devices.  A user can thus pause, play and go in reverse for a live TV stream.  For mobile phones and similar user devices, a hot\nkey can be implemented.\nUsers can view live TV program and recorded TV/video contents on a TV display simultaneously.  Gateway 150 supports pause, replay, rewind, and fast-forward functions for both live TV programs and recorded TV/Video content while watching on TV or\non PC or mobile or handheld user device.\nA user can play recorded content on a user device while available tuners are allocated for recording.  Gateway 150 should have a buffer, for example a 60 minute buffer for time shifting (recording and playback) a TV program.  This buffer should\nalways contain the past 60 minutes of live TV content being watched.  This buffer can be emptied when gateway 150 is turned off.\nA user can also switch from a watched program from live TV (with time shifting buffering in progress), to a recorded TV or video content while the time shifting buffering continues, or to programs being recorded.\nFor TV streams using PPV and VOD services, the availability of the content is subject on the subscriber package.  For example, if a user has purchased a VOD, the user may be allowed to watch the stream on any of his/her user devices for a period\nof 24 hrs, or perhaps just on a single device at a time.  The user may also be able to watch a portion of the content on a first user device and another portion on another user device.  If this is the case, Gateway 150 must follow the user's device and\ntrack the used content and the content usage policy and subscription.  Therefore gateway 150 is able to inspect the content and differentiate PPV/VOD streams from regular TV streams, allow conditional access and obtain information about the subscriber\npackage from the service provider.\nA users' service subscription is resident on servers controlled and managed by the service provider, Gateway 150 can communicate with such servers and obtain information about the user's subscription to determine the services for Gateway 150\nusers.\nUsers can also use gateway 150 to watch TV streams wherever they are located with a registered user device, including PPV and VOD TV streams.  Thus the local receiver capacity (number of TV tuners) is no longer the limitation in receiving live\nTV contents.  Gateway 150 is required to share the live TV/PPV/VOD contents with Gateway 150 users and the process is managed by service provider.  A user may be able to watch TV/PPV/VOD contents through other Gateway 150s in any geographic location. \nFor example, assume User A only two TV tuners in his Gateway 150.  Both tuners are already engaged in recording activities and user A would like to watch a live hockey game on channel X. User A subscribes this live hockey game program thru a PPV service. Service provider will search all Gateway 150 users and locate the closest Gateway 150 user who is watching the live hockey game on channel X, User B. The service provider will have user B's Gateway 150 share the content with user A's Gateway 150.  In the\ncase where user B switches to different channel, the service provider will locate another Gateway 150 to share contents with user A.\nIf gateway 150 has two tuners (gateway 150 may have more than two), then users could record TV programs through local TV tuners, and record a maximum of 2 TV programs including live TV/PPV/VOD at the same time while watching only one of the two\nprograms in a given time.  The users could play recorded content while both tuners are allocated to record by using Recording and Playback at the same time.  Users could record one program and navigate through any other in-progress TV program.  They\ncould switch a watched program from live TV, to pre-recorded programs or to the programs being recorded.\nThe user has two ways to start recording a TV stream, \"Quick\" or \"Programmed\".  For the Quick method, on any in progress content, the user presses Remote Control's Record Button or on any listed content in the EPG, the user highlights the\nprogram and presses the Remote Control's Record Button.  Gateway 150 then starts recording the stream or for a future program sets the program in schedule to be recorded.  When programming gateway 150 to record, the user selects record from an onscreen\nmenu, and then follows the onscreen instructions including the option of selecting a program from the EPG.  Recording options include time, extended time, auto deletion setting (per recording), user restriction for viewing, format/codec for saving\nrecording, and whether it is a recurring recording.\nThe user is able cancel a recording before the recording start or during recording.  In the case of a schedule conflict of recording, priority will be given to user with higher priority set by admin, and for users with the same priority setting;\nthe first to set the recording has priority.  Gateway 150 has an LED or message to indicate a show recording start or in progress.  A warning will be provided if a user would like to switch channels during recording when both tuners are engaged.  In this\ncase, the recording should be stopped first before the channel switch is permitted.  Gateway 150 maintains a personal library of recorded programming, accessed through content grid guide.  Users can access right protection on recorded content, e.g. user\nA can normally view/play user B's recorded contents unless User B restricts it.  Users can also record programs using a gateway 150 to which they are not registered.  The service provider can have a second gateway 150 record a TV stream and send it to\nthe first gateway 150.\nGateway 150 allows the recording of digital content from sources such as digital camcorders to be saved in original HD or standard formats on its hard drive.  These viewing interfaces will be called from each user account.  The recording and\nviewing of such files can be started and stopped at any time based on availability of Gateway 150 resources.\nThe user interface could be a direct interface with gateway 150 or use an Internet browser.  In the case of a direct interface, the user can connect a digital camcorder to gateway 150, an interface appears indicating that a device is connected\nto the gateway 150, the user navigates through and sets parameters and starts recording and gateway 150 software automatically converts the content to a format suitable for replay on gateway 150 if needed.  If using an Internet Browser, the user calls\nthe gateway 150 interface and navigates to an \"Other Recording\" feature, an interface opens asking to connect the camcorder, the user connects the digital camcorder, PC, or Laptop and selects to start recording on interface, and the gateway 150 starts\ntranscoding and transferring the content.  Users should be prompted if hard drive is becoming full.\nGateway 150 can perform video playback, and can play recorded content at the same time both tuners are allocated to record other streams.  Any user can replay saved video contents, subject to permissions regarding the content.  The playback can\nbe started and stopped at any time.\nThe video playback option allows users to access a saved contents list; browse the list and sort the list based on: original recording time; category of content; time of recording and channel.  The user can replay from the last second watched or\nreplay partially watched saved video contents from the moment stopped.\nThe replay can be tagged at certain points by a user.  The replay can be started directly from such tagged points.  The user can access the saved tagged points, browse the points and preview screenshots.  Gateway 150 also allows volume balance\nfor each channel.  The sound volume may be automatically adjusted based on pre-set specifications which may be based on the particular user separately.  Gateway 150 can detect the audio quality and adjust the volume accordingly.\nGateway 150 also allows video streaming to user devices other than a TV, such as laptops, pads and smartphones.  The user of the user device must have permission to view the content.  The video stream may need to be transcoded by gateway 150, in\na manner invisible to the user, based on the user device, taking into account the user devices' physical and performance constraints, such as screen size, CPU, memory size, and supported audio/video codecs.  Gateway 150 transrates the content using a\ndynamic speed control to suit the network bandwidth during streaming.  The user is able to resume playing from a previous stop point.\nGateway 150 saves the data of a TV stream in a cache in its original HD or standard format on hard drive.  It can be accessed by a user at any time.  The user can tag any point of the stream for future reference.  The tagged point will\nautomatically be saved and given a name, and a user may later modify the tags name, start and end point.\nGateway 150 includes multiple interfaces that allow saving digital contents from auxiliary user devices, such as digital camera or iPod (image or MP3 files) in a specific format suitable for replay on gateway 150.  These interfaces will be\ncalled from each user account.  They can be started and stopped at any time based on availability of gateway 150 resources.  They allow users to record content and saving it on the gateway 150 hard drive, create a slideshow or playlist and manage images. Users may also delete saved content from the gateway 150 hard drive dependent on their permission to access the content.\nVideo playback on gateway 150 can be controlled using a TV remote, including adjustments for channel and volume.  Gateway 150 will also be provided with a remote control.\nGateway 150 will provide users with a \"content grid guide\" which is an interface for previewing a TV guide (through the EPG) and the stored content, such as music, video files, and recordings on gateway 150.  The content grid guide will serve as\na TV preview using the EPG provided by the service provider.  It may have various display modes including full screen, quarter, double, and a miniaturized version, such as a strip at the bottom of the screen.  It may be accessed by different user devices\nincluding, computers, TVs, smart phones and pads.  The user may scroll through in a variety of ways, including a fast scroll option (e.g. by day or page).  The user may change preferences, such as color and font, and permissions using parental control\nfeatures.\nThe content grid guide allows users to sort channels by certain criteria, such as channel number, program category and time.  Users can access record and search (by name or category) functions.  Users can also access information about a program\n(or for any file or content being played).  Such information may be available from the EPG, and may also include recording status, remainder (time left in program or file), the rating and an image representing the content.\nGateway 150 may also be used to exert parental control over the ability of children to watch certain content, or to limit the time that they may view certain user devices (for example TV viewing hours may be limited from 7 pm to 9 pm), or the\nmaximum amount of use per day.  Users access a parental control interface and can select permissions based on parameters such as ratings, channels, programs or combinations of these.  Likewise, controls can be established based on time per week or\ncertain time periods.  The parameters can also be based on particular user devices.\nGateway 150 is able to connect to the Internet through Ethernet or Wi-Fi.  User devices can connect to the Internet through Gateway 150 via Wi-Fi.  Gateway 150 thus functions as a web server to provide content (such as streaming videos).\nGateway 150 also allows files to be transferred between registered user devices.  Gateway 150 can receive a file through the Internet, a user device, or another source, such as a USB device.  Once on gateway 150, the content can be transferred,\nused to create a playlist, modified and organized.\nGateway 150 can auto detect devices connected to it, and allow the user to set parameters such as user account, location and content management parameters.  Alternatively, the user can select to import files, and respond to prompts regarding\nconnecting the device, and setting the parameters.  Gateway 150 allows user to access saved content through user devices or directly through gateway 150 interface.\nGateway 150 can also be used for VoIP, either directly or through a user device.  One use of gateway 150 is to allow VoIP to be used though a user device such as a smart phone, through the Internet access via gateway 150 or a PC.\nMobile devices can connect to gateway 150 when they are distant by using a web interface.  Gateway 150 provides the appropriate transcoding and transrating.\nGateway 150 provides several basic services.  For example, gateway 150 is configured to assist users through activation.  Activation may proceed through a web interface, and should follow the service provider's activation flow.  Customer service\nrepresentatives from service provider may have an admin ability to overwrite configurations completed by users.  Each gateway 150 may have an associated \"family\" account with all administrative privileges.\nEach user should create their own account (which may or may not be password protected) using the \"family\" account.  The service provider may set a limit on the number of accounts.  Each user account may be given different privileges, for example\nwhether the account has admin privileges, password change rights, name change rights, or theme change rights.  Each user may have their own profile, e.g. color, desktop, theme, font, font size.  Typically, only an admin account will be able to do device\nregistration.  The \"family\" account can also be used to reset passwords, such as forgotten passwords, possibly including the \"family\" account password.  Restrictions can be placed on accounts using the parental control options.\nUser devices 180 are also registered with gateway 150.  Multiple user devices 180 may be assigned to a user account, and likewise, multiple user accounts may be associated with the same user device 180.  Gateway 150 provides an interface to\nallow a user to connect a user device 180 based on the capabilities and parameters of the device.  The service provider may wish to limit the type or number of user devices 180 that can connect to gateway 150.  If a device 180 is connecting to gateway\n150 via a 3G network, gateway 150 is able to identify the device through the device's EG cookie.\nGateway 150 should authenticate and authorize users and their user devices 180 when connecting to gateway 150.  The user's service subscription must be used to determine the privileges of the user.  The gateway 150 thus uses the user account and\npassword, the user device 180, and the user's subscription.\nGateway 150 performs a TV and video content transcoding role.  The transcoding is done automatically, taking into account the content, the user device 180, the bit rate and location of the user device 180, and other constraints such as memory\navailable, processing power, screen size and audio and video codec available.  The bit rate and transrating shape the video stream.  Transcoding occurs whenever video content is shared or transmitted to user devices 180 via the Ethernet or Bluetooth\nports.  Gateway 150 includes a user interface whereby a user can transcode and save content in different formats; making the content cross-device transferable.  This interface will be called from each user account and can be started and stopped at any\ntime based on availability of Gateway 150 resources.  Once converted the content can be saved on a user device 180 storage or on Gateway 150's hard drive.  The interface generates a list of compatible registered user devices 180 based on the content\nformat selected.\nGateway 150 can thus reformat content, such as that found on the Internet, for different smart phones.  Gateway 150 does this by inspecting the content, downloading it, and if necessary for the user device, converting it to a different format. \nIn an embodiment of gateway 150, when a user downloads content, for example, a video, that the user device cannot play, the user is prompted with a note the video is unplayable on that device, and asking the user if they would like to convert. \nAlternatively, any content reformatting could be done seamlessly, invisible to the user.\nGateway 150 can display Internet content on a TV display and TV content on an Internet display, like a smart phone or laptop screen.  In doing so, gateway 150 receives the content, transcodes it into the most suitable format for the receiving\ndisplay.  In such a manner, gateway 150 can provide a split screen TV display including both TV video and other content.  The other content can include widgets, advertising, a menu, a keyboard, a game or Internet browsing.  A video call, audio call, or\ntext chat interface may be included.  Gateway 150 should support popular chat functions, such as MSN, Google Talk and Skype Chat.  The display options may be limited and determined by the service provider, or by the user.  Gateway 150 provides an\ninterface whereby a user may split the TV display screen, for example by half, quarters (with each quarter displaying different content), or other combinations.\nA user can use the user device 180 displaying the TV content, for example a laptop, iPad or smartphone, to control the TV, for example change channels, pause the program, etc.\nGateway 150 may be configured to have interfaces with popular social networking or file sharing sites, such as Facebook and LinkedIn.  Transcoding may be used to make these sites usable on user devices such as TV displays.  User particulars,\nsuch as user names and passwords may be stored on Gateway 150.\nGateway 150 should be able to assist content provider in digital rights management.  In an embodiment of the invention in which client software is on the user device 180, the user device 180, such as a smart phone can, for example, remove\ncontent as it is played in the case of PVRed or VOD content.  The policy of DRM is an additional layer over the top of all engines which decides on the sharing mechanism of digitally protected content.\nGateway 150 should offer several security features for box access, communication of content amongst users, user access to data, intrusion detection, authenticating users and user devices and encryption/decryption of video streams.\nGateway 150 includes a graphical user interface (GUI) to interact with users and perform certain functions.  Users may use the GUI to record TV content.  The GUI allows scheduling future recording of TV content as single program or a TV series. \nIt can be set by any user can be called at any time.  A user may use the GUI to access a TV listing, and modify record settings such as start time, stop time, new or repeating episodes on different channels, time of broadcasting, and deletion conditions.\nThe GUI can be used to review the scheduled recording list.  Users can access the schedule and make modifications.  The list can be sorted based on user, date and time, category of program and channel.  The GUI can also be used to access and\ninput items into a calendar with a reminder system (which may be sync to calendar systems on user's smart phones or PCs).  Users can use the GUI to review the scheduled reminder list, access the schedule, and modify, browse and sort the schedule based on\nuser, date and time, category and channel.\nThe GUI can also be used to search content stored on Gateway 150 or user devices.  Such a search can be done using different filters.  An on screen keyboard may be used for this purpose.  The results of the search may be sorted by user, type of\ncontent, category of content, channel, recorded or future status, PPV or source.  Trailers or previews of the content may also be provided.\nEach user may have certain favourite channels associated, which can be based on particular channels or categories of channels.  This can be sued to assign characteristics to the user.  The channels may be sorted by user, type of content,\ncategory, channel, PPV status, on demand or downloadable status.\nRemote Management\nGateway 150 may be managed remotely by the service provider.  Such management may include monitoring and diagnostic functions, trouble management, performance management, and configuration and customer support.\nThe remote monitoring and diagnostics module is responsible for monitoring the health of gateway 150.  It includes low level debugging tools that monitor errors and warnings reported by the various modules and the resource usage of each module\naccording to its upper bound limit as set by configuration.  The module operates in active mode, in which it automatically reports on errors; or a request/response mode, in which it responds to command generated by a remote technical support team.  The\nrequest/response could be a request for gateway 150 to run a test script.  The configuration file for this module could be updated remotely, as could the module and its parameters.\nThe remote trouble management module can provide full control of gateway 150 to the service provider's technical support team.  The module enables technical support to control all levels, allowing the support team to run procedures, look up\nregistries, communicate directly with any hardware and software components, measure throughput, signal quality, etc., check the communication between hardware components and software, isolate the cause of problems, apply updates and software patches, and\nenable or disable components.\nThe remote performance management module tests the performance of the connectivity to gateway 150 and within gateway 150, which includes timing: per transport layer protocol; per application layer protocol; per transaction; per session and per\nnumber of concurrent sessions and transactions.  The module also tests packet loss, throughput, error rate and delay, and connectivity to user devices registered to Gateway 150.\nThe remote customer support module provides high level customer support, including allowing users to contact customer support.  It may include demos and some remote management functions, such as activation, authorization, setup, configuration,\nand software updates.\nGateway 150 can support multiplexing streaming video to add layers and images to video content.  For example gateway 150 can add logos to TV content.  Gateway 150 also supports closed captioning, including the language, font size, and colours\nused with captions.\nApplication Manager\nGateway 150 has an application manager to manage the state transition among the applications operating within gateway 150.  The application manager processes a user's request and delivers the request quickly to a desired application service or\nmodule.  The application manager receives user request from a native key event manager, and dispatches the request to application services.  The application manager resides between application modules and middleware inside Gateway 150 system.\nThe application manager, when the power is on, leads the system into a \"main\" service in the factory setting mode of gateway 150.  The main service waits the user's choice, and notifies the application manager to launch the selected application\nservice.  The application manager, when the power begins as \"off\", leads the system to enter the previous application service which is \"remembered\" in the last power off state.  The application manager, when the power is turned \"off\", notifies the\napplication service shut down and brings the time update automatically in front panel.\nArchitecture\nA representative architecture for gateway 150 is now described.  Gateway 150, as previously described includes hardware, middleware and services.  The software in gateway 150 is based on modules and engines characterized by their main\nfunctionalities, and therefore the modules in each group or category have the same basic functionalities and can be distinguished from each other by specific functions.  Modules are grouped into the following categories:\nEssentials: The main container will not be loaded without the essential modules.\nActors: The service engines that need to be running and providing an automatic series of functionalities.\nReactors: The modules that act according to reflexes (invoked and/or behave differently as per request and types of others).\nAn important aspect of the design is that all modules support protocol communication interface through a message structure.  This isolates the dependency of the modules from each other to the extent that each module could even be developed in a\ndifferent language and each module is a sub system of a larger system.  The output of one module could be the input of another module and the output of the system is the result of the collaborations among all modules.\nThe Main Container is a small module that reads a configuration file and then, based on the information presented in the file, loads the modules.  The path to the configuration file is an input parameter to the main container.  The path could be\nan address to a file and/or a database which has either resides locally on gateway 150 and/or remotely within a data centre.  Therefore the Main Container must have the communication capability of connecting to a database or file, and receive command\nlines as input for mandatory functionalities and as well as the ability to read a path for loading modules.\nActors are the modules that actively and automatically act on a task.  Actors will load and run in the background at all the time.  For example, a watcher is an actor the loads at runtime and is invoked periodically to check the status and\nhealth of the system.  Actors have the same behavior without any dependency in change of state.  For example, a deep content inspection module is always scanning the content of a stream no matter what type of network, device or content is involved.\nReactors are the modules that change their behaviour based on the state change.  Reactors can be considered as intelligent modules where the module learns through the time and behaves differently based on the state.  For example, the\nPersonalization Engine (PE) module could be considered as a Reactor.  The PE Module learns from the user's behavior under different conditions and based on the collected knowledge, provides the service differently i.e. looking for different content to\npresent and push to the user based on knowing the network, device, and state the user is.  The collection of the affirmative information forms a state that causes the PE Module to move or behave differently and produce a different result.  The video rate\nshaping module is another example of such a reactor, wherein summation of users' state information, including type of device, connectivity type and quality, state in terms of mobile or stationary, at home or distant location, and type of content and the\nviewing capabilities under the state condition, would result in a rate shaping algorithm which could be completely different under different conditions (for example when only one parameter is different).  The design of reactors is based on separation of\nlogical layer from data layer.\nActor or Reactor defines the main framework of each module, whether the framework is an algorithm that changes based on input event and parameters, or is a static framework.  The modules also differ on basic functionalities for each module of\nthe defined type.  Therefore, the modules can be further grouped into the following categories:\n1.  Content Inspectors, including all client portals: the entry point of the client user device, such as game consoles, OS-based mobile devices, non OS mobile devices, laptops, TVs, remote controls.  This also includes deep content inspection\nmodules for both textual content and non-textual content (graphic and video).\n2.  Content Consumers: are the modules that process and analyze the content/data collected and consume it for other purpose and tasks.  These include the characterization module which receives the data collected from the inspectors and analyzes\nit in order to characterize the user.  Note that this module is of Reactor framework type.  It also includes the personalization module wherein different menu options will be loaded based on user preference, context (state) and the output from\ncharacterization module.  This module could also be the module that draws the page on the display based on user characterization output.  For example while one user may like to have the screen layout and information organization different from one user\nto another, or even from one user's device (e.g. Mobile) to another (e.g. laptop).  All storage, backup modules are also consumers; they are also of type Reactors, e.g. based on type of content, and user's status where to backup and how often to backup\nvary from one condition to another.\n3.  Content Producers: are the modules that produce the content for the user.  These include the intelligent search engine which searches for content on behalf of the user based on the output of characterization module.  Note that this module is\nof type Actor since its behavior/task (searching) doesn't change while the input (type of content to search for) is changing.  It also includes the multiplexer which is also a content producer, since it receives, for example, a video (TV stream) and the\nadvertisement and produces a new mixed content.  Note that this architecture allow gateway 150 to have a dedicated multiplexer for each content format; e.g. 3GPP or MPEG4, etc. which makes it of type Actor, or alternatively one multiplexer handles all\ntypes and changes its algorithm based on the request which makes it of type Reactor.\n4.  Data Communicators: are modules that are responsible for networking related activities and include determining the fastest/shortest path to available content; the fastest/shortest path to the user device; video rate shaping and others.\nAs shown in FIG. 14, all user devices, when accessing gateway 150, pass through an access portal 1400, which act as a proxy or gateway.  These access portals pass the identity of the device (and user if it is shared) to the AAA module 1410,\ncollect user data to send to the data collection module, access the resource through the resource management modules.  Examples of these access portals 1400 include the Web Portal 1420, the WAP Portal 1430, the remote control portal 1440, the SMS portal\n1450, and the game console portal 1460.\nThe Remote control portal 1440 has two modes, basic and advanced.  The basic mode allows turning on gateway 150 with a remote control and ignores the password notification.  A default account is selected for the basic mode for the AAA module\n1410.  In the advanced mode, the user inputs 2-4 digit number as a password when prompted, allowing the gateway 150 to identify the user.  Alternatively, gateway 150 waits until the user selects a certain option, for example favourite channels or web\nsites, at which point it prompts the user to provide the password.\nThe Access Control layer includes the AAA modules in charge of user activation, identification and authorization.  Each user device needs activation through AAA module 1410 to start to use the gateway 150, and during this process, the AAA module\n1410 will record (and update if possible) the configuration of the user device.  For shared devices, the module will activate the user account.\nAfter activation, each time a user device or a user account starts using gateway 150 via an access portal 1400, the portal will pass the user device (or user account) to the AAA module 1410 for authorization.  The AAA module 1410 will identify\neach user device and user, and through a content filter system, search the local content container for suitable content, and pass it to content blender 1470.\nThus the access control layer 1400 is responsible for: keeping all user profiles and box defaults and updated data centrally; communicating with the data center database to get, update, backup, and restore user device data; providing\nauthentication interface for the Portal layer; providing user profile interface for other modules; and providing an interface for User UI for any parameter that can be updated by the user, such as parental control; providing interface to Configuration\nand Manage net program; and providing interface to data center to activate gateway 150.\nThe Data Inspection/Collection modules 1480 act as the interface between data collection points (the portals 1400 and the AAA modules 1410) and Data storage (files and local database).\nThe content blender module 1470 has two major functions, to blend the video content for TV streams, recorded TV and other resources, and to transcode Internet content (typically web pages) for adapting to user device screen size and webpage\nreformatting to add more content.\nAn example of the work flow process from a user device connecting to the Internet via gateway 150 and is communication with gateway 150 via a Wi-Fi or LAN interface follows.  First, the gateway 150 WEP portal 1430 intercepts the Internet\nrequest, abstracts the user device's IP/MAC information, and interacts with the AAA module 1410 for authorization.  The AAA modules 1410 passes the user device ID (and account ID if available) to the content blender module 1470, and checks if available\n(matched) content is in local Add-on container 1490.\nIf the content blender 1470 finds the matched add-on in Add-on container 1490, it links the add-on for later blundering usage when it receives the feedback from interface agents.  The data inspection/collection module 1480 starts to collect the\nuser data along with the device ID (and account ID) passed by the AAA module 1470.  The content blender module 1470 transcodes the feedback from the resource/request with the add-on together, sends it back to portal module.  The data collection module\nalso gathers the information from feedback data.\nThe SMS portal 1450 may be a set of pre-defined SMS commands, and also uses the AAA module 1410 to authenticate the sender authority, and to decide to allow or deny the commands send from the user device.  After a command passes the AAA\nverification, the control and management module 1500 parses the command and processes it.  Depending on the command type, the control/management module 1500 may determine if it will send the feedback/confirm back to SMS sending user device.\nFIG. 15 shows an embodiment of a high level explanation about data communication inside gateway 150 ND between Gateway 150 and the service provider Data Center 1510.\nSoftware Architecture\nAn alternative embodiment of the software architecture for gateway 150 is now disclosed.  The Gateway software architecture includes the following components: Application layer; Access control layer; Services Layer; Hardware abstraction layer;\nManagement interfaces; and Database interface.\nFIG. 16 shows an embodiment of the high-level software architecture of Gateway 150.  As shown, function modules of Gateway 150 include transcoding 1610, multiplexing 1620, proximity based content inspection 1630, presentation and streaming\nmodules 1640.\nAn embodiment of the basic workflow of gateway 150 is shown in FIG. 17.  As shown, the first step taken is user authentication through permission control 1710, and the following depends on the use cases.  The user can call an application to send\nout a request to certain services, which includes answering \"Who I am\" (user and device identity), \"Where I am\" (geographic location) and \"What I want\" (service information).  The access control activates a portal guard and queries the database for\naccount information.  If the request is allowed, the request is passed on to the related service.  Once the service processes the request, it also logs the activity and the result to the database.\nThe application layer 1720 includes application catalogued into four different groups (based on the access portal used).  The first group are web-based applications, which refer to the Webpages which run on top of the Gateway 150 web server. \nAny web enabled user device could open these Webpages, and thereby access the Gateway 150.  The webpages are of two types; management pages which are used to configure gateway 150 and users thereof; and application webpages with provide web access for\nfunctions such as live streaming TV, EPG information, recording programs, and personal content management, such as image and music files.\nThe second group are local applications, which include management applications, which as described above, can be used to configure gateway 150 or the users thereof, and function applications, such as media players, media guides, and photo\nalbums.  The GUI interface with users is dedicated to providing use of such modules.\nThe third group are carrier applications, which is the group of services, web pages and applications which favour the service provider requirements.  Gateway 150's carrier applications provide control and information collection interfaces for\nthe service provider.  Control interfaces in this group include gateway 150 activation; gateway 150 configuration; services configuration; and advertisement content pushing.  Information collection interfaces in this group include gateway 150 activity\nreporting; user characterization reporting; and content inspection reporting.  These reports include proximity information.\nThe fourth group is third party applications which are the applications not developed and controlled by the gateway service provider or manufacturer but follow the same interface as above application.  These applications could be developed by\nany third party company or developer following the Gateway 150 SDK.  Examples include Skype, and Google Widgets.\nThe access control layer acts as a portal for all the service requests.  This layer defines a service framework and is implemented by two libraries, libdsService and libdsLog.  libdsService defines the service framework.  The framework\nimplements a skeleton of CLI and the access control interface.  libdsLog defines a post instrumentation interface for all the services.\nThe Service layer 1730 includes all the core services Gateway 150 provides.  Each service is a slave of the control manager.  In other words, the service can only be accessed by an application and by other services through the control manager\nusing the access control interface.  The reason behind this approach is to hide and centralize all the complex service-interaction logic in the control manager.\nFor example, an application could dispatch a request to the control manager to start recording a live TV channel.  Behind the scenes, the control manager would communicates with the user manager service to see if the application has the needed\npermissions, and then setup the recording session with the recording service.  As soon as the schedule arrives, the recording service could inform the control manager.  The control manager then can inform the replication service to start delivering live\nstreaming to the recording service.  FIG. 18 shows an overview of gateway 150 services, and the interaction with other applications.\nThe gateway 150 Control Manager 1800 exposes a list of important Gateway 150 functions, and delegates to one or more Gateway 150 services to complete a request.  It also serializes and prioritizes requests, and provides event subscription\nmechanisms to other Gateway 150 services.  Control manager 1800's duty is to manage complex interactions of services to carry out each gateway 150 function request.  Therefore, the Gateway 150 control manager provides the following functions: a facade of\nGateway 150 services; serialization and prioritization of requests, and event subscriptions.\nThe UI Service 1810 acts as the presentation interface of gateway 150.  This service contains an application audio control 1830.  The application audio control 1830 is a volume mixer that can perform volume control on all the applications used\nby the UI Service 1810.  The UI Service 1810 also interacts and manages layouts with the following major applications: the Media Guide; 1820 the Setup and Activation Manager; the Personal Content Manager; the Photo Album; Recorded Content; the Media\nPlayer 1840; Personalized Advertisements; Channel-based Chat; Skype 1850; Google Widgets 1860; and the IP Cam Viewer.\nThe iAccess service 1870 is a web service that exposes a list of UI features to remote user devices such as iPhone, laptop, etc. This service is similar to a PHP web service.  The service features include: serving over HTTP using PHP; converting\nweb requests into equivalent DS messages to invoke action supported by the Gateway 150 control manager 1800; dynamically creating thumbnails for personal images; and automatically providing different layout based on the end-user device.\nThe RF receiver module 1880 allows a user to use the remote control.  To be more specific, this service receives RF signals, then translates the signals into Gateway 150 remote control command, and notifies the control manager 1800.  The service\nlistens for RF command; and converts the RF command into equivalent DS message to invoke action supported by the Gateway 150 control manager 1800.\nThe Stream Replication Service 1890 retrieves remote media streams and then replicates them to the recording service 1815, rStreamer 1825, media player, and/or EPG guide service 1835.  An IP cam module 1845 is also available to retrieve IP\ncamera streams.  The service demuxes and muxes MPEGTS containers; demuxes DVB Electronic Program Guide from MPEGTS stream; receives multicast, and RTSP; and replicates MPEGTS packet via RTP.\nThe rStreamer service 1825 mainly runs in the background as a streaming server 1900 to output media content into the required format.  As shown in FIG. 19, the rStreamer includes two entities: the trans-coding/muxing module 1910, and a streaming\nserver 1900.  The trans-coding component 1910 is able to trans-code a video stream into H.264 baseline profile and mux logos and advertisements; trans-code an audio stream into AAC LC; mux video streams and audio into a MPEGTS container; and deliver to\nstreaming server 1900 using RTP.\nThe streaming server component 1900 supports the streaming protocol, including Flash RTMP (to support all the desktop/laptop computers); Apple HTTP Live Streaming, (to support all iOS user devices); Microsoft Smooth Streaming (to support all\nWindows 7 Phones); and RTSP/RTP interleaved (to support all the RIM (Blackberry) devices) and, TV tuner sharing.  The server 1900 services all the required streaming protocol over 1 single port.  The streaming server 1900 receives the MPEGTS RTP stream. \nThe video coder may be H.264 and the audio coder may be AAC.  The streaming server interacts with the User Manager 1920 to provide access control.\nThe Recording service 1815 works with the replication service 1890 to record live TV programs to the gateway 150 hard disk.  The Recording service 1815 also has a scheduler that tracks all recording sessions.  Therefore the recording service\nreceives MPEGTS RTP (the video may be encoded with H.264 and audio with AAC); tracks all recording schedules; and updates and reflects changes in the database.\nGateway 150 Content inspection function provides the \"be aware\" ability to Gateway 150, and allows Gateway 150 be a \"smart box\" and understand what is going on inside.  It detects information and uses it to further characterization data to\ncreate value-add service for the service provider.  The structure flow chart of content inspection is shown in FIG. 20.\nTwo types of data are generally handled by gateway 150, video and internet.  A different interface is used for each type to implement basic capture and store functions.  The result analyzing and audit function can be implemented in offline mode\non either client side or server side.  The content inspection provides the data for the multiplex to achieve accurate advertisement push.\nThe EPG service 1835 receives a DVB electronic guide from the Stream Replication Service 1890 via RTP.  Then it updates the local cached EPG list, and notifies if there is a change in the guide.  The EPG service 1835 thus receives MPEGTS RTP\nstream with DVB electronic program guide; processes DVB electronic program guide; and updates the locally cached EPG list.\nThe Media Share Service 1855 is responsible for searching and accessing multimedia contents shared in a home network.  The Media Share Service thus supports DLNA; supports CIFS; and mounts all supported network file sharing protocols.\nThe User Manager service 1865 provides product feature control, account management, and parental control as well as gateway 150 user management.  The product feature control ensures certain functionalities of gateway 150 are disabled or enabled\nbased on the accounting information of the customer.  Account management lets the customer review the services currently subscribed to, and order or cancel services.\nThe Advertisement Service 1875 provides logos and advertisements by communicating with an external advertisement server.  The actual delivery protocol and specification of the format of the advertisement contents are defined in advance.\nThe DS Monitor 1885 enables post instrumentation and is based on three modules--monitoring, reporting and updating.\nThe Domain Service 1895 helps exposing gateway 150 to the public internet from a home network.\nA Wi-Fi AP Service 1896 controls the built-in Gateway 150 wireless access point.  Once enabled, this service will be responsible for providing DHCP service; detecting new user devices; and managing firewall and application port-forwarding.\nA Channel-based Chat service manages and services channel based chats.  A channel-based chat is a service that allows the end user to automatically join a different social chat room based on the current channel selected.\nThe Proximity P4P service 1897, like content inspection, will mainly be used by the service provider to provide network-optimized content delivery.\nAn Application Loader Service allows third parties to develop applications that make use of Gateway 150 services.  This service thus can load and unload a signed Java application package JAR, and enforce rules; and the service contains a Java\nAPI that exposes Gateway 150 services.\nGateway 150 can use previously determined location information to optimize content delivery.  If more than more node trying to access the same content, the content deliver path could be optimized for the best usage of network.  The services\ninvolved include the content management and location management services, and each gateway 150 acts as a network node of this matrix with a content management server on the service provider side.\nThe workflow includes the gateway 150 sending out the content request with the location information to the content server in the service provider data center.  The content management services will check the current mapping table, select the most\n\"near\" (network topology) node (which already contain the content) and sends a redirect for the request to the select node.  The proximity based P4P service could become a cloud computing platform.\nThe hardware abstraction layer (HAL) allows for hardware independent software.  This layer requires the whole software to be aware of the porting capacity.  Software modules that interact with HAL include: USB Keyboard Driver; Audio Mixer; Video\ndemux; Video decoder/encoder; Audio decoder/encoder; and TV tuner interface.\nThe Database Layer includes two database interfaces, one inside the Gateway 150 locally and the other in the service provider data center.  The local database stores user account information, content information, inspection data and audit\ninformation as well as providing data access interface through an API.  The service provider manages the Gateway 150 and collects data related to Gateway 150, such as default data, characterization data, user backup data and so on.  The Gateway 150\nmanagement client also communicates with third party Management systems to retrieve customer account information.\nProduct Description\nGateway 150 does not require installation of a client application on any of the user devices.  All traffic is handled by Gateway 150 through an AI (Artificial Intelligent) engine that can distinguish and manage each user's traffic, under varying\nconnectivity situations (device, network type, location, etc.).  The approach distributes the CPU and memory requirements for millions of users out to the subscribers' premises, similar to a subnet, enabling faster and more accurate data processing. \nThis unique capability enables Gateway 150 to provide the following features.\nTherefore, there is no need for large advertisement insertion servers; Gateway 150 directly connects to the advertisement content and brings the content to the content being watched.  It can simply be an unmanaged content overlaid from any\nsource the user was actually browsing when s/he was on the Internet.  An advertisement for one user within a premise could be completely different from another user within the same premise, each based on the Internet content they are viewing.\nAnother feature is that all types of usage traffic can be reported to the operator for billing purposes.  Operators will be able to bill users and content vendors.  This includes generating advertisement revenue from managed or unmanaged sources\nor generally speaking from any content vendor that the user is subscribed to.\nSharing content among Gateway 150(s) can be either a known feature for the users or a transparent feature focusing on delivering benefits to the operators.\nUser's Content Sharing:\nGateway 150 can provide user owned storage and allow users to share their content between users, either within or outside the premise and between friends.  It must be noted that either are possible and configurable based on a service provider's\npolicy model.\nOperator's Content Sharing:\nGateway 150 enables a service provider to reduce the traffic over their core-network and enables them to use gateway 150 as distributed CPU and storage.  An example of such functionality is when two users are watching the same VoD.  One of the\nusers could get the VoD from the other gateway 150 instead of accessing the service provider server in a regional office or deep core-network.  Another example of such functionality could turn gateway 150 into distributed nodes with a massive number of\nTV tuners for recording different programs that run simultaneously.  Two users, each with a single TV tuner gateway 150 that have access to same channels, can share the program that each has recorded (be peer-to-peer content streaming).  Note that the\npolicy of access, digital right management and billing for all features always taken into consideration.\nControl Manager\nThe control manager is responsible for registration of user devices 180 with gateway 150, including information about the capabilities and limitations of such user device 180 and the users with whom the user device 180 is associated.\nThe control manager maintains usage logs for each user device.  These logs can be used to improve Quality of Service (QoS) by determining when errors or poor performance occurs and taking steps to improve the quality (for example by increasing\nthe buffer for streaming video).\nThe control manager acts as a message broker between user devices when one is used to interact with or access content on the other.\nThe control manager also manages the TV tuners included as part of the box.  Such tuners, and associated EPG information, are set to the appropriate channel and output to the appropriate user device by sending a request to the streaming server.\nAll traffic is being handled by Gateway 150 through an AI (Artificial Intelligent) engine that can distinguish and manage each user's traffic, under varying connectivity situations (device, network type, location, etc.).  The approach\ndistributes the CPU and memory requirements for millions of users out to the subscribers' premises, similar to a subnet, enabling faster and more accurate data processing.\nStreaming Server\nThe streaming server handles several duties.  It opens the port to the TV tuner, and handles the Electronic Program Guide (EPG), and modifying the format to the type of user device requesting the signal.\nThe streaming server also sets and monitors the quality of signal based on the size of the display.  The streaming serve controls the size of the buffer needed based on the limitations of the device, its processor, the size of the display and\nthe signal.  Past user experiences with the device may be taken into account.\nThe streaming server thus can adapt to learned limitations of a user device to improve experience and can handle resolution changes and needs\nWhen a user device is receiving Internet content, the streaming server can send a link to the user device to the content and play no other role.  Alternatively, if the user device is local to the streaming server, the gateway can receive the\ncontent and adapt it for the user device.\nThe streaming server uses a transcoder to convert content from one format to another.  The transcoder provides both a coding and decoding process.  It receives input, decodes it, and codes it again as needed by the user device.\nDistant User Devices\nOne use of gateway 150 is to take advantage of the connection between the user device and gateway 150, which occurs even if the user device is distant from the gateway (e.g. outside of Wi-Fi range).  This allows a user device to receive\nadvertisements based on the user's past experiences, like browsing history on other user devices and personal interests, not available when logging in directly to a distant network.\nIn this use of gateway 150, a user at a remote location uses a user device, such as a smart phone, to access the Internet from a location far from the gateway 150.  By accessing gateway 150, advertisements presented to the user on the user\ndevice may be directly target based on the UCC-E and CP-E.\nBit Rate\nThe gateway 150 streaming server approach uses a single transcoder to provide multi-bitrate adaptive streaming for a single user, as opposed to a separate transcoder for each bitrate that is provided to the user.\nOur streaming server advertises multiple bitrates to a client device, and then begins transcoding at the first requested bitrate.  As the client device requests different bitrates (based on network conditions and the client's hardware\ncapability), the server reads these requests and seamlessly adapts its transcoder parameters to accommodate the client's request.  A single transcoder has its parameters tuned for a single client's requests, so a single transcoder can only service a\nsingle client.\nThis allows gateway 150 to scales well with the number of bitrates provided (as only one transcoder is ever required for a single user).  This approach does not scale well with the number of users requesting streams (each user requires one\ntranscoder), so this approach is likely not preferable if the number of users with access to the server is larger than the number of transcoders available.  However, as the number of expected users of gateway 150 is relatively low (family sized), the\nexpected range of bitrates provided will have a larger impact on gateway 150 then the number of users.\nMulti-Tasking on User Devices\nGateway 150 allows users to use their TV display for a many functions simultaneously.  For example, a user can view a movie, make an Internet call (Skype) with a friend, and share a video or audio file all at the same time on the same device.\nGateway 150 accomplishes this by splitting the TV display into different zones, and displaying different content, transcoded appropriately, in each zone.  Each zone can individually access a menu and receive commands, and can display content\nfetched, including content from Internet, and can enable users to access other viewers' feedback and ratings regarding any targeted or in progress content in an assigned zone.\nUser Information & Communication with Service Provider Server\nGateway 150 frequently communicates with service provider servers for a number of reasons.  For examples when a new gateway 150 is purchased and configured, the gateway 150 registers with the service provider server and becomes associated with a\ncustomer, as well as the registered users and user devices.  When changes are made, for examples, new users or user devices registered, the service provider server is updated accordingly.\nGateway 150 functions as a dynamic DNS server by handling changes to the IP addresses associated with gateway 150, as assigned by the ISP (which may be the service provider).\nGateway 150 can function as a feedback device for the service provider.  Questions and surveys can be directed to registered users, and provided by gateway 150 to the service provider with appropriate demographic information about the responding\nuser.\nThis feedback could be used to instantly obtain user opinions on content, such as TV pilot episodes, scheduling, or services.  For example, a TV pilot could be aired, and followed immediately with a few questions asking user who viewed the pilot\nwhether they would follow the series, perhaps even with questions related to time slot, plots, and characters.\nThe feedback could also be used to determine TV ratings.  Gateway 150 already knows at least one registered user is watching a particular program at a particular time.  A question as to identifying any other users watching is all that is\nrequired to allow gateway 150 to provide information for inclusion in TV ratings, such as those provided by Nielsen.  If a sufficient number of gateway 150s are in use, it can be a simple function to determine very accurate viewership of a particular\nprogram, along with information about PVRing of the program.\nOther feedback could be used to inquire about the success of advertisements (e.g. is a trailer more likely to draw someone to a movie; or is a political advertisement likely to sway a voter).\nGateway 150 can also be used for policy management, particularly as the files are stored on gateway 150, and the content can be policed by the service provider, for example by deleting PPV files, once watched.\nThe parental control system is quite flexible.  If the video or audio content, either from Internet or the TV, is encoded to include rating information by scene, then gateway 150 can modify the content accordingly.  In this embodiment, for\nexample, if the parental control indicates that certain language is not to be output, then gateway 150 can simply leave out such audio according to the audio stream information.  Similar systems could be used for sexual or violent content, as gateway 150\ncould simply excise portions of the video stream.\nAlternatively if a database about the times within a video or audio stream when potentially objectionable content takes place is available at service provider servers, the content could be excised based on the time played of the video stream.\nService provider can also monitor gateway 150 by logging into gateway 150 remotely, accessing logs and hard drives, etc. The service provider can even take control of gateway 150 if necessary.\nGateway 150 provides users with means for social networking.  Users will have access to popular social networking services, such as Facebook and LinkedIn, but will also have access to a community of gateway 150 users.  Users of gateway 50 can\nmark content stored on gateway 150 as public or private.  Public content may be made available to users of other gateway 150s, possibly limited to \"friends\" of the user with the content.  Private content will not be so accessible.  Public content can be\nstreamed to other gateway 150s, even when the user is performing a different task, for example watching TV.  Both the sending user and receiving user may be watching the same TV program simultaneously while engaging in VoIP discussion about same and\nexchanging a file.\nGateway 150 can also be used to target advertising specifically to user activities.  For example, when a user is watching TV content, for example a music video, if an Internet viewing screen is enabled; they could be presented with an\nadvertisement for purchase of that same, or very similar, music content.  Likewise, when a user is watching a TV program, they could be presented with an advertisement for the DVD of the previous season of that same program.  When selecting appropriate\nadvertising for users, the service provider will also have the information from gateway 150 about the user's interests, demographics, and habits.\nFor the sake of convenience, the embodiments above are described as various interconnected functional blocks or distinct software modules.  This is not necessary, however, and there may be cases where these functional blocks or modules are\nequivalently aggregated into a single logic device, program or operation with unclear boundaries.  In any event, the functional blocks and software modules or features of the flexible interface can be implemented by themselves, or in combination with\nother operations in either hardware or software.\nWhile particular embodiments have been described in the foregoing, it is to be understood that other embodiments are possible and are intended to be included herein.  It will be clear to any person skilled in the art that modifications of and\nadjustments to the foregoing embodiments, not shown, are possible.", "application_number": "14389601", "abstract": " A gateway for location at a user premises is provided. Bother users and\n     user devices are registered with the gateway which is provided by a\n     service provider. Users access services, such as video streaming, on\n     their user devices via the gateway. The gateway allows users to share\n     content. The gateway collects information about the user's patterns of\n     behavior for the system provider, and can be used to directly obtain\n     feedback from the users.\n", "citations": ["4566030", "8291453", "8858313", "20020184629", "20040070593", "20050021387", "20050157217", "20060074708", "20060253330", "20070088553", "20070244752", "20080235746", "20080279216", "20100031162", "20100138868", "20110106750", "20110197240", "20110282749", "20120011529", "20120023518", "20130024293", "20130073387", "20130073400", "20130073473", "20130159499", "20140236732", "20140237053", "20150180747", "20160029155", "20170201779"], "related": []}, {"id": "20150365448", "patent_code": "10375129", "patent_name": "Facilitating conversations with automated location mapping", "year": "2019", "inventor_and_country_data": " Inventors: \nStifelman; Lisa (Palo Alto, CA), Chinthakunta; Madhusudan (Saratoga, CA), Odell; Julian James (Kirkland, WA), Heck; Larry Paul (Los Altos, CA), Dole; Daniel (Seattle, WA)  ", "description": "<BR><BR>BACKGROUND\nWithin the field of computing, many scenarios involve a conversation among two or more individuals about one or more topics that are associated with a location, such as plans to meet at a restaurant for dinner or to attend a film at a theater. \nIn such scenarios, an individual who is also using a device may alternate between conversing with one another and with interacting with a device to identify, explore, and select the location-based topics.  Devices may provide a variety of tools that may\nfacilitate such conversations, such as applications for identifying or recommending topics based on a set of search criteria, such as an events guide application or a restaurant recommendations application; applications for viewing details about a topic,\nsuch as viewing restaurant menus and availability for reservations; applications for initiating transactions with a location-based topic, such as making a reservation at a restaurant or purchasing tickets to a theater or music concert; applications for\nidentifying discounts on such and applications for logging the presence of the user at the location of a topic, such as a \"check-in\" application enabling a user to inform social contacts of their presence at a particular event or location; applications\nfor finding discounts on topics, such as discounted theater tickets and restaurant coupons; and applications for presenting maps and identifying routes to reach the locations of the respective topics.\n<BR><BR>SUMMARY\nThis Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description.  This Summary is not intended to identify key factors or essential features of the claimed subject\nmatter, nor is it intended to be used to limit the scope of the claimed subject matter.\nMany devices may provide a variety of tools that may facilitate users in conversations involving location-based topics.  However, in order to use these tools, a user may have to direct his or her attention, alternatively, to the other\nindividuals involved in the conversation and to directing commands at the device to invoke such tools.  For example, a set of users may agree to have dinner together; request a device to provide a list of nearby restaurants; discuss the options with each\nother; request the device to provide more information about a restaurant, such as a menu or a location; discuss and agree on a selected restaurant; and then request the device to plot a route from a current location to the location of the selected\nrestaurant.  In this manner, the users may utilize their devices to facilitate the conversation of location-based topics.\nWhile the tools provided by such devices may significantly facilitate the conversation, it may be appreciated that the alternatively directed attention of the users may also impair the conversation.  For example, a user who is interacting with a\ndevice to retrieve such information may miss part of the conversation.  Additionally, sharing such information among users may be difficult; e.g., a first user may use a recommendations application to search for location-based topics of interest, but may\nhave to relate the identified options to a second individual through the conversation, rather than enabling the second individual to view the same options as presented to the user of the device.\nPresented herein are techniques for configuring devices to facilitate conversations about location-based topics.  In an embodiment of these techniques, a device may be configured to monitor a conversation among at least two individuals to detect\ntopics referenced by an individual.  Upon detecting a topic referenced by a first individual to a second individual of the conversation, where the topic is associated with a location and is not referenced as a command to the device, the device may\nidentify the location of the topic, and present on the display a map indicating the location of the topic.  In particular, the map may be presented alongside the conversation, such as adjacent to a text-based conversation (e.g., a textual conversation, a\nverbal conversation that is automatically translated into text, or a video conversation where speech and/or nonverbal communication are automatically translated into text), a map may be presented that automatically displays the locations of topics\nreferenced within the conversation by the individuals.  In further embodiments, a device may maintain a list of the topics referenced in the conversation; may automatically suggest topics for the conversation (e.g., presenting a set of recommendations\nfor restaurants that are compatible with the dietary preferences specified in the social profiles of the individuals, and that are near the current locations of the individuals); may allow individuals to alter the list; and may synchronize the\npresentation of the list with a second device of a second individual, thereby enabling the individuals to review the options together.  These and other variations in the presentation of location-based topics in furtherance of a conversation among the\nindividuals may be achievable in accordance with the techniques presented herein.\nTo the accomplishment of the foregoing and related ends, the following description and annexed drawings set forth certain illustrative aspects and implementations.  These are indicative of but a few of the various ways in which one or more\naspects may be employed.  Other aspects, advantages, and novel features of the disclosure will become apparent from the following detailed description when considered in conjunction with the annexed drawings. <BR><BR>DESCRIPTION OF THE DRAWINGS\nFIG. 1 is an illustration of an exemplary scenario featuring a conversation among two individuals about a set of location-based topics.\nFIG. 2 is an illustration of an exemplary scenario featuring the facilitation of a conversation among two individuals about a set of location-based topics in accordance with the techniques presented herein.\nFIG. 3 is an illustration of an exemplary method of facilitating a conversation among at least two users about a topic associated with a location in accordance with the techniques presented herein.\nFIG. 4 is a component block diagram illustrating an exemplary system for facilitating a conversation among at least two users about a topic associated with a location in accordance with the techniques presented herein.\nFIG. 5 is an illustration of an exemplary computer-readable medium including processor-executable instructions configured to embody one or more of the provisions set forth herein.\nFIG. 6 is an illustration of an exemplary scenario featuring a grouping of topic groups referenced within a conversation among two individuals in accordance with the techniques presented herein.\nFIG. 7 is an illustration of an exemplary scenario featuring a viewing of information about a location-based topic referenced in a conversation among at least two individuals in accordance with the techniques presented herein.\nFIG. 8 is an illustration of an exemplary scenario featuring a presentation of recommendations of location-based topics to a user in accordance with the techniques presented herein.\nFIG. 9 is an illustration of an exemplary scenario featuring a recommendation of location-based topics for a conversation among at least two individuals in accordance with the techniques presented herein.\nFIG. 10 is an illustration of an exemplary computing environment wherein a portion of the present techniques may be implemented and/or utilized.\n<BR><BR>DETAILED DESCRIPTION\nThe claimed subject matter is now described with reference to the drawings, wherein like reference numerals are used to refer to like elements throughout.  In the following description, for purposes of explanation, numerous specific details are\nset forth in order to provide a thorough understanding of the claimed subject matter.  It may be evident, however, that the claimed subject matter may be practiced without these specific details.  In other instances, structures and devices are shown in\nblock diagram form in order to facilitate describing the claimed subject matter.\n<BR><BR>A. Introduction\nFIG. 1 is an illustration of an exemplary scenario 100 featuring a conversation among a set of individuals 102 using a pair of devices 104 to engage in a conversation 106 that includes references to topics that are associated with a location.\nIn this exemplary scenario 100, at a first time point 122, a first individual 102 uses a first device 104 to express a first message 108, indicating an invitation to a second individual 102 to join the first individual 102 for dinner.  The first\ndevice 104 transmits 110 the first message 108 to a second device 104 operated by the second individual 102, who receives the first message 108 and begins to consider the options for dinner.\nAt a second time point 124, the second individual 102 momentarily withdraws from the conversation 106 in order to direct a command 112 to the second device 104, e.g., a request to present a list of restaurants in the area of the second\nindividual 102.  The second device 104 presents a set of search results 114, and the second individual 102 provisionally selects a restaurant to suggest to the first individual 102.\nAt a third time 126, the second individual 102 returns to the conversation 106 and expresses a second message 108, indicating a suggestion of a restaurant.  The second device 104 transmits 110 the second message 108 to the first device 104,\nwhich presents the second message 108 to the first individual 102.\nAt a fourth time 128, the first individual 102 momentarily withdraws from the conversation 106 to investigate the restaurant referenced in the second message 108 from the second individual 102.  The first individual 102 directs a command 112 to\nthe first device 104 to show a location map including a location 120 associated with the topic 116 of the second message 108 (e.g., the street address of the restaurant).  The first device 104 therefore presents to the first individual 102 a mapping\napplication 114, including a map 118 that depicts the location 120 of the topic 116 in relation to the first individual 102.\nAt a fifth time point 130, the first individual 102 determines that the restaurant suggested by the second individual 102 is acceptable, and returns to the conversation 106 to create a third message 108 directed to the second individual 102. \nThe first device 104 of the first individual 102 transmits 110 the third message 108 to the second individual 102 to complete the conversation 106.  In this manner, the devices 104 of the individuals 102 facilitate the conversation 106 by presenting\ninformation about the topics 116 and the locations 120 associated therewith.\n<BR><BR>B. Presented Techniques\nThe exemplary scenario 100 of FIG. 1 provides examples of the manner in which devices 104 may facilitate a conversation 106 among at least two individuals 102 involving topics 116 that are associated with locations 120 (e.g., restaurants for\nwhich a street address or geopositioning coordinate may be identified).  However, in the exemplary scenario 100 of FIG. 1, several aspects of this interaction may be disadvantageous to one or both individuals 102.\nAs a first example, each individual 102 utilizes a device 104 to retrieve information about the topics 116 referenced in the conversation 106 by directing a command 112 to the device 104.  However, doing so diverts the attention of the\nindividual 102 from the conversation 106 to the device 104 in order to initiate the command 112 and to view the provided information.  During this period of diversion, the individual 102 may miss parts of the conversation; e.g., at the fourth time point\n128, while the first individual 102 is examining the map 118 of the location 102 of the restaurant, the second individual 102 may not be aware that the first individual 102 has momentarily withdrawn form the conversation 106.  The second individual 102\nmay therefore continue speaking, and the first individual 102 may not hear the second individual 102, resulting in miscommunication.  Alternatively, the first individual 102 may instruct the second individual 102 to wait while the first individual 102\nexamines the map 118, causing a disruptive delay in the conversation 106 that may frustrate the second individual 102.\nAs a second example, while one individual 102 is using a device 104 to view information that is pertinent to the conversation 106, the other individual 102 is unable to view the same information.  Rather, each individual 102 individually views\nthe information, and then creates a message 108 relaying the information to the other individual 102.  This exchange of information may be inefficient and/or time-consuming.  For example, the first individual 102 executes a query, views some information,\nand then creates a message 108 conveying some details the second individual 102.  The second individual 102 then re-inputs the same query in order to review the same information.  Thus, while each device 104 assists one of the individuals 102, the\ndevices 104 do not interoperate to share the information with the other individual 102; rather, the individuals 102 convey such information through the exchange of manually created messages 108.\nAs a third example, the exchange of information through messages 108 may result in a variety of communication errors.  For example, the first device 104 of the first user 102 may fulfill the command 112 to view a map to \"Joe's Pizza\" by\nsearching for and finding a restaurant matching the specified name.  However, this restaurant may differ from the restaurant being displayed on the second device 104 of the second user 102.  Thus, the individuals 102 may exchange messages 108 agreeing to\nmeet at a particular location 120, but may inadvertently be referring to different locations presented by the devices 104.  These and other disadvantages may arise from the manner in which the individuals 102 and devices 104 interact in the exemplary\nscenario 100 of FIG. 1.\nPresented herein are techniques for alternatively configuring a device 104 to facilitate a conversation 106 among at least two individuals 102 involving a topic 116 that is associated with a location 120.  In accordance with these techniques,\nrather than waiting for a command 112 from an individual 102 to present information about a topic 116 while the individual 102 engages in the conversation 106, a device 104 may monitor the conversation 106 to detect a reference from a first individual\n102 to a second individual 102 within the conversation 106 that involves a topic 116 associated with a location 120.  Notably, the device 104 may detect this reference as part of the conversation 106 among the individuals 102, e.g., in a message 108\nwritten by the first individual 102 and directed to the second individual 102, rather than as a command 112 directed by the individual 102 to the device 104.  Upon detecting such a reference, the device 104 may identify the location 120 of the topic 116\n(e.g., performing a search of a location database to identify a street address or geolocation coordinate identifying the location 120 of the topic 116).  The device 104 may then present to the individual 102 a map 118 indicating the location 120 of the\ntopic.  Notably, this map 118 may be presented concurrently with the conversation 106; e.g., a textual conversation interface (such as an exchange of written messages 108, and/or a speech-to-text transcript of a verbal communication) may be presented\nadjacent to the map 118 indicating the locations 120 of the topics 116 of the conversation 106.\nFIG. 2 presents an illustration of an exemplary scenario 200 featuring a pair of devices 104 respectively configured to facilitate a conversation 106 among two individuals 102 in accordance with the techniques presented herein.  In this\nexemplary scenario 200, at a first time point 210, the first individual 102 creates a message 108 to the second individual 102 within the conversation 106, where the first message 108 references a topic 116 that is associated with a location 120 (e.g., a\nrestaurant having a street address).  In this exemplary scenario 200, the first individual 102 creates the message 108 by speaking to the first device 104.  The first device 104 of the first individual 102 transmits 110 the first message 108 to the\nsecond device 104.  For example, the first message 108 may be translated by the first device 104 into text, which is transmitted to the second device 104; or, the first device 104 may transmit the first message 108 in verbal form to the second device 104\nfor presentation to the second individual 102 in verbal form; or, the second device 104 may, upon receiving the verbal first message 108 from the first device 104, translate the verbal message into a text message for presentation to the second user 102. \nA variety of such techniques may also be utilized, e.g., to present the first message 108 of the first user 102 in both verbal and text form.\nAs further illustrated in the exemplary scenario 200 of FIG. 2, one or both of the first device 104 and the second device 104 also monitors the conversation 106, and thus detects the reference in the first message 108 to the topic 116 associated\nwith a location 120.  For example, the first device 104 and/or the second device 104 may evaluate the first message 108 of the first individual 102, and may detect the mention of the location 120 (\"Joe's Pizza\").  Such detection may be achieved, e.g.,\nthrough comparison of the content of the first message 108 with a general set of keywords or key phrases (such as a directory of known locations), and/or a specific set of identifiers (such as locations 120 included in an address book of either\nindividual 102), and/or according to the context of the first message 108 (such as applying lexical analysis to determine that a phrase such as \"let's have dinner at .  . . \" is frequently followed by the name of a restaurant).  The first device 104\nand/or the second device 104 may also utilize various forms of natural language parsing, such as machine learning techniques that are trained to understand spoken language; dialogue management techniques to determine a particular domain (e.g., a list of\nplaces to go); the referenced entities (e.g., relative references to the entities of the conversation, such as \"the other one\" when discussing two restaurants); the action and/or intent of the participants, such as a consensus for making a reservation;\nand related attributes, such as ratings, menus, reviews, and hours of operation.\nAt a second time point 212, the first device 104 presents a conversation interface 204 on a display 202 for the first individual 102 (e.g., a text messaging interface, a transcript of a voice chat such as a telephone call, or a videoconference)\nthat includes the first message 108 of the first individual 102.  The second device 104 also presents the conversation interface 204 on a display 202 for the second individual 102 that includes the first message 108 from the first individual 102.  In\naddition, each device 104 presents on the display 202, adjacent to the conversation interface 204, a map interface 206 including a map 118 that depicts the location 120 of the topic 116 referenced in the first message 108.  Notably, the map interface 206\nis presented by each device 104 in the absence of a command 112 from either individual 102 to present a map 118 or otherwise provide information about the topic 116.  Rather, one or both devices 104 has detected the location-based reference in the first\nmessage 108, and has automatically inserted the map interface 206 alongside the conversation interface 204.  Additionally, both devices 104 automatically present the same location 120 for the topic 116 (e.g., because each device 104 has individually\nidentified the reference to the topic 116 in the first message 108 and the location 120 of the topic 116, and/or because one device 104 has automatically synchronized with the other device 104 to share the information and to coordinate the presentation\nof the map 118 of the location 120 of the topic 116 to both individuals 102).\nAt a third time point 214, the second individual 102 presents a second message 108 within the conversation 106 that references a second topic 116.  The second device 104 transmits 110 the second message 108 to the first device 104 for\npresentation to the first individual 102.  Again, because one or both devices 104 monitors the conversation 106, and thus detects that the second message 108 references a second topic 116 that is associated with a location 120.\nAt a fourth time point 216, the conversation interface 204 presented to each individual 102 is updated with the second message 108.  Additionally, on one or both devices 104, the location 120 of the second topic 116 is identified, and the map\n118 presented on the display 202 alongside the conversation interface 204 is updated to present the location 120 of the second topic 116 as well as the location 120 of the first topic 116.  Again, the map interface 206 is updated in the absence of a\ncommand 112 from either individual 102 to show the location 120 of the second topic 116; and, again, both devices 104 may update the map 118 either by individually detecting the reference to the second topic 116 and inserting the location 120 of the\nsecond topic 116 into the map 118, and/or by synchronizing the presentation to the individuals 102 with the other device 104.\nAt a fifth time point 218, the first individual 102 and the second individual 102 may conclude the conversation 106 when the first individual 102 sends a third message 108 accepting the suggestion of the second individual 102.  One or both\ndevices 104 may detect that the conversation 106 has ended and that the individuals 102 have agreed on a particular topic 116 (e.g., to have dinner together at the restaurant suggested by the second individual 102).  Additionally, one or both devices 104\nmay present on the display 202 a map 118 of the location 120 of the selected topic 116, and well as a route 208 from the current location of each individual 102 to the location 120.\nIn comparison with the exemplary scenario 100 of FIG. 1, the manner in which the devices 104 facilitate the conversation 106 among the individuals 102, in the exemplary scenario 200 of FIG. 2 may provide some advantages.  As a first example, one\nor both devices 104 provides maps 118 and/or related information (such as routing) about the topics 116 discussed in the conversation 106 automatically by monitoring the conversation 106 and detecting references to such topics 116, rather than in\nresponse to a command 112 from an individual 102.  Accordingly, both individuals 102 may remain present in and focused upon the conversation 106, rather than upon interacting with the devices 104 to request a retrieval of such information.  As a second\nexample, the devices 104 may interoperate to present the same information (e.g., the same location of the same restaurant) at the same time on each device 104 to each individual 102, and without either individual 102 having to convey this information to\nthe other individual 102 (e.g., neither individual 102 has to convey the information presented on one device 104 to the other individual 102, and neither individual 102 has to re-enter information into a device 104 that the other individual 102 already\nprovided to the other device 104).  In this manner, the configuration of the devices 202 may facilitate the convenience, efficiency, and/or accuracy of the conversation 106 in accordance with the techniques presented herein.\n<BR><BR>C. Exemplary Embodiments\nFIG. 3 presents an illustration of an exemplary first embodiment of the techniques presented herein, illustrated as an exemplary method 300 of facilitating a conversation 106 among at least two individuals 102.  The exemplary first method 300\nmay be implemented, e.g., as a set of instructions stored in a memory component (e.g., a memory circuit, a platter of a hard disk drive, a solid-state storage device, or a magnetic or optical disc) of a device 104 having a processor and a display 202,\nwhere the instructions, when executed on the processor, cause the device 104 to operate according to the techniques presented herein.  The exemplary first method 300 begins at 302 and involves executing 304 the instructions on the processor of the device\n104.  In particular, the execution of the instructions on the processor causes the device 104 to monitor 306 the conversation 106 to detect topics 116 referenced by an individual 102.  The execution of the instructions also causes the device 104 to, upon\ndetecting 308 a topic 116 referenced by a first individual 102 to a second individual 102 of the conversation 106, where the topic 116 is associated with a location 102, and where the topic 116 is not referenced as a command 112 to the device 104,\nidentify 310 the location 120 of the topic 116, and present 312, on the display 202 of the device 104, a map 118 indicating the location 120 of the topic 116.  In this manner, the exemplary method 300 enables the device to facilitate the conversation 106\namong the individuals 102 in accordance with the techniques presented herein, and so ends at 314.\nFIG. 4 presents an illustration of an exemplary second embodiment of the techniques presented herein, illustrated as an exemplary system 408 for enabling a device 104 to facilitate a conversation 106 among at least two individuals 102.  One or\nmore components of the exemplary system 408 may be implemented, e.g., as instructions stored in a memory 406 of the device 402 that, when executed on a processor 404 of the device 402, cause the device 402 to perform at least a portion of the techniques\npresented herein.  Alternatively (though not shown), one or more components of the exemplary system 408 may be implemented, e.g., as a volatile or nonvolatile logical circuit, such as a particularly designed semiconductor-on-a-chip (SoC) or a\nconfiguration of a field-programmable gate array (FPGA), that performs at least a portion of the techniques presented herein, such that the interoperation of the components completes the performance of a variant of the techniques presented herein.  The\nexemplary system 408 includes a conversation monitor 410 that monitors the conversation 106 to detect topics 116 referenced by an individual 102.  The exemplary system 408 also includes a topic presenter 412 that, upon the conversation monitor 410\ndetecting a topic 116 that is referenced by a first individual 102 to a second individual 102 of the conversation 106, where the topic is associated with a location 102 and is not referenced as a command 112 to the device 402, identify a location 120 of\nthe topic 116, and present, on the display 202, a map 118 indicating the location 120 of the topic 116.  In this manner, the interoperation of the components of the exemplary system 406 facilitates the conversation 106 among the at least two individuals\n102 in accordance with the techniques presented herein.\nStill another embodiment involves a computer-readable medium comprising processor-executable instructions configured to apply the techniques presented herein.  Such computer-readable media may include, e.g., computer-readable storage devices\ninvolving a tangible device, such as a memory semiconductor (e.g., a semiconductor utilizing static random access memory (SRAM), dynamic random access memory (DRAM), and/or synchronous dynamic random access memory (SDRAM) technologies), a platter of a\nhard disk drive, a flash memory device, or a magnetic or optical disc (such as a CD-R, DVD-R, or floppy disc), encoding a set of computer-readable instructions that, when executed by a processor of a device, cause the device to implement the techniques\npresented herein.  Such computer-readable media may also include (as a class of technologies that are distinct from computer-readable storage devices) various types of communications media, such as a signal that may be propagated through various physical\nphenomena (e.g., an electromagnetic signal, a sound wave signal, or an optical signal) and in various wired scenarios (e.g., via an Ethernet or fiber optic cable) and/or wireless scenarios (e.g., a wireless local area network (WLAN) such as WiFi, a\npersonal area network (PAN) such as Bluetooth, or a cellular or radio network), and which encodes a set of computer-readable instructions that, when executed by a processor of a device, cause the device to implement the techniques presented herein.\nAn exemplary computer-readable medium that may be devised in these ways is illustrated in FIG. 5, wherein the implementation 600 comprises a computer-readable memory device 502 (e.g., a CD-R, DVD-R, or a platter of a hard disk drive), on which\nis encoded computer-readable data 504.  This computer-readable data 504 in turn comprises a set of computer instructions 506 configured to operate according to the principles set forth herein.  In a first such embodiment, the processor-executable\ninstructions 506 may be configured to cause a computing device to perform a method of facilitating a conversation 106 among at least two individuals 102, such as the exemplary method 300 of FIG. 3.  In a second such embodiment, the processor-executable\ninstructions 506 may be configured to implement one or more components of a system of facilitating a conversation 106 among at least two individuals 102, such as the exemplary system 408 of FIG. 4.  Some embodiments of this computer-readable medium may\ncomprise a computer-readable storage device (e.g., a hard disk drive, an optical disc, or a flash memory device) that is configured to store processor-executable instructions configured in this manner.  Many such computer-readable media may be devised by\nthose of ordinary skill in the art that are configured to operate in accordance with the techniques presented herein.\n<BR><BR>D. Variations\nThe techniques discussed herein may be devised with variations in many aspects, and some variations may present additional advantages and/or reduce disadvantages with respect to other variations of these and other techniques.  Moreover, some\nvariations may be implemented in combination, and some combinations may feature additional advantages and/or reduced disadvantages through synergistic cooperation.  The variations may be incorporated in various embodiments (e.g., the exemplary method 300\nof FIG. 3; the exemplary system 408 of FIG. 4; and the exemplary computer-readable memory device 502 of FIG. 5) to confer individual and/or synergistic advantages upon such embodiments.\nD1.  Scenarios\nA first aspect that may vary among embodiments of these techniques relates to the scenarios wherein such techniques may be utilized.\nAs a first variation of this first aspect, the techniques presented herein may be utilized with many types of devices 104, including workstations; servers; game consoles; laptop and palmtop form factor computers; and mobile devices, such as\nphones, tablets, texting devices, cameras, portable media players, portable game players, and global positioning system (GPS) receivers.  Such techniques may also be provided by as a first device as a service for a second device (e.g., a server and/or\nclient), and/or by an interoperating set of devices (e.g., devices of respective individuals that synchronize and coordinate the presentation of information to the individuals 102).  Additionally, one or more devices 104 may be utilized by one of the\nindividuals 102 of the conversation 106, or a device 104 may be utilized by two or more individuals 102 (e.g., passengers in an automobile, or a group of individuals 102 together consulting an information kiosk).\nAs a second variation of this first aspect, the techniques presented herein may be utilized to facilitate many types of conversations 106.  Such conversations may include a variety of modalities, such as an exchange of text messages (e.g., email\nmessages, Simple Message Service (SMS) messages, and/or chat sessions, such as instant messaging scenarios); an exchange of voice communication (e.g., a phone call or voice-over-internet-protocol (VoIP) session); an exchange of images (e.g., a photo\nexchange service); an exchange of video streams (e.g., a videoconference); and/or an exchange of personal information (e.g., a mutual location sharing application).  In some such scenarios, different individuals may participate in different modalities\n(e.g., a first individual 102 may generate text messages, while a second individual 102 may generate a voice stream).  A device 104 of an individual 102 may therefore provide an asymmetric, mixed-modality communication session, and/or may translate from\na first modality to a second modality (e.g., translating an incoming voice stream into a textual transcript, such as by using a speech-to-text interpreter).  Alternatively or additionally, a particular individual 102 in the conversation may only send or\nonly receive messages (e.g., a passive listener of the conversation 106).  Alternatively or additionally, each device 104 may be utilized by one individual 102, or by a group of two or more individuals 102 participating collectively in the conversation\n106.  As a further variation, a conversation server that interoperates with one or several devices 104 to facilitate the conversation between the individuals 102 may perform all or some of the monitoring, detecting, identifying, and/or presenting\ninvolved in the techniques presented herein.\nAs a third variation of this first aspect, the techniques presented herein may be utilized to facilitate many types of conversations 106.  In some such scenarios, translation may be performed (e.g., translating a first language of a first\nindividual 102 to a second language of a second individual 102, or translating from a native language of a first individual 102 to an accessibility language understood by a second individual 102, such as Braille).  Alternatively or additionally, such\nconversations 106 may include references to many types of topics 116 associated with a location 120, such as a residence of an individual 102 (including an individual 102 participating in the conversation 106); a business (e.g., a restaurant, a theater,\nor an office); an event (e.g., a concert or site of a news story); and/or an individual (e.g., the current location of an individual 102 of the conversation 106).  In some variations, the conversation between the individuals 102 may be asynchronous, such\nas an exchange of text or email messages, and the evaluation of such messages to identify referenced locations 120 may be performed in a non-realtime manner.  In other scenarios, the conversation between the individuals 102 may be fluid and realtime,\nsuch as a telephone call or videoconference, and respective devices 104 may perform the evaluation of messages 108 in a realtime or near-realtime manner concurrently with the conversation, which may enable a rapidly responsive updating of a map to\nreflect the topics and locations 120 currently being discussed by the individuals 102.  Many such scenarios may be devised where the techniques presented herein may be advantageously utilized.\nD2.  Detecting and Tracking Referenced Topics\nA second aspect that may vary among embodiments of the techniques presented herein relates to the manner of identifying topics 116 referenced within the conversation 106, and of tracking such topics 116 through the conversation.\nAs a first variation of this second aspect, a device 104 may monitor the conversation 106 to detect references to topics 116 in various ways.  As a first such example, the device 104 may comprise a set of recognizable keywords or phrases, such\nas the names of restaurants in the area of one or more individuals 102 of the conversation 106, and may detect instances of these keywords for phrases in the messages 108 of the conversation 106.  Such keywords or phrases may include, e.g., the names of\nlocations 120 where one or more of the individuals 102 has recently and/or frequently visited, and/or the names of locations 120 that are associated with a calendar event for one or more of the individuals 102.  Such keywords and/or phrases key may also\nbe drawn from data sources that are particular to the individuals 102, such as those associated with locations 120 that are near the current location of one or more individuals 102, and/or an address book or travel history of one or more individuals 102. As another example, such keywords may comprise the names of locations 120 that are of high and/or trending popularity in the vicinity of one or more of the individuals 102, and/or among contacts of one or more of the individuals 102 within a social\nnetwork or an institution.  As a further variation, such concepts may be selected from two or more individual profiles of the individuals 102 of the conversation, such as including a second individual profile of a second individual 102 of the\nconversation.\nAs a second such example of this first variation of this second aspect, the device 104 may utilize a natural-language parsing technique to detect the references, such as linguistic structure and conversational patterns (e.g., phrases such as\n\"meet me at [name of location]\" and \"let's visit [name of location]\" indicating the name of a referenced location).  As a third such example, the device 104 may utilize metadata, such as a hashtag or hyperlink inserted by a user 102 into a message 108\nthat identifies a topic 116 having a location 120.  As a fourth such example, the device 104 may analyze the conversation 106 with a machine learning evaluation technique, such as an artificial neural network that has been trained to recognize topics 116\narising within conversations 106.  Such machine-learning evaluation techniques may involve, e.g., identifying the domain of the conversation, including the topics 116 and/or locations 120 that are associated with such domains (e.g., conversations about\nmeals, which frequently involve references to restaurant locations 120, vs.  conversations about social plans, which frequently involve references to locations 120 such as movie theaters and concert halls).  The first device 104 and/or the second device\n104 may also utilize various forms of natural language parsing, such as machine learning techniques that are trained to understand spoken language; dialogue management techniques to determine a particular domain (e.g., a list of places to go); the\nreferenced entities (e.g., relative references to the entities of the conversation, such as \"the other one\" when discussing two restaurants); the action and/or intent of the participants, such as a consensus for making a reservation; and related\nattributes, such as ratings, menus, reviews, and hours of operation.\nAs a second variation of this second aspect, in addition to identifying the topics 116 of the conversation 106, a device 104 may track a set of one or more topic groups arising within the conversation 106 that are respectively associated with at\nleast one topic 116.  For example, the individuals 102 may discuss a set of evening plans, including (as a first topic group) restaurants where the individuals 102 may meet for dinner, and (as a second topic group) locations to view a film following\ndinner.  As the individuals 102 reference and discuss the topics 116, in addition to identifying the topics 116 and presenting a map 118 indicating the locations 120 of the topics 116, a device 104 may group the list of topics 116 into topic groups,\nthereby facilitating an organized summary of the conversation 106.  For example, a topic presenter may, upon detecting a topic 116 referenced by a first individual 102 to a second individual 102 of the conversation 106, identify a selected topic group of\nthe conversation 106 that is associated with the topic 116, and store the topic 116 in the conversation topic set, where the storage associated the topic 116 with the selected topic group.  The device 104 may also present, on the display 202, a list\ncomprising at least two topic groups, and the topics 116 of the conversation 106 that are associated with the respective topic groups.\nAs a third variation of this second aspect, a device 104 monitoring the conversation 106 may track a set of topics 116 referenced by the individuals 102 during the conversation 106.  For example, upon detecting a topic 116 referenced by a first\nindividual 102 of the conversation 106, a device 104 may store the topic 116 in a conversation topic set.  The device 104 may then present, on the display 202, a list comprising the conversation topic set, and upon receiving from an individual 102 a\nselection of a selected topic 116 in the list, present on the map 118 the location 120 of the selected topic 116 (e.g., a list of the restaurants discussed by the individuals 102 during a conversation 106 about where to meet for dinner).  The device 104\nmay also detect the selection by an individual 102 of a topic 116 in the list, and may present on the map 118 the location 120 of the selected topic 16.  Additionally, the device 104 may persistently store the conversation topic list for later review by\nthe individuals 102.  In an embodiment, upon detecting a conversation end, the device 104 stores the conversation topic set; and upon receiving a request from an individual 102 to present the conversation topic set after the conversation end, the device\n104 may present, on the display 202, a list comprising the conversation topic set.\nIn addition to collecting, storing, and presenting the topics 116 of the conversation 106, further variations of this third variation of this second aspect involve altering the list comprising the conversation topic set to reflect the substance\nof the conversation 106 among the individuals 102.  For example, upon receiving a request from an individual 102 to alter the conversation topic set, the device 104 alter the conversation topic set according to the request of the individual 102.  Such\nrequests may include, e.g., a request from an individual 102 for a removal of a selected topic 116 from the conversation topic set, such as an expression of disapproval of a topic 116 suggested by another individual 102 of the conversation 106.  In\naddition to removing the topic 116 from the conversation 106, the device 104 may subsequently refrain from presenting the selected topic 116 in the conversation topic set.  As a still further variation, one or more individuals 102 within a conversation\n106 may be designated as a conversation leader of the conversation 106 (e.g., a moderator of a large group discussion).  A device 104 may alter the conversation topic set only in accordance with requests received from a conversation leader, and may\nrefrain from altering the conversation in response to request from individuals 102 who are not a conversation leader of the conversation 106.  That is, while many individuals 102 in the discussion may propose the addition and/or removal of topics 116,\nthe device 104 may alter the conversation topic set only when such proposals are accepted by a conversation leader of the conversation 106.\nFIG. 6 presents an illustration of an exemplary scenario 600 featuring several of the variations of the second aspect presented herein.  In this exemplary scenario 600, at a first time point 604, a first individual 102 in a conversation 106\nprovides a message 108 referencing two topics 116: the name of a restaurant, and the name of a theater.  In view of this message 108, a device 104 may identify a different topic group 602 for each topic 116 (e.g., a \"dinner\" topic group 602 for topics\n116 comprising restaurants, and a \"movie\" topic group 602 for topics 116 comprising theaters); and upon presenting the conversation interface 204, the device 104 may group the list of topics 116 according to the topic groups 602 of the topics 116.  At a\nsecond time point 606, a second individual 102 of the conversation 106 may provide a second message 108 declining consideration of a selected topic 116 of the conversation 106, and the device 104 may alter the conversation topic set by removing the\nselected topic 116 from the list.  The second message 108 may also specify an alternative topic 116 for each of the first topic group 602 (e.g., a different place to have dinner) and the second topic group 602 (e.g., a different theater where the\nindividuals 102 may view a film).  The device 104 may identify the topic group 602 associated with each topic 116 of the second message 108 (e.g., by referring to a location database to identify the type of location 120 referenced by the topic 116,\nand/or by evaluating the context of the conversation 106 in which the topic 116 was referenced), and may present the list on the display 202 with the added topics 116 respectively grouped according to the topic group 602 associated with the topic 116. \nEmbodiments may utilize many such variations while identifying and tracking the topics 116 referenced within a conversation 106 in accordance with the techniques presented herein.\nD3.  Presenting Locations of Conversation Topics\nA third aspect that may vary among embodiments of the techniques presented herein involves the manner of presenting to an individual 102 the location(s) 120 of the topic(s) 116 of the conversation 106 on a map 118.\nAs a first variation of this third aspect, many types of maps 118 may be presented, including an area map, a road map, an aerial map (e.g., a bird's-eye view captured by a satellite of an area including a location 120), a topographical map, and\na population or traffic map.  The map 118 may also present only one location 120, or the respective locations 120 of two or more topics 116, and/or may also indicate a current or projects location 120 of one or more individuals 102 of the conversation\n106.\nAs a second variation of this third aspect, the map interface 206 may be presented on the display 202 to an individual 102 in various ways.  For example, the device 104 may initially refrain from presenting the map 118 in a map interface 206 of\nthe conversation interface 204 before detecting in the conversation 106 at least one reference to a topic 116 that is associated with a location 120.  Upon detecting such a reference, the device 104 may insert the map interface 206 into the conversation\ninterface 204, including the map 118 indicating the location 120 of the topic 116.  Alternatively or additionally, upon detecting a shift in the conversation 106 away from references to topics 116 associated with locations 120, the device 104 may remove\nthe map interface 206 from the conversation interface 204.\nAs a third variation of this third aspect, the device 104 may occasionally substitute the presentation of the map 118 of a location 120 with other information about the topic 116 of the location 120.  For example, upon receiving from an\nindividual 120 a selection of a selected topic 116 of the conversation 106, the device 104 may replace the map 118 on the display 202 with an information interface describing the topic 106 (e.g., a picture of the location 120 of the topic 116, a menu of\na restaurant, or a listing of movies playing at a theater); and upon receiving from an individual 102 a request to return to the map 118, the device 104 may replace the information interface on the display 202 with the map 118.  Such information may be\nretrieved from a variety of sources (e.g., from a web page associated with the topic 106; from a location database or a reviews database; or from an advertiser, such as an advertisement associated with a topic 116 including a discount for the individuals\n102, where the advertisement is retrieved and presented with the topic 116 in the map interface 206).\nAs a fourth variation of this third aspect, the device 104 may present a set of action options for actions that are associated with a topic 116 and/or location 120.  The device 104 may therefore identify at least one action that is associated\nwith the topic 106, The device 104 may therefore present on the display 202, associated with the topic 116, an action option to invoke the respective actions; and, upon receiving from an individual 102 a selection of a selected action option for a\nselected topic 116, invoke the selected action option for the selected topic 116.\nFIG. 7 presents an illustration of an exemplary scenario 700 featuring several variations in the presentation, on the display 202 of a device 104, of locations 120 for a selected topic 116 of a conversation 106.  In this exemplary scenario 700,\nat a first time point 708, the device 104 may include in the presentation of the conversation interface 204 (not shown) a map interface 206 that, in addition to indicating the location 120 of the topic 116, presents action options 702 for respective\nactions that may be invoked in relation to the topic 116.  In this exemplary scenario 700, the selected topic 116 comprises a restaurant, and the device 104 identifies and presents action options 702 for actions including plotting a route between a\ncurrent location of the individual 102 and the location 120; initiating a reservation of a table at the restaurant; and initiating a communication session between the restaurant and the individual 102.  If the individual 102 selects 704 an action option\n702 offering to show a menu for the restaurant, then at a second time point 710, the display 202 of the device 104 hides the map interface 206 and presents an information interface 706 providing information about the topic 116, i.e., the menu of the\nrestaurant.  A user selection 704 of an action option to return to the map causes a removal of the information interface 706 and a second presentation of the map interface 206.  In this manner, the presentation of the topic 116 and location 120 may be\nresponsive to user interaction initiated by an individual 102, and may respond accordingly by adjusting the contents of the display 202 of the device 104, in accordance with several variations of this third aspect of the techniques presented herein.\nD4.  Topic Suggestions\nA fourth aspect that may vary among embodiments of the techniques presented herein involves suggested topics 106 for a topic group 602 of the conversation 106.  For example, if the conversation 106 of the individuals 102 focuses on where to have\ndinner, a device 104 may identify at least one suggested topic 116 for the topic group 602, and present the least one suggested topic 116 to the individual 102.\nAs a first variation of this fourth aspect, suggested topics 116 may be inserted into a conversation 106 in response to various events.  As a first such example, suggested topics 116 may be inserted upon detecting a new topic group 602 (e.g., a\nsuggestion from a first individual 102 to a second individual 102 to have dinner somewhere).  As a second such example, upon receiving a request for removal of a topic 116 from a topic group 602, a device 104 may identify an alternative topic 116 for the\nselected topic group 602, and suggest the alternative topic in lieu of the removed topic 116.  As a third such example, alternative topics may be identified and suggested if a topic group 602 remains unfilled for at least a threshold duration (e.g., if\nthe individuals 102 are unable to agree on a restaurant after a minute of conversation 106).  As a fourth such example, the device 104 may present to an individual 102 an option to identify and present suggested topics 116 for a topic group 602, and may\ndo so upon receiving an activation of the option by the individual 102 (e.g., upon the individual 102 selecting a \"suggestions\" button).\nAs a second variation of this fourth aspect, suggested topics 116 may be automatically presented by the device 104 to the individual 102.  For example, upon detecting a reference to a topic 116, the device 104 may automatically present\nlocation-based suggestions on a map 118 without awaiting a request from the individual 102 to do so.  Alternatively, and as illustrated in the exemplary scenario 800 of FIG. 8, the suggested topics 116 may be presented in response to input from the\nindividual 102.  At a first time point 806, the individuals 102 in a conversation 106 may exchange messages 108 involving a topic 116, and a device 104 may identify one or more suggested topics 116 respectively associated with a location 120.  At a\nsecond time point 808, the device 104 may present a suggestions pane 802 adjacent to the conversation 110 with a list of suggested topics 116.  At a third time point 810, the device 104 may detect user input from the individual 102 comprising a selection\n804 of a suggested topic 116, and in response and at a fourth time point 810, the device 104 may present a map interface 206 adjacent to the conversation 110 with a map 118 indicating the location 120 of the suggested topic 116 selected by the individual\n102.  In this manner, the device 104 may utilize manual user input in the presentation of the suggested topics 116 to the individual 102.\nAs a third variation of this fourth aspect, suggested topics 116 may be integrated with at least one topic 116 referenced by an individual 102 of the conversation 106 in various ways.  For example, the suggested topics 116 may be presented in a\ndifferent area of the map interface 206 than the topics 116 referenced by the individuals 102 (e.g., a \"referenced restaurants\" list and a \"suggested restaurants\" list).  Alternatively, the suggested topics 116 may be presented together with the topics\n116 referenced by the individuals 102, e.g., as a mixed list, optionally with the suggested topics 116 presented in a different visual manner than the topics 116 referenced by the individuals 102 of the conversation 106.\nAs a fourth variation of this fourth aspect, suggested topics 116 may be identified in a variety of ways.  As a first such example, at least one individual 102 may identify at least one topic group constraint for a topic group 602 (e.g., for the\ntopic group of restaurants, an individual 102 may specify a preference for a particular type of food), and the device 104 may identify only suggested topics that satisfy the topic group constraints of the topic group 602.  As a second such example, a\ndevice 104 may refer to an individual profile of at least one individual 102 in the conversation 106, and may identify suggested topics that are consistent with the individual profile (e.g., restaurants that are consistent with dietary restrictions of\nthe individual 102 specified in the individual profile, or restaurants where the individual profile indicates that the individual 102 has previously visited and enjoyed).  If a plurality of suggested topics 116 are available for presentation, a device\n104 may calculate, for each suggested topic, a suggested topic score associating the suggested topic 16 with the topic group constraints of the topic group 602, and may present the suggested topics 602 sorted according to the suggested topic scores.  In\nsome scenarios, the identification of suggested topics 116 may be difficult, e.g., if individuals 102 in the conversation 106 have conflicting preferences or restrictions, and the device 104 may have to select suggested topics as a consensus among the\nindividuals 102.\nFIG. 9 presents an illustration of an exemplary scenario featuring a presentation of suggested topics to individuals 102 in view of several of the variations of this fourth aspect.  In this exemplary scenario 900, a set of individuals 102 are\nhaving a conversation 106 about where to meet for dinner.  A device 104 of an individual 102 may have access to an individual profile 902 for each individual 102, where the individual profile 902 specifies a location 120 of the individual 102 (e.g., a\ncurrent location of the individual 102, or a projected location 102 of the individual 102 prior to a selected meeting time); a set of specified dietary preferences and dietary restrictions; and a price range preference.  The details of the individual\nprofiles 902 may be regarded by the device 104 as topic group constraints 904 for a \"dinner restaurants\" topic group 602.  The device 104 may also have access to a topic database 906, such as a restaurant database providing various details about the\nrestaurants.  The device 104 may endeavor to select, for presentation to the individuals 120 as suggested topics 116, a subset of topics 116 that are consistent with the details of the individual profiles 902.  Additionally, in this scenario, none of the\ntopics 116 is consistent with all of the topic group constraints 904 (e.g., because the individuals 102 are located in different areas of a region, no location 120 is within a short distance of all of the individuals 102).  Therefore, the device 104 may\ncalculate, for each topic 116, a suggested topic score 908 indicating a conformity of the topic 116 with the topic group constraints 904.  Additionally, various topic group constraints 904 may be attributed different weights (e.g., a dietary restriction\nof a single individual 102, such as availability of gluten-free food options, may be a rigid constraint, while food preferences may be resolved by consensus).  As another example, a consensus about locations 102 may be selected to minimize the collective\ntravel time of all of the individuals 102, and/or to achieve an earliest meeting time, taking into account the different travel options available to each individual 102.  The device 104 may then present the suggested topics 116 for the meeting to the\nindividuals 102 as a topic group 602 with a map 118 presenting the location 120 of each topic 116, and optionally including a rating 910 for each topic 116 in the topic group 602 indicating an anticipated degree of consensus among the individuals 102,\nand/or sorting the topics 116 according to the respective ratings 910.  In this manner, a device may provide a topic group 602 with a map 118 indicating the locations 120 of suggested topics 116 in accordance with the techniques presented herein.\n<BR><BR>E. Computing Environment\nThe techniques discussed herein may be devised with variations in many aspects, and some variations may present additional advantages and/or reduce disadvantages with respect to other variations of these and other techniques.  Moreover, some\nvariations may be implemented in combination, and some combinations may feature additional advantages and/or reduced disadvantages through synergistic cooperation.  The variations may be incorporated in various embodiments to confer individual and/or\nsynergistic advantages upon such embodiments.\nFIG. 10 and the following discussion provide a brief, general description of a suitable computing environment to implement embodiments of one or more of the provisions set forth herein.  The operating environment of FIG. 10 is only one example\nof a suitable operating environment and is not intended to suggest any limitation as to the scope of use or functionality of the operating environment.  Example computing devices include, but are not limited to, personal computers, server computers,\nhand-held or laptop devices, mobile devices (such as mobile phones, Personal Digital Assistants (PDAs), media players, and the like), multiprocessor systems, consumer electronics, mini computers, mainframe computers, distributed computing environments\nthat include any of the above systems or devices, and the like.\nAlthough not required, embodiments are described in the general context of \"computer readable instructions\" being executed by one or more computing devices.  Computer readable instructions may be distributed via computer readable media\n(discussed below).  Computer readable instructions may be implemented as program modules, such as functions, objects, Application Programming Interfaces (APIs), data structures, and the like, that perform particular tasks or implement particular abstract\ndata types.  Typically, the functionality of the computer readable instructions may be combined or distributed as desired in various environments.\nFIG. 10 illustrates an example of a system 1000 comprising a computing device 1002 configured to implement one or more embodiments provided herein.  In one configuration, computing device 1002 includes at least one processing unit 1006 and\nmemory 1008.  Depending on the exact configuration and type of computing device, memory 1008 may be volatile (such as RAM, for example), non-volatile (such as ROM, flash memory, etc., for example) or some combination of the two.  This configuration is\nillustrated in FIG. 10 by dashed line 1004.\nIn other embodiments, device 1002 may include additional features and/or functionality.  For example, device 1002 may also include additional storage (e.g., removable and/or non-removable) including, but not limited to, magnetic storage, optical\nstorage, and the like.  Such additional storage is illustrated in FIG. 10 by storage 1010.  In one embodiment, computer readable instructions to implement one or more embodiments provided herein may be in storage 1010.  Storage 1010 may also store other\ncomputer readable instructions to implement an operating system, an application program, and the like.  Computer readable instructions may be loaded in memory 1008 for execution by processing unit 1006, for example.\nThe term \"computer readable media\" as used herein includes computer storage media.  Computer storage media includes volatile and nonvolatile, removable and non-removable media implemented in any method or technology for storage of information\nsuch as computer readable instructions or other data.  Memory 1008 and storage 1010 are examples of computer storage media.  Computer storage media includes, but is not limited to, RAM, ROM, EEPROM, flash memory or other memory technology, CD-ROM,\nDigital Versatile Disks (DVDs) or other optical storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium which can be used to store the desired information and which can be accessed by\ndevice 1002.  Any such computer storage media may be part of device 1002.\nDevice 1002 may also include communication connection(s) 1016 that allows device 1002 to communicate with other devices.  Communication connection(s) 1016 may include, but is not limited to, a modem, a Network Interface Card (NIC), an integrated\nnetwork interface, a radio frequency transmitter/receiver, an infrared port, a USB connection, or other interfaces for connecting computing device 1002 to other computing devices.  Communication connection(s) 1016 may include a wired connection or a\nwireless connection.  Communication connection(s) 1016 may transmit and/or receive communication media.\nThe term \"computer readable media\" may include communication media.  Communication media typically embodies computer readable instructions or other data in a \"modulated data signal\" such as a carrier wave or other transport mechanism and\nincludes any information delivery media.  The term \"modulated data signal\" may include a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal.\nDevice 1002 may include input device(s) 1014 such as keyboard, mouse, pen, voice input device, touch input device, infrared cameras, video input devices, and/or any other input device.  Output device(s) 1012 such as one or more displays,\nspeakers, printers, and/or any other output device may also be included in device 1002.  Input device(s) 1014 and output device(s) 1012 may be connected to device 1002 via a wired connection, wireless connection, or any combination thereof.  In one\nembodiment, an input device or an output device from another computing device may be used as input device(s) 1014 or output device(s) 1012 for computing device 1002.\nComponents of computing device 1002 may be connected by various interconnects, such as a bus.  Such interconnects may include a Peripheral Component Interconnect (PCI), such as PCI Express, a Universal Serial Bus (USB), Firewire (IEEE 1394), an\noptical bus structure, and the like.  In another embodiment, components of computing device 1002 may be interconnected by a network.  For example, memory 1008 may be comprised of multiple physical memory units located in different physical locations\ninterconnected by a network.\nThose skilled in the art will realize that storage devices utilized to store computer readable instructions may be distributed across a network.  For example, a computing device 1020 accessible via network 1018 may store computer readable\ninstructions to implement one or more embodiments provided herein.  Computing device 1002 may access computing device 1020 and download a part or all of the computer readable instructions for execution.  Alternatively, computing device 1002 may download\npieces of the computer readable instructions, as needed, or some instructions may be executed at computing device 1002 and some at computing device 1020.\n<BR><BR>F. Use of Terms\nAlthough the subject matter has been described in language specific to structural features and/or methodological acts, it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific\nfeatures or acts described above.  Rather, the specific features and acts described above are disclosed as example forms of implementing the claims.\nAs used in this application, the terms \"component,\" \"module,\" \"system\", \"interface\", and the like are generally intended to refer to a computer-related entity, either hardware, a combination of hardware and software, software, or software in\nexecution.  For example, a component may be, but is not limited to being, a process running on a processor, a processor, an object, an executable, a thread of execution, a program, and/or a computer.  By way of illustration, both an application running\non a controller and the controller can be a component.  One or more components may reside within a process and/or thread of execution and a component may be localized on one computer and/or distributed between two or more computers.\nFurthermore, the claimed subject matter may be implemented as a method, apparatus, or article of manufacture using standard programming and/or engineering techniques to produce software, firmware, hardware, or any combination thereof to control\na computer to implement the disclosed subject matter.  The term \"article of manufacture\" as used herein is intended to encompass a computer program accessible from any computer-readable device, carrier, or media.  Of course, those skilled in the art will\nrecognize many modifications may be made to this configuration without departing from the scope or spirit of the claimed subject matter.\nVarious operations of embodiments are provided herein.  In one embodiment, one or more of the operations described may constitute computer readable instructions stored on one or more computer readable media, which if executed by a computing\ndevice, will cause the computing device to perform the operations described.  The order in which some or all of the operations are described should not be construed as to imply that these operations are necessarily order dependent.  Alternative ordering\nwill be appreciated by one skilled in the art having the benefit of this description.  Further, it will be understood that not all operations are necessarily present in each embodiment provided herein.\nMoreover, the word \"exemplary\" is used herein to mean serving as an example, instance, or illustration.  Any aspect or design described herein as \"exemplary\" is not necessarily to be construed as advantageous over other aspects or designs. \nRather, use of the word exemplary is intended to present concepts in a concrete fashion.  As used in this application, the term \"or\" is intended to mean an inclusive \"or\" rather than an exclusive \"or\".  That is, unless specified otherwise, or clear from\ncontext, \"X employs A or B\" is intended to mean any of the natural inclusive permutations.  That is, if X employs A; X employs B; or X employs both A and B, then \"X employs A or B\" is satisfied under any of the foregoing instances.  In addition, the\narticles \"a\" and \"an\" as used in this application and the appended claims may generally be construed to mean \"one or more\" unless specified otherwise or clear from context to be directed to a singular form.\nAlso, although the disclosure has been shown and described with respect to one or more implementations, equivalent alterations and modifications will occur to others skilled in the art based upon a reading and understanding of this specification\nand the annexed drawings.  The disclosure includes all such modifications and alterations and is limited only by the scope of the following claims.  In particular regard to the various functions performed by the above described components (e.g.,\nelements, resources, etc.), the terms used to describe such components are intended to correspond, unless otherwise indicated, to any component which performs the specified function of the described component (e.g., that is functionally equivalent), even\nthough not structurally equivalent to the disclosed structure which performs the function in the herein illustrated exemplary implementations of the disclosure.  In addition, while a particular feature of the disclosure may have been disclosed with\nrespect to only one of several implementations, such feature may be combined with one or more other features of the other implementations as may be desired and advantageous for any given or particular application.  Furthermore, to the extent that the\nterms \"includes\", \"having\", \"has\", \"with\", or variants thereof are used in either the detailed description or the claims, such terms are intended to be inclusive in a manner similar to the term \"comprising.\"", "application_number": "14307439", "abstract": " Individuals may utilize devices to engage in conversations about topics\n     respectively associated with a location (e.g., restaurants where the\n     individuals may meet for dinner). Often, the individual momentarily\n     withdraws from the conversation in order to issue commands to the device\n     to retrieve and present such information, and may miss parts of the\n     conversation while interacting with the device. Additionally, the\n     individual often explores such topics individually on a device and\n     conveys such information to the other individuals through messages, which\n     is inefficient and error-prone. Presented herein are techniques enabling\n     devices to facilitate conversations by monitoring the conversation for\n     references, by one individual to another (rather than as a command to the\n     device), to a topic associated with a location. In the absence of a\n     command from an individual, the device may automatically present a map\n     alongside a conversation interface showing the location(s) of the\n     topic(s) referenced in the conversation.\n", "citations": ["7917465", "7925716", "8037047", "8234239", "8312380", "8483948", "8886655", "8990344", "9374327", "20060129455", "20070281689", "20070288164", "20080201434", "20080249778", "20090063439", "20090319917", "20100100809", "20120253788", "20120254227", "20130218897", "20130226453", "20130226690", "20130227017", "20130326425", "20140164533"], "related": []}, {"id": "20160007218", "patent_code": "10375590", "patent_name": "Method and system for performance estimation of a communication link", "year": "2019", "inventor_and_country_data": " Inventors: \nBhagavatula; Ramya (Mountain View, CA), Song; Guocong (Union City, CA), Kerpez; Kenneth (Long Valley, NJ), Rhee; Wonjong (San Francisco, CA)  ", "description": "<BR><BR>CROSS-REFERENCE TO RELATED APPLICATION\nThis application is the U.S.  National Phase of International Application No. PCT/US2012/046810, filed Jul.  13, 2012, the disclosure of which is incorporated herein by reference in its entirety for all purposes.\n<BR><BR>BACKGROUND\nMonitoring performance of a communication link is performed, for instance, for proactively addressing and preventing user complaints, for deciding when to upgrade hardware associated with the communication link, for deciding when to trigger an\noptimization algorithm, for verifying that the optimization algorithm has improved performance, etc.\nCommunication system performance can be evaluated using traditional testing software applications such as iperf, netperf, ttcp, etc. Such software applications need to be installed at two communication devices where the application on one device\ngenerates and sends test data and the application on the other device receives the test data.  After the test is complete, statistics of data transportation are evaluated to evaluate the performance of the communication link between the two devices.\nThe term \"performance\" herein refers generally to network throughput (e.g., TCP/UDP), latency, jitter, connectivity, error rates, power consumption, transmit power, etc. Improving performance of the communication system includes increasing\nthroughput, reducing error rate and latency, improving (i.e., reducing) jitter, reducing power consumption, etc. for the communicating system.  The term \"TCP\" stands for transmission control protocol.  The term \"UDP\" refers to user datagram protocol.\nHowever, testing of a communication system or network to gauge its performance via such traditional testing software applications is intrusive to customer network service.  These traditional tests impose test traffic on the network which can\nhave a deleterious impact on the customer's traffic. <BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nEmbodiments of the disclosure will be understood more fully from the detailed description given below and from the accompanying drawings of various embodiments of the disclosure, which, however, should not be taken to limit the disclosure to the\nspecific embodiments, but are for explanation and understanding only.\nFIG. 1 is a communication network which is operable to estimate and improve communication system performance algorithm, according to one embodiment of the disclosure.\nFIG. 2 is a flowchart for training the performance algorithm, according to one embodiment of the disclosure.\nFIG. 3 is a flowchart for training the performance algorithm for a communication device by a server, according to one embodiment of the disclosure.\nFIG. 4 is a processor-based system having machine-readable storage medium with computer executable instructions operable to estimate and improve communication system performance algorithm, according to one embodiment of the disclosure.\n<BR><BR>DETAILED DESCRIPTION\nThe traditional method to test network traffic, which is intrusive to user network service, is referred to \"active probing.\" The term \"active probing\" herein generally refers to testing of a communication network by sending test pattern/data\nover the network from one communication device to another communication device, and then measuring the response from the sent test pattern.  The response data is also referred herein as \"active data\" or \"active measurement data\" which is data associated\nwith active probing of a communication network.\nTraditional active probing software such as iperf, netperf, ttcp, etc, is run at application layers, where a data transmission application software and a data reception application software are used together for accurately measuring performance\nbetween the two transmission and reception devices.  Traditional active probing is accurate because actual test data is transmitted in the same way as user traffic would be transmitted over the network.  Frequent active probing can be annoying to the\nuser because it may delay user traffic.  It is possible to run active probing without stopping user traffic, but such a measurement is not accurate because the testing traffic competes with the user traffic, and furthermore active probing can\nsignificantly impair the user experience due to lower throughput and/or higher latency.  To overcome this and other limitations, method and system for measuring performance without impacting customer's traffic are described herein.\nAn advanced active probing method is described in (PCT Application No. 07/77572 entitled \"Method and System for Performance Measurement of a Communication Link\" filed concurrently with this application on Jul.  13, 2012, incorporated by\nreference herein in its entirety, and co-owned by ASSIA Inc.  of Redwood City, Calif., 94065, USA.), can avoid the user traffic issue, by considering operational data that account for the user traffic as well as the test traffic.\nAnother mechanism to gauge performance of a communication link and/or communication device is to monitor operational data associated with a communication device.  The operational data is generated for several purposes.  For example, operational\ndata is sometimes generated as a by-product of normal operation of the communication device.  In another example, operational data is generated to provide basic performance or operation information associated with the communication device.  Reading or\ncollecting of such operational data is not intrusive to user network service.  Monitoring or reading of such communication data (operational data) is sometimes referred as \"passive probing,\" herein.  Usually, operational data of communication devices do\nnot contain the most important and advanced performance metrics such as throughput or latency, but a rough estimation of advanced metrics can be possible using operational data.\nFor instance, throughput may be roughly estimated from typical operational data such as packet error counts and PHY-layer constellation information that indicate how many bits are being transmitted per data symbol.  Such an estimate, however,\nmight not be accurate because the used operational data might not contain sufficient information about throughput and because the relation between the operational data and throughput is often dependent on noise (including interference) and channel\ncharacteristics that quickly change for different locations and different time.\nIn the embodiments of this disclosure, operational data are used together with active-probing data to get a reliable estimate of performance of a communication link.  In one embodiment, while active-probing data is used, the operational data can\nbe collected together.  With the complete set of active-probing data and operational data, active-probing data results are considered as an accurate estimation of performance of the communication link and are used for training operational-data-only\nestimation algorithms.\nIn one embodiment, once the training is complete and the accuracy of operational-data-only estimation is fully understood, the system is monitored with operational data without frequent active-probing that is service intrusive.  In one\nembodiment, active-probing is invoked infrequently, or even dynamically depending on the need for accurate performance estimation and need for training data for updating operational-data-only estimator.\nThe embodiments of the disclosure can be used in a few different ways.  For example, at a higher level abstraction, active-probing and operational data may be collected from a large (e.g., 100 or more communication devices forming a network)\ncommunication network and analysis can be performed over the entire data to develop passive estimators with a good accuracy.  In one embodiment, such passive estimations are performed with any well known machine learning techniques such as SVM (Support\nVector Machine).\nIn another example, at a lower level abstraction, passive estimator can be adaptively tuned for each communication link in the communication network.  Each environment is unique and the best estimator can be dependent on the environment.  In one\nembodiment, machine learning or any learning is performed for each communication device in the communication system such that the passive estimator provides the best performance for the given environment.\nIn one embodiment, the performance estimation algorithm performs updates as follows.  First, an initial step size is defined.  If the throughput estimation using passive data is determined to be too low by the active probing data, then this\nthroughput estimation is increased proportional to the step size.  If the throughput estimation using passive data is determined to be too high by the active probing data, then this throughput estimation is decreased proportional to the step size.  The\nterms \"low\" and \"high\" refer to programmable or predetermined thresholds distinct from one another.  If the throughput estimation is decreased and then increased at the next iteration, or if the throughput estimation is increased and then decreased at\nthe next iteration, then the step size is lowered.\nIn one embodiment, the operational data are read from counters (also referred herein as operational counters associated with the communication device) that increase in count value for successfully delivered packets.  The term \"successful\" herein\nrefers to an indication suggesting safe receipt of a packet by a communication device that is often confirmed by ACK (acknowledge) message packet.  In another embodiment, operational data such as error counts, retransmission counts, modulation, signal\nstrength, etc. are used to estimate the throughput of the communication link.\nDuring the process of passive probing, i.e., reading of operational data, customer network service is not interrupted.  Operational data is generally user visible or accessible data and is generally used for debugging and basic performance\nmonitoring of communications systems, but generally not for advanced performance estimation because the data was not designed for the performance monitoring, does not carry sufficient information related to performance and there is no known estimation\nalgorithms with high accuracy.  Therefore, passive probing alone may not be enough to determine advanced performance of a communication system and operational data generally includes counter values that are only weakly associated with the current\nperformance of a communication system.\nThe embodiments herein disclose a method and system for improving performance estimation of a communication device by using operational data together with active probing data to train a performance estimation algorithm.  In one embodiment, after\ntraining the performance estimation algorithm using both active probing data that is accurate and passive probing data that is not intrusive, operational data is monitored regularly and used to accurately update the performance estimation without\ninterrupting customer traffic over the network.\nIn one embodiment, active probing is initiated when there is a need to update the performance estimation algorithm.  Thereafter, the performance estimation algorithm is trained via passive operational probing data.  In another embodiment, active\nprobing is initiated periodically (i.e., at regular intervals) to check if the performance estimation algorithm that uses passive probing data only is estimating performance with comparable accuracy to the active probing data's algorithm.  The\nembodiments herein provide an efficient and nearly non-intrusive method for estimating performance of a communication device, and for managing a network system with little or no interruption to the users of the network.\nIn the following description, numerous details are discussed to provide a more thorough explanation of embodiments of the present disclosure.  It will be apparent, however, to one skilled in the art, that embodiments of the present disclosure\nmay be practiced without these specific details.  In other instances, well-known structures and devices are shown in block diagram form, rather than in detail, in order to avoid obscuring embodiments of the present disclosure.\nNote that in the corresponding drawings of the embodiments, signals are represented with lines.  Some lines may be thicker, to indicate more constituent signal paths, and/or have arrows at one or more ends, to indicate primary information flow\ndirection.  Such indications are not intended to be limiting.  Rather, the lines are used in connection with one or more exemplary embodiments to facilitate easier understanding of a circuit or a logical unit.  Any represented signal, as dictated by\ndesign needs or preferences, may actually comprise one or more signals that may travel in either direction and may be implemented with any suitable type of signal scheme.\nIn the following description and claims, the term \"coupled\" and its derivatives may be used.  The term \"coupled\" herein refers to two or more elements which are in direct contact (physically, electrically, magnetically, optically, etc.).  The\nterm \"coupled\" herein may also refer to two or more elements that are not in direct contact with each other, but still cooperate or interact with each other.\nAs used herein, unless otherwise specified the use of the ordinal adjectives \"first,\" \"second,\" and \"third,\" etc., to describe a common object, merely indicate that different instances of like objects are being referred to, and are not intended\nto imply that the objects so described must be in a given sequence, either temporally, spatially, in ranking or in any other manner.  The term \"substantially,\" \"approximately,\" \"nearly,\" \"about,\" \"close,\" and such similar terms refer to a quantity being\nwithin +/-20% of a target value.\nFIG. 1 is a communication network 100 which is operable to estimate and improve communication system performance estimation algorithm, according to one embodiment of the disclosure.  In one embodiment, the communication network comprises an\noptimization center 101 (e.g., server) communicatively coupled to one or more communication devices 103.sub.1-N, where `N` is a positive integer.  In one embodiment, communication device 103.sub.2 is coupled to a Customer Premises Equipment (CPE) modem\n104 via a Digital Subscriber Line (DSL) link.  In one embodiment, the CPE modem 104 is coupled to an access point (AP) 105.  In one embodiment, the AP 105 is coupled to one or more stations (STAs) 106.sub.1-M, where `M` is a positive integer.\nIn one embodiment, performance estimation algorithm 102 is an equation with input variables being the passive probing data.  In one embodiment, performance estimation algorithm 102 either increases or decreases in proportion to passive probing\ndata.\nIn one embodiment, instructions for updating and/or developing a performance estimation algorithm 102 are stored on the optimization server 101 and/or one or more of the communication devices 103.sub.1-N. While the embodiment of FIG. 1 does not\nshow that the other devices 104, 105, and 106.sub.1-M include instructions for updating and/or developing a performance estimation algorithm 102, in one embodiment any communication device coupled directly or indirectly to the network (wired or wireless)\nmay have instructions for updating and/or developing a performance estimation algorithm 102.  In one embodiment, the performance estimation algorithm 102 can be tuned per each communication device according to the communication device's data and\nenvironments.  In one embodiment, the resulting performance estimation algorithm 102 can be different over the communication devices 103.sub.1-N.\nIn one embodiment, the communication devices 103.sub.1-N include an access point (AP); a base station; a wireless local area network (LAN) device; a Digital subscriber line access multiplexer (DSLAM); a gateway; a performance enhancement device;\na Digital Subscriber Line (DSL) CPE (Customer premises equipment) modem; an in-home powerline device; a Home Phoneline Network Alliance (HPNA) based device; an in-home coax distribution device; a G.hn (Global Home Networking Standard) compatible device;\nan in-home metering communication device; an in-home appliance communicatively interfaced with the LAN; a wireless femtocell base station; a wireless WiFi compatible base station; a wireless mobile device repeater; a wireless mobile device base station;\nnodes within an ad-hoc/mesh network; an set-top box (STB)/set-top unit (STU) customer electronics device; an Internet Protocol (IP) enabled television; an IP enabled media player; an IP enabled gaming console; an Ethernet gateway; a computing device\nconnected to the LAN; an Ethernet connected computer peripheral device; an Ethernet connected router; an Ethernet connected wireless bridge; an Ethernet connected network bridge; and an Ethernet connected network switch.\nIn one embodiment, the one or more communication devices 103.sub.1-N are operable to execute active probing to determine active probing data.  In this embodiment, the one or more communication devices 103.sub.1-N flood traffic on their\nrespective communication links 107.sub.1-N to the optimization center 101.  In this embodiment, response received by the one or more communication devices 103.sub.1-N from the optimization center 101 over the communication links 107.sub.1-N is the active\ndata, which is used by the respective performance estimation algorithms 102 in the corresponding one or more communication devices 103.sub.1-N to train the performance estimation algorithms.\nIn one embodiment, the one or more communication devices 103.sub.1-N are operable to execute active probing by transmitting active probing data from one communication device to another communication device.  For example, communication device\n103.sub.1 transmits active probing data to communication device 106.sub.1 and/or communication device 103.sub.2 transmits active probing data to CPE 104 over a DSL link.  In another example, communication device 106.sub.1 transmits active probing data to\noptimization center 101 via communication links including 107.sub.1.\nIn one embodiment, the one or more communication devices 103.sub.1-N are further operable to wait for a predetermined time before reading the operational data including counter values related to user data traffic on the communication links\n107.sub.1-N. In one embodiment, the predetermined time is in the range of 0.001 seconds to 60 seconds.  In other embodiments other waiting periods may be used.  In one embodiment, the waiting period is programmable by software or hardware.\nSo as not to obscure the embodiments of the disclosure, communication devices 103.sub.1, 103.sub.2, 104, and optimization center 101 are discussed.  The same discussion is applicable to other communication devices.  In one embodiment, the\ncommunication device 103.sub.1 is further operable to receive a report indicating amount of data or data received by the other communication device (e.g., optimization center 101, and/or communication device 103.sub.2).\nIn one embodiment, the one or more communication devices 103.sub.1-N are operable to read operational data which includes data related to channel (e.g., links 107.sub.1-N, links between 105 and 106.sub.1-M, links between 103.sub.1 and\n106.sub.1-M, and/or DSL links between 103.sub.2 and 104) and its noise condition, data relevant to the current setting of the communication devices 103.sub.1-N, and counter values related to user data traffic between the communication devices 103.sub.1-N\nand another communication device (e.g., optimization center 101, 105, 106.sub.1-M, 104, etc), wherein the operational data is relevant to the current settings of the communication device.  Examples of such operational data are successful transmit packet\ncounts, successful receive packet counts, ACK packet counts, error packet counts, discarded packet counts, retransmission counts, etc.\nIn one embodiment, the one or more communication devices are operable to execute active probing fewer times than to execute passive probing.  For example, active probing is executed at most 5 times per day because it is an intrusive process, and\npassive probing is executed 1440 times per day (e.g., every one minute).\nIn one embodiment, the one or more communication devices 103.sub.1-N are operable to train their respective performance estimation algorithms 102 according to the active probing data and the operational data.  In one embodiment, the one or more\ncommunication devices 103.sub.1-N are operable to, prior to executing active probing, read operational data (i.e., passive probing) from counter values related to the user data traffic on communication links.  For example, links 107.sub.1-N, links\nbetween 105 and 106.sub.1-M, links between 103.sub.1 and 106.sub.1-M, and/or DSL links between 103.sub.2 and 104.\nIn one embodiment, the counter values include at least one of packet error counts, packet retransmission counts, successful ACK message counts, etc. In one embodiment, the one or more communication devices 103.sub.1-N are operable to read\noperational data (i.e., execute passive probing) during or after executing active probing.\nThe accuracy of the performance estimation algorithm may be dependent on the characteristics of the user's traffic patterns and the characteristics of the noise and channel environments.  In an environment, noise and channel might vary\nfrequently.  In another environment, noise and channel might vary very infrequently.  In yet another environment, noise and channel might vary frequently but mostly between two states only.\nIn one embodiment, the performance estimation algorithm 102 for each device is adaptively tuned.  In one embodiment, the one or more communication devices 103.sub.1-N are operable to train the performance estimation algorithm 102 by updating the\nperformance estimation algorithm 102 as a function of one or more criteria including at least one of: time of day, time of the week, type of communication device, manufacturer and model of equipment, equipment characteristics, firmware, backbone\nlimitations, user's network usage pattern, radio-frequency (RF) characteristics including at least one of: signal power, frequency bands and mode of operation, environment statistics, or data on operation of communication devices adjacent to the\ncommunication device, wherein the data includes at least one of interference channels and levels.\nIn one embodiment, the one or more communication devices 103.sub.1-N are operable to compute throughput of the communication devices 103.sub.1-N using active probing data for training the performance estimation algorithm.  In one embodiment, the\none or more communication devices 103.sub.1-N are operable to transmit the active probing data and read operational data over the communication links 107.sub.1-N to the optimization center 101 (e.g., a server), where the operational data is related to\nuser data traffic from the one or more communication devices 103.sub.1-N before, during and/or after executing active probing.  In one embodiment, the optimization center 101 is operable to train the performance estimation algorithm 102 for the\ncommunication device according to active probing data and read operational data from the one or more communication devices 103.sub.1-N.\nIn one embodiment, the optimization center 101 is operable to apply machine learning algorithm for training the performance estimation algorithm for the communication device.  In this embodiment, the accurate active probing data is used together\nwith passive probing data for machine learning, and performance estimation algorithm 102, that uses only the passive data as input, is determined accordingly.\nFor example, the optimization center 101 (or any other communication device) may apply one or more of: decision tree learning, associated rule learning, artificial neural networks learning algorithm, genetic programming algorithm, inductive\nlogic programming approach, support vector machine approach, clustering, Bayesian network based probabilistic graphical model, reinforcement learning, representation learning, sparse dictionary learning, etc. In other embodiments, other machine learning\nalgorithms may be used.  While the embodiments herein describe the machine learning algorithm applied by the optimization center 101, any communication device may have executable instructions and associated hardware to apply and perform machine learning\nfor training performance estimation algorithm.\nIn one embodiment, after completing the training process for the performance estimation algorithm, the network 100 can be monitored with operational data (data from passive probing) without any interruption to user traffic.  In one embodiment,\nactive probing can be initiated by any communication device infrequently and/or dynamically depending on the need for accurate performance estimation and the need for training data for updating operational data estimator.  For example, when the\nperformance of the network falls below a threshold and the performance estimation does not provide accurate data, the communication device 103.sub.2 may invoke active probing to train the performance estimation algorithm so that the network 100 can be\nmonitored via operational data in future.\nFIG. 2 is a flowchart 200 for training the performance algorithm, according to one embodiment of the disclosure.  Although the blocks in the flowcharts with reference to FIG. 2 are shown in a particular order, the order of the actions can be\nmodified.  Thus, the illustrated embodiments can be performed in a different order, and some actions/blocks may be performed in parallel.  The flowchart of FIG. 2 is illustrated with reference to the embodiments of FIG. 1.  So as not to obscure the\nembodiment of this flowchart, details of each method step is not reiterated.\nIn one embodiment, the method comprises recording running values of counters related to data traffic on communication links, for example, links 107.sub.1-N, links between 105 and 106.sub.1-M, links between 103.sub.1 and 106.sub.1-M, and/or DSL\nlinks between 103.sub.2 and 104.  In one embodiment, the running values of the counters include at least one of packet error counts, packet retransmission counts, successful ACK message counts, etc. For example, B1 is the total transmitted bytes recorded\nby the counters.  In such an embodiment, the operational counters increase in count value for successfully delivered packets.  In one embodiment, the communication device (e.g., 103.sub.1 or the optimization center 101) begins to execute active probing. \nIn such an embodiment, active probing data is transmitted from the communication device (e.g., 103.sub.1, 105, 103.sub.2 or the optimization center 101) to another communication device (e.g., 101, 106.sub.1-M, or 104) via respective communication links\n(e.g., links 107.sub.1-N, links between 105 and 106.sub.1-M, and/or links between 103.sub.2 and 104).\nIn one embodiment, after waiting for `t` seconds (e.g., 0.001 seconds to 60 seconds) the operational counter values are read again, for example, a total of B2 transmitted bytes are now recorded from the operational counters.  In one embodiment,\nthroughput is calculated, where throughput=(B2-B1)/t in bytes/second.  The calculated throughput may not be accurate due to any bias in the reported bytes from operational data compared to the actual user data bytes that were used.  Another reason for\ninaccurate calculated throughput may be the reported bytes being much lower than the capacity of the link simply because user did not use the link heavily enough and did not generate enough traffic to cause the counters to increase their values with full\nspeed.  In one embodiment, such bias and inaccuracy in the calculated throughput may be detected by comparing the throughput calculated form operational data with throughput calculated with active probing data.  In such an embodiment, the method\ndiscussed herein can be used to come up with a more accurate throughput estimation algorithm compared to the straightforward but inaccurate method of using (B2-B1)/t.\nAt block 201, the communication device (e.g., one or more of 103.sub.1-N, 105, and/or the optimization center 101) reads operational data associated with the physical or Media Access Control (MAC) address layer (e.g., gateway) of the\ncommunication device.  For example, the communication device 103.sub.2 reads operational data associated with the DSL link between the communication device 103.sub.2 and the CPE 104.\nAt block 202, the communication device executes active probing.  For example, test data is transmitted and received over links 107.sub.1-N, links between devices 105 and 106.sub.1-M, links between 103.sub.1 and 106.sub.1-M. In another example,\ntest data is transmitted and received over DSL links between 103.sub.2 and 104.  In other embodiments, test data from active probing is transmitted and received over other links and other communication devices.\nAt block 203, the communication device 103.sub.2 reads operational data again followed by executing active probing.  In this embodiment, the counter values that correspond to the passive data or operational data are read again and now their\ncontent (counter values) represent a snapshot of network performance.  The counter values may not provide an accurate snapshot of network performance using active probing data in the absence of a trained performance estimation algorithm for the link.\nAt block 204, the Optimization Center 101 uses the counter values (passive data i.e., operational data) along with active data determined by executing active probing to train the performance estimation algorithm 102.  While the embodiments\nherein are explained using the Optimization Center 101 for training the performance estimation algorithm 102, any other communication device (of FIG. 1) in the network may be used for training the performance estimation algorithm 102.  In one embodiment,\nthe communication device 103.sub.2 can be using the data to train the performance algorithm 102.\nIn one embodiment, the Optimization Center 101 continues to refine the performance estimation algorithm 102 using the operational data because the operational data now has more relevant data after having executed active probing that normally\ngenerates full traffic (e.g., by flooding the links).  In such an embodiment, execution of active probing can be limited so that data traffic is not interrupted.  For example, the performance estimation algorithm 102 is updated using operational data\nwhich now provides an accurate estimation of the network performance.\nFIG. 3 is a flowchart 300 for training the performance algorithm for a communication device by a server, according to one embodiment of the disclosure.  As mentioned before, any one of the communication devices 103.sub.1-N may be the server as\nwell.  Although the blocks in the flowcharts with reference to FIG. 3 are shown in a particular order, the order of the actions can be modified.  Thus, the illustrated embodiments can be performed in a different order, and some actions/blocks may be\nperformed in parallel.  The flowchart of FIG. 3 is illustrated with reference to the embodiments of FIGS. 1-2.\nThe flowchart 300 is illustrated with reference to activities performed at the server end 301 and activities performed at the communication end 302.  At block 303 the communication device 103.sub.2 executes active probing.  For example, the\ncommunication device 103.sub.2 sends test data over the communication link 107.sub.2 to the server 101 and then receives the active data from the server 101.  In another example, the communication device 103.sub.2 sends test data over the DSL link to the\nCPE 104, which behaves like a server, and then receives active data from the CPE 104.  In the embodiments discussed herein, any of the communication device may behave as a server to process data (active and/or passive) for updating the performance\nestimation algorithm.\nAt block 304, the communication device 103.sub.2 executes passive probing i.e., reads operational data.  At block 305, the communication device 103.sub.2 transmits operational data to the sever end 301.  For example, the communication device\n103.sub.2 transmits operational data over the communication link 107.sub.2 to the server 101.  In another example, the communication device 103.sub.2 transmits operational data over the DSL link to the CPE 104 which behaves like a server.\nAt block 306, operational data is received by at the server end 301.  For example, operational data is received by the server 101.  In another example, operational data is received over the DSL link to the CPE 104 which behaves as a server.\nAt block 307, the communication device at the server end 301 trains the performance estimation algorithm 102 according to data received from active probing and/or operational data (passive probing data).  At block 308, the trained algorithm is\nsent to the communication device 103.sub.2 which may use that training algorithm to gauge the performance of the communication device 103.sub.2.\nAs discussed herein, performance estimation using operational data is not intrusive as opposed to using traditional network monitoring utilities (NMUs) with active probing.  Operational data are generally readily available and can be used for\ncontinuous updating or training of the performance estimation algorithm and for evaluating network performance.  In one embodiment, accurate NMUs are used intermittently (e.g., once a week) to calibrate, enhance, or fine-tune operational data based\nperformance estimation methods.  In such an embodiment, operational data is used to continuously monitor the network while NMUs are used intermittently to calibrate the performance estimation methods.  The results obtained from the NMUs and the\noperational data can be combined together using a learning-based algorithm.  For example, throughput estimates of the network obtained using operational data can be calibrated by active probing of the network using NMU based techniques.\nIn the situation where each communication link, for example, links 107.sub.1-N, links between 105 and 106.sub.1-M, links between 103.sub.1 and 106.sub.1-M, and/or DSL links between 103.sub.2 and 104 are unique, results from the NMUs and the\noperational data can be used in link-tailored algorithms.\nFor example, a particular link e.g., links 107.sub.1-N, links between 105 and 106.sub.1-M, links between 103.sub.1 and 106.sub.1-M, and/or DSL link between 103.sub.2 and 104, may have very high data traffic which does not allow for frequent\ncalibrations using NMUs because executing NMUs interfere with user traffic.  In such an embodiment, the learning algorithm may combine the occasional result from the NMU and the more frequent results from the operational data (passive data from passive\nprobing) to tune the performance estimation algorithm to suite the particular links' operational data characteristics.  In some embodiments, relevant operational data fields may be unavailable but their absence is accommodated by the occasional per-link\ncalibration using NMU measurement to overcome any limitation from the unavailability of relevant operational data fields.\nIn one example, patterns in the transmission and reception characteristics may be identified using operational data (i.e., passive probing data) and confirmed (or calibrated) using NMUs (i.e., active probing).  In one embodiment, such patterns\nin the transmission and reception characteristics may be based on time, traffic, channel, application, etc. These patterns can also be used for performance estimation.\nIn another example, performance estimation or performance evaluation of a network may be performed in real-time using real-time data by a user of the communication device 103.sub.2.  For example, a user wants to perform self-diagnosis of the\ncommunication device 103.sub.2 may initiate performance estimation which executes active probing and reads operational data.  In another example, a service provider may monitor performance of a network and diagnose a communication link in the network in\nresponse to a help request from a customer.\nFIG. 4 is a processor-based system 400 having machine-readable storage medium 404 with computer executable instructions 102/404a operable to estimate and improve communication system performance algorithm, according to one embodiment of the\ndisclosure.  The storage medium and associated computer executable instructions may be in any of the communication devices and/or servers discussed herein.  The computer-machine-readable/executable instructions 102/404a are executed by a processor 401. \nElements of embodiments are provided as machine-readable medium for storing the computer-executable instructions (e.g., instructions to implement the flowcharts of FIGS. 2-3 and other processes discussed in the description).\nIn one embodiment, the processor-based system 400 further comprises a database 402 to store data used by the instructions 102/404a.  In one embodiment, the processor-based system 400 includes a network interface 405 to communicate with other\ndevices.  In one embodiment, the components of the processor-based system 400 communicate with one another via a network bus 403.\nThe machine-readable storage medium 404 may include, but is not limited to, flash memory, optical disks, hard disk drive (HDD), Solid State Drive (SSD), CD-Read Only Memory (CD-ROMs), DVD ROMs, RAMs, EPROMs, EEPROMs, magnetic or optical cards,\nor other type of machine-readable media suitable for storing electronic or computer-executable instructions.  For example, embodiments of the disclosure may be downloaded as a computer program (e.g., BIOS) which may be transferred from a remote computer\n(e.g., a server) to a requesting computer (e.g., a client) by way of data signals via a communication link (e.g., a modem or network connection).\nReference in the specification to \"an embodiment,\" \"one embodiment,\" \"some embodiments,\" or \"other embodiments\" means that a particular feature, structure, or characteristic described in connection with the embodiments is included in at least\nsome embodiments, but not necessarily all embodiments.  The various appearances of \"an embodiment,\" \"one embodiment,\" or \"some embodiments\" are not necessarily all referring to the same embodiments.  If the specification states a component, feature,\nstructure, or characteristic \"may,\" \"might,\" or \"could\" be included, that particular component, feature, structure, or characteristic is not required to be included.  If the specification or claim refers to \"a\" or \"an\" element, that does not mean there\nis only one of the elements.  If the specification or claims refer to \"an additional\" element, that does not preclude there being more than one of the additional element.\nFurthermore, the particular features, structures, functions, or characteristics may be combined in any suitable manner in one or more embodiments.  For example, a first embodiment may be combined with a second embodiment anywhere the particular\nfeatures, structures, functions, or characteristics associated with the two embodiments are not mutually exclusive.\nWhile the disclosure has been described in conjunction with specific embodiments thereof, many alternatives, modifications and variations of such embodiments will be apparent to those of ordinary skill in the art in light of the foregoing\ndescription.  The embodiments of the disclosure are intended to embrace all such alternatives, modifications, and variations as to fall within the broad scope of the appended claims.\nThe following examples pertain to further embodiments.  Specifics in the examples may be used anywhere in one or more embodiments.  All optional features of the apparatus described herein may also be implemented with respect to a method or\nprocess.\nFor example, in one embodiment a method for performance estimation of a communication device, the method comprises: executing active probing to determine active probing data; reading operational data which includes data related to channel and\nits noise condition and counter values related to user data traffic between the communication device and another communication device, wherein the operational data is relevant to the current settings of the communication device; and training a\nperformance estimation algorithm for the communication device according to the active probing data and the operational data.\nIn one embodiment, the method further comprises: prior to executing active probing, reading operational data.  In one embodiment, reading operational data is performed during or after executing active probing.  In one embodiment, training the\nperformance estimation algorithm comprises: updating the performance estimation algorithm as a function of one or more criteria including at least one of: time of day, time of the week, type of communication device, manufacturer and model of equipment,\nequipment characteristics, firmware, backbone limitations, user's network usage pattern, RF characteristics including at least one of: signal power, path loss, noise level, frequency bands and mode of operation, environment statistics, or data on\noperation of communication devices adjacent to the communication device, wherein the data includes at least one of interference channels and levels.\nIn one embodiment, executing active probing comprises: transmitting active probing data from the communication device to the other communication device; and waiting for a predetermined time before reading the operational data.  In one\nembodiment, executing active probing comprises: transmitting active probing data from the communication device to the other communication device; and receiving a report indicating amount of data or data received by the other communication device.  In one\nembodiment, executing active probing comprises: transmitting traffic from the communication device to the other communication device; and recording measured data associated with the transmitted traffic.\nIn one embodiment, the method further comprises: computing at least one of throughput of the communication device, connectivity, latency, jitter, or error rate using active probing data for training the performance estimation algorithm.  In one\nembodiment, executing active probing is performed fewer times than executing passive probing.\nIn one embodiment, the method further comprises: transmitting the active probing data and read operational data to a server, before, during and/or after executing active probing.  In one embodiment, the server to train the performance estimation\nalgorithm for the communication device according to active probing data and read operational data from the communication device and other communication devices.  In one embodiment, the server to apply a machine learning algorithm for training the\nperformance estimation algorithm for the communication device.\nIn one embodiment, the communication device comprises at least one of: an access point (AP); a base station; a wireless local area network (LAN) device; a digital subscriber line access multiplexer (DSLAM); a gateway; a performance enhancement\ndevice; a Digital Subscriber Line (DSL) Customer Premises Equipment (CPE) modem; an in-home powerline device; a Home Phoneline Network Alliance (HPNA) based device; an in-home coax distribution device; a G.hn (Global Home Networking Standard) compatible\ndevice; an in-home metering communication device; an in-home appliance communicatively interfaced with the LAN; a wireless femtocell base station; a wireless WiFi compatible base station; a wireless mobile device repeater; a wireless mobile device base\nstation; nodes within an ad-hoc/mesh network; a set-top box (STB)/set-top unit (STU) customer electronics device; an Internet Protocol (IP) enabled television; an IP enabled media player; an IP enabled gaming console; an Ethernet gateway; a computing\ndevice connected to the LAN; an Ethernet connected computer peripheral device; an Ethernet connected router; an Ethernet connected wireless bridge; an Ethernet connected network bridge; and an Ethernet connected network switch.\nIn another example, in one embodiment there is a machine-readable storage medium for storing machine-executable instructions that when executed cause a processor to perform a method according to the method discussed herein.\nIn another example, a system comprises: an optimization center communicatively coupled to one or more communication devices, wherein the one or more communication devices are operable to: execute active probing to determine active probing data;\nread operational data which includes data related to channel and its noise condition and counter values related to user data traffic between the communication device and another communication device, wherein the operational data is relevant to the\ncurrent settings of the communication device; and train a performance estimation algorithm for the communication device according to the active probing data and the operational data.\nIn one embodiment, the optimization center is implemented as a server or as a communication device from among the one or more communication devices.  In one embodiment, the one or more communication devices are operable to, prior to executing\nactive probing, read operational data.  In one embodiment, the one or more communication devices are operable to read operational data during or after executing active probing.\nIn one embodiment, the one or more communication devices are operable to train the performance estimation algorithm by updating the performance estimation algorithm as a function of one or more criteria including at least one of: time of day,\ntime of the week, type of communication device, manufacturer and model of equipment, equipment characteristics, firmware, backbone limitations, user's network usage pattern, RF characteristics including at least one of: signal power, path loss, noise\nlevel, frequency bands and mode of operation, environment statistics, or data on operation of communication devices adjacent to the communication device, wherein the data includes at least one of interference channels and levels.\nIn one embodiment, the one or more communication devices are operable to execute active probing by: transmitting active probing data from the communication device to the other communication device; and waiting for a predetermined time before\nreading the operational data.  In one embodiment, the one or more communication devices are operable to execute active probing by: transmitting active probing data from the communication device to the other communication device; and receiving a report\nindicating amount of data or data received by the other communication device.  In one embodiment, the one or more communication devices are operable to execute active probing by: transmitting traffic from the communication device to the other\ncommunication device; and recording measured data associated with the transmitted traffic.\nIn one embodiment, the one or more communication devices are operable to compute at least one of throughput of the communication device, connectivity, latency, jitter, or error rate using active probing data for training the performance\nestimation algorithm.  In one embodiment, the one or more communication devices are operable to execute active probing fewer times than to execute passive probing.\nIn one embodiment, the one or more communication devices are operable to: transmit the active probing data and read operational data to a server, before, during and/or after executing active probing.  In one embodiment, the server is operable to\ntrain the performance estimation algorithm for the communication device according to active probing data and read operational data from the communication device and other communication devices.  In one embodiment, the server is operable to apply a\nmachine learning algorithm for training the performance estimation algorithm for the communication device.\nIn one embodiment, the communication device comprises at least one of: an access point (AP); a base station; a wireless local area network (LAN) device; a digital subscriber line access multiplexer (DSLAM); a gateway; a performance enhancement\ndevice; a Digital Subscriber Line (DSL) Customer Premises Equipment (CPE) modem; an in-home powerline device; a Home Phoneline Network Alliance (HPNA) based device; an in-home coax distribution device; a G.hn (Global Home Networking Standard) compatible\ndevice; an in-home metering communication device; an in-home appliance communicatively interfaced with the LAN; a wireless femtocell base station; a wireless WiFi compatible base station; a wireless mobile device repeater; a wireless mobile device base\nstation; nodes within an ad-hoc/mesh network; a set-top box (STB)/set-top unit (STU) customer electronics device; an Internet Protocol (IP) enabled television; an IP enabled media player; an IP enabled gaming console; an Ethernet gateway; a computing\ndevice connected to the LAN; an Ethernet connected computer peripheral device; an Ethernet connected router; an Ethernet connected wireless bridge; an Ethernet connected network bridge; and an Ethernet connected network switch.\nIn another example, in one embodiment a method for performance estimation of a communication device, the method comprises: receiving operational data including counter values from the communication device after executing active probing and\npassive probing, the counter values related to user data traffic from the communication device to another communication device; and training a performance estimation algorithm for the communication device according to the operational data before or after\nexecuting active probing.\nIn one embodiment, the method further comprises: prior to executing active probing, receiving operational data.  In one embodiment, the operational data is received during or after executing active probing.\nIn one embodiment, training the performance estimation algorithm comprises: updating the performance estimation algorithm as a function of one or more criteria including at least one of: time of day, time of the week, type of communication\ndevice, manufacturer and model of equipment, equipment characteristics, firmware, backbone limitations, user's network usage pattern, RF characteristics including at least one of: signal power, path loss, noise level, frequency bands and mode of\noperation, environment statistics, or data on operation of communication devices adjacent to the communication device, wherein the data includes at least one of interference channels and levels.\nIn one embodiment, executing active probing comprises: transmitting active probing data from the communication device to the other communication device; and waiting for a predetermined time before reading the operational data.  In one\nembodiment, executing active probing comprises: transmitting active probing data from the communication device to the other communication device; and receiving a report indicating amount of data or data received by the other communication device.\nIn one embodiment, the method further comprises: computing at least one of throughput of the communication device, connectivity, latency, jitter, or error rate using active probing data for training the performance estimation algorithm.  In one\nembodiment, executing active probing is performed fewer times than executing passive probing.\nIn one embodiment, the method further comprises: receiving the active probing data and read operational data, the operational data related to user data traffic from the communication device before, during and/or after executing active probing. \nIn one embodiment, training the performance estimation algorithm for the communication device is performed according to active probing data and read operational data from the communication device and other communication devices.  In one embodiment,\ntraining the performance estimation algorithm comprises applying a machine learning algorithm.\nIn one embodiment, the communication device comprises at least one of: an access point (AP); a base station; a wireless local area network (LAN) device; a digital subscriber line access multiplexer (DSLAM); a gateway; a performance enhancement\ndevice; a Digital Subscriber Line (DSL) Customer Premises Equipment (CPE) modem; an in-home powerline device; a Home Phoneline Network Alliance (HPNA) based device; an in-home coax distribution device; a G.hn (Global Home Networking Standard) compatible\ndevice; an in-home metering communication device; an in-home appliance communicatively interfaced with the LAN; a wireless femtocell base station; a wireless WiFi compatible base station; a wireless mobile device repeater; a wireless mobile device base\nstation; nodes within an ad-hoc/mesh network; a set-top box (STB)/set-top unit (STU) customer electronics device; an Internet Protocol (IP) enabled television; an IP enabled media player; an IP enabled gaming console; an Ethernet gateway; a computing\ndevice connected to the LAN; an Ethernet connected computer peripheral device; an Ethernet connected router; an Ethernet connected wireless bridge; an Ethernet connected network bridge; and an Ethernet connected network switch.\nIn yet another example, there is a machine-readable storage medium for storing machine-executable instructions that when executed cause a processor to perform the method discussed herein.\nAn abstract is provided that will allow the reader to ascertain the nature and gist of the technical disclosure.  The abstract is submitted with the understanding that it will not be used to limit the scope or meaning of the claims.  The\nfollowing claims are hereby incorporated into the detailed description, with each claim standing on its own as a separate embodiment.", "application_number": "14414435", "abstract": " Described is a method for performance estimation of a communication\n     device. The method comprises: executing active probing to determine\n     active probing data; reading operational data which includes data related\n     to channel and its noise condition and counter values related to user\n     data traffic between the communication device and another communication\n     device, wherein the operational data is relevant to the current settings\n     of the communication device; and training a performance estimation\n     algorithm for the communication device according to the active probing\n     data and the operational data.\n", "citations": ["5720003", "8208604", "8544087", "9479341", "20080052394", "20080205501", "20090161741", "20110202645", "20120151035"], "related": []}, {"id": "20160065903", "patent_code": "10375357", "patent_name": "Method and system for providing at least one image captured by a scene\n     camera of a vehicle", "year": "2019", "inventor_and_country_data": " Inventors: \nWang; Lejing (Munich, DE), Alt; Thomas (Pullach, DE)  ", "description": "<BR><BR>BACKGROUND OF THE INVENTION\n1.  Technical Field\nThe present disclosure is related to a method and system for providing at least one image of at least one real object captured by at least one scene camera of a plurality of scene cameras mounted on a vehicle.\n2.  Background Information\nIn a potential situation, a person may be interested in an object of a real environment (often called \"object of interest\"), e.g. in a surrounding environment.  The person (herein also referred to as user) may be interested in further\nidentifying the object of interest and/or in determining similar objects.  In a particular example, the user may want to find out if the same object (also called item) and/or similar objects are available for purchasing.  The user may then wish to make\nan order for purchasing one or more objects (e.g. the same or a similar object) or may then wish to find a store for hands-on checking and purchasing the one or more objects.\nFor this purpose, for example, the user may use a camera to capture an image of the object of interest and identify the same or similar items based on image analysis.\nU.S.  Pat.  No. 8,228,364 B2 discloses a method and system for an omnidirectional camera which can be used to record an event happening around a police vehicle.  The system and method include an omnidirectional camera and a digital processor\nthat processes the images taken by the camera.  The direction of the signal generated by the microphone determines the region of interest.\nU.S.  Pat.  No. 6,580,373 B1 discloses a vehicle-mounted image record system for encouraging safe driving of a vehicle by recording images of the surface of the road and part of the vehicle.  The system includes one or more cameras mounted on\nthe vehicle and a recording device that records the images captured by the cameras.  In the event of an accident the recorded images can be used as proof of safe driving.\nU.S.  Pat.  No. 7,119,832 B1 discloses an in-car video system where a wireless microphone is configured with bi-directional communications capability.  When an RF activation signal is received, the wireless microphone is automatically switched\non to capture an audio soundtrack that accompanies the images captured by the car-mounted video camera.  A wireless microphone controller mounted in the car transmits the RF activation signal to the wireless microphone.  When the video recording device\nstarts recording, the wireless microphone controller transmits the RF activation signal.\nFurther, there is known a mobile phone application (so-called \"App\") called \"ASAP54\" that allows the user, upon seeing a particular outfit or style the user is interested in on a real person or magazine, to take a photograph of the outfit.  By\nusing image recognition the application can find that piece of clothing the user is looking at and a number of similar fashions.  For employing this, the user is required to focus the camera of his/her mobile phone on the object of interest.\n<BR><BR>SUMMARY\nIt would be desirable to provide a method and system which facilitate for a user to retrieve information on a region of interest in the surrounding real environment of the user which can be electronically stored and processed for later use.\nAccording to a first aspect, there is disclosed a method of providing at least one image of at least one real object captured by at least one scene camera of a plurality of scene cameras mounted to a vehicle, the method comprising providing\ncamera poses of respective scene cameras of the plurality of scene cameras relative to a reference coordinate system associated with the vehicle, providing user attention data related to at least one user captured by an information capturing device,\nproviding at least one attention direction relative to the reference coordinate system from the user attention data, determining at least one of the scene cameras among the plurality of scene cameras according to the at least one attention direction and\nthe respective camera pose of the at least one of the scene cameras, and providing at least one image of at least one real object captured by the at least one of the scene cameras.\nAccording to another aspect, there is disclosed a system for providing at least one image of at least one real object, comprising at least one processing device coupled to a plurality of scene cameras mounted to a vehicle, and to an information\ncapturing device configured to capture user attention data related to at least one user.  The at least one processing device is configured to provide camera poses of respective scene cameras of the plurality of scene cameras relative to a reference\ncoordinate system associated with the vehicle, to provide at least one attention direction relative to the reference coordinate system from the user attention data, to determine at least one of the scene cameras among the plurality of scene cameras\naccording to the at least one attention direction and the respective camera pose of the at least one of the scene cameras, and to provide at least one image of at least one real object captured by the at least one of the scene cameras.\nFor example, the at least one processing device according to the present disclosure, which may comprise one or more processing devices such as one or more microprocessors, is comprised, at least in part, in a mobile device (such as a mobile\nphone, wearable computer, tablet computer, mobile computer, often called laptop, or a head mounted display, such as used for optical see-through augmented reality applications), in the vehicle, and/or in a server computer adapted to communicate with the\nmobile device and/or the vehicle.  The at least one processing device may be comprised in only one of these devices, e.g. in the mobile device or in the server computer, or may be a distributed system in which one or more processing tasks are distributed\nand processed by one or more processing devices of a processing system which are distributed and are communicating with each other, e.g. by point to point communication or via a network.\nAny steps, embodiments, aspects and examples described herein with respect to the method can equally or analogously be implemented by the at least one processing device being configured (by software and/or hardware) to perform the respective\nsteps, embodiments, aspects or examples.  Any used processing device, such as one or more microprocessors, may be configured as such by software and/or hardware and communicate via a communication network, e.g. via a server computer or a point to point\ncommunication, with one or more cameras, displays, sensors and/or any other components disclosed herein.\nAccording to another aspect, the disclosure is also related to a computer program product comprising software code sections which are adapted to perform a method according to the invention, particularly as set out in any one of the claims. \nParticularly, the software code sections are contained on a computer readable medium which is non-transitory.  The software code sections may be loaded into the memory of one or more processing devices (such as microprocessors) as described herein.  Any\nused processing devices, such as one or more microprocessors, may communicate via a communication network, e.g. via a server computer or a point to point communication, with other devices as described herein. <BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nAspects and embodiments of the invention will now be described with respect to the drawings, in which:\nFIG. 1 shows a flow diagram of a method according to an embodiment of the invention.\nFIG. 2 shows an exemplary scenario according to an embodiment of the invention for providing at least one image of at least one real object captured by at least one camera mounted on a vehicle.\nFIG. 3 shows an embodiment of a system setup which may be used in connection with aspects of the invention.\nFIG. 4 shows another exemplary scenario according to an embodiment of the invention,\nFIGS. 5A, B show further exemplary scenarios according to embodiments of the invention.\nFIG. 6 shows another exemplary scenario according to an embodiment of the invention.\nFIG. 7 shows a flow diagram of a method according to a further embodiment of the invention.\nFIG. 8 shows a scenario of a user holding a mobile device that has a front facing camera, a back facing camera, and a display screen for recognizing, tracking and/or reconstructing an object of interest.\nFIG. 9 shows another scenario of a user holding a mobile device that has a front facing camera, a back facing camera, and a display screen for recognizing, tracking and/or reconstructing an object of interest.\nFIG. 10 shows a flow diagram of a method according to an embodiment of the invention,\nFIG. 11 shows a flow diagram of a method according to another embodiment of the invention.\n<BR><BR>DETAILED DESCRIPTION\nNowadays, people often spend a lot of time in a vehicle, for example when they are on the way to shopping, commuting or sightseeing.  Potentially, there may be many different objects (e.g. pedestrians, clothes worn by pedestrians, advertisement\nposters, real stores, etc.) in the surroundings of the vehicle when it travels along its way.  It would be difficult and impractical for people sitting in a vehicle to use, e.g., a mobile device equipped with a camera (e.g. a standard camera or a mobile\nphone with a camera) for capturing an image of an object of interest of an environment surrounding the vehicle.  This is particularly the case for a driver who is driving the vehicle.  The driver would not be able and allowed to hold the mobile device to\ncapture an image during driving.\nIn a potential exemplary scenario, in which a person is driving a car from one location to another, he or she may find an object of interest, for instance a skirt worn by a pedestrian walking on a sidewalk next to the vehicle.  The inventors\nhave found that it is beneficial to employ cameras mounted on the vehicle to capture an image containing the object of interest, instead of the driver holding a camera for capturing an image.\nIncreasingly, multiple cameras are mounted on vehicles, such as cars.  The inventors further considered that images captured by each of the car mounted cameras and the processing thereof would increase complexity of object detection and/or\nrecognition.  Thus, the inventors found that it would be beneficial to determine a subset (i.e. one or more) of the vehicle mounted cameras and further process or analyze only images captured by the determined subset of the vehicle mounted cameras.\nAspects of the present disclosure are related to a method and system for determining at least one of a plurality of vehicle mounted cameras for capturing at least one image of a part of a real environment based on the direction and/or position\nof the user's attention while being in the vehicle.  At least one image of an environment captured by the determined at least one camera can be used to determine one or more items (i.e. objects of interest) of the real environment.\nThe provided user attention data, e.g. captured by an information capturing device, according to the present invention are related to the user.  Particularly, the user attention data are indicative of a user's attention towards a real object\n(particularly the object of interest) or a part of the real environment containing the real object.\nAccording to an embodiment of the invention, as set out in more detail below, it is possible to look at an object of interest in the surroundings of the vehicle and to activate one or more vehicle cameras, e.g. by using voice, or gesture, or\ngaze direction, to capture an image of the object of interest (e.g., a piece of clothing), and then to determine whether the object of interest or similar items are available for purchasing.\nFIG. 1 shows a flow diagram of a method according to an embodiment of the invention providing at least one image of at least one real object captured by at least one camera mounted on a vehicle and further shows optional steps related to\nexemplary applications based on objects of interest determined according to the at least one image.  FIG. 2 shows an exemplary scenario according to an embodiment of the invention for providing at least one image of at least one real object captured by\nat least one camera mounted on a vehicle.  FIG. 3 shows an embodiment of a system setup which may be used in connection with aspects of the invention.\nIn the scenario of FIG. 2, a vehicle, in this example a car 201, is driving through a real environment 220.  The real environment 220 includes real objects fixed in the environment, e.g. a gas station 221, a tree 222, a building 223, a park sign\n224, and a road 227.  The real environment 220 may also include movable real objects, like a person 226 and a person 225.  The car 201 may also be considered as a part of the real environment 220.  The car 201 is equipped with multiple scene cameras\n211-214 mounted on the car.  The driver 202 is looking or facing or pointing toward the direction 203.\nFIG. 3 shows an embodiment of an interior setup for the car 201.  There are provided two information capturing devices mounted in the car 201.  In principle, one may suffice.  In this example, the information capturing devices are comprising\ncameras, herein called user cameras 205 and 206.  The car 201 may further be equipped with a communicating device 207, such as a wireless communication device (e.g. WLAN device or SIM card device), and a processing device 208, such as a microprocessor. \nAll the steps or a part of the steps disclosed in this disclosure may be performed by the processing device 208 alone or in combination with any other processing device.  All the steps or a part of the steps may also be performed by a remote processing\ndevice that is separate to the car 201, such as a server computer or a mobile device.  In the present case, the car 201 may communicate with the remote processing device through the communicating device 207 via cable or wirelessly.  The remote processing\ndevice may be a server computer 301 (e.g. a workstation) or a mobile device, e.g. a mobile phone 302.\nAccording to the flow diagram of FIG. 1, step 101 captures user attention data related to a user (in FIG. 2, the driver 202) by an information capturing device (such as one of the cameras 205, 206 according to FIG. 3).  The user attention data\nmay be any data that represents at least one aspect of a user attention.  Particularly, the user attention data represents or encodes information related to at least one direction, at least one position, and/or at least one indicated space or area of the\nuser attention.\nThe user attention may be indicated by a gaze (or a stare).  A direction from the user's eyes to where the eyes are looking may represent the gaze direction, which may be considered as an attention direction.  Further, a field of view of one eye\nor two eyes of the user represents a space of the user attention of the gaze.  A position where the user is looking at represents a position of the user attention.\nIn another example, the user attention may also be indicated by the user's face (e.g. a pose of the face or head).  The pose of the user's face may represent where the user is focusing.  At least one attention direction may be derived from the\npose of the face.  In one implementation, the at least one attention direction may be the same as the normal direction of the frontal face.\nIn a further example, the user attention may be indicated by finger pointing or any gesture indicative of directions, positions, and/or areas.  An attention direction may be modeled by a direction axis.  For example, the direction axis may be\nrepresented by a 2-vector.  Further, the attention direction may be modeled by a field of view.  For example, the user's position may determine the view point, and the field of view of the user's eye may define an attention direction.  In another\nexample, standard deviations of the direction axis (e.g. estimated errors of the frontal face direction from the face pose estimation) may determine an angle (e.g. vertical or horizontal range) for the field of view, which may be considered as an\nattention direction.\nFor example, the user attention data comprises at least one of, but is not limited to, one or more images captured by one or more cameras, a bioelectric signal (e.g. electrooculogram), and a mechanical signal (e.g. hand pressure).\nIn one embodiment, the information capturing device may comprise a camera device called user camera.  The user camera may capture at least one user image of at least part of the user.  For example, the camera 205 (i.e. a user camera) mounted on\nthe car 201 may capture an image (i.e. a user image) of the front face of the user 202, as shown in FIG. 3.\nIn another embodiment, the information capturing device may be an eye tracking device.  The eye tracking device (also called eye tracker) may measure the orientation of one or two eyes of the user and, thus, can provide gaze directions of the\nuser.  There are different types of eye tracking methods, like eye-attached tracking, optical tracking, and electric potential measurement.  The eye-attached tracking may be implemented as special contact lens with an embedded sensor (like mirror or\nmagnetic field sensors).  The optical tracking can employ cameras to capture images of the eyes and determine the eye orientation from the images, for example as disclosed in Kaminski, Jeremy Yrmeyahu, DotanKnaan, and AdiShavit.  \"Single image face\norientation and gaze detection.\" Machine Vision and Applications 21.1 (2009): 85-98 (hereinafter \"Kaminski et al.\").  The electric potential measurement devices can measure electric potentials with electrodes placed around the eyes.  One technique called\nelectrooculography (EOG) system can measure electric potentials (the measured signal called electrooculogram).  Bulling et al. present a wearable EOG goggle; e.g. see Bulling, Andreas, Daniel Roggen, and Gerhard Troster.  \"Wearable EOG goggles: Seamless\nsensing and context-awareness in everyday environments.\" Journal of Ambient Intelligence and Smart Environments 1.2 (2009): 157-171.\nIn another embodiment, the information capturing device may be a mechanical sensor, like a pressure or force sensor.  For example, it measures force or pressure applied by the user.  The mechanical sensor may be a mechanical joystick.\nStep 102 provides at least one attention direction of the user relative to a reference coordinate system associated with a vehicle, wherein the at least one attention direction is derived from the user attention data.  For example, the attention\ndirection 203 of the user 202 (i.e. driver) could be determined or defined in the reference coordinate system 209 associated with car 201.  The attention direction 203 may indicate a gaze direction or face direction of the user 202, which may be\nestimated from an image (i.e. the user attention data) of the face of the user 202 captured by the camera 205 (i.e. the information capturing device) mounted in the car 201.\nIn some implementations, attention direction information may be contained in the captured user attention data.  For example, an eye tracker may provide a gaze direction in the output signals.  In other implementations, attention direction\ninformation may be derived (e.g. estimated) from the captured user attention data.  For example, when a camera is used to capture one or more images of the user's face or eyes, the face pose or the gaze direction may have to be estimated from the camera\nimages based on a computer vision method like that disclosed in Kaminski et al. or in Fanelli, Gabriele, Juergen Gall, and Luc Van Gool.  \"Real time head pose estimation with random regression forests.\" Computer Vision and Pattern Recognition (CVPR),\n2011 IEEE Conference on.  IEEE, 2011 (hereinafter \"Fanelli et al.).  According to the invention, the step of providing at least one attention direction relative to the reference coordinate system from the user attention data shall encompass all of these\nimplementations and embodiments.\nThe attention direction may be determined relative to the information capturing device.  In order to have the attention direction in a reference coordinate system associated with the vehicle, a device spatial relationship, e.g. 6DOF (degrees of\nfreedom) rigid transformation, between the vehicle and the information capturing device may be required.  Then, the attention direction relative to the reference coordinate system may be determined from the attention direction relative to the information\ncapturing device and the device spatial relationship.\nThe device spatial relationship may be determined from a calibration procedure.  The calibration procedure is, for example, a mechanical calibration.  For instance, the information capturing device (e.g. the camera 205) may be mounted at a known\npose in the reference coordinate system of the vehicle (e.g. the car 201) using mechanical arms.  The camera 205 may also be mounted at an arbitrary pose (i.e. unknown at the moment of the mounting).  In this case, the camera 205 could capture an image\nof a part of the car 201.  The image of the part of the car can be used to estimate the device spatial relationship based on a computer vision method (e.g. feature based pose estimation).  It is also possible to use another tracking system to determine\nthe device spatial relationship between the vehicle and the information capturing device.  The tracking system may be a mechanical arm, an optical camera system, or a magnetic tracking system, or any motion or position sensor (e.g. gravity sensor,\naccelerometer, GPS).\nIn one embodiment, the information capturing device is or comprises one or more cameras.  As shown in FIG. 3, the camera 205 (and 206) mounted inside the car 201 is part of the information capturing device.  For example, the camera 205 has a\nknown device spatial relationship with the reference coordinate system 209 associated with the car 201.  It is possible to capture a user image of at least part of the face of the user 202.\nThe pose of the user's face can be estimated from the user image based on various computer vision methods (like proposed in Fanelli et al.).  From the face pose, a direction of the frontal face can be determined as an attention direction.  The\nattention direction 203 shown in FIG. 2 may represent the face direction.\nAccording to an embodiment, multiple attention directions are determined.  When the user image contains multiple faces (of multiple users sitting in the car), multiple face poses may be determined.  In another example, the face of the user may\nmove, and then multiple face directions may be determined for the same face.  In this case, it is possible to estimate one main direction from the multiple attention directions.  Different mathematical methods can be employed to determine a main\ndirection from multiple directions.  For example, each direction could have an angle relative to a common coordinate system.  The main direction may be determined by an angle, which may be computed as an average, maximum, minimum, median or mean of the\nangles associated with the multiple directions.  In FIG. 2, the attention direction 203 may be one direction estimated based on one user or a main direction based on multiple attention (e.g. face and/or gaze) directions estimated from one or more user\nimages of one or more users.\nIt is also possible to estimate a gaze direction from the user image of least part of the face of the user 202 (e.g. as proposed in Kaminski et al.).  The attention direction 203 shown in FIG. 2 can represent the gaze direction.  The gaze\ndirection of the user 202 may also be estimated from an eye tracker.\nFurther, when one or more user images capture at least part of a hand or an arm of the user, a hand pose can be estimated from the one or more user images according to any appropriate vision based method (e.g. like that proposed in de La Gorce,\nMartin, David J. Fleet, and Nikos Paragios.  \"Model-Based 3D Hand Pose Estimation from Monocular Video\" or Erol, Ali, et al. \"Vision-based hand pose estimation: A review.\" Computer Vision and Image Understanding 108.1 (2007): 52-73 (hereinafter \"Erol et\nal.\")).  A hand gesture (or generally a gesture) may also be estimated, like according to a method as proposed in Erol et al. A direction (e.g. a pointing direction) as an attention direction may be derived from the hand pose.  A hand pointing direction,\nor generally a gesture, may also be computed from the one or more user images.  The attention direction 203 shown in FIG. 2 may represent such hand pointing direction.\nAccording to an embodiment, face poses, gaze directions, and/or hand poses are estimated for one or more users from the same user attention data (e.g. from the same user image captured by a camera).  Multiple directions (e.g. at least two of\nface directions, gaze directions, and hand directions) may be determined.  The attention direction 203 may be determined as one of the multiple directions or as a main direction estimated based on the multiple directions.\nIn another embodiment, the camera 205 has unknown device spatial relationships with respect to the reference coordinate system 209 when a user image is captured by the camera 205.  It is possible to determine the attention direction 203 relative\nto the reference coordinate system from the user image captured by the camera 205 without knowing the device spatial relationship.  In an example, the user image captured by the camera 205 contains at least part of the car and at least part of the user. \nThe at least part of the car has a known pose relative to the reference coordinate system of the car.  The at least part of the user may contain the user face.  In this case, the pose of the user's face or gaze direction can be determined in the\nreference coordinate system of the car based on a computer vision method using pixel information of the captured user image.\nStep 103 provides a plurality of scene cameras mounted to the vehicle at respective camera poses relative to the reference coordinate system.  In the embodiment shown in FIG. 2, four scene cameras 211-214 are mounted to the car 201, and the\nrespective camera poses of the scene cameras 211-214 are known in the reference coordinate system 209.\nStep 104 determines at least one of the scene cameras among the plurality of scene cameras according to the at least one attention direction and at least one respective camera pose.  For example, it is possible to determine at least one of the\nscene cameras 211-214 as desired scene camera(s) according to the attention direction 203 and the respective camera poses of at least part of the scene cameras 211-214.  One thought behind this is that attention directions of the user and/or positions of\nthe user could indicate where or in which region or along which direction an object of interest locates in the real environment.\nIn an embodiment, multiple scene cameras mounted to the vehicle capture different regions of the real environment.  For example, the scene cameras 211-214 capture different regions of the real environment 220 (e.g. four different sides) around\nthe car 201.  For example, they are arranged facing perpendicularly to one another, thus being directed towards four perpendicular sides.  At least one attention direction of the user may be used to determine at least one scene camera among the scene\ncameras.  The determined scene camera(s) could capture at least one scene image that may contain at least part of the object of interest indicated by the at least one attention direction.\nThe at least one scene image may be processed by various computer vision methods in order to recognize an object of interest, determine objects similar to the object of interest, reconstruct the 3D geometry of the object of interest, determine\nthe position of the object of interest relative to the car or to the real environment, and/or determine the position of the vehicle in the real environment.  Further, an attention direction relative to the car or relative to the scene camera may be\nconsidered in the computer vision methods.  For example, the attention direction may be used to determine a region of interest in the at least one scene image.  In another example, the attention direction may be used to determine image features based on\ndistances between the corresponding features and the attention direction in 3D space or image distances between the image features and the image projection of the attention direction in the captured image.\nA scene camera determined from attention directions:\nIt is possible to determine a scene camera (called desired scene camera) among the plurality of scene cameras according to an attention direction of the user.  Further, multiple desired scene cameras among the plurality of scene cameras could\nalso be determined similarly according to methods mentioned below.  In one implementation, spatial relationships between the attention direction and each respective camera direction of at least part of the plurality of scene cameras is evaluated to\ndetermine a desired scene camera.  The camera directions can be derived from related camera poses.  A camera direction may indicate the direction of a respective camera optical axis.\nIn an example, a spatial relationship between the attention direction and a respective camera direction is an angle between the two direction axes.  A threshold value may be given, and then a scene camera may be determined as a desired scene\ncamera if the related angle is below the threshold.  It is also possible to select a scene camera as a desired scene camera if the angle related to the selected scene camera is the smallest among angles related to the at least part of the scene cameras.\nIn another example, a spatial relationship between the attention direction and a respective camera direction is defined as an intersection.  A camera direction may be defined as it originates from the position of the camera.  It is also possible\nto select a scene camera as a desired scene camera if the direction axis of the selected scene camera intersects the attention direction.  Further, when the attention direction intersects the direction axes of multiple scene cameras, the multiple scene\ncameras can be determined as desired scene cameras.\nIn a further embodiment, a desired scene camera is determined according to spatial relationships between the attention direction and each respective camera position of at least part of the plurality of scene cameras.  A camera position may be\nderived from a camera pose of a related scene camera.  A spatial relationship between the attention direction and a respective camera position may be defined as a distance from the camera position to the attention direction axis or a distance from the\ncamera position to the user.\nIn a further embodiment, a desired scene camera is determined according to spatial relationships between the attention direction and each respective camera pose (including both direction and position) of at least part of the plurality of scene\ncameras.\nIn a further embodiment, a capturing coverage (e.g. field of view of a camera or manually defined) of a scene camera is provided.  As shown in FIG. 2, the scene camera 211-214 cover front left, back and right of the car 201, respectively.  When\nthe attention direction is determined to point right of the car 201, then the scene camera 214 is determined as a desired camera.\nIn another embodiment, a desired scene camera is determined according to spatial relationships between the attention direction and each respective camera field of view of at least part of the plurality of scene cameras.  For example, a spatial\nrelationship between the attention direction and a respective camera field of view may be defined as intersects, (partially) covered by, or (partially) covers.  A scene camera may be determined as a desired scene camera if the related camera field of\nview covers the attention direction or has the largest cover (i.e. an uncovered part of the attention direction is the smallest) or covers a certain part (e.g. a certain part beginning from the user position) of the attention direction among the at least\npart of the scene cameras.  In another implementation, when the attention direction is covered by multiple scene cameras, the multiple scene cameras may be determined as desired scene cameras.  When depth information is available for the scene cameras,\nthe camera field of view may be limited to a certain depth based on the depth information, as objects behind the certain depth with respect to the camera would be occluded and not be captured in images.\nOne or more embodiments of determining at least one scene camera as at least one desired scene cameras disclosed herein could be combined for the determination.\nIn another embodiment, a scene camera may be an omni-camera (or a wide-angle camera) mounted to the car.  The at least one attention direction may be used to determine a region of interest in at least one scene image captured by the omni-camera.\nIn a further embodiment, it is possible to determine a scene camera (called desired scene camera) according to multiple attention directions.  The multiple attention directions may come from one user or different users.  The multiple attention\ndirections may be obtained from the same or several different user attention data.\nIn one implementation, a desired attention direction (may be or may be not one of the multiple attention directions) may be estimated from the multiple attention directions.  Each respective attention direction of the multiple attention\ndirections has an angle relative to a common axis in a common coordinate system (e.g. the reference coordinate system of the car).  The desired attention direction may be estimated to have a maximum, minimum, average, mean, or median angle based on the\nangles of the multiple attention directions.\nThe multiple attention directions may be clustered or grouped (e.g. according to their angles or direction axes).  A desired attention direction may be estimated from a group with the majority of the multiple attention directions.\nIn a further implementation, for each respective attention direction of the multiple attention directions, it is possible to determine an angle, a distance, a covered region relative to a scene camera according to the embodiments mentioned\nabove.  For the scene camera, statistics related to angles, distances, covered regions of the multiple attention directions may be calculated.  At least one scene camera may be selected (i.e. determined) from the at least part of the scene cameras\naccording to the calculated statistics.  For example, a sum of the angles of the multiple attention directions related to a scene camera may be calculated.  One or more scene cameras having minimal values may be determined as desired cameras.\nStep 105 provides at least one scene image of at least one real object captured by the determined at least one scene camera.  In an embodiment, the plurality of scene cameras capture a plurality of scene images.  Then, at least one scene image\ncaptured by the determined at least one scene camera (i.e. desired scene camera) is provided.  For example, each of the scene cameras 211-214 captures a respective scene image.  The scene camera 214 may be determined as the desired scene camera according\nto the attention direction 203.  The scene image captured by the scene camera 214 may be provided for further processing.\nIn an embodiment, the step of capturing the plurality of scene images and the step of capturing the user attention data may be synchronized.\nIn another embodiment, the step of capturing the plurality of scene images and the step of capturing the user attention data are not synchronized.  For example, the user attention data may be captured and at least one attention direction is\nestimated.  Then, at least one scene camera is determined according to the at least one attention direction.  Afterwards, the determined at least one scene camera captures at least one scene image.\nThere are many computer vision applications designed for use in or with vehicles which could take advantages of the at least one scene image captured by the determined at least one scene camera mounted to the vehicle.\nIn an example, it is possible to determine at least one object of interest according to the at least one scene image as shown in the optional step 106.  Different potential embodiments related to the determination of the at least one object of\ninterest are described together with FIG. 7 (e.g. for step 702).\nIt is also optional to perform step 107 to generate purchasing information related to the at least one object of interest.  One specific potential embodiment is described together with FIG. 7.\nIt is further optional to perform step 108 to integrate the at least one object of interest to a digital map.\nWhen a user is driving a car, he or she normally relies solely on his/her ability to remember objects of interest surrounding the car in the environment, e.g. locations of shops of interest, or gas stations with exceptional prices, or the first\nfree parking lot seen for a while.  A problem is that a human's memory cannot always be trusted, and locations and details get lost.\nAccording to embodiments, it is possible to add information related to the determined at least one object of interest to a digital map in order to customize the digital map.  The information related to the determined at least one object of\ninterest could be location, name, type of the object of interest.  The location may be derived from the current location of the car (e.g. from GPS).  The location may be further improved (e.g. improving its accuracy) by considering the attention\ndirection and depth information along the attention direction.  The depth information may be provided from a depth sensor or from two attention directions or from two optical cameras.  For example, the two attention directions may be captured when the\nvehicle at two positions, which is described in detail below.\nA name or a type of the object of interest may be determined from image classification methods based on the captured at least one scene image.  For this, known reference image features or objects may be used for the classification.\nAccording to an embodiment, the at least one scene image may be captured after the determination of the at least one scene camera, after the determination of the at least one attention direction, or after the capture of the user attention data. \nOften, computation and/or processing time is required for the determination of the at least one scene camera, the determination of the at least one attention direction, and/or the capture of the user attention data.  A lag between the capture of the user\nattention data and the capture of the at least one scene image may exist.  In reality, the vehicle may move.  Therefore, based on embodiments disclosed above, the at least one scene image (i.e. the desired scene camera) may not capture an object of\ninterest indicated by the at least one attention direction provided from the user attention data, and/or the at least one attention direction may not correctly indicate a region of interest where the object of interest is contained in the at least one\nscene image.\nAccording to the example of FIG. 4, the vehicle is at a first vehicle position where the user attention data is captured.  As shown in FIG. 4, the car 201 is at a first position 441 where the user looks at the person 226 and the user attention\ndata is captured.  The user attention data is indicative of the user attention direction 203 that may indicate an object of interest (e.g. the person 226).  A first coordinate system may be derived from the reference coordinate system of the vehicle at\nthe first vehicle position 441.  In FIG. 4, the reference coordinate system 209 is the first coordinate system.\nAt a later time, the vehicle is at a current vehicle position (i.e. a second vehicle position) where the at least one scene image is captured.  A second coordinate system may be derived from the reference coordinate system of the vehicle at the\ncurrent vehicle position.  As shown in FIG. 4, the car 201 is at the current position 442, and the reference coordinate system 449 is the second coordinate system.  During the computation and/or processing period for the determination of the at least one\nscene camera, the determination of the at least one attention direction, and/or the capture of the user attention data, the car 201 has moved from the first position 441 to the current position 442.  Based on embodiments disclosed above, the attention\ndirection 443 in the reference coordinate system 449 of the car 201 is determined.\nThe scene camera 214 may be determined as a desired scene camera accordingly.  Then, a scene image is captured by the scene camera 214.  However, the attention direction 443 does not accurately indicate the object of interest (e.g. does not\nindicate the person 226).  When the car is at the current position 442, the scene image captured by the scene camera 214 may not contain the object of interest (e.g. the person 226).  Similarly, if the car 201 is equipped with an omni-camera, a region of\ninterest in an image of the omni-camera may be determined by the attention direction 443 and the determined region of interest may not contain the object of interest (e.g. the person 226).\nIn order to address the problems discussed above, according to an embodiment, a vehicle spatial relationship between the vehicle at the first vehicle position 441 and the vehicle at the current vehicle position 442 is considered to determine at\nleast one attention direction and/or at least one scene camera.  The vehicle spatial relationship can represent a distance and/or a rotation between the vehicle at the first vehicle position and the vehicle at the current vehicle position.\nFor example, the vehicle spatial relationship is determined or partially determined according to, but is not limited to, a GPS device, an odometer, a compass, an accelerometer, an inertial sensor, a camera or their combinations, mounted to or\ncontained in the vehicle.  For example, a vision based tracking method may analyze one or more images captured by at least one scene camera of the vehicle in order to estimate the motion of the vehicle (from which the vehicle spatial relationship may be\nderived).  Further, the vehicle spatial relationship may be obtained from the speed of the vehicle or the GPS positions and/or orientations of the vehicle (e.g. a compass sensor).\nHaving the vehicle spatial relationship, the attention direction 443 estimated in the coordinate system 449 associated with the car 201 at the position 442 can be transformed in order to obtain the attention direction 203.  The attention\ndirection 203 may also be expressed in the coordinate system 449 to determine the at least one scene camera among the scene cameras 211-214 when the car 201 is at the position 442.  In the example shown in FIG. 4, the scene camera 213 may be determined\nas the desired scene camera, as the field of view of the scene camera 213 contains a certain beginning part (e.g. between 4 and 0.5 meters from the user position) of the attention direction 203.  Then, the scene camera 213 is used to capture a scene\nimage that includes the object of interest 226.\nBlind Spots:\nIt is also possible that none of the scene cameras mounted to the vehicle satisfies a criterion of the desired scene cameras.  Blind spots may exist for the cameras mounted to the vehicle.  The area of the blind spots in the vehicle coordinate\nsystem may be provided.  An example is shown in FIG. 5A, in which the object of interest 521 of the real environment is not covered by any of the scene cameras mounted on the car being at the position 541.  In this example, the attention direction 503\nrelated to the object of interest 521 is determined in the coordinate system 509.  In one example, none of the scene cameras 211-214 satisfies a criterion of a desired scene camera when the car 201 is at the position 541.  For instance, when the car 201\nis at the position 541, none of the scene cameras 211-214 has an optical axis with an angle below a certain threshold relative to the attention direction 503, and/or none of the scene cameras 211-214 has a field of view containing a certain part of the\nattention direction 503.  Thus, it is determined that none of the scene cameras can capture the object of interest 521, for example the petrol sign, indicated by the attention direction 503.\nIn another example, as the area of the blind spots in the coordinate system 509 is known, it could be directly determined that a certain part (e.g. between 4 and 0.5 meters from the user position) of the attention direction 503 is not covered by\nfield of view of any scene cameras.  From this, it could be also determined that none of the scene cameras can capture the object of interest 521.\nIn order to address the problems mentioned above, a scene image might be captured when the vehicle arrives at another position.  For example, as shown in the FIG. 5A, the car 201 moves to the position 542.  At the position 542, the scene camera\n213 may capture a scene image 531 that includes at least part of the object of interest 521 (e.g. the petrol sign).  The position 542 may be unknown when the attention direction 503 is determined and the car is at the position 541.\nThe position 542 (equivalent to a vehicle spatial relationship between the car at the position 541 and at the position 542) is, for example, determined first, and at least one desired scene camera is also determined together with the position\n542.  Then, at least one scene image is captured by the determined at least one scene camera when the car 201 is at the determined position 542.\nOne or more criteria of the determination of at least one desired scene camera, as disclosed above, are provided in order to determine the position 542 and the at least one desired scene camera.  For example, the criteria may include, but is not\nlimited to, spatial relationships between the attention direction and camera poses of the scene cameras and/or spatial relationships between the attention direction and the field of view of the scene cameras.\nIn one implementation, the position 542 and/or at least one desired scene camera may be determined in real time during the movement of the car 201.  For a new position of the car 201, it is possible to determine if one or more scene cameras\nsatisfy the criteria.  In this case, the attention direction 503 is provided in a common coordinate system with the scene cameras mounted to the car being at the new position.  For this, a transformation between the car being at the new position and at\nthe position 541 may be required, which could be estimated from methods mentioned above.  When at least one scene camera satisfies the criteria, the at least one scene camera is determined as the desired scene camera to capture at least one scene image,\nand then the new position is determined to be the position 542.\nIn another implementation, the position 542 and/or at least one desired scene camera are pre-determined according to the position 541.  For example, it is possible to test a position for the car and check if one or more scene cameras satisfy the\ncriteria when the car at that position.  An environment map (e.g. a city street map) and/or a moving direction of the car may also be considered to choose the position.\nConsideration of rear mirror for the determination of attention directions:\nRear mirrors may also be considered for determining the at least one attention direction.  For example, the vehicle often has three mounted rear mirrors.  Normally, the user (e.g. a passenger or a driver) may look at one of the rear mirrors of\nthe vehicle in order to look at surrounding objects of interest.  Therefore, an attention direction (e.g. a gaze direction or face direction) towards the mirror may not be considered, while a reflected direction by the mirror may be considered to\ndetermine at least one scene camera.  As one example shown in FIG. 5B, the attention direction 554 is toward a rear mirror and is not used to determine at least one scene camera.  The attention direction 555 that is a reflection from the attention\ndirection 554 could be used to determine at least one scene camera according to any method mentioned above.\nIn an implementation, the attention direction 555 can be estimated according to the attention direction 554 and the pose of the mirror in a common coordinate system (e.g. the reference coordinate system 509) based on light reflection law.  The\nattention direction 554 may be first estimated according to methods proposed in the present invention.  The pose of the mirror may be provided or determined by a camera.  The attention direction 555 may be determined only if the attention direction 554\nintersects with the mirror.\nIn another implementation, the camera 206 mounted inside the car 201 captures a mirror image containing at least part of the user reflected by the mirror.  The attention direction 555 could be estimated directly from the captured mirror image. \nFor example, the camera 206 mounted inside the car 201 captures the eye or at least part of the face of the user through the mirror.\nIn a further implementation, the attention direction 555 may be derived from the attention direction 554 and the pose of the mirror, e.g. according to light reflection law.  The attention direction 555 is used to determine at least one scene\ncamera only if the attention direction 554 intersects with the mirror.\nFrom two attention directions, according to an embodiment, a position may be determined according to triangulation.  During a period of the vehicle moving from one position to another position, the user may look at a point or an object of\ninterest multiple times.  As an exemplary scenario shown in FIG. 6, the user 202 looks at the person 226 two times, i.e. when the car 201 is at the position 661 and at the position 662, respectively.  The attention directions 203 and 603 are determined\nrespectively.  The position of the person 226 may be estimated from a triangulation of the attention directions 203 and 603 (e.g. an intersection area or point of the attention directions 203 and 603), and a spatial relationship between the vehicle at\nthe positions 661 and 662.  The position of the person 226 may be determined in at least one of the reference coordinate systems 209 and 609 associated with car 201.  The position of the person 226 may also be determined in a coordinate system of the\nreal environment 220, for example, when the position of the car is known in the real environment 220.\nFIG. 7 shows an embodiment of a flow diagram of generating an order to purchase at least one item according to at least one scene image captured by at least one scene camera mounted to a vehicle, as disclosed herein.  E-commerce and online\nshopping are common techniques and make life simple for both buyers and sellers.  With a computer or even a smart phone, a buyer may find some candidate items (e.g. some objects of interest) for purchasing based on key word searching with modem\nE-commerce and online shopping system.  For example, a user may find an object of interest in a surrounding environment and may like to further find out whether the same object (also called item) and/or similar objects are available for purchasing and\nthen may perform an order to purchase one or more objects and/or find a real store for hands-on checking and/or purchasing.  It is possible to capture an image of the object of interest and identify the same or similar items available for purchasing\nbased on image analysis of the captured image.\nNowadays, people also spend a lot of time in a vehicle, for example when they are shopping, commuting or sightseeing.  Potentially, there may be many different objects (e.g. pedestrians, advertisement posters, and real stores) surrounding the\nvehicle.  It would be difficult for people sitting in a vehicle to use a mobile device equipped with a camera (e.g. a standard camera or a mobile phone with a camera) to capture an image of the environment surrounding the vehicle.  This is particularly\ntrue for a driver who is driving the vehicle.  The driver is not able to hold the mobile device to capture an image while he is driving.\nIt is therefore beneficial to employ one or more cameras mounted on a car to capture an image containing an object of interest instead of asking the driver to hold on and take a camera to capture an image.  Further, an attention direction of the\ndriver (e.g. a gaze direction or a face direction or a hand pointing direction) may be employed to determine at least one camera among the car mounted cameras and/or determine regions of interest in one or more images captured by at least one of the car\nmounted cameras.\nAgain referring to FIG. 7, step 701 provides at least one scene image of at least one real object.\nIn one example, the at least one scene image may be captured by at least one scene camera mounted on a vehicle.  The event of capturing the at least one scene image by the at least one scene camera may be triggered by a user command and/or by a\nstate or state changing of the vehicle.  A user command may include at least one of, but is not limited to, clicking a button, a gesture command, and a voice command.  The states of the vehicle may include, but are not limited to, speed, state of engine,\nstate of braking system, position of gears, light, distance of another object to the front or rear of the car, open/close state of the driver's door, steering wheel lock, hand break, open/close state of the trunk, or a combination of the above.\nWhen multiple scene cameras mounted on the vehicle are available, one or more cameras among all the vehicle mounted scene cameras may be determined according to at least one user attention direction.  This may be realized based on methods and\nsystems disclosed above (e.g. as shown in FIG. 1).  The determined cameras (i.e. the at least one scene camera) are used to capture one or more scene images (i.e. the at least one scene image).  Said one or more cameras among the scene cameras may also\nbe determined according to a user command and/or by a state or state changing of the vehicle.  For example, as shown in FIG. 2, the at least one scene camera may be the camera 214 determined among the cameras 211-214 according to the attention direction\n203.  In another implementation the camera 214 may also be manually determined by the user (e.g. by a voice command, like \"front\" or \"front camera\", given by the driver, or by a button triggering the camera).\nIn another embodiment, the at least one scene image may be captured by at least one camera attached to a mobile device (e.g. a mobile phone or a tablet).  The at least one scene image may be captured by any camera.\nStep 702 determines at least one target object among a plurality of objects according to the at least one scene image.  The at least one target object may be contained or partially contained in the captured at least one scene image.  In the\nexample in FIG. 2, the determined scene camera 214 captures the person 226 in the scene image 231.  The clothing (e.g. skirt) of the person 226 (which may be the object of interest or a part of the object of interest indicated by the attention direction\n203) may be determined as a target object.\nThe at least one target object may not be contained in the at least one scene image.  For example, the skirt of the person 226 contained in the at least one scene image may not be determined as a. target object.  However, an image region (e.g.\nthe image region.  233 as shown in FIG. 3) in the at least one scene image and containing at least part of the skirt 232 may be analyzed, e.g. its texture or color may be analyzed.  The image region 233 may also be determined according to at least one\nuser attention direction (e.g. at least one of gaze directions, face directions, and hand pointing directions.) In one embodiment, the image region may be determined based on a spatial relationship between the least one user attention direction and the\ncamera.  For example, the user attention direction 203 (e.g. represented by a direction axis) could be projected as a point or a line in the image 231 captured by the camera 214 based on the spatial relationship between the camera 214 and the user\nattention direction 203.  Various methods are disclosed herein to determine the region of interest in the image 231 according to the project point(s).  The image region 233 may be determined accordingly.\nOne or more objects having similar texture or color as at least part of the image region 233 may be determined as the at least one target object.  For example, another skirt, even a shirt, a skirt, cup, a car and/or glasses may be determined as\nthe at least one target object.\nOne or more of a plurality of objects may be determined to be the at least one target object.  The plurality of objects may be provided by one or more databases (e.g. the databases 711-713).  In one example, the plurality of objects may include\na plurality of shopping items available (e.g. online and/or in real stores) for purchasing.  Each respective object of the plurality of objects may be associated with at least one reference image containing the respective object.  Further, the respective\nobject may have price information, manufacturer information, location information (e.g. a location for a real store), web link information, type or category information, etc. The plurality of objects are represented by their associated information in any\nmethod or system disclosed herein.\nThe databases 711-713 may be located on a server computer side.  For example, an online shop provides, on its online server computer, various clothing items with their reference images and prices, e.g. for skirts, jeans and shirts.  The clothing\nitems may be compared to the skirt of the person 226 in terms of their colors, shapes, and/or textures in order to determine at least one of the clothing items as the at least one target object.  For this, image based matching or similarity measures\ncould be employed for the comparison, e.g. match the image 231 or only the image region 233 with the reference images associated with the clothing items.\nIn one embodiment, it is possible to automatically determine one or more target objects among the plurality of objects based on matching the at least one scene image with at least part of reference images associated with the plurality of\nobjects.  One or more reference images that are matched with the at least one scene image could be determined.  Then respective objects related to the matched reference images can be determined as target objects.  The image matching may be based on,\ne.g., image features (e.g. SIFT; SURF), template matching, histogram, texture model (e.g. co-occurrence matrices, wavelets), and/or machine learning (e.g. random forest).\nA computer vision method may be applied to detect at least one object in the at least one scene image based on pixel information of the scene image and further determine a type or a class of the at least one object.  For example, the skirt 232\nmay be detected in the scene image 231 and recognized as a type of cloth.  The determined type may be used to select target objects among the plurality of objects.  For example, objects that have the type of cloth may be determined as target objects.  In\nanother example, reference images related to objects that have the type of cloth may be matched to the at least one scene image.\nAt least one image region contained in the at least one scene image may be chosen manually by the user or automatically (e.g. according to computer vision methods).  The chosen image region may be matched to the reference images related to the\nplurality of objects.  In one implementation, the image region 233 in the scene image 231 may be manually chosen by the user.  In another implementation, the image region 233 in the scene image 231 may be automatically determined based on a computer\nvision method or based on one or more user attention directions.\nIn one embodiment, the plurality of objects includes a plurality of clothing items.  The plurality of clothing items may be provided from one or more databases.  For example, one or more clothing providers (e.g. cloth manufacturers and/or\n(online) shopping stores) could provide clothing items.  Each of the plurality of clothing items may have associated texture information, shape, size, reference image features (e.g. represented by visual words, SIFT features and/or SURF features) and/or\na reference image containing the respective clothing item.  The plurality of clothing items (represented by their associated information) may be stored in the vehicle, or in one or more server computers separate from the vehicle.  A mobile device (e.g. a\nmobile phone, a tablet, or a laptop) may store the plurality of clothing items.  The vehicle, the mobile device, and the one or more server computers may communicate with each other via cables and/or wirelessly.\nThe step 702 of determining the at least one target object or a part of the step 702 may be performed in the vehicle, in the server computer, or in the mobile device.  As an example scenario shown in FIGS. 2 and 3, the scene image 231 is\ncaptured.  The plurality of clothing items (represented by their associated information) may be stored in the server computer 301.  The following computation of determining a target object may be performed in the server computer 301.  In this case, the\nscene image 231 may be sent from the car 201 to the server computer 301.  In another example, the scene image 231 may be sent from the car 201 to the mobile phone 302, and then sent from the mobile phone 302 to the server computer 301.  Multiple scene\nimages may be captured by cameras mounted to the car and used to determine one target object.\nA vision based visual search method like that disclosed in Girod, Bernd, et al. \"Mobile visual search.\" Signal Processing Magazine, IEEE 28.4 (2011): 61-76 or Philbin, James, et al. \"Object retrieval with large vocabularies and fast spatial\nmatching.\" Computer Vision and Pattern Recognition, 2007.  CVPR'07.  IEEE Conference on.  IEEE, 2007 (e.g. based on image features, similarity measures, template matching, and/or machine learning) may be performed in order to search, among the plurality\nof clothing items, one or more clothing items that have visual information (e.g. texture, color, and/or shape) similar or relevant to at least part of the scene image 231 (e.g. the region of interest 233) or to an object contained in the scene image 231\n(e.g. the skirt 232).  For this, at least part of the image 231 could be matched with reference image features or reference images associated with the plurality of clothing items.\nIt is optional to recognize an object of interest and/or determine a region of interest contained in the scene image.  For example, the scene image 231 is analyzed automatically.  For instance, an object recognition/classification method is\nperformed on the scene image 231 in order to determine an object of interest or a region of interest.  A machine learning method (e.g. based on random forest) could be employed to train the recognition/classification method (or system) to detect objects\nof interest by providing a plurality of training images containing different objects of interest (e.g. different kinds of skirts).  It is possible to recognize the skirt 232 in the scene image 231 and/or determine the image region 233 containing at least\npart of the skirt 232 based on the trained recognition/classification method.  The plurality of training images may come from scene images captured previously by the scene cameras mounted to the car 201.  This could automatically generate a customized\ntrained method based on favorites of the user.  It is also possible to manually recognize the skirt 232 and/or the image region 233 in the scene image 231 by a user input.\nWhen at least one object of interest in the scene image and/or its type is recognized, this information may be provided to search at least one target object.  For example, among the plurality of clothing items, only skirts may be considered as\npotential target objects and other clothing items are excluded from subsequent searching.  For example, a skirt among the plurality of clothing items having similar color or texture as the skirt 232 may be determined based on an image matching method.\nIn one implementation, current image features are extracted in the scene image 231.  The current image features may be extracted only in the determined region of interest (e.g. the image region 233).  The extracted current image features may be\nmatched with the reference image features associated with at least part of the plurality of clothing items in order to determine one or more clothing items as target objects.  Image features may be represented by high level feature descriptors, like SIFT\nor SURF.\nIn another implementation, an image region 233 contained in the scene image 231 may be matched to reference images associated with at least part of the plurality of clothing items based on template matching in order to determine one or more\nclothing items as target objects.  Various similarity measures, e.g. NCC, SSD and/or histogram, may be employed for the template matching.  From a vision based visual search method (like the methods disclosed above), any clothing items having the same or\nsimilar visual texture and/or color may be determined as a target object.  For example, the target object is not limited to be a skirt, but could also be a shirt or a skirt.  Further, objects relevant to the recognized object (e.g. the recognized skirt\n232) may be determined.  For example, a special washing detergent or a lipstick having a similar color may be relevant to the recognized skirt 232.  This may require the special washing detergent or the lipstick to be included in the plurality of\nobjects.\nAdditional preference data may be provided in order to determine the at least one target object.  Preference data may include at least one of, but are not limited to, an image and/or text database of preferred target objects, online shop member\ninformation, properties related to the vehicle (e.g. type, color, brand, registration year, maintenance status, gas or diesel).  For example, the online shop member information may be used to determine which server computers or databases should be used\nto provide the plurality of objects or a part of the plurality of objects.  Further, the properties related to the vehicle may be used to determine items related to vehicles.  For example, tires or painting material that could be used for the type of the\nvehicle may be searched or determined as target objects.\nStep 703 creates target object information related to the at least one target object.  Target object information related to the determined at least one target object may be created.  In one example, one or more skirts among the plurality of\nclothing items may be determined as the at least one target object.  The skirts may come from one or more clothing providers.  The target object information includes at least one of images containing the determined at least one target object, sizes,\nmaterials, prices, brands, clothing providers, online information links, and/or online store links related to the determined at least one target object.  In the example scenario shown in FIG. 3, the target information may be created in the server\ncomputer 301 and sent from the server computer to the car 201 and/or the mobile phone 302.\nOptional step 704 displays the target object information on a display device.  The target object information may be displayed on a display device, e.g. a LCD screen.  The display device may be attached to the vehicle or separate with the\nvehicle.  The display device may be a screen mounted inside the vehicle.  The display device may also be a screen of a mobile device or a desktop computer.\nStep 705 determines at least one shopping item among the at least one target object.  The user may choose one or more target objects as shopping items from the determined at least one target object.  The user input may include some user\npreferences, e.g. a maximum price limit and particular brands.\nStep 706 generates order information to purchase the at least one shopping item.  Personal data related to a user, e.g. a postal address, payment information (e.g. credit card information, voucher, and/or virtual currency), contact information,\nand membership information (e.g. membership of an online or real store) may be considered for generating the order information.  In the example scenario shown in FIG. 3, the order information may be generated in the server computer 301, the car 201 or\nthe mobile phone 302.  The order information may be submitted to an ordering server computer, which may be different from or the same as the server computer 301.  For example, the server computer 301 may be a server providing information of a plurality\nof items available for purchasing.  The ordering server computer may be an online shop that sells respective items.  In another example, the server computer 301 may provide information of a plurality of items available for purchasing and sell the\nplurality of items.\nAny embodiments described above can be applied, in principle, to any device to which a plurality of cameras is mounted, for determining at least one camera among the plurality of cameras.  For example, a mobile device, such as a laptop or a\ntablet computer, may be equipped with a front-facing camera and two back-facing cameras.  The front-facing camera may be used to determine a user attention direction, such as a gaze direction and/or face direction.  The determined gaze direction and/or\nface direction can be used to select a desired camera from the two back-facing cameras.  Images captured by the desired camera may contain objects of interest indicated by the user attention.\nThe present disclosure is further related to the following aspects and embodiments.  These aspects and embodiments may be applied individually and separately or in combination with aspects and embodiments of the disclosure as described herein.\nA method of determining at least one item available for purchasing, the method comprising: a) providing at least one scene image captured by at least one scene camera mounted to a vehicle; b) providing user attention data related to at least one\nuser captured by an information capturing device; c) providing, from the user attention data, at least one attention direction relative to a reference coordinate system associated with the vehicle; d) providing image information related to each of a\nplurality of items available for purchasing, wherein the image information comprises at least one of reference images and reference image features; and e) determining at least one item of the plurality of items according to the at least one attention\ndirection, the at least one scene image, and the image information related to the at least one item.\nThe method according to the preceding paragraph, further comprising selecting the at least one scene camera among a plurality of scene cameras mounted to the vehicle according to the at least one attention direction.\nThe method according to one of the preceding paragraphs, further comprising determining at least one current image features in the at least one scene image.\nThe method according to one of the preceding paragraphs, further comprising determining at least one current image features in the at least one scene image further according to the at least one attention direction.\nThe method according to one of the preceding paragraphs, wherein the step e) further comprises matching at least part of the at least one scene image and at least part of reference images of the image information related to the at least one item\nor matching at least one current image features and at least part of reference image features of the image information related to the at least one item.\nThe method according to one of the preceding paragraphs, further comprising providing purchasing information related to the at least one item.\nFor a majority of applications in computer vision, images are captured by one or more cameras, an operator is a human being, and a display screen is used to display the images such that the human being could observe the images and move the\ncamera accordingly.  In this case, a user attention direction relative to the screen indicates objects of interest that should be recognized, reconstructed, and/or tracked.  Image features unrelated to the objects of interest could be removed from all\nextracted image features or only image features related to the objects of interest are extracted according to the user attention direction.  For example, a gaze direction or a face direction or a hand direction relative to the screen could be used to\nidentify objects of interest and/or relevant image features contained in the images for recognition, reconstruction, and/or tracking applications.  In another example, a spatial relationship between at least one user attention direction and a camera\ncould also be used to determine image features contained or derived from images captured by the camera.\nFurther, the image location of the user attention direction (e.g. gaze direction or the frontal face direction) in the image could be used to determine image features of interest contained in the image.  In another example, the screen location\nof the user attention direction on the screen could be used to determine image features of interest.  The screen location may be determined as an intersection between the screen plane and the user attention direction (e.g. the direction of the gaze or\nthe face).  The screen locations of the user attention directions may be transformed to image locations in the image according to where the image is display on the screen.  These image locations would represent the image locations of the user attention\ndirections.\nAugmented reality systems could present enhanced information of a real object by providing a visualization of overlaying computer-generated virtual information with visual impressions or an image of the real object.  For this, a real object is\ndetected or tracked in order to retrieve or generate the relevant virtual information.  The overlay of the virtual and real information can be seen by a user using a well-known video see-through device comprising a camera and a display screen.  In this\ncase, the object of interest is captured in an image by the camera.  The overlay of the virtual information and the captured image is shown on the display screen to the user.  The user often looks at the object of interest captured in the image displayed\non the screen, but not at other objects captured in the image.  Thus, the gaze information of the user or a pose of the user's face relative to the screen or the camera can determine the object of interest.\nIn another embodiment, the overlay of the virtual and real information can be seen by a user in a well-known optical see-through device having semi-transparent glasses.  In this case, the user sees through the semi-transparent glasses real\nobjects of the real environment augmented with the virtual information blended in in the semi-transparent glasses.  At least one camera is often attached to the optical see-through device in order to identify, track or reconstruct the object of interest\nby using computer vision methods.  In this case, a spatial relationship between the camera attached to the optical see-through device and the user attention direction could be used to determine or detect image features in images captured by the camera. \nThe image locations of the user attention directions in one image captured the camera could be determined according to that spatial relationship.\nHaving the image positions of the user attention directions in one image, image features contained the image may be determined according to an image region of interest defined by the image positions of the user attention directions.\nFor conciseness, some embodiments given here are based on gaze image locations.  However, all the embodiments related to gaze image locations could also be applied to the image positions of other user attention directions, e.g. face direction\nimage locations, and hand pointing direction image locations.\nAccording to an embodiment, the step of determining at least one image feature of interest comprises determining an image region of interest in at least one image according to the at least one gaze image location, wherein the at least one image\nfeature of interest is determined according to the image region of interest.  The at least one image feature of interest may comprise information of at least part of pixel information of the image region of interest or information derived from at least\npart of pixel information of the image region of interest.\nAccording to an embodiment, the step of determining the image region of interest comprises performing a segmentation on the at least one image to obtain a plurality of image regions and determining at least one of the plurality of image regions\nas the image region of interest according to the at least one gaze image location and the positions of the at least one of the plurality of image regions.\nImage segmentation may be performed to segment an image region of interest in the image from the at least one gaze image location as one or more seed points.\nThe image region of interest may also be determined as surrounding regions (e.g. represented by various 2D geometrical shapes) around the at least one gaze image location.  For example, a circle or a rectangle or a square could be determined\nbased on one or more gaze image locations, as a center point, or as corner points, or as points on boarders to restrict the 2D geometrical shape.\nMany Augmented Reality (AR) applications can benefit from the present invention.  For example, in AR shopping, AR maintenance, and AR touring applications, there are multiple real objects located in the real world (e.g. clothing for AR shopping,\nengine components for AR maintenance, and monuments for AR touring).  The user is often interested in one object at a time.  The object of interest to the user could be determined according to the user attention direction, e.g. the gaze of the user, the\npose of the face, or a hand pointing direction at that time.  Then, only the object of interest may be detected, tracked, or reconstructed.  Further, digital information related only to the object of interest would be generated and visually displayed on\nthe top of an image of the object in an AR view.\nAccording to an embodiment, a processing system for performing a method as described herein may be comprised at least in part in a handheld device, in the vehicle, and/or in a server computer.  Such processing system may be comprised in only one\nof these devices or may be a distributed system in which one or more processing tasks (performing one or more method steps) are distributed and processed by one or more processing devices (such as microprocessors) which are spatially distributed and are\ncommunicating with each other, e.g. wirelessly.\nGenerally, the following aspects and embodiments may be applied individually or in any combination with each other with the aspects of the invention as disclosed above.\nAccording to an embodiment, the user attention data is related to at least one or more of a face, a gaze, a hand, and a gesture of the user.\nAccording to an embodiment, the user attention data comprises at least one or more of an optical image, bioelectric signal, e.g. electrooculogram.\nAccording to an embodiment, the information capturing device comprises a user camera, and the step of providing user attention data comprises providing at least one user image of at least part of the user captured by the user camera.\nFor example, the at least one user image further comprises at least part of the vehicle, and the at least one attention direction is determined according to pixel information of the at least part of the user and pixel information of the at least\npart of the vehicle in the at least one user image.\nAccording to an embodiment, the information capturing device comprises an eye tracking device, and the step of providing user attention data comprises providing at least one gaze information associated to at least one eye of the user captured by\nthe eye tracking device.\nAccording to an embodiment, the step of providing the at least one attention direction comprises at least one or more of: determining at least one face direction of the user's face, determining at least one gaze direction of a user's gaze, and\ndetermining at least one hand pointing direction of a user's hand.\nAccording to an embodiment, the method further comprises providing a device position of the information capturing device in the reference coordinate system, and providing the at least one attention direction relative to the reference coordinate\nsystem according to the device position and a spatial relationship between the at least one attention direction and the information capturing device.\nAccording to an embodiment, the step of providing the at least one image of at least one real object captured by the at least one of the scene cameras comprises providing a plurality of images captured by the plurality of scene cameras and\nselecting the at least one image from the plurality of images.\nFor example, the step of providing the plurality of images and the step of providing the user attention data are synchronized.\nAccording to an embodiment, the step of providing the at least one image comprises capturing the at least one image by the at least one of the scene cameras.\nAccording to an embodiment, the attention direction is determined in consideration of at least one mirror mounted on the vehicle.\nAccording to an embodiment, the method further comprises capturing the user attention data when the vehicle is at a first vehicle position, determining a spatial relationship between the vehicle at the first vehicle position and the vehicle at a\nsecond vehicle position different from the first vehicle position, and determining the at least one of the scene cameras according to the spatial relationship between the vehicle at the first vehicle position and the vehicle at the second vehicle\nposition.\nAccording to an embodiment, the method further comprises capturing a second user attention data when the vehicle is at the second vehicle position, determining at least one second attention direction from the second user attention data, and\ndetermining a position data in the reference coordinate system associated with the vehicle according to the at least one attention direction, the at least one second attention direction and the spatial relationship between the vehicle at the first\nvehicle position and the vehicle at the second vehicle position.\nAccording to an embodiment, the at least one attention direction is represented by an axis or field of view.\nAccording to an embodiment, the method further comprises determining at least one object of interest according to the at least one image, wherein the at least one object of interest is contained in the at least one image, or is not contained in\nthe at least one image.\nAccording to an embodiment, the method further comprises determining at least one object of interest according to the at least one image, and generating an order information related to the at least one object of interest for transmission to a\nprovider for purchasing of a product.\nAccording to an embodiment, the method further comprises determining at least one object of interest according to the at least one image, and determining a location of the vehicle in the real world with respect to a global coordinate system and\na position of the at least one object of interest relative to the vehicle.\nThe present disclosure is further related to the following aspects and embodiments.  These aspects and embodiments may be applied individually and separately or in combination with aspects and embodiments of the disclosure as described herein.\nAccording to an aspect, there is disclosed a method of determining at least one image feature in at least one image, comprising providing at least one image of at least part of an object captured by at least one camera, displaying at least part\nof the at least one image on at least one display screen, determining at least one attention image location of at least one user in the at least one image, and determining at least one image feature of interest in the at least one image according to the\nat least one attention image location.\nAccording to another aspect, there is disclosed a method of determining at least one image feature in at least one image, comprising providing at least one image of at least part of an object captured by at least one camera, determining at least\none attention direction of at least one user with respect to the at least one camera where the at least one image is captured, determining at least one attention image location of at least one user in the at least one image according to the at least one\nattention direction, and determining at least one image feature of interest in the at least one image according to the at least one attention image location.\nAccording to another aspect, there is disclosed a system for determining at least one image feature in at least one image, comprising a processing system which is configured to provide at least one image of at least part of an object captured by\nat least one camera, to display at least part of the at least one image on at least one display screen, to determine at least one attention image location of at least one user in the at least one image, and to determine at least one image feature of\ninterest in the at least one image according to the at least one attention image location.\nAccording to another aspect, there is disclosed a system for determining at least one image feature in at least one image, comprising a processing system which is configured to provide at least one image of at least part of an object captured by\nat least one camera, to determine at least one attention direction of at least one user with respect to the at least one camera where the at least one image is captured, to determine at least one attention image location of at least one user in the at\nleast one image according to the at least one attention direction, and to determine at least one image feature of interest in the at least one image according to the at least one gaze attention image location.\nParticularly, according to the present disclosure, a gaze direction of a user's eye or eyes is an attention direction of the user.  A gaze screen location is an attention screen location.  A gaze image location is an attention image location.\nParticularly, according to the present disclosure, a face direction of a user is an attention direction of the user.  A face direction screen location is an attention screen location.  A face direction image location is an attention image\nlocation.\nAccording to an embodiment, the face direction is the frontal face direction.\nParticularly, according to the present disclosure, a hand pointing direction of a user is an attention direction of the user.  A hand pointing screen location is an attention screen location.  A hand pointing image location is an attention image\nlocation.\nFor conciseness, embodiments given here are based gaze directions, gaze screen locations, gaze image positions as specific examples of the attention direction of the user, the attention screen location, the attention image location.\nHowever, all the embodiments related to the gaze directions, the gaze screen locations, the gaze image positions could also be applied to other user attention directions, e.g. face directions and hand pointing directions, other user attention\nscreen locations, e.g. face direction screen locations and hand pointing screen locations, and other user attention image locations, e.g. face direction image locations and hand pointing image locations.\nParticularly, according to the present disclosure, a gaze image location of human eyes, particularly of the user's eye or eyes, in one or more images may be detected and used to determine image features in the one or more images.  The extracted\nimage features may be used to detect, track, and/or reconstruct objects of interest captured in the one or more images.  Thus, unrelated detected image features may be removed or only image features of interest may be detected in images for use in\ncomputer vision methods.\nFor a plurality of applications in computer vision, based on images captured by one or more cameras, an operator of a method or system, as described according to the present disclosure, is a human being, and a display screen is used to display\nthe images such that the human being could observe the captured images and move the camera accordingly.  In such an embodiment, a gaze location of human eyes of the user in the images may indicate objects of interest that should be recognized,\nreconstructed, and/or tracked.  Image features unrelated to objects of interest may be removed from any extracted image features, or only image features related to objects of interest may be extracted according to the determined gaze location or gaze\nlocations, or any identified objects of interest in a computer vision method for recognition, reconstruction, and/or tracking.\nAugmented reality systems could present enhanced information of a real object by providing a visualization of overlaying computer-generated virtual information with visual impressions or an image of a real object.  For this, the real object is\ndetected or tracked in order to retrieve or generate the relevant virtual information.  The overlay of the virtual and real information can be seen by a user, e.g., by employing a video see-through device comprising a camera and a display screen.  In\nthis case, the object of interest is captured in an image by the camera.  The overlay of the virtual information and the captured image is shown on the display screen to the user.  The user would often look at the object of interest captured in the image\ndisplayed on the screen, but not at other objects captured in the image.  Thus, the gaze information of the user may be used to determine an object of interest.\nThe overlay of the virtual and real information can also be seen by a user by means of a well-known optical see-through device having semi-transparent glasses.  In this case, the user then sees through the semi-transparent glasses objects of the\nreal environment augmented with the virtual information blended in, in the semitransparent glasses.  At least one camera is often attached to the optical see-through device in order to identify, track or reconstruct the object of interest by using\ncomputer vision methods.\nAccording to an embodiment, the method further comprises determining at least one gaze screen location of the at least one eye on the at least one display screen, wherein the at least one gaze image location is determined according to the at\nleast one gaze screen location.\nFor example, it further comprises synchronizing the step of determining the at least one gaze screen location with the step of displaying the at least part of the at least one image on the at least one display screen.\nAccording to an embodiment, the step of determining the at least one gaze image location comprises providing at least one second image of the at least one eye captured by at least one capturing device (which may be the same camera or a different\ncamera) that has a known spatial relationship with the at least one display screen, and determining the at least one gaze image location according to the at least one second image.\nAccording to a further embodiment, the at least one image comprises a plurality of images captured by the at least one camera, and the method further comprises, for each respective image of the plurality of images, displaying at least part of\nthe respective image on one of the at least one display screen and determining one or more gaze image locations in the respective image.  The method then further comprises determining at least one image transformation between the plurality of images, and\ntransforming the determined one or more gaze image locations from the each respective image of the plurality of images into at least one of the plurality of images according to the at least one image transformation, wherein the determined at least one\ngaze image location comprises the transformed gaze image locations.\nAccording to an embodiment, the method further comprises providing depth information associated with at least part of the at least one image, and determining the at least one gaze image location according to the at least one gaze direction and\nthe depth information.\nAccording to an embodiment, the method further comprises performing a computer vision algorithm according to the determined at least one image feature, wherein the computer vision algorithm comprises at least one of image based recognition,\nimage based tracking, image based reconstruction, and image based classification.\nAccording to an embodiment, the method further comprises matching the at least one image feature of interest with reference image features and recognizing the object according to the matching.\nAccording to a further embodiment, the method comprises matching the at least one image feature of interest with reference image features and estimating a pose of the at least one camera with respect to the object according to the matching,\nwherein the reference image features have 3D positions.\nAccording to an embodiment, the method further comprises providing depth information associated with the at least one image, and determining a 3D position for the at least one image feature of interest according to the depth information.\nAccording to an embodiment, the at least one image is at least one first image, and the method further comprises providing at least one second image, determining at least one second image feature in the second image corresponding to the at least\none image feature of interest, and determining a 3D position for the at least one image feature of interest according to image positions of the at least one second image feature and the at least one image feature of interest.\nAccording to an embodiment, the at least one gaze image location is at least one first gaze image location, and the step of determining the at least one second image feature in the at least one second image comprises displaying at least part of\nthe at least one second image on at least one display screen, determining at least one second gaze image location of at least one eye in the at least one second image, and determining the at least one second image feature in the at least one second image\naccording to the at least one second gaze image location.\nAccording to another embodiment, the at least one gaze direction is at least one first gaze direction and the at least one gaze image location is at least one first gaze image location, wherein the step of determining the at least one second\nimage feature in the at least one second image comprises determining at least one second gaze direction of at least one eye with respect to the at least one camera where the at least one camera captures the at least one second image, determining at least\none second gaze image location of at least one eye in the at least one second image according to the at least one second gaze direction, and determining the at least one second image feature in the at least one second image according to the at least one\nsecond gaze image location.\nFor example, the at least one image feature of interest may be stored in a processing device.\nAccording to an embodiment, the object is a real object and the at least one camera is at least one real camera.  According to another embodiment, the object is a virtual object and the at least one camera is at least one virtual camera.\nAccording to an embodiment, the step of determining the at least one image feature of interest comprises determining an image region of interest in the at least one image according to the at least one gaze image location, wherein the at least\none image feature of interest is determined according to the image region of interest.  The at least one image feature of interest may comprise information of at least part of pixel information of the image region of interest or information derived from\nat least part of pixel information of the image region of interest.\nAccording to an embodiment, the step of determining the image region of interest comprises performing a segmentation on the at least one image to obtain a plurality of image regions and determining at least one of the plurality of image regions\nas the image region of interest according to the at least one gaze image location and the positions of the at least one of the plurality of image regions.\nFor example, the step of determining the image region of interest comprises performing a segmentation on the at least one image according to the at least one gaze image location and pixel information of at least part of the at least one image.\nAccording to an embodiment, the at least one gaze image location contains at least two gaze image locations and the step of determining the image region of interest comprises clustering the at least two gaze image locations and determining the\nimage region according to the result of the clustering.\nFor example, the step of determining the at least one image feature of interest according to the image region of interest comprises extracting the at least one image feature of interest in the image region of interest.\nThe step of determining the at least one image feature of interest according to the image region of interest may comprise extracting a plurality of image features in the at least one image, and selecting the at least one image feature of\ninterest from the extracted plurality of image features, wherein the extracted plurality of image features comprises at least one image feature of the object and at least one image feature unrelated to the object.\nFor example, the processing system according to the invention is comprised, at least in part, in a mobile device (such as a mobile phone, wearable computer, tablet computer, mobile computer, often called laptop, or a head mounted display, such\nas used for optical see-through augmented reality applications) and/or in a server computer adapted to communicate with the mobile device.  The processing system may be comprised in only one of these devices, e.g. in the mobile device or in the server\ncomputer, or may be a distributed system in which one or more processing tasks are distributed and processed by one or more processing devices which are distributed and are communicating with each other, e.g. by point to point communication or via a\nnetwork.\nAccording to an embodiment, the system comprises a mobile device which comprises one or more cameras and, for example, a display screen.\nAny steps, embodiments, aspects and examples described herein with respect to the method can equally and analogously be implemented by the processing system being configured (by software and/or hardware) to perform the respective steps,\nembodiments, aspects or examples.  Any processing device used within the processing system may be configured as such and communicate via a communication network, e.g. via a server computer or a point to point communication, with one or more cameras,\ndisplays and/or any other components.\nAccording to another aspect, the invention is also related to a computer program product comprising software code sections which are adapted to perform a method according to the invention.  Particularly, the software code sections are contained\non a computer readable medium which is non-transitory.  The software code sections may be loaded into the memory of one or more processing devices (such as microprocessors) as described herein.\nAny used processing devices may communicate via a communication network, e.g. via a server computer or a point to point communication, as described herein.\nEye tracking as such is a well-established technology for detecting the position or direction of gaze (where one is looking) or the motion or position of an eye relative to a reference position, e.g. the head.  Many eye tracker systems are\ncommercially available, like systems produced by Tobii technology (e.g., see http://www.tobii.com).\nAn eye tracker system may also be a camera that could capture images of the eye.  Gaze detection could also be performed using the captured images, like that proposed in Blum, Tobias, et al. \"The effect of out-of-focus blur on visual discomfort\nwhen using stereo displays.\" Mixed and Augmented Reality (ISMAR), 2010 9th IEEE International Symposium on.  IEEE, 2010 (hereinafter \"Blum et al.\").\nMany applications have benefited from gaze detection.  For example, WO 2014/052058 A1 discloses obtaining a 3D gaze position from an estimated 2D gaze position of a user on a display screen by using a Tobii eye tracker.  In their application,\nartificial out-of-focus blur would be added to images displayed on the screen according to the 3D gaze position.  WO 2014/052058 A1 further discloses a solution to improve the visualization, while they do not propose or motivate any method to process or\nanalyse an image of a real environment captured by a camera according to the gaze position.  Particularly, WO 2014/052058 A1 does not propose detecting image features in the image according to the gaze position and then perform computer vision methods to\ntrack, recognize, classify and/or reconstruct a real object contained in the image.\nSrinivasan et al. in reference [16] develop a multi-modal touch screen emulator based on detected gaze positions of one or more eyes on a display screen and gestures of one or more hands.  They use a camera to capture the one or more eyes in\norder to determine the gaze positions on the display screen.\nNone of these prior art references disclose a relationship between the user gaze position or direction and an object of interest when the user is involved to conduct or use a computer vision product or method based on images that contain the\nobject of interest and that such a relationship could be employed in the computer vision product or method.\nFIG. 8 shows a scenario with a user 806 holding a mobile device 801 that has a front facing camera 803, a back facing camera 802, and a display screen 804.  An image 810 is captured by using the back facing camera 802 in order to recognize,\ntrack and/or reconstruct a plastic rabbit 808 (the object of interest in this embodiment) placed on a table 807.  The mobile device 801 includes one or more processing devices 809, such as one or more microprocessors.\nFIG. 9 shows another scenario with a user 906 holding a mobile device 901 that has a front facing camera 903, a back facing camera 902, and a display screen 904, and is capturing an image 910 by using the back facing camera 902 in order to\nrecognize, track and/or reconstruct the plastic rabbit 808 (i.e. the object of interest) placed on the table 807.  The mobile device 901 includes one or more processing devices 909, such as one or more microprocessors.\nIn the two scenarios shown in FIGS. 8 and 9, the users 806 and 906 may be the same or different.  The mobile devices 801 and 901 may be the same or different.  The front facing cameras 803 and 903, the back facing cameras 802 and 902, and the\ndisplay screens 804 and 904 may be the same or different, respectively.\nIn one application, the plastic rabbit 808 may have to be recognized based on one image (e.g. the image 810) of at least part of the plastic rabbit 808 in order to retrieve or generate digital information (such as its manufacturer information,\nits CAD model, etc.) related to the plastic rabbit 808.  The generated digital information could be visualized and overlaid on top of the image 810 of the plastic rabbit 808 in order to have an overlay image that could be shown on the display screen\n(like the screen 804).  This would create Augmented Reality visualization.\nIn order to recognize the plastic rabbit 808, reference image features stored in a database (e.g., included in the mobile device 801, 901, or included in a remote server computer 890) are matched to current image features extracted in the image\n810.  The current image features may be the image features 831, 832, and 833, which could be represented based on pixel information of their respective rectangle regions by using high level descriptors, like SIFT, SURF, etc.\nIn one embodiment, when at least part of an image feature covers, or is derived from, pixel information of a part of an image region of an object of interest (e.g. the plastic rabbit 808), the image feature is related to the object of interest\nand would be considered as an image feature of interest.  For example, high contrast textures within the object of interest could be determined as image features related to the object of interest, and/or borders (e.g. edges, corners) of the object of\ninterest may be determined as image features related to the object of interest.  In FIG. 8, the image features 831 and 833 are related to the plastic rabbit 808, while the image feature 832 that represents a corner of the table 807 is unrelated to the\nplastic rabbit 808.\nThe image features of interest could be created according to one embodiment (see FIG. 10) of the present invention.  In this embodiment, only image features related to the plastic rabbit 808 may be determined as the image features of interest. \nIn the present example, the image features 831 and 833, but not the image feature 832, would be determined as image features of interest.  Depending on real scenarios, image features unrelated to the object of interest may also be determined as a part of\nthe image features of interest.  Nevertheless, the present invention may significantly remove image features unrelated to the object of interest from the image features of interest, which are to be used in various computer vision methods, such as\nAugmented Reality applications.\nFurther, any reference image features stored in a database may be generated by extracting image features from other images of the plastic rabbit 808.  By using one embodiment (see FIG. 10) of the present invention, it is possible to determine\nimage features of interest (e.g. image features related to the plastic rabbit 808) extracted from the other images as the reference image features.\nHaving only image features related to the object of interest or having a minimum number of image features unrelated to the object of interest in the reference image feature set and/or in the current image feature set, a matching between the\nreference image feature set and the current image feature set may be performed with a high accuracy and low operation time.  This may improve the image based recognition, image based tracking, and/or image based reconstruction.\nFurther, in another application of tracking the plastic rabbit 808, which would be similar to the recognition, the current features extracted in the image 810 are matched to the reference features.  From 3D positions associated with the\nreference features and the matching result, 2D-3D correspondences may be produced.  The pose of the plastic rabbit 808 relative to the back-facing camera 802 that captures the image 810 may be determined based on the 2D-3D correspondences.  Without the\npresent invention, image features related to the table 807 (like image feature 832) may be extracted and matched to reference features and then 2D-3D correspondences would be generated for pose estimation.  This may introduce errors in the pose\nestimation method to track the rabbit 808, especially when the rabbit 808 moves relative to the table 807.\nIn another application, the rabbit 808 may have to be reconstructed based on at least one image of at least part of the plastic rabbit 808.  In one example, the reconstruction procedure may generate a plurality of 3D points from a plurality of\nimage features (e.g. point features) extracted in the at least one image.  A 3D geometrical model could then be derived from the plurality of 3D points.  When depth information is available, the 3D points could be estimated from a single image according\nto the image positions of the image features and depth information associated with the image features.  In another implementation, at least two images are used, and a 3D point could be estimated based on two corresponding image features in the at least\ntwo images, like that proposed in Davison, Andrew J., et al. \"MonoSLAM: Real-time single camera SLAM.\" Pattern Analysis and Machine Intelligence, IEEE Transactions on 29.6 (2007): 1052-1067 (hereinafter \"Davidson et al.\").  It is advantageous if image\nfeatures unrelated to the rabbit 808 are not used to create the 3D points for the reconstruction of the rabbit 808.  For example, the image feature 832 in the image 810 and the image feature 932 in the image 910 are not used to create the 3D points for\nthe rabbit 808.\nIn FIG. 8, the front facing camera 803, the back facing camera 802 and the display screen 804 are rigidly attached to the mobile device 801.  Further, the mobile device 801 may have a processing device 809 and a wireless network unit.  Any step\nof any embodiment disclosed herein could be either executed locally in the mobile device 801 by the processing device 809 or sent to a remote server computer 890 or another mobile device through the wireless network unit.  The front facing camera 803,\nthe back facing camera 802 and the display screen 804 may have known spatial relationships between each other.\nFIG. 10 shows a workflow diagram of an embodiment of determining image features according to gaze location information.\nStep 1001 provides at least one image of at least part of a real object captured by at least one camera.  In the example shown in FIG. 8, an image 810 (i.e. the at least one image) of the rabbit 808 (i.e. the real object) is provided and is\ncaptured by the back facing camera 802.\nStep 1002 displays at least part of the at least one image on at least one display screen.  For example, the image 810 is displayed on the display screen 804 as shown in FIG. 8.  It is also possible to display a part of the image 810 on the\ndisplay screen 804.\nStep 1003 determines at least one gaze screen location of the at least one eye on the at least one display screen.  Particularly, the gaze locations of the eye 805 on the display screen 804 may be determined.\nStep 1002 and step 1003 may be synchronized such that a determined gaze may be related to an image displayed on the screen.  For example, a gaze detected during a time period of displaying an image could be associated with the image.\nStep 1004 determines at least one gaze image location of at least one eye in the at least one image.  In the image 810, gaze image locations represented by circles are determined.  The gaze image locations may be determined from the gaze screen\nlocations.  The gaze image locations may also be directly determined without explicitly computing the gaze screen locations.  For example, when a conversion coefficient from a position on the screen to a position in the image is available, the gaze\nscreen locations are not necessarily to be computed and stored.\nIn order to determine gaze locations on the screen and in the image, an eye tracker, like Tobii eye trackers (e.g. see http://www.tobii.com) or a camera, could be used.\nIn the scenario shown in FIG. 8, the front facing camera 803 attached to the display screen 804 may capture images of the eye 805.  Various vision based methods (e.g. as disclosed in Blum et al.), could be used to determine gaze locations on the\nscreen 804 based on the captured images of the eye 805.\nThe front facing camera 803 may also capture at least part of the face of the user 806, Various vision based face tracking or detection methods may be used to estimate at least one face direction relative to the camera 803 based on one or more\nimages of the at least part of the face of the user 806 captured by the camera 803.  Face direction locations on the screen 804 could be determined according to the at least one face direction.  Face direction image locations may be computed from the\nface direction screen locations.\nThe front facing camera 803 may also capture at least part of one or two hands of the user 806.  Various vision based methods may be used to estimate at least one hand pointing direction relative to the camera 803 based on one or more images\ncaptured by the camera 803.  Hand pointing locations on the screen 804 could be determined according to the at least one hand pointing direction.  Hand pointing image locations may be computed from the hand pointing screen locations.\nStep 1005 determines at least one image feature of interest in the at least one image according to the at least one gaze image location.\nIn one embodiment of the present invention, the at least one image comprises one image and the at least one gaze image location comprises one gaze image location.  Image segmentation may be performed to segment an image region of interest in the\nimage from a seed point of the gaze image location.  Image features that cover, or are derived from, pixel information of at least part of the image region of interest may be determined as the at least one image feature of interest.  In one\nimplementation, the image region of interest may first be determined.  Then, a feature detection method is performed on the image region of interest to extract image features.  The extracted image features would be the image features of interest.  In\nanother implementation, the feature detection method is performed on the original image to extract a plurality of image features.  Image features are selected from the plurality of image features as the image features of interest if they cover or are\nderived from pixel information of at least part of the image region of interest.  For a point feature, if its pixel position is within the image region of interest, the point feature would be an image feature of interest.\nIn FIG. 8, the image 810 is provided.  The rabbit 808 (i.e. the object of interest to be recognized, tracked, and/or reconstructed) is captured in the image 810.  The table 807 is also partially captured in the image 810.  In a first example,\nonly the gaze image location 821 is determined in the image 801.  The gaze image location 821 may be used as an input for a segmentation method to segment the image region of interest in the image 810.  For example, the gaze image location 821 is used as\na starting point for a region grow method.  In another implementation, a threshold value based on pixel information of a region around the gaze image location may be determined and used for thresholding based segmentation.  In another way, a 2D geometry\nmay be determined as the image region of interest in the image 810 based on the gaze image location 821.  For example, a region of a square or a circle may be defined by the gaze image location 821 as their center point.  In a further example, at least\npart of the image 810 may be divided into several image blocks, for instance according to image segmentation based on pixel values.  At least one of the image blocks may be chosen as the image region of interest according to the gaze image location 821. \nIn this case, an image block may be chosen if the gaze image location 821 is inside the image block, or close to the image block within a threshold.  Moreover, when the image 810 has depth information for at least part of its pixels, the segmentation may\nfurther be based on the depth information.  For example, the gaze image position may be used as a starting point of a region grow method, in which the depth information will be compared or analysed during the growing procedure.\nBased on a region grow or thresholding segmentation that uses the gaze image location 821, the image region 818 of the rabbit may be determined.  The image features 831 and 833 are determined as image features of interest, since they are within\nthe image region 818, while the image feature 832 (i.e. the corner of the table 807) is not determined as an image feature of interest, since it is outside of the image region 818.  The determined image features of interest (here, the image features 831\nand 833) could be used to match with reference features (e.g. derived from reference images or derived from CAD models).  Based on the matching result, the pose of the rabbit 808 relative to the back facing camera 802 may be estimated or the rabbit 808\nmay be recognized.  The determined image features of interest (i.e. the image features 831 and 833) may also be used for the reconstruction of the rabbit 808.\nIn a second example, an image 810 is provided, and a plurality of gaze image locations (e.g., the gaze image locations 821-826) are determined in the image 801.  In one implementation, at least part of the image 810 may be divided into several\nimage blocks, for instance according to image segmentation based on pixel values.  At least one of the image blocks may be chosen as the image region of interest according to positions of the plurality of gaze image locations.  In this case, an image\nblock may be chosen if a certain number of gaze image locations are inside the image block or close to the image block within a threshold.  Thus, multiple image blocks may be chosen as the image region of interest.  In another example, the plurality of\ngaze image locations may be grouped or clustered.  In this case, an image block may be chosen if at least part of a certain group of gaze image locations are inside the image block or close to the image block within a threshold.\nPoint clustering methods may be employed to cluster or group the plurality of gaze image locations according to their 2D image locations.  Methods, like connectivity based clustering (hierarchical clustering), distribution-based clustering (e.g.\nGaussians for modelling clusters), and density-based clustering, could be used.  Particularly, Gaussians distribution-based clustering may be preferred, as the user would most likely focus on the object of interest shown on the display screen.  Further,\nwhen the image 810 has associated depth information for at least part of its pixels, the gaze image locations could also be clustered according to their depth.  For example, gaze image locations having similar depth are grouped.  In another example, gaze\nimage locations whose corresponding 3D positions locate on the same plane are grouped.\nA group with a largest number of gaze image locations may be selected from the clustering method.  The gaze image locations of the selected group are determined as a plurality of gaze image locations of interest.  Segmentation methods, like\nregion grow and thresholding, may be used to determine a region of interest according to at least part of the plurality of gaze image locations of interest.  Further, a region associated with each of the groups may also be determined from a clustering\nmethod, like Gaussians distribution-based clustering or density-based clustering methods.  The region associated with the group having the largest number would be the image region of interest.\nIn one implementation, in order to have multiple determined gaze image locations in one image, a frequency of determining gaze may have to be higher than a frequency of displaying different images on the screen.  For example, when displaying one\nimage, more than one gaze should be able to be detected.  In another implementation, one or more gaze image locations determined in one image may be mapped to image locations in another image by applying a transformation or a mapping.  From this,\nmultiple gaze image locations could be obtained in a single image.  For example, the gaze image locations 925 and 926 (marked by crosses) in image 910 are mapped from the gaze image locations 825 and 826 in the image 810.  The mapping may be a\ntransformation (e.g. affine or projective) computed from a relative motion between the two back facing cameras 802 and 902 at positions where the images 810 and 910 are captured respectively.  The mapping may also be computed by matching or registering\nthe two images based on pixel information of the images.\nIn another embodiment of the present invention, the at least one image comprises multiple images and each of the multiple images has at least one gaze image location detected in each respective image.  The multiple images may be captured by the\nsame or different cameras.  Further, the multiple images may be displayed on the same or different display screens.\nThe at least one gaze image location associated with each respective image may be mapped into one of other multiple images.  The mapping may be a 2D transformation (e.g. rigid transformation, affine transformation, projective transformation, or\nhomography) estimated based on the pose of the camera where the camera captures each respective image or the motion between the camera capturing the two images.\nThe camera motion or camera pose may be computed by using vision based methods, sensors (like GPS, compass, inertial sensor), or a dedicated tracking system.  The dedicated tracking system may be an optical tracking system, a mechanical tracker\n(e.g. a robotic arm), or an indoor positioning system based on nearby anchors (e.g. Bluetooth receivers).  The motion or pose may be represented by a 6 degree of freedom transformation including translations and rotations.\nHaving multiple gaze image locations in one of the multiple images, similar embodiments as mentioned above could be applied analogously to detect or select image features of interest in the image.\nIn a third example, images 810 and 910 are provided.  The gaze image locations 821-826 are determined in the image 810 while the image 810 is shown on the screen 804, and the gaze image locations 921-923 are determined in the image 910 while the\nimage 910 is shown on the screen 904.  A motion between the back facing camera 802 where it captures the image 810 and the back facing camera 902 where it captures the image 910 can be determined based on computer vision based methods (e.g. using the\nimages 810 and 910) or based on localization sensors (e.g. GPS, compass, gravity sensor, etc.).  When the cameras 802 and 902 are attached to the same mobile device, inertial sensors attached to the mobile device could be used to estimate the motion or\nat least part of the motion.\nFrom at least part of the motion and intrinsic parameters of the camera 802 and 902, a transformation, such as a homography, can be computed.  Then, the transformation could be used to transform (or map) pixel coordinates between the images 810\nand 910.\nThe gaze image locations 825 and 826 in the image 810 may be transformed to the image 910 at the locations 925 and 926 (marked by \"X\") according to the computed transformation.  When multiple gaze image locations (921-926) in the image 910 are\navailable, similar embodiments as mentioned above could be applied analogously to detect or select image features of interest in the image 910.  For example, image features 931 and 933 may be determined as image features of interest.\nThe gaze image location 923 in the image 910 could also be transformed to the image 810 at the location 827 (marked by \"X\") according to the computed transformation.  When multiple gaze image locations (821-827) in the image 810 are available,\nimage features 831 and 833 may be determined as image features of interest according to the embodiments mentioned above.\nThen, according to an embodiment, the image features of interest (e.g., the image features 931 and 933) detected in the image 910 are matched to the image features of interest (e.g., image features 831 and 833) detected in the image 810. \nFeature correspondences could be estimated according to similarity measures (e.g. sum of squared differences (SSD), sum of absolute differences (SAD), normalized cross correlation (NCC), and mutual information (MI)).  Having the correspondences, 3D\npositions for the image features may be computed based on triangulation, like that proposed in Davidson et al. The 3D positions may be associated with the respective image features.\nAccording to a further embodiment, it is possible to assign weights to the image features detected in the at least one image according to the at least one gaze image location.  Then the image feature with assigned weights may be considered as\nthe at least one image feature of interest.\nFor example, the image features 831, 832 and 833 are detected in the image 810.  The image region 818 of the rabbit may be determined according to one or more gaze image locations based on any method disclosed herein.  In this example, the image\nfeatures 831 and 833 that are within the image region 818 may be assigned a weight A and the image feature 832 that is out of the image region 818 may be assigned a weight B. The weight A and the weight B are different.  The weight A may also be assigned\nto the image region 818 and the weight B is assigned to the rest of the image 810.  The weight A may be preferred to be higher than the weight B, as the weight A indicates a region of interest determined from the gaze image locations.\nThe weights may be determined according to the result of the clustering of a plurality of gaze image locations.  For example, one or more groups of the gaze image locations may be determined according to the clustering, and thus one or more\nimage regions may be determined based on the determined groups of the gaze image locations.  One image region may be determined based on one group of the gaze image locations.  A weight assigned to an image feature that is within or overlapped with an\nimage region may be determined according to the number of the gaze image locations used to determine the image region.  The larger number of the gaze image locations, the higher value may be determined for the weight.\nIn another implementation, the image feature, i.e. histogram, which is derived from pixels of an image region, may have a weight determined from gaze image locations in the image region.  For example, the weight may be computed from the number\nof the gaze image locations in the image region or from a size of a cover area of the gaze image locations.\nThe determined image features and related weights may be provided to a subsequent computer vision method, e.g. tracking, recognition, classification and 3D reconstruction, or may be saved as reference image features in a database.\nStep 1006 performs a computer vision algorithm according to the determined at least one image feature of interest.  Various computer vision algorithms and applications based on image features may be applied as disclosed herein.\nIn one embodiment, a computer vision algorithm may be performed according to the determined at least one image feature of interest and the related weights.  For example, a pose of the camera 802 relative to the rabbit 808 may be determined\naccording to the image features 831, 832 and 833 detected in the image 810 and corresponding reference (3D or 2D) features.  2D projection errors or 3D errors (e.g. Euclidean distances) computed from the image features detected in the image 810 and\ncorresponding reference (3D or 2D) features may be weighted based on respective weights associated with the image features 831, 832 and 833.  The pose could be determined by minimizing the 2D projection errors or 3D errors.\nStep 1007 stores the determined at least one image feature of interest.  For example, the determined at least one image feature of interest may be saved as at least one reference image feature in a database in a local computer (e.g. of the\nmobile device) or a remote computer (e.g. a web server).\nOptical-See-Through Embodiments:\nIn another embodiment using an optical see-through device (like a head mounted display with semitransparent glasses, as known in the art), the real object (like the rabbit) would not be shown on a display screen to the user.  Rather, the user\nwould directly see or observe the real object, e.g. through the semitransparent glasses.  Thus, a gaze location of an eye appears directly on the real object, which is called herein a gaze real location.\nIn order to relate the gaze real location on the real object with an image of the real object captured by a camera (e.g., attached to the head mounted display), a gaze direction, that is a direction from the eye to the gaze real location with\nrespect to the camera where the camera captures the image, may be used to determine an image position of the gaze real location in the image.  The determined image position is equivalent to a gaze image location as described above.  For this, depth\ninformation related to the image is used in order to find an intersection (i.e. the gaze real location) between the gaze direction and a surface of the real object in the camera coordinate system.  The depth information may be from a depth sensor or\nestimated from two images or provided from the camera.\nThe gaze direction may be directly determined based on an eye tracker, like Tobii eye trackers (e.g. see http://www.tobii.com) or a camera (e.g. like proposed in Blum et al.),\nThen, image features of interest extracted in the image may be determined from the determined gaze image location or locations according to the various embodiments mentioned above.\nIn another embodiment, the gaze image location may be computed from the gaze direction without using the depth information.  For example, the gaze direction from the eye to the gaze real location may be projected to the image plane of the camera\nwhere the camera captures the image.  In this case, the gaze direction may be projected as a line in the image.  This line would be the gaze image location.\nMultiple gaze directions may be determined with respect to the camera at one location or with respect to the camera at different locations.  When the camera is at different locations, the multiple gaze directions may be transformed to the camera\ncoordinate system of the camera at any of the different locations according to the camera motion between the different locations.  Having the multiple gaze directions, multiple projected lines in the image may be determined.  At least part of an image\narea between the multiple projected lines may be used to segment the object in the image (e.g. to determine the image region of interest).  Image features of interest may be extracted from the at least part of an image area between the multiple projected\nlines or from the determined image region of interest.\nFIG. 11 shows a workflow diagram of another embodiment of determining image features according to gaze location information, which may be applied in such optical-see-through embodiment.\nIn step 1101, there is provided at least one image of at least part of a real object captured by at least one camera.  According to step 1102, at least one gaze direction of at least one eye is determined with respect to the at least one camera,\nthe at least one camera being at a position where the at least one image is captured.  Step 1104 includes determining at least one gaze image location of at least one eye in the at least one image according to the determined at least one gaze direction. \nSteps 1105 to 1107 correspond to the respective steps 1005 to 1007 as described above with reference to FIG. 10.\nTo estimate at least one gaze direction of at least one eye, an eye tracker system (e.g. a commercial eye tracker (e.g. see http://www.tobii.com) or a camera that captures the at least one eye (e.g. see Blum et al.)) may be used to determine a\ngaze direction in a coordinate system associated with the eye tracker.  From a spatial relationship between the eye tracker and the camera that captures the image of the real object, the gaze direction could be determined with respect to the camera, the\ncamera being at a position where the image of the real object is captured.\nIn another embodiment, the user 806 may not look at the display screen 804, but directly observes the real environment including the rabbit 808 and table 807, while the user 806 holds the mobile device 801 and points the back facing camera 802\nto the rabbit 808.  The rabbit 808 may be recognized, tracked and/or reconstructed based on one or more images of the rabbit 808 captured by the back facing camera 802.  In this case, one or more gaze directions of the eye 805 with respect to the back\nfacing camera 802 may be determined based on an image of the eye 805 captured by the front facing camera 803 using a method; e.g. as proposed in Blum et al. In this case, to perform any feature detection, computer vision process and/or augmented realty\napplication (e.g. overlaying computer-generated graphics with the view of the real environment), the optical see-through device may not be required, while a video-see through device (e.g. the mobile device 801) may be employed.\nIt may be required to synchronize the process of capturing an image of the eye 805 by the camera 803 and the process of capturing an image of the rabbit 808 by the camera 802.\nAccording to an embodiment, the processing system as described herein may be comprised at least in part in one of the mobile devices 801, 901 and/or in the server computer 890 adapted to communicate with the mobile devices 801, 901, e.g.\nwirelessly.  The processing system may be comprised in only one of these devices, e.g. in the mobile device 801, 901 or in the server computer 890, or may be a distributed system in which one or more processing tasks (performing one or more method steps)\nare distributed and processed by one or more processing devices which are distributed and are communicating with each other.\nGenerally, the following further aspects and embodiments may be applied in connection with aspects of the invention.\nMany Augmented Reality (AR) applications may benefit from the present invention.  For example, in AR shopping, AR maintenance, and AR touring applications, there are multiple real objects located in the real world (for example, T-shirts for AR\nshopping, engine components for AR maintenance, and monuments for AR touring).  The user is often interested in one object (i.e. the object of interest) at a time.  The object of interest to the user could be determined according to the gaze of the user\nat that time.  Then, only the object of interest may be detected, tracked, or reconstructed.  Further, digital information related only to the object of interest could be generated and visually displayed in an image of the object in an AR view.\nGaze:\nGaze as used herein describes where the eye or eyes of a user is/are directed.  A gaze direction of an eye or two eyes of a user is a direction from the eye(s) of the user to where the eye(s) is/are looking at. A gaze location of an eye or eyes\nof a user is a location where the eye(s) of the user is/are looking at. The gaze location may be a point, an area (e.g. a circle, a square, etc.), a line, etc. A gaze image location of an eye is an image position (or image positions) of a gaze location\nof an eye in an image.  Similar to the gaze location, the gaze image location may be a point, an area (e.g. a circle, a square, etc.), a line, etc. A user should be understood as a person who is using the method or system as described herein, e.g. is\nlooking at the display screen.\nAttention Location:\nAn attention location of a user is a location where the user's attention direction focuses at in the real world.  The attention location may be computed as an intersection between the attention direction and at least part of the real world.  The\nuser's attention direction originates from the user position and may be represented as a direction axis or as a cone shape wherein the apex is defined by the user position.  The attention location may be a point, an area (e.g. a circle, a square, etc.),\na line, etc.\nAn attention screen location of a user is a screen position (or image positions) of an attention location of the user in a screen.  An image may be displayed on the screen.  An attention image location of the user is an image position (or image\npositions) of an attention location of the user in the image.  The attention screen location may be determined as an intersection between the attention direction and the screen plane.  The attention image location may be derived from the corresponding\nattention screen location based on where the image is displayed on the screen.\nThe attention image location in an image captured by a camera may also be derived from the corresponding attention direction and a spatial relationship between the camera and the corresponding attention direction.\nSimilar to the attention location, the attention image (or screen) location may be a point, an area (e.g. a circle, a square, etc.), a line, etc.\nA gaze direction is one specific example of the attention direction.  Analogously, a gaze screen location is one specific example of the attention screen location, and a gaze image location is one specific example of the attention image\nlocation.\nA face direction (e.g. the frontal face direction) is another specific example of the attention direction.  Analogously, a face direction screen location is another specific example of the attention screen location, and a face direction image\nlocation is another specific example of the attention image location.\nObject:\nAn object may be a real object which physically exists in the real world.  The real object could be an indoor office or an outdoor scene.  The real object could also be or include another real object, such as a sofa, a car, a human, a tree, a\nbuilding, or a picture.  An object may also be a virtual object which is digital information generated by a computer.  The virtual object can be rendered as a virtual image, which could be on a screen.  For example, the virtual object could be a virtual\nsofa or a virtual indoor room generated by a computer graphic software.  A virtual object may also include another virtual object.  For example, the virtual indoor room may include a virtual sofa.\nComputer Vision Algorithm:\nPotential computer vision methods or algorithms include processing, analyzing, and/or understanding images with the at least one knowledge of, but not limited to, camera intrinsic and/or extrinsic parameters, geometry, physics, statistics, and\nmachine learning theory.  A computer vision algorithm comprises at least one of, but is not limited to, image based recognition, image based tracking, image based reconstruction, image based classification, and image warping.\nThe image based recognition analyzes image features extracted in at least part of an image in order to recognize the image or identify at least one object visualized or captured in the image.  For example, in visual search applications, a\npicture (e.g., on a cover of a CD album) is captured by a camera in a camera image and the image based recognition would identify the picture by analyzing the camera image in order to trigger relevant actions.  For this, in one embodiment, an image\nregion of the picture in the camera image may first be determined and the image of the picture would be compared with some reference images stored in a computer system in order to identify the picture.  One challenge is to accurately detect the image\nregion of the picture.  In another embodiment, image features may be extracted from the camera image and matched with reference image features stored in a computer system.  However, extracted image features that do not belong to the picture of the CD\ncover may disturb the matching.\nComputer vision also includes 3D analysis from at least one image.  Vision based Simultaneous Localization and Mapping (SLAM) (e.g. see Davidson et al.) is a well-known technology for creating a geometrical model of a real environment (or real\nobject) without requiring any pre-knowledge of the real environment (or object) by using one or more images captured by a camera.  Further, SLAM could track the position of the camera relative to the real environment (or object).  The geometrical model\nthat has at least depth information is also referred to as a 3D map of the real environment.  The creation of the model of the environment is also called the reconstruction of the environment.  The reconstructed geometrical model could be represented by\na plurality of 3D image features (i.e. image features have associated 3D positions), such as 3D points, 3D edges, and/or 3D planes.\nAn object may be captured by a camera in at least one image.  The image based tracking could detect positions and/or orientations of the object in a 3D space or in the 2D space of the at least one image.  The image based tracking could also\ndetermine a pose of the camera.  For this, image features (e.g. features of the object captured in the at least one image) are often detected in the at least one image and used in various image based tracking methods to determine poses of the object\nand/or the camera.  However, detected image features that are not belonging to the object may disturb the tracking method.\nThe image based reconstruction is to compute a geometrical model of an object or a part of the object according to images of the object.  Commonly, the reconstruction requires at least two images of the object captured from two different\npositions by a camera or different cameras.  Image features are extracted in the at least two images and matched between the two images.  For a matched feature, a triangulation could be used to determine a 3D position of the matched feature, which is an\nintersection of two rays.  Each ray is defined by the image feature position in each of the two images and the camera optical center, the camera being at a position where the respective image is captured.  Thus, a plurality of 3D positions of image\nfeatures can be determined and used to compute the geometry of an object.  However, image features extracted in the two images may not relate to the object.  This would introduce errors or additional checks in the feature matching process.  If a matched\nfeature is not related to the object, the 3D position of the matched feature should not be used to determine geometry of the object.\nTherefore, it is preferred to have a method to remove the unrelated detected image features or to detect only related image features in images, as described herein with aspects of the present invention.\nGenerally, in the following, a further explanation of terms is given and the following further aspects and embodiments may be applied in connection with aspects of the invention.\nCamera:\nA camera is often called an imaging device or capturing device.  The proposed invention can generally be applied with any camera capable of providing images.  It is not restricted to cameras providing color images in the RGB format.  It can also\nbe applied to any other color format and also to monochrome images, for example to cameras providing images in grayscale format or YUV format.\nA camera often has intrinsic parameters including focal length and principal point.  A camera may capture light that is visible to the human eye.  A camera may also capture light that is invisible to the human eye, such as infrared light.  For\nexample, the camera may be a thermal imaging camera or infrared camera.\nA camera providing an image with depth data is often called a depth camera.  A depth camera system could be a time of flight (TOF) camera system or a passive stereo camera or an active stereo camera based on structured light.  A depth camera may\nprovide images whose pixels represent only depth data.  A depth camera may capture both the light and depth data in a single image, e.g. RGB-D image.  For the depth camera, the depth data does not need to be provided in the same resolution as the\n(color/grayscale) image.\nFor a standard camera model with a simplified pinhole or fisheye camera model, only light falling through a single point, i.e. the pinhole, is measured.\nThis invention may further use a light field camera that could capture an image from multiple different viewpoints and optionally with different focuses for the different viewpoints.  Light field cameras are not limited to capturing only light\nfalling through a single point, but measure also a plurality of light intensities incident at different locations.\nImage:\nAn image is any data depicting or recording visual information or perception.  The image could be a 2-dimensional image.  The image could also be a 3-dimensional image, e.g. a depth image.  An image may capture an object that reflects, refracts,\nand/or emits light that is visible and/or invisible to the human eye.  The image may be in the RGB format.  It can also be applied to any other color format and also to monochrome images, for example in grayscale format or YUV format.  For example, an\ninfrared image could capture an object that reflects, refracts, and/or emits light that is invisible to the human eye.\nA depth image may be a 2D (color/grayscale) image with a corresponding depth map.  The depth images do not need to be provided in the same resolution as the 2D image.  The depth image may also be called 3D image.  A depth image may only provide\ndepth data.\nDisplay Screen:\nA display screen visually displays digital information.  A display screen could be a reflective or emissive screen, e.g. LCD, LED, or OLED.  In Augmented Reality applications, the visual integration of digital information (e.g. a virtual object)\nand a real object may be performed using a video-see-though device which comprises a camera and a reflective or emissive screen.  In this configuration the camera captures an image of the real object or environment and then displays the captured image\noverlaid with a spatially registered computer-generated virtual object on the display screen.  This configuration is referred to as video-see-through AR.\nA display screen could also be a semi-transparent screen, like google glasses.  One example is to place an optical-see-though device between the user's eye and the real object.  The real object can then be directly observed through this\nsemi-transparent screen of the optical-see-though device, while the virtual object is computer-generated and shown on the semi-transparent screen.  This configuration is referred to as optical-see-through AR.\nAt least part of the display screen may be planar (e.g. a display surface) and may have a normal direction associated with the planar part.  Typically, the normal direction is perpendicular to the planar part.  The normal direction typically\npoints from the display device to a user who observes, in front of the display device, visual information displayed on the at least part of the display device.  At least part of the display screen may also be curved.\nImage Feature:\nA feature of an object is used to denote a piece of information related to the object.  The piece of information may be visually perceivable to anatomical eyes or optical imaging devices.  For example, a real object may emit or reflect visible\nlight that could be captured by human eyes or cameras.  The real object may also emit or reflect invisible light that could not be captured by human eyes, but could be captured by a camera (i.e. is optically perceivable).  In another example, the feature\nmay be associated with a virtual object (i.e. computer-generated object).  The feature of the virtual object may be known or detectable in a computer or computer program, like computer graphic simulation software.\nA feature may describe specific colors and/or structures, such as blobs, edge points, a particular region, and/or more complex structures of the real object.  A feature may be represented by an image patch (e.g. pixel intensity) or a high level\ndescriptor (e.g. SIFT, SURF).  A feature may have 3D position and/or 3D orientation information in 3D Euclidean space relative to a coordinate system of the real object.  This is often called a 3D feature.\nA feature (i.e. piece of information related to the real object) may be extracted from an image of the real object captured by a camera, and thus a feature may have 2D image position and/or orientation in a coordinate system of the image.  This\nis often called an/the image feature.  An image feature may be a 2D image feature or a 3D image feature.  When a camera could provide depth information, the feature extracted from an image of the camera may also have 3D position and/or orientation\ninformation.\nA feature could be described by an equation that describes a geometric shape, for example a point, ray, straight line, circle, cone, or cylinder.  Methods to detect features in an image that could be used in a method of the invention include,\nbut are not limited to, local extrema of Laplacian of Gaussian (LoG), Difference of Gaussians (DoG) or Determinant of Hessian (DoH), Maximally Stable ExtremalRegions (MSER), Harris features, or learning-based corner detectors such as FAST.  Also, methods\nthat detect edges (edge elements) are suitable to be used in such methods.  The feature detection method to be used is not limited to approaches working on a 2D intensity grayscale image, but can also be performed on images in any other color space,\nincluding RGB, HSV, and Lab, or range images that either exclusively contain depth information or provide both depth and intensity information.  A method to describe features can work on any of the types of images explained above and may include SIFT\n(Scale-invariant feature transform), SURF (Speeded Up Robust Feature), GLOH (Gradient Location and Orientation Histogram), HOG (Histogram of Oriented Gradients), 2.5D-SIFT, or any other method to compare local features including classification-based\nmethods like Random Ferns.\nReal Object:\nAn object may be a real object which physically exists in the real world.  A real object could be an indoor or an outdoor scene.  A real object could also be or include another real object, such as a sofa, a car, a human, a tree, a building, or\na picture.  A real world or real environment may also be considered as a real object.  The real environment may also include one or more real objects.  For example, a city or a street may be a real environment.  The street may further include buildings,\ntrees, and a parking lot.  The parking lot may be considered as another real environment.  An image of a real object captured by a camera may contain the whole real object or a part of the real object.\nAn object may also be a virtual object which is digital information generated by a computer.  The virtual object can be rendered as a virtual image, which could be on a screen.  For example, the virtual object could be a virtual sofa or a\nvirtual indoor room generated by a computer graphic software.  The virtual object may also include another virtual object.  For example, the virtual indoor room may include the virtual sofa.\nPose:\nA pose of a first object relative to a second object (e.g. a coordinate system associated with the second object) describes a rigid transformation including a translation and/or a rotation between the first object and the second object.  In some\nliteratures, the pose of a first object relative to a second object indicates a rigid transformation from the first object to the second object, while the pose of the second object relative to the first object indicates a rigid transformation from the\nsecond object to the first object.  Essentially, the pose of the second object relative to the first object may be considered equivalent to the pose of the first object relative to the second object, as they describe a transformation between the two\nobjects.\nIn 3D space, a pose may include information for six degrees of freedom (DOFs) or a part of the six DOFs.  The six DOFs include three DOFs for translations and three DOFs for rotations.  In 2D space, a pose may include information for three\ndegrees of freedom (DOFs) or a part of the three DOFs.  The three DOFs include two DOFs for translations and one DOF for rotations.\nA motion of a real object (e.g. a camera) describes a rigid transformation between the real object at one position and the real object at another position in a common coordinate system.\nCamera Pose Determination:\nA pose of a camera relative to a real world (e.g. a real object or a real environment) may be determined based on an image of at least part of the environment captured by the camera.  In one implementation, a model of the real object can be used\nfor model based matching.  The model based matching could, for example, be based on point features, edge features, or image patches of any size and form.  While point features are frequently used for highly textured objects, edge features are preferred\nif the real object has little texture.  Model based matching requires the image used for pose determination to contain at least part of the real object described by the model.  The real object could for example also include a fiducial marker in the\nenvironment.  A model of the real object could be described by points, edges, polygons or their combinations.  A model of the real object may also contain texture information, e.g. colors.\nDetermining a pose of the camera can also be realized by using a visual marker.  This requires the visual marker at a known position relative to the real object.  In this case, the camera pose with respect to the real object could be determined\naccording to a camera pose with respect to the visual marker, which is estimated based on an image of the camera containing the visual marker.  It is not necessary for the image to contain at least part of the real object when the visual marker is used\nfor the camera pose determination.\nA camera motion between two camera positions may be determined by camera poses at the two camera positions.  A camera motion may also be determined according to two images captured by the camera at the two camera positions.  For example, common\nimage features in the two images may be employed to determine the camera motion.\nSpatial Relationship:\nA spatial relationship specifies how an object is located in 2D or 3D space in relation to another object.  For example, a spatial relationship is defined in terms of translation, and/or rotation, and/or scale.  A spatial relationship may be a\nrigid transformation.  A spatial relationship may define topological relations between a first object and a second object.  The topological relations may include at least one of, but is not limited to, equals, disjoints, intersects, touches, covers,\ncovered by, and within.  A spatial relationship may define directional relations between a first object and a second object.  For example, the spatial relationship may indicate the first object is on the back inside the second object.  In another\nexample, the spatial relationship may indicate the first object is behind (outside) the second object.  A spatial relationship may define distance relations between a first object and a second object, e.g. at; nearby; in the vicinity; far away.\nPurchasing Information:\nAny information related to a process of purchasing one or more items may be regarded as purchasing information of the one or more items.  In an example, the purchasing information of a jacket could be the location of a real store or the web link\nof an online shop where the jacket is available for purchasing.  The purchasing information could also be prices or material.  The purchasing information could also be an order or invoice.  The purchasing information may also be the availability (e.g. in\nstock or in 3 days available) of a jacket in a shop.", "application_number": "14581609", "abstract": " The present disclosure relates to a method of providing at least one\n     image of at least one real object captured by at least one scene camera\n     of a plurality of scene cameras mounted on a vehicle. The method\n     includes: providing camera poses of respective scene cameras of the\n     plurality of scene cameras relative to a reference coordinate system\n     associated with the vehicle, providing user attention data related to a\n     user captured by an information capturing device, providing at least one\n     attention direction relative to the reference coordinate system from the\n     user attention data, determining at least one of the scene cameras among\n     the plurality of scene cameras according to the at least one attention\n     direction and the respective camera pose of the at least one of the scene\n     cameras, and providing at least one image of at least one real object\n     captured by the at least one of the scene cameras.\n", "citations": ["20050202844", "20070008091", "20080181591", "20090128618", "20090299684", "20100049393", "20100201816", "20140093187", "20140139655", "20140204193", "20140354798", "20150006278", "20150221341"], "related": ["2014068165"]}, {"id": "20170078376", "patent_code": "10375156", "patent_name": "Using worker nodes in a distributed video encoding system", "year": "2019", "inventor_and_country_data": " Inventors: \nCoward; Michael Hamilton (Solano Beach, CA), Puntambekar; Amit (Fremont, CA)  ", "description": "<BR><BR>BACKGROUND\nUniversal availability of digital video cameras and storage products has enabled new uses of digital video.  In contrast to the traditional network delivery of video, e.g., digital satellite or cable television, more and more video is being\ncaptured by users using their cameras and smartphones and shared over the Internet with others.\nWhile the availability of internet-connected video capturing devices has soared, the ability of these devices to encode, or compress, and upload the captured audio/video content remains limited.  Further, these devices, such as\ninternet-connected cameras, tablets and smartphones, may frequently be offline and thus rely on a service provider to make content captured from the devices available to others at the time and place others want to view the content. <BR><BR>BRIEF\nDESCRIPTION OF THE DRAWINGS\nThe techniques introduced here may be better understood by referring to the following Detailed Description in conjunction with the accompanying drawings, in which like reference numerals indicate identical or functionally similar elements:\nFIG. 1 illustrates a prior art video encoding system;\nFIG. 2 illustrates an example of a distributed video encoding system;\nFIG. 3 illustrates another example of a distributed video encoding system;\nFIG. 4 illustrates an example method of placing key frames at desired locations in an output video bitstream;\nFIG. 5 illustrates an example of a distributed video encoding system for computing and allocating bits on a per frame basis using a complexity analyzer;\nFIG. 6 illustrates another example of a distributed video encoding system for computing and allocating bits on a per frame basis using a complexity analyzer;\nFIG. 7 another example of a distributed video encoding system for computing and allocating bits on a per frame basis using a complexity analyzer;\nFIG. 8 illustrates an example of a method for computing and allocating bits on a per frame basis in a distributed video encoding using a complexity analyzer;\nFIG. 9 illustrates an example of a method for computing and allocating bits per frame in a distributed video encoding using a separate complexity analyzer for each segment of a video;\nFIG. 10 illustrates an example of a method for computing and allocating bits per frame in a distributed video encoding by using a separate complexity analyzer for each segment of a video and adding a pathway that passes each segment complexity\nmeasurement to a corresponding encoder;\nFIG. 11 illustrates an example of a communication network for video uploading and sharing;\nFIG. 12 illustrates example graphs illustrating relationships between encoding time and quality and video bitrate and encoding effort;\nFIG. 13 illustrates an example time line of video uploading;\nFIG. 14 illustrates an example method for uploading digital video from a user device to a network server;\nFIG. 15 illustrates an example of a distributed video encoding workflow;\nFIG. 16 illustrates an example of a video frame encoding sequence;\nFIG. 17 illustrates another example of a video frame encoding sequence;\nFIG. 18 illustrates an example of operation of an intelligent segmenter;\nFIG. 19 illustrates a method for encoding a video by changing positions of at least some key frames in the resulting encoded bitstream;\nFIG. 20 illustrates an example graph of relationship between encoder parameter settings and quality of encoded video;\nFIG. 21 illustrates an example system for performing ultra-high video encoding using multiple encoders;\nFIG. 22 illustrates an example flowchart of a method for encoding a video;\nFIG. 23 illustrates an example of destabilization caused in video frames due to camera movement;\nFIG. 24 illustrates an example block diagram for generating a transform file from an encoded video;\nFIG. 25 illustrates an example block diagram of distributed image stabilization of a video;\nFIG. 26 illustrates an example block diagram of performing distributed image stabilization using multiple worker nodes;\nFIG. 27 illustrates an example block diagram of receiving results of image stabilization from multiple worker nodes;\nFIG. 28 illustrates an example block diagram of generating an image stabilized video at a master node;\nFIG. 29 illustrates an example flowchart of a method of distributed image stabilization;\nFIG. 30 illustrates an example of a distributed video encoding system;\nFIG. 31 illustrates an example flowchart of a decision process for node selection performed at a master node in a distributed video encoding system;\nFIG. 32 illustrates an example flowchart of a decision process at a worker node in a distributed video encoding system;\nFIG. 33 illustrates another example flowchart of a decision process at a worker node in a distributed video encoding system;\nFIG. 34 illustrates an example flowchart of a method of distributed video encoding by assigning video segments to worker nodes;\nFIG. 35 illustrates an example of a video encoding pipeline in a distributed video encoding system;\nFIG. 36 illustrates an example flowchart of a method of distributed encoding of multiple videos;\nFIG. 37 illustrates an example server apparatus;\nFIG. 38 illustrates an example of video complexity as a function of time.\nWhile the flow and sequence diagrams presented herein show an organization designed to make them more comprehensible by a human reader, those skilled in the art will appreciate that actual data structures used to store this information may\ndiffer from what is shown, in that they, for example, may be organized in a different manner; may contain more or less information than shown; may be compressed and/or encrypted; etc.\nThe headings provided herein are for convenience only and do not necessarily affect the scope or meaning of the claimed embodiments.  Further, the drawings have not necessarily been drawn to scale.  For example, the dimensions of some of the\nelements in the figures may be expanded or reduced to help improve the understanding of the embodiments.  Similarly, some components and/or operations may be separated into different blocks or combined into a single block for the purposes of discussion\nof some of the embodiments.  Moreover, while the various embodiments are amenable to various modifications and alternative forms, specific embodiments have been shown by way of example in the drawings and are described in detail below.  The intention,\nhowever, is not to limit the particular embodiments described.  On the contrary, the embodiments are intended to cover all modifications, equivalents, and alternatives falling within the scope of the disclosed embodiments as defined by the appended\nclaims.\n<BR><BR>DETAILED DESCRIPTION\nVarious of the disclosed embodiments disclose allocating a number of bits on a per-frame basis in a distributed video encoding using a complexity analyzer.  The analyzer receives an input video for allocating bits for each segment of the input\nvideo based on the complexity of the segment and splits the input video into multiple segments.  The analyzer determines the bits per frame to be allocated to the multiple segments based on the complexity measurement of the input video.  In some\nembodiments, a single complexity analyzer can be used to determine the complexity of the plurality of segments.  In another aspect, a separate complexity analyzer can be used for each segment to determine the complexity.  The analyzer can allocate the\nnumber of bits to the multiple segments of the input video and combine the multiple segments to form a single output video; thereby, obtaining the encoded output video.\nVarious of the disclosed embodiments disclose encoding a segment of an input video with accurate placement of key frames, e.g., I-Frames, using Group of Pictures (GOPs) length required for output in a distributed video encoding system with two\nor more encoders.  The encoder creates overlapping segments to allow precise key frame placement, uses the desired segment length and the desired key frame placement to create the overlapping segments, and can skip a number of frames at the beginning of\nthe segment and a certain number of frames at the end of the segment, thereby achieving accurate placement of key frames.\nVarious of the disclosed embodiments disclose techniques in which video is transmitted from a client device to a server in the network by minimizing the amount of time it takes to upload the video from the client device to the server while being\nencoded from a first video encoding format in which the video is stored at the client devices to a second video encoding format specified by a user and/or by the server.  Operational conditions such as fullness of a transmission buffer at the client\ndevice and the target quality of encoded video are used to control the video encoding operation.\nVarious of the disclosed embodiments relate to a distributed video encoding system that splits an input video into video segments.  The video segments are encoded using multiple video encoding nodes.  Prior to splitting the video into video\nsegments, the video is analyzed to ensure that each video segment includes all the video frames from which other video frames within that segment have been encoded in the input video.  For example, picture headers are inspected to determine the temporal\ndistance of the farthest past and future reference frames used for encoding frames of a video segment.\nVarious of the disclosed embodiments describe techniques in which multiple video encoders are used to simultaneously encode a video using encoders configured using different encoding parameters.  A segment selector selects an encoded version of\nthe encoded video segment using operational criteria such as video quality and bandwidth.  A configuration determination module may analyze the video segment to make a decision about which encoding parameter configurations may be suitable for encoding\nthe video segment.  The configuration determination module may be trainable, based on past encoding results.\nVarious of the disclosed embodiments perform stabilization of images in a video.  In a distributed video encoding system, a video is encoded by splitting into video segments and encoding the segments using multiple encoders.  Prior to segmenting\nthe video for distributed video encoding, image stabilization is performed on the video.  For each frame in the video, a corresponding transform operation is first computed based on an estimated camera movement.  Next, the video is segmented into\nmultiple video segments and the corresponding per-frame transform information for the multiple video segments.  The video segments are then distributed to multiple processing nodes that perform the image stabilization of the corresponding video segment\nby applying the corresponding transform.  The results from all the stabilized video segments are then stitched back together for further a video encoding operation.\nVarious of the disclosed embodiments relate to a distributed video encoding or transcoding system that may utilize multiple encoding nodes to encode/transcode a video by splitting the video into multiple smaller video segments.  The assignment\nof video segments to the encoding nodes is performed to balance the use of the encoding nodes by selecting a node based on its encoding capabilities, e.g., whether the node employed a central processing unit (CPU) based encoding or graphics processor\nunit (GPU) based encoding.\nVarious of the disclosed embodiments disclose video encoding or transcoding operation in a distributed video encoding system to meet a service layer agreement such as a target time for completion of the video encoding operation.  Each of\nmultiple videos being encoded is split into smaller jobs corresponding to video segments of the videos.  The jobs are processed according to a job queue.  The sequence of jobs in the job queue is periodically updated such that jobs corresponding to each\nvideo are processed within the corresponding target encoding time.\nIn various embodiments, distributed video and image processing may be performed using computational resources that are available on hardware platforms that are communicatively coupled to each other via network connections.  The network between\ntwo such computational hardware platforms may be geographically distributed, and the communication may travel over networks that may or may not be controlled by the same entity that controls the availability and use of the computational resources.  For\nexample, the computational resources may include servers available in the cloud at unspecified or unknown locations, e.g., at locations not known to the video encoders.  As another example, the computational resources may be available on servers\navailable at data centers managed by a social networking service provider.\nVideo is being produced and consumed more than ever, and in more formats, device types and variety of networks than ever.  Transcoding, or encoding, is a process of translating or converting a video encoded from one format to another.  In some\nembodiments, only bitrate of the video bitstream may be changed.  Encoding is a computationally intensive process.  Due to the proliferation of video content used, distributed computing approach has been applied to video encoding to exploit the extra\ncomputational resources available among multiple machines, multi-core CPUs, and distributed computing resources in a given facility, home or a dedicated cloud computing infrastructure, and so on.  This distributed encoding approach has been helpful in\nreducing rendering and start-up time of a video for on demand encoding.\nEncoding systems take an input video, composed of many frames (I, P, or B frame types), and create a new output video, often with a different output resolution or bit-rate and often with different frame types.  In normal encoding systems, there\nis no relation between the placement of I-Frames in the input video and the output video; the encoding system can output frames in any order and type specified by the user.  This is important because the type of frames used has a significant impact on\nthe output bit-rate and quality, and selection is done based on output bit-rate and/or quality requirements.\nTraditional video encoding systems use a single encoder to encode input video.  In this implementation, it is easy to configure the system to place I-Frames accurately in the output video.  As the encoder is working through the input video, it\ncan make a decision for the frame type for each frame of the output video based on user parameters.\nDistributed Encoding Systems enable multiple different computers to work together to create a single output video by encoding individual segments of the video before the encoded segments are joined together into a single output vide.  The input\nvideo is broken into output segments, where the segments are divided at I-Frames.  When a computer takes an input video segment and creates an output video segment, it must start the output video segment with an I-Frame.  This means that there is now a\nrelation between the placement of the I-Frames in the input video and in the output video, and it makes it difficult to guarantee accurate I-Frame placement in the output video, using known mechanisms.\nVarious examples of the disclosed techniques will now be described in further detail.  The following description provides specific details for a thorough understanding and enabling description of these examples.  One skilled in the relevant art\nwill understand, however, that the techniques discussed herein may be practiced without many of these details.  Likewise, one skilled in the relevant art will also understand that the techniques can include many other obvious features not described in\ndetail herein.  Additionally, some well-known structures or functions may not be shown or described in detail below, so as to avoid unnecessarily obscuring the relevant description.\nThe terminology used below is to be interpreted in its broadest reasonable manner, even though it is being used in conjunction with a detailed description of certain specific examples of the embodiments.  Indeed, certain terms may even be\nemphasized below; however, any terminology intended to be interpreted in any restricted manner will be overtly and specifically defined as such in this section.\nOverview--Example Network Topology\nTurning now to the figures, FIG. 11 Illustrates an example of a communication network 1100 in which a distributed network 1102 is operating to facilitate the use of digital video among multiple users--user 1104 who may be the originator of a\nvideo using his user device 1106 and users 1114 with their user devices 1116, who may download and view the video sequence that the user device 1106 uploads to a server 1110 in the distributed network 1102.  The user devices 1106, 1116 may communicate\nwith the distributed network 1102 via communication networks or channels 1108, 1112.  The channels 1108, 1112 may be same or different networks, e.g., the Internet or World Wide Web, and may change from time to time based on location.  For example, user\ndevices 1106, 1116 may include multiple network modules, e.g., a Wi-Fi modem, a 3G modem, a 4G modem, a WiMax modem, etc. and may use one or more of the network connections for communication in the upstream direction (from the device to the network) or\nthe downstream direction (from the network to the device).\nIn one example use case, the user 1104 may capture a video clip using a camera-enabled smartphone (user device 1106).  The user 1104 may then instruct the user device 1106 to upload the video to the server 1110, which may be a server operated by\na service provider, e.g., a social media website.  Once the user device 1106 uploads the video to the server 1110, the service provider may operate the distributed network 1102, e.g., a geographically distributed server farm, to propagate the\navailability of the user's video clip to other users with whom the user 1104 wishes to share the video clip, e.g., user 1114.\nIn conventional video upload and sharing systems, a user typically cannot control the video upload process.  For example, many users capture videos using their mobile phones, but cannot control the quality or other encoding parameters when the\ncaptured video is converted into a compressed digital video bitstream.  Further, the user may be able to initiate video uploading to a social media website or to a service provider's file transfer protocol (ftp) site, but may not be able to control\noperational characteristics of the upload such as the upload speed or quality throttling of the uploading process in real time.\nFurthermore, depending on the user's operating condition, the user's preferences for how the video should be uploaded may change.  For example, when the user is on the go, the user may want uploading to happen as fast as possible, where video\nquality may be sacrificed to meet faster upload performance.  As another example, in another operational scenario, the user may want to upload the video by encoding at the highest possible quality, e.g., when the user device is operating in a Wi-Fi\nbroadband network.  In yet another operating scenario, the user may incur byte-based uploading costs and thus may want to upload a video using the minimum file size while still meeting some low quality threshold.  In some embodiments, encoded video\nquality may be held constant, but encoding effort may be varied based on the available bandwidth for upload.\nMany service providers today operate distributed computer networks that can propagate user-uploaded files to geographically distributed file stores for access from anywhere in the world.  These service providers may also interface with and rely\non content delivery network (CDN) infrastructure such as CDNs operated by Akamai or LimeLight Networks.  When it comes to the distribution of user-uploaded videos though, the service providers often fail to fully utilize the available computational power\nin a distributed network.\nIn traditional distributed video encoding systems as shown in FIG. 1, the input video is split into multiple segments by a splitter module.  Each video segment is then passed onto an encoder, where the encoder processes the received video\nsegment into an output video segment (i.e., changing the video segment from the source format to other format which is compatible with any other device).  The output video segments from all the video encoders are combined together as shown in the FIG. 1;\nthereby, achieving a single output video.\nFor example, if the input video has the I-Frame for every 60 frames then there will be an I-Frame at frame 0, 60, 120, 180, 240, 300, 360, etc. The distributed video encoding system will break up the input video at predetermined points, and pass\neach segment to the separate encoder.  If the configured split size is 600 frames, each encoder will receive 600 frames to encode.  If the user wants the output video to have an I-Frame every 250 frames, it is not possible for the system to follow this\nrestriction.  Each encoder will place the I-Frame at frame 0, frame 250, and frame 500.  When 2 of these pieces are combined together, the output video will have frames at 0, 250, 500, 600, 850, and 1100; thus not achieving the task of placing an I-Frame\nevery 250 frames as specified by the user\nThus, it would be beneficial to accurately place I-Frames in the output video when a distributed video encoding system is used to produce the output video.\nThe techniques disclosed herein can be used to, among other uses, address the shortcomings of present-day video uploading and processing in a distributed network.  These, and other, detail are disclosed in the present document.\nIn some embodiments, input video is segmented into overlapping segments that each contain the requisite data to construct an output segment that begins with a key frame (e.g., an independently decodable frame) and has a desired length.  The\noverlapping segmentation is performed such that everywhere there is a desired output key frame, there will be enough data after that location to have a complete segment and enough data before that location to have an input key frame.  To achieve this,\nevery encoder is configured to ignore extra video frames before and/or after the desired output segments.\nFIG. 2 illustrates a high level overview of a distributed video encoding system 200 to accurately place I-Frames in an output video using a controller module, according to the embodiments as disclosed herein.  In an embodiment, the distributed\nvideo encoding system 200 includes a splitter module 102, a controller module 202, one or more encoder modules 104.sub.1-N (hereafter referred as encoder module(s) 104), and a combiner module 106.\nThe splitter module 102 can be configured to receive an input video including an I-Frame for some number of (e.g., 60) frames of the input video that is to be encoded from a source format to any other format.  The splitter module 102 splits the\nreceived input video into a plurality of segments each including a specified number of frames.  Here, the splitter module 102 can be configured to receive information from the controller module 202 to split the received input video into pieces of length\n(i.e., segment length) having 600 frames and also placement of I-Frames for every 250 frames (Output Group of Pictures (GOP) length) of the input video.  For example, the splitter module 102 splits the input video into a video segment-1, a video\nsegment-2, and a video segment-3 as shown in the FIG. 2.  Here, the video segment-1 includes 0 to 539 frames of the input video.  The video segment-2 includes 480 to 1019 frames of the input video.  The video segment-3 includes 960 to 1500 frames of the\ninput video.  Each segment includes one or more overlapping frames at the beginning of the segment with immediately preceding segment, and one or more overlapping frames at the end of the segment with immediately following segment.  Also, the two\nsequential segments carry the starting frame of the overlapping GOP.\nThe splitter module 102 determines a number of output GOPs that can fit into each segment length.  Here, the splitter module 102 determines 2 output GOPs for each segment of the input video.  In an embodiment, the number of frames in each\nsegment can be based on for example, a scene changes in the input video.  For example, if the video segment contains the scene change, the frames before the scene change could be drastically different than the frames after the scene change.  The encoding\nof the segment may involve significant challenges.  In this case, it would be desirable to alter the length of the segment such that it may not include the scene change for the effectively performing the encoding.\nThe splitter module 102 segments entire input GOPs to the segment until the segment has enough frames to create the desired number of output GOPs.  The splitter module 102, for the first segment, 540 frames (i.e., start frame 0 to 539 frames)\ncan be segmented so that the 2 output GOPs of length 250 each (i.e., 500 frames total) can be supported.  The splitter module 102, for the second segment and all subsequent segments, the algorithm will repeat this process, but it will first rewind by one\nor more GOPs in the input.  The second segment starts at frame 480 so that the segment includes frame 500, which is used for the 2nd output GOP.  The third segment starts at frame 960 so that the segment includes frame 1000, which is used for the 3rd\noutput GOP.  The splitter module 102 sends the length of each segment, and the start and stop frame of each segment to the controller module 202.\nUnlike conventional systems, the controller module 202 can be configured to send information to split the received input video into various pieces of length (e.g., segment length) and also placement of I-Frames in the input video.  The\ncontroller module 202 can be configured to send a number of frames to skip at the beginning of each video segment, where to place each I-Frame, and a number of frames to skip at the end of each video segment to the encoder module 104.\nFurther, the splitter module 102 outputs the video segments (i.e., video segment-1, video segment-2, and video segment-3) to the encoder modules 104.  After receiving multiple segments from the splitter module 102, the encoder module 104 can\nencode according to a Windows Media Video or VC-1 format, MPEG-x format (e.g., MPEG-1, MPEG-2, or MPEG-4), H.26x format (e.g., H.261, H.262, H.263, or H.264), or other format.  In an embodiment, the encoder module 104 receives the number of frames to\nskip at the beginning of the frame and at the end of the frame from the controller module 202 as shown below:\nVideo Segment-1 (0 to 539 frames): Skip \"0\" frame at the beginning of the video segment and place the I-Frame at every 250 frames of the video segment.  Skip \"40\" frames at the end of the video segment.\nVideo Segment-2 (480 to 1019 frames): Skip \"20\" frames at the beginning of the video segment and place the I-Frame at every 250 frames.  Skip \"20\" frames at the end of the segment.\nVideo Segment-3 (960 to 1500 frames): Skip \"40\" frames at the beginning of the video segment and place the I-Frame at every 250 frames.  Skip \"0\" frames at the end of the segment.\nFurther, the encoder module 104 then processes each video segment received from the splitter module 102 and by using the information from the controller module 202 to determine how many frames to skip and then skips the frames at the beginning\nof the frame and at the end of the frame for accurate placement of I-Frames.  The video segments from the encoder module 104 are sent to the combiner module 106.  On receiving the video segments from the encoder module 104, the combiner module 106\ncombines the plurality of segments to form the single encoded output video with accurate placement of I-Frames.\nFIG. 2 illustrates a limited overview of a distributed video encoding system 200 but it is to be understood that another embodiment is not limited thereto.  Further, the system 200 can include any number of video processing modules and\ntechniques along with other hardware or software components communicating with each other.  For example, the component can be, but not limited to, a process running in the controller or processor, an object, an executable process, a thread of execution,\na program, or a computer.  By way of illustration, both an application running on a device and the device itself can be a component.\nFIG. 3 illustrates another high level overview of a distributed video encoding system 300 to accurately place I-Frames in an output video, according to embodiments as disclosed herein.  In an embodiment, the distributed video encoding 300\nincludes a splitter module 102, a controller module 202, one or more encoder modules 104.sub.1-N (hereafter referred as encoder module(s) 104), and a combiner module 106.\nThe detailed operations of each module included in the FIG. 3 are explained in conjunction with the FIG. 2.  The FIG. 3 illustrates a limited overview of a distributed video encoding system 300 but it is to be understood that another embodiment\nis not limited thereto.  Further, the system 300 can include any number of video processing modules and techniques along with other hardware or software components communicating with each other.  For example, the component can be, but not limited to, a\nprocess running in the controller or processor, an object, an executable process, a thread of execution, a program, or a computer.  By way of illustration, both an application running on a device and the device itself can be a component.\nFIG. 4 is a flow diagram illustrating a method to accurately place I-Frames in an output video using a controller module in a distributed video encoding system, according to embodiments as disclosed herein.  At block 402, the method 400 includes\nreceiving an input video for converting from a source format to any other format with accurate placement of I-Frames.  The method 400 allows the splitter module 102 to receive the input video for converting from a source format to any other format with\naccurate placement of I-Frames.  At block 404, the method 400 includes receiving the information to split the input video into a predetermined segment length and also placement of I-frames from the controller module 202.  Unlike conventional systems, the\nmethod 400 allows the splitter module to receive the information to split the input video into a predetermined segment length and also placement of I-frames from the controller module 202.  For example, the splitter module 102 receives information from\nthe controller module 202 to split the received input video into pieces of length (i.e., segment length) 600 frames and also placement of I-Frames for every 250 frames (Output Group of Pictures (GOP) length) of the input video.\nAt block 406, the method 400 includes determining the segment length based on a number of GOPs that can be included in each segment.  The method 400 allows the splitter module 102 to determine the segment length based on the number of GOPs that\ncan be included in each segment of the input video.  At block 408, the method 400 includes segmenting the received input video into plurality of segments after determining the segment length.  The method 400 allows the splitter module 102 to determine\nthe segment length based on the number of GOPs that can be included in each segment of the input video.  In an embodiment, the splitter module 102 splits the input video into a video segment-1, a video segment-2, and a video segment-3.  Here, the video\nsegment-1 includes 0 to 539 frames of the input video.  The video segment-2 includes 480 to 1019 frames of the input video.  The video segment-3 includes 960 to 1500 frames of the input video.  The splitter module 102 determines 2 output GOPs that can\nfit into each segment length.\nAt block 410, the method 400 includes sending the input video along with the length of the video segment, start and stop frames for each segment to be encoded.  Unlike conventional systems, the method 400 allows the splitter module 102 sends the\ninput video along with the length of the video segment, start and stop frames for each segment to be encoded to the controller module 202 to decide on the number of frames to skip at the beginning and at the end of each segment.  Also, the splitter\nmodule 102 sends the plurality of segments to the encoder modules 104.  At block 412, the method 400 includes sending the information to plurality of encoders on the I-Frame position, number of frames to skip at the beginning of the video segment, and\nnumber of frames to skip at the end of the video segment.  The method 400 allows the encoder module 104 to receive the information to plurality of encoders on the I-Frame position, number of frames to skip at the beginning of the video segment, and\nnumber of frames to skip at the end of the video segment to accurate place I-Frames in the video segment.\nIn an embodiment, a separate encoder can be used to receive each segment of the video for transferring the segment from one format to another format while achieving accurate placement of the I-Frames in the input video.  For example, consider an\nencoder-1, an encoder-2, and an encoder-3 receives an input video split into three segments namely, segment-1, segment-2, and segment-3 respectively.  Also, the encoder-1 receives information to skip `0\" frames at the beginning of the video segment and\nplacing the I-Frame at every 250 frames of the video segment.  Skip \"40\" frames at the end of the video segment.  The encoder-2 receives information to skip \"20\" frames at the beginning of the video segment and placing the I-Frame at every 250 frames. \nSkip \"20\" frames at the end of the segment.  The encoder-3 receives Skip \"40\" frames at the beginning of the video segment and place the I-Frame at every 250 frames.  Skip \"0\" frames at the end of the segment.  The encoder-1 Unlike conventional systems,\ndifferent encoder's uses information received from the controller module 202 to accurately place I-Frames in each segment of the video such as to generate a uniform quality of video.\nFurther, at block 414, the method 400 includes encoding the received segments and places the I-Frames using the information from the controller module 202.  The method allows the encoder module 104 to encode the received segments and places the\nI-Frames using the information from the controller module 202.  At block 416, the method 400 includes combining the plurality of segments received from the encoder module 104 to form a single encoded output video with accurate placement of I-Frames.  The\nmethod 400 allows the combiner module 106 to combine the plurality of segments to form the single output video.  For example, the segment-1 from the encoder-1, the segment-2 from the encoder-2, and the segment-3 from the encoder-3 are combined to form\nthe single output video.  Unlike conventional systems, creating overlapping segments in order to allow precise I-Frame placement in the input video.  The splitter uses the desired segment length and the desired I-Frame placement to create overlapping\nsegments.  The encoder skips the number of frames at the beginning and at the end of the video segment for accurate placement of I-Frames.\nThe various actions, acts, blocks, steps, and the like in the method 400 may be performed in the order presented, in a different order or simultaneously.  Further, in some embodiments, some actions, acts, blocks, steps, and the like may be\nomitted, added, modified, skipped, and the like without departing from the scope of the invention.\nTraditionally, a variable bit-rate (VBR) bitstream is created using a single-pass encoding or a multi-pass encoding.  The single-pass encoding analyzes and encodes the data \"on the fly\" and it is used in a constant bit-rate (CBR) encoding.  The\nsingle-pass encoding is used when the encoding speed is most important (e.g., for real-time encoding).  The single-pass VBR encoding is usually controlled by the fixed quality setting or by the bit-rate range (i.e., minimum and maximum allowed bit-rate)\nor by the average bit-rate setting.  The multi-pass encoding is used when the encoding quality is most important.  The multi-pass encoding may not be used in real-time encoding and live broadcast or live streaming scenarios.\nThe multi-pass encoding is typically used for the VBR encoding because the CBR encoding doesn't offer any flexibility to change the bit-rate.  VBR encoding allows a higher bit-rate to be allocated to the more complex segments of the input video\nwhile a lower bit-rate is allocated to the less complex segments of the input video.  The average of these bit-rates can be calculated to produce the average bit-rate for the video.  The most common VBR encoding is a two-pass encoding.  In the first pass\nof the two-pass encoding, the input data is being analyzed to locate the scenes that have higher complexity (i.e., which require a larger amount of bits in order to achieve the desired quality) and the result is stored in a log file.  In the second pass,\nthe collected data from the first pass is used to allocate bits-per-frame to each frame in the video for achieving the best video encoding quality.  In the video encoding process, the two-pass encoding is usually controlled by the average bit-rate\nsetting or by the bit-rate range setting (e.g., minimal and maximal allowed bit-rate) or by the target video file size setting.\nHowever, in the distributed video encoding process, the input video can be split into multiple segments and the multiple segments are assigned to different computers to perform the video encoding process.  Here, each computer, on receiving a\nvideo segment, performs video encoding within the single segment available to it.  The distributed video encoding system performing video encoding using variable number of bits per frame within the single segment may not achieve good encoding performance\nacross multiple segments as each computer is working on a single segment and thus cannot access information about complexity in other segments to better allocate a number of bits per frame.  For example, consider a scenario where the input video has a\nhigh complexity region in the middle of the video.  The high complexity region of the video is allocated (in a non-distributed system) with additional bits to deliver good video quality.  In a traditional distributed video encoding system where the input\nvideo is split into multiple segments, each segment encoder achieves the average bit-rate for its segment and therefore at the video level, bits are undesirably allocated to the high and low complexity segments of the video equally.\nIn traditional distributed video encoder systems as shown in FIG. 1, the input video is split into multiple segments by a splitter module.  Each video segment is then passed onto an encoder, where the encoder processes the received video segment\ninto an output video segment (i.e., changing the video segment from the source format to other format which is compatible with any other device).  The output video segments from all the video encoders are combined together as shown in the FIG. 1;\nthereby, achieving a single output video.\nThus, it is beneficial to accomplish high quality encoding by allocating a variable number of bits per frame in the distributed video encoding system; thereby, maintaining uniform quality across the video.\nFIG. 5 illustrates a high level overview of a distributed video encoding 500 for allocating a variable number of bits per frame in a distributed video encoding using a complexity analyzer, according to the embodiments as disclosed herein.  In an\nembodiment, the distributed video encoding 500 includes a splitter module 502, a complexity analyzer module 504, one or more encoder modules 506.sub.1-N (hereafter referred as encoder module(s) 506), and a combiner module 508.\nThe splitter module 502 can be configured to receive an input video that is to be encoded from a source format to any other format.  Here, the input video generally refers to a stream including both audio components and video components.  The\nsplitter module 502 splits the received input video into a plurality of segments each including a certain number of frames.  For example, the splitter module 502 splits the input video into a video segment-1, a video segment-2, and a video segment-3.  In\nan embodiment, each segment includes 60 frames.  In another embodiment, each segment can vary across a range of values such as including between 30 frames to 90 frames.  The number of frames in each segment can be based on for example, a scene changes in\nthe input video.  For example, if the video segment contains the scene change, the frames before the scene change could be drastically different than the frames after the scene change.  The encoding of the segment may involve significant challenges.  In\nthis case, it would be desirable to alter the length of the segment such that it may not include the scene change for the effectively performing the encoding.\nFurther, the splitter module 502 can be configured to output the video segments to the encoder modules 506.  After receiving multiple segments from the splitter module 502, the encoder module 506 can encode according to a Windows Media Video or\nVC-1 format, MPEG-x format (e.g., MPEG-1, MPEG-2, or MPEG-4), H.26x format (e.g., H.261, H.262, H.263, or H.264), or other format.  The encoder module 206 can encode according to one or more audio standards such as WAV, FLAC, MP3, WMA, or some other\nstandard.  The encoder module 506 can also receive the number of bits to be allocated to each segment of the input video from the complexity analyzer module 504.\nThe complexity analyzer module 504 can be configured to receive the input video that is to be encoded from a source format to any other format.  The complexity analyzer module 504 can be configured to process the entire video to measure the\ncomplexity of the video at every frame of the video.  After determining the complexity at every frame of the video, the complexity analyzer module 504 can be configured to compute the bits-per-frame to be allocated to each segment.  Thus, a given segment\ncan be encoded at a bitrate independently from any other segment.  Unlike conventional systems, the complexity analyzer module 504 can be configured to send the bits per frame allocated to each segment to the respective encoder module 506, in order to\nachieve a uniform video quality across segments of the video.\nFurther, the encoder module 506 receives the video segments from the splitter module 502 and also receives the bitrate settings to be used in each segment for generating a uniform video quality.  Each segment is allocated with the bitrate\nsettings received from the splitter module 502 based on the complexity associated thereof.  The combiner module 508 receives the output video segments from the encoder modules 506 and combines the output video segments to generate the encoded video\noutput.\nFIG. 5 illustrates a limited overview of a distributed video encoding system 500 but it is to be understood that another embodiment is not limited thereto.  Further, the system 500 can include any number of video processing modules and\ntechniques along with other hardware or software components communicating with each other.  For example, the component can be, but not limited to, a process running in the controller or processor, an object, an executable process, a thread of execution,\na program, or a computer.  By way of illustration, both an application running on a device and the device itself can be a component.\nFIG. 6 illustrates another high level overview of a distributed video encoding system 600 for computing and allocating a variable number of bits per frame using a separate complexity analyzer for each segment of a video, according to embodiments\nas disclosed herein.  In an embodiment, the input video is split into plurality of segments including a set of frames.  Each segment of the video is passed through a separate complexity analyzer module 604, which produces an output file that describes\nthe video complexity for each frame of the video.  Once the complexity analyzer modules 604 determine the complexity of each segment, a bit-rate for each video segment is computed and is passed to the encoder module 506 to achieve a desired quality of\nvideo.  Further, detailed operations by the system 600 are described in conjunction with the FIG. 9.\nFIG. 6 illustrates a limited overview of the system 600 but it is to be understood that another embodiment is not limited thereto.  Further, the system 600 can include any number of video processing modules and techniques along with other\nhardware or software components communicating with each other.  For example, the component can be, but not limited to, a process running in the controller or processor, an object, an executable process, a thread of execution, a program, or a computer. \nBy way of illustration, both an application running on a device and the device itself can be a component.\nFIG. 7 illustrates another high level overview of a distributed video encoding system 400 for computing and allocating a variable number of bits per frame using a separate complexity analyzer for each segment of a video and adding a pathway that\npasses each segment complexity measurement to a corresponding encoder, according to embodiments as disclosed herein.  In an embodiment, the encoder module 206 can use the segment complexity information computed earlier to make effective local decisions\nwithin the segment.  Further, the various operations performed by the distributed video encoding system 700 are described in conjunction with the FIG. 10.\nFIG. 7 illustrates a limited overview of the system 700 but it is to be understood that another embodiment is not limited thereto.  Further, the system 700 can include any number of video processing modules and techniques along with other\nhardware or software components communicating with each other.  For example, the component can be, but not limited to, a process running in the controller or processor, an object, an executable process, a thread of execution, a program, or a computer. \nBy way of illustration, both an application running on a device and the device itself can be a component.\nFIG. 8 is a flow diagram illustrating a method 800 for computing and allocating a variable number of bits per frame (or simply \"bits\") in a distributed video encoding using a complexity analyzer, according to embodiments as disclosed herein.  At\nblock 802, the method 800 includes receiving an input video for allocating bits for each segment based on the complexity of the segment.  At block 804, the method 800 includes splitting the input video into a plurality of segments.  The method 800 allows\nthe splitter module 602 to split the input video into the plurality of segments including a set of frames.\nAt block 806, the method 800 includes determining the bits to be allocated to the plurality of segments based on the complexity of the input video.  The method 800 allows the complexity analyzer module 804 to determine the bits to be allocated\nto the plurality of segments based on the complexity measurement of each segment of the input video.  Here, the complexity analyzer module 804 measures the complexity of every frame in each segment and also complexity of the entire input video to measure\noverall complexity of the segment to determine the bits to be allocated to each segment of the video.  For example, consider an input video is split into 3 segments i.e., Segment-1, Segment-2, and Segment-3.  Unlike conventional systems, a complexity\nassociated with each segment (the Segment-1, Segment-2, and Segment-3) is analyzed and accordingly a bit budget for each segment is computed to generate a uniform video quality.\nAt block 808, the method 800 includes allocating the bits to plurality of segments of the input video for encoding the plurality of segments into a different format.  Based on the complexity of each segment, the method 800 allows the encoder\nmodule 506 to allocate the bits to the plurality of segments.  Here, the encoder module 506 receives the plurality of segments of the input video from the splitter module 202 and also receives the bits to be allocated to the plurality of segments.  The\nencoder module 506 accepts the information from the complexity analyzer module 604 and accordingly allocates the bits to each segment to encode the each segment into a different format.  For each segment the encoder will achieve an average bit rate for\nthe segment, and can allocate bits to the high and low complexity sections of the video.  For example, a high complexity region in a segment is allocated with additional bits in order to deliver good video quality.  Similarly, a low complexity region in\na segment is allocated with uniform bits so as to deliver the good video quality.\nIn an embodiment, a separate encoder can be used to receive each segment of the video for transferring the segment from one format to another format while achieving a desired video quality.  For example, consider an encoder-1, an encoder-2, and\nan encoder-3 receives an input video split into three segments namely, segment-1, segment-2, and segment-3 respectively.  The encoder-1 allocates the bits to the Segment-1 based on the complexity of the segment-1.  Similarly, the encoder-2 allocates the\nbits to the Segment-1 based on the complexity of the segment-2.  Similarly, the encoder-3 allocates the bits to the Segment-1 based on the complexity of the segment-3.  Unlike conventional systems, different encoder's uses variable bit rates encoding\neach segment of the video such as to generate a uniform quality of video.  Each bit rate for the segment is computed based on the complexity analysis of the segment.\nFurther, at block 810, the method 800 includes combining the plurality of segments to form a single output video.  The method 800 allows the combiner module 208 to combine the plurality of segments to form the single output video.  For example,\nthe segment-1 from the encoder-1, the segment-2 from the encoder-2, and the segment-3 from the encoder-3 are combined to form the single output video.  The combining may be performed by concatenating the segments in the same temporal order as in the\nreceived video.\nAt block 812, the method 800 includes obtaining a encoded output video from the combiner module 508.  Unlike conventional systems, distributed video encoding system obtains the encoded output video while maintaining a desired quality by using\nvariable bit rates for each segment instead of a globally optimized bit rate.  The complexity of the input video in the distributed video encoding is measured and the bit budget on a per-frame basis is determined for each segment of the input video.  The\ninformation about the per frame bit allocation is passed onto the encoder for allocating bits to each segment thereby obtaining good average video quality.\nFIG. 9 is a flow diagram illustrating a method 900 for allocating encoding bits in a distributed video encoding system using a separate complexity analyzer for each segment of a video, according to the embodiments as disclosed herein.  At block\n902, the method 900 includes receiving an input video for allocating bits for each segment based on the complexity of the segment.  At block 904, the method 900 includes splitting the input video into a plurality of segments.  The method 900 allows the\nsplitter module 502 to split the input video into the plurality of segments including a set of frames.\nAt block 906, the method 900 includes determining the complexity of the plurality of segments by using separate complexity analyzer for each segment of the input video.  The method 900 allows the complexity analyzer module 604 including the\nplurality of complexity analyzers to determine the complexity of the plurality of segments by using separate complexity analyzer for each segment.  Here, each complexity analyzer measures the complexity of every frame in each segment; thereby,\ndetermining the complexity of the segment.  For example, consider an input video is split into 3 segments i.e., Segment-1, Segment-2, and Segment-3.  Unlike conventional systems, a complexity associated with each segment (the Segment-1, Segment-2, and\nSegment-3) is analyzed and accordingly bits for each segment are computed to generate a uniform video quality.  Here, the complexity analyzer module includes plurality of complexity analyzers i.e. complexity analyzer-1, complexity analyzer-2, and\ncomplexity analyzer-3.  The complexity analyzer-1 determines the complexity of the segment-1 of the input video.  The complexity analyzer-2 determines the complexity of the segment-2 of the input video.  The complexity analyzer-3 determines the\ncomplexity of the segment-3 of the input video.\nAt block 908, the method 900 includes determining the number of bits for allocating to each segment of the input video.  The method 900 allows the complexity analyzer module 604 to determine the bits for allocating to each segment of the input\nvideo.  At 910, the method 900 includes encoding the plurality of segments and allocates the bits to each segment based on the complexity of the segment by comparing with the plurality of segments.  Based on the complexity of each segment, the method 900\nallows the encoder module 506 to allocate bits to each segment by comparing with the plurality of segments received from the splitter module 502.  Here, the encoder module 506 will use the segment information to make good local decisions within the\nsegment.  The encoder module 506 accepts the information from the complexity analyzer module 504 and accordingly allocates bits to each segment to encode the each segment into a different format.  For each segment the encoder will achieve an average bit\nrate for the segment, and can allocate bits to the high and low complexity sections of the video.\nIn some embodiments, a separate encoder can be used to receive each segment of the video from the complexity analyzer for transferring the segment from one format to another format while achieving a desired video quality.  For example, consider\nthe encoder module 506 consists an encoder-1, an encoder-2, an encoder-3 receives three segments namely, segment-1, segment-2, and segment-3 respectively from the complexity analyzers.  The encoder-1 allocates bits determined by the complexity analyzer\nmodule 604 to the segment-1 by taking the inputs from the segment-1.  The encoder-2 allocates bits determined by the complexity analyzer module 604 to the segment-2 by taking the inputs from the complexity analyzer-2 and the segment-2.  The encoder-3\nallocates bits determined by the complexity analyzer module 604 to the segment-3 by taking the inputs from the complexity analyzer-2 and the segment-3.  Unlike conventional systems, different encoder's uses variable bit rates encoding each segment of the\nvideo such as to generate a uniform quality of video.  Each bit rate for the segment is computed based on the complexity analysis of the segment.\nFurther, at 912, the method 900 includes combining the plurality of segments to form a single output video.  The method 900 allows combiner module 508 to combine the plurality of segments to form the single output video.  For example, the\nsegment-1 from the encoder-1, the segment-2 from the encoder-2, and the segment-3 from the encoder-3 are combined to form the single output.  At 912, the method 900 includes obtaining an encoded output video from the combiner module 208.  Unlike\nconventional systems, distributed video encoding system obtains the encoded output video while maintaining a desired quality by using variable bit rates for each segment instead of a globally optimized bit rate.  Each segment is sent to a separate\ncomplexity analyzer, which produces the output file that describes the video complexity for each frame.  As the separate complexity analyzer is placed for each segment, the performance of the system is improved and the bits can be allocated to the\nsegments more accurately and quickly.  Further, the complexity of the input video in the distributed video encoding is measured and bits per frame to be allocated are determined for each segment of the input video.  The information about the per-frame\nbit allocation is passed onto the encoder for allocating bits to each segment; thereby, obtaining good average video quality.\nFIG. 10 is a flow diagram illustrating a method 1000 for allocating bits in a distributed video encoding using a separate complexity analyzer for each segment of a video and adding a pathway that passes each segment complexity measurement to an\nencoder, according to the embodiments as disclosed herein.  At block 1002, the method 1000 includes receiving an input video for allocating bits for each segment based on the complexity of the segment.  At block 1004, the method 1000 includes splitting\nthe input video into the plurality of segments.  The method 1000 allows the splitter module 202 to split the input video into the plurality of segments including a set of frames.\nAt block 1006, the method 1000 includes determining the complexity of the plurality of segments by using separate complexity analyzer for each segment of the input video.  The method 1000 allows the complexity analyzer module 604 including the\nplurality of complexity analyzers to determine the complexity of the plurality of segments by using separate complexity analyzer for each segment.  Here, each complexity analyzer measures the complexity of every frame in each segment, and thereby\ndetermining the complexity of the segment.  For example, the input video is split into 3 segments i.e. Segment-1, Segment-2, and Segment-3.  In another example, the input video can be split into N segments (i.e. Segment-1 to Segment-N).  Unlike\nconventional systems, a complexity analyzer module includes the plurality of complexity associated with each segment (the segment-1, segment-2, and segment-3) is analyzed and accordingly a per frame bit allocation for each segment is computed to generate\na uniform video quality.  Here, the complexity analyzer module includes plurality of complexity analyzers i.e. complexity analyzer-1, complexity analyzer-2, and complexity analyzer-3.  The complexity analyzer-1 determines the complexity of the segment-1\nof the input video.  The complexity analyzer-2 determines the complexity of the segment-2 of the input video.  The Complexity analyzer-3 determines the complexity of the segment-3 of the input video.\nAt block 1008, the method 1000 includes determining the per frame bit budget for allocating to each segment of the input video.  The method 1000 allows the complexity analyzer module 604 to determine bits for allocating to each segment of the\ninput video.  At block 1010, the method 1000 includes encoding the plurality of segments and allocates bits to each segment based on the complexity of the segment.  Based on the complexity of each segment, the method 500 allows the encoder module 506 to\nencode the plurality of segments and allocates bits to each segment.  Here, the encoder module 506 will use the segment information and also segment complexity information computed earlier to make good local decisions within the segment.  The encoder\nmodule 506 accepts the information from the complexity analyzer module 604 and accordingly allocates bits to each segment to encode the each segment into a different format.  For each segment the encoder will achieve an average bit rate for the segment,\nand can allocate bits to the high and low complexity sections of the video.\nIn an embodiment, a separate encoder can be used to receive each segment of the video from the complexity analyzer for transferring the segment from one format to another format while achieving a desired video quality.  For example, consider the\nencoder module 506 includes an encoder-1, encoder-2, and encoder-3 receives three segments namely, segment-1, segment-2, and segment-3 respectively from the complexity analyzers.  The encoder-1 allocates the bits per frame budget determined by the\ncomplexity analyzer module 604 to the segment-1 by taking the inputs from the complexity analyzer-1 (i.e. complexity information of the segment-1) and segment-1.  The encoder-2 allocates the bits per frame budget determined by the complexity analyzer\nmodule 604 to the segment-2 by taking the inputs from the complexity analyzer-2 (i.e. complexity information of the segment-2) and segment-2.  The encoder-3 allocates the bits per frame budget determined by the complexity analyzer module 604 to the\nsegment-3 by taking the inputs from the complexity analyzer-3 (i.e., complexity information of the segment-3) and segment-3.  Unlike conventional systems, different encoder's uses variable bit rates encoding each segment of the video such as to generate\na uniform quality of video.  Each bit rate for the segment is computed based on the complexity analysis of the segment.\nFurther, at block 1012, the method 1000 includes combining the plurality of segments to form a single output video.  The method 1000 allows combiner module 508 to combine the plurality of segments to form the single output video.  For example,\nthe segment-1 from the encoder-1, the segment-2 from the encoder-2, and the segment-3 from the encoder-3 are combined to form the single output.  At block 1014, the method 1000 includes obtaining a encoded output video from the combiner module 208. \nUnlike conventional systems or other embodiments of the present system, each encoder uses the segment complexity information computed earlier to make good local decisions within the segment and works faster, more accurately for allocating bits to the\nsegments of the video; thereby, obtaining best overall video quality.  The encoder assigns bit rates across all the segments (which may or may not be the same for each segment) of the video to obtain uniform quality.\nAs discussed with respect to FIG. 11, video is often uploaded to a video sharing site like YouTube, Vimeo, or Facebook by a user with a network-connected user device such as a wireless phone or a desktop computer.  The user may be an amateur\nuser, wishing to share video with friends and family, or may be a professional videographer or a studio or a news-gathering and distribution service.\nUser devices are typically equipped with video encoders to reduce the size of captured video.  The encoding may be performed by running a process on a central processor (CPU) with or without assistance from specialized hardware circuitry.\nVideo quality, video bitrate, and CPU encoding time are all related.  For a given video encoder, typically for a constant video quality, a lower video bitrate can be achieved by increasing the CPU encoding time, e.g., by using higher complexity\nencoding settings.  If the CPU encoding time budget per frame is held constant, then increasing video quality requires a higher video bitrate.\nSome video encoders, generally called software encoders, e.g., video encoder programs that run on a CPU, have settings that adjust how much time the CPU spends on each frame.\nSome video encoders, generally called hardware encoders, also have settings that control how fast the video encoding runs and how well the compression occurs.  Examples of hardware encoders include GPU based encoders or encoders that use an\napplication-specific integrated circuit (ASIC), e.g., in a mobile phone or a camera.\nVideo cameras (phones, DSLRs, video cameras, GoPro.TM.) normally capture video at a very high quality and high resolution.  In order to maximize quality and reduce the processing load on the consumer device, the devices usually perform only a\nlight encode, which means that the high video quality and resolution requires a very high bitrate.  Such devices therefore generate a lot of bits for captured video.\nVideo sharing sites (e.g., YouTube or Vimeo or Facebook, etc.) often limit their bitrate on videos that are displayed.  In order to limit this bitrate, these sites lower the video resolution and lower the quality so that the video fits within\nthe bitrate.  This video quality is normally much lower than what was originally captured by the camera.\nMany video sharing sites will lower the video quality before displaying the video to users.  It may thus be wasteful to send the high quality video up to the video sharing site because higher quality videos typically require more bandwidth and\nmay require a longer upload time.  In one advantageous aspect, the disclosed embodiments can encode videos at the client side to match the video quality that the video sharing site will use.\nSince video quality, video bitrate, and CPU cycles are often related and controlled by the encoding settings, the encoding at the user device can be controlled to match available upload resources, e.g., bandwidth, CPU power and battery life. \nEvery client device will have a different amount of upload bandwidth available (e.g., depending on the network, such as Wi-Fi, cell, fixed) and will have a different amount of CPU power available (high end servers, low end desktops, laptops, tablets or\nphones).\nFIG. 12 illustrates an example graph 1201 of trade-offs possible in encoding digital video for a given output bitrate of an encoder.  The curve 1202 may represent the operation of a software-based video encoding algorithm, e.g., implemented\nmainly using a CPU of a user device, with or without hardware assistance for various processing such as transform computation and run-length or arithmetic encoding.  The curve 1204 may represent the operation of a graphics-processor (GPU) based video\nencoding algorithm, e.g., implemented mainly using the parallelized pipeline of encoding, as is found in prevalent GPUs, The curves 1202, 1204 are plotted with the horizontal axis 1206 representing a measure of complexity of encoding, e.g., based on the\namount of time taken for per-frame encoding and the vertical axis 1208 representing the resulting video bitrate of the resulting video.\nAs can be seen from curves 1202, 1204, in general for a given encoding implementation, e.g., GPU-based or CPU-based, the more time an encoder spends on the encoding, the better the quality of the resulting encoded video tends to be.  As the\namount of time spent per frame increases, however, benefit of encoded video size reduction may taper off for GPU based encoding.\nIn some disclosed embodiments, the target output video quality is set to a fixed level, and then encoding settings are continuously adjusted such that upload bandwidth is maximally consumed and available CPU or GPU capacity is maximally\nconsumed.  This ensures that the upload occurs as quickly as possible.  On a device with low-powered CPU or GPU and a high bandwidth connection, an embodiment may only lightly encode the uploaded video because if video is encoded more diligently, then it\nwould take too long for the CPU or GPU to encode the video and the upload bandwidth would not be fully used, and the upload would take longer.\nOn a powerful device with a low bandwidth Internet connection, an embodiment may apply strong encoding to use as much CPU power as possible and compress the video as much as possible to fit through the limited upload bandwidth.\nIn both these cases, the video quality may be set to a constant value to match the optimal or desired settings of the video sharing site.\nIn FIG. 12, graph 1250 illustrates the difference between video bitrate, shown by curve 1254 and the encoding bitrate, represented by curve 1252.  In the graph 1250, the horizontal axis 1256 represents the encoding effort or processor resources\nspent and the vertical axis 1258 shows the resulting bitrate.  Operational point 1260 represents a case where video encoding may be performed, e.g., at ten times the real time rate (e.g., 300 frames encoded in one second for a 30 frames per second\nplayback rate) as shown by the fact that the encoding bitrate curve 1252 is much higher at operational point 1260 than the video bitrate curve 1254.  The video bitrate of an encoder may represent the number of encoding bits per second used for encoding\nthe video.  For example, when the video bitrate of an encoded video is 1 Mbps, then approximately 1 million bits from the encoded video may be used by a decoder every second during decoding and playback of the video.  By contrast, if an encoder is\noperating at 10.times.  speed, then the encoder generates the 1 Mbps video at an encoder rate of 10 Mbps.  During operation of a video encoder, the encoder bitrate and the video bitrate may be related by the equation: encoder bitrate=video\nbitrate.times.encoding speed.  Thus, when the encoder is operating at 10.times.  real-time speed, then the encoder bitrate is ten times more than the video bitrate.  The encoder bitrate may non-linearly change as a function of the CPU time setting during\noperation, e.g., based on how much time is spent in CPU processing for encoding each frame.  The intersection between the two curves 1252, 1254 may represent the operational point when video encoding is being performed in real time such that the encoding\nrate and the video bitrate are equal.  The graph 1250 illustrates a difference between video bitrate, e.g., the size (in bits) of a video, divided by the playback duration of the video, and encoder bitrate, which is, e.g., the rate at which an encoder is\noutputting encoded bitstream.\nExample Single Stream Mode Embodiments:\nIn Single Stream mode, the input video is run through a single video encoder.  The output of the video encoder is buffered, and this buffer is uploaded to the Internet (uploader process).  A rate control module monitors the size of the outbound\nbuffer--how much has been encoded but not uploaded yet--and uses this size to control the strength of the encoding.  The system may set a high mark and a low mark for this buffer.  When the buffer hits the high mark, it means that the buffer is filling\nup: this means that the encoder is running faster than the upload process because there is not enough upload bandwidth, so the encoding settings are increased to tell the encoder to spend more time on each frame to make the frame smaller.  When the low\nmark of the buffer is hit, this means that the encoder is running slower than the upload process--there is extra upload bandwidth available.  In this case, the encoding settings are lowered to tell the encoder to spend less time on each frame.\nExample Multi-Stream Mode Embodiments:\nIn this mode, the input video is segmented and run through multiple video encoders.  The output of each video encoder is connected to an uploader process that pushes the content up to the video sharing site.  Using multiple streams has two main\nbenefits: 1.  by using multiple parallel connections, higher upload throughput can often be achieved, and 2.  some hardware and software encoders have higher throughput when multiple streams run through them.  The algorithm for adjusting the encoding\nsettings is similar to the above approach: the buffer size is calculated using the sum of all of the output buffers, and a high and low mark is established in the same way as above.\nFIG. 13 is an example timing diagram 1301 of a video encoding process during uploading of video from a user device (e.g., 1106) to a server 1110.  As depicted along the time line 1308, a first number of video segments that make up the entire\nvideo to be uploaded may be encoded using a default encoding setting (e.g., time per frame).  In the depicted example, two video segments 1302 and 1304 are initially encoded and uploaded to the server 1110 using a default encoding setting, which may\nresult in, e.g., 1 Mbit/s video bitrate.\nAfter the first number of segments are encoded and transferred at the default time per frame setting, a rate control module, which may be implemented at the server 1110 and/or on the user device 1106, makes a determination about the encoding\nsetting (e.g., time per frame) that is to be used for encoding the next video segment.  For example, with the default encoding settings, the video bitrate may be 1 Mbps.  With these encoding settings, however, the encoder may be running at 10.times. \nreal-time (e.g., at operational point 1260), such that the encoder is generating data at an output rate of 10 Mbps.  After the first two segments, which may be 6 seconds long each, are encoded, the rate control module may observe that the encoder data\nrate was 10 Mbps and the uploaded data rate was 5 Mbps.  The outbound video buffer on the user device in which the encoded video is stored thus may be increasing in occupancy during the encoding and uploading of the first two segments due to the mismatch\nbetween the encoder rate and the available upload bandwidth.\nThe rate control module may then perform an encoding rate correction.  Noting that buffer occupancy is increasing and the actual encoding is occurring faster than target encoding rate, the rate control module might instruct the encoding process\nto spend more CPU or GPU time on each frame and therefore reduce the rate at which encoded frames are generated.  The change in encoding rate may be reflected in the subsequent video segments being uploaded, e.g., segment 1306.\nThe rate control module may similarly control monitoring of the outbound video buffer to perform periodic correction to the encoded video bitrate by controlling various encoding parameters, e.g., the motion search window, the target bitrate, the\nnumber of encoding options considered for encoding each macroblock of video, etc. The use, or lack of use, of these encoding parameters may cause the encoding speed to go up or down.  For example, when using a CPU-based encoder, using a larger motion\nsearch window may slow down encoding process because more computations have to be performed per macroblock, where a macroblock represents a pre-defined pixel array, e.g., 16.times.16 pixels or video frames.\nBy monitoring operational parameters of the uploading process, the rate control module can thus control the rate at which the encoder generates the encoded video bitstream such that the outbound buffer occupancy stays within an upper or a lower\nmargin.  Further, by ensuring that the outbound buffer is never empty, the rate control module may thus facilitate the uploading of video at maximum possible rate at all times.\nThe above-outlined uploading process may be modified in various embodiments using features such as user control or service provider control of the quality of uploading, the use of different operational parameters, e.g., battery power left in a\nbattery-operated device, the tariff a user may incur for bytes being uploaded, and so on.\nFIG. 14 is flow diagram illustrating a method 1400 for uploading encoded or compressed digital video bitstream from a user device to a network server.\nThe method 1400 includes, at block 1402, determining an operating setting for uploading a video.  The determination of operating settings may be made at a user device based on operating systems received from a server in the network.  For\nexample, a service provider operating the server to which the video is being uploaded may provide a minimum and/or a maximum acceptable quality for the video being uploaded.  Alternatively or in addition, the operating settings may be determined by\npresenting, on a user interface, a menu of operating setting selection and by receiving a selection of the operating setting from the user interface.  In some embodiments, the menu of operating setting selection includes an encoded video quality\nselection menu.\nIn some embodiments, the user device may derive the default encoding setting from the operating setting.  The encoding settings may include, e.g., an output video bitrate for encoding the video, number of B or P frames (Bidirectional or\nPredictive frames), and so on.  In some embodiments, the default encoding settings may be selected as the encoding settings that were used when the last segment of a previously uploaded video was encoded.  In some embodiments, the default encoding\nsettings may be based on the current network connectivity of the user device.  For example, a higher bitrate, e.g., 5 Mbps, may be used for encoding when the user device is connected to broadband internet and a lower bitrate, e.g., 1 Mbps, may be used\nwhen the device is connected with the server using a wide area network connection such as a 3G or a 4G connection.\nThe method 1400 includes, at block 1404, transmitting a first set of segments using a default encode setting.  For example, as described in this document, in some embodiments, the first set of segments may include first two GOPs of a video.\nAs described in this document, after the first few segments are transmitted by the user device and received by the server, the server may be able to determine whether the user device is encoding video at a faster-than-transmission rate or a\nslower-than-transmission rate.  For example, the knowledge of the occupancy of outbound buffer may be taken into consideration.  The rate control module described herein may thus determine revised encoding settings.\nThe method 1400 includes, at block 1406, receiving a revised encoder setting that is based on an optimal uploading setting.  For example, the revised encoder setting may be lower limited by a minimum time spent per frame and upper limited by a\nmaximum time spent per frame.\nThe method 1400 includes, at block 1408, encoding video using the revised encoder setting.  The encoding may use CPU and/or GPU for encoding, as controlled by the encoding setting.  In some embodiments, to continually ensure that the encoded\nvideo bytes are available for transfer over the network interface, the CPU (or GPU) of the user device may be used to its maximum possible availability during the time video encoding and uploading is being performed.  As described herein, other optimal\nsetting criteria, e.g., maximum battery saving etc. may be used for encoding also.\nExample Uploading Quality Setting\nIn some embodiments, the network server to which the client device wants to upload the video may specify quality of video that is acceptable for upload.  The network server may specify an upper limit on the quality of video, e.g., based on a\nservice layer agreement (SLA).  Additionally or alternatively, the network server may specify an acceptable lower threshold for quality below which video will not be accepted by the server.  The quality setting may take place every time a user wants to\nupload a video.  Alternatively or additionally, the network server may specify quality setting on a periodic basis, depending on the user's use of her storage space at the network server.  Alternatively or additionally, the network server may specify a\ndifferent quality setting every time there is a change in the network over which the user device is accessing the network server.  For example, the network server may specify the use of one quality setting when the user device is operating with a\nbroadband Internet (Wi-Fi) connection and another quality setting when the user device is operating in a 3G or 4G network.\nAlternatively or additionally, a user may be able to specify or request a quality setting prior to uploading a video.  For example, a user interface may be presented to the user at the beginning of the uploading, requesting which one of a number\nof quality settings (e.g., low, medium or high) does the user want to use for the uploading of the video.\nExample Optimal Criterion for Uploading\nUsers (and service providers) may want to use different optimal criteria to control the uploading process.  The criteria may include, e.g., upload time, total number of bits uploaded, upload quality, and so on.\nIn some embodiments, the above techniques may be implemented in a user device that includes at least one processor, at least one memory comprising instructions configured to cause the at least one processor to perform the method 1400 described\nherein.\nDuring the encoding, an encoded bitstream may have to be generated to ensure that a key frame occurs every given number of seconds.  For example, in some implementations that use hypertext transfer protocol based live streaming (HLS), a key\nframe may be expected by a video player at some frequency, for example, at least once every 10 seconds.  However, the original video available from the user device may not follow this periodicity of key frames.  For example, video captured from a user\ndevice may use a 6-second GOP, while the video sharing service provider may want to store and distribute video sequences at a different GOP length, e.g. 10 seconds apart.\nFIG. 15 depicts an example workflow in a distributed video encoding system 1500.  The system 1590 may be operated by a video uploading and sharing service provider.  Distributed video encoding involves splitting the input videos into segments\nand passing the segments to a cluster of worker nodes.  The worker nodes independently encode the videos and ship the output segments back to the master node.  The master node then stitches all the segments together to create a final output.\nAt block 1501, an input video sequence is received by a master node from a user device 1106, 1116.  The input video sequence may be a compressed video sequence or an uncompressed video sequence.  At block 1502, the master node may split the\nvideo into multiple segments.  The master node may then assign segments to different worker nodes for performing video encoding or encoding.  At 1503, each worker node encodes the segment received.  At 1504, each worker node sends back the resulting\nencoded bitstream to the master node.  At 1505, the master node combined the segments back to generated encoded bitstream for the video.  A similar workflow of splitting, encoding, and combining is also described with respect to FIG. 3, FIG. 5 and FIG.\n6.\nThis technique may be sufficient when the video has intermittent IDR (Instantaneous Decoder Refresh) frames, which are key frames (Intra encoded, or I, frames is another example of key frames), since the splitter can look for IDR frames to\nsegment the input video file.  This is called a \"closed GOP\" type bitstream, where the frames inside a GOP refer to other frames in the same GOP.  Since an IDR is an anchor frame any future frames cannot refer to a frame earlier than the anchor frame. \nHence, the sanctity of each video segment is preserved.  Thereby the encoder (worker) nodes can independently process each chunk.\nFIG. 16 illustrates an example of a video 1600 when the input video is a closed GOP sequence.\nAs depicted in FIG. 16, all the references, indicated by connectors 1602, 1604, are within the GOP structure.  So in this case, the video segmenter creates segments or chunks at the end of frame P4 and creates segments as I1B2B3P4 and I5B6B7P8\nand so on.  Each of these segments is independently processed by the worker nodes because there is no cross GOP referencing.\nFIG. 17 depicts an example of an open GOP video 1700.  The illustrated example shows that the frame P4 is predicted from the frame I1, the frame B2 is bi-directionally predicted using references from I1 and I5 and the frame B3 is\nbi-directionally predicted from the reference frames I1 and I5.\nWhen encoding an open GOP video, the above described chunking strategy does not work where there are references outside of GOP boundaries.  In this case, just segmenting at an I-frame boundary would cause the resulting segments to fail during\nencoding, because the dependencies are not met, resulting in bad video quality.\nAs seen from FIG. 17, if the segmenter cuts at I-frame boundaries, the worker nodes would fail encoding because B2 has dependency on I5 (depicted by connector 1702), which falls in a different segment than B2.\nIn one aspect, an intelligent segmenter described herein preserves segment level sanctity of open GOP video frames.  In case of an open GOP video, the segmenter includes a few extra frames at the GOP boundary to account for cross GOP\nreferencing.  If a previous IDR frame, I frame or a key frame, exists then the segmenter can choose to include all the previous GOPs until that key frame.  Alternatively, the segmenter could just include the maximum reference length of frames for that\nvideo codec.  Every codec has a maximum length beyond which it generally cannot refer a frame for motion estimation/compensation.  This leads to multiple segments with over lapping frames.  In order for these segments to be encoded on the worker nodes,\nadditional header information is required, which is copied over from the previous key frame.\nIn order to not duplicate the overlapping frames while encoding, metadata can be sent along with the file name to the worker nodes.  The metadata contains the number of initial frames to be skipped before the encoder starts encoding.  At the end\nof encoding the worker node submits the encoded segments back to master, which then merges them to create a final output.\nIn some embodiments, each encoder in a distributed encoding system may be provided with contiguous video frames in excess of the frames that make up a video segment that the encoder is asked to encode.  For example, a first encoder may be given\nframes 1 to 12 of a video, but will be asked to encode frames 1 to 10.  This way, regardless of the GOP dependency, the worker node is assured that all possible video frames from which motion vectors are derived in the input video are available to the\nworked node for encoding.  At the same time, because a sufficient number of video frames are available and are of duration longer that the desired key-frame insertion period (e.g., 10 frames), the worker node can perform encoding to precisely insert a\nkey frame where desired.\nFIG. 18 illustrates how worker nodes can perform encoding.  With reference again to the video 1700 which may be input to the encoding process, the operation of segmentation based encoded may be performed as follows.  In some embodiments, prior\nto the segmentation operation, the master node may generate a dependency map.  The dependency map may identify, for each frame in a segment, other frames on which decoding of that frame depends, e.g., the reference frame(s) used in encoding that frame. \nBased on the dependency map, the intelligent segmenter may include in each segment frames of two types--frames that are to be encoded by worker nodes, and additional frames that are included in the segment because the frames to be encoded depend on the\nadditional frames.  For example, in 1800, a first worker node may start encoding at I1 (first frame of received segment 1), and stop encoding at the rightmost P4 frame (last frame of received segment 1), as indicated by the metadata.  However, because\nframes B2 and B3 are bi-directionally predicted from the frame 15, the intelligent segmenter includes the frame I5 in Segment 1.  In other words, the intelligent segmenter may include, in some segments, additional video frames that are not to be encoded\nin the output video, but are used only for encoding other video frames in the segment.\nSegment 2, which is the next segment temporally after Segment 1, represented in 1802, represents the case when all frames in the segment are sent to a worker node and are also to be encoded by the worker node, e.g., as may be the case for a\nclosed GOP segment.  Based on the dependency map, the intelligent segmenter may thus send some frames, e.g., frame I5 in multiple segments, e.g., both in Segment 1 (but not encoded by the worker node that receives Segment 1) and in Segment 2, where it\nwill be encoded by the worker node that receives Segment 2.\nFIG. 19 illustrates an example method 1900 of encoding a compressed video bitstream.  The method 1900 may be implemented on the network-side of a social media network or a video sharing service provider's network.\nThe method 1900 includes, at block 1902, receiving a video.  The video may be a compressed video bitstream.  In the received video, key frames may occur nominally a first time duration apart, wherein a key frame is encoded without depending on\nanother frame in the compressed video bitstream.  The time duration may be specified in seconds, frame numbers, and so on.  For example, in some embodiments, the time duration may be 10 seconds to 20 seconds and may correspond to an intended streaming\nprotocol used when allowing other users to access the video bitstream.\nThe method 1900 includes, at block 1904, building a dependency map that identifies the reference frames used for each encoded video frame.  Some examples of the dependency map are illustrated in FIG. 17 and FIG. 18.  At block 1906, the received\nvideo is segmented using the dependency map.  For example, as discussed with respect to FIG. 17 and FIG. 18, each segment may include frames that are encoded using only other reference frames within the same segment.  The method 1900 includes, at block\n1908, providing, or distributing, segments of the compressed video bitstream representing corresponding to a second nominal time duration to multiple encoding nodes, wherein each segment includes at least one key frame, and wherein at least some segments\ninclude video frames that occur temporally before and after the at least one key frame in a display order.\nThe method 1900 includes, at block 1910, receiving back results of encoding performed by each encoding nodes.  The received encoded segment include the corresponding segment of compressed video bitstream such that key frames in resulting encoded\nvideo bitstream occur a second time duration apart.  The method 1900 may include, at block 1912, combining, or concatenating, the received encoded segments into a single bitstream representing the encoded representation of the received video.\nIn some embodiments, the encoding includes encoding by changing temporal location of a key frame in resulting encoded video bitstream.\nIn some embodiments, the method 1900 includes determining, for each frame, in the display order, most distant past and future frames from which motion vectors for each frame are formed.  In some embodiments, the determining is performed using\nmotion vector information from the compressed video bitstream.  In some embodiments, the determining is performed without uncompressing the compressed video bitstream.\nIn some embodiments, the encoding by each node is performed on fewer than all frames making up the corresponding segment.  For example, as discussed herein, some frames before and/or some frames after key frames may be used for the sake of\ncontinuity of motion vectors only and may be dropped from the encoding process.\nIn some embodiments, the method 1900 further includes providing, to each of the multiple encoding nodes, information regarding location of a key frame in the encoding operation.  In some embodiments, the method further includes encoding, by one\nof the multiple encoding nodes, the at least one key frame in received video segment to become a non-key frame in resulting encoded video bitstream.\nVideo encoders normally have many settings that adjust specific attributes of the encoding process.  The effect of each of these settings can depend on the input video--some of the settings might make an encode worse for a certain input video,\ne.g., high motion, but will make a different input video, e.g., low motion better.\nFIG. 20 depicts a graph 2000 to highlight this observed behavior of the video encoders at a given bitrate.  The horizontal axis 2002 represents an encoding parameter setting, e.g., the amount of time spent by the encoder to encode each frame,\nand the vertical axis 2004 represents quality of output compressed video for multiple video encoders (multiple curves 2006, representing multiple encoder parameter settings 1, 2, .  . . Z, etc., where Z is an integer).  The actual variations may depend\nfrom feature to feature, but generally, video encoders show varying quality outputs based on how the encoder setting.\nIt is often difficult to select a priori the best settings and, in practice, encoder settings are often selected based on operator-experience, or are pre-selected without attempting to customize video encoding settings to individual video\ncontent.\nSome examples of video encoding settings can include: frame distance between key frames, how many predictive or bi-directional frames intervening key frames to use, threshold used for detecting scene changes by comparing two successive video\nframes, whether or not to perform intra-frame motion prediction, whether or not to use different quantization matrices, which of the multiple coding options to use for coding bits (e.g., variable length encoding or arithmetic encoding), whether or not to\nuse fading detection, the motion search window to be used for each video frame, and so on.  Some of these settings may have discrete (e.g., on/off or high/medium/low) values, while others may have continuous values.  This is just an example list and one\nof skill in the art would appreciate that there are hundreds of design trade-off options that can be made in selecting a particular mode in which to run a video encoder, with each design option being a good choice for one type of video but not a good\nchoice, in terms of compression efficiency, for another type of video.\nIn one example aspect, the realization that different settings are optimal for different types of video, and that a single video might have multiple different segments of video for which different settings are optimal, is used to achieve video\ncompression ratios higher than that can be achieved by a given single video encoding setting on a multi-segment video.  The present document discloses, among other techniques, a mechanism to find a relatively optimal setting (e.g. from a set of thousands\nof settings) for each different segment, and then assemble all of the encoded results into a single output video.\nIn some embodiments, a set of test video clips, or segments, may be encoded through an automated encoder engine.  The encoder engine may encode each test clips with hundreds or thousands of different settings to understand the effect of each\nvideo setting on each test clip.  The results of these encodings are used to train a training module that can determine optical encoding configurations for future video encodings.\nIn some embodiments, the training phase may be implemented using a neural network that uses a learning algorithm in which a cost criterion such as rate-distortion or visual rating score is used for training the encoding parameter learning.\nFIG. 21 illustrates an example arrangement of video encoders 2100 to produce video encoding or encoding with very high compression ratios, while maintaining quality.  In some embodiments, a library of test videos 2102 may be used as input to an\nautomated tester module 2104 which cycles through all test sequences using all possible (could be in thousands) combinations of video encoding parameters to generate a database 2106 of results.  This database 2108 may be used to train a machine learning\nalgorithm such as a neural network settings generator 2116.\nThe arrangement 2100 may receive an input video 2108 for encoding or encoding, e.g., from a user upload.\nIn some embodiments, an intelligent segmenter 2110 may divide the input video into different segments 2114.  This intelligent segmenter may learn an optimal place to segment the video and place output key frames to maximize quality by detecting\nscene change.\nIn some embodiments, a fingerprint generator 2112 may measure various video characteristics of each segment.  The fingerprint generator 2112 may thus identify a \"fingerprint\" of a video, e.g., the qualities of the video that make it more or less\namenable to certain configuration of encoding parameters.  The fingerprint generator may feed these characteristics to a neural network settings generator 2116.\nIn some embodiments, the neural network settings generator 2116 may be provided with input video segment characteristics and may determine a set of video encoding settings that should include a setting for a relatively optimal encoding, e.g.,\nthe smallest output file size for a given quality level.  The number of settings returned for each segment may be a configurable parameter: additional settings increases the chance of finding the optimal setting and improves compression efficiency, while\nfewer settings results in a faster overall encode.\nIn various embodiments, various machine learning techniques may be used to train the encoding process for how to pick the best video settings for each segment of a video to be encoded.  For example, the complexity analyzer 504, 604, could be\nused to identify and report complexity of a segment as an input to the fingerprinting process.\nIn some embodiments in which multiple encoders 2118 are used, each video segment is encoded multiple times with the settings returned from the Neural Network Generator 2116.\nAfter the video segments have been encoded, a segment selector 2120 analyzes each segment and picks the smallest sized segment 2112 that meets a predetermined quality requirement (e.g., a threshold quality).\nThe video segments are joined together into a single output video 2124.\nAs a result, each segment of the output video has been encoded with different settings that are relatively optimal for that segment, which means that as a whole, the output video meets the predetermined quality and has the relatively smallest\nfile size.\nFIG. 22 illustrates an example flowchart for a method 2200 of encoding a video.\nThe method 2200 includes, at block 2202, dividing a video into multiple segments for encoding.\nThe method 2200 includes, at block 2204, deriving for each segment, a corresponding encoding fingerprint.  In some embodiments, the deriving operation includes analyzing a ratio of bit utilization by different frame types in the videos, wherein\nthe frame types include an intra-encoded frame type and an inter-encoded frame type.  In some embodiments, the deriving operation includes analyzing a ratio of bits used in the videos for encoding motion based blocks and bits used in the video sequence\nfor encoding non-motion based blocks.\nThe method 2200 includes, at block 2206, determining, based on the encoding fingerprint, an encoder setting for each segment.  In some embodiments, the determining the encoder setting includes searching a database of encoder settings for a best\nmatch with the encoding fingerprinting of each segment.  In some embodiments, the database of encoder settings is produced by encoding multiple test videos using multiple encoder settings and collecting quality measurements of resulting videos.  In some\nembodiments, more than one encoder setting may be determined for each segment.  Alternatively, or additionally, the parameter values for a given encoder setting may also be determined.  For example, one encoder setting may relate to using a rectangular\nsearch window while another encoder setting may relate to the sequence in which motion vector search is performed within the search window, and within the encoder setting, a parameter may specify the size of the search window.\nThe method 2200 includes, at block 2208, encoding each segment using the determined encoder setting or settings.  In some embodiments, the encoding of each segment is performed using distributed resources in which at least some of the encoding\nis performed by different encoders.  The method 2200 includes, at block 2209, selecting from among the different encoded bitstreams generated by using different video encoder settings, a bitstream that meets a certain criterion.  The criterion may be,\ne.g., a bitstream having a smallest size or a least number of key frames.\nThe method 2200 includes, at block 2210, generating an encoded version of the video by concatenating the selected encoded segments.  The generation of an encoded version of the video may thus be accomplished such that each segment of the encoded\nversion is selected to be a best choice according to a specified criterion, from among may other segments encoded using a variety of different ways of encoding the corresponding video portion.\nIn some embodiments, the method 2200 may further include encoding by assigning, to multiple worker nodes, segments of the video for encoding along with encoding settings to be used; and receiving, from the multiple worker nodes, a resulting\nbitstream for each segment; and collating together the received resulting bitstreams to produce the encoded version of the video.\nAs described in FIG. 21, e.g., in some embodiments all segments may be encoded concurrently, or at the same time, by submitting the segments to multiple encoders for encoding.\nIt will be appreciated that the disclosed techniques can be used to produce video that is segment-wise encoded using the best possible encoder setting for each segment, without having to incur time penalty by using distributed encoding\nresources.  Furthermore, by using machine learning, e.g., generator 2116, the encoding parameter selection algorithm can be continuously improved.\nIn various embodiments, techniques for improving image stabilization using distributed processing are disclosed.  In a distributed video encoding system, a video is encoded by splitting the video into video segments and encoding the segments\nusing multiple encoders.  Prior to segmenting the video for distributed video encoding, image stabilization is performed on the video.  For each frame in the video, a corresponding transform operation is first computed based on an estimated camera\nmovement.  Next, the video is segmented into multiple video segments and their corresponding per-frame transform information.  The video segments are then distributed to multiple processing nodes that perform the image stabilization of the corresponding\nvideo segment by applying the corresponding transform.  The results from all the stabilized video segments are then stitched back together for further video encoding operation.\nWith the mass proliferation of cell phones with camera, there is growth in videos being taken and shared.  Non-professionally captured videos and videos captured using hand-held devices such as cell phones and tablet computers, often suffer from\nlot of shakiness due to instability of human hand and external factors like wind, etc. The resulting video may be shaky, e.g., stationary objects may change position in successive video frames not from movement in the object but due to movement in the\ncapture device.  The resulting video is not just unpleasant to watch but also may suffer from poor video compression quality because bits may be expended in capturing object movement where, in fact, there was not movement of the object.  It may be\nbeneficial to process the video using an image stabilization stage in which instability due to camera movement is detected and suppressed or eliminated.\nIn some embodiments, a two-step image stabilization process may be used.  In the first pass, video frames may be analyzed to detect shakiness and a corrective transform or transforms may be computed to substantially mitigate the effects of\nunstable camera.  In the second pass, the transforms are applied to the source video frames to produce stabilized video.\nThe existing methods perform these passes linearly on a single machine, which is extremely slow.  We disclose a method to accomplish image stabilization in a distributed encoding environment.\nFIG. 23 depicts an example of a video frame sequence 2300 in which camera shakiness has occurred.  The sequence 2300 illustrates 6 consecutive frames of a video captured by a camera when the camera was shaking.  With respect to the first\ncaptured frame F1, an upward shake is observed at frames F2, F3 and F4.  Frames F4, F5 and F6 represent a downward shake of the camera.  The dashed line 2302 represents the approximate path, or up-down movement, of the camera.  While not depicted\nexplicitly in the drawings, a determination can also be made about camera rotation, whether camera rotated clock-wise or anti-clockwise can also be made.\nIn some embodiments, image stabilization may be performed as follows:\nPass 1: In this pass, camera motion is detected by figuring out displacements in the positions of some objects that occur in two or more temporally sequential frames.  An object from a frame is matched with same object in the next frame(s) to\ndetermine Frame Motion, Mf.  The estimated frame motion Mf is then utilized to determine overlapping areas between adjacent frames.  Each frame is then cropped and zoomed to ensure that the maximum overlapping area persists between adjacent frames.  This\nis accomplished by creating a transform T(n) for every frame 0 to N (N+1 total frames) in the video.  In some embodiments, a list with N+1 entries may be generated, with each entry in the list corresponding to one of the T(n) transforms.\nPass 2: For each video frame n, the Transform T(n) is applied to the video frame.  Subsequently, the video is encoded to improve image stability.\nIn a conventional method, the operations of Pass 1 may be performed on an entire video and a list may be created for details of transforms T(n) performed for every step.\nNext, the entire video is encoded, or compressed, by first applying the appropriate transform T(n) for each frame 0 to N to stabilize the contents for the frame against camera movement.\nSince this approach is linear, it may be extremely slow, making it undesirable in real world situations.\nIn some embodiments, distributed computing resources are used to achieve distributed image stabilization.\nIn some embodiments, the process of image stabilization is distributed among multiple nodes for achieving better performance (e.g., end-to-end processing speed).  For example, Pass 1 may be performed as described above on a single node/machine\n(master node) since it is data dependent across adjacent frames.  A transforms file T is created which contains transform T(n) to be applied for each frame 0 to N in the input video.\nNext the input video is split (e.g., at key frames) into multiple segments.  For example, the N+1 frames may be divided into K segments, S(k), where k=0 to K-1.  The number of frames in each segment S(k) may be represented as L(k).  In general,\ndepending on key frames in the input video, L(k) may have different values.  The transforms file T (which contains the transforms T(n) corresponding to frames 0 to N of the input video) is also split and re-indexed per the segment frame offsets matching\nthe input video segments.  The segments and corresponding segment transforms file Ts(k) (created from the original transforms file T) is sent to each worker node for processing.  Each worker node applies the transforms Ts(k) in the received transform\nfile Ts(k) to the frames of the segment (e.g., 0 to K frames in the segment) and generates an image-stabilized segment.  As an example, in some embodiments, an input video may comprise 3,000 video frames (N=2,999), which may be divided into 200 segments\n(K=200).  The number of frames in each segment may be between 1 to 15 frames, and there may be 3 worker nodes for image stabilization.  The master node may send first 65 segments to the first worker node, along with first 65 entries of the transform\nmatrix list T, the next 65 segments and corresponding transform entries to the second worker node, and the last 70 segments and the corresponding transform entries to the third worker node.\nAt the end of processing the master node collects all individual segments and stitches them together, thereby creating the final image stabilized video.\nFIG. 24, FIG. 25, FIG. 26, FIG. 27 and FIG. 28 depict an operations performed by the master node and worker nodes.\nAt 2400, one of the nodes, called a master node, analyzes the frames of the video and generates a file T of transforms T(n).\nAt 2500, the master node splits the input video into multiple video segments S(k).  The master node computes a frame offset used for each split and keeps track of these frame offsets to produce the segment transform file Ts.\nAt 2502, the master node splits the transforms file, T, into multiple segment transforms Ts(k) using segment frame offsets calculated at 2500.\nAt 2600, the master node distributes the segments and corresponding transform files to worker nodes.  For example, first worker node may receive S1 segments, and T1 segment transforms, the nth worker node (where n may be between 1 and W, where W\nis the number of worker nodes) may receive Sn segments and corresponding Tn segment transforms from the transform file T.\nAt 2700, the worker nodes apply the transforms and create stabilized segments.  Upon completion of the stabilized segment, each worker node sends each segment S1e, S2e, .  . . up to Sne, where n is between 1 and W, to the master node.\nAt 2800, the master node stitches all the stabilized video segments back together.\nFIG. 29 illustrates an example method 2900 of distributed imaged stabilization.\nThe method 2900 includes, at 2902, analyzing, using a master processor, frames of a video to generate a list of per-frame transform for mitigating camera movement for each frame;\nThe method 2900 includes, at 2904, partitioning, using the first processor, the video into multiple video segments.  In some embodiments, the master processor and the first processor may be the same processor.\nThe method 2900 includes, at 2906, providing, from the first processor to a plurality of worker processors, one or more of the multiple video segments and a corresponding segment transform file.\nThe method 2900 includes, at 2908, performing, by the plurality of worker processors, image stabilization on the received video segment(s) by applying the corresponding segment transforms to the received video segments to produce stabilized\nvideo segments.\nThe method 2900 includes, at 2910, conveying the stabilized video segments from the plurality of worker processors to the master processor.\nThe method 2900 includes, at 2912, generating (e.g., by stitching or concatenating), by the master processor, a stabilized video from the stabilized video segments.\nIn some embodiments, analyzing frames includes analyzing motion vectors associated with the frames to identify a global component to the motion vectors.  In some embodiments, the analysis is performed on a compressed representation of the\nframes.  In some embodiments, frame analysis further includes decompressing at least some of the frames at least partially.\nIn some embodiments, the frame analysis includes applying a rotational transform to at least some of the frames.  In some embodiments, the transform for mitigating camera movement includes compensating for at least one of a left-right movement,\nan up-down movement and a rotational movement.\nIn some embodiments, generating the stabilized video from the stabilized video segments includes smoothing camera movement across contiguous stabilized video segments.\nIn some embodiments, partitioning into multiple video segments includes creating each video segment by including a key frame at a given position in the video segment.\nAs described before, distributed video encoding involves splitting the input videos into segments and passing the segments to a cluster of worker nodes.  The worker nodes independently encode the videos and ship the output segments back to the\nmaster node.  The master node then stitches all the segments together to create a final output.\nFIG. 30 illustrates an example of a distributed video encoding system 3000.  At 3001, an input video is received by a master node from a user device 1106, 1116.  The input video may be a compressed video or an uncompressed video.  At 3002, the\nmaster node may split the video into multiple segments.  The master node may then assign segments to different worker nodes for performing video encoding.  At 3003, each worker node encodes the segment received.  At 3004, each worker node sends back the\nresulting encoded bitstream to the master node.  At 3005, the master node combines the segments back to generate an encoded bitstream for the video.  A similar workflow of splitting, encoding, and combining is also described with respect to FIG. 3, FIG.\n5 and FIG. 6.\nOne aspect to streamline the workflow described in FIG. 30 is to ensure that the worker nodes are highly utilized without being over-utilized.  Over-burdened worker nodes can have a direct hit on performance and underutilized nodes are a wasted\nresource of the distributed system.\nConsidering that encoding can be done on the CPU or GPU, these two may be considered independent resources in a system.  The quality of encoding on a CPU is different from quality of encoding on a GPU.\nFIG. 31 depicts an example flowchart of a method 3100 implemented at the master node.  Each input video, Vin, is analyzed by the master node using a classification scheme C (Vin, output) and determined if the video is to be encoded using CPU or\nGPU.  Factors like target resolution and quality are considered in making this decision.\nVin-resource=C(Vin, output), calculated at 3102.\nAt 3104, the master node segments the video, The Vin-resource attribute is attached to each segment, Vs.  Alternatively, individual segments could also be analyzed to determine if they would be ideal for GPU or CPU encoding.  This yields true\nheterogeneous behavior, even with a file.  At 3106, a determination is made about whether a given segment is for encoding by a CPU or by a GPU.\nSegments Vs with a \"GPU\" attribute are queued in a GPU-queue (3108).\nSegments Vs with a \"CPU\" attribute are queued in a CPU-queue (3110).\nAs depicted in the example flowchart of FIG. 32, each worker node run a process, P, 3200 that determines its available CPU and GPU resources (3202).\nIf the CPU resources of a node fall below a threshold (e.g., as checked in 3204), new job (segment) is picked from a CPU queue.  Then, CPU based encoding is performed and encoded segment is returned to the master (3206).\nAs shown in FIG. 33, a similar process 3300 is performed at the worked node to determine GPU resource availability (3302).  Similarly if the GPU resources of a node fall below a threshold, a new job (segment) is picked from the GPU queue for\nprocessing (3304).\nAt the end of processing the worker nodes submit encoded chunks back to the master node (3306).\nMaster node reassembles the individual segments and generates the output file.\nFIG. 34 is a flow diagram illustrating an example method 3400 for distributed video encoding.  The method 3400 may be implemented at a master server, as described herein.\nThe method 3400 includes, at block 3402, receiving a video for encoding.  In some embodiments, e.g., the video may be received from a user device that uploads the video to the master server.\nThe method 3400 includes, at block 3404, establishing a pool of worker server nodes for encoding the video.  In some embodiments, the establishing the pool of worker server nodes includes querying each server node, prior to the receiving the\nvideo, for information about encoding capabilities of the worker server node.  In some embodiments, the encoding capabilities include at least one of protocol compliance, a software-based encoding capability, e.g., a CPU-based encoder, and a graphics\nprocessor based processing capability, e.g., a GPU encoder.\nThe method 3400 includes, at block 3406, splitting the video into multiple segments.  In some embodiments, the splitting includes splitting the video such that each segment includes at least one key frame from the video that is independently\ndecodable.\nThe method 3400 includes, at block 3408, submitting the multiple segments to the pool of worker nodes for encoding based such that each segment is submitted to a corresponding worker node based on a run-time availability of the corresponding\nworker node.  In some embodiments, the run-time availability includes determining whether the corresponding worker node is able to meet a target time within which a segment is to be encoded.  The method 3400 includes, at block 3409, instructing to each\nworker node in the pool whether that worker node should use CPU-based encoding or GPU-based encoding to encode the segment submitted to the worker node.  In some embodiments, the master node may first decide whether a segment should be encoded using\nCPU-based encoding or GPU-based encoding and based on this decision, the master node may select one of the available worker nodes from the pool of worker nodes that can perform encoding using the selected encoding method (GPU-based or CPU-based).\nIn some embodiments, the method 3400 further includes receiving encoding parameters for the video and wherein the establishing the pool includes selecting worker server nodes whose encoding capabilities are compatible with the encoding\nparameters.\nIn some embodiments, the method 3400 further includes receiving encoded segments from the worker nodes, and reassembling the encoded segments to produce encoded sequence.  In some embodiments, the method 3400 includes performing another encoding\non the video sequence at a different encoding bitrate to produce another encoded copy of the video sequence.  In some embodiments, the method 3400 may further including making a decision about whether to use a CPU or a GPU for encoding segments of the\nvideo, e.g., based on resource availability.\nWhen a distributed encoding system, e.g., as depicted in FIGS. 2, 3, 5, 6, 7, 15, 30, etc. is used for encoding segments of multiple video sequence, the resulting encoding can finish at a very fast rate.  For example, a 30 minute video may be\nsplit into 1 minute long segments, with the resulting 30 segments being fed to 30 worker nodes that can encode the video at 2.times.  rate, thereby generating encoded bitstream for the 30 minute video in slightly more than 30 seconds (depending on the\nnetwork traffic and splitting/combining overhead).\nHowever, in video file upload and sharing networks, often, hundreds of users are uploading their video files for storage and sharing.  Also, some users may have service level agreements with the service provider that their video file, once\nuploaded, should become available for worldwide viewing within a specified time period.  This time period may be \"real time\" especially for news sources that want their news to become available for public viewing as soon as the news clip us uploaded to\nthe server.\nSuch demands on the distributed video encoding system of having a large number of videos in a queue for encoding, while at the same time wanting to encode some videos at extremely quick turnaround, may pose conflicting demand on resources.  For\nexample, while the above-discussed 30 minute video may be encoded in 30 seconds if all distributed resources were available to the video, this video may be queued behind 500 videos from other users, which may be 2 minutes long each, thereby causing\nseveral minutes of delay before the encoding of the 30 minute video finished.\nThe techniques presented in this document can be used, in one aspect, to overcome this operational disadvantage by reprioritizing video encoding on a segment by segment basis instead of a video by video basis.  This allows, for example, for\nencoding to be paused after a single segment and resources to be redistributed or reallocated as desired, rather than needing to wait for the end of encoding of an entire video, as required by traditional encoding systems.\nDistributed video encoding can achieve must faster compression than a serial operation of a single encoder since a master node distributes the task of encoding among multiple worker nodes.  If Ms is the time taken for encoding one video on a\nsingle machine, the following equation yields the time taken on a distributed environment.  Md=(Ms/n)+O, where\nMd=total time taken to process a video on a distributed encoding platform;\nN=number of worker nodes; and\nO=overhead for splitting, merging and job distribution.  This is linear to the length of the video.\nIt is beneficial to some users to have an arrangement with a video sharing service provider to be able tm meet a fixed speed of encoding irrespective of number of resources available or any other users' encoding jobs undertaken by the service\nprovider.  The disclosed techniques can be used to monitor and control speed of each encode via a feedback loop from worker nodes to the master node.\nFIG. 35 depicts an example distributed video encoding system 3500 in which a master node 3501 performs the task of splitting a video into multiple video segments of smaller sizes and dispatching them to multiple worker nodes as encoding jobs, as\ndescribed in the present document.\nA job submitter component, residing in master node, may be responsible for submitting segment level jobs to a job_queue which holds a queue of segments to be sent to worker nodes.\nIn a traditional case, the job_submitter may keep pushing each video segment as it arrives into the job_queue and the worker nodes pull jobs from the job queue, process them and send them back to the master node.  Assuming, an input video I is\nsplit into \"n\" segments thereby creating n jobs.  Ij1, Ij2, .  . . , Ijn.  Similarly a second input video K would have Kj1, Kj2, .  . . Kjm jobs.  In the traditional case, because of the queuing, job Kj1 starts only after Ijn is done.  Also, Until Ijn is\ndone, all the worker nodes are only processing jobs related to input I that were added to the job queue ahead of the segments of video K.\nIn some embodiments, the master node may associate, with every video received, a target speed of processing.  For example, a processing speed of 5.times.  implies that the job is to be done 5 times faster than real time.  Next the jobs are\nsegmented at key frames while maintaining the speed requirements.\nThe Job Dispatcher may start by submitting a small number of segments, S, from every input into the job queue.  The number S may be one or a higher number based on the granularity of service layer agreement that is to be met.\nThe worker nodes receive their respective encoding segments, and at the end of encoding a chunk, return the results of encoding back to the master node.\nThe job dispatcher may keep a progress check for each job.  The job dispatcher may also monitor the processing time taken by each worker node for encoding the previously sent segments.\nBased on the progress of each input, the desired speed of encoding for that job and the available worker nodes, the job dispatcher dynamically decides the number of new segments to be submitted to the job queue. \nNumNewSegmentsToSubmit(I,S)=Status(Progress of input I,Number of worker nodes)\nUsing this relation, for example, when the master node notices that a given video is falling behind on the target speed of encoding, the master node may submit more segments from that video to the job queue.  Similarly, for another video, whose\nencoding is running faster than its target rate, fewer number of segments may be submitted to the queue.\nFIG. 36 illustrates an example flowchart for a method 3600 of encoding multiple videos in a distributed computer network.\nThe method 3600 includes, at 3602, receiving at least a portion of a video.  For example, the submission of segments described above may begin without having to wait until the entire video is received, e.g., simply after first S number of video\nsegments are available for encoding.\nThe method 3600 includes, at 3604, splitting the portion into multiple video segments.  In some embodiments, the received portion may be split into same sized video segments (e.g., 6 seconds long).  In some embodiments, the techniques descried\nherein, e.g., for including key frames in an open GOP scenario, may be used.\nThe method 3600 includes, at 3606, submitting the multiple video segments to a video encoding pipeline that includes segments from at least one other video, wherein the video encoding pipeline operates to issue video segments to encoding nodes\nfor video encoding in a priority order.  The priority order may depend on the monitoring of the completion speed of encoding of the video.\nThe method 3600 includes, at 3608, adjusting a number of video segments sent to the video encoding pipeline based on encoding performance.\nIn some embodiments, the adjustment is performed in response to a target encoding time budget for the video and an encoding latency in the distributed network of servers.  For example, when a video is falling behind on a scheduled time budget, a\ngreater number of segments of that video may be submitted to the encoding pipeline.\nIn some embodiments, the encoding of the video is started after encoding of the at least one other video and the encoding of the video is finished before the encoding of the at least one other video.\nIn some embodiments, the splitting operation includes splitting the portion into multiple video segments so that each video segment includes at least one key frame.\nIn some embodiments, at least some video segments include at least one frame occurring temporally prior to the at least one key frame.  In other words, the first frame in some segments may not be an independently decodable frame such as an I\nframe but may be a P or B type frame.\nIn some embodiments, the sending to the encoding node occurs prior to receiving the entirety of the video at the server.\nIn some embodiments, the multiple video segments of the input video and segments from the at least one other video may be of same size, and may thus include the number of video frames.\nIn some embodiments, the method 3600 further includes selecting, based on a video encoding load, the one of the encoding nodes to which the at least one of the multiple video segments is sent out of order.\nIt will be appreciated that techniques are disclosed for prioritizing submissions of video encoding segments to a pipeline of video encoding such that encoding jobs from one video being encoded do not block encoding of another video segment.  By\nusing the target completion rate or time budget as criteria, the queue filling technique ensures that priority of encoding is increased when a video encoding may fall behind its schedule.\nComputer System\nFIG. 37 is a block diagram of a computer system as may be used to implement features of some of the embodiments, e.g., master nodes or worker nodes, as described herein.  The computing system 3700 may include one or more central processing units\n(\"processors\") 3705, memory 3710, input/output devices 3725 (e.g., keyboard and pointing devices, display devices), storage devices 3720 (e.g., disk drives), and network adapters 3730 (e.g., network interfaces) that are connected to an interconnect 3715. The interconnect 3715 is illustrated as an abstraction that represents any one or more separate physical buses, point to point connections, or both connected by appropriate bridges, adapters, or controllers.  The interconnect 3715, therefore, may\ninclude, for example, a system bus, a Peripheral Component Interconnect (PCI) bus or PCI-Express bus, a HyperTransport or industry standard architecture (ISA) bus, a small computer system interface (SCSI) bus, a universal serial bus (USB), IIC (I2C) bus,\nor an Institute of Electrical and Electronics Engineers (IEEE) standard 1394 bus, also called \"Firewire\".\nThe memory 3710 and storage devices 3720 are computer-readable storage media that may store instructions that implement at least portions of the various embodiments.  In addition, the data structures and message structures may be stored or\ntransmitted via a data transmission medium, e.g., a signal on a communications link.  Various communications links may be used, e.g., the Internet, a local area network, a wide area network, or a point-to-point dial-up connection.  Thus, computer\nreadable media can include computer-readable storage media (e.g., \"non transitory\" media) and computer-readable transmission media.\nThe instructions stored in memory 3710 can be implemented as software and/or firmware to program the processor(s) 3705 to carry out actions described above.  In some embodiments, such software or firmware may be initially provided to the\nprocessing system 3700 by downloading it from a remote system through the computing system 3700 (e.g., via network adapter 3730).\nIt will be appreciated by one of ordinary skill in the art that the distributed video encoding techniques described in the present document can be used to accelerate video encoding by breaking an input video up into multiple segments and then\nassigning segments to different computers to perform the encoding task.  In one advantageous aspect, the system can not only perform effective variable bits-per-frame encoding within a single segment, but cannot achieve good encoding across multiple\nsegments as each computer is only working on a single segment.  One advantageous aspect is explained with reference to FIG. 38.\nIn FIG. 38, graph 3802 shows the complexity of a typical input video with a high complexity region in the middle of the video as a function of time along the horizontal axis (3804) and encoding complexity of the video along the vertical axis\n3806.\nGraph 3820 shows the video bitrate allocation that will occur in a good VBR system.  The high complexity region of the video is allocated additional bits in order to deliver good video quality during this time, with the average bitrate\nallocation across all frames of the video depicted by dashed line 3822.\nGraph 3840 shows the same video complexity as in 3806, in a distributed video encoding system and shows where the video will be segmented along time axis 3804, in this case, into 3 segments, represented by vertical dashed lines.\nGraph 3860 shows the video bitrate allocation that will occur in a prior art distributed video encoding system.  Each segment encoder will achieve the average bitrate for the segment, and will allocate bits to the high and low complexity\nsections of the video.  As can be seen, this bitrate allocation is not globally optimized compared to the graph 3820.  The above-discussed techniques enable a distributed video encoding system to achieve a performance similar to 3820, by performing\ncomplexity analysis of segments, and by using the results of segment-wise complexity analysis during encoding of each segment.\nThe various embodiments introduced herein can be implemented by, for example, programmable circuitry (e.g., one or more microprocessors) programmed with software and/or firmware, or entirely in special-purpose hardwired (non-programmable)\ncircuitry, or in a combination of such forms.  Special-purpose hardwired circuitry may be in the form of, for example, one or more ASICs, PLDs, FPGAs, etc.\nRemarks\nThe above description and drawings are illustrative and are not to be construed as limiting.  Numerous specific details are described to provide a thorough understanding of the disclosure.  However, in certain instances, well-known details are\nnot described in order to avoid obscuring the description.  Further, various modifications may be made without deviating from the scope of the embodiments.  Accordingly, the embodiments are not limited except as by the appended claims.\nReference in this specification to \"one embodiment\" or \"an embodiment\" means that a particular feature, structure, or characteristic described in connection with the embodiment is included in at least one embodiment of the disclosure.  The\nappearances of the phrase \"in one embodiment\" in various places in the specification are not necessarily all referring to the same embodiment, nor are separate or alternative embodiments mutually exclusive of other embodiments.  Moreover, various\nfeatures are described which may be exhibited by some embodiments and not by others.  Similarly, various requirements are described which may be requirements for some embodiments but not for other embodiments.\nThe terms used in this specification generally have their ordinary meanings in the art, within the context of the disclosure, and in the specific context where each term is used.  Certain terms that are used to describe the disclosure are\ndiscussed below, or elsewhere in the specification, to provide additional guidance to the practitioner regarding the description of the disclosure.  For convenience, certain terms may be highlighted, for example using italics and/or quotation marks.  The\nuse of highlighting has no influence on the scope and meaning of a term; the scope and meaning of a term is the same, in the same context, whether or not it is highlighted.  It will be appreciated that the same thing can be said in more than one way. \nOne will recognize that \"memory\" is one form of a \"storage\" and that the terms may on occasion be used interchangeably.\nConsequently, alternative language and synonyms may be used for any one or more of the terms discussed herein, nor is any special significance to be placed upon whether or not a term is elaborated or discussed herein.  Synonyms for certain terms\nare provided.  A recital of one or more synonyms does not exclude the use of other synonyms.  The use of examples anywhere in this specification including examples of any term discussed herein is illustrative only, and is not intended to further limit\nthe scope and meaning of the disclosure or of any exemplified term.  Likewise, the disclosure is not limited to various embodiments given in this specification.\nWithout intent to further limit the scope of the disclosure, examples of instruments, apparatus, methods and their related results according to the embodiments of the present disclosure are given above.  Note that titles or subtitles may be used\nin the examples for convenience of a reader, which in no way should limit the scope of the disclosure.  Unless otherwise defined, all technical and scientific terms used herein have the same meaning as commonly understood by one of ordinary skill in the\nart to which this disclosure pertains.  In the case of conflict, the present document, including definitions will control.", "application_number": "14852467", "abstract": " Various of the disclosed embodiments relate to a distributed video\n     encoding or transcoding system may utilize multiple encoding nodes to\n     encode a video sequence by splitting the video into multiple smaller\n     video segments. The assignment of video segments to the encoding nodes is\n     performed to balance the use of the encoding nodes by selecting a node\n     based on its encoding capabilities, e.g., whether the node employed a\n     central processing unit (CPU) based encoding or a graphics processor unit\n     (GPU) based encoding.\n", "citations": ["5768609", "5805225", "5917962", "6038256", "6064748", "6115131", "6285661", "6507617", "7161983", "8135061", "8649426", "8659679", "8786716", "9374532", "9484954", "9635334", "9756351", "20020131084", "20020181588", "20030091311", "20030177503", "20040146109", "20040190615", "20040196905", "20040202248", "20050207442", "20050243922", "20060017814", "20060050971", "20060114985", "20060233237", "20060291807", "20070002946", "20070110161", "20070274340", "20080137736", "20080170795", "20080219349", "20080232687", "20090219415", "20090323824", "20100104017", "20100146363", "20100322310", "20110305273", "20120093221", "20120128061", "20120147958", "20120250755", "20120281748", "20130064305", "20130083843", "20130114744", "20130128066", "20130136184", "20130142425", "20130188717", "20130230100", "20130329062", "20130329066", "20140212002", "20140254952", "20140267801", "20140359678", "20140359679", "20140376624", "20150181218", "20150195461", "20150206596", "20150281746", "20150363635", "20160092561", "20160140695", "20160156926", "20170064329", "20170078574", "20170078671", "20170078676", "20170078680", "20170078681", "20170078686", "20170078687", "20170209112"], "related": []}, {"id": "20170134516", "patent_code": "10375188", "patent_name": "Sending notifications as a service", "year": "2019", "inventor_and_country_data": " Inventors: \nGutman; Julian Kevin (Brooklyn, NY), Gerson; Elad (Short Hills, NJ), Shin; Key K. (New York, NY), Dauer; Benjamin Thamas (Irvington, NY), Dassa; Guy (Mamaroneck, NY)  ", "description": "<BR><BR>TECHNICAL FIELD\nThis disclosure generally relates to delivering notifications to a user.\n<BR><BR>BACKGROUND\nA social-networking system, which may include a social-networking website, may enable its users (such as persons or organizations) to interact with it and with each other through it.  The social-networking system may, with input from a user,\ncreate and store in the social-networking system a user profile associated with the user.  The user profile may include demographic information, communication-channel information, and information on personal interests of the user.  The social-networking\nsystem may also, with input from a user, create and store a record of relationships of the user with other users of the social-networking system, as well as provide services (e.g., wall posts, photo-sharing, event organization, messaging, games, or\nadvertisements) to facilitate social interaction between or among users.\nThe social-networking system may send over one or more networks content or messages related to its services to a mobile or other computing device of a user.  A user may also install software applications on a mobile or other computing device of\nthe user for accessing a user profile of the user and other data within the social-networking system.  The social-networking system may generate a personalized set of content objects to display to a user, such as a newsfeed of aggregated stories of other\nusers connected to the user.\nA mobile computing device--such as a smartphone, tablet computer, or laptop computer--may include functionality for determining its location, direction, or orientation, such as a GPS receiver, compass, gyroscope, or accelerometer.  Such a device\nmay also include functionality for wireless communication, such as BLUETOOTH communication, near-field communication (NFC), or infrared (IR) communication or communication with a wireless local area networks (WLANs) or cellular-telephone network.  Such a\ndevice may also include one or more cameras, scanners, touchscreens, microphones, or speakers.  Mobile computing devices may also execute software applications, such as games, web browsers, or social-networking applications.  With social-networking\napplications, users may connect, communicate, and share information with other users in their social networks.\n<BR><BR>SUMMARY OF PARTICULAR EMBODIMENTS\nIn particular embodiments, a notification providing system may be a universal platform that may allow an entity (e.g., person, organization, company, etc.) to send content to another entity via a subscription (e.g., consumer, organization,\ncompany, etc.).  The content may comprise meta-data, which may be used to direct particular content to particular subscribers.  The entity receiving the content may subscribe to and/or customize any stream of content by way of filters, which will be\nexplained below.  The notification providing system may be understood to be a canonical representation of entities across content providers and outside of any other standard platform.\nIn particular embodiments, the notification providing system described herein may enable a curated and personalized stream of content from publishers to subscribers.  The notification providing system may reduce the need to install many\ndifferent news providing applications on a personal computing device.  As such, the notification providing system described herein may also reduce the amount of notifications a user receives from different news providing applications (from, e.g.,\nTWITTER, NYTIMES, WSJ, etc.).  The notification providing system described herein may also reduce the need for publishers to create their own applications to create and send content to users.  The notification providing system described herein may also\nenable the creation of universal profiles (based on canonical entities) that are customizable by the user.\nIn particular embodiments, a notification-providing system may allow third-party publishers to push notifications of interest to a user device as part of a notification subscription service.  A publisher may have several stations that each\nproduce content in various formats (e.g., blog posts, articles, photos, videos, interactive software, etc.).  The publisher may also associate one or more tags with each different piece of content.  A tag may be understood to mean a content identifier\nthat is stored in the content's metadata.  The publisher may send the tagged content to the social network.  A user of the social networking system may obtain a subscription to the notification service by selecting at least one station and indicating at\nleast one interest.  The social networking system may compare the received tags to the user's interest(s).  If any tag matches (or is related to) an interest, the social networking system may send the user a notification of the content through at least\none delivery channel.  A user may obtain a subscription across multiple stations and multiple publishers.  If this is the case, the social networking system may send to the user notifications whose content tags match the user's interest(s).\nThe steps of sending relevant notifications to users as part of a subscription service may be summarized as follows: a publisher may create content for a subscription; the publisher may assign tags to the content; the publisher may send the\ntagged content to a social networking system; the social networking system may determine a user's interests based on the user's social graph information or any other suitable method; the social networking system may identify tags sent by the publisher\nthat match the user's interests; and the social networking system may send the identified notifications to the user.\nUsers may also configure the notification service to send notifications based on their interests across a variety of verticals (e.g., sports, movies, celebrity gossip, geographic location) and notifications from multiple publishers may be\npresented to the user in an aggregated platform.  This allows the publishers to focus on content creation while the social networking system focuses on audience targeting and notification delivery.\nSuch notifications may be sent through one or more delivery channels, e.g., sent by one or more communication media (e.g., SMS, MMS, email, particular application, voice) to one or more unique endpoints (e.g., a telephone number, an email\naddress, a particular client device as specified by a unique device identifier, a particular user account for the particular application or for the client device).  In particular embodiments, the notification-providing system may utilize different\ntechniques to attempt to provide a notification to a user in a manner that increases the likelihood that the user will interact with the notification (e.g., a \"click-through\" action whereby the user clicks on a link presented in a visual notification\npresenting promotional content, which then brings up a third-party website on the user's screen), which hopefully increases the likelihood that a \"conversion\" takes place--that the user takes some final action that is the ultimate goal of delivering the\nnotification (e.g., completes an action, such as a registration, content consumption, or a purchase, on the third-party website).\nThe embodiments disclosed above are only examples, and the scope of this disclosure is not limited to them.  Particular embodiments may include all, some, or none of the components, elements, features, functions, operations, or steps of the\nembodiments disclosed above.  Embodiments according to the invention are in particular disclosed in the attached claims directed to a method, a storage medium, a system and a computer program product, wherein any feature mentioned in one claim category,\ne.g. method, can be claimed in another claim category, e.g. system, as well.  The dependencies or references back in the attached claims are chosen for formal reasons only.  However any subject matter resulting from a deliberate reference back to any\nprevious claims (in particular multiple dependencies) can be claimed as well, so that any combination of claims and the features thereof are disclosed and can be claimed regardless of the dependencies chosen in the attached claims.  The subject-matter\nwhich can be claimed comprises not only the combinations of features as set out in the attached claims but also any other combination of features in the claims, wherein each feature mentioned in the claims can be combined with any other feature or\ncombination of other features in the claims.  Furthermore, any of the embodiments and features described or depicted herein can be claimed in a separate claim and/or in any combination with any embodiment or feature described or depicted herein or with\nany of the features of the attached claims. <BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nFIG. 1 illustrates an example method for sending user-relevant content in a notification subscription service.\nFIG. 2 illustrates an example notification providing system that provides user-relevant notifications.\nFIG. 3 illustrates an exemplary interaction diagram between a publisher, user, and social networking system.\nFIG. 4A illustrates an example network environment associated with a social-networking system.\nFIG. 4B illustrates architecture for delivering notifications to a user.\nFIG. 5 illustrates an example social graph\nFIG. 6 illustrates an example embodiment of a notification-providing system.\nFIG. 7 illustrates an example method for handling notification delivery in a user-aware manner.\nFIG. 8 illustrates an example method for providing a user-aware notification delivery service.\nFIG. 9 illustrates an example computer system.\n<BR><BR>DESCRIPTION OF EXAMPLE EMBODIMENTS\nIn particular embodiments, a notification providing system may be a universal platform that may allow an entity (e.g., person, organization, company, etc.) to send content to another entity via a subscription (e.g., consumer, organization,\ncompany, etc.).  The content may comprise meta-data, which may be used to direct particular content to particular subscribers.  The entity receiving the content may subscribe to and/or customize any stream of content by way of filters to create a\npersonalize stream of content, which will be explained below.  The notification providing system may be understood to be a canonical representation of entities across content providers and outside of any other standard platform.\nIn particular embodiments, the notification providing system described herein may enable a curated and personalized stream of content from publishers to subscribers (i.e., a subscriber may \"personalize\" his subscription so that she receives the\ncontent she is most interested in).  The notification providing system may reduce the need to install many different news providing applications on a personal computing device.  As such, the notification providing system described herein may also reduce\nthe amount of notifications a user receives from different news providing applications (from, e.g., TWITTER, NYTIMES, WSJ, etc.).  The notification providing system described herein may also reduce the need for publishers to create their own applications\nto create and send content to users.  The notification providing system described herein may also enable the creation of universal profiles (based on canonical entities) that are customizable by the user.\nIn particular embodiments, the notification-providing system described herein may allow third-party publishers to push notifications of interest to a user device as part of a notification subscription service.  A publisher may have several\nstations that each produce content in various formats (e.g., blog posts, articles, photos, videos, interactive software, etc.).  The publisher may also associate one or more tags with each different piece of content.  A tag may be understood to mean a\ncontent identifier that is stored in the content's metadata.  The publisher may send the tagged content to the social network.  A user of the social networking system may obtain a subscription to the notification service by selecting at least one station\nand indicating at least one interest.  The social networking system may compare the received tags to the user's interest(s).  If any tag matches (or is related to) an interest, the social networking system may send the user a notification of the content\nthrough at least one delivery channel.  A user may obtain a subscription across multiple stations and multiple publishers.  If this is the case, the social networking system may send to the user notifications whose content tags match the user's\ninterest(s).\nThe steps of sending relevant notifications to users as part of a subscription service may be summarized as follows: a publisher may create content for a subscription; the publisher may assign tags to the content; the publisher may send the\ntagged content to a social networking system; the social networking system may determine a user's interests based on the user's social graph information or any other suitable method; the social networking system may identify tags sent by the publisher\nthat match the user's interests; and the social networking system may send the identified notifications to the user.\nUsers may also configure the notification service to send notifications based on their interests across a variety of verticals (e.g., sports, movies, celebrity gossip, geographic location) and notifications from multiple publishers may be\npresented to the user in an aggregated platform.  This allows the publishers to focus on content creation while the social networking system focuses on audience targeting and notification delivery.  Further, the invention described herein may enable\npublishers to send content to users of mobile devices without the need to create their own mobile applications.\nSuch notifications may be sent through one or more delivery channels, e.g., sent by one or more communication media (e.g., SMS, MMS, email, particular application, voice) to one or more unique endpoints (e.g., a telephone number, an email\naddress, a particular client device as specified by a unique device identifier, a particular user account for the particular application or for the client device).  The delivery method may also be personalized for each subscriber.  In particular\nembodiments, the notification-providing system may utilize different techniques to attempt to provide a notification to a user in a manner that increases the likelihood that the user will interact with the notification (e.g., a \"click-through\" action\nwhereby the user clicks on a link presented in a visual notification presenting promotional content, which then brings up a third-party website on the user's screen), which hopefully increases the likelihood that a \"conversion\" takes place--that the user\ntakes some final action that is the ultimate goal of delivering the notification (e.g., completes an action, such as a registration, content consumption, or a purchase, on the third-party website).\nThe embodiments disclosed herein are only examples, and the scope of this disclosure is not limited to them.  Particular embodiments may include all, some, or none of the components, elements, features, functions, operations, or steps of the\nembodiments disclosed herein.\nIn particular embodiments, a notification-providing system may deliver notifications to a user in a user-aware manner.  Such notifications may be sent through one or more delivery channels, e.g., sent by one or more communication media (e.g.,\nSMS, MMS, email, particular application, voice) to one or more unique endpoints (e.g., a telephone number, an email address, a particular client device as specified by a unique device identifier, a particular user account for the particular application\nor for the client device).  In particular embodiments, the notification-providing system may utilize different techniques to attempt to provide a notification to a user in a manner that increases the likelihood that the user will interact with the\nnotification (e.g., a \"click-through\" action whereby the user clicks on a link presented in a visual notification presenting promotional content, which then brings up a third-party website on the user's screen), which hopefully increases the likelihood\nthat a \"conversion\" takes place--that the user takes some final action that is the ultimate goal of delivering the notification (e.g., completes an action, such as a registration, content consumption, or a purchase, on the third-party website).\nIn particular embodiments, the notification-providing system may be implemented as part of a social-networking system that may handle requests from third-party systems to deliver notifications to a user of the social-networking system in a\nuser-aware manner.  The social-networking system may register one or more delivery channels for delivery of notifications to the user.  Upon receiving one or more requests from a third-party system to send notifications to the user, the social-networking\nsystem may assess the user's current delivery context with respect to the registered delivery channels and determine a delivery policy to be applied to the request(s).  The social-networking system may then handle the requests in accordance with the\ndelivery policy, which may include sending at least one notification to the user in fulfillment of the requests.\nIn particular embodiments, a policy engine of the notification-providing system may assess different factors in order to determine the delivery policy (e.g., the delivery instructions) for a notification.  For any particular notification, the\npolicy engine may assess not only (1) information associated with the notification (e.g., the source, the content, or the format) and (2) information associated with a particular user (e.g., demographic information for the user, the user's location, the\nuser's available delivery channels and the status thereof, the user's current delivery context, user profile information, or social-networking information for the user), but also (3) historical notification information about this particular user's\nresponses to past notifications (e.g., conversion rates for different notification/context/delivery patterns) and about prior context/delivery patterns (if any) for the current notification (and interaction levels, if any, for those prior\ncontext/delivery patterns).\nIn particular embodiments, a history service of the notification-providing system may collect and analyze the user's responses to past notifications in order to determine the user's level of interaction (if any) with the past notifications. \nInformation about the user's responses to past notifications may be stored in a historical notification data store.  The type of historical data collected about a past notification may include, by way of example and not limitation: the notification\ncontent and format, the source of the notification, the date and time when the past notification was delivered to the user, the delivery channel(s) to which the notification was sent, whether the notification was successfully delivered to the delivery\nchannel(s) (and attempted context/delivery patterns), or information about a subsequent completed transaction (wherein the completed transaction is associated with the past notification), including time-to-completion.\nUsing such historical data, the history service may also rank, by their conversion scores, different aspects of the notification/context/delivery patterns for past notifications sent to a particular user, such as, by way of example and not\nlimitation: delivery channels, notification content types, notification sources, delivery contexts, or delivery patterns.  In particular embodiments, the history service may compute the ranking of conversion scores by combining together two sets of data,\nfor example, the average conversion score for a particular delivery channel at a first time after delivery of the notification (e.g., 3 hours after delivery) and the average conversion score for a particular delivery channel at a second time after\ndelivery of the notification (e.g., 36 hours after delivery).  Taking a global view of such historical data, the history service may also rank, by their conversion scores, different aspects of the notification/context/delivery patterns for past\nnotifications across all users.\nAs information about user interactions with notifications sent to the user are sent back to the notification-providing system, the history service may continuously update the historical notification data based on received information, so as to\nprovide the policy engine with the most up-to-date information about past user interactions.  In some embodiments, the history service may also maintain a decision-tree model, based on the historical notification data, for determining delivery\ninstructions for a current notification.  The decision-tree model itself may be initially constructed using a machine-learning algorithm, based on a set of training data and/or a pre-existing set of historical data.\nIn particular embodiments, a registration service of the notification-providing system may collect and store information sent by a device of the user upon enabling a new delivery channel (a communication medium-endpoint combination).  For\nexample, if the user installs a software application on their computing device through which notifications may be delivered, the application may send registration information back to the registration service indicating that a new delivery channel is now\navailable for this particular user--that a new communication medium (e.g., a particular application) is available for a particular endpoint (e.g., the computing device).  Such registration information may be provided in the form of a registration token\nidentifying the user, the installed instance, and the computing device.  The registration information about the user's available delivery channels may then be provided to the policy engine for use when determining the delivery policy to be applied to a\nparticular notification.  Information about the user's available delivery channels may be stored in a registration data store.  The registration data may include, by way of example and not limitation: a unique identifier for the endpoint, features and\ncapabilities of the endpoint (e.g., audio-visual specifications, battery capacity, or network connectivity specifications), a unique identifier for the communication medium, features and capabilities of the communication medium (e.g., maximum message\nsize, data transfer allotment, or maximum bandwidth), or a unique identifier for the installed instance of the software application.\nBy assessing information such as that described above, the policy engine may generate a delivery policy for the notification.  The delivery policy may provide instructions for a notification delivery service to deliver the notification in\naccordance with a specified context/delivery pattern.  The context/delivery pattern may provide instructions regarding when to send the notification (e.g., day, time, ideal delivery context), how to send the notification (e.g., which delivery channels\nshould be utilized), a maximum duration beyond which the notification should be re-delivered, when and how to re-deliver the notification in the absence of user interaction and/or successful conversion, or whether to deliver the notification in light of\n(1) the information associated with the notification, (2) the information associated with a particular user, and (3) the historical notification information.\nActual delivery of the notification may be handled by a notification delivery service, which receives the notification and the delivery policy.  The notification delivery service may generate an appropriate form of the notification for delivery\nthrough the selected delivery channel(s).  The notification delivery service may schedule the notification for delivery at a specified time and day, for delivery upon detecting a particular user delivery context (e.g., upon detecting that the user has\nbegun actively using their mobile device; upon determining, based on the user's calendar information, that the user should be available; upon determining that the user's location has changed; or upon determining that the user has moved within a threshold\nproximity to one or more social-networking contacts of the user).\nInformation about user interactions with the notification may be sent back to a response-handling service by way of the same delivery channel by which the related notification was delivered.  Such information may include, for example, and\nwithout limitation: whether the user ever actively opened the notification (including, e.g., how many times the user actively opened the notification), user attention level with respect to the opened notification (including, e.g., how many times the user\nviewed or listened to the notification, how long the user paid attention on each occasion, and the user's delivery context on each occasion), whether the user clicked on a link in the notification, or whether the user provided feedback regarding the\nnotification (e.g., clicking to \"Like\" or rate the notification, or commenting on the notification).  Such information may also factor in negative feedback, such as, for example, and without limitation: whether the user dismissed the notification without\nopening it, whether the user subsequently blocked notifications from the source of the notification, whether the user subsequently disabled push notifications, whether the user subsequently logged out of the application, or whether the user subsequently\nunsubscribed from receiving certain email notifications.  Other factors may be considered when computing a conversion success rate, such as: comparison as against an expected level of interaction, comparison as against an average level of interaction,\nthe duration of time between delivery of the notification and the user interaction with the notification, delivery patterns leading to the interaction, or the number and/or pattern of lower-level interactions leading up to a higher-level interaction).  A\nconversion success rate may be determined based on a target user interaction (e.g., in some cases, the ultimate goal of a notification may be to cause the user to open and view the full text of the notification, whereas, in other cases, the ultimate goal\nof a notification may be to motivate the user to click on a link in the notification and then complete a purchase, download, or registration on a third-party website).  The response-handling service may then forward the user interaction information to\nthe history service, which may collect and analyze the user's responses to past notifications, as described above.\nThe steps of sending relevant notifications to users as part of a subscription service may be summarized as follows: a publisher may create content for a subscription; the publisher may assign tags to the content; the publisher may send the\ntagged content to a social networking system; the social networking system may determine a user's interests based on the user's social graph information or any other suitable method; the social networking system may send recommendations to the user to\nsign up for particular subscriptions (notifications of content corresponding to one or more particular stations and one or more particular interests) and register any subscriptions for which the user signs up; the social networking system may identify\ntags sent by the publisher that match the user's interests; and the social networking system may send the identified notifications to the user.\nUsers may also configure the notification service to send notifications based on their interests across a variety of verticals (e.g., sports, movies, celebrity gossip, geographic location) and notifications from multiple publishers may be\npresented to the user in an aggregated platform.  This allows the publishers to focus on content creation while the social networking system focuses on audience targeting and notification delivery.\nUsers may additionally configure the notification service to send notifications based on privacy settings, maturity ratings, and friends' affinity, among other things.\nSuch notifications may be sent through one or more delivery channels, e.g., sent by one or more communication media (e.g., SMS, MMS, email, particular application, voice) to one or more unique endpoints (e.g., a telephone number, an email\naddress, a particular client device as specified by a unique device identifier, a particular user account for the particular application or for the client device).  In particular embodiments, the notification-providing system may utilize different\ntechniques to attempt to provide a notification to a user in a manner that increases the likelihood that the user will interact with the notification (e.g., a \"click-through\" action whereby the user clicks on a link presented in a visual notification\npresenting promotional content, which then brings up a third-party website on the user's screen), which may increases the likelihood that a \"conversion\" takes place--that the user takes some final action that is the ultimate goal of delivering the\nnotification (e.g., completes an action, such as a registration, content consumption, or a purchase, on the third-party website).\nThe embodiments disclosed herein are only examples, and the scope of this disclosure is not limited to them.  Particular embodiments may include all, some, or none of the components, elements, features, functions, operations, or steps of the\nembodiments disclosed herein.\nIn particular embodiments, a notification-providing system may allow third-party publishers to push notifications of interest to a user device as part of a notification subscription service.  FIG. 1 illustrates an example method 100 for sending\nuser-relevant content in a notification subscription service.  These steps may be performed in a different order than that presented in FIG. 1, or they may also be performed simultaneously with one another.  In step 110, a publisher may create content\nfor a subscription.  A publisher may be any entity that creates online content.  It is contemplated that many publishers may be news media outlets, such as CNN, MSNBC, HUFFINGTON POST, the WALL STREET JOURNAL, the NEW YORK TIMES, etc., as well as sports\nand music networks, such as ESPN, MTV, VH1, etc. Additionally, any person may be a publisher as well.  Any blogger, amateur reporter, or novelist, etc. may be a publisher.  The subscription service may operate in an open platform format; thus, anyone may\nbe permitted to publish and send content to the social networking system as part of the notification subscription service.  In step 120, the publisher may assign or associate one or more tags to the content it has produced.  A tag may be a content\nidentifier that may be stored in the content's metadata.  A publisher may have several stations that each produce content in various formats (e.g., blog posts, articles, photos, videos, interactive software, etc.).  The publisher may also associate one\nor more tags with each different piece of content.  As an example and not by way of limitation, the celebrity gossip network TMZ may publish an article called \"celebrity sidewalk altercations,\" documenting several quarrels that have occurred over the\nyears between celebrities like Justin Bieber or Buzz Aldrin and the paparazzi or general members of the public.  The article may include text, photos, and video.  As part of the invention, TMZ may assign one or more tags to the content, such as \"Justin\nBieber,\" \"Buzz Aldrin,\" \"Sidewalk altercations,\" \"fist fights,\" \"face punches,\" etc. Note that these tags may be one or more separate words, or may be multiple words combined as one word (e.g., \"SidewalkAltercation\").  The tags may also use a hashtag or\nother symbol that designates the term as a tag (e.g., #JustinBieber).\nIn step 130 of FIG. 1, the publisher sends the tagged content to the social networking system.  The tagged content may comprise both the content media (e.g., blog post, photos, video, interactive software, etc.) and the content's metadata.  The\ntags associated with the content media may be stored in the content's metadata.  As an example, and not by way of limitation, ANIMAL PLANET may be a publisher, and may produce a station called \"dog training.\" Animal planet may then publish content of a\nvideo of Cesar Milan training a Labrador how to sit quietly on the sidewalk while its owner buys coffee at a local coffee shot.  The content may also include an article about how to properly train hyperactive Labradors.  The content's metadata may\ninclude one or more hashtags, such as \"Cesar Milan,\" \"Dog Whisperer,\" \"Dog Training,\" and the like.\nThe publisher may send the tagged content to the social network.  The social networking system then compares the tags associated with that content to user interests.  If there is a match between tags and user interests, the content that is\nassociated with the relevant tag is sent to the user along one or more delivery channels in the form of a notification.\nA user of the social networking system may obtain a subscription to the notification service by selecting at least one station and indicating at least one interest.  In particular embodiments, the social-networking system may only sign up a user\nfor a subscription upon receiving an explicit request to sign up for a subscription.  In some embodiments, the social-networking system may register a user for a subscription sua sponte based on an assessment of the user's interests and/or affinities. \nIn step 140, the social networking system may determine a user's interests based on the user's social-graph information.  Alternatively, the social networking system may determine the user's interests based on interests that the user has selected or\nindicated, or it may determine interests based on a combination of social graph information and interests that the user has selected.  Note that this step can be performed at any time; the social networking system may determine a user's interests before\nit receives content from publishers or while it is receiving content from publishers.  Additionally, the social networking system may determine a user's interests through semantic analysis, NLP, or a combination of all the methods described.\nAs another aspect of the invention, a user may select an interest, but may \"carve out\" a sub-topic in that interest.  For example a user may subscribe to an NFL football station, and may select the \"OAKLAND RAIDERS\" as an interest.  The user may\nonly be interested in receiving notifications about the RAIDERS' regular season and playoff games (if any), but may not be interested in receiving notifications about pre-season games.  The user may indicate that he does not want to receive such\nnotifications, either by deselecting that sub-topic, or by disliking (or otherwise expressing disinterest for) the notification and/or associated content.  The social networking system may determine, based on the user's past interaction with the social\nnetwork, that preseason RAIDERS football is not an interest of the user, and may refrain from notifying the user of such content.  This may occur automatically (i.e., without any input from the user, aside from disliking the notification/content or other\nexpression of disinterest).\nThe social networking system may automatically identify a user's interests and disinterests by analyzing how the user interacts with notifications without requiring the user to affirmatively select or de-select interests.  Additionally, the\nsocial networking system may or may not notify a user that a new interest has been added.  For example, if a user \"likes,\" reads, or watches several articles and videos related to Donald Trump's presidential race, the social networking system may\ndetermine that \"Donald Trump\" is an interest of the user.  The social networking system may also determine \"2016 election\" may be another interest of the user.  The social networking system may then add \"Donald Trump\" and \"2016 election\" to the user's\ninterests (with or without notifying the user of the added interest), and begin notifying the user of content with the \"Donald Trump\" and/or the \"2016 election\" tags.  Further, the user may not be interested in Donald Trump generally, but only with\nregard to his presidential race.  And, the user may only be interested in Donald Trump's election efforts, not Hillary Clinton or Ben Carson.  The social networking system may learn this information about the user when, for example, the user skips over\n(or otherwise expresses disinterest for) a notification of article about Donald Trump selling the MISS UNIVERSE organization.  Such an article may have the \"Donald Trump\" tag, but may not have a \"2016 election\" tag.  The user may have clicked through to\nall the articles having both \"Donald Trump\" and \"2016 election\" tag, but not clicked through to articles having the \"Donald Trump\" tag but no \"2016 election\" tag.  Moreover, the user could have passed over content with the \"Hillary Clinton\" tag, and so\nthe social networking system may determine that the user is not interested in Hillary Clinton's presidential race.  In this way, the user's interests may be more precisely identified by the social networking system.  This system may enable a curated and\npersonalized stream of content for each user of the notification providing system.\nThe social networking system may compare the received tags to the user's interest(s).  If any tag matches (or is related to) an interest, the social networking system may send the user a notification of the content through at least one delivery\nchannel.  In step 150, the social networking system may identify tags sent by the publisher that are associated with content and that also match at least one user interest.  A tag may \"match\" an interest in several ways.  First, if the tag and interest\nare identical, a match may exist.  For example, a publisher may tag an article with the term \"HELL'S KITCHEN,\" and the user may select or indicate \"HELL'S KITCHEN\" as one of the user's interests.  Because the tag and the interest are identical, there may\nbe a match.  Second, the tag could be related to the interest.  For example, the user may simply select \"culinary arts\" as an interest, and because HELL'S KITCHEN is a reality TV show about culinary arts, the two terms are related, and a match may exist\nbetween the tag and the interest.  Whether two terms are related enough to match may be determined by a number of ways, including affinity (explained below), natural language processing, or semantic analysis.  Additionally, the social networking system\nmay use multiple user interests to determine whether a match exists.  For example, the user may indicate \"culinary arts\" and \"reality TV competitions\" are interests.  Because HELL'S KITCHEN is a reality TV competition about culinary arts, there may be a\nmatch.  Additionally, the social networking system may determine the user's interest based on the user's social graph information.  For example, the user may have attended a cooking class, posted a link to a recipe, liked a TV episode from the Food\nNetwork, among other activities, all of which may indicate an affinity for cooking and/or reality TV.  This may lead to a match between the \"HELL'S KITCHEN\" tag and the user's interests.\nAnother method to determine a user's interest may occur when the user selects a station.  A user may obtain a subscription across multiple stations and multiple publishers.  If this is the case, the social networking system may send to the user\nnotifications whose content tags match the user's interest(s).  To continue the above example, a station may exist called \"All About Food.\" \"All About Food\" may publish content related to recipes, cooking, BBQ-ing, restaurants, etc. After the user\nselects this station, the notification service may request the user to select interests within that station.  These interests may include \"quick recipes,\" \"fine dining,\" \"best dives,\" \"favorite food trucks,\" \"TV shows about cooking,\" etc. If the user\nselects as an interest \"TV shows about cooking,\" the tag \"HELL'S KITCHEN\" may be a match because HELL'S KITCHEN is a TV show about cooking.\nIn step 160, the social networking system may send the identified notifications (i.e., the notifications whose tags match the user's interests) to the user using the notification delivery method described herein.\nThe social networking system may filter out certain content from or add content to a subscription based on a number of factors, including, but not limited to, a user's privacy settings, age, maturity level, and friends' affinity.  Users may\nadditionally configure the notification service to send notifications based on privacy settings, maturity ratings, and friends' affinity, among other things.  For example and not by way of limitation, a user may subscribe to a station called \"Race Cars.\"\nThe user may either select certain interests or the social networking system can identify interests based the user's social graph information.  A user who subscribes to a \"Race Car\" station may have interests that include \"Formula 1,\" \"drag racing\"\n\"Daytona 500\" \"street racing,\" and the like.  In conjunction with photos and articles about race cars, it may be common for publishers to also publish photos of inadequately clothed women.  If a user is under a certain age, or has otherwise designated or\nexpressed a distaste for such material, the social networking system may filter out content that includes inadequately clothed women before it sends content to the user.  The filtering mechanism may occur by identifying tags that are related to\npotentially inappropriate material (e.g., content tagged with \"model\" \"bikini\" \"model photos\" may be filtered out and not sent to the user).  Other suitable filtering methods may also be used.\nAs another example, a user may subscribe to a \"fails\" station.  The user may either select certain interests or the social networking system can identify interests based the user's social graph information.  A user who subscribes to a \"fails\"\nstation may have interests that include, \"crossfit fails,\" \"cat fails,\" \"dog fails,\" \"skateboard fails,\" and the like.  Some of these fails may include horrific and/or graphic accidents, (e.g., a skater rides into the middle of the street and is hit by a\ncar).  If a user is under a certain age, or has otherwise designated or expressed a distaste for such material, the social networking system may filter out content that includes graphic or violent content.  The filtering mechanism may occur by\nidentifying tags that are related to potentially inappropriate material (e.g., content tagged with \"bone crushing\" \"adult only\" \"graphic content,\" or similar tags may be filtered out and not sent to the user).  Other suitable filtering methods may also\nbe used.  Thus, this system may enable the creation of universal profiles that are customizable by the user, either by the affirmatively filtering of content, or via online activity.\nAlthough this disclosure describes and illustrates particular steps of the method of FIG. 1 as occurring in a particular order, this disclosure contemplates any suitable steps of the method of FIG. 1 occurring in any suitable order.  Moreover,\nalthough this disclosure describes and illustrates an example method for sending user-relevant content in a notification subscription service including the particular steps of the method of FIG. 1, this disclosure contemplates any suitable method for\nsending user-relevant content in a notification subscription service, including any suitable steps, which may include all, some, or none of the steps of the method of FIG. 1, where appropriate.  Furthermore, although this disclosure describes and\nillustrates particular components, devices, or systems carrying out particular steps of the method of FIG. 1, this disclosure contemplates any suitable combination of any suitable components, devices, or systems carrying out any suitable steps of the\nmethod of FIG. 1.\nFIG. 2 illustrates an example of a notification-refinement system 200.  The term refinement may be understood to mean the user indicating an interest after selecting a station.  For example, if the user selects a station called \"NBA Big Men,\" he\nmay refine (i.e., indicate a further interest) by either selecting or indicating \"Yao Ming,\" \"Shaquille O'Neal,\" or perhaps even \"Carlos Boozer\" as big men of the NBA that the user is interested in. The system may be comprised of one or more publishers\n220, and each publisher 220 may have at least one station 230.  Each station 230 may publish content in the form of articles, photos, videos, interactive software, or a combination of these formats, as well as other formats.  The publishers may create\ncontent and send the content to the social network; they are not required to attempt to target particular users or to determine when to send alerts.  The social networking system may then direct the content to appropriate users in the further refinement\nstep 250.  The combination of receiving content from a subscriber and further refining that content in some way (either by affirmatively selecting interests from a list of interests, or by making available social-graph information, or by some other way)\nmay create a subscription 260 for the user.  The content may then be sent to the user 280 by any of the delivery channels 270 that are available.  To continue the above example, once the user selects Yao Ming, Shaquille O'Neal, and/or Carlos Boozer, the\nuser 280 may then receive content related to those three NBA big men.  Content is related if it is an exact match, or if it has a tag that is sufficiently related to the user interest.  For example, content tagged with \"Shaquille O'Neal\" is an exact\nmatch to the player.  \"SHAQ\" would also be suitable, since that is a well-known nickname.  Other suitable tags that are associated with Shaquille O'Neal may include \"Laker Greats,\" \"LSU Tigers,\" \"NBA rappers,\" \"huge feet,\" and the like.  Such tags would\nresult in a match between the user's interest (\"Shaquille O'Neal\") and the content with the tags, because the user interest and the tags are sufficiently related.\nRelatedness may be determined in several ways.  First, the social networking system, third party publishers, or another third party may affirmatively identify tags that are related to one another, and send users content that lack the specific\nuser-identified tag, but have one or more related tags.  One way that this may be achieved is by identifying larger categories of tags, under which more specific tags may be categorized.  Tags under which other tags are categorized may be referred to as\n\"super tags,\" and the tags which fall under the super tags may be referred to as sub tags.  For example, a user may identify \"Big Men of the West\" as a user interest.  \"Big Men of the West\" may serve as a super tag under which publishers, the social\nnetworking system, or third parties may categorize sub tags, such as, but not limited to \"Kareem,\" \"Mutumbo\" \"SHAQ,\" etc. This way, if a user selects \"Big Men of the West\" as a user interest, the user may receive content with tags related to all the big\nmen that have played for Western Conference NBA teams, without having to affirmatively select every big man that has played in the Western Conference.  The user may receive content having the \"Kareem\" tag but not the \"Big Men of the West\" tag, because\n\"Kareem\" may be a sub tag that is categorized under the super tag \"Big Men of the West.\" Therefore, the two tags may be related, and the user may receive notification of the content.\nAnother way relatedness may be determined is by natural language processing and/or semantic analysis.  This may operate by identifying words or phrases that commonly appear in content with a particular tag, and then automatically generating new\ntags with the identified words or phrases, and appending those newly generated tags to the content's metadata.  For example, a user may specify the \"SHAQ\" tag as a user interest.  Several articles having the \"SHAQ\" tag may also discuss how big O'Neal's\nfeet are.  The articles may include the phrase \"big feet\" or \"size 22 shoe\" at a frequency that is disproportionate to the rate those phrases appear in a general corpus of content.  Then, the social networking system or another third party may\nautomatically generate \"big feet,\" or \"size 22 shoe\" tags, and associate those tags with the \"SHAQ\" tag.  Because these tags may have been generated in response to content that has the \"SHAQ\" tag, these generated tags may be associated with \"SHAQ.\" Thus,\na user may receive notifications of content with \"size 22 shoe\" or \"big feet\" tags that do not necessarily also have the \"SHAQ\" tag.  These notifications may be further modified and/or refined by the user or by the social networking system, or by a third\nparty, as discussed herein.\nNotification delivery occurs by way of delivery channels 270, which may include, but are not limited to, a text message to the user's personal device, a notification on a social media account, an email, a notification from a third party\napplication, or any other suitable delivery method.\nFIG. 2 demonstrates aggregation of notifications across all stations for a particular tag.  In other words, a user may express interest in a particular tag, and, as a result, may receive content related to that tag across multiple stations and\nmultiple publishers.  As an example, and not by way of limitation, Billy may be a sports fan, and he may especially like basketball.  Billy may select the following stations from ESPN (the publisher): NBA Highlights, NFL Highlights, Game Summaries, and\nSPORTSCENTER Top 10 Plays.  If Game Summaries requires a further refinement of team selection, Billy may select the LA CLIPPERS and the GOLDEN STATE WARRIORS.  The social networking system may also learn that Billy is interested in \"ankle-breaking\ncrossovers,\" either because Billy previously liked or shared a video with that tag, or because he affirmatively selected that interest.  ESPN may publish a highlight reel called \"Best Crossovers of 2015\" and may assign it a tag called \"crossover,\" and\nmay publish it via the NBA Highlights station.  The social networking system may identify a match between the \"crossover\" tag and Billy's interest and send a notification to Billy alerting him of this video.\nContinuing with the example, ESPN may create a different highlight reel called \"Michael Vick's Most Amazing Plays\" and assign two tags to the content: \"jukes\" and \"Michael Vick.\" Because \"jukes\" is related to \"crossover\" (both involve feats of\nathleticism where one athlete moves past another athlete) and Billy likes \"ankle-breaking crossovers,\" it is likely that Billy may enjoy \"Michael Vick's Most Amazing Plays.\" There may be a correlation between Billy's interest and the tag on this content. Thus, the social networking system may send a notification to Billy alerting him of this video.\nHowever, if Billy still has not forgiven Michael Vick for his dog fighting days from 2007, he may edit this notification and opt out of receiving content with the \"Michael Vick\" tag.  Additionally, if the social networking system determines that\ncontent relates to Michael Vick even if it does not contain a tag called \"Michael Vick,\" the social networking system may decline from sending the content to Billy, given his disdain for Michael Vick.  In this way the user may express disinterest in\ncertain tags.  The disinterest may apply across publishers and stations, so that if Billy wants no information regarding Michael Vick, the social networking system will not send Billy anything that is tagged with \"Michael Vick,\" regardless of publisher\nor station.  This is, of course, reverse-able if Billy decides he would like to be notified of information regarding Michael Vick.\nAlternatively, interests, subscriptions, and/or delivery channels may be removed based on a click-through rate of the identified notifications being lower than a pre-determined threshold value.  For example, the notification providing system may\nsend a notification to users both as a text message and as an email.  If the click-through rate of the notification sent via email is below a pre-determined threshold, the notification providing system may remove email as a delivery channel.  The\nforegoing may apply to the click-through rate of an individual user, or alternatively, to the click-through rate of multiple users.  As another example, a user may select or the social networking system may identify \"Taylor Swift\" as a user interest. \nThen, if the user's click-through rate of Taylor Swift related content is below a pre-determined threshold, the social networking system may ask the user if Taylor Swift is really an interest of the user.  Alternatively, the social networking system may\nremove Taylor Swift from the user's interests without asking or notifying the user.  Whether or not a user is notified of an interest removal or is asked if he or she would like to remove an interest may be a feature that the user could opt in or out of\nin a settings menu.  The default method may be to notify users when an interest has been removed, and the user may confirm the removal, or instruct the social networking system to retain the interest.  If the user does not wish to be notified every time\nan interest is added or removed, this may be handled by the user in a settings menu.\nAs another aspect of the invention, the social networking system may index the information it receives from either or both users and/or publishers.  The indexing may include indexing user interests and subscriptions, delivery channels by which\nto deliver notifications, indications of interests and disinterests from a user, among other things.  The information that is indexed may be recorded in a look-up table or any other suitable information recording system.\nFIG. 3 illustrates an interaction diagram 300, depicting the interaction that may occur between a publisher 310, the social networking system 320, and the user 330.  More than one publisher, social networking system, and user may exist in the\nsystem described herein.  In step 3101, a publisher 310 may create content in the ways described above (e.g., write articles, publish photos, or videos, etc.).  The publisher 310 then tags the content in step 3102.  The tags may be generated by the\npublisher, or by a third party.  Alternatively, the social networking system may create tags.  The tags created by the social networking system may be generated by computer algorithm or by human input.  In step 3103, the publisher may send the social\nnetworking system the tagged content.  Alternatively, the publisher 310 may send the social networking system untagged content, and the social networking system or a third party may tag the content after receiving it from the publisher.\nIn step 3301, a user 330 may interact with a social network.  Interaction may occur through any of the means described herein, including, but not limited to, adding a friend, liking a post, reading an article, \"checking in\" at a location,\nattending an event, posting a photo, video, or status update, joining a group, following a person, organization, or other entity, etc.\nIn step 3201, the social networking system 320 may add user activity information to the user's social graph using the methods described herein.  Based on the user's interaction with the social network, the social networking system may identify\ninterests of the user.  As an example on not by way of limitation, the social networking system may learn that a user has clicked on a link to an article about slow roasted meatballs, watched a video about how to make pork and sausage meatballs, and\nstarted following Paula Deen, who may be known for making delicious meatballs.  The social networking system may determine, based on the user's interaction with the social network, that the user is interested in meatballs.  Thus, the social networking\nsystem may identify meatballs as an interest of the user.\nIn step 3202, the social networking system may identify matches between a user's interests and tagged content received from one or more publishers.  Identifying matches may be accomplished by the methods described above, or by any suitable\nmethod.  Upon identifying a match between tagged content and a user interest, the social networking system, in step 3203, may notify the user of the matched content by any of the delivery methods described herein.  In step 3302, the user may then\ninteract with the notification(s) in a number of ways, including but not limited to \"clicking it\" (i.e., selecting it and opening the associated content), ignoring it, liking it, disliking it (or otherwise expressing distaste for the notification and/or\nassociated content), sharing it, or saving it for later (i.e., \"bookmarking\" it).  The social networking system may gather this information about how the user interacts with the sent notifications to update the user's interests, in step 3204.  The social\nnetworking system may then repeat steps 3202 and 3203 several times and thus continuously update the user's interests.  This repeated updating of user interests allows the social networking system to notify the user of increasingly relevant content\n(i.e., content that the user is most likely to read, like, and/or share).\nIn an embodiment, one aspect of the invention relates to bookmarking notifications.  The social networking system may notify the user of matched content.  These notifications may appear in a notifications center on the user's computing device,\non the computing device's locked screen, or in a different application on the computing device.  The user may see the notification, but may wish to view the notification and the associated content at a later time.  The user may indicate she wishes to\nview the notification and the associated content at a later time by tapping a \"save for later\" button, swiping, or dragging her finger across the display screen in an appropriate manner to accomplish her purpose of saving the notification for later. \nAlternatively, the user may make any gesture that will cause the notification to be saved for later consumption.  Upon being designating as \"saved for later,\" the notification may be placed on a bookmarks page or other suitable list/queue. \nAlternatively, the user may be able to request the notification be re-sent at a later time, or when the user enters a particular geographic location.  As an example of the above functionality, and not by way of limitation, a user may receive a\nnotification of a video about how to perform a magic card trick while she is on the subway headed to work.  The subway may be too loud for her to watch and listen to the video.  If that is the case, the user may \"bookmark\" the notification.  The\nbookmarked notification may then be sent to a bookmarks page, list, or queue, to be consumed when the user is at her desk at work.  Alternatively, the bookmarked notification may be re-sent by the social networking system at a later time.  The time at\nwhich the notification may be re-sent may be determined in a number of different ways, including, but not limited to, determining a specific time, determining a specific geographic location to resend the notification, determining when the user is\ninteracting with the device to resend, determining when the user is interacting with one or more particular applications on the device to resend the notification.  As an example and not by way of notification, the user may designate that the notification\nbe resent when she enters her office building, or when she is scrolling through her newsfeed on any one of a number of different applications, or when she next unlocks her device, or at another suitable time or location.\nFIG. 4A illustrates an example network environment 400 associated with a social-networking system.  Network environment 400 includes a client system 430, a social-networking system 460, and a third-party system 470 connected to each other by a\nnetwork 410.  Although FIG. 4A illustrates a particular arrangement of client system 430, social-networking system 460, third-party system 470, and network 410, this disclosure contemplates any suitable arrangement of client system 430, social-networking\nsystem 460, third-party system 470, and network 410.  As an example and not by way of limitation, two or more of client system 430, social-networking system 460, and third-party system 470 may be connected to each other directly, bypassing network 410. \nAs another example, two or more of client system 430, social-networking system 460, and third-party system 470 may be physically or logically co-located with each other in whole or in part.  Moreover, although FIG. 4A illustrates a particular number of\nclient systems 430, social-networking systems 460, third-party systems 470, and networks 410, this disclosure contemplates any suitable number of client systems 430, social-networking systems 460, third-party systems 470, and networks 410.  As an example\nand not by way of limitation, network environment 400 may include multiple client system 430, social-networking systems 460, third-party systems 470, and networks 410.\nThis disclosure contemplates any suitable network 410.  As an example and not by way of limitation, one or more portions of network 410 may include an ad hoc network, an intranet, an extranet, a virtual private network (VPN), a local area\nnetwork (LAN), a wireless LAN (WLAN), a wide area network (WAN), a wireless WAN (WWAN), a metropolitan area network (MAN), a portion of the Internet, a portion of the Public Switched Telephone Network (PSTN), a cellular telephone network, or a\ncombination of two or more of these.  Network 410 may include one or more networks 410.\nLinks 450 may connect client system 430, social-networking system 460, and third-party system 470 to communication network 410 or to each other.  This disclosure contemplates any suitable links 450.  In particular embodiments, one or more links\n450 include one or more wireline (such as for example Digital Subscriber Line (DSL) or Data Over Cable Service Interface Specification (DOCSIS)), wireless (such as for example Wi-Fi or Worldwide Interoperability for Microwave Access (WiMAX)), or optical\n(such as for example Synchronous Optical Network (SONET) or Synchronous Digital Hierarchy (SDH)) links.  In particular embodiments, one or more links 450 each include an ad hoc network, an intranet, an extranet, a VPN, a LAN, a WLAN, a WAN, a WWAN, a\nMAN, a portion of the Internet, a portion of the PSTN, a cellular technology-based network, a satellite communications technology-based network, another link 450, or a combination of two or more such links 450.  Links 450 need not necessarily be the same\nthroughout network environment 400.  One or more first links 450 may differ in one or more respects from one or more second links 450.\nIn particular embodiments, client system 430 may be an electronic device including hardware, software, or embedded logic components or a combination of two or more such components and capable of carrying out the appropriate functionalities\nimplemented or supported by client system 430.  As an example and not by way of limitation, a client system 430 may include a computer system such as a desktop computer, notebook or laptop computer, netbook, a tablet computer, e-book reader, GPS device,\ncamera, personal digital assistant (PDA), handheld electronic device, cellular telephone, smartphone, other suitable electronic device, or any suitable combination thereof.  This disclosure contemplates any suitable client systems 430.  A client system\n430 may enable a network user at client system 430 to access network 410.  A client system 430 may enable its user to communicate with other users at other client systems 430.\nIn particular embodiments, client system 430 may include a web browser 432, such as MICROSOFT INTERNET EXPLORER, GOOGLE CHROME or MOZILLA FIREFOX, and may have one or more add-ons, plug-ins, or other extensions, such as TOOLBAR or YAHOO TOOLBAR. A user at client system 430 may enter a Uniform Resource Locator (URL) or other address directing the web browser 432 to a particular server (such as server 462, or a server associated with a third-party system 470), and the web browser 432 may generate\na Hyper Text Transfer Protocol (HTTP) request and communicate the HTTP request to server.  The server may accept the HTTP request and communicate to client system 430 one or more Hyper Text Markup Language (HTML) files responsive to the HTTP request. \nClient system 430 may render a webpage based on the HTML files from the server for presentation to the user.  This disclosure contemplates any suitable webpage files.  As an example and not by way of limitation, webpages may render from HTML files,\nExtensible Hyper Text Markup Language (XHTML) files, or Extensible Markup Language (XML) files, according to particular needs.  Such pages may also execute scripts such as, for example and without limitation, those written in JAVASCRIPT, JAVA, MICROSOFT\nSILVERLIGHT, combinations of markup language and scripts such as AJAX (Asynchronous JAVASCRIPT and XML), and the like.  Herein, reference to a webpage encompasses one or more corresponding webpage files (which a browser may use to render the webpage) and\nvice versa, where appropriate.\nIn particular embodiments, social-networking system 460 may be a network-addressable computing system that can host an online social network.  Social-networking system 460 may generate, store, receive, and send social-networking data, such as,\nfor example, user-profile data, concept-profile data, social-graph information, or other suitable data related to the online social network.  Social-networking system 460 may be accessed by the other components of network environment 400 either directly\nor via network 410.  As an example and not by way of limitation, client system 430 may access social-networking system 460 using a web browser 432, or a native application associated with social-networking system 460 (e.g., a mobile social-networking\napplication, a messaging application, another suitable application, or any combination thereof) either directly or via network 410.  In particular embodiments, social-networking system 460 may include one or more servers 462.  Each server 462 may be a\nunitary server or a distributed server spanning multiple computers or multiple datacenters.  Servers 462 may be of various types, such as, for example and without limitation, web server, news server, mail server, message server, advertising server, file\nserver, application server, exchange server, database server, proxy server, another server suitable for performing functions or processes described herein, or any combination thereof.  In particular embodiments, each server 462 may include hardware,\nsoftware, or embedded logic components or a combination of two or more such components for carrying out the appropriate functionalities implemented or supported by server 462.  In particular embodiments, social-networking system 460 may include one or\nmore data stores 464.  Data stores 464 may be used to store various types of information.  In particular embodiments, the information stored in data stores 464 may be organized according to specific data structures.  In particular embodiments, each data\nstore 464 may be a relational, columnar, correlation, or other suitable database.  Although this disclosure describes or illustrates particular types of databases, this disclosure contemplates any suitable types of databases.  Particular embodiments may\nprovide interfaces that enable a client system 430, a social-networking system 460, or a third-party system 470 to manage, retrieve, modify, add, or delete, the information stored in data store 464.\nIn particular embodiments, social-networking system 460 may store one or more social graphs in one or more data stores 464.  In particular embodiments, a social graph may include multiple nodes--which may include multiple user nodes (each\ncorresponding to a particular user) or multiple concept nodes (each corresponding to a particular concept)--and multiple edges connecting the nodes.  Social-networking system 460 may provide users of the online social networking system the ability to\ncommunicate and interact with other users.  In particular embodiments, users may join the online social networking system via social-networking system 460 and then add connections (e.g., relationships) to a number of other users of social-networking\nsystem 460 to whom they want to be connected.  Herein, the term \"friend\" may refer to any other user of social-networking system 460 with whom a user has formed a connection, association, or relationship via social-networking system 460.\nIn particular embodiments, social-networking system 460 may provide users with the ability to take actions on various types of items or objects, supported by social-networking system 460.  As an example and not by way of limitation, the items\nand objects may include groups or social networks to which users of social-networking system 460 may belong, events or calendar entries in which a user might be interested, computer-based applications that a user may use, transactions that allow users to\nbuy or sell items via the service, interactions with advertisements that a user may perform, or other suitable items or objects.  A user may interact with anything that is capable of being represented in social-networking system 460 or by an external\nsystem of third-party system 470, which is separate from social-networking system 460 and coupled to social-networking system 460 via a network 410.\nIn particular embodiments, social-networking system 460 may be capable of linking a variety of entities.  As an example and not by way of limitation, social-networking system 460 may enable users to interact with each other as well as receive\ncontent from third-party systems 470 or other entities, or to allow users to interact with these entities through an application programming interfaces (API) or other delivery channels.\nIn particular embodiments, a third-party system 470 may include one or more types of servers, one or more data stores, one or more interfaces, including but not limited to APIs, one or more web services, one or more content sources, one or more\nnetworks, or any other suitable components, e.g., that servers may communicate with.  A third-party system 470 may be operated by a different entity from an entity operating social-networking system 460.  In particular embodiments, however,\nsocial-networking system 460 and third-party systems 470 may operate in conjunction with each other to provide social-networking services to users of social-networking system 460 or third-party systems 470.  In this sense, social-networking system 460\nmay provide a platform, or backbone, which other systems, such as third-party systems 470, may use to provide social-networking services and functionality to users across the Internet.\nIn particular embodiments, a third-party system 470 may include a third-party content object provider.  A third-party content object provider may include one or more sources of content objects, which may be communicated to a client system 430. \nAs an example and not by way of limitation, content objects may include information regarding things or activities of interest to the user, such as, for example, movie show times, movie reviews, restaurant reviews, restaurant menus, product information\nand reviews, or other suitable information.  As another example and not by way of limitation, content objects may include incentive content objects, such as coupons, discount tickets, gift certificates, or other suitable incentive objects.\nIn particular embodiments, social-networking system 460 also includes user-generated content objects, which may enhance a user's interactions with social-networking system 460.  User-generated content may include anything a user can add, upload,\nsend, or \"post\" to social-networking system 460.  As an example and not by way of limitation, a user communicates posts to social-networking system 460 from a client system 430.  Posts may include data such as status updates or other textual data,\nlocation information, photos, videos, links, music or other similar data or media.  Content may also be added to social-networking system 460 by a third-party through a \"delivery channel,\" such as a newsfeed or stream.\nIn particular embodiments, social-networking system 460 may include a variety of servers, sub-systems, programs, modules, logs, and data stores.  In particular embodiments, social-networking system 460 may include one or more of the following: a\nweb server, action logger, API-request server, relevance-and-ranking engine, content-object classifier, notification controller, action log, third-party-content-object-exposure log, inference module, authorization/privacy server, search module,\nadvertisement-targeting module, user-interface module, user-profile store, connection store, third-party content store, or location store.  Social-networking system 460 may also include suitable components such as network interfaces, security mechanisms,\nload balancers, failover servers, management-and-network-operations consoles, other suitable components, or any suitable combination thereof.  In particular embodiments, social-networking system 460 may include one or more user-profile stores for storing\nuser profiles.  A user profile may include, for example, biographic information, demographic information, behavioral information, social information, or other types of descriptive information, such as work experience, educational history, hobbies or\npreferences, interests, affinities, or location.  Interest information may include interests related to one or more categories.  Categories may be general or specific.  As an example and not by way of limitation, if a user \"likes\" an article about a\nbrand of shoes the category may be the brand, or the general category of \"shoes\" or \"clothing.\" A connection store may be used for storing connection information about users.  The connection information may indicate users who have similar or common work\nexperience, group memberships, hobbies, educational history, or are in any way related or share common attributes.  The connection information may also include user-defined connections between different users and content (both internal and external).  A\nweb server may be used for linking social-networking system 460 to one or more client systems 430 or one or more third-party system 470 via network 410.  The web server may include a mail server or other messaging functionality for receiving and routing\nmessages between social-networking system 460 and one or more client systems 430.  An API-request server may allow a third-party system 470 to access information from social-networking system 460 by calling one or more APIs.  An action logger may be used\nto receive communications from a web server about a user's actions on or off social-networking system 460.  In conjunction with the action log, a third-party-content-object log may be maintained of user exposures to third-party-content objects.  A\nnotification controller may provide information regarding content objects to a client system 430.  Information may be pushed to a client system 430 as notifications, or information may be pulled from client system 430 responsive to a request received\nfrom client system 430.  Authorization servers may be used to enforce one or more privacy settings of the users of social-networking system 460.  A privacy setting of a user determines how particular information associated with a user can be shared.  The\nauthorization server may allow users to opt in to or opt out of having their actions logged by social-networking system 460 or shared with other systems (e.g., third-party system 470), such as, for example, by setting appropriate privacy settings. \nThird-party-content-object stores may be used to store content objects received from third parties, such as a third-party system 470.  Location stores may be used for storing location information received from client systems 430 associated with users. \nAdvertisement-pricing modules may combine social information, the current time, location information, or other suitable information to provide relevant advertisements, in the form of notifications, to a user.\nFIG. 4B illustrates an example architecture for delivering notifications to a user (Alice Liddell).  In one example embodiment described herein, elements of the notification-providing system may be implemented as part of a social-networking\nsystem, and the notification-providing system may handle delivery of notifications generated by third-party systems as well as by the social-networking system itself.  In particular embodiments, elements of the notification-providing system may be\nimplemented as part of a third-party system.\nAs shown in FIG. 4B, notifications may be delivered by way of a number of different delivery channels 440.  As discussed above, a delivery channel 440 may comprise one or more uniquely-identified endpoints 442 and one or more communication media\n444.  As shown in FIG. 4B, notifications may be delivered by one or more communication media (e.g., SMS, MMS, email, particular application, voice, newsfeed, flag) to one or more unique endpoints (e.g., a telephone number, an email address, a particular\nclient device as specified by a unique device identifier, a particular user account for the particular application or for the client device).  In some embodiments, a particular communication media may be able to deliver a notification to more than one\nendpoint--for example, a third-party application such as SNAPCHAT (communication media) may be installed on the user's smartphone client device 430A (first endpoint) and also on the user's laptop 430B (second endpoint).  Communication media may be a\npush-type medium, such as SMS or email, or it may be a pull-type medium, such as newsfeed.\nIn particular embodiments, the notification-providing system may select different delivery channels for notifications based on the user's available delivery channels and the status thereof.  As discussed above, the information about the user's\navailable delivery channels may be retrieved from the registration data store (e.g., information to enable the notification-providing system to deliver the notification to a SNAPCHAT application).  The notification-providing system may also select\ndifferent delivery channels for notifications based on the user's current delivery context, which may include device status.  For example, if Alice's smartphone is currently placed in Silent mode, and she just checked in at a movie theater with her\nfriends, then delivery of any notifications may be delayed until movement detected by the phone indicates that she is exiting the theater.  The notification-providing system may also choose to \"escalate\" a notification from a lower-ranked delivery\nchannel (e.g., newsfeed) to a higher-ranked delivery channel (e.g., SMS) when re-delivering a notification, in order to increase the likelihood that the receiving user will interact with the notification.\nFIG. 5 illustrates example social graph 500.  In particular embodiments, social-networking system 460 may store one or more social graphs 500 in one or more data stores.  In particular embodiments, social graph 500 may include multiple\nnodes--which may include multiple user nodes 502 or multiple concept nodes 504--and multiple edges 506 connecting the nodes.  Example social graph 500 illustrated in FIG. 5 is shown, for didactic purposes, in a two-dimensional visual map representation. \nIn particular embodiments, a social-networking system 460, client system 430, or third-party system 470 may access social graph 500 and related social-graph information for suitable applications.  The nodes and edges of social graph 500 may be stored as\ndata objects, for example, in a data store (such as a social-graph database).  Such a data store may include one or more searchable or queryable indexes of nodes or edges of social graph 500.\nIn particular embodiments, a user node 502 may correspond to a user of social-networking system 460.  As an example and not by way of limitation, a user may be an individual (human user), an entity (e.g., an enterprise, business, or third-party\napplication), or a group (e.g., of individuals or entities) that interacts or communicates with or over social-networking system 460.  In particular embodiments, when a user registers for an account with social-networking system 460, social-networking\nsystem 460 may create a user node 502 corresponding to the user, and store the user node 502 in one or more data stores.  Users and user nodes 502 described herein may, where appropriate, refer to registered users and user nodes 502 associated with\nregistered users.  In addition or as an alternative, users and user nodes 502 described herein may, where appropriate, refer to users that have not registered with social-networking system 460.  In particular embodiments, a user node 502 may be\nassociated with information provided by a user or information gathered by various systems, including social-networking system 460.  As an example and not by way of limitation, a user may provide his or her name, profile picture, contact information,\nbirth date, sex, marital status, family status, employment, education background, preferences, interests, or other demographic information.  In particular embodiments, a user node 502 may be associated with one or more data objects corresponding to\ninformation associated with a user.  In particular embodiments, a user node 502 may correspond to one or more webpages.\nIn particular embodiments, a concept node 504 may correspond to a concept.  As an example and not by way of limitation, a concept may correspond to a place (such as, for example, a movie theater, restaurant, landmark, or city); a website (such\nas, for example, a website associated with social-network system 460 or a third-party website associated with a web-application server); an entity (such as, for example, a person, business, group, sports team, or celebrity); a resource (such as, for\nexample, an audio file, video file, digital photo, text file, structured document, or application) which may be located within social-networking system 460 or on an external server, such as a web-application server; real or intellectual property (such\nas, for example, a sculpture, painting, movie, game, song, idea, photograph, or written work); a game; an activity; an idea or theory; another suitable concept; or two or more such concepts.  A concept node 504 may be associated with information of a\nconcept provided by a user or information gathered by various systems, including social-networking system 460.  As an example and not by way of limitation, information of a concept may include a name or a title; one or more images (e.g., an image of the\ncover page of a book); a location (e.g., an address or a geographical location); a website (which may be associated with a URL); contact information (e.g., a phone number or an email address); other suitable concept information; or any suitable\ncombination of such information.  In particular embodiments, a concept node 504 may be associated with one or more data objects corresponding to information associated with concept node 504.  In particular embodiments, a concept node 504 may correspond\nto one or more webpages.\nIn particular embodiments, a node in social graph 500 may represent or be represented by a webpage (which may be referred to as a \"profile page\").  Profile pages may be hosted by or accessible to social-networking system 460.  Profile pages may\nalso be hosted on third-party websites associated with a third-party server 470.  As an example and not by way of limitation, a profile page corresponding to a particular external webpage may be the particular external webpage and the profile page may\ncorrespond to a particular concept node 504.  Profile pages may be viewable by all or a selected subset of other users.  As an example and not by way of limitation, a user node 502 may have a corresponding user-profile page in which the corresponding\nuser may add content, make declarations, or otherwise express himself or herself.  As another example and not by way of limitation, a concept node 504 may have a corresponding concept-profile page in which one or more users may add content, make\ndeclarations, or express themselves, particularly in relation to the concept corresponding to concept node 504.\nIn particular embodiments, a concept node 504 may represent a third-party webpage or resource hosted by a third-party system 470.  The third-party webpage or resource may include, among other elements, content, a selectable or other icon, or\nother inter-actable object (which may be implemented, for example, in JavaScript, AJAX, or PHP codes) representing an action or activity.  As an example and not by way of limitation, a third-party webpage may include a selectable icon such as \"like,\"\n\"check-in,\" \"eat,\" \"recommend,\" or another suitable action or activity.  A user viewing the third-party webpage may perform an action by selecting one of the icons (e.g., \"check-in\"), causing a client system 430 to send to social-networking system 460 a\nmessage indicating the user's action.  In response to the message, social-networking system 460 may create an edge (e.g., a check-in-type edge) between a user node 502 corresponding to the user and a concept node 504 corresponding to the third-party\nwebpage or resource and store edge 506 in one or more data stores.\nIn particular embodiments, a pair of nodes in social graph 500 may be connected to each other by one or more edges 506.  An edge 506 connecting a pair of nodes may represent a relationship between the pair of nodes.  In particular embodiments,\nan edge 506 may include or represent one or more data objects or attributes corresponding to the relationship between a pair of nodes.  As an example and not by way of limitation, a first user may indicate that a second user is a \"friend\" of the first\nuser.  In response to this indication, social-networking system 460 may send a \"friend request\" to the second user.  If the second user confirms the \"friend request,\" social-networking system 460 may create an edge 506 connecting the first user's user\nnode 502 to the second user's user node 502 in social graph 500 and store edge 506 as social-graph information in one or more of data stores 164.  In the example of FIG. 5, social graph 500 includes an edge 506 indicating a friend relation between user\nnodes 502 of user \"A\" and user \"B\" and an edge indicating a friend relation between user nodes 502 of user \"C\" and user \"B.\" Although this disclosure describes or illustrates particular edges 506 with particular attributes connecting particular user\nnodes 502, this disclosure contemplates any suitable edges 506 with any suitable attributes connecting user nodes 502.  As an example and not by way of limitation, an edge 506 may represent a friendship, family relationship, business or employment\nrelationship, fan relationship (including, e.g., liking, etc.), follower relationship, visitor relationship (including, e.g., accessing, viewing, checking-in, sharing, etc.), subscriber relationship, superior/subordinate relationship, reciprocal\nrelationship, non-reciprocal relationship, another suitable type of relationship, or two or more such relationships.  Moreover, although this disclosure generally describes nodes as being connected, this disclosure also describes users or concepts as\nbeing connected.  Herein, references to users or concepts being connected may, where appropriate, refer to the nodes corresponding to those users or concepts being connected in social graph 500 by one or more edges 506.\nIn particular embodiments, an edge 506 between a user node 502 and a concept node 504 may represent a particular action or activity performed by a user associated with user node 502 toward a concept associated with a concept node 504.  As an\nexample and not by way of limitation, as illustrated in FIG. 5, a user may \"like,\" \"attended,\" \"played,\" \"listened,\" \"cooked,\" \"worked at,\" or \"watched\" a concept, each of which may correspond to a edge type or subtype.  A concept-profile page\ncorresponding to a concept node 504 may include, for example, a selectable \"check in\" icon (such as, for example, a clickable \"check in\" icon) or a selectable \"add to favorites\" icon.  Similarly, after a user clicks these icons, social-networking system\n460 may create a \"favorite\" edge or a \"check in\" edge in response to a user's action corresponding to a respective action.  As another example and not by way of limitation, a user (user \"C\") may listen to a particular song (\"Imagine\") using a particular\napplication (SPOTIFY, which is an online music application).  In this case, social-networking system 460 may create a \"listened\" edge 506 and a \"used\" edge (as illustrated in FIG. 5) between user nodes 502 corresponding to the user and concept nodes 504\ncorresponding to the song and application to indicate that the user listened to the song and used the application.  Moreover, social-networking system 460 may create a \"played\" edge 506 (as illustrated in FIG. 5) between concept nodes 504 corresponding\nto the song and the application to indicate that the particular song was played by the particular application.  In this case, \"played\" edge 506 corresponds to an action performed by an external application (SPOTIFY) on an external audio file (the song\n\"Imagine\").  Although this disclosure describes particular edges 506 with particular attributes connecting user nodes 502 and concept nodes 504, this disclosure contemplates any suitable edges 506 with any suitable attributes connecting user nodes 502\nand concept nodes 504.  Moreover, although this disclosure describes edges between a user node 502 and a concept node 504 representing a single relationship, this disclosure contemplates edges between a user node 502 and a concept node 504 representing\none or more relationships.  As an example and not by way of limitation, an edge 506 may represent both that a user likes and has used at a particular concept.  Alternatively, another edge 506 may represent each type of relationship (or multiples of a\nsingle relationship) between a user node 502 and a concept node 504 (as illustrated in FIG. 5 between user node 502 for user \"E\" and concept node 504 for \"SPOTIFY\").\nIn particular embodiments, social-networking system 460 may create an edge 506 between a user node 502 and a concept node 504 in social graph 500.  As an example and not by way of limitation, a user viewing a concept-profile page (such as, for\nexample, by using a web browser or a special-purpose application hosted by the user's client system 430) may indicate that he or she likes the concept represented by the concept node 504 by clicking or selecting a \"Like\" icon, which may cause the user's\nclient system 430 to send to social-networking system 460 a message indicating the user's liking of the concept associated with the concept-profile page.  In response to the message, social-networking system 460 may create an edge 506 between user node\n502 associated with the user and concept node 504, as illustrated by \"like\" edge 506 between the user and concept node 504.  In particular embodiments, social-networking system 460 may store an edge 506 in one or more data stores.  In particular\nembodiments, an edge 506 may be automatically formed by social-networking system 460 in response to a particular user action.  As an example and not by way of limitation, if a first user uploads a picture, watches a movie, or listens to a song, an edge\n506 may be formed between user node 502 corresponding to the first user and concept nodes 504 corresponding to those concepts.  Although this disclosure describes forming particular edges 506 in particular manners, this disclosure contemplates forming\nany suitable edges 506 in any suitable manner.\nIn particular embodiments, an advertisement may be text (which may be HTML-linked), one or more images (which may be HTML-linked), one or more videos, audio, one or more ADOBE FLASH files, a suitable combination of these, or any other suitable\nadvertisement in any suitable digital format presented on one or more web pages, in one or more e-mails, or in connection with search results requested by a user.  In addition or as an alternative, an advertisement may be one or more sponsored stories\n(e.g., a news-feed or ticker item on social-networking system 460).  A sponsored story may be a social action by a user (such as \"liking\" a page, \"liking\" or commenting on a post on a page, RSVPing to an event associated with a page, voting on a question\nposted on a page, checking in to a place, using an application or playing a game, or \"liking\" or sharing a website) that an advertiser promotes, for example, by having the social action presented within a pre-determined area of a profile page of a user\nor other page, presented with additional information associated with the advertiser, bumped up or otherwise highlighted within news feeds or tickers of other users, or otherwise promoted.  The advertiser may pay to have the social action promoted.  The\nsocial action may be promoted within or on social-networking system 460.  In addition or as an alternative, the social action may be promoted outside or off of social-networking system 460, where appropriate.  In particular embodiments, a page may be an\non-line presence (such as a webpage or website within or outside of social-networking system 460) of a business, organization, or brand facilitating its sharing of stories and connecting with people.  A page may be customized, for example, by adding\napplications, posting stories, or hosting events.\nA sponsored story may be generated from stories in users' news feeds and promoted to specific areas within displays of users' web browsers when viewing a web page associated with social-networking system 460.  Sponsored stories are more likely\nto be viewed by users, at least in part because sponsored stories generally involve interactions or suggestions by the users' friends, fan pages, or other connections.  In connection with sponsored stories, particular embodiments may utilize one or more\nsystems, components, elements, functions, methods, operations, or steps disclosed in U.S.  patent application Ser.  No. 13/327,557, entitled \"Sponsored Stories Unit Creation from Organic Activity Stream\" and filed 15 Dec.  2011, U.S.  Patent Application\nPublication No. 2012/0203831, entitled \"Sponsored Stories Unit Creation from Organic Activity Stream\" and filed 3 Feb.  2012 as U.S.  patent application Ser.  No. 13/020,745, or U.S.  Patent Application Publication No. 2012/0233009, entitled \"Endorsement\nSubscriptions for Sponsored Stories\" and filed 9 Mar.  2011 as U.S.  patent application Ser.  No. 13/044,506, which are all incorporated herein by reference as an example and not by way of limitation.  In particular embodiments, sponsored stories may\nutilize computer-vision algorithms to detect products in uploaded images or photos lacking an explicit connection to an advertiser as disclosed in U.S.  patent application Ser.  No. 13/212,356, entitled \"Computer-Vision Content Detection for Sponsored\nStories\" and filed 18 Aug.  2011, which is incorporated herein by reference as an example and not by way of limitation.\nAs described above, an advertisement may be text (which may be HTML-linked), one or more images (which may be HTML-linked), one or more videos, audio, one or more ADOBE FLASH files, a suitable combination of these, or any other suitable\nadvertisement in any suitable digital format.  In particular embodiments, an advertisement may be requested for display within third-party webpages, social-networking-system webpages, or other pages.  An advertisement may be displayed in a dedicated\nportion of a page, such as in a banner area at the top of the page, in a column at the side of the page, in a GUI of the page, in a pop-up window, over the top of content of the page, or elsewhere with respect to the page.  In addition or as an\nalternative, an advertisement may be displayed within an application or within a game.  An advertisement may be displayed within dedicated pages, requiring the user to interact with or watch the advertisement before the user may access a page, utilize an\napplication, or play a game.  The user may, for example view the advertisement through a web browser.\nA user may interact with an advertisement in any suitable manner.  The user may click or otherwise select the advertisement, and the advertisement may direct the user (or a browser or other application being used by the user) to a page\nassociated with the advertisement.  At the page associated with the advertisement, the user may take additional actions, such as purchasing a product or service associated with the advertisement, receiving information associated with the advertisement,\nor subscribing to a newsletter associated with the advertisement.  An advertisement with audio or video may be played by selecting a component of the advertisement (like a \"play button\").  In particular embodiments, an advertisement may include one or\nmore games, which a user or other application may play in connection with the advertisement.  An advertisement may include functionality for responding to a poll or question in the advertisement.\nAn advertisement may include social-networking-system functionality that a user may interact with.  For example, an advertisement may enable a user to \"like\" or otherwise endorse the advertisement by selecting an icon or link associated with\nendorsement.  Similarly, a user may share the advertisement with another user (e.g., through social-networking system 460) or RSVP (e.g., through social-networking system 460) to an event associated with the advertisement.  In addition or as an\nalternative, an advertisement may include social-networking-system content directed to the user.  For example, an advertisement may display information about a friend of the user within social-networking system 460 who has taken an action associated with\nthe subject matter of the advertisement.\nSocial-networking-system functionality or content may be associated with an advertisement in any suitable manner.  For example, an advertising system (which may include hardware, software, or both for receiving bids for advertisements and\nselecting advertisements in response) may retrieve social-networking functionality or content from social-networking system 460 and incorporate the retrieved social-networking functionality or content into the advertisement before serving the\nadvertisement to a user.  Examples of selecting and providing social-networking-system functionality or content with an advertisement are disclosed in U.S.  Patent Application Publication No. 2012/0084460, entitled \"Providing Social Endorsements with\nOnline Advertising\" and filed 5 Oct.  2010 as U.S.  patent application Ser.  No. 12/898,662, and in U.S.  Patent Application Publication No. 2012/0232998, entitled \"Selecting Social Endorsement Information for an Advertisement for Display to a Viewing\nUser\" and filed 8 Mar.  2011 as U.S.  patent application Ser.  No. 13/043,424, which are both incorporated herein by reference as examples only and not by way of limitation.  Interacting with an advertisement that is associated with\nsocial-networking-system functionality or content may cause information about the interaction to be displayed in a profile page of the user in social-networking-system 460.\nParticular embodiments may facilitate the delivery of advertisements to users that are more likely to find the advertisements more relevant or useful.  For example, an advertiser may realize higher conversion rates (and therefore higher return\non investment (ROI) from advertising) by identifying and targeting users that are more likely to find its advertisements more relevant or useful.  The advertiser may use user-profile information in social-networking system 460 to identify those users. \nIn addition or as an alternative, social-networking system 460 may use user-profile information in social-networking system 460 to identify those users for the advertiser.  As examples and not by way of limitation, particular embodiments may target users\nwith the following: invitations or suggestions of events; suggestions regarding coupons, deals, or wish-list items; suggestions regarding friends' life events; suggestions regarding groups; advertisements; or social advertisements.  Such targeting may\noccur, where appropriate, on or within social-networking system 460, off or outside of social-networking system 460, or on mobile computing devices of users.  When on or within social-networking system 460, such targeting may be directed to users' news\nfeeds, search results, e-mail or other in-boxes, or notifications channels or may appear in particular area of web pages of social-networking system 460, such as a right-hand side of a web page in a concierge or grouper area (which may group along a\nright-hand rail advertisements associated with the same concept, node, or object) or a network-ego area (which may be based on what a user is viewing on the web page and a current news feed of the user).  When off or outside of social-networking system\n460, such targeting may be provided through a third-party website, e.g., involving an ad exchange or a social plug-in. When on a mobile computing device of a user, such targeting may be provided through push notifications to the mobile computing device.\nTargeting criteria used to identify and target users may include explicit, stated user interests on social-networking system 460 or explicit connections of a user to a node, object, entity, brand, or page on social-networking system 460.  In\naddition or as an alternative, such targeting criteria may include implicit or inferred user interests or connections (which may include analyzing a user's history, demographic, social or other activities, friends' social or other activities,\nsubscriptions, or any of the preceding of other users similar to the user (based, e.g., on shared interests, connections, or events)).  Particular embodiments may utilize platform targeting, which may involve platform and \"like\" impression data;\ncontextual signals (e.g., \"Who is viewing now or has viewed recently the page for COCA-COLA?\"); light-weight connections (e.g., \"check-ins\"); connection lookalikes; fans; extracted keywords; EMU advertising; inferential advertising; coefficients,\naffinities, or other social-graph information; friends-of-friends connections; pinning or boosting; deals; polls; household income, social clusters or groups; products detected in images or other media; social- or open-graph edge types; geo-prediction;\nviews of profile or pages; status updates or other user posts (analysis of which may involve natural-language processing or keyword extraction); events information; or collaborative filtering.  Identifying and targeting users may also include privacy\nsettings (such as user opt-outs), data hashing, or data anonymization, as appropriate.\nTo target users with advertisements, particular embodiments may utilize one or more systems, components, elements, functions, methods, operations, or steps disclosed in the following, which are all incorporated herein by reference as examples\nand not by way of limitation: U.S.  Patent Application Publication No. 5009/0119167, entitled \"Social Advertisements and Other Informational Messages on a Social Networking Website and Advertising Model for Same\" and filed 18 August 5008 as U.S.  patent\napplication Ser.  No. 12/193,702; U.S.  Patent Application Publication No. 5009/0070219, entitled \"Targeting Advertisements in a Social Network\" and filed 20 August 5008 as U.S.  patent application Ser.  No. 12/195,321; U.S.  Patent Application\nPublication No. 2012/0158501, entitled \"Targeting Social Advertising to Friends of Users Who Have Interacted With an Object Associated with the Advertising\" and filed 15 Dec.  2010 as U.S.  patent application Ser.  No. 12/968,786; or U.S.  Patent\nApplication Publication No. 2012/0166532, entitled \"Contextually Relevant Affinity Prediction in a Social-Networking System\" and filed 23 Dec.  2010 as U.S.  patent application Ser.  No. 12/978,265.\nAn advertisement may be presented or otherwise delivered using plug-ins for web browsers or other applications, iframe elements, news feeds, tickers, notifications (which may include, for example, e-mail, Short Message Service (SMS) messages, or\nnotifications), or other means.  An advertisement may be presented or otherwise delivered to a user on a mobile or other computing device of the user.  In connection with delivering advertisements, particular embodiments may utilize one or more systems,\ncomponents, elements, functions, methods, operations, or steps disclosed in the following, which are all incorporated herein by reference as examples and not by way of limitation: U.S.  Patent Application Publication No. 2012/0159635, entitled \"Comment\nPlug-In for Third-Party System\" and filed 15 Dec.  2010 as U.S.  patent application Ser.  No. 12/969,368; U.S.  Patent Application Publication No. 2012/0158753, entitled \"Comment Ordering System\" and filed 15 Dec.  2010 as U.S.  patent application Ser. \nNo. 12/969,408; U.S.  Pat.  No. 7,669,123, entitled \"Dynamically Providing a News Feed About a User of a Social Network\" and filed 11 August 5006 as U.S.  patent application Ser.  No. 11/503,242; U.S.  Pat.  No. 8,402,094, entitled \"Providing a Newsfeed\nBased on User Affinity for Entities and Monitored Actions in a Social Network Environment\" and filed 11 August 5006 as U.S.  patent application Ser.  No. 11/503,093; U.S.  Patent Application Publication No. 2012/0072428, entitled \"Action Clustering for\nNews Feeds\" and filed 16 Sep. 2010 as U.S.  patent application Ser.  No. 12/884,010; U.S.  Patent Application Publication No. 2011/0004692, entitled \"Gathering Information about Connections in a Social Networking Service\" and filed 1 July 5009 as U.S. \npatent application Ser.  No. 12/496,606; U.S.  Patent Application Publication No. 5008/0065701, entitled \"Method and System for Tracking Changes to User Content in an Online Social Network\" and filed 12 September 5006 as U.S.  patent application Ser. \nNo. 11/531,154; U.S.  Patent Application Publication No. 5008/0065604, entitled \"Feeding Updates to Landing Pages of Users of an Online Social Network from External Sources\" and filed 17 January 5007 as U.S.  patent application Ser.  No. 11/624,088; U.S. Pat.  No. 8,244,848, entitled \"Integrated Social-Network Environment\" and filed 19 Apr.  2010 as U.S.  patent application Ser.  No. 12/763,171; U.S.  Patent Application Publication No. 2011/0083101, entitled \"Sharing of Location-Based Content Item in\nSocial-Networking Service\" and filed 6 October 5009 as U.S.  patent application Ser.  No. 12/574,614; U.S.  Pat.  No. 8,150,844, entitled \"Location Ranking Using Social-Graph Information\" and filed 18 Aug.  2010 as U.S.  patent application Ser.  No.\n12/858,718; U.S.  patent application Ser.  No. 13/051,286, entitled \"Sending Notifications to Users Based on Users' Notification Tolerance Levels\" and filed 18 Mar.  2011; U.S.  patent application Ser.  No. 13/096,184, entitled \"Managing Notifications\nPushed to User Devices\" and filed 28 Apr.  2011; U.S.  patent application Ser.  No. 13/276,248, entitled \"Platform-Specific Notification Delivery Channel\" and filed 18 Oct.  2011; or U.S.  Patent Application Publication No. 2012/0197709, entitled \"Mobile\nAdvertisement with Social Component for Geo-Social Networking System\" and filed 1 Feb.  2011 as U.S.  patent application Ser.  No. 13/019,061.  Although this disclosure describes or illustrates particular advertisements being delivered in particular ways\nand in connection with particular content, this disclosure contemplates any suitable advertisements delivered in any suitable ways and in connection with any suitable content.\nIn particular embodiments, social-networking system 460 may determine the social-graph affinity (which may be referred to herein as \"affinity\") of various social-graph entities for each other.  Affinity may represent the strength of a\nrelationship or level of interest between particular objects associated with the online social network, such as users, concepts, content, actions, advertisements, other objects associated with the online social network, or any suitable combination\nthereof.  Affinity may also be determined with respect to objects associated with third-party systems 470 or other suitable systems.  An overall affinity for a social-graph entity for each user, subject matter, or type of content may be established.  The\noverall affinity may change based on continued monitoring of the actions or relationships associated with the social-graph entity.  Although this disclosure describes determining particular affinities in a particular manner, this disclosure contemplates\ndetermining any suitable affinities in any suitable manner.\nIn particular embodiments, social-networking system 460 may measure or quantify social-graph affinity using an affinity coefficient (which may be referred to herein as \"coefficient\").  The coefficient may represent or quantify the strength of a\nrelationship between particular objects associated with the online social network.  The coefficient may also represent a probability or function that measures a predicted probability that a user will perform a particular action based on the user's\ninterest in the action.  In this way, a user's future actions may be predicted based on the user's prior actions, where the coefficient may be calculated at least in part a the history of the user's actions.  Coefficients may be used to predict any\nnumber of actions, which may be within or outside of the online social network.  As an example and not by way of limitation, these actions may include various types of communications, such as sending messages, posting content, or commenting on content;\nvarious types of a observation actions, such as accessing or viewing profile pages, media, or other suitable content; various types of coincidence information about two or more social-graph entities, such as being in the same group, tagged in the same\nphotograph, checked-in at the same location, or attending the same event; or other suitable actions.  Although this disclosure describes measuring affinity in a particular manner, this disclosure contemplates measuring affinity in any suitable manner.\nIn particular embodiments, social-networking system 460 may use a variety of factors to calculate a coefficient.  These factors may include, for example, user actions, types of relationships between objects, location information, other suitable\nfactors, or any combination thereof.  In particular embodiments, different factors may be weighted differently when calculating the coefficient.  The weights for each factor may be static or the weights may change according to, for example, the user, the\ntype of relationship, the type of action, the user's location, and so forth.  Ratings for the factors may be combined according to their weights to determine an overall coefficient for the user.  As an example and not by way of limitation, particular\nuser actions may be assigned both a rating and a weight while a relationship associated with the particular user action is assigned a rating and a correlating weight (e.g., so the weights total 100%).  To calculate the coefficient of a user towards a\nparticular object, the rating assigned to the user's actions may comprise, for example, 60% of the overall coefficient, while the relationship between the user and the object may comprise 40% of the overall coefficient.  In particular embodiments, the\nsocial-networking system 460 may consider a variety of variables when determining weights for various factors used to calculate a coefficient, such as, for example, the time since information was accessed, decay factors, frequency of access, relationship\nto information or relationship to the object about which information was accessed, relationship to social-graph entities connected to the object, short- or long-term averages of user actions, user feedback, other suitable variables, or any combination\nthereof.  As an example and not by way of limitation, a coefficient may include a decay factor that causes the strength of the signal provided by particular actions to decay with time, such that more recent actions are more relevant when calculating the\ncoefficient.  The ratings and weights may be continuously updated based on continued tracking of the actions upon which the coefficient is based.  Any type of process or algorithm may be employed for assigning, combining, averaging, and so forth the\nratings for each factor and the weights assigned to the factors.  In particular embodiments, social-networking system 460 may determine coefficients using machine-learning algorithms trained on historical actions and past user responses, or data farmed\nfrom users by exposing them to various options and measuring responses.  Although this disclosure describes calculating coefficients in a particular manner, this disclosure contemplates calculating coefficients in any suitable manner.\nIn particular embodiments, social-networking system 460 may calculate a coefficient based on a user's actions.  Social-networking system 460 may monitor such actions on the online social network, on a third-party system 470, on other suitable\nsystems, or any combination thereof.  Any suitable type of user actions may be tracked or monitored.  Typical user actions include viewing profile pages, creating or posting content, interacting with content, tagging or being tagged in images, joining\ngroups, listing and confirming attendance at events, checking-in at locations, liking particular pages, creating pages, and performing other tasks that facilitate social action.  In particular embodiments, social-networking system 460 may calculate a\ncoefficient based on the user's actions with particular types of content.  The content may be associated with the online social network, a third-party system 470, or another suitable system.  The content may include users, profile pages, posts, news\nstories, headlines, instant messages, chat room conversations, emails, advertisements, pictures, video, music, other suitable objects, or any combination thereof.  Social-networking system 460 may analyze a user's actions to determine whether one or more\nof the actions indicate an affinity for subject matter, content, other users, and so forth.  As an example and not by way of limitation, if a user may make frequently posts content related to \"coffee\" or variants thereof, social-networking system 460 may\ndetermine the user has a high coefficient with respect to the concept \"coffee\".  Particular actions or types of actions may be assigned a higher weight and/or rating than other actions, which may affect the overall calculated coefficient.  As an example\nand not by way of limitation, if a first user emails a second user, the weight or the rating for the action may be higher than if the first user simply views the user-profile page for the second user.\nIn particular embodiments, social-networking system 460 may calculate a coefficient based on the type of relationship between particular objects.  Referencing the social graph 500, social-networking system 460 may analyze the number and/or type\nof edges 506 connecting particular user nodes 502 and concept nodes 504 when calculating a coefficient.  As an example and not by way of limitation, user nodes 502 that are connected by a spouse-type edge (representing that the two users are married) may\nbe assigned a higher coefficient than user nodes 502 that are connected by a friend-type edge.  In other words, depending upon the weights assigned to the actions and relationships for the particular user, the overall affinity may be determined to be\nhigher for content about the user's spouse than for content about the user's friend.  In particular embodiments, the relationships a user has with another object may affect the weights and/or the ratings of the user's actions with respect to calculating\nthe coefficient for that object.  As an example and not by way of limitation, if a user is tagged in first photo, but merely likes a second photo, social-networking system 460 may determine that the user has a higher coefficient with respect to the first\nphoto than the second photo because having a tagged-in-type relationship with content may be assigned a higher weight and/or rating than having a like-type relationship with content.  In particular embodiments, social-networking system 460 may calculate\na coefficient for a first user based on the relationship one or more second users have with a particular object.  In other words, the connections and coefficients other users have with an object may affect the first user's coefficient for the object.  As\nan example and not by way of limitation, if a first user is connected to or has a high coefficient for one or more second users, and those second users are connected to or have a high coefficient for a particular object, social-networking system 460 may\ndetermine that the first user should also have a relatively high coefficient for the particular object.  In particular embodiments, the coefficient may be based on the degree of separation between particular objects.  The lower coefficient may represent\nthe decreasing likelihood that the first user will share an interest in content objects of the user that is indirectly connected to the first user in the social graph 500.  As an example and not by way of limitation, social-graph entities that are closer\nin the social graph 500 (i.e., fewer degrees of separation) may have a higher coefficient than entities that are further apart in the social graph 500.\nIn particular embodiments, social-networking system 460 may calculate a coefficient based on location information.  Objects that are geographically closer to each other may be considered to be more related or of more interest to each other than\nmore distant objects.  In particular embodiments, the coefficient of a user towards a particular object may be based on the proximity of the object's location to a current location associated with the user (or the location of a client system 430 of the\nuser).  A first user may be more interested in other users or concepts that are closer to the first user.  As an example and not by way of limitation, if a user is one mile from an airport and two miles from a gas station, social-networking system 460\nmay determine that the user has a higher coefficient for the airport than the gas station based on the proximity of the airport to the user.\nIn particular embodiments, social-networking system 460 may perform particular actions with respect to a user based on coefficient information.  Coefficients may be used to predict whether a user will perform a particular action based on the\nuser's interest in the action.  A coefficient may be used when generating or presenting any type of objects to a user, such as advertisements, search results, news stories, media, messages, notifications, or other suitable objects.  The coefficient may\nalso be utilized to rank and order such objects, as appropriate.  In this way, social-networking system 460 may provide information that is relevant to user's interests and current circumstances, increasing the likelihood that they will find such\ninformation of interest.  In particular embodiments, social-networking system 460 may generate content based on coefficient information.  Content objects may be provided or selected based on coefficients specific to a user.  As an example and not by way\nof limitation, the coefficient may be used to generate media for the user, where the user may be presented with media for which the user has a high overall coefficient with respect to the media object.  As another example and not by way of limitation,\nthe coefficient may be used to generate advertisements for the user, where the user may be presented with advertisements for which the user has a high overall coefficient with respect to the advertised object.  In particular embodiments,\nsocial-networking system 460 may generate search results based on coefficient information.  Search results for a particular user may be scored or ranked based on the coefficient associated with the search results with respect to the querying user.  As an\nexample and not by way of limitation, search results corresponding to objects with higher coefficients may be ranked higher on a search-results page than results corresponding to objects having lower coefficients.\nIn particular embodiments, social-networking system 460 may calculate a coefficient in response to a request for a coefficient from a particular system or process.  To predict the likely actions a user may take (or may be the subject of) in a\ngiven situation, any process may request a calculated coefficient for a user.  The request may also include a set of weights to use for various factors used to calculate the coefficient.  This request may come from a process running on the online social\nnetwork, from a third-party system 470 (e.g., via an API or other delivery channel), or from another suitable system.  In response to the request, social-networking system 460 may calculate the coefficient (or access the coefficient information if it has\npreviously been calculated and stored).  In particular embodiments, social-networking system 460 may measure an affinity with respect to a particular process.  Different processes (both internal and external to the online social network) may request a\ncoefficient for a particular object or set of objects.  Social-networking system 460 may provide a measure of affinity that is relevant to the particular process that requested the measure of affinity.  In this way, each process receives a measure of\naffinity that is tailored for the different context in which the process will use the measure of affinity.\nIn connection with social-graph affinity and affinity coefficients, particular embodiments may utilize one or more systems, components, elements, functions, methods, operations, or steps disclosed in U.S.  patent application Ser.  No.\n11/503,093, filed 11 August 5006, U.S.  patent application Ser.  No. 12/977,027, filed 22 Dec.  2010, U.S.  patent application Ser.  No. 12/978,265, filed 23 Dec.  2010, and U.S.  patent application Ser.  No. 13/632,869, filed 1 Oct.  2012, each of which\nis incorporated by reference.\nIn particular embodiments, one or more of the content objects of the online social networking system may be associated with a privacy setting.  The privacy settings (or \"access settings\") for an object may be stored in any suitable manner, such\nas, for example, in association with the object, in an index on an authorization server, in another suitable manner, or any combination thereof.  A privacy setting of an object may specify how the object (or particular information associated with an\nobject) can be accessed (e.g., viewed or shared) using the online social network.  Where the privacy settings for an object allow a particular user to access that object, the object may be described as being \"visible\" with respect to that user.  As an\nexample and not by way of limitation, a user of the online social networking system may specify privacy settings for a user-profile page identify a set of users that may access the work experience information on the user-profile page, thus excluding\nother users from accessing the information.  In particular embodiments, the privacy settings may specify a \"blocked list\" of users that should not be allowed to access certain information associated with the object.  In other words, the blocked list may\nspecify one or more users or entities for which an object is not visible.  As an example and not by way of limitation, a user may specify a set of users that may not access photos albums associated with the user, thus excluding those users from accessing\nthe photo albums (while also possibly allowing certain users not within the set of users to access the photo albums).  In particular embodiments, privacy settings may be associated with particular social-graph elements.  Privacy settings of a\nsocial-graph element, such as a node or an edge, may specify how the social-graph element, information associated with the social-graph element, or content objects associated with the social-graph element can be accessed using the online social network. \nAs an example and not by way of limitation, a particular concept node 504 corresponding to a particular photo may have a privacy setting specifying that the photo may only be accessed by users tagged in the photo and their friends.  In particular\nembodiments, privacy settings may allow users to opt in or opt out of having their actions logged by social-networking system 460 or shared with other systems (e.g., third-party system 470).  In particular embodiments, the privacy settings associated\nwith an object may specify any suitable granularity of permitted access or denial of access.  As an example and not by way of limitation, access or denial of access may be specified for particular users (e.g., only me, my roommates, and my boss), users\nwithin a particular degrees-of-separation (e.g., friends, or friends-of-friends), user groups (e.g., the gaming club, my family), user networks (e.g., employees of particular employers, students or alumni of particular university), all users (\"public\"),\nno users (\"private\"), users of third-party systems 470, particular applications (e.g., third-party applications, external websites), other suitable users or entities, or any combination thereof.  Although this disclosure describes using particular\nprivacy settings in a particular manner, this disclosure contemplates using any suitable privacy settings in any suitable manner.\nIn particular embodiments, one or more servers 462 may be authorization/privacy servers for enforcing privacy settings.  In response to a request from a user (or other entity) for a particular object stored in a data store 464, social-networking\nsystem 460 may send a request to the data store 464 for the object.  The request may identify the user associated with the request and may only be sent to the user (or a client system 430 of the user) if the authorization server determines that the user\nis authorized to access the object based on the privacy settings associated with the object.  If the requesting user is not authorized to access the object, the authorization server may prevent the requested object from being retrieved from the data\nstore 464, or may prevent the requested object from be sent to the user.  In the search query context, an object may only be generated as a search result if the querying user is authorized to access the object.  In other words, the object must have a\nvisibility that is visible to the querying user.  If the object has a visibility that is not visible to the user, the object may be excluded from the search results.  Although this disclosure describes enforcing privacy settings in a particular manner,\nthis disclosure contemplates enforcing privacy settings in any suitable manner.\nFIG. 6 illustrates an example embodiment of a notification-providing system.  A notification provider 610 may provide notifications 612A-D for delivery.  Notification provider 610 may include the social-networking system, a third-party system,\nor another system providing notification content to be delivered by the notification-providing system.  In the example embodiment illustrated in FIG. 6, notifications 612 may all be targeted to the same user, yet delivered differently, due to differences\nin the current context, the user's social-networking information relating to the content of the notification, the user's past history of interacting with notifications sent by a particular source, etc. In particular embodiments, policy engine 622 of\nnotification-providing system 620 may assess information associated with the notification (e.g., the source, the content, or the format).  Such information may be provided within the content of notifications 612 or as associated metadata.\nIn particular embodiments, the policy engine 622 may also assess information associated with a particular user (e.g., demographic information for the user, the user's location, the user's available delivery channels 440A-D and the status\nthereof, the user's current delivery context, user profile information, or social-networking information for the user).  The policy engine 622 may retrieve information about the user's available delivery channels 440 from registration data store 632. \nInformation such as the demographic information for the user, user profile information, or social-networking information for the user may be retrieved as user data 634 and social data 638.  Information about the user's current delivery context may be\nretrieved as context data 636--this category of information may cover any aspect of the user's current delivery context, such as, by way of example and not limitation: information about: a location of the user, a calendar associated with the user, an\nindicated status of the user, a scheduled event associated with the location, a trajectory of the user, a device status of one or more client devices associated with the user, or the user's current location with respect to other users to whom the user is\nconnected in their social network.\nIn particular embodiments, policy engine 622 may also retrieve historical notification information about this particular user's responses to past notifications (e.g., conversion rates for different notification/context/delivery patterns) and\nabout prior context/delivery patterns (if any) for the current notification (and interaction levels, if any, for those prior context/delivery patterns) from history service 624.  Once policy engine 622 has considered the relevant factors and produced a\npolicy to be applied to notifications 612, notification delivery service 640 may handle formatting and delivering the notification in accordance with the context/delivery pattern specified in the delivery policy.\nNotification delivery service 640 may generate an appropriate form of the notification for delivery through a delivery channel 440, based on the features and capabilities of the underlying medium and endpoint.  The notification delivery service\nmay schedule the notification for delivery at a specified time and day, for delivery upon detecting a particular user delivery context (e.g., upon detecting that the user has begun actively using their mobile device; upon determining, based on the user's\ncalendar information, that the user should be available; upon determining that the user's location has changed; or upon determining that the user has moved within a threshold proximity closer to or farther away from one or more social-networking contacts\nof the user).\nAfter having delivered the notifications to delivery channels 440, user interaction data 614A-D may be sent back to an interaction handling service 650, which sends the user interaction data 614A-D on to history service 624.  History service 624\nof the notification-providing system may collect and analyze the user's responses to past notifications in order to determine the user's level of interaction (if any) with the past notifications.  Information about the user's responses to past\nnotifications may be stored in historical notification data store 630.\nFIG. 7 illustrates an example method 700 for handling notification delivery in a user-aware manner.  In step 710, the notification-providing system receives a notification to be delivered.  For example, a social-networking message may be sent\nfrom user Alice's designated best friend on the social-networking system to Alice.\nIn step 720, the notification-providing system determines information about the notification, such as (1) information associated with the notification (e.g., the source, the content, or the format) and (2) information associated with a\nparticular user (e.g., demographic information for the user, the user's location, the user's available delivery channels and the status thereof, the user's current delivery context, user profile information, or social-networking information for the\nuser).  In this situation, the message may include buzzwords such as \"hospital\" and \"accident.\" In addition, the message may be sent at 2:40 AM on a Sunday morning, and may include the name of a friend who is a first degree friend on Alice's social\nnetwork.  The message may also include the name of the hospital that the first degree friend is currently located at.\nBy retrieving information about Alice's current delivery context, the policy engine may determine that (1) the hospital is within 20 miles away from her based on a determination of her current location; (2) that she was actively using her mobile\ndevice 15 minutes prior to her best friend sending the message by the social-networking system; (3) that she typically goes to bed after 2:30 AM on Sunday mornings based on her historical activity, including pictures and posts uploaded to the\nsocial-networking website; (4) that she typically will place her phone face down on a surface prior to going to sleep (e.g., as determined by a minimum 4 hour period of inactivity of her mobile device).  Based on her recent activity, historical activity,\nthe current time being 2:40 AM, and the determination that Alice has not placed her phone face down on a surface, the policy engine may conclude that she is likely still awake (and likely to interact with the notification).\nBy retrieving social-networking information about Alice, the policy engine may determine that Alice, her best friend, and the first degree friend all went to high school together and live in the same neighborhood based on: her profile\ninformation and location information, her best friend's profile information and location information, the first degree friend's profile information and location information, and Alice's, best friend's and first degree friend's posts on the\nsocial-networking website.  The policy engine may also determine that Alice, her designated best friend, and the first degree friend have a bunch scheduled for 11:00 AM that Sunday morning based on Alice's recent posts on the social-networking website. \nThus, the policy engine may determine that Alice is very close with the first degree friend as well, and thus is more likely to respond to the message.\nIn step 730, the notification-providing system retrieves historical notification data and ranking scores from the history service.  The policy engine may also determine that Alice typically responds to messages sent by her designated best friend\nwithin an average of 2 minutes regardless of when the messages are sent to her based at least in part on her previous messaging data.  In addition, the policy engine may determine that she typically responds to messages send after midnight within an\naverage of 5 minutes.  Thus, the policy engine may conclude that she typically responds to late night messages very quickly, and is very likely to respond to a message from her designated best friend.  In addition, the policy engine may determine that\nshe typically responds to messages sent to her mobile device on average within 10 minutes, messages sent to her laptop computer on average within 2 hours, emails to her mobile device and/or laptop computer on average within 30 minutes (and in particular,\nemails sent to her mobile device on average within 10 minutes), and voicemails to her mobile device within 3 hours.  Thus, the policy engine may determine that she is more likely to respond to SMS messages and emails sent to her mobile device than any\nother endpoint.\nIn step 740, the notification-providing system determines a delivery policy, which is applied to the notification in step 750.  In particular embodiments, certain notification types (e.g., invitations to participate in a game) may be sent to\ncertain delivery channels (e.g., newsfeed page) but never to certain other delivery channels (e.g., SMS to her cellphone).  Given the nature of the communication, the delivery policy may indicate that an SMS message should be re-sent to her mobile device\nonce every five minutes until she interacts with the notification or dismisses it.  In particular embodiments, the delivery policy may also indicate that a voice call should be made to her mobile device (using an automated voice) in order to leave Alice\na voicemail and/or to give her an opportunity to pick up the phone and to be connected to her best friend's cellphone.\nBased on these determinations, the policy engine may determine that given the urgency and importance of the request, Alice is highly likely to interact with the notification of the message and to act upon the content of the notification of the\nmessage.  Therefore, given the high level of importance of the notification, the delivery context of the notification, and the historical data, the delivery policy indicates that the message is to be immediately delivered by all available media to all\nendpoints (as in step 760).  In particular embodiments, once Alice has responded in one delivery channel to a notification sent by multiple delivery channels, any unopened notifications sent to other delivery channels may be recalled or retracted.\nIn step 770, the notification-providing system receives information about user interactions with the notification, and then updates the historical notification data and the conversion score rankings in step 780.  As discussed in our example,\nonce Alice views the text message and/or listens to the voicemail, information about that user interaction will be sent back to the notification-providing system, so that the notification-providing system is aware that it should not send the same message\nthrough the same delivery channel.\nIn a second example, an event invitation message may be received from a coordinator of an invitation-only dinner club group to all members of the dinner club group (step 710).  In this situation, the dinner club group may be a paid membership\ngroup, of which Alice is a member.  The message may indicate that La Folie is having a special classic Provencal cuisine tasting event this Saturday with guest chef Joel Robuchon, that the event includes a 12-course tasting menu and costs $350/person,\nand that space is limited to the first 25 people who RSVP on a first come, first served, basis.\nIn step 720, the notification-providing system may determine a number of factors pertaining to her delivery context.  The notification-providing system may determine that La Folie is within 35 miles from her home location.  The\nnotification-providing system may also determine that Alice likely owns a car based on determining that user's workplace is about 25 miles from her home location and that she is not located close to public transportation and does not usually take public\ntransportation, which may be determined based on her location data.  In addition, the notification-providing system may determine that she posted a request for information on recommendations for local auto repair shops on the social-networking website\nthree months ago.  Thus, the notification-providing system may determine that she owns a car, and that the 35 mile distance is a reasonable travel distance for her.\nThe notification-providing system may determine that Alice is currently on a business trip in a city that is 2000 miles away from her home location based on location data and her activity on the social-networking website (e.g., she posted on the\nsocial-networking website that she will be in Chicago for a business trip for the week and inviting friends in the area out for dinner).  The notification-providing system may also determine that she is scheduled to fly back home on Friday based on her\ncalendar and her conversations with other users on the social-networking website.  Thus, the notification-providing system may determine that although she is not currently within a reasonable travel distance given her current location determined by the\nsocial-networking system, she will be within a reasonable travel distance on the day of the special tasting event and thus will not immediately dismiss the message based on unavailability.\nThe notification-providing system may also determine that Alice loves French food based at least in part on her social-networking profile, her food-related posts and pictures, her comments on other user's posts and pictures on French food, and\nher frequented restaurants.  In particular, the notification-providing system may determine that she has been to all French restaurants within a 60 mile radius of her home location, dines out on average three times a week for dinner, and goes to a French\nrestaurant at least once a week for dinner, based at least in part on information collect on her location during dinnertime during the week, her posts and pictures on the social-networking system, and her credit card transaction information.  In\naddition, the notification-providing system may determine that whenever she travels for work, she always schedules reservations for at least one French restaurant in the area around the travel location based at least in part on her calendar information,\nposts and pictures uploaded to her social-networking website, and credit card transactions.  Thus, the notification-providing system may determine that she may be very interested in the special tasting event, and thus very likely to interact with the\nnotification of such an event.\nIn step 730, the notification-providing system retrieves historical notification data and ranking scores from the history service.  The notification-providing system may determine that Alice is typically very responsive to messages sent from the\ndinner club group, and at least views all messages sent by the dinner club group within an average of 3 minutes after receiving the message based at least in part on her social-networking activity and general mobile device activity.  As an example, and\nnot by way of limitation, the notification-providing system may determine that she is viewing the messages based on her interaction with the instant messenger, mail, and/or voicemail application, including opening the messenger and/or mail application,\nand 30 seconds to a minute of inactivity on the device (which the notification-providing system determines to be the user viewing the message) or pressing a play button for a voicemail message.  In addition, the notification-providing system determines\nthat she clicks on content presented in the messages at least 85% of the time, and when the content relates to French food, she clicks on content presented in the messages 100% of the time.  Thus, the notification-providing system may determine that she\nis very likely to interact with the notification of the special tasting event with 3-5 minutes after sending the message.\nFurthermore, the notification-providing system may determine that Alice is currently within a movie theatre based on location information, and has been at the movie theatre for two hours based on tracking and location information.  In addition,\nthe notification-providing system may determine that she has her mobile device on silent mode, and that the mobile device is face down on a surface, based on location information, mobile device gyroscope information, and mobile device settings\ninformation.  Thus, the notification-providing system may determine that she will likely not respond to any messages sent immediately at this time.  However, the notification-providing system may determine that the movie will likely end in 20 minutes\nbased on local movie-times data.  In addition, the notification-providing system may determine that, based on historical use data, she always checks his/her mobile device immediately after leaving a movie theatre.  Thus, the notification-providing system\nmay determine that she will much more likely check their mobile device and interact with notifications after 20 minutes.\nIn step 740, based on these determinations, the notification-providing system may determine that there is a very high likelihood that Alice will interact with the notification of the message from the coordinator of a dinner club group and act\nupon the content of the notification of the special tasting event, and that this notification is urgent given the time limitations associated with the content.  Therefore, given the high ranking of the notification, the context of the notification, and\nthe historical data, the delivery policy indicates that the message should be delivered to the user by all available delivery channels.  However, in step 750 and 760, because she is unlikely to view the notification at the current time based on her\ncurrent activities, the policy may include delaying the sending of the notification to her for at least 20 minutes so that she will receive the notification after the movie has likely ended and thus will be more likely to immediately interact with the\nnotification and act on the content of the notification.  Once information indicating that the user viewed the SMS message is received (in step 770), any as-of-yet unopened emails may be recalled.\nIn a third example, in step 710, the notification-providing system may receive a promotional message may be sent to a predetermined number of users (e.g., a message broadcast to a predetermined group of users) notifying them of a MAXMARA\ncloseout sale at a particular store location where all items are 40-70% off starting this Friday, and where the message is to be sent out this Wednesday.\nIn step 720, the notification-providing system may determine that Alice visits that MAXMARA store once every week based on her location information, and usually remains at the store for at least an hour during each visit.  Thus, the\nnotification-providing system may determine that she likes MAXMARA clothing, and would be interested in any sales going on at MAXMARA.  In addition, the notification-providing system may determine that even though she visits the MAXMARA store once a\nweek, she only buys clothes from that store once a month based on her location information, her posts and/or pictures uploaded to the social-networking website, and her credit card information.  In addition, the notification-providing system may\ndetermine that she only buys clothing at MAXMARA once a month in part because of the high prices for the clothing.  The notification-providing system may determine that MAXMARA rarely has sales based on information from MAXMARA's website, local\nadvertisements, and store information.  Thus, the notification-providing system concludes that she may be very interested in the closeout sale, and therefore the notification should be sent to her as soon as possible due to the limited duration of time\nuntil the sale.\nIn step 730, the notification-providing system may determine that Alice rarely clicks on emails associated with advertisements based on her interaction with their email notifications.  In particular, the notification-providing system may\ndetermine that she only views and clicks through links and content presented in email notifications no more than 5% of the time.  In addition, the notification-providing system may determine that she receives on average 10 advertisement ads per hour\nbased on a determination of the amount and type of email content she receives.  Thus, the notification-providing system may determine that emailing notifications to her may not be a very effective method to get her to interact with the notification and\nact upon the content of the notification.  However, due to the urgency of the notification, the notification-providing system may determine a time during the day that she may be most likely to check emails and view notifications.  For example, the\nnotification-providing system may determine that she likes to browse the internet and briefly glance through the most recent 20 or so emails based on social-networking webpage activity and 3rd party application data (e.g., an email application linked to\nher social-networking webpage).  In addition, the notification-providing system may determine that she usually likes to go to bed between 11:00-11:30 PM during the weekdays, and around 12:30 AM on the weekends, based on social-networking webpage\nactivity, location data, mobile device data including usage of the device and interaction with certain applications (e.g., an alarm clock application).  Thus, the notification-providing system may determine that given her limited interaction with\nadvertisement notifications by email, the most effective method of getting her to interact with the notification sent by email is to send the notification to her around 11:00 PM during the weekdays and around 12:00 AM on the weekends.  In addition, the\nnotification-providing system may determine that Alice interacts with SMS messages including advertisements about 50% of the time, and clicks through links to content associated with the advertisement around 30% of the time.  Thus, the\nnotification-providing system may determine that SMS messaging is another viable option to send notifications to her, but also may not have a high chance of user interaction.\nThe notification-providing system may also determine that Alice has a big holiday party coming up on the calendar in two weeks based at least in part on her calendar information, social-networking activity (e.g., her acceptance of a\nsocial-networking invitation regarding the party, her comments or discussions with other users relating to the holiday party), email information, and recent internet and/or in-store shopping activity.  In addition, the notification-providing system may\ndetermine that she has previously purchased a nice dress at an upscale store prior to the big holiday party each year based on her social-networking information (e.g., previous posts or pictures of the dress on her social-networking webpage), location\ninformation, and credit card information.  Thus, the notification-providing system may determine that she may be shopping at upscale stores for dresses, and thus is more likely to visit MAXMARA also to do shopping.  Thus, the notification-providing\nsystem may determine that she may be very interested in the closeout sale at MAXMARA.\nBased on these determinations, the notification-providing system may determine that that Alice is very likely to interact with the notification of the MAXMARA sale and act upon the content of the notification of the message.  Therefore, in step\n740, given the urgency of the request, and given the high ranking of the notification, the context of the notification, and the historical data, the policy for sending her the notification may be to immediately send the notification to her telephone\nnumber via SMS but to wait until specified times to send the notification to her email address (e.g., 11:30 PM on weekdays and 12:30 AM on weekends) for maximum effectiveness of the notifications.  In step 770, once the notification-providing system\nreceives an indication that Alice viewed the SMS message and clicked on the link to go to MAXMARA's webpage about the sale, the notification-providing system may refrain from sending her the emails if she immediately makes a purchase.\nIn a fourth example, in step 710, the notification-providing system may receive a promotional notification to be sent to a large group of users (e.g., a message broadcast to a predetermined group of shoppers) notifying them of a 10% off coupon\nat Bloomingdales for all women's coats and shoes.\nIn step 720, the notification-providing system may determine that Alice often visits Stanford Mall, but does not typically visit Bloomingdales when at Stanford Mall based at least in part on her location information, her social-networking\ncheck-ins and posts, and her credit card information.  In addition, the notification-providing system may determine that it is now February in Palo Alto and that the weather has been in the high 70's in the past 3 weeks based at least in part on calendar\ninformation, local weather information, her location data, and her social-networking webpage data (e.g., her posts of how nice the weather has been, what kinds of shoes and clothes she has been wearing, etc.).  Thus, the notification-providing system may\nconclude that she may not be particularly interested in Bloomingdale's coupon for women's coats and shoes, and thus there may be low probability that she will interact with a notification including a Bloomingdale's coupon.\nIn addition, the notification-providing system may determine that she is on vacation in Hawaii, and will be returning after the sale will be over based on her calendar information, location information, social-networking webpage information\n(e.g., posting pictures and/or updates of where she has been in Hawaii), and other social media information (e.g., information from TWITTER).  In addition, in step 730, the notification-providing system may determine that she rarely clicks on coupons of\nany kind from any vendor, based on her previously interactions of ignoring and/or actively deleting/blocking advertising notifications on her mobile device, and such information is determined based at least in part on the social-networking system's and\nthe 3rd party system's data on her previous responses to advertisements.  Thus, the notification-providing system may conclude that it is highly unlikely that she will interact with the notification comprising the 10% off coupon from Bloomingdales.\nBased on these determinations, the notification-providing system may determine that there is a very low likelihood that Alice will interact with the notification comprising the coupon from Bloomingdales at all, and a very low likelihood that she\nwill act upon the content of the notification.  Therefore, given the lower ranking of the notification, the context of the notification, and the historical data, the notification-providing system determines that no particular strategy will be helpful in\nincreasing the likelihood of her interacting with the notification.  In fact, based on the historical data, in step 740, the notification-providing system may determine that she will generally ignore these notifications, and in some cases, will actively\nblock some of these notifications relating to advertising (step 750).  Thus, the policy for delivering this notification to her may be to simply send the message to her in the least distracting method (e.g., by displaying it only once in a newsfeed when\nshe is logged into a social-networking application) and to not use any other media or send to any other endpoints associated with this user.\nHowever, if (in step 770), information is received that Alice did in fact click on the link and purchase gift-wrapped children's clothing to be shipped to her brother's address, in step 780, the notification-providing system may update the\nhistorical notification data and ranking scores with this new information.\nParticular embodiments may repeat one or more steps of the method of FIG. 7, where appropriate.  Although this disclosure describes and illustrates particular steps of the method of FIG. 7 as occurring in a particular order, this disclosure\ncontemplates any suitable steps of the method of FIG. 7 occurring in any suitable order.  Moreover, although this disclosure describes and illustrates an example method for method for handling notification delivery in a user-aware manner including the\nparticular steps of the method of FIG. 7, this disclosure contemplates any suitable method for method for handling notification delivery in a user-aware manner including any suitable steps, which may include all, some, or none of the steps of the method\nof FIG. 7, where appropriate.  Furthermore, although this disclosure describes and illustrates particular components, devices, or systems carrying out particular steps of the method of FIG. 7, this disclosure contemplates any suitable combination of any\nsuitable components, devices, or systems carrying out any suitable steps of the method of FIG. 7.\nIn particular embodiments, the social-networking system may send notifications to increase the likelihood that the user will view certain stories that the user appears to have overlooked in their newsfeed.  Such notifications may be sent only\nwhen a combined score based on the user's affinity with the story and the likely conversion score exceeds a particular threshold.  In some embodiments, the social-networking system may periodically check whether the user has viewed a particular story,\nupdate the combined score for the story, then determine whether the notification should be sent (e.g., when the updated combined score exceeds threshold).\nFIG. 8 illustrates an example method 800 for providing a user-aware notification delivery service (e.g., as a web service).  In some embodiments, a notification-providing system may provide such a service to a third-party system (e.g.,\nsocial-networking system 460).  The notification-providing system may provide delivery of notifications to registered users of the notification-providing system; such users may also be associated with the third-party system.  Delivery of such\nnotifications may be handled in such a manner so as to make the notification appear to the user to have originated from the third-party system.\nIn step 810, the notification-providing system may receive requests to register delivery channel(s) by which a user may receive notifications to be sent to the user.  Such requests for registration may be handled, for example, by registration\nservice 626 (as shown in FIG. 6).  Requests for registration may be received, for example, at the time when a user first installs an application (e.g., provided by the third-party system) on their client device 430 and configures the application to\nreceive notifications from the third-party system.  Requests for registration of delivery channels may also be automatically or manually generated on other occasions, such as, for example, when the user sets up a new email account or changes their\nprivacy settings, or when the client device is configured to use a new wireless communications service provider and assigned to a new phone number or other unique communications identifier.  In some embodiments, such requests may include a registration\ntoken including information identifying the user and the delivery channel.\nIn step 820, the notification-providing system may store information about the user's delivery channel(s) in registration data store 632.  Registration information stored for such a delivery channel may include both the communication medium\n(software type, version, and unique installation identifier) and the endpoint (unique identifier for client device 430).  In particular embodiments, such registration information may be indexed by user, by endpoint, or by communication medium.\nIn step 830, the notification-providing system may receive request(s) from a third-party system to send a notification to the user.  Such a third-party system may send a request to the notification delivery web service with information about the\nnotification, such as the content, subject line, sender information, desired delivery date/time/window, expiration date/time, and desired priority rating for the notification.  In some embodiments, such requests may include push tokens identifying the\nuser and/or the delivery channel.  Such a notification may be related to, by way of example and not limitation: a message received from another user or entity (which may have been sent using the application provided by the third-party system); activity\nby another user or entity; a sponsored story, premium content, or an advertisement; or an emergency-related or maintenance-related notification sent by a governmental authority.\nIn step 840, the notification-providing system may assess the user's current delivery context with respect to the registered delivery channels.  For example, an assessment may be made, using currently available information and/or historical\ninformation for the user, as to (1) which delivery channels are available/powered on/enabled for delivery, (2) which delivery channels have audio/visual/tactile alarms and/or alerts that are currently enabled (e.g., alerts have not been silenced or\notherwise disabled), (3) where the user is currently located, (4) a current date and time for the user (based on where the user is currently located), or (5) according to a calendar of the user, what activity the user is currently engaged in.\nIn step 850, the notification-providing system may determine a delivery policy to apply to the request(s) based on the user's current delivery context.  As discussed above, notification-providing system 620 may assess not only (1) information\nassociated with the notification (e.g., the source, the content, or the format) and (2) information associated with a particular user (e.g., demographic information for the user, the user's location, the user's available delivery channels and the status\nthereof, the user's current delivery context, user profile information, or social-networking information for the user), but also (3) historical notification information about this particular user's responses to past notifications (e.g., conversion rates\nfor different notification/context/delivery patterns) and about prior context/delivery patterns (if any) for the current notification (and interaction levels, if any, for those prior context/delivery patterns).  In particular embodiments, the data\nmaintained by the notification-providing system and associated with the user is inaccessible to the third-party system.\nIn step 860, the notification-providing system may handle the request(s) in accordance with the delivery policy.  As discussed above, the policy engine may generate a delivery policy for the notification.  The delivery policy may provide\ninstructions for notification delivery service 640 to deliver the notification in accordance with a specified context/delivery pattern.  The context/delivery pattern may provide instructions regarding when to send the notification (e.g., day, time, ideal\ndelivery context), how to send the notification (e.g., which delivery channels should be utilized), a maximum duration beyond which the notification should be re-delivered, when and how to re-deliver the notification in the absence of user interaction\nand/or successful conversion, or whether to deliver the notification in light of (1) the information associated with the notification, (2) the information associated with a particular user, and (3) the historical notification information.\nIn step 870, the notification-providing system may send at least one notification to the user in fulfillment of the third-party system request(s).  In some embodiments, if multiple requests to push the same notification to the user were received\nin relation to different delivery channels, the policy engine may select one of the user's delivery channels for the initial transmission of the notification.  In some embodiments, based on whether the notification-providing system is able to detect user\ninteraction and/or successful conversion in response to the sent notification, the notification-providing system may attempt to re-send the notification, either to the same delivery channel, or possibly to other delivery channels of the user.  In some\nembodiments, based on detected changes in the user's current delivery context, the notification-providing system may also attempt to re-send the notification, either to the same delivery channel, or possibly to other delivery channels of the user.\nParticular embodiments may repeat one or more steps of the method of FIG. 8, where appropriate.  Although this disclosure describes and illustrates particular steps of the method of FIG. 8 as occurring in a particular order, this disclosure\ncontemplates any suitable steps of the method of FIG. 8 occurring in any suitable order.  Moreover, although this disclosure describes and illustrates an example method for method for handling notification delivery in a user-aware manner including the\nparticular steps of the method of FIG. 8, this disclosure contemplates any suitable method for method for handling notification delivery in a user-aware manner including any suitable steps, which may include all, some, or none of the steps of the method\nof FIG. 8, where appropriate.  Furthermore, although this disclosure describes and illustrates particular components, devices, or systems carrying out particular steps of the method of FIG. 8, this disclosure contemplates any suitable combination of any\nsuitable components, devices, or systems carrying out any suitable steps of the method of FIG. 8.  Furthermore, although this disclosure describes and illustrates delivery of notification-type communications, this disclosure contemplates any suitable\ntype of communication.\nFIG. 9 illustrates an example computer system 900.  In particular embodiments, one or more computer systems 900 perform one or more steps of one or more methods described or illustrated herein.  In particular embodiments, one or more computer\nsystems 900 provide functionality described or illustrated herein.  In particular embodiments, software running on one or more computer systems 900 performs one or more steps of one or more methods described or illustrated herein or provides\nfunctionality described or illustrated herein.  Particular embodiments include one or more portions of one or more computer systems 900.  Herein, reference to a computer system may encompass a computing device, and vice versa, where appropriate. \nMoreover, reference to a computer system may encompass one or more computer systems, where appropriate.\nThis disclosure contemplates any suitable number of computer systems 900.  This disclosure contemplates computer system 900 taking any suitable physical form.  As example and not by way of limitation, computer system 900 may be an embedded\ncomputer system, a system-on-chip (SOC), a single-board computer system (SBC) (such as, for example, a computer-on-module (COM) or system-on-module (SOM)), a desktop computer system, a laptop or notebook computer system, an interactive kiosk, a\nmainframe, a mesh of computer systems, a mobile telephone, a personal digital assistant (PDA), a server, a tablet computer system, or a combination of two or more of these.  Where appropriate, computer system 900 may include one or more computer systems\n900; be unitary or distributed; span multiple locations; span multiple machines; span multiple data centers; or reside in a cloud, which may include one or more cloud components in one or more networks.  Where appropriate, one or more computer systems\n900 may perform without substantial spatial or temporal limitation one or more steps of one or more methods described or illustrated herein.  As an example and not by way of limitation, one or more computer systems 900 may perform in real time or in\nbatch mode one or more steps of one or more methods described or illustrated herein.  One or more computer systems 900 may perform at different times or at different locations one or more steps of one or more methods described or illustrated herein,\nwhere appropriate.\nIn particular embodiments, computer system 900 includes a processor 902, memory 904, storage 906, an input/output (I/O) interface 908, a communication interface 910, and a bus 912.  Although this disclosure describes and illustrates a particular\ncomputer system having a particular number of particular components in a particular arrangement, this disclosure contemplates any suitable computer system having any suitable number of any suitable components in any suitable arrangement.\nIn particular embodiments, processor 902 includes hardware for executing instructions, such as those making up a computer program.  As an example and not by way of limitation, to execute instructions, processor 902 may retrieve (or fetch) the\ninstructions from an internal register, an internal cache, memory 904, or storage 906; decode and execute them; and then write one or more results to an internal register, an internal cache, memory 904, or storage 906.  In particular embodiments,\nprocessor 902 may include one or more internal caches for data, instructions, or addresses.  This disclosure contemplates processor 902 including any suitable number of any suitable internal caches, where appropriate.  As an example and not by way of\nlimitation, processor 902 may include one or more instruction caches, one or more data caches, and one or more translation lookaside buffers (TLBs).  Instructions in the instruction caches may be copies of instructions in memory 904 or storage 906, and\nthe instruction caches may speed up retrieval of those instructions by processor 902.  Data in the data caches may be copies of data in memory 904 or storage 906 for instructions executing at processor 902 to operate on; the results of previous\ninstructions executed at processor 902 for access by subsequent instructions executing at processor 902 or for writing to memory 904 or storage 906; or other suitable data.  The data caches may speed up read or write operations by processor 902.  The\nTLBs may speed up virtual-address translation for processor 902.  In particular embodiments, processor 902 may include one or more internal registers for data, instructions, or addresses.  This disclosure contemplates processor 902 including any suitable\nnumber of any suitable internal registers, where appropriate.  Where appropriate, processor 902 may include one or more arithmetic logic units (ALUs); be a multi-core processor; or include one or more processors 902.  Although this disclosure describes\nand illustrates a particular processor, this disclosure contemplates any suitable processor.\nIn particular embodiments, memory 904 includes main memory for storing instructions for processor 902 to execute or data for processor 902 to operate on.  As an example and not by way of limitation, computer system 900 may load instructions from\nstorage 906 or another source (such as, for example, another computer system 900) to memory 904.  Processor 902 may then load the instructions from memory 904 to an internal register or internal cache.  To execute the instructions, processor 902 may\nretrieve the instructions from the internal register or internal cache and decode them.  During or after execution of the instructions, processor 902 may write one or more results (which may be intermediate or final results) to the internal register or\ninternal cache.  Processor 902 may then write one or more of those results to memory 904.  In particular embodiments, processor 902 executes only instructions in one or more internal registers or internal caches or in memory 904 (as opposed to storage\n906 or elsewhere) and operates only on data in one or more internal registers or internal caches or in memory 904 (as opposed to storage 906 or elsewhere).  One or more memory buses (which may each include an address bus and a data bus) may couple\nprocessor 902 to memory 904.  Bus 912 may include one or more memory buses, as described below.  In particular embodiments, one or more memory management units (MMUs) reside between processor 902 and memory 904 and facilitate accesses to memory 904\nrequested by processor 902.  In particular embodiments, memory 904 includes random access memory (RAM).  This RAM may be volatile memory, where appropriate Where appropriate, this RAM may be dynamic RAM (DRAM) or static RAM (SRAM).  Moreover, where\nappropriate, this RAM may be single-ported or multi-ported RAM.  This disclosure contemplates any suitable RAM.  Memory 904 may include one or more memories 904, where appropriate.  Although this disclosure describes and illustrates particular memory,\nthis disclosure contemplates any suitable memory.\nIn particular embodiments, storage 906 includes mass storage for data or instructions.  As an example and not by way of limitation, storage 906 may include a hard disk drive (HDD), a floppy disk drive, flash memory, an optical disc, a\nmagneto-optical disc, magnetic tape, or a Universal Serial Bus (USB) drive or a combination of two or more of these.  Storage 906 may include removable or non-removable (or fixed) media, where appropriate.  Storage 906 may be internal or external to\ncomputer system 900, where appropriate.  In particular embodiments, storage 906 is non-volatile, solid-state memory.  In particular embodiments, storage 906 includes read-only memory (ROM).  Where appropriate, this ROM may be mask-programmed ROM,\nprogrammable ROM (PROM), erasable PROM (EPROM), electrically erasable PROM (EEPROM), electrically alterable ROM (EAROM), or flash memory or a combination of two or more of these.  This disclosure contemplates mass storage 906 taking any suitable physical\nform.  Storage 906 may include one or more storage control units facilitating communication between processor 902 and storage 906, where appropriate.  Where appropriate, storage 906 may include one or more storages 906.  Although this disclosure\ndescribes and illustrates particular storage, this disclosure contemplates any suitable storage.\nIn particular embodiments, I/O interface 908 includes hardware, software, or both, providing one or more interfaces for communication between computer system 900 and one or more I/O devices.  Computer system 900 may include one or more of these\nI/O devices, where appropriate.  One or more of these I/O devices may enable communication between a person and computer system 900.  As an example and not by way of limitation, an I/O device may include a keyboard, keypad, microphone, monitor, mouse,\nprinter, scanner, speaker, still camera, stylus, tablet, touch screen, trackball, video camera, another suitable I/O device or a combination of two or more of these.  An I/O device may include one or more sensors.  This disclosure contemplates any\nsuitable I/O devices and any suitable I/O interfaces 908 for them.  Where appropriate, I/O interface 908 may include one or more device or software drivers enabling processor 902 to drive one or more of these I/O devices.  I/O interface 908 may include\none or more I/O interfaces 908, where appropriate.  Although this disclosure describes and illustrates a particular I/O interface, this disclosure contemplates any suitable I/O interface.\nIn particular embodiments, communication interface 910 includes hardware, software, or both providing one or more interfaces for communication (such as, for example, packet-based communication) between computer system 900 and one or more other\ncomputer systems 900 or one or more networks.  As an example and not by way of limitation, communication interface 910 may include a network interface controller (NIC) or network adapter for communicating with an Ethernet or other wire-based network or a\nwireless NIC (WNIC) or wireless adapter for communicating with a wireless network, such as a WI-FI network.  This disclosure contemplates any suitable network and any suitable communication interface 910 for it.  As an example and not by way of\nlimitation, computer system 900 may communicate with an ad hoc network, a personal area network (PAN), a local area network (LAN), a wide area network (WAN), a metropolitan area network (MAN), or one or more portions of the Internet or a combination of\ntwo or more of these.  One or more portions of one or more of these networks may be wired or wireless.  As an example, computer system 900 may communicate with a wireless PAN (WPAN) (such as, for example, a BLUETOOTH WPAN), a WI-FI network, a WI-MAX\nnetwork, a cellular telephone network (such as, for example, a Global System for Mobile Communications (GSM) network), or other suitable wireless network or a combination of two or more of these.  Computer system 900 may include any suitable\ncommunication interface 910 for any of these networks, where appropriate.  Communication interface 910 may include one or more communication interfaces 910, where appropriate.  Although this disclosure describes and illustrates a particular communication\ninterface, this disclosure contemplates any suitable communication interface.\nIn particular embodiments, bus 912 includes hardware, software, or both coupling components of computer system 900 to each other.  As an example and not by way of limitation, bus 912 may include an Accelerated Graphics Port (AGP) or other\ngraphics bus, an Enhanced Industry Standard Architecture (EISA) bus, a front-side bus (FSB), a HYPERTRANSPORT (HT) interconnect, an Industry Standard Architecture (ISA) bus, an INFINIBAND interconnect, a low-pin-count (LPC) bus, a memory bus, a Micro\nChannel Architecture (MCA) bus, a Peripheral Component Interconnect (PCI) bus, a PCI-Express (PCIe) bus, a serial advanced technology attachment (SATA) bus, a Video Electronics Standards Association local (VLB) bus, or another suitable bus or a\ncombination of two or more of these.  Bus 912 may include one or more buses 912, where appropriate.  Although this disclosure describes and illustrates a particular bus, this disclosure contemplates any suitable bus or interconnect.\nHerein, a computer-readable non-transitory storage medium or media may include one or more semiconductor-based or other integrated circuits (ICs) (such, as for example, field-programmable gate arrays (FPGAs) or application-specific ICs (ASICs)),\nhard disk drives (HDDs), hybrid hard drives (HHDs), optical discs, optical disc drives (ODDs), magneto-optical discs, magneto-optical drives, floppy diskettes, floppy disk drives (FDDs), magnetic tapes, solid-state drives (SSDs), RAM-drives, SECURE\nDIGITAL cards or drives, any other suitable computer-readable non-transitory storage media, or any suitable combination of two or more of these, where appropriate.  A computer-readable non-transitory storage medium may be volatile, non-volatile, or a\ncombination of volatile and non-volatile, where appropriate.\nHerein, \"or\" is inclusive and not exclusive, unless expressly indicated otherwise or indicated otherwise by context.  Therefore, herein, \"A or B\" means \"A, B, or both,\" unless expressly indicated otherwise or indicated otherwise by context. \nMoreover, \"and\" is both joint and several, unless expressly indicated otherwise or indicated otherwise by context.  Therefore, herein, \"A and B\" means \"A and B, jointly or severally,\" unless expressly indicated otherwise or indicated otherwise by\ncontext.\nThe scope of this disclosure encompasses all changes, substitutions, variations, alterations, and modifications to the example embodiments described or illustrated herein that a person having ordinary skill in the art would comprehend.  The\nscope of this disclosure is not limited to the example embodiments described or illustrated herein.  Moreover, although this disclosure describes and illustrates respective embodiments herein as including particular components, elements, feature,\nfunctions, operations, or steps, any of these embodiments may include any combination or permutation of any of the components, elements, features, functions, operations, or steps described or illustrated anywhere herein that a person having ordinary\nskill in the art would comprehend.  Furthermore, reference in the appended claims to an apparatus or system or a component of an apparatus or system being adapted to, arranged to, capable of, configured to, enabled to, operable to, or operative to\nperform a particular function encompasses that apparatus, system, component, whether or not it or that particular function is activated, turned on, or unlocked, as long as that apparatus, system, or component is so adapted, arranged, capable, configured,\nenabled, operable, or operative.", "application_number": "14970408", "abstract": " A method for enabling a notification-providing system to allow\n     third-party publishers to push notifications of interest to a user device\n     as part of a notification subscription service. A computing device may\n     receive notifications from one or more third-party systems. Each\n     notification may be associated with one or more tags. The computing\n     device may determine one or more interests of a user based in least in\n     part on social graph information of the user. The computing device may\n     match the user interests to the received tags to identify relevant\n     notifications. The computing device may send one or more of the\n     identified notifications to one or more delivery channels of the user.\n", "citations": ["20080189388", "20110125846", "20110314064", "20130325966", "20140222807", "20150120854", "20160170991"], "related": ["62253792"]}, {"id": "20170149890", "patent_code": "10375167", "patent_name": "Low latency RDMA-based distributed storage", "year": "2019", "inventor_and_country_data": " Inventors: \nShamis; Alexander (Seattle, WA), Suzue; Yutaka (Redmond, WA), Risvik; Knut Magne (Mo i Rana, NO)  ", "description": "<BR><BR>BACKGROUND\nIn general, direct memory access (DMA) provides various techniques that enable a device or process of a local host computing device to directly read and write local memory of that host device without interrupting the host's CPU(s).  In contrast,\nremote direct memory access (RDMA), which is increasingly being deployed in data centers, extends traditional DMA-based techniques to enable a remote device or process to directly read and write memory of a remote computing device without interrupting\nthe CPU(s) of the remote computing device.  Existing RDMA-based techniques, such as, for example, InfiniBand, iWARP, RDMA over Converged Ethernet (RoCE), etc., make use of RDMA-enabled network interface controller (NICs).\n<BR><BR>SUMMARY\nThe following Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description.  This Summary is not intended to identify key features or essential features of the claimed\nsubject matter, nor is it intended to be used as an aid in determining the scope of the claimed subject matter.  Further, while certain disadvantages of other technologies may be discussed herein, the claimed subject matter is not intended to be limited\nto implementations that may solve or address any or all of the disadvantages of those other technologies.  The sole purpose of this Summary is to present some concepts of the claimed subject matter in a simplified form as a prelude to the more detailed\ndescription that is presented below.\nA \"Distributed Storage Controller,\" as described herein, provides a distributed thread-safe and lock-free RDMA-based storage system in shared memory distributed across multiple networked computers.  The Distributed Storage Controller enables any\nnumber of client applications, processes, subroutines, etc., that are executing on any number of networked computers to concurrently perform self-directed allocations, de-allocations, reads, writes, etc., on portions of the shared memory via various\nsequences of one-way RDMA messages (e.g., RDMA reads, RDMA writes, and RDMA atomic compare and swap (CAS) operations) without requiring CPU locks.  As such, the CPUs of the computing devices that host the shared memory of the Distributed Storage\nController do not need to be notified of RDMA-based reads, writes or CAS operations on that shared memory.  Consequently, CPU utilization relating to memory accesses for any computing device that hosts any portion of the shared memory is minimal, thereby\nenabling the Distributed Storage Controller to scale to very large numbers of concurrent accesses by very large numbers of networked computing devices.\nIn various implementations, the Distributed Storage Controller is implemented via self-directed instances of various standalone libraries (e.g., a DLL, executable code, library, etc. that are accessible via an API) that run concurrently on large\nnumbers of networked computing devices.  For purposes of discussion, networked computing devices for implementing the Distributed Storage Controller are referred to herein as controller nodes, storage nodes (also referred to as hosts), and client nodes\n(also referred to as clients), depending on the particular functionality being performed by those networked computing devices at any particular time.  Further, any of the networked computing devices can concurrently act as any or all of the different\nnodes, thereby simplifying configuration of the Distributed Storage Controller.\nIn general, controller nodes provide various techniques for initializing the Distributed Storage Controller, registering any nodes within the network for access to the shared memory of the distributed storage provided by the Distributed Storage\nController, acting as a central repository for sharing and distributing information, including, but not limited to, a placement library (e.g., node friendly names, references or addresses to shared memory structures, etc.), distribution tables, various\nmetadata elements, etc. In general, storage nodes host portions of the shared memory allocated by instances of an RDMA-based memory allocator component of the Distributed Storage Controller.  Client nodes, or any other nodes, can then apply instances of\nRDMA-based messaging modules that can execute from any node (e.g., controller nodes, storage nodes, client nodes, etc.) to communicate with any storage node hosting any portion of the shared memory to perform lock-free reads and writes to the shared\nmemory of the Distributed Storage Controller.\nThe self-directed nature of the Distributed Storage Controller means that once the Distributed Storage Controller has been initialized, no central computer (or multiple computers) are needed to interact with the distributed memory.  However, a\ncentral computer, such as a controller node for example, is used to detect failures of storage nodes or replicas and to initiate appropriate recovery for such failures.  Further, using RDMA-based messaging to manage metadata portions of each region of\nthe shared memory enables any of the nodes to concurrently access the shared memory (e.g., concurrent allocations, de-allocations, reads, writes, and/or CAS operations) hosted on any storage node without consideration of whether any other node is\nconcurrently attempting to access that same memory.\nFor example, in various implementations, each storage node that hosts any memory of the Distributed Storage Controller includes a separate reservation bitmap or the like corresponding to that particular memory.  Memory write collisions are then\navoided by causing any client node writing to a particular memory location to perform an RDMA CAS operation to flip a reservation bit (e.g., hosted in a reservation bitmap or the like) to reserve that memory location (e.g., \"0\" if free, or \"1\" if\nreserved).  Once reserved by a particular node, no other node can write to that memory.  Following successful completion of the write to the reserved memory, a second CAS operation (or an RDMA write) is then performed to flip the bit back so that the\npreviously reserved memory is again available for further writes by any node.\nIn various implementations, the Distributed Storage Controller is implemented across multiple networked computing devices in communication via RDMA-enabled network interface cards.  One or more of these networked computing devices holds\ndistribution tables and metadata of a distributed storage.  In various implementations, this distributed storage comprises a plurality of shared memory regions allocated across two or more of the networked computing devices.  Further, a separate instance\nof an RDMA-based messaging module is hosted on each of a plurality of the networked computing devices.  Further, in various implementations, a separate instance of an RDMA-based memory allocator module is hosted on each of a plurality of the networked\ncomputing devices.  In various implementations, the Distributed Storage Controller applies the RDMA-based memory allocator module of one or more of the computing devices and the RDMA-based messaging module of one or more of the computing devices in\ncombination with the distribution tables and metadata to concurrently perform any combination of lock-free memory allocations, lock-free memory de-allocations, lock-free memory reads and lock-free memory writes on the shared memory of the distributed\nstorage.\nThe Distributed Storage Controller described herein provides various techniques for applying RDMA messaging to implement a distributed thread-safe and lock-free RDMA based storage system in shared memory distributed across multiple servers in an\nRDMA-based network.  The distributed nature of the shared memory with the ability of the Distributed Storage Controller to perform lock-free operations reduces latency and improves throughput in the RDMA-based network while enabling the Distributed\nStorage Controller to scale to very large numbers of concurrent accesses by multiple networked computing devices.  In addition to the benefits described above, other advantages of the Distributed Storage Controller will become apparent from the detailed\ndescription that follows hereinafter. <BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nThe specific features, aspects, and advantages of the claimed subject matter will become better understood with regard to the following description, appended claims, and accompanying drawings where:\nFIG. 1 illustrates an exemplary data center or other network of servers communicating via any combination of RDMA-enabled switches, routers and direct connections for use with a \"Distributed Storage Controller,\" as described herein.\nFIG. 2 illustrates a general architectural diagram for applying the Distributed Storage Controller to provide an RDMA-based distributed shared memory.\nFIG. 3 illustrates a general architectural diagram for that shows exemplary communication between various nodes of the Distributed Storage Controller.\nFIG. 4 illustrates an exemplary distributed memory structure controlled by the Distributed Storage Controller.\nFIG. 5 illustrates a general flow diagram that illustrates various exemplary techniques for effecting RDMA-based distributed memory allocations.\nFIG. 6 illustrates a general flow diagram that illustrates various exemplary techniques for effecting RDMA-based distributed memory de-allocations.\nFIG. 7 provides a general flow diagram that illustrates various exemplary techniques for effecting RDMA-based reads of shared memory.\nFIG. 8 provides a general flow diagram that illustrates various exemplary techniques for effecting RDMA-based writes to shared memory.\nFIG. 9 illustrates an exemplary process for implementing the RDMA-based RPC messaging.\nFIG. 10 provides a general flow diagram that illustrates various exemplary techniques for effecting RDMA-based reads of a key-value store implemented via a co-processor module of the Distributed Storage Controller.\nFIG. 11 provides a general flow diagram that illustrates various exemplary techniques for effecting RDMA-based writes to a key-value store implemented via a co-processor module of the Distributed Storage Controller.\nFIG. 12 illustrates an exemplary implementation of the RDMA-based Distributed Storage Controller.\nFIG. 13 illustrates an exemplary implementation of the RDMA-based Distributed Storage Controller.\nFIG. 14 illustrates an exemplary implementation of the RDMA-based Distributed Storage Controller.\nFIG. 15 is a general system diagram depicting a simplified general-purpose computing device having simplified computing and I/O capabilities for use in effecting various implementations of the Distributed Storage Controller, as described herein.\n<BR><BR>DETAILED DESCRIPTION\nIn the following description of various implementations of a \"Distributed Storage Controller,\" reference is made to the accompanying drawings, which form a part hereof, and in which is shown by way of illustration specific implementations in\nwhich the Distributed Storage Controller may be practiced.  Other implementations may be utilized and structural changes may be made without departing from the scope thereof.\nSpecific terminology will be resorted to in describing the various implementations described herein, and it is not intended for these implementations to be limited to the specific terms so chosen.  Furthermore, it is to be understood that each\nspecific term includes all its technical equivalents that operate in a broadly similar manner to achieve a similar purpose.  Reference herein to \"one implementation,\" or \"another implementation,\" or an \"exemplary implementation,\" or an \"alternate\nimplementation\" or similar phrases, means that a particular feature, a particular structure, or particular characteristics described in connection with the implementation can be included in at least one implementation of the Distributed Storage\nController.  Further, the appearance of such phrases throughout the specification are not necessarily all referring to the same implementation, and separate or alternative implementations are not mutually exclusive of other implementations.  The order\ndescribed or illustrated herein for any process flows representing one or more implementations of the Distributed Storage Controller does not inherently indicate any requirement for the processes to be implemented in the order described or illustrated,\nand any such order described or illustrated herein for any process flows do not imply any limitations of the Distributed Storage Controller.\nAs utilized herein, the terms \"computing device,\" \"component,\" \"system,\" \"client,\" \"host,\" \"server,\" and the like are intended to refer to a computer-related entity, either hardware, software (e.g., in execution), firmware, or a combination\nthereof.  For example, a component can be a process running on a processor, an object, an executable, a program, a function, a library, a subroutine, a computer, or a combination of software and hardware.  By way of illustration, both an application\nrunning on a server and the server itself can be a component.  One or more components can reside within a process and a component can be localized on one computer and/or distributed between two or more computers.  Further, in the context of the\nDistributed Storage Controller, any of a plurality of networked servers may concurrently or separately act as either or both client computing devices and host computing devices.  The term \"processor\" is generally understood to refer to a hardware\ncomponent, such as a processing unit of a computer system.\nFurthermore, to the extent that the terms \"includes,\" \"including,\" \"has,\" \"contains,\" variants thereof, and other similar words are used in either this detailed description or the claims, these terms are intended to be inclusive in a manner\nsimilar to the term \"comprising\" as an open transition word without precluding any additional or other elements.\n1.0 Introduction:\nIn various implementations, the Distributed Storage Controller provides individual instances of an RDMA-based memory allocation component (e.g., a DLL, executable code, library, etc. that is accessible via an API) that executes from individual\nservers to enable any server in an RDMA-based network to perform allocation and/or de-allocation of shared memory (e.g., memory regions, blocks and slabs) distributed across multiple servers in that network.  Further, the Distributed Storage Controller\nenables each of these servers to perform concurrent self-directed access to the shared memory (e.g., reads, writes, etc.) without requiring the use of a central controller or the like via various sequences of RDMA messages.  Specifically, in various\nimplementations, RDMA-based messaging is enabled by individual instances of an RDMA-based messaging component (e.g., a DLL, executable code, library, etc.) that execute from individual servers to enable any of those servers to access and interact with\nthe shared memory distributed across the network.\nIn various implementations, the Distributed Storage Controller assumes a protected network where all servers have appropriate permissions to access one or more reserved regions of shared memory of other computers for allocating, de-allocating,\nreading and writing to the shared memory.  Otherwise, an additional permission step is performed prior to performing RDMA operations (e.g., registration of nodes via a controller node, etc.) to ensure that each server has appropriate permissions to\naccess shared memory distributed on various storage nodes throughout the network.\nIn various implementations, information such as a placement library, distribution tables, metadata, etc., used by one or more of the networked servers to access and interact with the shared memory is obtained from one of the servers referred to\nherein as a \"controller node.\" One or more controller nodes, or controller node replicas, may be available in the network at any time.  In various implementations, the controller node is also applied to initialize instances of the Distributed Storage\nController and to register nodes in the network for access to the shared memory of the distributed storage provided by the Distributed Storage Controller.  Any server hosting any of the shared memory is referred to herein as a \"storage node\" (also\nreferred to as a host).  Further, servers initiating interactions with the shared memory (e.g., allocations, de-allocations, reads, writes, etc.), are referred to herein as \"client nodes\" (also referred to a client).\nAny of the servers in the RDMA-based network can act as any or all of the different node types (e.g., controller node, storage node, client node, etc.) at any particular time, depending on the functionality being performed by that server.  In\nother words, any particular server can concurrently act as a controller node, a storage node and/or a client node at any particular time with respect to any other servers in the network.  However, for purposes of explanation and discussion, the various\nserver node types will be described herein as if they were implemented on different servers.\nIn various implementations, any of the functionality of Distributed Storage Controller (e.g., memory allocations, reads, writes, etc.) is exposed to arbitrary client applications by individual instances of a co-processor module component (e.g.,\na DLL, executable code, library, etc.) of the Distributed Storage Controller that executes on individual servers.  These instances of the co-processor module component enable clients to run customized applications, processes or subroutines on the\ndistributed storage without requiring those customized applications, processes or subroutines to concern themselves with the fact that the shared memory is distributed across multiple servers.  In various implementations, the co-processor module\ncomponent provides these capabilities by interfacing with the aforementioned RDMA-based memory allocation component, the RDMA-based messaging component, and the RDMA-enabled NIC of the server on which the instance of the co-processor module component is\nexecuting.\nIn other words, the co-processor module component provides customized client applications, processes or subroutines with a simple thread-safe and lock-free interface to a very large pool of memory that can be allocated, de-allocated, read,\nwritten, erased, etc., without requiring those applications, processes or subroutines to perform any of the underlying thread-safe and lock free management of that memory provided by the RDMA-based memory allocation component and the RDMA-based messaging\ncomponent of the Distributed Storage Controller.  As such, the co-processor module component of the Distributed Storage Controller enables customized client applications, processes or subroutines to access the shared memory that is distributed across\nmultiple servers as if that shared memory were local memory on the server accessing that shared memory.\nAdvantageously, the co-processor module component enables client applications to run on the same server on which other processes of the Distributed Storage Controller are being executed, although such processes may also be accessed by remote\nclient applications running on other servers.  For example, the co-processor module component enables client applications to run on any client node while that client node is performing any of the operations described herein.  Consequently, the\nco-processor module component further reduces any latency that would result from remote accesses to the functionality of the Distributed Storage Controller by enabling those client applications to execute on the same physical or virtual machine as\nvarious processes of the Distributed Storage Controller.  In addition, the co-processor module component can aggregate local information on any node, thereby avoiding or reducing any need for remote access.\nAs illustrated by FIG. 1, the Distributed Storage Controller may be configured for use with any desired network configuration including any combination of servers (100, 110, 120 and 130) communicating via any combination of RDMA-enabled routers\n140, switches (150 and 160), and direct connections 170.  Advantageously, the communications and messaging techniques enabled by the Distributed Storage Controller are scalable to any number of networked computers and any number of concurrent\nallocations, de-allocations, reads, writes, etc., on memory distributed on servers throughout the network.\n1.1 System Overview:\nAs mentioned above, the Distributed Storage Controller provides various techniques for applying RDMA messaging to implement a distributed thread-safe and lock-free RDMA based storage system in shared memory distributed across multiple servers in\nan RDMA-based network.\nThe processes summarized above are illustrated by the general architectural diagram of FIG. 2.  More specifically, FIG. 2 illustrates a high-level exemplary architecture of the Distributed Storage Controller that shows various implementations of\nthe Distributed Storage Controller.  However, FIG. 2 is not intended to provide an exhaustive or complete illustration of every possible implementation of the Distributed Storage Controller as described throughout this document.  In addition, any boxes\nand interconnections between boxes that may be represented by broken or dashed lines in FIG. 2 represent alternate implementations of the Distributed Storage Controller described herein.  Further, any or all of these alternate implementations, as\ndescribed herein, may be used in combination with other alternate implementations that are described throughout this document.\nIn particular, a plurality of servers (200, 205, 210, and 215) are configured as a network via RDMA-enabled network communications hardware 220 such as a router, switch, or other networking hardware or infrastructure.  Each of the servers (200,\n205, 210, and 215) may concurrently operate as any or all of a \"controller node\" (e.g., initializing the Distributed Storage Controller, registering servers for access to the shared memory of the distributed storage provided by the Distributed Storage\nController, providing information and metadata for use in interacting with the Distributed Storage Controller, etc.), a \"storage node\" (hosting a portion of the distributed shared memory) and a \"client node\" (e.g., allocations, de-allocations, reads,\nwrites, etc., on the distributed shared memory) with respect to any of the other servers in the network.  In addition, as discussed in further detail below, in various implementations, one or more servers in the network may act as dedicated controller\nnodes (225, 230, and 235) by providing a subset of the functionality of the networked servers (200, 205, 210, and 215).\nIn general, each controller node, whether implemented as a controller node API (240) executing on one of the servers (200, 205, 210, and 215) or as a dedicated controller node (225, 230 and 235) includes the same basic functionality.  In\nparticular each controller node includes, but is not limited to, an initialization module 245, a registration module 250, and an information module 255, all of which can communicate with any other computing devices in the network via its own RDMA-enabled\nNIC 260 and its own local instance of an RDMA-based messaging API module 265.\nIn various implementations, the initialization module 245 component of the controller node initializes the overall Distributed Storage Controller.  For example, in various implementations, a controller node resides at an address that is known by\n(or published to) each of the other nodes of the overall Distributed Storage Controller.  Once the controller node is operational (e.g., fully booted or initialized), each of the other nodes in the network (which may also include one or more replicas of\nthe controller node) contact the controller node to report their status as ready (e.g., operational and available to participate in the overall distributes storage system.  Once all of these the nodes have indicated that they are ready, the controller\nsends a message to all the other nodes that all of the participating nodes are ready.  At this time, the controller node also sends a copy of a current distribution table that defines where all of the memory regions are located, as discussed in further\ndetail in the following paragraphs.\nIn particular, the registration module 250 component of the controller node communicates with each server (200, 205, 210, and 215) and optional dedicated controller nodes (225, 230 and 235) in the network to admit or register each of those\ncomputing devices to access the shared memory of the distributed storage provided by the Distributed Storage Controller.\nFurther, the information module 255 component of the controller node provides a variety of information to servers (200, 205, 210, and 215) and optional dedicated controller nodes (225, 230 and 235).  For example, in various implementations, the\ninformation module 255 provides information that allows any authorized computing device in the network to interact with the Distributed Storage Controller including, but not limited to, a placement library, distribution tables, metadata, etc.\nIn general, the distribution table is a table that maps a virtual memory region to one or more physical memory regions residing on one or more storage nodes.  As discussed in further detail herein, in various implementations, each physical\nmemory region is a 2 GB section of memory that is registered on a particular storage node.  In various implementations, each virtual region is mapped to a collection of multiple memory regions (e.g., multiple replicas) that exist on different storage\nnodes, in different failure domains, so that if a single storage node goes offline or is otherwise unavailable, there are multiple backups of the virtual regions.  When writing (or reading) a particular memory region, those writes (or reads) are to a\nvirtual region and use the distribution table which routes the write or read to one of the multiple corresponding physical memory regions on one of the storage nodes.  Further, because each virtual region is mapped to a collection of multiple memory\nregions that exist on different storage nodes, this enables any particular physical memory region (or entire storage nodes) to be added, lost, removed, swapped, etc., at any time by simply updating the distribution table to refer to a new storage node\nfor a particular memory region.\nWhen accessing or interacting with any shared memory 280 or shared memory structure (e.g., key-value stores, databases, etc.) enabled by the Distributed Storage Controller, client nodes may obtain information including, but not limited to,\naddresses or references to any particular memory or memory structure via a placement library, distribution tables, metadata, etc., from the information module of one or more controller nodes.  In general, the placement library provides a mechanism to\nplace and discover the location of the particular memory structures (e.g., key-value stores, databases, etc.), via a friendly name or other identifier associated with the memory or memory structure.  The placement library, and/or the entire controller\nnode, may be hosted on any server and optionally replicated.\nIn various implementations, each of the servers (200, 205, 210, and 215) and optional dedicated controller nodes (225, 230 and 235) is configured to run an operating system (e.g., 270), which in turn may allow execution of one or more procedures\n275, e.g., subroutines, applications, processes, etc. As mentioned above, an instance of the RDMA-based messaging API module 265 resides on each server (200, 205, 210, and 215) and on each optional dedicated controller node (225, 230 and 235), and is\napplied to enable communications between each computing device in the network, including, but not limited to obtaining information from controller nodes and performing RDMA-based reads, writes and CAS operations on shared memory 280 distributed on any\nserver acting as a storage node in the network via the RDMA-enabled NIC 260 of each server.\nAs mentioned above, portions of the distributed shared memory 280 may be hosted by any or all of the servers (200, 205, 210, and 215) and optional dedicated controller nodes (225, 230 and 235).  In various implementations, the shared memory 280\nof the Distributed Storage Controller is managed and distributed across multiple computing devices by individual instances of an RDMA-Based Memory Allocator API Module 285 that resides on each server (200, 205, 210, and 215).  The RDMA-Based Memory\nAllocator API Module 285 is applied to intercept and handle any memory allocation and de-allocation requests from the operating system 270, procedures 275 and/or individual instances of an optional co-processor module 290 executing on each particular\nserver (200, 205, 210, and 215).\nIn various implementations, whenever any client node writes to any shared memory 280 location, that client first reserves that memory location via a reservation bitmap 295 or the like by flipping a corresponding bit via an RDMA CAS operation. \nIn various implementations, the reservation bitmap 295 is hosted in or in conjunction with the region of the shared memory 280 being addressed by the write operation.  However, the reservation bitmap may optionally be implemented as a global reservation\nbitmap or other data structure (not shown).  In either case, the reservation bitmap 295 is applied to reserve any particular memory slab for writing to a particular memory location within that memory slab by any particular client node.\nThe use of local reservation bitmaps 295 for the shared memory 280 of each server acting as a storage node simplifies the reservation process since any knowledge of the address of any particular shared memory location will inherently provide the\naddress of the corresponding reservation bitmap on the server hosting that shared memory.  However, by using some additional record-keeping functionality (e.g., tracking whether a server is online or otherwise operational, addressing potential\nreservation request races between different servers for the same node, etc.), one or more optional global instances of the reservation bitmap 295 for the entire Distributed Storage Controller (or particular memory structures or portions the distributed\nshared memory) may be hosted on any server and optionally replicated.  In this case, the location or address of the global reservation bitmap can be obtained from one of the controller nodes via the aforementioned placement library.\nAs mentioned above, in various implementations, the co-processor module 290 provides a connection library (e.g., a DLL, executable code, library, etc. that is accessible via an API) that allows any desired functionality (e.g., a key-value store\nor other client application) to access and interact with the memory allocation and shared memory features of the Distributed Storage Controller via thread-safe and lock-free RDMA-based messaging.  In particular, the co-processor module 290 interfaces\nwith either or both the RDMA-based messaging API module 265 and the RDMA-Based Memory Allocator API Module 285, in combination with the RDMA-enabled NIC 260 of the server on which the instance of the co-processor module component is executing to provide\ntransparent access to the underlying functionality of the overall Distributed Storage Controller.\n2.0 Operational Details of the Distributed Storage Controller:\nThe above-described program modules are employed for enabling various implementations of the Distributed Storage Controller.  As summarized above, the Distributed Storage Controller provides various techniques for applying RDMA messaging to\nimplement a distributed thread-safe and lock-free RDMA based storage system in shared memory distributed across multiple servers in an RDMA-based network.  The following sections provide a detailed discussion of the operation of various implementations\nof the Distributed Storage Controller, and of exemplary methods for implementing the program modules and features described in Section 1 with respect to FIG. 1 and FIG. 2.  In particular, the following sections provides examples and operational details\nof various implementations of the Distributed Storage Controller, including: An operational overview of the Distributed Storage Controller; RDMA messaging; Nodes of the Distributed Storage Controller; Co-processor module; Memory regions and allocations\ninto bocks and slabs RDMA-based memory allocations and de-allocations; RDMA-based reads of shared memory; RDMA-based writes to shared memory; RDMA-based RPC messaging; Replicating server nodes across multiple computing devices; and Exemplary\napplications.\n2.1 Operational Overview:\nAs mentioned above, the Distributed Storage Controller provides various techniques for applying RDMA messaging to implement a distributed thread-safe and lock-free RDMA-based storage system in shared memory distributed across multiple servers in\nan RDMA-based network.  In various implementations, this shared memory is allocated from memory regions that are partitioned into memory blocks by a block-allocator component of the RDMA-based memory allocator component of the Distributed Storage\nController.  Further, each allocated memory block is divided into equal sized memory slabs by slab allocator components of the RDMA-based memory allocator component of the Distributed Storage Controller.  In general, the size of memory regions, blocks,\nand slabs may vary based on various considerations, as discussed in further detail herein.  However, in various implementations, each memory slab (each containing a plurality of memory elements, e.g., bits, bytes, or other subsections of the memory slab)\nare limited to a maximum size that can be fully read via a single RDMA read and fully written via a single RDMA write.\nThe shared memory provided by the Distributed Storage Controller is distributed throughout multiple servers in the network.  As a result, no one server is required to handle the load for large volumes of concurrent allocations, de-allocations,\nreads, writes, etc., on the shared memory, thereby reducing server load and latency for interactions with the shared memory.  Therefore, otherwise heavy network traffic for the shared memory is mitigated by the distributed and self-directed nature of\naccess to that shared memory.  Advantageously, the lock-free nature (e.g., no CPU locks) of allocations and accesses to the shared memory improves overall performance of the Distributed Storage Controller by reducing latency associated with large numbers\nof concurrent allocations, de-allocations, reads and writes of the shared memory by multiple networked computers.  Further, the ability of the messaging techniques described herein to use commodity RDMA-based NICs reduces networking costs relative to\nnetworks based on specialized NIC hardware.\nIn addition, networks implemented using the Distributed Storage Controller are very flexible in that any server acting as a client node (e.g., reading and writing to the shared memory and optionally requesting memory allocations and/or\nde-allocations) with respect to one or more storage nodes (e.g. hosting some portion of the distributed shared memory) may concurrently be acting as a storage node or controller node with respect to one or more other client nodes, storage nodes, or\ncontroller nodes.  In other words, the Distributed Storage Controller is easily scalable to multiple simultaneous, concurrent, or sequential requests from servers acting as any or all of a controller node, a storage node, and a client node, thereby\nimproving network performance.\nIn various implementations, the RDMA messaging capability of the Distributed Storage Controller is enabled by a connection library or the like that provides an application programming interface (API) for use of RDMA-based messaging via the\nRDMA-enabled NICs of each server.  This connection library encapsulates various protocols to both establish and maintain connections between the NICs of any number of networked computers.  Any application or process running on any server may access the\nAPI to interface with the Distributed Storage Controller to perform allocations, de-allocations, reads and writes on the shared memory.  The API will then automatically initiate communication between servers via RDMA-enabled NICs to process such memory\nrequests.\nThe use of RDMA-enabled NICs to process allocations, de-allocations, and read and write requests enables the Distributed Storage Controller to apply \"kernel bypass\" techniques that further reduce CPU load on the server for accesses to the shared\nmemory.  Kernel bypass is a concept that is applied to improve network performance by carrying out various operations and memory reads and writes without access or notification to the kernel.  For example, in a typical networking scenario, the kernel\ndecodes network packets, e.g., TCP, and passes the data from the kernel space to \"user space\" by copying it.  The term \"user space\" refers to code which runs outside the kernel (e.g., outside kernel space).  User space typically refers to various\nprograms and libraries that the OS uses to interact with the kernel, such as, for example, software that performs input/output, manipulates file system objects, application software etc. The copy process from kernel space to user space typically involves\nsaving user space process context data and then loading kernel context data.  This step of saving the user space process context data and then loading the kernel process information is typically referred to as context switch.  However, application\ncontext switching has been observed to constitute a large fraction of network overhead, thereby reducing bandwidth and latency performance of computer interconnects.\nThe Distributed Storage Controller adapts various kernel bypass techniques to enable user space applications to communicate with the Distributed Storage Controller, which is adapted to communicate both with user space applications and the NIC on\nthe computing device on which the Distributed Storage Controller is executing.  This process takes the kernel out of the path of communication between the user space process and an I/O subsystem enabled by the Distributed Storage Controller that handles\nnetwork communication via the RDMA enabled NICs.  Advantageously, this configuration eliminates context switching and copies from kernel space to user space.  For example, in various implementations, the Distributed Storage Controller generally transmits\nRDMA reads, writes, and CAS operations via the following order of user space operations: 1.  Make a determination on a local server acting as a client node that an RDMA message is to be transmitted to a remote server acting as a storage node for hosting\nsome portion of the shared memory.  In general, this determination is made in response request to allocate, de-allocate, read or write to the shared memory received or otherwise intercepted by the Distributed Storage Controller; 2.  Apply a local\ninstance of the Distributed Storage Controller on the client node to communicate with the NIC of the client; 3.  Apply the NIC of the client node to communicate with physical networking hardware (e.g., RDMA-enabled switches, routers, etc.) of the network\nto send the appropriate RDMA message (e.g., various sequences of reads, writes and CAS operations) across the network to the NIC of the storage node (e.g., the computer hosting a particular regions, block or slab of the shared memory being allocated,\nde-allocated, read, written, etc.); and 4.  In response to receipt of the RDMA message by the NIC of the storage node, applying that NIC to perform the requested read, write, or CAS operation on memory of that storage node via DMA operations between that\nNIC and the memory on that storage node.  In other words, each server performs local DMA operations (e.g., reads, writes, and CAS operations) in response to RDMA messages received by its NIC from other servers.\nFor example, in various implementations, an application, process, or thread executing on one of the networked client nodes initiates a request to perform an allocation, de-allocation, read or write on the shared memory.  In various\nimplementations, the Distributed Storage Controller receives or otherwise intercepts that request, either directly or via the aforementioned co-processor module, and performs various sequences of RDMA reads, write, and CAS operations to implement the\nrequested memory operation.\nAdvantageously, the actual process, thread, or application making the read or write request may not necessarily be aware of, or care, that the distributed shared memory enabled by the Distributed Storage Controller exists on a different server\n(or multiple servers) in the network.  That process, thread, or application simply makes use of the memory references obtained from any source (e.g., controller node, co-processor module, custom client application, etc.) to perform any desired operation\non that memory via the various components of the Distributed Storage Controller.  In other words, various combinations of RDMA reads, writes and CAS operations are used in combination with references to the address of particular memory locations to\nallocate, de-allocate, read and/or write to that memory.  As such, the Distributed Storage Controller provides these networked servers with fast lock-free access to a shared memory distributed within a significantly larger memory space (across multiple\nservers) than would be possible by accessing only local server memory.\n2.2 RDMA Messaging:\nAs mentioned previously, remote direct memory access (RDMA) is a direct memory access from the memory of one computer into the memory of a different computer that is performed via the RDMA-enabled network interface controllers (NICs) of those\ncomputers without involving the operating system of either computer.  In other words, RDMA operations are performed directly by the NICs reading and writing to server memory without any interruption or notification to the CPU of either server.  For\nexample, the NIC of a client node (or controller node) sends an RDMA message or request to the NIC of the storage node.  Any storage node receiving an RDMA message or request then applies its own NIC to interact with its own local memory via\ncorresponding local direct memory access (DMA) operations between the NIC of that storage node and the memory of that storage node to service the received RDMA request.  However, for purposes of explanation, the following discussion will simply refer to\nthe use of RDMA messages or requests sent between various servers to implement a distributed thread-safe and lock-free RDMA based storage system in shared memory distributed across multiple servers in an RDMA-based network.\nWhenever any RDMA message is transmitted across the network between any two NICs that RDMA message includes a message checksum value computed by the sending NIC and verified on receipt by the receiving NIC.  In particular, the receiving NIC\nre-computes the message checksum from the received message contents and compares that re-computed checksum to the checksum in the received RDMA message.  If the checksums match, then the RDMA message is presumed to have been received correctly.  If the\nchecksums do not match, then it is assumed that the RDMA message has been somehow corrupted during the transmission from the sender to the receiver.\nIn various implementations, the Distributed Storage Controller makes use of various RDMA verbs to enable reads and writes on the shared memory and to provide RDMA-based allocations and de-allocations of the shared memory.  For example, in\nvarious implementations, these RDMA verbs include, but are not limited to, various sequences of RDMA writes, RDMA reads, and RDMA atomic compare and swap (CAS) operations.\nRDMA reads enable any client node to read a section of memory of any storage node via the RDMA NICs of those servers without the operating system of the storage node being aware of that memory read.  Similarly, RDMA writes enable any client node\nto write to a section of memory of any storage node via the RDMA NICs of those servers.  As with RDMA reads, RDMA write operations are performed with no notification to the CPU of the storage node.  Further, depending on the processes being performed by\nthe client node, the CPU of the client node may or may not be notified or involved in portions of any reads or writes to any storage node.\nThe RDMA atomic compare and swap (CAS) operation is used to atomically compare a value in the RDMA CAS message from a first server to a value of a specified virtual address of a second server.  If the compared values are equal, a value specified\nby the RDMA CAS message will be stored at the virtual address of the second server as an atomic transaction.  In other words, in an atomic transaction, a series of database operations either all occur, or nothing occurs.  A guarantee of atomicity\nprevents updates to the memory address occurring only partially.  Consequently, the RDMA transaction is not observed to be in progress by the receiving server because at one moment in time, it has not yet happened, and at the next moment, it has already\noccurred in whole (or nothing happened if the transaction failed due to a mismatch of the sent value and the value held by the receiving server).\nIn various implementations, the Distributed Storage Controller applies RDMA CAS operations, in combination with metadata obtained via RDMA reads of particular blocks, regions and/or slabs of the shared memory, to reserve particular memory slabs\nprior to performing RDMA writes to that memory, and to allocate and/or de-allocate particular memory regions, memory blocks and memory slabs for use by the Distributed Storage Controller by modifying metadata block headers and free slab maps of the\nallocated memory via RDMA CAS operations on those block headers and free slab maps, as discussed in further detail in the following paragraphs.\nFurther, a remote procedure call (RPC) is an inter-process communication that allows a computer program, process, or thread running on one computer to cause a subroutine or procedure to execute in an address space of a different computer on a\nshared network.  Typically, to execute an RPC, one server sends an RPC request to another server.  The RPC request includes an ID of a particular procedure (e.g., a subroutine, application, process, etc.) to be executed by the server and one or more\noptional parameters to be used for execution of that procedure.  In various implementations, a \"cookie\" or the like may also be sent along with the ID.  In general, this cookie is an identifier (e.g., number, name, etc.) that uniquely identifies the\nserver sending the RPC request in a way that enables the original message sent by that server to be matched to the response that is eventually received by the sending server from the server that originally received the RPC request.\nAs mentioned above, in various implementations, the Distributed Storage Controller performs allocations, de-allocations, reads and writes on the shared memory using RDMA-based messaging.  As such, these memory allocations and accesses are\nperformed without involving the CPU of the server hosting the shared memory.  However, in various implementations, the Distributed Storage Controller combines RDMA messaging and RPC requests to perform various functions, including, but not limited to,\nretrieval of information from controller nodes (e.g., server cluster information, metadata, placement libraries, distribution tables, etc.), replication of controller nodes, storage nodes, and/or particular memory locations or memory structures, etc. In\naddition, in various implementations, various combinations of RDMA messaging and RPC requests is applied to perform memory allocations, de-allocations, reads and/or writes on the shared memory.  In such cases, the CPUs of the client and host servers only\nbecome involved when they access the shared memory in response to an RPC request or execute a memory allocation or de-allocation request specified by an RPC request transmitted between the client and the host servers via RDMA-based messaging.\nFIG. 3 illustrates a general architectural diagram for that shows an exemplary configuration and communication between various nodes of the Distributed Storage Controller.  FIG. 3 is not intended to provide an exhaustive or complete illustration\nof every possible implementation of the Distributed Storage Controller as described throughout this document.  In addition, any boxes and interconnections between boxes that may be represented by broken or dashed lines in FIG. 3 represent alternate\nimplementations of the Distributed Storage Controller described herein.  Further, any or all of these alternate implementations, as described herein, may be used in combination with other alternate implementations that are described throughout this\ndocument.\nFor example, as illustrated by FIG. 3, one or more client nodes 300 are in communication with one or more storage nodes 305 and one or more controller nodes 310 via RDMA-enabled NOCs (e.g., 335, 340) of each of these nodes.  As illustrated, both\nthe client node 300 and the storage nodes 305 register via the controller node 310 (via NICs of each node) so as to become authorized to access the shared memory of other nodes in the network.  In addition, the client node 300 may contact the controller\nnode 310 in order to retrieve server cluster information (e.g., names and/or addresses of servers (e.g., controller nodes, storage nodes, and client nodes) registered to participate in the shared memory of the Distributed Storage Controller, memory\ndistribution tables, metadata, etc.).\nIn the example illustrated in FIG. 3, client node 300 includes an arbitrary client application 315 that interfaces with the functionality of the Distributed Storage Controller via a co-processor module 320.  The co-processor module 320\nintercepts various memory requests (e.g., write requests, read requests, custom memory access requests, memory allocation requests, memory de-allocation requests, etc., from the client application 315 and passes those requests to either or both an\nRDMA-based messaging module 325 and an RDMA-based memory allocator module 330.\nThe RDMA-based messaging module 325 and/or the RDMA-based memory allocator module 330 then apply the RDMA-enabled NIC 335 of the client node 300 to pass various sequences of RDMA reads, RDMA writes, RDMA CAS operations, RDMA-based RPC messages,\nRPC messages, etc., to the RDMA-enabled NIC of one or more storage nodes 305 in order to service the original memory access request of the client application 315.  The RDMA-enabled NIC of those storage nodes 305 then perform various sequences of local\nDMA operations (e.g., DMA reads, DMA writes, and DMA CAS operations) corresponding to the RDMA messages received from the client node 300 on local shared memory 345 of the storage nodes 305.\nIn the case that RPC messages are received by the storage nodes 305, either via RDMA writes of RPC messages to a message buffer or as a direct RPC message from the client node 300, an RPC request processing module is applied to perform whatever\ntask is requested by the RPC message.  In this case, unlike processing of RDMA messages by the NIC 340, the CPU of the storage nodes 305 may become involved in servicing those RPC messages.\n2.3 Nodes of the Distributed Storage Controller:\nAs mentioned previously, the Distributed Storage Controller includes, but is not limited to, a collection of servers, any or all of which can concurrently operate as any combination of controller nodes, storage nodes and client nodes.\n2.3.1 Controller Nodes:\nIn general, the controller node exists at an address that is known by or published to all of the nodes of any type.  Further, the controller node maintains a current operational status of each node (e.g., storage nodes, client nodes, other\ncontroller nodes, and replicas of any type of node) and is capable of communicating with and being addressed by all of the nodes of any type via various combinations of RDMA and/or RPC messaging.  The overall Distributed Storage Controller can operate\nwith a single primary controller node.  However, in various implementations, additional system redundancy is provided by replicating the functionality and information of the primary controller node to one or more additional controller nodes.  Replicas\nmay be accessed either upon failure of the primary controller node or to relieve congestion on the controller node.\nIn various implementations, controller nodes operate as dedicated servers implementing a controller API module.  Alternately, controller nodes can operate as any combination of controller nodes, storage nodes, and client nodes by executing a\nlocal instance of the controller API module.  Regardless of whether or not the controller node is implemented as a dedicated server or as a component of a storage node and/or client node, on startup, the controller node performs operations including, but\nnot limited to, evaluating the network to ensure that there are a sufficient number of networked computing devices available to support the Distributed Storage Controller and that RDMA-based communications with those machines are functioning within\nexpected parameters.\nIn general, the functionality of the controller node includes, but is not limited to, keeping track of which nodes (and replicas) in the cluster are healthy, maintaining current versions of the distribution tables that are up to date with which\nmachines are healthy, providing the distribution tables to any node that requests this information, and saving and distributing any additional information that is intended to be globally or highly accessible to the various nodes.  A simple example if\ninformation that is intended to be accessible is the mapping of a B-Tree name to an address of a head node of that B-Tree in shared memory hosted by the various storage nodes.  In various implementations, the Distributed Storage Controller applies\nvarious protocols including, but not limited to, Paxos or ZooKeeper for information that is intended to be globally or highly accessible.  In general, Paxos is a well-known family of protocols for solving consensus in a network of unreliable processors. \nSimilarly, ZooKeeper is well-known software that provides an open source distributed configuration service, synchronization service, and naming registry for large distributed systems.\n2.3.2 Storage Nodes:\nIn general, storage nodes host portions of the shared memory of the Distributed Storage Controller.  There are typically many storage nodes in the network, depending on the size of the network and the desired amount of shared memory that is to\nbe made available.  Typically, every storage node is a different physical machine.  In various implementations, the Distributed Storage Controller uses virtual machines for implementation of storage nodes.  However, physical machines tend to be faster\nthan virtual machines for implementing the storage nodes.\nIn various implementations, the shared memory hosted by storage nodes is allocated and de-allocated by instances of RDMA-based memory allocation modules that can execute from any node (e.g., controller nodes, storage nodes, client nodes, etc.)\nin the network to perform lock-free RDMA-based allocations and de-allocations of memory on any storage node in the network.  Further, this shared memory may be replicated across multiple computing devices across the network for redundancy and reduction\nof network congestion.  Further, the co-processor module component of the Distributed Storage Controller may also execute on storage nodes.\n2.3.3 Client Nodes:\nIn general client nodes apply instances of RDMA-based messaging modules that can execute from any node (e.g., controller nodes, storage nodes, client nodes, etc.) to communicate with any controller nodes and any storage nodes hosting any portion\nof the shared memory to perform lock-free reads and writes to the shared memory of the Distributed Storage Controller.  In addition, client nodes apply instances of RDMA-based memory allocator modules that can execute from any node to perform allocations\nand de-allocations of shared memory on any storage node in the network.  In addition, as discussed in further detail below, in various implementations, client nodes may include instances of a co-processor module that provides an interface to any or all\nof the functionality of the Distributed Storage Controller for use by any client application, procedure, process, subroutine, thread, etc. Further, a client node can optionally be run on computers that are not inside of a Distributed Storage Controller\ncluster but are connected to that cluster via a network that supports RDMA.\n2.4 Co-Processor Module:\nAs mentioned above, in various implementations, client nodes include a co-processor module that provides a connection library (e.g., a DLL, executable code, library, etc. that is accessible via an API) that allows any desired functionality\n(e.g., a key-value store or other client application) to access and interact with the memory allocation and shared memory features of the Distributed Storage Controller via thread-safe and lock-free RDMA-based messaging.  For example, in various\nimplementations, the co-processor module provides a wrapper that enables the Distributed Storage Controller to run client applications or code within the actual cluster, thereby reducing latency of access to the shared memory of the cluster.  Examples of\nsuch client applications or code include, but are not limited to map-reduce jobs, Spark jobs, search index serving jobs, machine-learning jobs, etc.\nIn other words, in various implementations, the shared memory of the Distributed Storage Controller is managed by an RDMA-based memory allocator that can be addressed by customized client applications via the co-processor module.  After any\nmemory has been allocated for use by the Distributed Storage Controller, the Distributed Storage Controller enables any node, and any co-processor-based application executing on any node, to make use of that shared memory via thread-safe and lock-free\naccesses to the shared memory via various sequences of RDMA messaging enabled by the RDMA-based messaging module.  For example, an implementation of a key-value store enabled by the co-processor module is described in Section 2.11 of this document.\n2.5 Memory Regions and Allocations into Blocks and Slabs:\nAs mentioned above, multiple servers (e.g., storage nodes) in the network host one or more memory regions of the distributed shared memory that is reserved for and accessible by the various components of the Distributed Storage Controller. \nReserving these memory regions for use by the Distributed Storage Controller ensures that any authorized servers in the network, or other processes executing on those servers, do not write to memory regions of the reserved memory except via the various\ncomponents of the Distributed Storage Controller, thereby avoiding conflicts or corruption of memory managed by the Distributed Storage Controller.  The reserved memory on any particular storage node is then only accessed by the Distributed Storage\nController via DMA read, write, and CAS operations on that memory by the NIC of that storage node in response to RDMA messages sent by the NIC of some client node.\nIn general, each memory region is defined as a contiguous section of memory that is reserved for use by the Distributed Storage Controller.  In various implementations, a memory region size of 2 GB was used, with one or more of these regions\nbeing reserved on any of the networked servers.  However, memory regions of any desired size may be used based on the techniques described herein.  These reserved memory regions are directly addressable by RDMA-enabled NICs in response to RDMA reads,\nwrites and CAS operations transmitted and received by the RDMA-enabled NICs of each server.\nFurther, each individual memory region contains a section of memory (e.g., one or more memory blocks) reserved for metadata in the form of block headers and a free slab map.  In various implementations, the size of block header and the size of\nthe free slab maps may be the same sizes as the blocks into which each region is divided.  However, there is no requirement that block headers be the same size as free slap maps, or that either of these are the same sizes as the blocks in the same memory\nregion.  In addition, depending on memory region size and the amount of metadata in block headers and free slab maps, multiple block header sections and multiple free slab maps may be included in any particular memory region.\nIn various implementations, the metadata of the block headers comprises information including, but not limited to, an indication of whether each block in a particular memory region is free or allocated, slab sizes for each allocated block, a\ncount of the number of allocated slabs (or a count of the number of free slabs) in each allocated block of the memory region, a reservation bitmap for reserving particular memory slabs for writes, etc. The free slab map comprises metadata including, but\nnot limited to, a bitmap or other metadata, indicating whether each individual slab in each of the memory blocks of a particular memory region are free or allocated (e.g., \"0\" if free, or \"1\" if used or allocated for any particular slab).\nIn various implementations, the remainder of each memory region (other than the metadata sections) is divided into equal sized blocks by block allocators of the RDMA-Based Memory Allocator.  However, the block size in different memory regions\nmay be different from that of other memory regions, depending on system configuration.  Similarly, in various implementations, each memory block is divided into equal sized memory slabs by slab allocators of the RDMA-based memory allocator component of\nthe Distributed Storage Controller.  However, the slab size in different blocks may be different from that of other blocks, depending on system configuration.  As mentioned above, in various implementations, each the size of each slab is limited to a\nmaximum size the enable the entire slab to be read via a single RDMA read operation or written via a single RDMA write operation.\nIn various implementations, the block allocator component of the RDMA-based memory allocator component reserves two or more of the first blocks in its memory region for metadata.  The first group of blocks, which are at least one block, contain\nthe block headers for the entire memory region.  These block headers contain the shared information needed for the RDMA-based memory allocator to perform lock-free operations.  The next group of blocks is the free slab map for all of the allocatable\nblocks in the memory region.  The amount of blocks used for this metadata is based on the number of blocks, block size and the maximum number of slabs in a block.  In other words, memory blocks are allocated from the memory region by block allocator\ncomponents of the RDMA-based memory allocator component of the Distributed Storage Controller.\nEach block allocator is responsible for allocating and de-allocating blocks from within a particular region.  The block allocator is also responsible for setting a block's size and marking a block as un-used (via the block header associated with\nthat block) when there are no more used slabs in a block.  In various implementations, the block allocator applies a \"best fit\" allocation strategy to find space in the next appropriately sized slab and/or block.  This results in a trade-off between an\nincrease in internal fragmentation and latency of the system.\nThese block allocators are instantiated as needed by the RDMA-based memory allocator component of the Distributed Storage Controller and set the size of the blocks being allocated from the memory region.  In various implementations, block size\nwas set to 1 MB.  However, there is no requirement that blocks be any particular size, and as such, block size can be any desired size up to the entire size of the memory region (less the portion of the memory region reserved for metadata).\nSimilarly, memory slabs are allocated from memory blocks by slab allocator components of the RDMA-based memory allocator API.  These slab allocators are instantiated as needed by any block allocator associated with a particular memory region. \nIn general, slabs are the smallest unit in the allocator and it is the actual individual allocation unit that can be allocated for memory reads and writes.  As noted above, the size of slabs may differ between different blocks, but is typically the same\nsize within individual blocks.\nFor example, in various implementations, possible slab sizes are set anywhere between one byte and one megabyte (or whatever the maximum block size has been set to, with intermediate sizes of increasing powers of 2 (e.g., 2 bytes, 4 bytes, 8\nbytes, 16 bytes, 32 bytes, 64 bytes, 128 bytes, 256 bytes, etc., with a maximum slab size corresponding to maximum RDMA read and write sizes enabled by the NIC hardware).  Allocating the same size slabs within individual blocks makes it a simple matter\nto compute references to allocated slabs by simply determining an offset based on the slab count in the block.  However, by including additional metadata in either the free slab map or the block header to allow determination of references to reserved\nslabs, the slabs can be set at any desired size within blocks.\nFIG. 4 illustrates an exemplary distributed memory structure of memory regions divided into a metadata section including block headers for use by the various components of the Distributed Storage Controller.  For example, as illustrated by FIG.\n4, the RDMA-based memory allocator API module 285 (also shown as corresponding element 285 in FIG. 2) instantiates one or more block allocators 400 and slab allocators, on as as-needed basis.  As noted above, the block allocators 400 allocate and\nde-allocate memory blocks from contiguous regions of memory.  Further, block allocators 400 do not need to reside on the computing device for which they are allocating and de-allocating blocks.  Similarly, slab allocators 405 are instantiated when needed\nby block allocators 400 for allocation and de-allocation of memory slabs of individual memory blocks.  As with the block allocators 400, the slab allocators 405 do not need to reside on the computing device for which they are allocating and de-allocating\nslabs.\nFurther, as illustrated by FIG. 4, the RDMA-based memory allocator API 285 resides on one or more networked computing devices or servers (410, 415, and 420).  One or more of these networked computing devices or servers (410, 415, and 420) hosts\none or more memory regions (425, 430, 435) that is reserved for use by the Distributed Storage Controller.  Each memory region (425, 430, 435) is a contiguous section of RDMA addressable memory on any of the networked computing devices.  Further, each\nmemory region (425, 430, 435) includes block headers 440 and a free slab map 445.\nIn addition, as illustrated by FIG. 4, in various implementations, each memory region (425, 430, 435) is divided into one or more equal sized memory blocks (450, 455, 460, 465).  Different memory regions (425, 430, 435) may be divided into\ndifferent sized memory blocks (450, 455, 460, 465) by corresponding block allocators 400.\nFinally, as illustrated by FIG. 4, in various implementations, each memory block (450, 455, 460, 465) is divided into one or more equal sized memory slabs (470, 475, 480).  Different memory blocks (450, 455, 460, 465) may be divided into\ndifferent sized memory slabs (470, 475, 480) by corresponding slab allocators 405.  In various implementations, a portion of each slab is reserved for metadata 485.  In various implementations, this metadata 485 includes an optional checksum computed\nfrom each memory element (e.g., bits, bytes, or other subsections) of each memory slab (470, 475, 480).  As discussed in further detail herein, in various implementations, this slab checksum is applied to verify integrity of the data in the memory slab\n(470, 475, 480) at the level of the entire slab when reading the slab.  Further, as also discussed in further detail herein, in various implementations, this checksum is re-computed by the Distributed Storage Controller and written to the slab as a part\nof any write to any memory element of the slab.  The use of the checksum in this manner enables the Distributed Storage Controller to perform concurrent lock-free reads and writes to memory slabs, using the aforementioned reservation bitmap for reserving\nparticular memory slabs for writes.\n2.6 RDMA-Based Memory Allocations and De-Allocations:\nShared memory is distributed and managed across multiple computing devices by the aforementioned RDMA-based memory allocator component of the Distributed Storage Controller (e.g., see FIG. 2 (RDMA-Based Memory Allocator API Module (285), FIG. 4,\nand FIGS. 5 and 6, discussed below).  The following paragraphs describe this RDMA-based memory allocation process.\nIn general, the RDMA-based memory allocator component of the Distributed Storage Controller applies remote direct memory access (RDMA) messaging to provide fast lock-free memory allocations and de-allocations for shared memory distributed across\nmultiple servers in an RDMA-based network.  Alternately, in various implementations, the RDMA-based memory allocator component of the Distributed Storage Controller combines RDMA messaging and remote procedure call (RPC) requests to provide fast\nlock-free memory allocations and de-allocations for shared memory distributed across multiple servers in an RDMA-based network.  In either case, any of the networked servers can act as either or both a client node for requesting (or releasing) memory\nallocations and a host or storage node for hosting a portion of the distributed memory.  Further, any server (including the requesting client node and controller nodes) may act as the host for the distributed memory being allocated or de-allocated by any\nclient node via RDMA messaging.\nMemory allocations and de-allocations are accomplished via a distributed memory allocator comprising multiple instances of block allocators and slab allocators that are instantiated when needed by the block allocators.  The block allocators and\nslab allocators generally apply RDMA read messages to determine status of particular memory blocks and memory slabs as used or free from metadata (e.g., block headers and free slab maps) associated with each memory block.  In addition, the block\nallocators and slab allocations apply RDMA atomic compare and swap (CAS) messages to allocate or de-allocate those blocks and/or slabs based on the metadata retrieved via the RDMA read messages.  In other words, block allocators perform allocations and\nde-allocations in combination with slab allocators via a sequence of RDMA read and CAS messages transmitted between servers via RDMA-enabled NICs to read and modify a block header and a free slab map associated with each memory block.  After memory slabs\nhave been allocated using this process, any client node can read or write to any allocated slab hosted by any storage node by applying RDMA reads and writes directed to a reference to the allocated slab.  More specifically, a client node NIC sends an\nRDMA message to the NIC of a storage node.  The storage node then performs corresponding DMA operations on its own local memory to complete the RDMA request of the client node.\nFIG. 5 illustrates various implementations of the RDMA-based memory allocation features of the RDMA-based memory allocator component of the Distributed Storage Controller of the Distributed Storage Controller.  Furthermore, while the system\ndiagram of FIG. 5 illustrates a high-level view of various implementations of the Distributed Storage Controller, FIG. 5 is not intended to provide an exhaustive or complete illustration of every possible implementation of the Distributed Storage\nController as described throughout this document.  In addition, any boxes and interconnections between boxes that may be represented by broken or dashed lines in FIG. 5 represent alternate implementations of the Distributed Storage Controller described\nherein.  Further, any or all of these alternate implementations, as described herein, may be used in combination with other alternate implementations that are described throughout this document.\nIn general, as illustrated by FIG. 5, the memory allocation processes enabled by the RDMA-based memory allocator component of the Distributed Storage Controller begin operation by receiving 505 a memory allocation (malloc) request from a client\n500, and determining a memory size for an appropriate slab size for that malloc request.  The size of this malloc request is generally based on a request by some application, process, thread, etc., of the client node 500 for some particular amount of\nmemory.  Although not a requirement, in various implementations, slab size is generally set to be equal to or less than the maximum size that can be read or written via a single RDMA read or write, thus enabling entire slabs to be read or written with a\nsingle RDMA read or write operation.  The RDMA-based memory allocator component then selects 510 a particular host and/or memory region from which to service the malloc request of the client 500.  In response to the client malloc request, the RDMA-based\nmemory allocator component performs 515 an RDMA read of the portion of the selected memory region that contains the block headers for that memory region.  The RDMA-based memory allocator component then evaluates 520 these block headers to determine if\nthat memory region of the host contains any allocated blocks of appropriately sized memory slabs (e.g., slabs of a size that are suitable for the intended malloc request).\nAssuming that suitably sized memory slabs are available in one or more blocks of the selected memory region, the RDMA-based memory allocator component then performs 525 an RDMA CAS operation on the block header of a selected one of those blocks\nwith suitably sized slabs (referred to as an \"appropriate block\").  This CAS operation serves to update the block header of the appropriate block to indicate that an additional slab is being used in that block.  In other words, the count of free slabs in\nthe appropriate block is decreased by one via successful completion 530 of this RDMA CAS operation on the block header.  In the case the multiple appropriate blocks exist for a particular slab size within the selected memory region, in various\nimplementations, selection of the particular appropriate block is based on various selection methodologies, including, but not limited to, \"best fit\" allocation strategies, random selection, selection based on block address order, etc.\nFurthermore, upon successful completion of the RDMA CAS to update the appropriate block header to decrement the number of free slabs, the RDMA-based memory allocator component then performs 535 an RDMA read of the free slab map of the selected\nmemory region.  Next, given the free slab map, the RDMA-based memory allocator component selects one of the free slabs in the appropriate block and performs 540 an RDMA CAS operation on the free slab map to update the free slab map to show the selected\nslab as being allocated (e.g., change 0 to 1 in the free slab map to indicate that the selected free slab is now an allocated slab).  In other words, if successful 545, this CAS operation on the free slab map reserves a selected free slab of the\nappropriate block by updating the corresponding entry in the free slab map.  In response to a successful 545 reservation of the slab via the CAS message, the RDMA-Based Memory Allocator API calculates 550 a reference to the allocated slab and provides\nthat reference to the client 500, thus completing the malloc request.\nAs mentioned above, the client evaluates 520 block headers received via the an RDMA read 515 of the block headers of the selected host to determine if the host contains any blocks of appropriately sized memory slabs.  In the case that suitably\nsized memory slabs are not available in one or more blocks of the host, the client further evaluates 555 those block headers to determine whether the host holds any free or unallocated blocks (e.g., memory blocks that have not yet been reserved for a\nparticular slab size, meaning that the block has not yet been divided into slabs, and is therefore available for allocation).  In this case, assuming that the host contains one or more free blocks, the client performs 560 an RDMA CAS operation on the\nblock header of a selected one of the free blocks to reserve or allocate that block.  If successful 565, this CAS operation configures the selected free block for the appropriate slab size by updating the metadata in the corresponding block header to\nspecify the slab size for that block and to decrement the number of free slabs in that block by one.  Once this previously free block has been configured for the appropriate slab size, the RDMA-based memory allocator component then proceeds to reserve\none of the slabs in the newly configured block via the above-described sequence of performing 535 the RDMA read of the free slab map, performing 540 the RDMA CAS operation on the free slab map, and calculating 550 and providing the corresponding\nreference to the client 500 to complete the malloc request.\nAs mentioned above, following the evaluation 520 to determine whether the host contains blocks of appropriately sized slabs, the RDMA-based memory allocator component performs 525 a CAS operation on the block header of a selected appropriate\nblock.  However, in the event that this CAS operation is not successful 530 for some reason (e.g., prior reservation via a malloc request by some other client), in various implementations, the RDMA-based memory allocator component restarts the malloc\nprocess by selecting 510 a new host for the malloc request.  However, memory is typically allocated at relatively high frequencies in the host in response to requests by the same or other clients.  Consequently, in various implementations, rather than\nselecting 510 a new host, the RDMA-based memory allocator component restarts the original malloc request on the same host by performing 515 a new RDMA read of the block headers for the originally selected memory region.  In either case, as illustrated by\nFIG. 5, the restarted malloc process then continues as discussed above.\nSimilarly, if the evaluation 555 to determine whether the host contains any free or unallocated blocks indicates that no unallocated blocks are available, in various implementations, the RDMA-based memory allocator component restarts the malloc\nprocess by either selecting 510 a new host for the malloc request, or repeating the performance 515 of the RDMA read of the memory block headers of the selected host.  As illustrated by FIG. 5, the restarted malloc process then continues as discussed\nabove.\nSimilarly, as discussed above, under various circumstances, the RDMA-based memory allocator component performs 560 an RDMA CAS operation on the block header of a selected one of the free blocks to reserve or allocate that block.  However, in the\nevent that this CAS operation is not successful 565 for some reason (e.g., prior reservation via a malloc request by some other client), in various implementations, the RDMA-based memory allocator component restarts the malloc process by either selecting\n510 a new host for the malloc request, or repeating the performance 515 of the RDMA read of the memory block headers of the selected host.  As illustrated by FIG. 5, the restarted malloc process then continues as discussed above.\nThe system diagram of FIG. 6 illustrates various implementations of the memory de-allocation features of the RDMA-based memory allocator component of the Distributed Storage Controller, as described herein.  Furthermore, while the system diagram\nof FIG. 6 illustrates a high-level view of various implementations of the Distributed Storage Controller, FIG. 6 is not intended to provide an exhaustive or complete illustration of every possible implementation of the Distributed Storage Controller as\ndescribed throughout this document.  In addition, any boxes and interconnections between boxes that may be represented by broken or dashed lines in FIG. 6 represent alternate implementations of the Distributed Storage Controller described herein. \nFurther, any or all of these alternate implementations, as described herein, may be used in combination with other alternate implementations that are described throughout this document.  In addition, the de-allocation processes illustrated by FIG. 6 may\nbe combined with the allocation processes illustrated by FIG. 5 to perform concurrent and ongoing memory allocations and de-allocations for shared memory distributed across the network.\nIn various implementations, the memory de-allocation process begins with the RDMA-based memory allocator component of the Distributed Storage Controller receiving 605 or otherwise intercepting a de-allocation request from a client 600.  This\nde-allocation request is directed to a particular memory slab on a particular host.  In response to the de-allocation request, in various implementations, the RDMA-based memory allocator component performs 610 an RDMA CAS operation on the entry in the\nfree slab map relating to the particular slab that is being de-allocated to update the free slab map to mark that slab as being free (e.g., change 1 to 0 in the free slab map to indicate that the selected allocated slab is now a free slab).  In the case\nthat this RDMA CAS operation is not successful 615, then the particular slab is already marked as free (e.g., in response to a prior de-allocation request from some other client or process in the network), and the de-allocation request is complete 625. \nHowever, in the event that this CAS operation is successful 615, in various implementations, the RDMA-based memory allocator component performs 620 an RDMA CAS operation on the block header for the block in which the memory slab is being de-allocated to\nshow one additional slab as being free in that block (e.g., increment the number of free slabs by 1 in the corresponding block header).  At this point, the de-allocation request is complete 625.\nIn various implementations, prior to performing the initial CAS operation 610, the RDMA-based memory allocator component first performs 630 an RDMA read of the free slab map for the block in which the memory slab is being de-allocated.  The\ninformation in the free slab map is then used to perform the aforementioned CAS operation 610 on the free slab map.\nAs noted above, in the case that the RDMA CAS operation 610 is successful 615, the RDMA-based memory allocator component can perform the aforementioned RDMA CAS operation 620 on the block header to complete 625 the de-allocation request. \nHowever, in various implementations, the RDMA-based memory allocator component performs additional operations to determine whether the corresponding block contains any allocated slabs following the de-allocation, and whether that entire block can\ntherefore be marked as free in the corresponding block header.\nIn particular, following successful 615 completion of the RDMA CAS operation 610 on the free slab map, in various implementations, the RDMA-based memory allocator component performs 635 an RDMA read of the block header for the block in which the\nmemory slab is being de-allocated.  The RDMA-based memory allocator component then evaluates 640 the block header to determine whether all of the slabs in the block are free (e.g., slab count in the metadata will show zero allocated slabs following\nde-allocation of the particular slab).  In the event that one or more of the other slabs in that block are still allocated, the RDMA-based memory allocator component simply performs 620 the aforementioned RDMA CAS on that block header to show one\nadditional slab as being free to complete 625 the de-allocation request.\nHowever, in the event that the block will have no allocated slabs following de-allocation of the particular slab, the RDMA-based memory allocator component instead performs 645 an RDMA CAS operation on the block header for the block in which the\nslab is being de-allocated to mark the entire slab as being free.  Upon success 650 of this CAS operation 645, the de-allocation request is complete 625.  However, in the event that this CAS operation 645 fails (e.g., some other client or process\nreserves or allocates an additional slab from the block before the block is freed), the RDMA-based memory allocator component simply performs 620 the aforementioned RDMA CAS operation on that block header to show one additional slab as being free to\ncomplete 625 the de-allocation request.\nIn general, as soon as a previously allocated slab or block of memory is de-allocated, that slab or block immediately becomes available for further allocations by the RDMA-based memory allocator component.  Further, in various implementations,\nin the case that all slabs in a particular block have been de-allocated, the RDMA-based memory allocator component maintains the existing slab size for that block rather than marking the block as free.  This further decreases the latency of memory\nallocations in the case that the existing slab size for that block is of a size that is likely to be frequently used.  However, in cases of uncommon slab sizes, or limited memory resources, it may be more advantageous to mark the entire block as free\nfollowing de-allocation of all slabs in that block.  Generally, the decision to mark blocks as free in such cases will depend on the particular application, network traffic profiles, and the availability of additional free memory regions for further\nallocations.\n2.6.1 Distributed Memory Allocation Considerations:\nAs mentioned above, the RDMA-based memory allocator component of the Distributed Storage Controller performs allocations of shared memory distributed across a network.  Further, references to allocated memory are provided to clients for use in\nreading and writing to allocated memory slabs.  The following paragraphs discuss some of the considerations of various implementations for applying the RDMA-Based Memory Allocator API to perform memory allocations based solely on RDMA messaging. \nFurther, a discussion of memory allocation using combinations of RDMA and RPC is provided below in Section 2.9.\nIn various implementations, the RDMA-based memory allocator component of the Distributed Storage Controller operates under several constraints, none of which is mandatory, for purposes of simplifying overall system configuration and reducing\nboth bandwidth and latency for performing memory allocations.  For example, in various implementations, the RDMA-Based Memory Allocator API enforces a maximum allocation size (e.g., 2 GB memory regions, 1 MB blocks, slab sizes less than or equal to\nmaximum RDMA read and write sizes, etc.) that is specified during initial setup of the Distributed Storage Controller on any particular network.\nIn various implementations, the RDMA-based memory allocator component uses a predetermined or static overhead for allocations.  For example, consider a 2 GB memory region size for allocations.  In this case, the RDMA-based memory allocator\ncomponent may use a static amount of that memory region (e.g., 0.1 GB) for metadata and the remainder (e.g., 1.9 GB) for block allocations.  However, in various implementations, the RDMA-Based Memory Allocator API applies a variable overhead size for\nmetadata based on the size of regions, block sizes, and number of and size of slabs in each block.\nAn additional constraint in various implementations is that once memory has been reserved, that memory is not moved.  This ensures that the allocated memory is accessible via RDMA reads and writes.  Further, in various implementations, the\nRDMA-based memory allocator component does not use any kind of blocking code (e.g., critical sections, wait for single object, etc.) because the underlying kernel code would introduce considerably more latency than RDMA reads, writes, or CAS operations. \nHowever, even though latency may increase, the use blocking code may be applied in various implementations of the Distributed Storage Controller to address particular network considerations for specific applications.\nThe following discussion summarizes communication between two networked computers (e.g., \"Client\" and \"Host\") during an exemplary memory allocation by the RDMA-based memory allocator component of the Distributed Storage Controller.  The\nfollowing sequence of events is not intended to describe or include all of the various implementations of the RDMA-based memory allocator component, and is provided only for purposes of example.  1.  Client (computer A) determines the following\ninformation: where the memory region that it wants to allocate memory from is located (e.g., computer B, Host), and the desired allocation size.  Consider that computer A can be the same computer as computer B, but does not have to be the same as\ncomputer B. In other words, this allocation process may be applied on a single computer acting as both Client and Host, or between different computers.  2.  The Client performs an RDMA read of the block headers in the memory region on the Host.  3.  The\nClient evaluates the block headers and determines one of the following: (a) The memory region of the Host contains an appropriate block from which to allocate the memory slab; (b) There are no appropriate allocated blocks on the Host, but there are one\nor more unallocated blocks on the Host; or (c) There are no appropriate allocated blocks on the Host and there are no unallocated blocks on the Host.  4.  Based on the determination in Step 3, the Client will perform one of the following actions: (a) If\n3(a), then the Client will perform an RDMA CAS operation on the block header of the selected block on the Host, this CAS operation will update the block header to indicate that an additional slab is being used in that block; (b) If 3(b), then the Client\nwill perform an RDMA CAS on the header of an unallocated block on the Host to allocate that block, this CAS operation will initialize all of the metadata in the corresponding block header including indicating that an additional slab is being used in that\nblock; (c) If 3(c), then the Client will fail the allocation.  In response to this failure, several options are possible.  i. Return an allocation error to the Client (or whatever process, thread or application on the Client made the initial malloc\nrequest); ii.  Return to Step 1.  Memory allocations are dynamic.  Therefore, it is possible that one or more blocks or slabs have become available on the Host since the allocation failure; or iii.  Return to Step 2.  Memory allocations are dynamic. \nTherefore, it is possible that one or more blocks or slabs have become available on the Host since the allocation failure.  5.  Assuming that the Client successfully performs Step 4(a) or Step 4(b), the Client will then perform an RDMA read of the free\nslab map of the Host.  6.  In response to Step 5, the Client will evaluate the free slab map to identify an unallocated slab, and will then reserve that unallocated slab via an RDMA CAS operation on the free slab map of the Host to update the\ncorresponding slab entry as being used.  7.  In response to Step 6, the RDMA-based memory allocator component computes a reference to the allocated slab and provides that reference to the Client to complete the malloc operation.\n2.6.2 RDMA-Based Memory De-Allocation Considerations:\nIn the case of memory de-allocations, in various implementations, the RDMA-based memory allocator component again operates with the shared memory distributed across two or more networked computing devices.  The RDMA-based memory allocator\ncomponent intercepts or otherwise receives a de-allocation request for a particular memory slab on a particular one of the networked computing devices acting as a host.  In various implementations, the RDMA-Based Memory Allocator then performs an RDMA\nCAS on a portion of the free slab map corresponding to a particular memory block of the host in which the memory slab is being de-allocated.  This RDMA CAS updates the free slab map to mark the slab being de-allocated as free.  Finally, in various\nimplementations, the RDMA-based memory allocator component performs an RDMA CAS on a memory block header of the particular memory block of the host to update that block header to show one additional slab as being free.\nThe following discussion summarizes communication between two networked computers during an exemplary memory de-allocation by the RDMA-based memory allocator component.  The following sequence of events is not intended to describe or include all\nof the various implementations of the RDMA-based memory allocator component, and is provided only for purposes of example.  1.  Client (computer A) determines the following information: where the memory region that it wants to de-allocate memory from is\nlocated (e.g., computer B, Host).  Consider that computer A can be the same computer as computer B, but does not have to be the same as computer B. In other words, this de-allocation process may be applied on a single computer acting as both Client and\nHost, or between different computers.  2.  The client performs an RDMA read the free slab map for the memory region in which the slab is being de-allocated.  3.  The Client performs an RDMA CAS operation on the free slab map of the Host to mark the slab\nbeing de-allocated as free.  4.  The Client then performs an RDMA read of the block header of the Host for the block in which the slab is being de-allocated.  5.  The Client evaluates the number of reserved or allocated slabs indicated by the block\nheader and determines one of the following case-based scenarios: (a) There are no other slabs in use in the block besides the slab being de-allocated; or (b) There are other slabs being used in the block.  6.  Based on the determination of Step 5, the\nClient will then perform one of the operations: (a) If 5(a), then the Client will perform an RDMA CAS operation on the block header of the Host so as to update the block header to show the entire block as being free (i.e., unallocated); (b) If 5(b), then\nthe Client will perform an RDMA CAS operation on the block header of the Host so as to update the block header to show that there is one less slab being used in the that block.  7.  Following successful completion of Step 6, the de-allocation is complete\nand the previously allocated slab (or block) is available for further allocations.\n2.6.3 Allocations and De-Allocations without Free Slab Maps:\nAs discussed above, memory allocations and de-allocations (and optional replications) may be performed by the RDMA-based memory allocator component of the Distributed Storage Controller via RDMA-based messaging and/or RDMA-based RPC messages. \nIn each of these cases, the preceding discussion referenced the use of a free slab map for allocations and de-allocations of particular memory slabs.  However, in various implementations, slab management (e.g., allocations, de-allocations and\nreplication) is provided without the use of free slab maps by including additional metadata in each block header and memory slab.\nFor example, as with the each of the preceding implementations, the RDMA-based memory allocator component makes use of block header metadata in each memory region.  As discussed above, the block header metadata in the preceding implementations\ncomprises information including, but not limited to, an indication of whether each block in a particular memory region is free or allocated, slab sizes for each allocated block, a count of the number of allocated slabs in each allocated block of the\nmemory region, and a slab reservation bitmap.  However, in implementations where the free slab map is eliminated, the metadata of each of the block headers in a memory region further includes an additional reference to a first free slab of that block.\nIn addition, each slab, including the first free slab in each block, includes its own metadata header with a reference that is pointing to the next free slab in the block, and so on (e.g., a linked list) until the last free slab in the block is\nreached.  This last free slab in the block either has no reference to any other free slab, or simply a null reference.  Further, as discussed above, whenever a block is first allocated, it is divided into a set of equal sized memory slabs.  Consequently,\nin implementations where the free slab map is not used, the metadata in each slab header is initialized during the initial block allocation process so that each slab in the block (which are initially all free) includes an initial reference to some other\nfree slab in that block.\nIn various implementations, the initial references from one free slab to the next are sequential (e.g., slab-1 has a reference to slab-2, which references slab-3, and so on).  However, any order of free slab references to other free slabs may be\napplied, including random orders, so long as all of the slabs in the block are referenced by one other slab in the block, and no slab is referenced by more than one other slab.  Then, during the allocation process, the metadata in the block header\nindicating the next free slab will change with each allocation.  Similarly, during the de-allocation process, both the metadata in the block header and the metadata in the header of the slab being freed will change with each de-allocation.\nThe following discussion summarizes an exemplary block allocation process by the RDMA-based memory allocator component for implementations in which the free slab map is eliminated.  Further, this process may be modified using the techniques\ndescribed herein that apply RDMA-based RPC messages.  The following sequence of events is not intended to describe or include all of the various implementations of the Distributed Storage Controller, and is provided only for purposes of example.  For\nexample, in the case of RDMA-based block allocations for newly allocated blocks, the RDMA-based memory allocator component can perform allocations via the following sequence, which assumes initial sequential slab references for purposes of explanation. \nThis block allocation process does not need to be repeated unless an entire block is de-allocated and then subsequently re-allocated.  1.  Determine the memory region from which to allocate a selected free block and the desired slab size for that block;\n2.  Divide the block into the appropriately sized slabs; 3.  Initialize the references in each slab header to point to the next sequential slab in the block.  More specifically, slab-n includes an initial reference pointing to slab-(n+1) as the next free\nslab, and so on, with the last free slab in the block including a null reference that may change to point to some other free slab in response to subsequent slab allocations and de-allocations.  For example, the slab header of the first slab (e.g.,\nslab-1) is initialized as pointing to slab-2 as its next free slab, while the slab header of slab-2 is initialized as pointing to slab-3 as its next free slab, and so on; and 4.  Initialize the block header of the block being allocated to include a\nreference to the first free slab (e.g., slab-1).\nThe following discussion summarizes an exemplary slab allocation process by the RDMA-based memory allocator component for implementations in which the free slab map is eliminated.  Further, this process may be modified using the techniques\ndescribed herein that apply RDMA-based RPC messages.  The following sequence of events is not intended to describe or include all of the various implementations of the Distributed Storage Controller, and is provided only for purposes of example.  For\nexample, in the case of RDMA-based slab allocations, the RDMA-based memory allocator component can perform slab allocations via the following sequence: 1.  Determine a particular memory region and slab allocation size from which to allocate a slab of\nmemory; 2.  Perform an RDMA read of the block headers in the memory region; 3.  Evaluate the block headers of the memory region to obtain the reference to the first free slab (e.g., slab-a) of a particular block (e.g., a block with slabs of an\nappropriate size); 4.  Perform an RDMA read of the metadata header of slab-a (i.e., the first free slab referenced in the block header); 5.  Evaluate the metadata of slab-a to obtain the reference to the next free slab, e.g., slab-b; 6.  Perform and RDMA\nCAS operation (based on the known reference to slab-a) on the block header to update the metadata of the block header to reference the first free slab as being slab-b. The CAS operation may fail for some reason, e.g., some other slab allocation or\nde-allocation occurred subsequent to step 2, above.  As such, upon failure of the CAS operation, the slab allocation process can restart from either step 1 or step 2, above, and then repeat through step 6 until the CAS operation is successful; and 7. \nReturn to a reference to slab-a to the thread, process, or application requesting the slab allocation.\nThe following discussion summarizes an exemplary slab de-allocation process by the RDMA-based memory allocator component of the Distributed Storage Controller for implementations in which the free slab map is eliminated.  Further, this process\nmay be modified using the techniques described herein that apply RDMA-based RPC messages.  The following sequence of events is not intended to describe or include all of the various implementations of the Distributed Storage Controller, and is provided\nonly for purposes of example.  For example, in the case of RDMA-based slab de-allocations, the RDMA-based memory allocator component can perform slab de-allocations via the following sequence: 1.  Perform an RDMA read of the block header in which a\nparticular slab, e.g., slab-j, is to be de-allocated; 2.  Evaluate the metadata in the block header to obtain the reference to the first free slab in the block, e.g., slab-k; 3.  Perform an RDMA write operation to update the metadata in the header of the\nslab being de-allocated (i.e., slab-j) to reference slab-k as the next free slab in the block; 4.  Perform an RDMA CAS operation (based on the known reference to slab-k) on the block header to update the metadata of that block header to reference the\nfirst free slab as being slab-j. The CAS operation may fail for some reason, e.g., some other slab allocation or de-allocation occurred subsequent to step 1, above.  As such, upon failure of the CAS operation, the slab de-allocation process can restart\nfrom step 1, and then repeat through step 4 until the CAS operation is successful.\n2.7 RDMA-Based Reads of Shared Memory:\nAdvantageously, RDMA-based reads of the shared memory are self-directed, with the client node managing its own reads of shared memory on any storage node, regardless of where that memory is being hosted within the RDMA-based network.  Further,\nas mentioned previously, RDMA-based reads of the shared memory by any client node are performed concurrently with self-directed RDMA-based reads, writes, and CAS operations on shared memory throughout the network by other client nodes, controller nodes,\nand storage nodes, thereby enabling the shared memory if the Distributed Storage Controller to scale to very large numbers of concurrent accesses while maintaining very low latencies for reads and writes to the shared memory.\nIn various implementations, the read process begins operation by determining the address of the memory slab where the read is to be performed.  This address may be obtained from some process or application that provides the address within a\nparticular memory slab where one or more memory elements (e.g., bits, bytes, or other subsections of the memory slab) are to be read, or by querying the controller node for addresses of particular memory elements or memory structures to obtain the\nappropriate address.  Regardless of how the address is obtained, given a particular memory address to be read (e.g., one or more memory elements of a particular memory slab), reads on the shared memory are performed by the client using RDMA operations to\nperform one-way reads on the corresponding memory slab.  Further, the lock-free nature of the Distributed Storage Controller enables multiple client nodes to read the same memory slab concurrently while any number of client nodes are concurrently\nperforming writes to any other slabs of shared memory on the same or other storage nodes.\nIn particular, given the address of a particular memory slab, the client performs an RDMA read of the entire memory slab.  Alternately, the client can perform an RDMA read of only the portion of the slab that contains the data it is interested\nin. However, reading the entire slab as a single read doesn't take any additional read operations relative to reading a portion of the slab because, in various implementations, slab size is set equal to or less than maximum RDMA read and write sizes. \nFurther, as mentioned above, in various implementations, each memory slab includes an optional metadata section that includes a checksum value jointly computed from each memory element of the slab.\nIn various implementations, whenever any memory slab is read, the client node computes a checksum value from the memory elements of the slab and compares the computed checksum to a checksum in the metadata portion of that slab.  The client then\nrepeats the RDMA read of that memory slab, if necessary, until such time as the computed checksum matches the checksum in the slab metadata.  Once the checksum match is confirmed for the memory slab, the Distributed Storage Controller returns the data in\nany particular memory elements of the slab being sought by the client.\nIn cases where there is a large volume of writes to a particular memory slab, or simply overlapping timing of reads and writes to that memory slab, the checksum of the slab may not match the checksum computed by the client until any particular\nwrite to that slab has been completed (see discussion of RDMA-based writes in Section 2.8).  Consequently, in various implementations, system overhead is reduced by checking the reservation bit of the memory slab via an RDMA read prior to performing slab\nreads and the subsequent computation and comparisons of checksums.  In such implementations, the RDMA reads of the reservation bitmap are repeated until the particular reservation bit of the memory slab shows that slab as being free, at which time the\nslab is read via an RDMA read.\nThe system diagram of FIG. 7 illustrates various implementations for performing RDMA-based reads of the shared memory provided by the Distributed Storage Controller.  Furthermore, while the system diagram of FIG. 7 illustrates a high-level view\nof various implementations of the Distributed Storage Controller, FIG. 7 is not intended to provide an exhaustive or complete illustration of every possible implementation of the Distributed Storage Controller as described throughout this document.\nFor example, as illustrated by FIG. 7, in various implementations, the read process begins operation by obtaining (700) the address of the shared memory to be read, either by querying a known source (e.g., the aforementioned placement library or\nany other source), in response to a publication of the address that is automatically pushed to the client node, or receipt of that address from any executing applications, processes, subroutines, etc. Next, the client performs an RDMA read (710) of the\nmemory slab holding the shared memory at the memory address being sought by the client.  The read process can be considered complete at this point.  However, data validity can be verified via the aforementioned computation and comparison of checksums.\nFor example, in various implementations, whenever a memory slab is read, the client node computes (720) a checksum from the entries in the memory slab and compares (730) that computed checksum to the checksum value in the metadata portion of the\nmemory slab obtained during the read of that memory slab.  The client then repeats the RDMA read (710) of that memory slab, if necessary, until such time as the computed checksum matches the checksum in the slab metadata.  Once the checksum match is\nconfirmed for the memory slab, the Distributed Storage Controller returns (740) the data from the corresponding shared memory to the client or to the client application, process, subroutine, etc., that originally requested the read.\nFurther, in various implementations, if the computed checksum does not match the checksum in the metadata after some predetermined number of read attempts, this may indicate that there are one or more bad or corrupt entries in the memory slab,\nor simply that a memory error may have occurred for some reason.  In this case, the Distributed Storage Controller then either restores the memory slab from a valid replica (e.g., reserve the memory slab on the primary, read the corresponding memory slab\nfrom the valid replica and then write that data to the corresponding memory slab of the primary followed by a release of the corresponding reservation bit) or marks that memory slab as bad in the aforementioned placement library and then moves to the\nreplica, now the primary, to perform the read process.  Further, whenever any data is written to any memory slab, that data is eventually replicated to each replica so insure consistency between the primary and each replica.\n2.8 RDMA-Based Writes to Shared Memory:\nWhenever any client wants to write to a particular memory slab (or one or more memory elements within that slab), the client first reserves that memory slab via the aforementioned reservation bitmap.  Once a slab has been successfully reserved,\nonly the reserving client can write to that slab until the reservation is released.  As such, this reservation ensures that there will not be any write collisions where two or more clients are attempting concurrent writes to the same memory slab.  In\ngeneral the reservation is for the entire slab that is being written.  However, as discussed in further detail below, larger reservations for entire memory blocks or memory regions and smaller reservations for individual memory elements within an\nindividual memory slab are also enabled by various implementations of the Distributed Storage Controller.  Writes by other clients are then only prevented for the memory that is actually reserved, whatever the size of that reserved memory, until such\ntime as that reservation is released.\nMore specifically, in various implementations, the write process begins operation by determining the address of the memory slab where the write is to be performed.  This address may be obtained from some process or application that provides the\naddress of one or more memory elements within a particular slab where data is to be written, or by performing RDMA-based reads of the controller node for addresses of particular memory elements or memory structures to obtain the appropriate address. \nRegardless of how the address is obtained, the RDMA-based writes to the address is protected via a reservation during the write process to ensure consistency of the shared memory.  For example, given the memory address to be written, the Distributed\nStorage Controller performs an RDMA CAS operation on the aforementioned reservation bitmap to flip the bit associated with the corresponding memory slab to indicate that the slab is reserved for writing (e.g., \"0\" if free, or \"1\" if reserved).  If the\nCAS operation is not successful, this means that the memory slab is already reserved for writing by some other client node.  In this case, the client node simply repeats the RDMA CAS operation until such time that the CAS operation is successful, thereby\nreserving that memory slab for writing by that client.\nOnce the slab has been reserved, the client node can simply perform an RDMA write to that slab, or one or more memory elements of that slab followed by a release of the reservation.  However, in order to ensure consistency of the data, in\nvarious implementations, following successful reservation of the memory slab, the client first performs an RDMA read of that slab prior to the write operation to obtain the current data of the various memory elements of that memory slab and, optionally,\nthe corresponding checksum in the slab metadata.  Given one or more new data elements to be written to the memory slab, the Distributed Storage Controller then computes a new checksum from the combination of all of the existing data elements in the\nmemory slab that are not being changed via the write operation and all of the new data elements that are being written to the memory slab.\nAs mentioned above, the entire slab may be written as a single RDMA-based write operation.  Therefore, when updating the checksum during write operations, a single RDMA write is performed to write the entire memory slab with all of the existing\ndata elements in that slab (read via the prior RDMA read) that are not being changed and all of the new data elements that are being changed along with the newly computed checksum.  Once this write is complete, the Distributed Storage Controller releases\nthe reservation of the memory slab by performing either an RDMA CAS operation or an RDMA write on the reservation bit associated with that slab to flip the bit back to indicate that the slab is no longer reserved.\nIn various implementations, whenever a memory slab write is to be performed, the Distributed Storage Controller adds an optional error checking step to confirm slab integrity prior to writing that slab.  For example, following the initial RDMA\nread of the memory slab, the Distributed Storage Controller computes a checksum from the existing data elements in that slab.  So long as the computed checksum matches the original checksum in the metadata, then the Distributed Storage Controller\ncontinues with the write operation, including the new checksum, as discussed above.  However, if the computed checksum does not match the checksum in the metadata, this indicates that there are one or more bad or corrupt entries in the slab, or simply\nthat a memory error may have occurred for some reason.  In this case, the Distributed Storage Controller either restores the memory slab from a valid replica and restarts the write process or marks that memory slab as bad in the aforementioned placement\nlibrary of the controller node and then moves to the replica, now the primary with respect to that memory slab, and restarts the write process.\nThe system diagram of FIG. 8 illustrates various implementations for performing RDMA-based writes to the shared memory.  Furthermore, while the system diagram of FIG. 8 illustrates a high-level view of various implementations of the Distributed\nStorage Controller, FIG. 8 is not intended to provide an exhaustive or complete illustration of every possible implementation of the Distributed Storage Controller as described throughout this document.  In addition, any boxes and interconnections\nbetween boxes that may be represented by broken or dashed lines in FIG. 8 represent alternate implementations of the Distributed Storage Controller described herein.  Further, any or all of these alternate implementations, as described herein, may be\nused in combination with other alternate implementations that are described throughout this document.\nIn general, as illustrated by FIG. 8, in various implementations, the write process begins by determining (800) the address of the memory slab where the write is to be performed.  This address where the write it to be performed may be obtained\neither by querying a known source (e.g., the aforementioned placement library or any other source), in response to a publication of the address that is automatically pushed to the client node, or receipt of that address from any executing applications,\nprocesses, subroutines, etc.\nOnce the address of the memory slab to be written is determined, the Distributed Storage Controller then performs an RDMA CAS operation (805) on the aforementioned slab reservation bitmap to flip the bit associated with that particular memory\nslab to indicate that the slab is reserved for writing.  If the CAS operation is not successful (810), this means that the memory slab is already reserved for writing by some other client node.  In this case, the requesting client node simply repeats the\nRDMA CAS operation (805) until such time that the CAS operation is successful, thereby reserving the memory slab for writing by that client.\nFollowing successful reservation of the memory slab, the client performs an RDMA read (815) of the memory slab to obtain the current key-value/pointer pairs of the memory slab and the corresponding checksum in the slab metadata.  Given one or\nmore new entries or data elements to be written to the memory slab, the Distributed Storage Controller then computes (820) a new checksum from the combination of all of the existing entries or data values in the memory slab that are not being changed and\nall of the new entries or data values that are being written.  A single RDMA write (825) is then performed to write the entire memory slab with all of the existing entries or data values in the memory slab that are not being changed and all of the\nentries or data values along with the new checksum.  Once this write is complete, the Distributed Storage Controller releases (830) the reservation of the memory slab by performing either an RDMA CAS operation or an RDMA write on the reservation bit\nassociated with that memory slab to flip the bit back to indicate that the memory slab is no longer reserved.\nIn various implementations, whenever a memory slab write is to be performed, the Distributed Storage Controller adds an optional error checking step to confirm validity of the data in the memory slab prior to writing that memory slab.  In\nparticular, as illustrated by FIG. 8, following the initial RDMA read (815) of the memory slab, the Distributed Storage Controller computes (835) a checksum from the existing entries or data values in the memory slab.  So long as the computed checksum\nmatches (840) the checksum in the metadata, then the Distributed Storage Controller continues with step (820), as discussed above.  However, if the computed checksum does not match the checksum in the metadata, this indicates (845) that there are one or\nmore bad or corrupt entries in the memory slab, or simply that a memory error may have occurred for some reason.  In this case, the Distributed Storage Controller then either restores (850) the memory slab from a valid replica and restarts the write\nprocess from step (815) or marks that memory slab as bad in the aforementioned placement library and then moves to the replica, now the primary, and restarts the write process from step (815).\n2.8.1 Reservation Considerations:\nIn general, the Distributed Storage Controller reserves the entire memory slab being written.  However, in various implementations, depending on the particular use and traffic profile for shared memory application, the Distributed Storage\nController can be configured to reserve a section of memory consisting of multiple memory slabs (or entire memory blocks or regions).  However, read and write efficiency is generally improved by reserving only the memory slab being written.  Further, in\nvarious implementations, the Distributed Storage Controller can be configured to reserve individual elements or entries of the memory slab being written so that multiple users can perform concurrent writes to the same memory slab.  In this case, rather\nthan providing a checksum over the entire memory slab, individual checksums would be used for each individual element of the memory slab.  In other words, the checksum is on the maximum reservation size.  So, in order to reserve smaller elements than an\nentire memory slab, there will be more checksums.  This configuration may provide performance improvements depending on the particular workload and traffic patterns of the shared memory application being enabled by the Distributed Storage Controller.\nIn other words, depending on the workload and network traffic, the Distributed Storage Controller can be configured to reserve: 1) an entire memory region; 2) one or one or more blocks of a particular memory region; 3) an entire memory slab; or\n4) one or more individual memory elements a single memory slab.  In each case, the checksum corresponds to the particular level of reservation.  However, given the relatively small size of the individual memory slabs, RDMA-based writes to the individual\nelements of memory slabs don't take much less time, if any, than to write the entire memory slab.  So, in terms of latency, there isn't typically much of a benefit to reserving individual slab elements over reserving the entire memory slab.  However,\nwhether or not a latency benefit is observed will depend on the particular workload (e.g., how many concurrent writes to a particular memory slab are being performed).  For example, if there is approximately evenly distributed high contention across the\nindividual elements of the memory slab, providing reservations for those individual elements would typically reduce overall system latency relative to reservation of the entire memory slab.\n2.8.2 Addressing Potential Write Failure Cases:\nAs with any computing system, various failures may occur during write operations by any client node to any slab of the shared memory.  For example, following reservation of a particular memory slab and prior to performing a successful write, the\nclient performing the write may go offline for some reason and thus be unable to remove the reservation bit or flag for that slab.  Similarly, following reservation of a particular memory slab and successful write to that slab, the client performing the\nwrite may go offline for some reason and thus be unable to remove the reservation bit or flag for that slab.  Advantageously, in either case, the checksum of that memory slab will be correct if either the write has not been performed at all or if the\nwrite has been completed without clearing the reservation bit.  As such, whether or not the write is performed following the reservation, the checksum will indicate whether the data in the slab is valid.  Partial writes to a slab do not occur in response\nto RDMA-based write operations.\nConsequently, to address the issue of a memory slab reservation that is not released for any reason, in various implementations, the Distributed Storage Controller assumes that the shared memory is operating properly but also puts a maximum\nbound (e.g., time t) on the length of time that any particular memory slab can be reserved.  For example, consider the scenario of server A wanting to write to memory slab n (hosted on server B) which is reserved via the reservation bitmap, while server\nC that originally reserved memory slab n goes offline for some reason either before or after writing to memory slab n, but prior to releasing the reservation.  In this scenario, server A will wait a predetermined amount of time t for the reservation to\nbe released.  If the reservation is then not released following expiration of that time period, server A sends an RPC message to server B to inform server B that the memory it is hosting for memory slab n has been reserved for too long, and asking for\nthat reservation to be released.  Assuming that the reservation bitmap for memory slab n is locally hosted by server B, server B can then flip the reservation bit, via an internal DMA CAS operation, to release the reservation for memory slab n.\nAlternatively the Distributed Storage Controller can acquire all reservations via sending an RPC message to the server hosting the memory in question and then the host machine can release the reservations following the timeout period.\nFurther, in the case that some sort of memory corruption has occurred in a memory slab for any reason, the metadata checksum will no longer match the computed checksum, thereby indicating incorrect, erroneous, or otherwise corrupted data in that\nslab.  In this case, as mentioned above, the Distributed Storage Controller can either restore the corrupted memory slab from a valid replica of that slab, or mark that slab as bad and then move to a valid replica for future reads and writes.\n2.9 RDMA-Based RPC Messages:\nAs mentioned above, in various implementations, the Distributed Storage Controller combines RDMA messaging and remote procedure call (RPC) requests to provide reads and writes to the shared memory and fast lock-free memory allocations and\nde-allocations for shared memory distributed across multiple servers in the RDMA-based network.  In such cases, the CPUs of the client and host servers only become involved when they execute a read, write, or memory allocation or de-allocation request\nspecified by an RPC request transmitted between the client and the host servers via RDMA-based messaging.\nFor example, in various implementations, an application, process, or thread executing on any particular networked server makes a memory access request (e.g., read, write, allocation or de-allocation request) that is intercepted by the local\ninstance of the Distributed Storage Controller executing on that server.  The Distributed Storage Controller instance of that server then applies an RDMA write to enter the appropriate RPC read, write, allocation or de-allocation request to a memory\nbuffer of another one of the servers acting as a storage node (which could be the same or a different one of the servers).  One or more local threads on the receiving server then monitor that buffer for receipt of an RPC memory access request.\nIn response to receipt of that RPC-based memory access request, the CPU of the receiving server (instead of the NIC in the case of RDMA-based requests) executes the same sequences of DMA reads, writes, and CAS operations on the reserved memory\nto perform local reads, writes, allocations and/or de-allocations on the shared memory.  For example, in response to memory slab reads, the receiving server then sends the contents of the memory slab to the requesting server by applying an RDMA write of\nthe contents of that memory slab to a memory buffer of the requesting server.  One or more local threads on the requesting server monitor the buffer of that server for receipt of the memory slab contents.\nSimilarly, in response to memory allocation requests, the receiving server executes the same sequence of reads and CAS operations on the block header and free slab maps described above to perform the requested lock-free allocation or\nde-allocation.  The receiving server then sends a reference to the allocated memory to the requesting server by applying an RDMA write of that reference to a memory buffer of the requesting server.  One or more local threads on the requesting server\nmonitor the buffer of that server for receipt of the reference to the allocated memory (or confirmation of a successful de-allocation request).\nThe overhead and latency of sending an RPC message via RDMA is higher than the techniques described herein that rely solely on RDMA messaging.  However, one of the advantages of this process is that reads, writes, and CAS operations performed by\nthe CPU in response to receipt of an RPC message are typically much faster than reads, writes, and CAS operations performed by the NIC in response to receipt of RDMA messages.\nAs mentioned above, the Distributed Storage Controller provides various techniques for enabling fast RPC requests via a sequence of RDMA messages transmitted using commodity NICs between networked computers in a data center or other network\nenvironment to provide an overall system that is both lock-free and thread-safe.  The processes summarized above are illustrated by the general system diagram of FIG. 9.  In particular, the system diagram of FIG. 9 illustrates the interrelationships\nbetween program modules for implementing various implementations of the Distributed Storage Controller, as described herein.  Furthermore, while the system diagram of FIG. 9 illustrates a high-level view of various implementations of the Distributed\nStorage Controller, FIG. 9 is not intended to provide an exhaustive or complete illustration of every possible implementation of the Distributed Storage Controller as described throughout this document.\nIn addition, any boxes and interconnections between boxes that may be represented by broken or dashed lines in FIG. 9 represent alternate implementations of the Distributed Storage Controller described herein.  Further, any or all of these\nalternate implementations, as described below, may be used in combination with other alternate implementations that are described throughout this document.\nIn general, as illustrated by FIG. 9, the processes enabled by the RDMA-Based RPC Request System begin operation by applying an RDMA-Based Messaging API Module 900 to perform various functions, including, but not limited to receiving RPC\nrequests from an operating system, procedure, subroutine, application, process, etc. (910), initiating and responding to communications with other servers in network via an RDMA-enabled NIC 920, updating server control data 930, etc.\nIn addition, an RPC Request Monitor Module 950 of the RDMA-Based Messaging API Module 900 applies one or more threads to monitor an RDMA Message Buffer 940 for relevant RPC requests or responses.  The threads of the RPC Request Monitor Module\n950 pull relevant RPC requests from the RDMA Message Buffer 940 and adds those requests to an RPC Queue 960.  In addition, the threads of the RPC Request Monitor Module 950 pull relevant responses from the RDMA Message Buffer 940 and pass those responses\nto the requesting operating system, procedure, subroutine, application, process, etc. (910) that initiated the RPC request on that server.\nIn various implementations, one or more blocks of server main memory 980 are allocated to host the control data 930, the RDMA Message Buffer 940, and the RPC queue 960, such that the allocated memory is only accessed by the RDMA-Based Messaging\nAPI Module 900 and via DMA read/write calls to that memory by the NIC.  In other words, no other processes executing on the host server other than the RDMA-Based Messaging API Module 900 or the NIC of the host server writes to allocated server main\nmemory 980.\nFurther, an RPC Request Processing Module 970 is applied in the case that a particular server applies one or more threads to monitor the RPC Queue 960 for relevant RPC requests (transmitted via RDMA write messages).  When any of these threads\nidentifies a relevant RPC request, that thread calls the procedure identified by the RPC request and, when that procedure has completed execution, the RPC Request Processing Module 970 passes the response to that RPC request to the RDMA-Based Messaging\nAPI 900 for return to the sender server via the RDMA-Enabled NIC 920.\n2.10 Replicating Allocated Memory Across Multiple Servers:\nIn various implementations, the Distributed Storage Controller optionally replicates allocated memory slabs, blocks, or entire memory regions, across one or more additional servers to provide data redundancy in the event of data corruption on a\nparticular server or loss of a particular server for any reason.  Writes to any allocated memory on a primary server are then automatically replicated to the corresponding allocated memory slabs, blocks, or entire memory regions on one or more replica\nservers.  Further, in various implementations, memory slabs or blocks, or entire memory regions that have been replicated to one or more additional servers are automatically de-allocated whenever a call for de-allocation of the original slab, block, or\nentire memory region is received by the Distributed Storage Controller.\nFor example, when replicating a state of allocated memory, that memory is first allocated (or de-allocated) to a primary location using the RDMA-based techniques described herein.  Once the memory has been allocated (or de-allocated) on the\nprimary, the Distributed Storage Controller optionally copies each corresponding memory region to one or more replicas (on the same or different servers).  As such, if there are no ongoing allocations or de-allocation each of the replicas will be\nidentical to the primary.\nIn general, in various implementations, the optional replication processes performed by the Distributed Storage Controller are implemented as a simplified version of the allocation and de-allocation processes performed by the RDMA-based memory\nallocator component of the Distributed Storage Controller, as described above.  However, any known technique for replicating memory may be applied following the original allocation.\nMore specifically, in the case of allocations, once a particular block and slab have been allocated to the primary, the RDMA-based memory allocator component of the Distributed Storage Controller knows exactly which block and slab have been\nallocated in particular memory regions.  Therefore, after that allocation has been completed, the RDMA-based memory allocator component applies the RDMA-based techniques described above to create each replica by performing the steps described below\n(which should be understood in view of the preceding detailed description regarding RDMA-based memory allocations).  However, as noted above, after any memory has been allocated, any known technique for replicating memory may be applied to create\nreplicas.  Assuming the use of the RDMA-based techniques described herein, replication is accomplished (for each replica of the primary) via the following steps: 1.  Apply an RDMA CAS operation to update the number of free slabs in the appropriate block\nheader of the replica memory region; 2.  Apply an RDMA CAS operation to mark the appropriate slab as being used in the free slab map of the replica; and 3.  Propagate any writes to allocated slabs on the primary to each of the replicas.\nIn the case of de-allocations, in various implementations, the replication process performed by the RDMA-based memory allocator component of the Distributed Storage Controller considers whether additional slabs are being used in a memory block\nof the primary following de-allocation of a particular slab.\nFor example, in the case where other slabs are still being used in a block following de-allocation of a particular slab, the replica is updated by using the above described RDMA-based techniques for changing the metadata to decrease number of\nslabs used in the block header of the replica and to mark the slab as free in the free slab map of the replica.  These processes are duplicated for each replica of the primary.\nConversely, in the case where the slab being freed on the primary is the last (or only) slab being in the block, in various implementations, the RDMA-based memory allocator component of the Distributed Storage Controller performs the following\nsteps: 1.  Apply an RDMA CAS operation to mark the block of the primary as reserved rather than free; 2.  Evaluate the first replica to ensure that the copy of the slab de-allocated on the primary is the only slab being used in the corresponding block,\nand when true, apply an RDMA CAS operation to mark the block of that replica as free; 3.  Repeat step 2 for all replicas of the primary to complete the de-allocation process for each replica; and 4.  After completing the de-allocation process on all\nreplicas, apply an RDMA CAS operation to mark the block of the primary as free.\n2.10.1 Additional Replication Considerations:\nFor safety and redundancy, in various implementations, the primary and each of its replicas (e.g., control nodes, storage nodes and/or client nodes) are put into different \"failure domains.\" For example, in various implementations, each primary\nand any replicas will run on different power sources, each may be in physically separate racks, rooms, or even buildings, etc. Consequently, any single type of failure (e.g., power failure, building or computer damage from earthquake, fire, flood,\nintentional acts, etc.) will not disable any particular primary and all of its replicas.  Consequently, by putting the primary and each replica of any node (e.g., control nodes, storage nodes and/or client nodes) into a different failure domain, data\nheld by the shared memory of the Distributed Storage Controller is never lost due to any single point failure.\nIn general, whenever any primary goes offline or becomes unavailable or unresponsive for any reason, the Distributed Storage Controller (via one of the controller nodes which maintains a current operational status of each node) immediately\ndesignates an existing replica of that primary as the new primary.  In addition, whenever any primary or replica goes offline or becomes unavailable or unresponsive for any reason, the Distributed Storage Controller, optionally allocates memory for a new\nreplacement replica to ensure continued data redundancy.  The allocated memory of the newly designated replica is then populated by reading data from the primary and one or more replicas and writing that data to the newly designated replica. \nDistribution tables for the primary and each of its replicas, including the newly designated replica are then updated to reflect any changes resulting from the change in primary and replica machine addresses.\nIn various implementations, whenever any client node performs a write, the write is sent concurrently to the primary and all of the replicas.  For example, in various implementations, rather than send a single RDMA write to the primary storage\nnode, in various implementations, the client node concurrently sends separate RDMA writes to the primary storage node and all of the corresponding replicas of that storage node.  For example, given a primary and two replicas, the client node sends three\nRDMA write messages (one to each of the primary storage node and each of the two replica storage nodes).  In various implementations, this write process is simplified by causing any reservation (for writes) of the memory on the primary to concurrently\nreserve the corresponding memory on all of the replicas of that primary.  This ensures consistency between the primary and all of its corresponding replicas.\nIn contrast to writes to the shared memory, when performing RDMA-based reads, the consistency between the primary and each of its replicas enables the client node to perform the read on either the primary storage node or on any of the replicas\nof that storage node.  Therefore, network traffic to particular primary machines is reduced by the Distributed Storage Controller by causing client nodes to select which of the primary or one its replicas is to be addressed to perform a read either on\nrandom basis or based on known network traffic to those machines (e.g., direct the read to the machine with the lowest traffic).  In either case, overall system latency tends to be reduced since not all machines are always trying to read from the\nprimary.\nIn addition to replication of various servers (e.g., control nodes, storage nodes and/or client nodes) within a single data center, server farm etc., in various implementations, the Distributed Storage Controller links multiple data centers in\ndifferent physical locations (e.g., geo-replication).  In other words, to provide an additional level of redundancy, in various implementations, the Distributed Storage Controller replicates entire data centers, or any desired portions of those data\ncenters.  Typically, Ethernet or other transmission means are applied to transmit data to remote data centers for replication and backup purposes.\n2.11 Exemplary Applications:\nAs mentioned above, in various implementations, client nodes include a co-processor module that provides a connection library (e.g., a DLL, executable code, library, etc. that is accessible via an API) that allows any desired functionality\n(e.g., a key-value store or other client application) to access and interact with the memory allocation and shared memory features of the Distributed Storage Controller via thread-safe and lock-free RDMA-based messaging.  For example, an example of a\nB-Tree based key-value store application implemented via the co-processor module is described in the following paragraphs.  Further, as noted above, other applications include, but are not limited to, map-reduce jobs, Spark jobs, search index serving\njobs, machine-learning jobs, etc.\n2.11.1 B-Tree Based Key-Value Store:\nThe following paragraphs describe a key-value store application that interfaces with the functionality of the Distributed Storage Controller via the aforementioned co-processor module.  However, such applications can also access the\nfunctionality of the Distributed Storage Controller directly by enabling those applications to make appropriate calls to the various components of the Distributed Storage Controller (e.g., the RDMA-based memory allocator component, the RDMA-based\nmessaging component, etc.).  As such, for purposes of explanation, rather than repeatedly referring to accessing the functionality of the Distributed Storage Controller via the co-processor module, the following discussion of the key-value store will\ngenerally refer to the functionality of the Distributed Storage Controller as if that functionality were being directly accessed.\nTypical key-value stores, also referred to as key-value databases, enable storage, retrieval and management of associative arrays such as a dictionary or hash.  Key-value stores typically contain a collection of objects, or records, which in\nturn have many different fields within them, each containing data.  These records are stored and retrieved using a key from the key-value store that uniquely identifies the record, and is used to locate and access specific data within the database.\nIn general, a B-Tree is a structure used to construct a key-value store that points a client down to a memory location of a data value by traversing from a root node of the B-Tree via a tree-based sequence of references to that data value in a\nleaf node of the tree.  In other words, a path through multiple branches from the root node of the tree are sequentially traversed to reach a leaf node that contains a reference to the data value being sought by the client.  Each leaf node of the tree is\ncomprised of multiple entries with each entry including a key-value/pointer pair to the data being sought by the client.  The root node and each branch node is comprised of multiple entries (e.g., a key range), which are key-value/pointer pairs to the\nnext lower sub-branch which are traversed to reach leaf nodes.\nFurther, each node of the B-Tree (e.g., root nodes, branch nodes, and leaf nodes) correspond to individual memory slabs (as discussed in the preceding paragraphs) that are allocated from memory blocks.  In other words, any particular node of the\nB-Tree is implemented within a single memory slab of the shared memory of the Distributed Storage Controller.  As such, operations such as allocations, reservations, reads, and writes to nodes of the B-Tree are performed in the same or similar manner as\nallocations, reservations, reads, and writes to individual memory slabs.\nMore specifically, in a typical B-Tree, each node of the B-Tree contains keys and pointers to lower B-Tree nodes (e.g., key-value/pointer pairs).  The keys act as separation values which divide its subtrees (e.g., further branch nodes or leaf\nnodes) while the corresponding pointer provides a reference to the memory of the next lower node of the B-Tree.  For example, if an internal branch node has three child nodes then that internal node will include two keys: k.sub.1 and k.sub.2 (e.g., 5 and\n10, respectively).  The values in the leftmost subtree will be less than k.sub.1 (e.g., values 1, 2, 3 are less than key value of 5), the values in the middle subtree will be between k.sub.1 and k.sub.2 (e.g., 6, 8, and 9 are between key values 5 and\n10), and the values in the rightmost subtree will be greater than k.sub.2 (e.g., 15 and 356 are greater than key value 10).  The leaves hold either the data record being sought via the B-Tree read or, alternately, the leaves hold pointers to the memory\nholding the data record.\nThe B-Tree based key-value store application implemented using various functionality of the Distributed Storage Controller improves performance of reads and writes to the key-value store by distributing the memory hosting the key-value store\nacross multiple networked servers.  Further, the functionality of the Distributed Storage Controller enables the B-Tree based key-value store application to perform self-directed reads and writes to the key-value store.  As such, scalability of the\nB-Tree based key-value store enabled by the Distributed Storage Controller is improved by avoiding the use of a central computer to coordinate or control those reads and writes.  In addition, the Distributed Storage Controller provides a metadata based\nmechanism to ensure that the key-value store maintains consistency for reads and writes without the requirement to perform significant amounts of locking.\nIn various implementations, the B-Tree based key-value store application provides a growth-only structure for the key-value store.  In other words, in the growth-only scenario, once memory is allocated for a new branch or leaf node of the\nB-Tree, that memory remains allocated (and available for future reads and writes), even if the data in that node of the B-Tree has been deleted.  Further, in the growth-only scenario, the key-value/pointer pairs in any particular node of the B-Tree\nremain pointing to the same child node.  In other words, in the growth-only case, nodes are added or appended to the tree but not removed from the tree.  Further, when growing the tree, leaf nodes may be converted into branch nodes, with appropriate\nkey-value/pointer pairs, to include additional lower levels of the tree.\nAlternately, in various implementations, rather than provide a growth-only configuration of the key-value store, the B-Tree based key-value store application allows nodes to be pruned or deleted from the B-Tree.  Further, branches may be\nconverted to leaves based on pruning of lower levels of the tree.  Similarly, leaf nodes may be converted into branches when adding new child nodes to the tree.  However, in this non growth-only scenario, it is possible that any particular parent or\nchild node may contain stale data (e.g., invalid key-value/pointer pairs) due to an intervening write operation or a prior node add or delete operation subsequent to the read of the parent node and prior to the read of the corresponding child node.\nConsequently, to address such issues in the non growth-only case, in various implementations, the metadata of each node of the B-Tree further includes a reverse pointer to the parent node (i.e., the higher-level node that immediately precedes\nand contains a key-value/pointer pair to a particular lower-level node (i.e., the child node) in the key-value store)).  These reverse pointers enable the B-Tree based key-value store application to validate that as it is traversing the tree it does not\njump to an incorrect branch or leaf node due to any combination of node write and prior deletes and adds of nodes.  In particular, while traversing the tree for the purpose of reading a particular key-value/pointer pair, the B-Tree based key-value store\napplication compares the reverse pointer read from the metadata of any child node to the address of the parent node from which the child node was reached.  In the event that the reverse pointer in the child node does not match the address of the parent\nnode from which that child node was reached, it is possible that an intervening node add or delete operation and/or a node write operation on the child and/or parent nodes may have occurred subsequent to the read of the parent node and prior to the read\nof the child node.  As such, the key-value/pointer pair originally read from the parent node may be stale.\nIn the event that the reverse pointer does not match the address of the parent node, the B-Tree based key-value store application repeats the read of the parent node and compares the key-value/pointer pair of that new read to the prior read.  If\nthe new and old key-value/pointer pairs of the parent node are different, this indicates that the parent has been updated via a new node write operation.  In this case, the B-Tree based key-value store application continues traversing the tree using the\nnew key-value/pointer pair of the parent obtained by the new read of the parent, while continuing to compare the reverse pointers, as described above.\nHowever, if the new and old key-value/pointer pairs of the parent node are the same between the new read to the prior read, this indicates that the parent may hold stale data.  In this case, the B-Tree based key-value store application bubbles\nup one level (i.e., to the immediate parent of the current parent node) and performs a new read of the immediate parent node.  The B-Tree based key-value store application then compares the key-value/pointer pair of that new read of the immediate parent\nnode to the prior read of the immediate parent node.  If the new and old key-value/pointer pairs of the immediate parent node are different, this indicates that the immediate parent has been updated via a new node write operation.  In this case, the\nB-Tree based key-value store application continues traversing the tree using the new key-value/pointer pair of the immediate parent obtained by the new read of the immediate parent while continuing to compare the reverse pointers, as described above. \nOtherwise, if the new and old key-value/pointer pairs of the immediate parent node are the same, the B-Tree based key-value store application iteratively bubbles up to next higher parents until a level of the tree is reached where the new and old\nkey-value/pointer pairs of the next higher parent node is different, at which point the B-Tree based key-value store application continues traversing the tree given the new key-value/pointer pair of that next higher parent node, while continuing to\ncompare the reverse pointers, as described above.\nIn various implementations, nodes of the key-value store enabled by the B-Tree based key-value store application have a fixed size that is less than or equal to the maximum RDMA read and write size enabled by the RDMA-enabled NIC hardware in the\nnetwork.  This enables the entire node to be read or written via a single RDMA read or write.  For example, in a tested implementation, nodes were configured using fixed sized memory slabs allocated by the RDMA-based memory allocator component of the\nDistributed Storage Controller.  Slab sizes were set anywhere between one byte and the maximum RDMA read size, with intermediate sizes of increasing powers of 2 (e.g., 2 bytes, 4 bytes, 8 bytes, 16 bytes, 32 bytes, 64 bytes, 128 bytes, 256 bytes, etc.). \nHowever, the B-Tree based key-value store application is operable with any desired memory allocation process, and the use of the RDMA-based memory allocator component of the Distributed Storage Controller is described herein only for purposes of\nexplanation and example.\nEach B-Tree has a publicized name (e.g., a \"friendly name\") by which the client first finds the appropriate key-value store.  For example, in various implementations, the client contacts a known address (which may have multiple backups for\nredundancy), and provides the friendly name of a particular B-Tree to a process that returns a memory reference to the root node of the corresponding B-Tree.  The client then applies an RDMA read of that memory address to obtain a list of branches of the\nroot node.  In various implementations, once a client receives the memory address for a particular B-Tree, the client maintains that address in order to perform future B-Tree reads or writes of the key-value store.  In other words, any client only needs\nto obtain the address of the root node of a particular B-Tree one time, and can then maintain a copy of that address for further interactions with that B-Tree.\nOptionally, in various implementations, the root node of each B-Tree is propagated to each client whenever any B-Tree is created on any server.  This eliminates the initial RDMA read of the root node of each B-Tree by individual clients to\nprovide a small performance improvement.  However, given the very large number of interactions between clients and particular B-Trees, elimination of a single RDMA read of the root node by the client doesn't significantly reduce overall system latency.\n2.11.2 Initializing the Key-Value Store:\nWhen the B-Tree based key-value store application initially creates a particular key-value store, a friendly name is associated with that key-value store and provided to the aforementioned placement library along with the address of the root\nnode of that particular key-value store.  In particular, the root of that key-value store is stored in a secure memory location that is available in the aforementioned placement library or the like, and that is published (or provided on request) to each\nof the clients based on the friendly name associated with the key-value store.  The friendly name (e.g., \"Database 1\") is simply an easy way for a client to reference a particular key-value store.  Typically, but not necessarily, the friendly name is\nspecified by the client requesting creation of the key-value store.\nFurther, when the B-Tree based key-value store application first creates any key-value store, the B-Tree based key-value store application applies the functionality of the Distributed Storage Controller to initially allocate memory for one root\nnode, and additional memory for a full set of empty leaf nodes.  In addition, when the key-value store is first created, the B-Tree based key-value store application populates key-value/pointer pairs in the root node to point to the allocated addresses\nof each of the corresponding leaf nodes.\nThe number of the leaf nodes for the root node (or any other branch node) is set at some specified value.  Typically, the depth of the tree (i.e., number of levels) increases in inverse proportion to the number of branch and leaf nodes\naddressable from the root or other branch node.  In other words, wide trees (more branches or leaves from each node) tend to have fewer levels than narrow trees.  This root branch can be cached on any client since it is unlikely to change, especially in\ninstances where the key-value store is set up in a growth-only configuration, as discussed in further detail in Section 2.11.4.\n2.11.3 Splitting or Updating Nodes:\nIn general, as the key-value store grows, existing leaf nodes are converted to branch nodes to expand the B-Tree downwards with additional levels.  When converting a leaf node to a branch node, new entries are first added to the reservation\nbitmap for each of the new leaf nodes that depend from the node being converted to a branch.  In addition, the reservation bits (same as the aforementioned reservation bitmap for reservations of memory slabs) for each of the new leaf nodes and the node\nbeing converted to a branch are all flipped to reserve those nodes until node write operations on all of those nodes are completed to write the appropriate key-value/pointer pairs to those nodes.  As discussed with respect to reservations of the\naforementioned memory slabs, these reservations are then released after the conversion process is complete so that reads and writes of the converted node of the B-Tree and the new leaf nodes may be performed, as described herein.\nConversely, when deleting data from a particular leaf node, that leaf node may no longer hold any key-value pairs that point to any data.  In this case, the leaf node may be maintained with null data that may be populated with key-value/pointers\nat some point via RDMA-based writes.  Alternately, in some cases, the leaf node is pruned from the B-Tree by converting the parent branch node into a leaf node.  As with conversion from a leaf to a branch node, whenever a node is converted from a branch\nto a leaf node, all of the affected nodes are reserved by flipping the corresponding bits of the reservation bitmap until the conversion process is complete via writes of new key-value/pointer pairs to the node being converted to a leaf node.\nFor example, when expanding the tree, the tree grows by converting one or more leaf nodes to branches and allocating memory (e.g., individual memory slabs) for new leaf nodes below the newly converted branch node.  Whenever a leaf node is\nconverted to a branch node, the B-Tree based key-value store application will perform an RDMA write, using the memory slab writing techniques described herein, to populate the converted branch with key-value/pointer pairs to reference the new leaf nodes\nin the newly allocated memory.  In other words, the B-Tree based key-value store application applies the functionality of the Distributed Storage Controller to allocate additional memory, on one or more servers in the network, for a new full set of empty\nleaf nodes, and will write new key-value/pointer pairs to the newly converted branch node to point to the newly allocated addresses of each of the corresponding leaf nodes.  In addition, some of the original key-value/pointer pairs in the node that was\nconverted from a branch to a leaf node may be written to the newly allocated leaf node.\nAs such, these techniques enable caching of nodes of the B-Tree without needing to validate the cache.  For example, in various implementations, when a branch is divided into key-value/pointer pair entries (e.g., branch contains keys 1 to 1000),\nthat branch is never recreated.  In particular, when a branch is created, the key-value/pointer pair entries are initialized as null pointers.  As such, in various implementations, the B-Tree based key-value store application does not pre-allocate\nbranches for future use, thereby saving a large amount of memory space.  Then, once a null pointer needs to be converted into a real branch (with actual data) the B-Tree based key-value store application applies the functionality of the Distributed\nStorage Controller to perform the allocation on demand and enables writes to the newly allocated memory using the RDMA-based write techniques described herein.  For example, when the key-value store is initially created, the root node is divided into\nsome number of branches.  These branches are then filled as writes are performed, and new branches (e.g., conversion of leaf nodes to branches) are allocated and filled with data on an as-needed basis.\n2.11.4 Optimizations for the Growth-Only Scenario:\nIn various implementations, efficiency of read and write accesses to the key-value store are improved by the B-Tree based key-value store application via a growth-only scenario.  In other words once memory is allocated for the key-value store\nthat memory remains allocated, even if one or more branch or leaf nodes are logically pruned from the B-Tree.  As such, from the client point of view, particular key-value/pointer pairs may be logically deleted, or entire branch or leaf nodes may be\nlogically deleted without shrinking the tree.  However, the space for that particular key or node is then available for new writes because that same allocated memory is reused whenever a new key or node is needed.  Thus, once memory is allocated for the\ntree, it stays allocated for that tree, regardless of whether that memory holds any data.\nFor example, consider an example of applying the B-Tree based key-value store application to construct and provide access to a key-value store for customer purchase history.  Any time that a customer makes a transaction (e.g., on a store\nwebsite, in a physical store, etc.), a corresponding entry can be added to the key-value store.  In the growth-only case, transactions are then never undone, even if the customer returns a purchase.  The return is simply treated as an additional refund\ntransaction.  As such, optimizing the B-Tree based key-value store application using a growth-only implementation provides real-world efficiency improvements.\n2.11.5 Exemplary Implementations of the Key-Value Store:\nIn view of the preceding paragraphs, the B-Tree based key-value store application applies the functionality of the Distributed Storage Controller to enable any client to create a key-value store and to read, write, and optionally delete records\nin the key value store.  Reads enabled by the Distributed Storage Controller provide transactional \"Get\" operations that are performed via a sequence of RDMA reads and checksum comparisons of the key-value store.  Writes enabled by the Distributed\nStorage Controller provide transactional \"Put\" operations that are performed via a sequence of RDMA reads, RDMA writes, and RDMA CAS operations.  Similarly, optional deletes enabled by the Distributed Storage Controller provide transactional \"Delete\"\noperations that are performed via a sequence of RDMA reads, RDMA writes, and RDMA CAS operations to delete the mapping of a specified key.  Because each node of the B-Tree is implemented as a single memory slab, these RDMA-based operations are performed\nin a manner similar to the memory operations on memory slabs, as described above.\nThe system diagram of FIG. 10 illustrates various implementations for performing RDMA-based reads of the key-value store.  Furthermore, while the system diagram of FIG. 10 illustrates a high-level view of various implementations of the B-Tree\nbased key-value store application enabled by the Distributed Storage Controller, FIG. 10 is not intended to provide an exhaustive or complete illustration of every possible implementation of either the B-Tree based key-value store application or the\nDistributed Storage Controller as described throughout this document.  Advantageously, the RDMA-based reads of the key-value store are self-directed, with the client managing its own reads regardless of where the memory hosting the B-Tree node being read\nis stored within the RDMA-based network.  Further, as mentioned previously, RDMA-based reads of the key-value store by any client are performed concurrently with self-directed RDMA-based reads, writes, and CAS operations on the key-value store by other\nclients, thereby enabling the key-value store to scale to very large numbers of concurrent client accesses while maintaining very low latencies for reads and writes to the key-value store.\nFor example, as illustrated by FIG. 10, in various implementations, the read process begins operation by obtaining (1000) the address of the root node of a particular instance of the B-Tree based key-value store either by querying a known source\n(e.g., the aforementioned placement library or any other source) or in response to a publication of the address that is automatically pushed to each client.  Next, the client performs an RDMA read (1010) of the root node to obtain a reference to the\naddress of the next node of the B-Tree in the path containing the data value being sought by the client.  The client can optionally cache the key-value/pointer pairs of the root node for performing future reads of branches below the root node.\nWhenever the root node is read, the client computes (1020) a checksum from the key-value/pointer pairs in the root node and compares (1030) that computed checksum to the checksum value in the metadata portion of the node obtained during the read\nof the root node.  The client then repeats the RDMA read (1010) of the root node, if necessary, until such time as the computed checksum matches the checksum in the node metadata.\nAfter reading the root node and verifying a checksum match, the client then performs an RDMA read (1040) of the next node in the path based on the address of that node obtained from the prior read.  This read is performed to obtain a reference\nto the address of the next node in the path containing the data value being sought by the client.  As with the read of the root node, the client again client computes (1050) a checksum from the key-value/pointer pairs in the node being read and compares\n(1060) that computed checksum to the checksum value in the metadata portion of that node.  The client then repeats the RDMA read (1040) of the node, if necessary, until such time as the computed checksum matches the checksum in the node metadata.\nOnce the checksum match is confirmed for the current node, a determination (1070) is made as to whether the current node is a leaf node, thus indicating the bottom level of the tree.  If the current node is a leaf node, the B-Tree based\nkey-value store application returns (1080) the value or address being sought by the client in the leaf node to the client.  Otherwise, if the node is not a leaf node, the client loops back and performs an RDMA read (1040) of the next node in the path\nbased on the address of that node obtained from the prior read.  This process continues, with node reads, checksum comparisons and determination of node level as a branch or leaf node, until the leaf node is reached and the value or address being sought\nis returned to the client.\nHowever, in various implementations, if the computed checksum does not match the checksum in the metadata after some predetermined number of read attempts, this may indicate that there are one or more bad or corrupt entries in the node, or\nsimply that a memory error may have occurred for some reason.  In this case, the B-Tree based key-value store application applies the functionality of the Distributed Storage Controller to either restore the node of the B-Tree from a valid replica (e.g.,\nreserve the node of the B-Tree on the primary, read the corresponding node of the B-Tree from the valid replica and then write that data to the corresponding node of the primary followed by a release of the corresponding reservation bit) or mark that\nnode of the B-Tree as bad in the aforementioned placement library and then move to the replica, now the primary, to perform the read process.  Further, whenever any data is written to any node, that data is eventually replicated to each replica to insure\nconsistency between the primary and each replica.\nThe system diagram of FIG. 11 illustrates various implementations for performing RDMA-based writes to the key-value store.  Furthermore, while the system diagram of FIG. 11 illustrates a high-level view of various implementations of the B-Tree\nbased key-value store application, FIG. 11 is not intended to provide an exhaustive or complete illustration of every possible implementation of either the B-Tree based key-value store application or the Distributed Storage Controller as described\nthroughout this document.  In addition, any boxes and interconnections between boxes that may be represented by broken or dashed lines in FIG. 11 represent alternate implementations of the Distributed Storage Controller described herein.  Further, any or\nall of these alternate implementations, as described herein, may be used in combination with other alternate implementations that are described throughout this document.\nIn general, as illustrated by FIG. 11, in various implementations, the write process begins by determining (1100) the address of the node of the key-value store where the write is to be performed.  This address may be obtained from some process\nor application that provides the address within a particular node where one or more key-value/pointer pairs are to be written, or by performing RDMA-based reads of the B-Tree (e.g., by applying the process illustrated with respect to FIG. 10) to obtain\nthe appropriate address.\nOnce the address of the node to be written is determined, the B-Tree based key-value store application applies the functionality of the Distributed Storage Controller to perform an RDMA CAS operation (1105) on the aforementioned node reservation\nbitmap (same as the reservation bitmap for individual memory slabs) to flip the bit associated with that particular node to indicate that the node is reserved for writing.  If the CAS operation is not successful (1110), this means that the node is\nalready reserved for writing by some other client.  In this case, the client simply repeats the RDMA CAS operation (1105) until such time that the CAS operation is successful, thereby reserving the node for writing by the client.\nFollowing successful reservation of the node, the client performs an RDMA read (1115) of the node to obtain the current key-value/pointer pairs of the node and the corresponding checksum in the metadata.  Given one or more new key-value/pointer\npair entries to be written to the node, the B-Tree based key-value store application then computes (1120) a new checksum from the combination of all of the existing key-value/pointer pairs in the node that are not being changed and all of the new\nkey-value/pointer pairs that are being written.  A single RDMA write (1125) is then performed to write the entire node with all of the existing key-value/pointer pairs in the node that are not being changed and all of the new key-value/pointer pairs\nalong with the new checksum in the metadata.  Once this write is complete, the B-Tree based key-value store application releases (1130) the reservation of the node by performing either an RDMA CAS operation or an RDMA write on the reservation bit\nassociated with that node to flip the bit back to indicate that the node is no longer reserved.\n3.0 Operational Summary of the Distributed Storage Controller:\nThe processes described above with respect to FIG. 1 through FIG. 11, and in further view of the detailed description provided above in Sections 1 and 2, are illustrated by the general operational flow diagrams of FIG. 12 through FIG. 14.  In\nparticular, FIG. 12 through FIG. 14 provide exemplary operational flow diagrams that summarizes the operation of some of the various implementations of the Distributed Storage Controller.  FIG. 12 through FIG. 14 are not intended to provide an exhaustive\nrepresentation of any or all of the various implementations of the Distributed Storage Controller described herein, and the implementations represented in these figures are provided only for purposes of explanation.\nFurther, any boxes and interconnections between boxes that may be represented by broken or dashed lines in FIG. 12 through FIG. 14 represent optional or alternate implementations of the Distributed Storage Controller described herein, and that\nany or all of these optional or alternate implementations, as described below, may be used in combination with other alternate implementations that are described throughout this document.\nIn general, as illustrated by FIG. 12, in various implementations, the Distributed Storage Controller begins operation by providing (1200) a plurality of networked computing devices in communication via RDMA-enabled NICs.  In addition,\ndistribution tables and metadata (1210) that define various parameters of a distributed storage are hosted on one or more of the networked computing devices.  This distributed storage comprises (1220) a plurality of memory regions allocated across two or\nmore of the networked computing devices.  Further, a separate instance of an RDMA-based messaging module (1230) is hosted on each of a plurality of the networked computing devices.  In addition, a separate instance of an RDMA-based memory allocator\nmodule (1240) is hosted on each of a plurality of the networked computing devices.  The overall distributed storage is implemented by applying (1250) the RDMA-based memory allocator module of one or more of the computing devices and the RDMA-based\nmessaging module of one or more of the computing devices in combination with the distribution tables and metadata to concurrently perform any combination of RDMA-based lock-free memory operations on the distributed storage.\nSimilarly, as illustrated by FIG. 13, in various implementations, the Distributed Storage Controller provides (1300) a shared memory comprising a plurality of memory regions allocated on two or more networked computing devices in communication\nvia RDMA-enabled NICs.  Each memory region of the shared memory is allocated (1310) by an instance of an RDMA-based memory allocator module hosted on one or more of the networked computing devices.  In addition, one or more memory blocks are allocated\n(1320) from each memory region by an instance of the RDMA-based memory allocator module hosted on one or more of the networked computing devices.  Further, one or more memory slabs are allocated (1330) from each memory block by an instance of the\nRDMA-based memory allocator module hosted on one or more of the networked computing devices.  One or more of the networked computing devices are applied (1340) to host distribution tables and metadata delimiting parameters comprising references to the\nmemory regions, memory blocks and memory slabs of the shared memory.  In addition, an instance of an RDMA-based messaging module hosted on one or more of the computing devices is applied (1350) in combination with the distribution tables and metadata to\nconcurrently perform any combination of RDMA-based lock-free reads and writes on memory slabs of the distributed storage.\nSimilarly, as illustrated by FIG. 14, in various implementations, the Distributed Storage Controller begins operation by applying (1400) an RDMA-based memory allocator to allocate a shared memory comprising memory slabs allocated from memory\nblocks allocated from memory regions hosted by a plurality of networked computing devices in communication via RDMA-enabled NICs.  Each memory slab of the shared memory further comprises (1410) a plurality of memory elements and a metadata checksum value\ncomputed from each of the memory elements of that memory slab.  In addition, any of a plurality of the networked computing devices are applied (1420) to perform concurrent self-directed lock-free RDMA reads of particular memory slabs of the shared\nmemory.  Further, any of a plurality of the networked computing devices are applied (1430) to perform concurrent self-directed lock-free RDMA writes to a particular memory slabs of the shared memory following application of an RDMA CAS operation on a\ncorresponding entry in a memory slab reservation bitmap to reserve the particular memory slab.  Finally, an RDMA CAS operation is applied (1440) on the corresponding entry in the reservation bitmap to release the reservation of the particular memory slab\nfollowing completion of the RDMA write.\n4.0 Exemplary Implementations:\nThe following paragraphs summarize various examples of implementations of the Distributed Storage Controller that may be claimed in the present document.  The implementations summarized below are not intended to limit the subject matter that may\nbe claimed in view of the detailed description of the Distributed Storage Controller.  Further, any or all of the implementations summarized below may be claimed in any desired combination with some or all of the implementations described throughout the\ndetailed description and any implementations illustrated in one or more of the figures, and any other implementations and examples described below.  The following implementations and examples are intended to be understood in view of the detailed\ndescription and figures described throughout this document.\nIn various implementations, the Distributed Storage Controller applies an RDMA-based memory allocator to implement a distributed thread-safe and lock-free storage system in shared memory distributed across multiple networked computers.  The\nDistributed Storage Controller enables any number of client applications, processes, subroutines, etc., on any number of networked computers to concurrently perform self-directed lock-free allocations, de-allocations, reads, writes, etc., on the shared\nmemory via sequences of one-way RDMA messages (e.g., RDMA reads, RDMA writes, and RDMA atomic compare and swap (CAS) operations) without requiring CPU locks.  As such, the CPUs of computing devices hosting the shared memory do not need to be notified of\nRDMA-based reads, writes or CAS operations on that memory.  Consequently, CPU utilization for computing devices hosting any portion of the shared memory is minimal, thereby enabling the Distributed Storage Controller to scale to very large numbers of\nconcurrent accesses by very large numbers of networked computing devices.\nAs a first example, in various implementations, a distributed storage system is implemented via means, processes or techniques comprising a plurality of networked computing devices in communication via RDMA-enabled NICs.  In addition,\ndistributed storage distribution tables and metadata hosted on one or more of the networked computing devices.  This distributed storage comprises a plurality of memory regions allocated across two or more of the networked computing devices.  Further, a\nseparate instance of an RDMA-based messaging module hosted on each of a plurality of the networked computing devices.  A separate instance of an RDMA-based memory allocator module is hosted on each of a plurality of the networked computing devices. \nFinally, the RDMA-based memory allocator module of one or more of the computing devices and the RDMA-based messaging module of one or more of the computing devices are applied in combination with the distribution tables and metadata to concurrently\nperform any combination of RDMA-based lock-free memory operations on the distributed storage.\nAs a second example, in various implementations, the first example is further modified via means, processes or techniques wherein the lock-free memory operations further comprise any combination of lock-free memory allocations, lock-free memory\nde allocations, lock-free memory reads and lock-free memory writes on the distributed storage.\nAs a third example, in various implementations, the second example is further modified via means, processes or techniques wherein the lock-free memory allocations further comprise applying an instance of the RDMA-based memory allocator module to\nallocate one or more of the memory regions on one or more of the networked computing devices via a sequence of RDMA messages, applying an instance of the RDMA-based memory allocator module to allocate one or more memory blocks from one or more of the\nmemory regions via a sequence of RDMA messages, and applying an instance of the RDMA-based memory allocator module to allocate one or more memory slabs from one or more of the memory blocks via a sequence of RDMA messages.\nAs a fourth example, in various implementations, the third example is further modified via means, processes or techniques wherein each allocated memory region further comprises a metadata section comprising block headers and a free slab map.\nAs a fifth example, in various implementations, any of the third example and the fourth example are further modified via means, processes or techniques wherein each memory slab further comprises a slab metadata section including a slab checksum\nvalue computed from data held in a plurality of memory elements of the memory slab.\nAs a sixth example, in various implementations, any of the second example, the third example, the fourth example, and the fifth example are further modified via means, processes or techniques wherein the lock-free memory writes further comprise\nreserving a particular memory slab allocated from a corresponding memory region by performing an RDMA-based CAS operation to flip a bit of a corresponding entry in a memory reservation bitmap, and performing an RDMA-based write to the reserved memory\nslab.\nAs a seventh example, in various implementations, any of the second example, the third example, the fourth example, the fifth example, and the sixth example are further modified via means, processes or techniques wherein the lock-free memory\nreads further comprise performing an RDMA-based read operation on a particular memory slab of a particular memory region.\nAs an eighth example, in various implementations, any of the second example, the third example, the fourth example, the fifth example, the sixth example, and the seventh example, are further modified via means, processes or techniques further\ncomprising a separate instance of a co-processor module hosted on one or more of the networked computing devices, wherein the co-processor module provides an interface for arbitrary client applications to the distributed storage via an instance of the\nRDMA-based memory allocator module and an instance of the RDMA-based messaging module.\nAs a ninth example, in various implementations, any of the second example, the third example, the fourth example, the fifth example, the sixth example, the seventh example, and the eighth example are further modified via means, processes or\ntechniques further comprising applying the co-processor module to provide a B-Tree based key-value store in the distributed storage.\nAs a tenth example, in various implementations, any of the second example, the third example, the fourth example, the fifth example, the sixth example, the seventh example, the eighth example, and the ninth example are further modified via\nmeans, processes or techniques wherein any of the networked computing devices concurrently acts as any or all of a controller node for hosting the distribution tables and metadata of the distributed storage, a storage node for hosting a plurality of the\nmemory regions of the distributed storage, and a client node for performing any combination of lock-free memory operations on the distributed storage.\nAs an eleventh example, in various implementations, a computer-implemented process is implemented via means, processes or techniques comprising a shared memory comprising a plurality of memory regions allocated on two or more networked computing\ndevices in communication via RDMA-enabled NICs.  Each memory region is allocated by an instance of an RDMA-based memory allocator module hosted on one or more of the networked computing devices.  Further, one or more memory blocks being allocated from\neach memory region by an instance of the RDMA-based memory allocator module hosted on one or more of the networked computing devices.  In addition, one or more memory slabs is allocated from each memory block by an instance of the RDMA-based memory\nallocator module hosted on one or more of the networked computing devices.  This process also applies one or more of the networked computing devices to host distribution tables and metadata delimiting parameters comprising references to the memory\nregions, memory blocks and memory slabs of the shared memory.  Further, this process also applies an instance of an RDMA-based messaging module hosted on one or more of the computing devices in combination with the distribution tables and metadata to\nconcurrently perform any combination of RDMA-based lock-free reads and writes on memory slabs of the distributed storage.\nAs a twelfth example, in various implementations, the eleventh example is further modified via means, processes or techniques wherein each RDMA-based lock-free write further comprises applying an RDMA CAS operation on a corresponding entry in a\nmemory slab reservation bitmap to reserve the particular memory slab, applying an RDMA write operation on the particular memory slab following the reservation, and applying an RDMA CAS operation on the corresponding entry in the reservation bitmap to\nrelease the reservation of the particular node following the RDMA write.\nAs a thirteenth example, in various implementations, any of the eleventh example and the twelfth example are further modified via means, processes or techniques wherein each memory slab further comprises a metadata section comprising a checksum\nvalue computed from a combination of the data in each memory element of that memory slab.\nAs a fourteenth example, in various implementations, the thirteenth example is further modified via means, processes or technique wherein each RDMA-based lock-free read further comprises applying an RDMA read operation to read a particular\nmemory slab, computing a checksum from a combination of the data in each memory element of that memory slab, comparing the computed checksum to the checksum in the metadata section of the memory slab, and repeating the read of the memory slab and\ncomputation of the checksum until the computed checksum value matches the checksum value in the metadata of the memory slab.\nAs a fifteenth example, in various implementations, any of the eleventh example, the twelfth example, the thirteenth example and the fourteenth example are further modified via means, processes or techniques further comprising hosting a separate\ninstance of a co-processor module on one or more of the networked computing devices, and applying the co-processor module to provide an interface for arbitrary client applications to the shared memory via an instance of the RDMA-based memory allocator\nmodule and an instance of the RDMA-based messaging module.\nAs a sixteenth example, in various implementations, any of the eleventh example, the twelfth example, the thirteenth example, the fourteenth example and the fifteenth example are further modified via means, processes or techniques wherein any of\nthe networked computing devices concurrently acts as any or all of a controller node for hosting the distribution tables and metadata of the shared memory, a storage node for hosting a plurality of the memory regions of the shared memory, and a client\nnode for performing any combination of lock-free memory operations on the shared memory.\nAs a seventh example, in various implementations, a computer-readable storage device having computer executable instructions that cause a computing device to execute a method is implemented via means, processes or techniques for applying an\nRDMA-based memory allocator to allocate a shared memory comprising memory slabs allocated from memory blocks allocated from memory regions hosted by a plurality of networked computing devices in communication via RDMA-enabled NICs.  In addition, each\nmemory slab of the shared memory further comprising a plurality of memory elements and a metadata checksum value computed from each of the memory elements of that memory slab.  Further, this method applies any of a plurality of the networked computing\ndevices to perform concurrent self-directed lock-free RDMA reads of particular memory slabs of the shared memory.  In addition, this method applies any of a plurality of the networked computing devices to perform concurrent self-directed lock-free RDMA\nwrites to a particular memory slabs of the shared memory following application of an RDMA CAS operation on a corresponding entry in a memory slab reservation bitmap to reserve the particular memory slab.  Finally, this method applies an RDMA CAS\noperation on the corresponding entry in the reservation bitmap to release the reservation of the particular memory slab following completion of the RDMA write.\nAs an eighteenth example, in various implementations, the seventeenth example is further modified via means, processes or techniques wherein any concurrent self-directed RDMA read of any memory slab further comprises applying an RDMA read\noperation to read a particular memory slab, computing a checksum from a combination of the data in each memory element of that memory slab, comparing the computed checksum to the metadata checksum value of the memory slab, and repeating the read of the\nmemory slab and computation of the checksum until the computed checksum value matches the metadata checksum value.\nAs an nineteenth example, in various implementations, any of the seventeenth example and the eighteenth example are further modified via means, processes or techniques further comprising hosting a separate instance of a co-processor module on\none or more of the networked computing devices, and applying the co-processor module to provide an interface for arbitrary client applications to the shared memory via an instance of the RDMA-based memory allocator module and an instance of an RDMA-based\nmessaging module hosted on one or more of the networked computing devices.\nAs an twentieth example, in various implementations, any of the seventeenth example, the eighteenth example and the nineteenth example are further modified via means, processes or techniques wherein any of the networked computing devices\nconcurrently acts as any or all of a controller node for hosting distribution tables and metadata of the shared memory, a storage node for hosting a plurality of the memory regions of the shared memory, and a client node for performing any combination of\nlock-free memory operations on the shared memory.\n5.0 Exemplary Operating Environments:\nThe Distributed Storage Controller implementations described herein are operational within numerous types of general purpose or special purpose computing system environments or configurations.  FIG. 15 illustrates a simplified example of a\ngeneral-purpose computer system on which various implementations and elements of the Distributed Storage Controller, as described herein, may be implemented.  Any boxes that are represented by broken or dashed lines in the simplified computing device\n1500 shown in FIG. 15 represent alternate implementations of the simplified computing device.  As described below, any or all of these alternate implementations may be used in combination with other alternate implementations that are described throughout\nthis document.\nThe simplified computing device 1500 is typically found in devices having at least some minimum computational capability such as personal computers (PCs), server computers, handheld computing devices, laptop or mobile computers, communications\ndevices such as cell phones and personal digital assistants (PDAs), multiprocessor systems, microprocessor-based systems, set top boxes, programmable consumer electronics, network PCs, minicomputers, mainframe computers, and audio or video media players.\nTo allow a device to realize the Distributed Storage Controller implementations described herein, the device should have a sufficient computational capability and system memory to enable basic computational operations.  In particular, the\ncomputational capability of the simplified computing device 1500 shown in FIG. 15 is generally illustrated by one or more processing unit(s) 1510, and may also include one or more graphics processing units (GPUs) 1515, either or both in communication\nwith system memory 1520.  The processing unit(s) 1510 of the simplified computing device 1500 may be specialized microprocessors (such as a digital signal processor (DSP), a very long instruction word (VLIW) processor, a field-programmable gate array\n(FPGA), or other micro-controller) or can be conventional central processing units (CPUs) having one or more processing cores and that may also include one or more GPU-based cores or other specific-purpose cores in a multi-core processor.\nIn addition, the simplified computing device 1500 may also include other components, such as, for example, a network interface controller 1530.  The simplified computing device 1500 may also include one or more conventional computer input\ndevices 1540 (e.g., touchscreens, touch-sensitive surfaces, pointing devices, keyboards, audio input devices, voice or speech-based input and control devices, video input devices, haptic input devices, devices for receiving wired or wireless data\ntransmissions, and the like) or any combination of such devices.\nSimilarly, various interactions with the simplified computing device 1500 and with any other component or feature of the Distributed Storage Controller, including input, output, control, feedback, and response to one or more users or other\ndevices or systems associated with the Distributed Storage Controller, are enabled by a variety of Natural User Interface (NUI) scenarios.  The NUI techniques and scenarios enabled by the Distributed Storage Controller include, but are not limited to,\ninterface technologies that allow one or more users user to interact with the Distributed Storage Controller in a \"natural\" manner, free from artificial constraints imposed by input devices such as mice, keyboards, remote controls, and the like.\nSuch NUI implementations are enabled by the use of various techniques including, but not limited to, using NUI information derived from user speech or vocalizations captured via microphones or other input devices 1540 or system sensors 1505. \nSuch NUI implementations are also enabled by the use of various techniques including, but not limited to, information derived from system sensors 1505 or other input devices 1540 from a user's facial expressions and from the positions, motions, or\norientations of a user's hands, fingers, wrists, arms, legs, body, head, eyes, and the like, where such information may be captured using various types of 2D or depth imaging devices such as stereoscopic or time-of-flight camera systems, infrared camera\nsystems, RGB (red, green and blue) camera systems, and the like, or any combination of such devices.\nFurther examples of such NUI implementations include, but are not limited to, NUI information derived from touch and stylus recognition, gesture recognition (both onscreen and adjacent to the screen or display surface), air or contact-based\ngestures, user touch (on various surfaces, objects or other users), hover-based inputs or actions, and the like.  Such NUI implementations may also include, but are not limited to, the use of various predictive machine intelligence processes that\nevaluate current or past user behaviors, inputs, actions, etc., either alone or in combination with other NUI information, to predict information such as user intentions, desires, and/or goals.  Regardless of the type or source of the NUI-based\ninformation, such information may then be used to initiate, terminate, or otherwise control or interact with one or more inputs, outputs, actions, or functional features of the Distributed Storage Controller.\nHowever, the aforementioned exemplary NUI scenarios may be further augmented by combining the use of artificial constraints or additional signals with any combination of NUI inputs.  Such artificial constraints or additional signals may be\nimposed or generated by input devices 1540 such as mice, keyboards, and remote controls, or by a variety of remote or user worn devices such as accelerometers, electromyography (EMG) sensors for receiving myoelectric signals representative of electrical\nsignals generated by user's muscles, heart-rate monitors, galvanic skin conduction sensors for measuring user perspiration, wearable or remote biosensors for measuring or otherwise sensing user brain activity or electric fields, wearable or remote\nbiosensors for measuring user body temperature changes or differentials, and the like.  Any such information derived from these types of artificial constraints or additional signals may be combined with any one or more NUI inputs to initiate, terminate,\nor otherwise control or interact with one or more inputs, outputs, actions, or functional features of the Distributed Storage Controller.\nThe simplified computing device 1500 may also include other optional components such as one or more conventional computer output devices 1550 (e.g., display device(s) 1555, audio output devices, video output devices, devices for transmitting\nwired or wireless data transmissions, and the like).  Typical network interface controllers (NICs) 1530, input devices 1540, output devices 1550, and storage devices 1560 for general-purpose computers are well known to those skilled in the art, and will\nnot be described in detail herein.\nThe simplified computing device 1500 shown in FIG. 15 may also include a variety of computer-readable media.  Computer-readable media can be any available media that can be accessed by the computing device 1500 via storage devices 1560, and\ninclude both volatile and nonvolatile media that is either removable 1570 and/or non-removable 1580, for storage of information such as computer-readable or computer-executable instructions, data structures, program modules, or other data.\nComputer-readable media includes computer storage media and communication media.  Computer storage media refers to tangible computer-readable or machine-readable media or storage devices such as digital versatile disks (DVDs), Blu-ray discs\n(BD), compact discs (CDs), floppy disks, tape drives, hard drives, optical drives, solid state memory devices, random access memory (RAM), read-only memory (ROM), electrically erasable programmable read-only memory (EEPROM), CD-ROM or other optical disk\nstorage, smart cards, flash memory (e.g., card, stick, and key drive), magnetic cassettes, magnetic tapes, magnetic disk storage, magnetic strips, or other magnetic storage devices.  Further, a propagated signal is not included within the scope of\ncomputer-readable storage media.\nRetention of information such as computer-readable or computer-executable instructions, data structures, program modules, and the like, can also be accomplished by using any of a variety of the aforementioned communication media (as opposed to\ncomputer storage media) to encode one or more modulated data signals or carrier waves, or other transport mechanisms or communications protocols, and can include any wired or wireless information or content delivery mechanism.  The terms \"modulated data\nsignal\" or \"carrier wave\" generally refer to a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal.  For example, communication media can include wired media such as a wired network or\ndirect-wired connection carrying one or more modulated data signals, and wireless media such as acoustic, radio frequency (RF), infrared, laser, and other wireless media for transmitting and/or receiving one or more modulated data signals or carrier\nwaves.\nFurthermore, software, programs, and/or computer program products embodying some or all of the various Distributed Storage Controller implementations described herein, or portions thereof, may be stored, received, transmitted, or read from any\ndesired combination of computer-readable or machine-readable media or storage devices and communication media in the form of computer-executable instructions or other data structures.  Additionally, the claimed subject matter may be implemented as a\nmethod, apparatus, or article of manufacture using standard programming and/or engineering techniques to produce software, firmware 1525, hardware, or any combination thereof to control a computer to implement the disclosed subject matter.  The term\n\"article of manufacture\" as used herein is intended to encompass a computer program accessible from any computer-readable device, or media.\nThe Distributed Storage Controller implementations described herein may be further described in the general context of computer-executable instructions, such as program modules, being executed by a computing device.  Generally, program modules\ninclude routines, programs, objects, components, data structures, and the like, that perform particular tasks or implement particular abstract data types.  The Distributed Storage Controller implementations may also be practiced in distributed computing\nenvironments where tasks are performed by one or more remote processing devices, or within a cloud of one or more devices, that are linked through one or more communications networks.  In a distributed computing environment, program modules may be\nlocated in both local and remote computer storage media including media storage devices.  Additionally, the aforementioned instructions may be implemented, in part or in whole, as hardware logic circuits, which may or may not include a processor.\nAlternatively, or in addition, the functionality described herein can be performed, at least in part, by one or more hardware logic components.  For example, and without limitation, illustrative types of hardware logic components that can be\nused include field-programmable gate arrays (FPGAs), application-specific integrated circuits (ASICs), application-specific standard products (ASSPs), system-on-a-chip systems (SOCs), complex programmable logic devices (CPLDs), and so on.\n6.0 Other Implementations:\nThe foregoing description of the Distributed Storage Controller has been presented for the purposes of illustration and description.  It is not intended to be exhaustive or to limit the claimed subject matter to the precise form disclosed.  Many\nmodifications and variations are possible in light of the above teaching.  Further, any or all of the aforementioned alternate implementations may be used in any combination desired to form additional hybrid implementations of the Distributed Storage\nController.  It is intended that the scope of the Distributed Storage Controller be limited not by this detailed description, but rather by the claims appended hereto.  Although the subject matter has been described in language specific to structural\nfeatures and/or methodological acts, it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above.  Rather, the specific features and acts described above are\ndisclosed as example forms of implementing the claims and other equivalent features and acts are intended to be within the scope of the claims.\nWhat has been described above includes example implementations.  It is, of course, not possible to describe every conceivable combination of components or methodologies for purposes of describing the claimed subject matter, but one of ordinary\nskill in the art may recognize that many further combinations and permutations are possible.  Accordingly, the claimed subject matter is intended to embrace all such alterations, modifications, and variations that fall within the spirit and scope of\ndetailed description of the Distributed Storage Controller described above.\nIn regard to the various functions performed by the above described components, devices, circuits, systems and the like, the terms (including a reference to a \"means\") used to describe such components are intended to correspond, unless otherwise\nindicated, to any component which performs the specified function of the described component (e.g., a functional equivalent), even though not structurally equivalent to the disclosed structure, which performs the function in the herein illustrated\nexemplary aspects of the claimed subject matter.  In this regard, it will also be recognized that the foregoing implementations include a system as well as a computer-readable storage media having computer-executable instructions for performing the acts\nand/or events of the various methods of the claimed subject matter.\nThere are multiple ways of realizing the foregoing implementations (such as an appropriate application programming interface (API), tool kit, driver code, operating system, control, standalone or downloadable software object, or the like), which\nenable applications and services to use the implementations described herein.  The claimed subject matter contemplates this use from the standpoint of an API (or other software object), as well as from the standpoint of a software or hardware object that\noperates according to the implementations set forth herein.  Thus, various implementations described herein may have aspects that are wholly in hardware, or partly in hardware and partly in software, or wholly in software.\nThe aforementioned systems have been described with respect to interaction between several components.  It will be appreciated that such systems and components can include those components or specified sub-components, some of the specified\ncomponents or sub-components, and/or additional components, and according to various permutations and combinations of the foregoing.  Sub-components can also be implemented as components communicatively coupled to other components rather than included\nwithin parent components (e.g., hierarchical components).\nAdditionally, one or more components may be combined into a single component providing aggregate functionality or divided into several separate sub-components, and any one or more middle layers, such as a management layer, may be provided to\ncommunicatively couple to such sub-components in order to provide integrated functionality.  Any components described herein may also interact with one or more other components not specifically described herein but generally known to enable such\ninteractions.", "application_number": "14947473", "abstract": " A \"Distributed Storage Controller\" applies an RDMA-based memory allocator\n     to implement a distributed thread-safe and lock-free storage system in\n     shared memory distributed across multiple networked computers. The\n     Distributed Storage Controller enables any number of client applications,\n     processes, subroutines, etc., on any number of networked computers to\n     concurrently perform self-directed lock-free allocations, de-allocations,\n     reads, writes, etc., on the shared memory via sequences of one-way RDMA\n     messages (e.g., RDMA reads, RDMA writes, and RDMA atomic compare and swap\n     (CAS) operations) without requiring CPU locks. As such, the CPUs of\n     computing devices hosting the shared memory do not need to be notified of\n     RDMA-based reads, writes or CAS operations on that memory. Consequently,\n     CPU utilization for computing devices hosting any portion of the shared\n     memory is minimal, thereby enabling the Distributed Storage Controller to\n     scale to very large numbers of concurrent accesses by very large numbers\n     of networked computing devices.\n", "citations": ["6360220", "6418542", "6697878", "7403945", "7565454", "7610348", "7617290", "7617370", "7702743", "7756943", "7761619", "7917597", "8155135", "8205015", "8255047", "8271996", "8407428", "8578127", "8676851", "8688798", "8972627", "9058122", "9792248", "20020073257", "20020166078", "20030145230", "20030202247", "20040049580", "20040109234", "20040150717", "20040225719", "20090204755", "20120124282", "20120198284", "20120233438", "20130054726", "20130086183", "20130159449", "20130246714", "20130262384", "20140089346", "20140143368", "20140317336", "20140325012", "20140359044", "20140359145", "20140359146", "20150067086", "20150186051", "20160077975", "20160182433", "20160308968", "20160357702", "20160378712", "20170075856", "20170091246"], "related": []}, {"id": "20180007055", "patent_code": "10375079", "patent_name": "Distributed communication between internet of things devices", "year": "2019", "inventor_and_country_data": " Inventors: \nInfante-Lopez; Gabriel G. (Cordova, AR), Firby; Robert J. (Santa Clara, CA)  ", "description": "<BR><BR>BACKGROUND\nThe Internet of Things (IoT) is a network of physical objects or \"things\" embedded with electronics, software, sensors, and connectivity to enable greater value and service by exchange of data with the manufacturer, operator, or other connected\ndevices.  In many IoT systems, the individual physical objects may have limited or no computational capability (e.g., sensors), but merely send data to and receive instruction from a central server.\nThe development of the IoT may lead to the existence of a great number of devices, potentially each of them sending and receiving information to and from a central server.  The use of a central server may, in some circumstances, require\nsignificant computational resources, and may also result in privacy and security concerns for the data generated the IoT devices. <BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nThe concepts described herein are illustrated by way of example and not by way of limitation in the accompanying figures.  For simplicity and clarity of illustration, elements illustrated in the figures are not necessarily drawn to scale.  Where\nconsidered appropriate, reference labels have been repeated among the figures to indicate corresponding or analogous elements.\nFIGS. 1A & 1B are simplified block diagrams of at least one embodiment of a distributed IoT system;\nFIG. 2 is a simplified block diagram of at least one embodiment of an IoT device of the distributed IoT system of FIG. 1;\nFIG. 3 is a block diagram of at least one embodiment of an environment that may be established by the IoT device of FIG. 2;\nFIG. 4 is a simplified flow diagram of at least one embodiment of a method for initializing the IoT device of FIG. 2;\nFIG. 5 is a simplified flow diagram of at least one embodiment of a method for handling an input by the IoT device of FIG. 2;\nFIGS. 6 & 7 are a simplified flow diagram of at least one embodiment of a method for processing a message from a universal bus by the IoT device of FIG. 2;\nFIG. 8 is a simplified flow diagram of at least one embodiment of a method for responding to a query for information by the IoT device of FIG. 2;\nFIG. 9 is a simplified flow diagram of at least one embodiment of a method for processing an administrative event by the IoT device of FIG. 2; and\nFIG. 10 is a simplified flow diagram of at least one embodiment of a method for processing an administrative message by the IoT device of FIG. 2.\n<BR><BR>DETAILED DESCRIPTION OF THE DRAWINGS\nWhile the concepts of the present disclosure are susceptible to various modifications and alternative forms, specific embodiments thereof have been shown by way of example in the drawings and will be described herein in detail.  It should be\nunderstood, however, that there is no intent to limit the concepts of the present disclosure to the particular forms disclosed, but on the contrary, the intention is to cover all modifications, equivalents, and alternatives consistent with the present\ndisclosure and the appended claims.\nReferences in the specification to \"one embodiment,\" \"an embodiment,\" \"an illustrative embodiment,\" etc., indicate that the embodiment described may include a particular feature, structure, or characteristic, but every embodiment may or may not\nnecessarily include that particular feature, structure, or characteristic.  Moreover, such phrases are not necessarily referring to the same embodiment.  Further, when a particular feature, structure, or characteristic is described in connection with an\nembodiment, it is submitted that it is within the knowledge of one skilled in the art to effect such feature, structure, or characteristic in connection with other embodiments whether or not explicitly described.  Additionally, it should be appreciated\nthat items included in a list in the form of \"at least one A, B, and C\" can mean (A); (B); (C): (A and B); (B and C); (A and C); or (A, B, and C).  Similarly, items listed in the form of \"at least one of A, B, or C\" can mean (A); (B); (C): (A and B); (B\nand C); (A and C); or (A, B, and C).\nThe disclosed embodiments may be implemented, in some cases, in hardware, firmware, software, or any combination thereof.  The disclosed embodiments may also be implemented as instructions carried by or stored on one or more transitory or\nnon-transitory machine-readable (e.g., computer-readable) storage medium, which may be read and executed by one or more processors.  A machine-readable storage medium may be embodied as any storage device, mechanism, or other physical structure for\nstoring or transmitting information in a form readable by a machine (e.g., a volatile or non-volatile memory, a media disc, or other media device).\nIn the drawings, some structural or method features may be shown in specific arrangements and/or orderings.  However, it should be appreciated that such specific arrangements and/or orderings may not be required.  Rather, in some embodiments,\nsuch features may be arranged in a different manner and/or order than shown in the illustrative figures.  Additionally, the inclusion of a structural or method feature in a particular figure is not meant to imply that such feature is required in all\nembodiments and, in some embodiments, may not be included or may be combined with other features.\nReferring now to FIG. 1A, an illustrative distributed Internet of Things (IoT) system 100 includes several IoT devices 102 connected by a network 104.  The distributed IoT system 100 is illustratively deployed in a house 106, but of course the\ndistributed IoT system 100 can be used in other environments as well, such as an office setting, a retail setting, a restaurant setting, a manufacturing setting, an industrial setting, a construction setting, etc. In use and as described in more detail\nbelow, each IoT device 102 of the distributed IoT system 100 acts as an independent agent, without relying on a central server.  Each IoT device 102 has at least one input or output device (such as a microphone or light switch) as well as a knowledge\nbase, which includes information known to the IoT device 102 such as nearby devices, past input or output received by the IoT device 102, and/or past user commands.  Each IoT device 102 may implement reasoning algorithms and/or machine-learning-based\nalgorithms to find correlation between events, determine responses to queries for information, and determine appropriate responses to user input.  The distributed knowledge base of the distributed IoT system 100 contained by the various IoT devices 102\nallows for the determination of answers to complex queries.  In the illustrative distributed IoT system 100, each IoT device 102 is subscribed to a universal bus, allowing each IoT device 102 to easily communicate with each other IoT device 102 when\nnecessary.\nFor example, in an illustrative use case of the distributed IoT system 100, a user may turn on a series of lights each time the user enters the house 106.  Over time, the IoT devices 102 recognize the pattern of what actions the user performs\nusing a machine-learning-based algorithm, and the series of lights may be subsequently turned on automatically when the user enters the house 106.\nThe network 104 may be implemented with any communication means that enables the IoT devices 102 to communicate with each other.  In the illustrative embodiment, the only necessary hardware for the network 104 is the IoT devices 102 themselves,\nwhich may be able to communicate with nearby devices through wireless connections such as Wi-Fi.RTM.  or Bluetooth.RTM..  In one example, the network 104 is embodied as a wireless ad-hoc network.  It should be appreciated that it is not necessary for\neach IoT device 102 to be able to directly communicate with each other IoT device 102, since an IoT device 102 may communicate with a desired IoT device 102 with use of one or more intermediate IoT devices 102.\nIn some embodiments, wired connections such as Ethernet may be used in addition to or as an alternative to wireless connections for the network 104.  In some cases, additional hardware elements such as routers or switches may be present in the\nnetwork 104.  In the illustrative embodiment, no IoT device 102 is connected to a network other than the network 104 connecting the IoT devices 102.  Additionally or alternatively, some or all of the IoT devices 102 may be connected to an additional\nnetwork, such as the Internet.\nReferring now to FIG. 1B, an embodiment of the illustrative distributed Internet of Things (IoT) system 100 is shown with the network 104 embodied as several direct connections 108 between IoT devices 102.  In the embodiment shown in FIG. 1B,\nthere is no central server.  In this embodiment, each IoT device 102 may be considered as a node on a graph, and each connection 108 between two IoT devices 102 may be considered as an edge on the graph.  For any two IoT devices 102 in the distributed\nIoT system 100, as long as there is some path to transverse the graph from one IoT device 102 to the other, communication between the two IoT devices 102 is possible.  Each connection 108 between two IoT devices 102 may be any type of wired or wireless\ncommunication connection (e.g., Ethernet, Bluetooth.RTM., Wi-Fi.RTM., WiMAX, near field communication (NFC), etc.).\nReferring now to FIG. 2, the illustrative IoT device 102 may be embodied as any type of compute device capable of performing the functions described herein.  For example, the IoT device 102 may be embodied as or otherwise be included in, without\nlimitation, a sensor device, an embedded computing system, a System-on-a-Chip (SoC), a control device (e.g., a light or temperature controller), a desktop computer, a server computer, a tablet computer, a notebook computer, a laptop computer, a\nsmartphone, a cellular phone, a wearable computer, a handset, a messaging device, a camera device, a multiprocessor system, a processor-based system, a consumer electronic device, and/or any other computing device.  The illustrative IoT device 102\nincludes a processor 202, a memory 204, an input/output (I/O) subsystem 206, one or more input devices 208 and/or one or more output devices 210, a communication circuit 212, and data storage 214.  In some embodiments, one or more of the illustrative\ncomponents of the IoT device 102 may be incorporated in, or otherwise form a portion of, another component.  For example, the memory 204, or portions thereof, may be incorporated in the processor 202 in some embodiments.\nThe processor 202 may be embodied as any type of processor capable of performing the functions described herein.  For example, the processor 202 may be embodied as a single or multi-core processor(s), a single or multi-socket processor, a\ndigital signal processor, a graphics processor, a microcontroller, or other processor or processing/controlling circuit.  Similarly, the memory 204 may be embodied as any type of volatile or non-volatile memory or data storage capable of performing the\nfunctions described herein.  In operation, the memory 204 may store various data and software used during operation of the IoT device 102 such as operating systems, applications, programs, libraries, and drivers.  The memory 204 is communicatively\ncoupled to the processor 202 via the I/O subsystem 206, which may be embodied as circuitry and/or components to facilitate input/output operations with the processor 202, the memory 204, and other components of the IoT device 102.  For example, the I/O\nsubsystem 206 may be embodied as, or otherwise include, memory controller hubs, input/output control hubs, firmware devices, communication links (i.e., point-to-point links, bus links, wires, cables, light guides, printed circuit board traces, etc.)\nand/or other components and subsystems to facilitate the input/output operations.  In some embodiments, the I/O subsystem 206 may form a portion of a system-on-a-chip (SoC) and be incorporated, along with the processor 202, the memory 204, and other\ncomponents of the IoT device 102 on a single integrated circuit chip.\nThe input device 208 may be any device capable of generating an input signal (e.g., in response to a stimulus) that may be useful to the distributed IoT system 100, such as a light sensor 216, a light switch 218, a motion sensor 220, a\nmicrophone 222, a camera 224, etc. The output devices 210 may include any device capable of generating an output that may be useful to the distributed IoT system 100, such as a light 226 or a speaker 228.  Of course, the input devices 208 and the output\ndevices 210 shown in FIG. 2 are not meant to be limiting, but merely provide a small sample of possible input and output devices.  For example, additional devices such as a motor or actuator may be included in some embodiments.  The broad nature of\n\"things\" that may be included as an IoT device 102 precludes a thorough listing of all devices, and as such the devices explicitly shown in FIG. 2 should not be considered limiting.  In the illustrative embodiment, each IoT device 102 of the distributed\nIoT system 100 includes one input device 208 or one output device 210.  Of course, in some embodiments, some of all of the IoT devices 102 may include any combination of input devices 208 and output devices 210, or even no input devices 208 and no output\ndevices 210.\nThe communication circuit 212 may be embodied as any type of communication circuit, device, or collection thereof, capable of enabling communications between the IoT device 102 and other devices.  To do so, the communication circuit 212 may be\nconfigured to use any one or more communication technology and associated protocols (e.g., Ethernet, Bluetooth.RTM., Wi-Fi.RTM., WiMAX, near field communication (NFC), etc.) to effect such communication.\nThe data storage 214 may be embodied as any type of device or devices configured for the short-term or long-term storage of data.  For example, the data storage 214 may include any one or more memory devices and circuits, memory cards, hard disk\ndrives, solid-state drives, or other data storage devices.\nOf course, in some embodiments, the IoT device 102 may include other or additional components, such as those commonly found in a compute device.  For example, the IoT device 102 may also have peripheral devices 230 and/or a display 232.  The\nperipheral devices 230 may include a keyboard, a mouse, etc. In some cases, a peripheral device or a display may be included as an input device 208, an output device 210, or both (e.g., a touchscreen display).\nThe optional display 232 may be embodied as any type of display on which information may be displayed to a user of the IoT device 102, such as a liquid crystal display (LCD), a light emitting diode (LED) display, a cathode ray tube (CRT)\ndisplay, a plasma display, an image projector (e.g., 2D or 3D), a laser projector, a touchscreen display, a heads-up display, and/or other display technology.\nReferring now to FIG. 3, in use, each IoT device 102 may establish an environment 300.  The illustrative environment 300 includes a configuration module 302, and administrative message monitor module 304, the knowledge representation module 306,\nthe input/output device interface module 308, the communication module 310, the query monitor module 312, and the reasoning module 314.  The various modules of the environment 300 may be embodied as hardware, software, firmware, or a combination thereof. For example, the various modules, logic, and other components of the environment 300 may form a portion of, or otherwise be established by, the processor 202 or other hardware components of the IoT device 102.  As such, in some embodiments, one or more\nof the modules of the environment 300 may be embodied as circuitry or collection of electrical devices (e.g., a configuration circuit 302, an administrative message monitor circuit 304, a knowledge representation circuit 306, etc.).  It should be\nappreciated that, in such embodiments, one or more of the circuits (e.g., the configuration circuit 302, the administrative message monitor circuit 304, the knowledge representation circuit 306, etc.) may form a portion of one or more of the processor\n202, the memory 204, the I/O subsystem 206, and/or the data storage 214.  Additionally, in some embodiments, one or more of the illustrative modules may form a portion of another module and/or one or more of the illustrative modules may be independent of\none another.\nSome of the modules of the IoT device 102 may be configured in some embodiments to act as independent actors of the IoT device 102 with a specific role.  For example, the administrative message monitor module 304, the query monitor module 312,\nand the reasoning module 314, may each be configured to act as independent actors performing the corresponding function described for each of those modules.  In the illustrative embodiment, each actor may, in response to a message it receives, make local\ndecisions, create more actors, send more messages, and/or determine how to respond to the next message received.  In some embodiments, the actors may have more or fewer capabilities.  Of course, this configuration is merely one possible design choice,\nand the functionality described for each module may be implemented without the modules acting as independent actors.\nThe configuration module 302 is configured to determine and apply automatic or manual configuration settings.  In the illustrative embodiment, if manual configuration settings are available when the IoT device 102 is initialized (i.e., turned\non), the manual configuration settings are then applied.  If manual configuration settings are not available when the IoT device 102 is initialized, the configuration module 302 may apply automatic configuration settings, such as by determining available\ninput devices 208 and output devices 210, applying default settings, etc. The configuration settings may be manually updated at any time after initialization in the illustrative embodiment.  In some embodiments, the configuration module 302 may apply or\nupdate the manual configuration settings prior to initialization, such as at the time of manufacture of the IoT device 102.  The configuration settings may include settings such as the capabilities of input devices 208 and/or output devices 210 of the\nIoT device 102, an indication of the environment of the IoT device 102, and privacy settings of the IoT device 102.  The manual configuration settings may be supplied by any means, such as by a wired connection, a wireless connection, removable media\nsuch as a USB memory stick, a peripheral device 230, etc.\nThe administrative message monitor module 304 is configured to monitor communication from other IoT devices 102 that are communicated to the IoT device 102 through the universal bus, through directed communication, or through other communication\nmeans.  The administrative message monitor module 304 is configured to determine if a message received by the IoT device 102 is an administrative message and to update the knowledge representation module 306 accordingly.  An administrative message may be\nembodied as any message describing the capability, functionality, or configuration of one or more of the IoT devices 102 of the distributed IoT system 100, as opposed to a message indicating an input from a user or a query for information.  For example,\nan administrative message may indicate that a new IoT device 102 has joined the network 104 of the distributed IoT system 100, and may indicate the capability of that new IoT device 102.  As another example, an administrative message may indicate that an\nIoT device 102 of the distributed IoT system 100 is not functioning properly.\nThe knowledge representation module 306 is configured to represent or otherwise store knowledge or information available to and relevant for the function of the IoT device 102.  The knowledge representation module 306 may include a semantic\nnetwork and/or an ontology, and be able to represent relationships between pieces of information stored in the knowledge representation module 306.  In the illustrative embodiment, the knowledge representation module 306 is not directly accessible by any\nother IoT device 102.  In some embodiments, such as embodiments with independent actors, there may be several knowledge representation modules 306, such as a separate knowledge representation module 306 for each actor.\nThe input/output device interface module 308 is configured to interface with the input devices 208 and output devices 210 present on the IoT device 102.  The input/output device interface module 308 may employ any means to interface with the\ninput devices 208 and output devices 210 such as electrical wires, a mechanical interface, a wireless interface, etc. The input/output device interface module 308 is configured to send appropriate messages to the universal bus or to other IoT devices 102\nupon receipt of an input.\nThe communication module 310 is configured to send and receive communication messages to and from other IoT devices 102.  The communication between IoT devices 102 may be accomplished through any appropriate communication protocol.  In the\nillustrative embodiment, there is no central server, and each IoT device 102 in the distributed IoT system 100 is able to communicate directly with at least one other IoT device 102.  As described above in regard to FIG. 1B, in such an embodiment, each\nIoT device 102 may be considered as a node on a graph, and each connection 108 between two IoT devices 102 may be considered as an edge on a graph.  For any two IoT devices 102 in the distributed IoT system 100, as long as there is some way of traversing\nthe graph from one IoT device 102 to the other, communication between the two is possible.\nThe communication module 310 includes a universal bus communication module 316.  The universal bus communication module 316 allows the IoT device 102 to subscribe to the universal bus, receive messages on the universal bus, send messages on the\nuniversal bus, and relay messages on the universal bus.  In the illustrative embodiment, the universal bus is embodied as a bus implemented by the collective behavior of all of the IoT devices 102 that are subscribed to the universal bus that ensures\neach message sent to the universal bus is received by every IoT device 102 subscribed to the universal bus.  Any suitable protocol may be used to implement the universal bus, such as a graph traversal, a flooding algorithm, or other peer-to-peer\nalgorithm.  In some embodiments, a gossip protocol module 318 may implement a gossip protocol.  In the illustrative embodiment, each IoT device 102 is aware of each other IoT device 102 subscribed to the universal bus.  In some embodiments, each IoT\ndevice 102 may only be aware of a portion of the other IoT devices 102 subscribed to the universal bus, such as each directly-connected IoT device 102.  In order to subscribe to the universal bus, an IoT device 102 sends a message indicating the request\nto one or more directly-connected IoT devices 102 that are subscribed to the universal bus.  The subscription message may either be sent only to the directly-connected IoT devices 102 or may be sent out onto the universal bus itself.  If an IoT device\n102 wishes to unsubscribe from the universal bus, it may do so by sending an unsubscribe message in a similar manner.\nThe communication module 310 also includes a directed communication module 320.  The directed communication module 320 is configured to send, relay, or receive a message that is directed to one or more specified IoT devices 102.  When the\nspecified IoT device 102 is not directly-connected to the source IoT device 102 that is the original source of the message, the source IoT device 102 may send the message to one or more directly-connected IoT devices 102, which will then relay the\nmessage to the specified IoT device 102 using any suitable protocol.  Of course, the directed communication module 320 may send a message directly to a specified IoT device 102 if there is a direct connection between the two IoT devices 102.\nThe query monitor module 312 is configured to monitor and analyze messages received from the universal bus as well as messages directed to the IoT device 102.  Each message is analyzed to determine whether the message contains a query.  A query\nmay be a request for information, an instruction from a user, an input from an input device 208, an instruction from another IoT device 102, information being disseminated by another IoT device 102, or any other query relating to the input or output of\nthe distributed IoT system 100.\nThe reasoning module 314 is configured to apply a reasoning algorithm to determine a course of action for the IoT device 102 to take in response to an input from a user of a message from another IoT device 102.  The reasoning module 314 may\nemploy any type of reasoning algorithm, such as a constraint solver, a logic program, a rules engine, etc. The reasoning module 314 includes a machine learning module 322 and a natural language processing module 322.\nThe machine learning module 322 is configured to apply one or more machine learning algorithms to recognize and respond to patterns in the inputs received by the distributed IoT system 100.  For example, if each time a user of the distributed\nIoT system 100 comes home, the user may turn on a specific sequence of lights.  The machine learning module 322 may recognize the pattern, and, if the IoT device 102 controls one of those lights, automatically turn on the light when the user comes home. \nThe machine learning module 322 may employ any one or more suitable machine learning algorithms, such as a neural network (including a deep neural network), a support vector machine, cluster analysis, etc.\nThe natural language processing module 324 is configured to parse commands spoken by the user that may be received by the input/output device interface module 308 or the communication module 310.  For example, the user might say, \"Light all\nlights,\" and after receiving that message the natural language processing module 324 may send a message to the universal bus that all lights should be turned on.  In some embodiments, the natural language processing module 324 may be a\nmachine-learning-based algorithm, and be fully trained to recognize natural language.  Additionally or alternatively, the natural language processing module 324 may employ machine learning techniques to train itself based on input from the user.  Of\ncourse, in some embodiments, natural language processing may not be required in order to respond to some or all commands of the user of the IoT system 100.\nIn some embodiments, such as embodiments with a microphone 222, the IoT device 102 may include an automatic speech recognition module 326.  The automatic speech recognition module 326 is configured to transcribe speech data of the user that is\ncaptured by the microphone 222.  In some embodiments, the microphone 222 may always be capturing sound data, and the automatic speech recognition module 326 may always be processing data captured by the microphone 222 looking for speech data.  In other\nembodiments, the microphone 222 may only be capturing sound data when, e.g., a user presses a button.  The automatic speech recognition module 326 includes a query identification module 328, which is configured to process transcribed speech data and\ndetermine if it includes a query.  Of course, if the microphone 222 is always capturing sound, the microphone 222 will often capture speech data not related to any query of the distributed IoT system 100, such as a conversation the user is having.  When\nthe query identification module 328 identifies a query, it responds appropriately, such as by performing an action, sending the query to the universal bus, or sending a directed communication containing the query to a specified IoT device 102.\nReferring now to FIG. 4, in use, the IoT device 102 may execute a method 400 for configuring the IoT device 102.  It should be appreciated that, in some embodiments, the techniques of the method 400 may be executed by one or more of the modules\nof the IoT device 102 as shown in FIG. 3.  The method 400 begins in block 402, in which the IoT device 102 determines whether automatic or manual configuration of the IoT device 102 will be used.  In the illustrative embodiment, the IoT device 102 may\ndetermine that manual configuration will be used by determining that manual configuration data is being received or is otherwise available.  In such an embodiment, operating the IoT device 102 may be the first indication that automatic configuration will\nbe used.  Of course, even if automatic configuration is initially used, manual configuration may still be done after operation of the IoT device 102 commences.  In other embodiments, the IoT device 102 may be configured to wait for manual configuration\nbefore becoming operational.  In block 404, the IoT device 102 proceeds to block 406 if manual configuration will be used and proceeds to block 414 if automatic configuration will be used.\nIn block 406, the IoT device 102 receives manual configuration data.  The IoT device 102 may receive the manual configuration data through any communication means, such as a wired or wireless communication, jumper pins, or switches on the IoT\ndevice 102 such as dual in-line package (DIP) switches.  In some embodiments, the manual configuration data may be stored in the IoT device 102 at the time of manufacture.  In other embodiments, the manual configuration data may be received by the IoT\ndevice 102 from an end user of the distributed IoT system 100.  The manual configuration data may indicate any type of configuration data for operating the IoT device 102, such as the input device(s) 208 and/or output device(s) 210 that are connected,\nthe environment of the IoT device 102 (such as home, office, industrial, etc.), the communication protocol to be used, training data or configuration data for any machine-learning-based algorithms, information to store in the knowledge representation,\nprogrammed responses to certain inputs (either from an input from an input device 208 connected to the IoT device 102 or an input received from another IoT device 102), etc. In some embodiments, the IoT device 102 may receive the manual configuration\ndata by determining what input device(s) 208 and/or output device(s) 210 are connected.\nOptionally, the IoT device 102 receives privacy settings in block 408.  The privacy settings may indicate what sort of data should and should not be shared and under what circumstances.  For example, the privacy settings may indicate that the\nIoT device 102 may indicate whether a person is currently present in a room, but only to certain other IoT devices 102 of the distributed IoT system 100.\nIn block 410, the IoT device 102 applies the manual configuration settings that were received in block 406.  In embodiments wherein privacy settings were received in block 408, the privacy settings are applied in block 412.\nIn block 414, the IoT device 102 determines which other IoT devices 102 of the distributed IoT system 100 are directly connected to the IoT device 102.  For example, the IoT device 102 may determine which other IoT devices 102 are directly\nconnected through a wireless connection (i.e., within range of the wireless signal) by sending a special announcement message that informs any IoT devices 102 in range that a new IoT device 102 is available.  Each IoT device 102 in range may then reply\nto the IoT device 102 that sent the announcement message.  In some embodiments, the IoT device(s) 102 that received the announcement message transmits the announcement message to other IoT devices 102 of the distributed IoT system 100 by sending the\nmessage on the universal bus.  Each IoT device 102 may, in some embodiments, periodically update which other IoT devices 102 are directly connected, such as once per day.\nIn block 416, the IoT device 102 subscribes to the universal bus.  The IoT device 102 may subscribe to the universal bus by sending a subscription message to one or more directly-connected IoT devices 102.  In some embodiments, the one or more\ndirectly-connected IoT devices 102 may transmit the subscription message to the universal bus.\nIn block 418, the IoT device 102 requests information from the other IoT devices 102 that are subscribed to the universal bus.  For example, the IoT device 102 may request information indicating each input device 208 and output device 210\nconnected to each other IoT device 102.  The IoT device 102 may also request information indicative of patterns that other IoT devices 102 have recognized or conclusions that other IoT devices 102 have reached, such as that certain lights should be\nturned on under certain conditions.\nIn block 420, the IoT device 102 receives the requested information from the other IoT devices 102.  In block 422 the IoT device 102 updates the knowledge representation with the information that was received from the other IoT devices 102.\nReferring now to FIG. 5, in use, the IoT device 102 may execute a method 500 for handling an input from an input device 208.  It should be appreciated that, in some embodiments, the techniques of the method 500 may be executed by one or more of\nthe modules of the IoT device 102 as shown in FIG. 3.  The method 500 beings in block 502, in which the IoT device 102 receives an input from an input device 208.  In embodiments with a microphone 222, the IoT device 102 may capture audio from a user in\nblock 504.  In block 506, the IoT device 102 may perform automatic speech recognition on the captured audio, and in block 508 the IoT device 102 may identify any query that may be present in the transcribed speech of the captured audio.\nIn embodiments with a light sensor 216 or a motion sensor 220, the IoT device 102 may receive an input indicative of movement in block 510, such as a user walking along a hallway.  In embodiments with a light switch 218, the IoT device 102 may\nreceive an indication of a change of state of the light switch 218 (e.g., that the switch was flipped).  Of course, in some embodiments, additional input devices 208 other than those shown in FIG. 2 may be present, and may provide input in a similar\nmanner as the input devices 208 explicitly described above.\nIn block 514, the IoT device 102 updates the knowledge representation based on the input, such as by storing the input.  In block 516, the IoT device 102 determines whether to send a message based on the input to one or more specified IoT\ndevices 102 with use of directed communication or to send the message on the universal bus.  For a generic input, the IoT device 102 will send the message on the universal bus.  However, in some cases, the IoT device 102 may recognize a pattern that the\ninput fits (such as by using the machine learning module 322), and then determine that the message need only be sent to one or more specified IoT devices 102.  For example, the IoT device 102 may receive as an input that a user is walking down the\nhallway.  If the IoT device 102 has determined, by using the machine learning module 322, that every time the user walks down the hallway she turns on a certain light, the IoT device 102 may send a message only to the IoT device 102 associated with that\nlight.  Of course, in some cases, the IoT device 102 may determine that the input received in block 502 is not significant or otherwise not of interest to other IoT devices 102, and not send any message at all.\nIn block 518, if the IoT device 102 is to use directed communication, the method 500 proceeds to block 520.  Otherwise, if the IoT device 102 is to use the universal bus, the method 500 proceeds to block 522.\nIn block 520, the IoT device 102 sends a message with directed communication to one or more specified other IoT devices 102.  In block 522, the IoT device 102 broadcasts the message to the universal bus.  In the illustrative embodiment, the\ndistributed IoT system 100 relays the message along the universal bus by performing a graph traversal in block 524.  In either case, the message contains or otherwise indicates the input that the IoT device 102 received in block 502.\nReferring now to FIG. 6, in use, the IoT device 102 may perform a method 600 for processing a message indicative of an input from the universal bus.  It should be appreciated that, in some embodiments, the techniques of the method 600 may be\nexecuted by one or more of the modules of the IoT device 102 as shown in FIG. 3.  The method 600 begins in block 602, in which the IoT device 102 receives a message from the universal bus.  In some cases, depending on certain factors such as the position\nof the IoT device 102 in the graph of connected IoT devices 102 and the protocol used for the universal bus, the IoT device 102 may relay the message to one or more other IoT devices 102 in block 604.  In block 606, the IoT device 102 updates the\nknowledge representation based on the message, such as by storing the message.\nIn block 608, the IoT device 102 analyzes the message with reasoning algorithms and/or machine-learning-based algorithms.  For example, the IoT device 102 may employ a machine-learning-based algorithm to determine if a particular output is\nexpected of the IoT device 102 based on the message received indicative of the input as well as past messages received indicative of past input.\nIn block 610, the IoT device 102 determines whether the IoT device 102 is a target of a query based on the message.  For example, if the input indicated by the message is a motion sensed by a motion sensor 220, and the IoT device 102 may\ndetermine that it is a target of a query, since in the past this sensed motion has indicated that a certain light controlled by the IoT device 102 should turn on.  In block 612, the IoT device 102 determines, in response to determining that the IoT\ndevice 102 is a target of the query, whether the IoT device 102 should perform an action.  In block 614, the IoT device 102 updates the machine-learning-based algorithm based on the received message.\nIn block 616, the IoT device 102 determines whether information is needed from another IoT device 102.  For example, the query may be a motion sensed by a motion sensor 220, and the IoT device 102 may determine that it should turn on a light if\nit is dark outside (e.g., if it is night).  In this example, the IoT device 102 may determine that information is required as to whether or not it is dark outside.  The method 600 then proceeds to block 618 in FIG. 7.\nIn block 618 in FIG. 7, if information is needed from another IoT device 102, the method 600 proceeds to block 620.  Otherwise, if information is not needed from another IoT device 102, the method 600 proceeds to block 632.\nIn block 620, the IoT device 102 sends a query to one or more other IoT devices 102 with a request for information.  In cases in which the IoT device 102 knows that the information is available at one or more specified IoT devices 102, the IoT\ndevice 102 may send directed communication to the one or more specified IoT devices 102 in block 622.  In cases where the IoT device 102 does not know that the information is available at a specific IoT device 102, the IoT device 102 broadcasts the query\nto the universal bus in block 624.  In some embodiments, in block 626, the IoT device 102 broadcasting the query to the universal bus initiates a graph traversal to send the message to some or all of the other IoT devices 102, such as all IoT devices 102\nsubscribed to the universal bus.  In block 628, the IoT device 102 receives the requested information from other IoT devices 102.  In some instances, depending on the nature of the information and the privacy settings of the IoT device 102 sending and/or\nrequesting the information, the communication of the requested information may be encrypted.  In block 628, the IoT device 102 updates the determination made in block 612 as to whether the IoT device 102 should perform an action based on the received\ninformation.\nIn block 632, if the IoT device 102 should perform an action, the method 600 proceeds to block 634.  If the IoT device 102 should not perform an action, the method 600 proceeds to the end of the method 600 after block 634.  In block 634, the IoT\ndevice 102 performs an action based on the message.  For example, the IoT device 102 may control an output device 210 based on the message in block 636.\nReferring now to FIG. 8, the IoT device 102 may execute a method 800 for responding to a query for information from another IoT device 102.  It should be appreciated that, in some embodiments, the techniques of the method 800 may be executed by\none or more of the modules of the IoT device 102 as shown in FIG. 3.  The method 800 begins in block 802, wherein the IoT device 102 receives a query for information from another IoT device 102.\nIn block 804, the IoT device 102 determines whether the relevant information is available in the knowledge representation.  In block 806, if the information is available, the method 800 proceeds to block 808.  Otherwise, if the information is\nnot available, the method 800 proceeds to block 816.\nIn block 808, the IoT device 102 determines whether the privacy settings allow for sharing the information with the requesting IoT device 102.  In block 810, if sharing the information is allowed, the method 800 proceeds to block 812. \nOtherwise, if sharing is not allowed, the method 800 proceeds to block 816.\nIn block 812, the IoT device 102 sends the requested information to the other IoT device 102.  Depending on certain factors such as the nature of the information and the privacy settings, the IoT device 102 may send the requested information to\nthe other IoT device 102 with use of encryption.\nReferring back to blocks 806 and 810, if the information is not available in the knowledge representation or the information is not allowed to be shared, the IoT device 102 jumps to block 816, in which the IoT device 102 sends a notification to\nthe requesting IoT device 102 that the information is not available.  In some embodiments, the IoT device 102 may specify whether the information is not in the knowledge representation or the information is not allowed to be shared.  In other\nembodiments, the IoT device 102 may simply notify the requesting IoT device 102 that it is unable to fulfill the request for information without detailing why.  In still other embodiments, the IoT device 102 may not send any notification at all.\nReferring now to FIG. 9, the IoT device 102 may execute a method 900 for processing an administrative event.  It should be appreciated that, in some embodiments, the techniques of the method 900 may be executed by one or more of the modules of\nthe IoT device 102 as shown in FIG. 3.  An administrative event is any event that affects the status of the distributed IoT system 100, such as available IoT devices 102 and their capabilities or the status of any connection 108 of the distributed IoT\nsystem 100.  The method 900 begins in block 902, in which the IoT device 102 determines an administrative event.  The IoT device 102 may determine an administrative event relating to itself in block 904, in which the IoT device 102 determines from a\nself-test that it is not operating properly, such as a non-functioning input device 208 or output device 210.  The IoT device 102 may also determine an administrative event that another IoT device 102 is not operating properly in block 906, such as by\ndetermining that the IoT device 102 is not responsive or that the IoT device 102 is responding with garbled or otherwise improper messages.  Of course, in some embodiments, the IoT device 102 may determine additional or alternative administrative events,\nsuch as events relating to a connection 108 with another IoT device 102.\nIn block 908, the IoT device 102 updates the knowledge representation based on the administrative event.  In block 910, the IoT device 102 sends a message indicative of the administrative event to the universal bus.\nReferring now to FIG. 10, the IoT device 102 may execute a method 1000 for processing an administrative message.  It should be appreciated that, in some embodiments, the techniques of the method 1000 may be executed by one or more of the modules\nof the IoT device 102 as shown in FIG. 3.  The method 1000 begins in block 1002, in which the IoT device 102 receives an administrative message from another IoT device 102.  In block 1004, the IoT device 102 updates the knowledge representation based on\nthe administrative message.\nIn order to further clarify the scope of the concepts disclosed herein, several example use cases are provided.  In one use case, a first IoT device 102 with a microphone 222 captures a user saying, \"Turn on all lights.\" The first IoT device 102\nperforms automatic speech recognition to transcribe the user's speech and then sends the message out on the universal bus.  Each other IoT device 102 of the distributed IoT system 100 receives the message, and each IoT device 102 that has a light 226\ndetermines it is the target of a query to turn on its light 226 and then turns on its light 226.\nIn another use case, a first IoT device 102 with a microphone 222 captures a user saying, \"Turn on the lights in the dining room.\" The first IoT device 102 performs automatic speech recognition to transcribe the user's speech and then sends the\nmessage out on the universal bus.  Each other IoT device 102 of the distributed IoT system 100 receives the message, and each IoT device 102 that has a light 226 and is in the dining room determines it is the target of a query to turn on its light 226\nand then turns on its light 226.  In this use case, an IoT device 102 that does not know where it is may decide not to turn on its light 226.  If the user then turns on the light 226 of that IoT device 102 manually, the IoT device 102 may determine that\nit is in the dining room, and update its knowledge representation accordingly.\nIn another use case, a first IoT device 102 with a microphone 222 captures a user saying, \"Turn on the lights where the kids are.\" The first IoT device 102 performs automatic speech recognition to transcribe the user's speech and then sends the\nmessage out on the universal bus.  Each other IoT device 102 of the distributed IoT system 100 receives the message, and a target IoT device 102 with a light 226 determines it is a target of a query, but may need more information.  The target IoT device\n102 sends a message to the universal bus asking if kids are present in the room of the target IoT device 102.  An additional IoT device 102 with a motion sensor 220 or a camera 224 may detect that a kid is present in that room.  The additional IoT device\n102 will check its privacy settings, and if the privacy settings permit, send a message to the target IoT device 102 that a kid is present in the room.  The target IoT device 102 may then turn on its light 226 based on the query from the user and the\ninformation received from the additional IoT device 102.\nIn yet another use case, a first IoT device 102 with a microphone 222 captures a user saying, \"Turn on the lights where any person is.\" The first IoT device 102 performs automatic speech recognition to transcribe the user's speech and then sends\nthe message out on the universal bus.  Each other IoT device 102 of the distributed IoT system 100 receives the message, and a target IoT device 102 with a light 226 determines it is a target of a query, but may need more information.  The target IoT\ndevice 102 sends a message to the universal bus asking if anyone is present in the room of the target IoT device 102.  An additional IoT device 102 with a motion sensor 220 or a camera 224 may detect that a person is present in that room.  The additional\nIoT device 102 will check its privacy settings, and if the privacy settings permit, send a message to the target IoT device 102 that a person is present in the room.  The target IoT device 102 may then turn on its light 226 based on the message from the\nfirst IoT device 102 and the information received from the additional IoT device 102.\nIn a further use case, a first IoT device 102 with a microphone 222 captures a user saying, \"Turn on the lights where Bob was at 5 PM.\" The first IoT device 102 performs automatic speech recognition to transcribe the user's speech and then sends\nthe message out on the universal bus.  Each other IoT device 102 of the distributed IoT system 100 receives the message, and a target IoT device 102 with a light 226 determines it is a target of a query, but may need more information.  The target IoT\ndevice 102 sends a message to the universal bus asking if Bob was present in the room of the target IoT device 102 at 5 PM.  An additional IoT device 102 with a camera 224 may have stored in its knowledge representation who was present in that room at 5\nPM based on facial recognition of images captured at that time.  The additional IoT device 102 will check its privacy settings, and if the privacy settings permit, send a message to the target IoT device 102 that Bob was not in the room at 5 PM.  The\ntarget IoT device 102 may then determine that it need not perform an action based on the message from the first IoT device 102 and the information received from the additional IoT device 102.\nIt should be appreciated that each of FIGS. 4-10 may be executed by some or all of the modules of the IoT device 102.  In particular, in some embodiments, such as embodiments with independent actors, different parts of the flowcharts may be\nexecuted by different modules of the IoT device 102, and the control of the execution of the flowcharts may be controlled by messages passed between modules.\n<BR><BR>EXAMPLES\nIllustrative examples of the devices, systems, and methods disclosed herein are provided below.  An embodiment of the devices, systems, and methods may include any one or more, and any combination of, the examples described below.\nExample 1 includes an Internet of Things (IoT) device comprising a communication module to receive, from a universal bus to which the IoT device is subscribed, a message transmitted by another IoT device to the universal bus, wherein the message\nis indicative of an input from a user; a query monitor module to determine whether the IoT device is a target of a query based on the message; a reasoning module to determine an action to be performed in response to a determination that the IoT device is\nthe target of the query; and perform, in response to the determination that the IoT device is the target of the query, the action.\nExample 2 includes the subject matter of Example 1, and wherein to determine the action to be performed comprises to determine, based on a machine-learning-based algorithm of the IoT device, the action to be performed.\nExample 3 includes the subject matter of any of Examples 1 and 2, and wherein the reasoning module is further to update the machine-learning-based algorithm based on the input.\nExample 4 includes the subject matter of any of Examples 1-3, and wherein the reasoning module is further to determine, with the machine-learning-based algorithm and in response to the determination that the IoT device is the target of the\nquery, whether information is needed from one or more additional IoT devices for the IoT device to perform the action; wherein the communication module is further to (i) send, in response to a determination that the information is needed from the one or\nmore additional IoT devices, an information query to the one or more additional IoT devices and (ii) receive, from the one or more additional IoT devices, the information; and wherein to determine, based on the machine-learning-based algorithm, the\naction to be performed comprises to determine, based on the machine-learning-based algorithm and the information, an action to be performed.\nExample 5 includes the subject matter of any of Examples 1-4, and wherein to determine whether the information is needed from one or more additional IoT devices comprises to determine, based on the machine-learning-based algorithm, that the\ninformation is needed from a specified IoT device, and wherein to send the query to the one or more additional IoT devices comprises to send the query with use of directed communication to the specified IoT device.\nExample 6 includes the subject matter of any of Examples 1-5, and wherein to send the query to the one or more additional IoT devices comprises to send the query to the universal bus.\nExample 7 includes the subject matter of any of Examples 1-6, and wherein to send the query to the universal bus comprises to send the query to each of a plurality of IoT devices to which the IoT device is directly connected, wherein the query\ncomprises an indication that the query is intended for the universal bus.\nExample 8 includes the subject matter of any of Examples 1-7, and further including a knowledge representation module to update a knowledge representation of the IoT device based on the input, wherein the knowledge representation defines\ninformation usable by the IoT device to perform the action.\nExample 9 includes the subject matter of any of Examples 1-8, and wherein to determine whether the IoT device is the target of the query comprises to access a knowledge representation of the IoT device, wherein the knowledge representation\ndefines information usable by the IoT device to perform the action.\nExample 10 includes the subject matter of any of Examples 1-9, and wherein to determine the action to be performed comprises to access a knowledge representation of the IoT device, wherein the knowledge representation defines information usable\nby the IoT device to perform the action.\nExample 11 includes the subject matter of any of Examples 1-10, and wherein to perform the action comprises to control an output device of the IoT device.\nExample 12 includes the subject matter of any of Examples 1-11, and wherein the output device comprises a light, a speaker, a motor, or an actuator.\nExample 13 includes the subject matter of any of Examples 1-12, and wherein to receive, from the universal bus, the message indicative of the input from the user comprises to receive the message from a source IoT device directly connected to the\nIoT device; to determine whether the message should be sent to one or more additional IoT devices different from the source IoT device and directly connected to the IoT device; and to send, in response to a determination that the message should be sent\nto the one or more additional IoT devices, the message to the one or more additional IoT devices.\nExample 14 includes the subject matter of any of Examples 1-13, and further including a knowledge representation module, wherein the knowledge representation module comprises a knowledge representation that defines information usable by an\nadditional IoT device to perform an additional action, wherein the communication module is further to receive an information query for the information from the additional IoT device; wherein the knowledge representation module is to determine whether the\ninformation is in the knowledge representation and to determine, based on a privacy setting of the IoT device, whether sharing of the information is allowed; and wherein the communication module is further to send, based on a determination that the\ninformation is in the knowledge representation and a determination that sharing of the information is allowed, the information to the additional IoT device.\nExample 15 includes the subject matter of any of Examples 1-14, and wherein to send the information to the additional IoT device comprises to encrypt the information and send the encrypted information to the additional IoT device.\nExample 16 includes the subject matter of any of Examples 1-15, and wherein the communication module is further to determine one or more directly-connected IoT devices to which the IoT device is directly connected; and subscribe the IoT to the\nuniversal bus, wherein to subscribe the IoT to the universal bus comprises to send a subscription message to each of the one or more directly-connected IoT devices.\nExample 17 includes the subject matter of any of Examples 1-16, and wherein the communication module is further to request information from each IoT device of a plurality of IoT devices subscribed to the universal bus; and receive information\nfrom each IoT device of the plurality of IoT devices subscribed to the universal bus.\nExample 18 includes the subject matter of any of Examples 1-17, and further including a configuration module to receive manual configuration settings defined by the user; and configure the IoT device based on the manual configuration settings,\nwherein to configure the IoT device comprises to update a knowledge representation of the IoT device based on the manual configuration settings, wherein the knowledge representation defines information usable by the IoT device to perform the action.\nExample 19 includes the subject matter of any of Examples 1-18, and further including an administrative message monitor module to determine an administrative event indicative of an operational characteristic of the IoT device or another IoT\ndevice; and a knowledge representation module to update a knowledge representation based on the administrative event, wherein the knowledge representation defines information usable by the IoT device to perform the action, wherein the communication\nmodule is further to send, to the universal bus, an administrative message indicative of the administrative event.\nExample 20 includes the subject matter of any of Examples 1-19, and wherein to determine the administrative event comprises to determine that the IoT device is not operating properly.\nExample 21 includes the subject matter of any of Examples 1-20, and wherein to determine the administrative event comprises to determine that an additional IoT device is not operating properly.\nExample 22 includes the subject matter of any of Examples 1-21, and wherein the communication module is further to receive, from the universal bus, an administrative message indicative of an operational characteristic of the IoT device or\nanother IoT device, further comprising a knowledge representation module to update a knowledge representation based on the administrative message.\nExample 23 includes the subject matter of any of Examples 1-22, and wherein the communication module is further to receive, through directed communication from an additional IoT device, an administrative message indicative of an operational\ncharacteristic of the IoT device or another IoT device; and further comprising a knowledge representation module to update a knowledge representation based on the administrative message, wherein the knowledge representation defines information usable by\nthe IoT device to perform the action.\nExample 24 includes an Internet of Things (IoT) device comprising an input/output device interface module to receive, with use of an input device, a first input from a user of the IoT device and a second input from the user of the IoT device; a\nreasoning module to determine, with use of a machine-learning-based algorithm, whether to (i) communicate a first message indicative of the first input with use of directed communication to one or more other IoT devices or (ii) communicate the first\nmessage to a universal bus to which the IoT device is subscribed; determine, with use of the machine-learning-based algorithm, whether to (i) communicate a second message indicative of the second input with use of directed communication to one or more\nother IoT devices or (ii) communicate the second message to the universal bus; a communication module to send, in response to a determination to communicate the first message to the universal bus, the first message to the universal bus; send, in response\nto a determination to communicate the second message with use of directed communication to the one or more other IoT devices, the second message with use of directed communication to the one or more other IoT devices.\nExample 25 includes the subject matter of Example 24, and wherein the input device comprises a microphone and wherein the first input comprises captured audio data, further comprising an automatic speech recognition module to perform automatic\nspeech recognition on the captured audio data to generate transcribed speech data; and determine whether the transcribed speech data comprises a query, wherein to determine whether to communicate the first message indicative of the first input with use\nof directed communication to one or more other IoT devices or to communicate the first message to the universal bus to which the IoT device is subscribed comprises to determine, based on a determination that the transcribed speech data comprises the\nquery, whether to communicate the first message indicative of the first input with use of directed communication to one or more other IoT devices or to communicate the first message to the universal bus to which the IoT device is subscribed.\nExample 26 includes the subject matter of any of Examples 24 and 25, and wherein the input device comprises a motion sensor.\nExample 27 includes the subject matter of any of Examples 24-26, and wherein the input device comprises a switch.\nExample 28 includes the subject matter of any of Examples 24-27, and wherein the input device comprises a light sensor.\nExample 29 includes the subject matter of any of Examples 24-28, and wherein the input device comprises a camera.\nExample 30 includes a method for operating an Internet of Things (IoT) device, the method comprising receiving, by the IoT device and from a universal bus to which the IoT device is subscribed, a message transmitted by another IoT device to the\nuniversal bus, wherein the message is indicative of an input from a user; determining, by the IoT device, whether the IoT device is a target of a query based on the message; determining, by the IoT device, an action to be performed in response to a\ndetermination that the IoT device is the target of the query; and performing, by the IoT device and in response to the determination that the IoT device is the target of the query, the action.\nExample 31 includes the subject matter of Example 30, and wherein determining the action to be performed comprises determining, based on a machine-learning-based algorithm of the IoT device, the action to be performed.\nExample 32 includes the subject matter of any of Examples 30 and 31, and further including updating, by the IoT device, the machine-learning-based algorithm based on the input.\nExample 33 includes the subject matter of any of Examples 30-32, and further including determining, by the IoT device and with the machine-learning-based algorithm, whether information is needed from one or more additional IoT devices for the\nIoT device to perform the action in response to the determination that the IoT device is the target of the query; sending, by the IoT device and in response to a determination that the information is needed from the one or more additional IoT devices, an\ninformation query to the one or more additional IoT devices; and receiving, by the IoT device and from the one or more additional IoT devices, the information, wherein determining, by the IoT device and based on the machine-learning-based algorithm, the\naction to be performed comprises determining, by the IoT device and based on the machine-learning-based algorithm and the information, an action to be performed.\nExample 34 includes the subject matter of any of Examples 30-33, and wherein determining whether the information is needed from one or more additional IoT devices comprises determining, based on the machine-learning-based algorithm, that the\ninformation is needed from a specified IoT device, and wherein sending the query to the one or more additional IoT devices comprises sending the query with use of directed communication to the specified IoT device.\nExample 35 includes the subject matter of any of Examples 30-34, and wherein sending the query to the one or more additional IoT devices comprises sending the query to the universal bus.\nExample 36 includes the subject matter of any of Examples 30-35, and wherein sending the query to the universal bus comprises sending the query to each of a plurality of IoT devices to which the IoT device is directly connected, wherein the\nquery comprises an indication that the query is intended for the universal bus.\nExample 37 includes the subject matter of any of Examples 30-36, and further including updating, by the IoT device, a knowledge representation of the IoT device based on the input, wherein the knowledge representation defines information usable\nby the IoT device to perform the action.\nExample 38 includes the subject matter of any of Examples 30-37, and wherein determining whether the IoT device is the target of the query comprises accessing a knowledge representation of the IoT device, wherein the knowledge representation\ndefines information usable by the IoT device to perform the action.\nExample 39 includes the subject matter of any of Examples 30-38, and wherein determining the action to be performed comprises accessing a knowledge representation of the IoT device, wherein the knowledge representation defines information usable\nby the IoT device to perform the action.\nExample 40 includes the subject matter of any of Examples 30-39, and wherein performing the action comprises controlling an output device of the IoT device.\nExample 41 includes the subject matter of any of Examples 30-40, and wherein the output device comprises a light, a speaker, a motor, or an actuator.\nExample 42 includes the subject matter of any of Examples 30-41, and wherein receiving, by the IoT device and from the universal bus, the message indicative of the input from the user comprises receiving, by the IoT device, the message from a\nsource IoT device directly connected to the IoT device; determining, by the IoT device, whether the message should be sent to one or more additional IoT devices different from the source IoT device and directly connected to the IoT device; and sending,\nby the IoT device and in response to a determination that the message should be sent to the one or more additional IoT devices, the message to the one or more additional IoT devices.\nExample 43 includes the subject matter of any of Examples 30-42, and further including receiving, by the IoT device, an information query for information from an additional IoT device; determining, by the IoT device, whether the information is\nin a knowledge representation of the IoT device that defines the information usable by the additional IoT device to perform an additional action; determining, by the IoT device and based on a privacy setting of the IoT device, whether sharing of the\ninformation is allowed; and sending, by the IoT device and based on a determination that the information is in the knowledge representation and a determination that sharing of the information is allowed, the information to the additional IoT device.\nExample 44 includes the subject matter of any of Examples 30-43, and wherein sending the information to the additional IoT device comprises encrypting the information and sending the encrypted information to the additional IoT device.\nExample 45 includes the subject matter of any of Examples 30-44, and further including determining, by the IoT device, one or more directly-connected IoT devices to which the IoT device is directly connected; and subscribing the IoT to the\nuniversal bus, wherein subscribing the IoT to the universal bus comprises sending a subscription message to each of the one or more directly-connected IoT devices.\nExample 46 includes the subject matter of any of Examples 30-45, and further including requesting information from each IoT device of a plurality of IoT devices subscribed to the universal bus; and receiving information from each IoT device of\nthe plurality of IoT devices subscribed to the universal bus.\nExample 47 includes the subject matter of any of Examples 30-46, and further including receiving, by the IoT device, manual configuration settings defined by the user; and configuring, by the IoT device, the IoT device based on the manual\nconfiguration settings, wherein configuring the IoT device comprises updating a knowledge representation of the IoT device based on the manual configuration settings, wherein the knowledge representation defines information usable by the IoT device to\nperform the action.\nExample 48 includes the subject matter of any of Examples 30-47, and further including determining, by the IoT device, an administrative event indicative of an operational characteristic of the IoT device or another IoT device; updating, by the\nIoT device, a knowledge representation based on the administrative event, wherein the knowledge representation defines information usable by the IoT device to perform the action; and sending, by the IoT device and to the universal bus, an administrative\nmessage indicative of the administrative event.\nExample 49 includes the subject matter of any of Examples 30-48, and wherein determining the administrative event comprises determining that the IoT device is not operating properly.\nExample 50 includes the subject matter of any of Examples 30-49, and wherein determining the administrative event comprises determining that an additional IoT device is not operating properly.\nExample 51 includes the subject matter of any of Examples 30-50, and further including receiving, by the IoT device and from the universal bus, an administrative message indicative of an operational characteristic of the IoT device or another\nIoT device; and updating, by the IoT device, the knowledge representation based on the administrative message, wherein the knowledge representation defines information usable by the IoT device to perform the action.\nExample 52 includes the subject matter of any of Examples 30-51, and further including receiving, by the IoT device and through directed communication from an additional IoT device, an administrative message indicative of an operational\ncharacteristic of the IoT device or another IoT device; and updating, by the IoT device, the knowledge representation based on the administrative message, wherein the knowledge representation defines information usable by the IoT device to perform the\naction.\nExample 53 includes a method for operating an Internet of Things (IoT) device, the method comprising receiving, by the IoT device and with use of an input device, a first input from a user; determining, by the IoT device and with use of a\nmachine-learning-based algorithm, whether to (i) communicate a first message indicative of the first input with use of directed communication to one or more other IoT devices or (ii) communicate the first message to a universal bus to which the IoT\ndevice is subscribed; sending, by the IoT device and in response to a determination to communicate the first message to the universal bus, the first message to the universal bus; receiving, by the IoT device and with use of the input device, a second\ninput from the user; determining, by the IoT device and with use of the machine-learning-based algorithm, whether to (i) communicate a second message indicative of the second input with use of directed communication to the one or more other IoT devices\nor (ii) communicate the message to the universal bus; and sending, by the IoT device and in response to a determination to communicate the second message with use of directed communication to the one or more other IoT devices, the second message with use\nof directed communication to the one or more other IoT devices.\nExample 54 includes the subject matter of Example 53, and wherein the input device comprises a microphone and wherein the first input comprises captured audio data, further comprising performing, by the IoT device, automatic speech recognition\non the captured audio data to generate transcribed speech data; and determining, by the IoT device, whether the transcribed speech data comprises a query, wherein determining whether to communicate the first message indicative of the first input with use\nof directed communication to one or more other IoT devices or to communicate the first message to the universal bus to which the IoT device is subscribed comprises determining, based on a determination that the transcribed speech data comprises the\nquery, whether to communicate the first message indicative of the first input with use of directed communication to one or more other IoT devices or to communicate the first message to the universal bus to which the IoT device is subscribed.\nExample 55 includes the subject matter of any of Examples 53 and 54, and wherein the input device comprises a motion sensor.\nExample 56 includes the subject matter of any of Examples 53-55, and wherein the input device comprises a switch.\nExample 57 includes the subject matter of any of Examples 53-56, and wherein the input device comprises a light sensor.\nExample 58 includes the subject matter of any of Examples 53-57, and wherein the input device comprises a camera.\nExample 59 includes one or more computer readable media comprising a plurality of instructions stored thereon that, when executed, cause an Internet of Things (IoT) device to perform the method of any of Examples 30-58.\nExample 60 includes an Internet of Things (IoT) device comprising means for receiving, from a universal bus to which the IoT device is subscribed, a message transmitted by another IoT device to the universal bus, wherein the message is\nindicative of an input from a user; means for determining whether the IoT device is a target of a query based on the message; means for determining an action to be performed in response to a determination that the IoT device is the target of the query;\nand means for performing, in response to the determination that the IoT device is the target of the query, the action.\nExample 61 includes the subject matter of Example 60, and wherein the means for determining the action to be performed comprises means for determining, based on a machine-learning-based algorithm of the IoT device, the action to be performed.\nExample 62 includes the subject matter of any of Examples 60 and 61, and further including means for updating, by the IoT device, the machine-learning-based algorithm based on the input.\nExample 63 includes the subject matter of any of Examples 60-62, and further including means for determining, with the machine-learning-based algorithm, whether information is needed from one or more additional IoT devices for the IoT device to\nperform the action in response to the determination that the IoT device is the target of the query; means for sending, in response to a determination that the information is needed from the one or more additional IoT devices, an information query to the\none or more additional IoT devices; and means for receiving, from the one or more additional IoT devices, the information, wherein the means for determining, based on the machine-learning-based algorithm, the action to be performed comprises means for\ndetermining, based on the machine-learning-based algorithm and the information, an action to be performed.\nExample 64 includes the subject matter of any of Examples 60-63, and wherein the means for determining whether the information is needed from one or more additional IoT devices comprises means for determining, based on the machine-learning-based\nalgorithm, that the information is needed from a specified IoT device, and wherein the means for sending the query to the one or more additional IoT devices comprises means for sending the query with use of directed communication to the specified IoT\ndevice.\nExample 65 includes the subject matter of any of Examples 60-64, and wherein the means for sending the query to the one or more additional IoT devices comprises means for sending the query to the universal bus.\nExample 66 includes the subject matter of any of Examples 60-65, and wherein the means for sending the query to the universal bus comprises means for sending the query to each of a plurality of IoT devices to which the IoT device is directly\nconnected, wherein the query comprises an indication that the query is intended for the universal bus.\nExample 67 includes the subject matter of any of Examples 60-66, and further including means for updating a knowledge representation of the IoT device based on the input, wherein the knowledge representation defines information usable by the IoT\ndevice to perform the action.\nExample 68 includes the subject matter of any of Examples 60-67, and wherein the means for determining whether the IoT device is the target of the query comprises means for accessing a knowledge representation of the IoT device, wherein the\nknowledge representation defines information usable by the IoT device to perform the action.\nExample 69 includes the subject matter of any of Examples 60-68, and wherein the means for determining the action to be performed comprises means for accessing a knowledge representation of the IoT device, wherein the knowledge representation\ndefines information usable by the IoT device to perform the action.\nExample 70 includes the subject matter of any of Examples 60-69, and wherein the means for performing the action comprises means for controlling an output device of the IoT device.\nExample 71 includes the subject matter of any of Examples 60-70, and wherein the output device comprises a light, a speaker, a motor, or an actuator.\nExample 72 includes the subject matter of any of Examples 60-71, and wherein the means for receiving, from the universal bus, the message indicative of the input from the user comprises means for receiving the message from a source IoT device\ndirectly connected to the IoT device; means for determining whether the message should be sent to one or more additional IoT devices different from the source IoT device and directly connected to the IoT device; and means for sending, in response to a\ndetermination that the message should be sent to the one or more additional IoT devices, the message to the one or more additional IoT devices.\nExample 73 includes the subject matter of any of Examples 60-72, and further including means for receiving an information query for information from an additional IoT device; means for determining whether the information is in a knowledge\nrepresentation of the IoT device that defines the information usable by the additional IoT device to perform an additional action; means for determining, based on a privacy setting of the IoT device, whether sharing of the information is allowed; and\nmeans for sending, based on a determination that the information is in the knowledge representation and a determination that sharing of the information is allowed, the information to the additional IoT device.\nExample 74 includes the subject matter of any of Examples 60-73, and wherein the means for sending the information to the additional IoT device comprises means for encrypting the information and sending the encrypted information to the\nadditional IoT device.\nExample 75 includes the subject matter of any of Examples 60-74, and further including means for determining one or more directly-connected IoT devices to which the IoT device is directly connected; and means for subscribing the IoT to the\nuniversal bus, wherein the means for subscribing the IoT to the universal bus comprises means for sending a subscription message to each of the one or more directly-connected IoT devices.\nExample 76 includes the subject matter of any of Examples 60-75, and further including means for requesting information from each IoT device of a plurality of IoT devices subscribed to the universal bus; and means for receiving information from\neach IoT device of the plurality of IoT devices subscribed to the universal bus.\nExample 77 includes the subject matter of any of Examples 60-76, and further including means for receiving manual configuration settings defined by the user; and means for configuring the IoT device based on the manual configuration settings,\nwherein the means for configuring the IoT device comprises means for updating a knowledge representation of the IoT device based on the manual configuration settings, wherein the knowledge representation defines information usable by the IoT device to\nperform the action.\nExample 78 includes the subject matter of any of Examples 60-77, and further including means for determining an administrative event indicative of an operational characteristic of the IoT device or another IoT device; means for updating a\nknowledge representation based on the administrative event, wherein the knowledge representation defines information usable by the IoT device to perform the action; and means for sending, to the universal bus, an administrative message indicative of the\nadministrative event.\nExample 79 includes the subject matter of any of Examples 60-78, and wherein the means for determining the administrative event comprises means for determining that the IoT device is not operating properly.\nExample 80 includes the subject matter of any of Examples 60-79, and wherein the means for determining the administrative event comprises means for determining that an additional IoT device is not operating properly.\nExample 81 includes the subject matter of any of Examples 60-80, and further including means for receiving, from the universal bus, an administrative message indicative of an operational characteristic of the IoT device or another IoT device;\nand means for updating the knowledge representation based on the administrative message, wherein the knowledge representation defines information usable by the IoT device to perform the action.\nExample 82 includes the subject matter of any of Examples 60-81, and further including means for receiving, through directed communication from an additional IoT device, an administrative message indicative of an operational characteristic of\nthe IoT device or another IoT device; and means for updating the knowledge representation based on the administrative message, wherein the knowledge representation defines information usable by the IoT device to perform the action.\nExample 83 includes an Internet of Things (IoT) device comprising means for receiving, with use of an input device, a first input from a user; means for determining, with use of a machine-learning-based algorithm, whether to (i) communicate a\nfirst message indicative of the first input with use of directed communication to one or more other IoT devices or (ii) communicate the first message to a universal bus to which the IoT device is subscribed; means for sending, in response to a\ndetermination to communicate the first message to the universal bus, the first message to the universal bus; means for receiving, with use of the input device, a second input from the user; means for determining, with use of the machine-learning-based\nalgorithm, whether to (i) communicate a second message indicative of the second input with use of directed communication to the one or more other IoT devices or (ii) communicate the message to the universal bus; and means for sending, in response to a\ndetermination to communicate the second message with use of directed communication to the one or more other IoT devices, the second message with use of directed communication to the one or more other IoT devices.\nExample 84 includes the subject matter of Example 83, and wherein the input device comprises a microphone and wherein the first input comprises captured audio data, further comprising means for performing automatic speech recognition on the\ncaptured audio data to generate transcribed speech data; and means for determining whether the transcribed speech data comprises a query, wherein the means for determining whether to communicate the first message indicative of the first input with use of\ndirected communication to one or more other IoT devices or to communicate the first message to the universal bus to which the IoT device is subscribed comprises means for determining, based on a determination that the transcribed speech data comprises\nthe query, whether to communicate the first message indicative of the first input with use of directed communication to one or more other IoT devices or to communicate the first message to the universal bus to which the IoT device is subscribed.\nExample 85 includes the subject matter of any of Examples 83 and 84, and wherein the input device comprises a motion sensor.\nExample 86 includes the subject matter of any of Examples 83-85, and wherein the input device comprises a switch.\nExample 87 includes the subject matter of any of Examples 83-86, and wherein the input device comprises a light sensor.\nExample 88 includes the subject matter of any of Examples 83-87, and wherein the input device comprises a camera.", "application_number": "15196229", "abstract": " Technologies for a distributed Internet of Things (IoT) system are\n     disclosed. Several IoT devices may form a peer-to-peer network without\n     requiring a central server. Information may be stored in a distributed\n     manner in the distributed IoT system, allowing for storing information\n     without transmitting it to a remote server, which may be costly and\n     introduce security or privacy risks. Each IoT device of the distributed\n     IoT system includes a machine learning algorithm that is capable of\n     uncovering patterns in the input of the distributed IoT system, such as a\n     pattern of user inputs in certain situations, and the distributed IoT\n     system may adaptively anticipate a user's intentions.\n", "citations": ["9571906", "9699814", "20140359131", "20150358777", "20150379158", "20160004871", "20160132816", "20160198536", "20160337322", "20160342906", "20170091277", "20170094592"], "related": []}, {"id": "20180042031", "patent_code": "10375707", "patent_name": "Dynamic resource allocation in wireless network", "year": "2019", "inventor_and_country_data": " Inventors: \nHampel; Karl Georg (New York, NY), Li; Junyi (Chester, NJ), Subramanian; Sundar (Bridgewater, NJ), Islam; Muhammad Nazmul (Edison, NJ), Abedini; Navid (Raritan, NJ)  ", "description": "<BR><BR>BACKGROUND\nThe following relates generally to wireless communication, and more specifically to dynamic resource allocation in a wireless network.\nWireless communications systems are widely deployed to provide various types of communication content such as voice, video, packet data, messaging, broadcast, and so on.  These systems may be capable of supporting communication with multiple\nusers by sharing the available system resources (e.g., time, frequency, and power).  Examples of such multiple-access systems include code division multiple access (CDMA) systems, time division multiple access (TDMA) systems, frequency division multiple\naccess (FDMA) systems, and orthogonal frequency division multiple access (OFDMA) systems, (e.g., a Long Term Evolution (LTE) system).  A wireless multiple-access communications system may include a number of access nodes, each simultaneously supporting\ncommunication for multiple communication devices, which may be otherwise known as user equipment (UE).\nWireless communications systems may include access nodes to facilitate wireless communication between user equipment and a network.  For example, an LTE base station may provide a mobile device access to the internet via the LTE wireless\nnetwork.  Access nodes typically have a high-capacity, wired, backhaul connection (e.g., fiber) to the network.  In some deployments, however, it may be desirable to deploy a larger number of access nodes in a small area to provide acceptable coverage to\nusers.  In such deployments, it may be impracticable to connect each access node to the network via a wired connection.\n<BR><BR>SUMMARY\nThe described techniques provide for a backhaul network that may be established between access nodes and/or base stations.  To support communications via the backhaul network, a synchronized frame structure and unique network topologies may be\nestablished.  Resources may be allocated to different wireless communication links based at least in part on the synchronized frame structure.  Occupancy/availability indications are shown and described, which enable the local redistribution of resources\nto account for variations in signal quality and/or variations in traffic experienced by the backhaul network.\nA method of wireless communication is described.  The method may include determining whether data is available to be transmitted via a first communication link using a radio access technology (RAT) that supports a synchronized frame structure,\nreceiving an indication whether a scheduled resource of a second communication link is available during a time interval defined by the synchronized frame structure, and transmitting the data via the first communication link using the scheduled resource\nduring the time interval based at least in part on the indication indicating that the scheduled resource of the second communication link is available during the time interval.\nAn apparatus for wireless communication is described.  The apparatus may include means for determining whether data is available to be transmitted via a first communication link using a radio access technology (RAT) that supports a synchronized\nframe structure, means for receiving an indication whether a scheduled resource of a second communication link is available during a time interval defined by the synchronized frame structure, and means for transmitting the data via the first\ncommunication link using the scheduled resource during the time interval based at least in part on the indication indicating that the scheduled resource of the second communication link is available during the time interval.\nAnother apparatus for wireless communication is described.  The apparatus may include a processor, memory in electronic communication with the processor, and instructions stored in the memory.  The instructions may be operable to cause the\nprocessor to determine whether data is available to be transmitted via a first communication link using a radio access technology (RAT) that supports a synchronized frame structure, receive an indication whether a scheduled resource of a second\ncommunication link is available during a time interval defined by the synchronized frame structure, and transmit the data via the first communication link using the scheduled resource during the time interval based at least in part on the indication\nindicating that the scheduled resource of the second communication link is available during the time interval.\nA non-transitory computer-readable medium for wireless communication is described.  The non-transitory computer-readable medium may include instructions operable to cause a processor to determine whether data is available to be transmitted via a\nfirst communication link using a radio access technology (RAT) that supports a synchronized frame structure, receive an indication whether a scheduled resource of a second communication link is available during a time interval defined by the synchronized\nframe structure, and transmit the data via the first communication link using the scheduled resource during the time interval based at least in part on the indication indicating that the scheduled resource of the second communication link is available\nduring the time interval.\nSome examples of the method, apparatus, and non-transitory computer-readable medium described above may further include processes, features, means, or instructions for receiving a second indication being indicative of whether a scheduled\nresource of a third communication link may be available during the time interval.  Some examples of the method, apparatus, and non-transitory computer-readable medium described above may further include processes, features, means, or instructions for\ntransmitting the data via the first communication link during the time interval may be further based at least in part on the second communication link and the third communication link being available during the time interval.\nSome examples of the method, apparatus, and non-transitory computer-readable medium described above may further include processes, features, means, or instructions for receiving a second indication being indicative of whether a scheduled\nresource of a third communication link may be available during a second time interval.  Some examples of the method, apparatus, and non-transitory computer-readable medium described above may further include processes, features, means, or instructions\nfor transmitting the data via the first communication link during the second time interval based at least in part on the third communication link being available during the second time interval.\nSome examples of the method, apparatus, and non-transitory computer-readable medium described above may further include processes, features, means, or instructions for forwarding the indication based at least in part on what entity schedules\ntransmissions on the first communication link.\nSome examples of the method, apparatus, and non-transitory computer-readable medium described above may further include processes, features, means, or instructions for determining an availability of a scheduled resource for transmission of data\non the first communication link.  Some examples of the method, apparatus, and non-transitory computer-readable medium described above may further include processes, features, means, or instructions for transmitting a second indication based at least in\npart on a scheduled resource of the first communication link being available.\nIn some examples of the method, apparatus, and non-transitory computer-readable medium described above, a first wireless node receives the indication from a second wireless node.\nSome examples of the method, apparatus, and non-transitory computer-readable medium described above may further include processes, features, means, or instructions for implementing, at the first wireless node, an access node function (ANF) for\nthe first communication link that allows the first wireless node to schedule transmissions on the first communication link.  Some examples of the method, apparatus, and non-transitory computer-readable medium described above may further include\nprocesses, features, means, or instructions for implementing, at the first wireless node, a user equipment function (UEF) for the second communication link, the UEF allowing the first wireless node to communicate via the second communication link,\nwherein the first wireless node implements simultaneously the ANF for the first communication link and a UEF for the second communication link.\nIn some examples of the method, apparatus, and non-transitory computer-readable medium described above, the second wireless node implements an ANF for the second communication link that allows the second wireless node to schedule transmissions\non the second communication link.\nIn some examples of the method, apparatus, and non-transitory computer-readable medium described above, the second wireless node transmits the indication if the scheduled resource of the second communication link may be available during the time\ninterval.\nSome examples of the method, apparatus, and non-transitory computer-readable medium described above may further include processes, features, means, or instructions for implementing, at the first wireless node, a UEF for the first communication\nlink.  Some examples of the method, apparatus, and non-transitory computer-readable medium described above may further include processes, features, means, or instructions for implementing, at the first wireless node, a UEF for the second communication\nlink.  Some examples of the method, apparatus, and non-transitory computer-readable medium described above may further include processes, features, means, or instructions for transmitting, by the first wireless node, a second indication received via the\nfirst communication link to the second wireless node via the second communication link.\nIn some examples of the method, apparatus, and non-transitory computer-readable medium described above, the indication includes identifying information about a scheduled resource defined by the synchronized frame structure.\nIn some examples of the method, apparatus, and non-transitory computer-readable medium described above, the synchronized frame structure may be for uplink transmissions and downlink transmissions and defines the scheduled resources, each\nscheduled resource including a control portion and a data portion.  In some examples of the method, apparatus, and non-transitory computer-readable medium described above, the indication may be transmitted via the control portion.\nSome examples of the method, apparatus, and non-transitory computer-readable medium described above may further include processes, features, means, or instructions for transmitting the data via the first communication link during the time\ninterval further comprises: transmitting via the first communication link during only the data portion of the scheduled resource of the time interval.\nIn some examples of the method, apparatus, and non-transitory computer-readable medium described above, transmitting the data via the first communication link during the time interval further comprises: transmitting uplink control data during a\nfirst portion of the data portion of the scheduled resource of the time interval.  Some examples of the method, apparatus, and non-transitory computer-readable medium described above may further include processes, features, means, or instructions for\ntransmitting the data during a second portion of the data portion of the scheduled resource of the time interval.\nIn some examples of the method, apparatus, and non-transitory computer-readable medium described above, a first subset of scheduled resources defined by the synchronized frame structure may be assigned to be used by the first communication link\nand a second subset of scheduled resources defined by the synchronized frame structure may be assigned to be used by the second communication link, the first subset being different than the second sub set.\nIn some examples of the method, apparatus, and non-transitory computer-readable medium described above, the first subset or the second subset of scheduled resources includes at least one of a time interval, a frequency band, a code, an antenna\nbeam, or a combination thereof.\nIn some examples of the method, apparatus, and non-transitory computer-readable medium described above, one of the first communication link or the second communication link may be a wireless backhaul link.  In some examples of the method,\napparatus, and non-transitory computer-readable medium described above, the RAT comprises a millimeter wave RAT. <BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nFIG. 1 illustrates an example of a system for wireless communication that supports dynamic resource allocation in a wireless network in accordance with aspects of the present disclosure.\nFIG. 2 illustrates an example of a communications network that supports dynamic resource allocation in a wireless network in accordance with aspects of the present disclosure.\nFIG. 3 illustrates an example of a communications network that supports dynamic resource allocation in a wireless network in accordance with aspects of the present disclosure.\nFIG. 4 illustrates an example of a schedule that supports dynamic resource allocation in a wireless network in accordance with aspects of the present disclosure.\nFIG. 5 illustrates an example of a communication swim diagram that supports dynamic resource allocation in a wireless network in accordance with aspects of the present disclosure.\nFIGS. 6A-6C illustrate examples of network connection diagrams that support dynamic resource allocation in a wireless network in accordance with aspects of the present disclosure.\nFIGS. 7A-7C illustrate examples of indication message transmission diagrams that support dynamic resource allocation in a wireless network in accordance with aspects of the present disclosure.\nFIG. 8 illustrates an example of resource allocation in a synchronized frame structure that supports dynamic resource allocation in a wireless network in accordance with aspects of the present disclosure.\nFIG. 9 illustrates another example of resource allocation in a synchronized frame structure that supports dynamic resource allocation in a wireless network in accordance with aspects of the present disclosure.\nFIGS. 10 through 12 show block diagrams of a device that supports dynamic resource allocation in a wireless network in accordance with aspects of the present disclosure.\nFIG. 13 illustrates a block diagram of a system including an access node that supports dynamic resource allocation in a wireless network in accordance with aspects of the present disclosure.\nFIGS. 14 through 16 illustrate methods for dynamic resource allocation in a wireless network in accordance with aspects of the present disclosure.\n<BR><BR>DETAILED DESCRIPTION\nAccording to some aspects of the disclosure, wireless backhaul links may be used to couple an access node (AN) to a network in place of high-capacity, wired backhaul link (e.g., fiber).  For example, a first AN may establish a wireless backhaul\nlink to a second AN, which has a high-capacity, wired backhaul link.  In this manner, the first AN may communicate access traffic to the network via the second AN through the combination of the wireless backhaul link and the wired backhaul link.  In some\nexamples, a backhaul network may use multiple wireless backhaul links before reaching a wired backhaul link.  The backhaul network should also provide robustness via topological redundancy.  In such an ad-hoc network, large-scale resource coordination\nschemes may be necessary.\nSuch a backhaul network, however, may also present some disadvantages for providing network access.  First, such large-scale coordination schemes tend to be inflexible to local variations in load or channel conditions.  In a backhaul network,\neach AN has equal permissions to send and receive.  As a result, the link between each AN may be symmetric.  However, such a symmetry in links may create large amounts of signaling to mediate conflicts that may arise in the backhaul links.  Second,\nwireless backhaul links may interfere with the access networks supported by each AN.  For example, a wireless backhaul link may interfere with a wireless communication link established between the AN and user equipment (UE).\nTechniques are described to address some of the shortcomings of a backhaul network.  To address the inflexibility of large-scale communication schemes, a backhaul network may be configured to allow an AN to release resources to other ANs via an\noccupancy/availability signaling scheme.  When the load conditions change or when the network topology changes, e.g., due to changes in channel conditions, link failure or addition/departure of nodes, the network-wide resource plan may become suboptimal. Due to its network-wide extension, updating the resource plan may require significant signaling overhead and may take a considerable amount of time.  It may therefore be desirable to have a mechanism to swiftly and locally shift resources in response to\navailability and demand.\nA number of wireless backhaul links may be established between ANs.  Each wireless backhaul link may be pre-assigned a resource based on a synchronized frame structure, a resource plan, or a schedule.  To reduce overhead in establishing and\nmaintaining wireless backhaul links between ANs, an asymmetric link may be established between ANs.  Each wireless backhaul link in the backhaul network may be terminated by an Access-Node Function (ANF) at a first AN and a UE-Function (UEF) at a second\nAN.  The ANF may control resource allocation on the link based on a large-scale resource schedule.  The UEF may communicate using the link, after receiving the permission of the link's ANF.  If an ANF determines that it is not using one of its assigned\nresources for a certain time period, the ANF may make the resource available to neighboring wireless backhaul links by signaling availability/occupancy indications to neighboring ANs.\nTo reduce interference with other signals, frequencies different from those used for the access network may be used to establish wireless backhaul links.  For example, millimeter waves, such as those used in 5G cellular technologies, may be used\nto establish wireless backhaul links between ANs.  In addition, beamforming techniques may be used to direct a wireless communication link to a neighboring AN.  It should be appreciated that wireless links between ANs are suitable for beamforming\ntechniques because ANs are stationary (i.e., they do not change physical locations) and ANs may have large antenna arrays that may be capable of producing highly focused pencil beams.\nAspects of the disclosure are initially described in the context of a wireless communications system.  Aspects of the disclosure are further illustrated by and described with reference to apparatus diagrams, system diagrams, and flowcharts that\nrelate to dynamic resource allocation in a wireless network.\nFIG. 1 illustrates an example of a wireless communications system 100 in accordance with various aspects of the present disclosure.  The wireless communications system 100 includes access nodes 105, UEs 115, and a core network 130.  In some\nexamples, the wireless communications system 100 may be a 5G, LTE, or LTE-Advanced network.  Wireless communications system 100 may support one or more node functions that enable resource allocation and scheduling between access nodes 105.\nAccess nodes 105 may wirelessly communicate with UEs 115 via one or more access node antennas.  Each access node 105 may provide communication coverage for a respective geographic coverage area 110.  Communication links 125 shown in wireless\ncommunications system 100 may include UL transmissions from a UE 115 to an access node 105, or DL transmissions, from an access node 105 to a UE 115.  UEs 115 may be dispersed throughout the wireless communications system 100, and each UE 115 may be\nstationary or mobile.  A UE 115 may also be referred to as a mobile station, a subscriber station, a mobile unit, a subscriber unit, a wireless unit, a remote unit, a mobile device, a wireless device, a wireless communications device, a remote device, a\nmobile subscriber station, an access terminal, a mobile terminal, a wireless terminal, a remote terminal, a handset, a user agent, a mobile client, a client, or some other suitable terminology.  A UE 115 may also be a cellular phone, a personal digital\nassistant (PDA), a wireless modem, a wireless communication device, a handheld device, a tablet computer, a laptop computer, a cordless phone, a personal electronic device, a handheld device, a personal computer, a wireless local loop (WLL) station, an\nInternet of Things (IoT) device, an Internet of Everything (IoE) device, a machine type communication (MTC) device, an appliance, an automobile, or the like.\nAccess nodes 105 may communicate with the core network 130 and with one another.  For example, access nodes 105 may interface with the core network 130 through a first set of backhaul links 132 (e.g., S1, etc.).  Access nodes 105 may communicate\nwith one another over a second set of backhaul links 134 (e.g., X2, etc.) either directly or indirectly (e.g., through core network 130).  Such backhaul links 134 may be wired or wireless.  In addition, such backhaul links 134 may form an ad-hoc backhaul\nnetwork to rely traffic from an originating access node 105 to an access node 105 with a wired connection to the desired network.  Access nodes 105 may perform radio configuration and scheduling for communication with UEs 115, or may operate under the\ncontrol of an access node controller (not shown).  In some examples, access nodes 105 may be macro cells, small cells, hot spots, or the like.  Access nodes 105 may also be referred to as eNodeBs (eNBs) 105.\nWireless communications systems are widely deployed to provide various types of communication content such as voice, video, packet data, messaging, broadcast, and so on.  These systems may be capable of supporting communication with multiple\nusers by sharing the available system resources (e.g., time, frequency, and power).  Examples of such multiple-access systems include CDMA systems, TDMA systems, FDMA systems, and OFDMA systems.  A wireless multiple-access communications system may\ninclude a number of access nodes, each simultaneously supporting communication for one or more multiple communication devices, which may be otherwise known as a user equipment (UE).\nSome wireless communications systems may operate in millimeter wave (mmW) frequency ranges, e.g., 28 GHz, 40 GHz, 60 GHz, etc. Wireless communication at these frequencies may be associated with increased signal attenuation, e.g., path loss,\nwhich may also be influenced by various factors, such as temperature, barometric pressure, diffraction, etc. As a result, signal processing techniques, such a beamforming, may be used to coherently combine energy and overcome path losses at these\nfrequencies.  Further, wireless communication links achieved through beamforming may be associated with narrow beams (e.g., \"pencil beams\") that are highly directional, minimize inter-link interference, and provide high-bandwidth links between access\nnodes.  Dynamic beam-steering and beam-search capabilities may further support, for example, discovery, link establishment, and beam refinement in the presence of dynamic shadowing and Rayleigh fading.  Additionally, communication in such mmW systems may\nbe time division multiplexed, where a transmission may be directed to one wireless device at a time due to the directionality of the transmitted signal.\nWireless communications networks may employ backhaul links (e.g., backhaul link 132 or backhaul link 134) as a connection between a core network and wireless nodes within the wireless communications network.  For example, wireless communications\nsystem 100 may include multiple access nodes 105 (e.g., base stations, remote radio heads, etc.), where at least one access node 105 is coupled to a wireline backhaul link, such as an optical fiber cable.  However, due to the widespread deployment of\naccess nodes 105 within a given region, installing wireline backhaul links to each access node 105 within a network may be cost prohibitive.  Therefore, some of the access nodes 105 within wireless communications system 100 may not be directly coupled to\nthe core network 130 or to another access node 105 via a wired backhaul link, and may use other means, such as wireless backhaul links, to communicate backhaul traffic.  For instance, cellular RATs may be used to provide backhaul links between multiple\naccess nodes 105 and a core network 130.  In such cases, the access nodes 105 may wirelessly communicate backhaul access traffic to a high-capacity fiber point (e.g., a location where a wireless node is coupled with a wireline link to core network 130).\nWhile mobile access may sometimes be associated with single-hop communication links between a source and destination (e.g., an asymmetric link), wireless backhaul communications may support multi-hop transport and provide robustness through\ntopological redundancy (e.g., alternative paths for data exchange within a wireless communications network).  Accordingly, underlying links using wireless backhaul communications may be symmetric in nature and use large-scale resource coordination among\nthe wireless communication links.\nIn some cases, cellular RATs, such as mmW-based RATs, may be used to support access traffic between UEs 115 and access nodes 105, in addition to backhaul access traffic among multiple access nodes 105.  Moreover, both access and backhaul traffic\nmay share the same resources (e.g., as in the case of integrated access and backhaul (TAB)).  Such wireless backhaul or TAB solutions may be increasingly beneficial with the evolution of cellular technologies due to enhancements in wireless link capacity\nand reduction in latency.  Further, the use of wireless backhaul links may reduce the cost of dense small cells deployments.\nIn some cases, an access link using a mmW-based RAT may be designed as an asymmetric single-hop link, which may be used for assigning control and scheduling tasks to an access node 105 while providing instruction to one or more UEs 115 for\nscheduling communication.  In such cases, an access node 105 may coordinate wireless resources among multiple UEs 115, while each UE 115 may be assigned to one access node 105 at a time.  In some cases, inter-access node links may be symmetric in nature\nand may form mesh topologies for enhanced robustness, where wireless transport may occur along multiple hops.\nThus, using a RAT may enable wireless backhaul communication using one or more node functions at a wireless node, such as a base station or access node.  Additionally, multiple wireless nodes may communicate in a backhaul network using a\nschedule that is aligned with a frame structure.  For example, a wireless node may establish a link with different wireless nodes using a RAT that supports a synchronized frame structure, such as a mmW RAT.  The wireless nodes may instantiate one or more\nnode functions, such as an ANF and a UEF.  The wireless nodes may then communicate according to active and suspended modes using the node functions, where the communication is based on a schedule aligned with the frame structure.\nIn addition, occupancy/availability signaling may be used to enable dynamic resource allocation of the resources defined in the synchronized frame structure between different wireless communication links.  For example, if a particular wireless\ncommunication link is not using some of its dedicated resources, it may signal to neighboring communication links that such resources are available to be used.  Upon receiving an indication signal that non-dedicated resources are available to be used,\nthe neighboring communication link may schedule data to be transmitted using those available resources.\nFIG. 2 illustrates an example of a backhaul network 200 for dynamic resource allocation in a wireless network.  In some cases, backhaul network 200 may be an example of a wireless communications network that communicates using mmW frequency\nranges.  The backhaul network 200 may include a number of access nodes 105 that communicate over a number of different communication links 220, where the communication links 220 may be associated with a same or different set of wireless resources.  The\naccess nodes 105 may be examples of the access nodes 105 described in reference to FIG. 1.  The backhaul network 200 may support the use of one or more node functions to enable efficient resource allocation for wireless backhaul communications.  In such\ncases, the access nodes 105 may instantiate one or more node functions to coordinate signaling and resource allocation.  That is, the access nodes 105 may instantiate one or more ANFs 205, one or more UEFs 210, or any combination thereof.\nFor example, access node 105-a may be located at a central point of a star, and may be coupled with a wireline backhaul link 230 (i.e., an optical fiber cable).  In some cases, access node 105-a may be the only access node 105 in backhaul\nnetwork 200 that is coupled with the wireline backhaul link 230.  Access node 105-a may instantiate an ANF 205, and the access nodes 105 at the leaves of the star (access node 105-b and access node 105-c) may each instantiate a UEF 210.  Access node\n105-a may then communicate with access node 105-b and access node 105-c using communication link 225-a according to an active mode or a suspended mode using the node functions.  In some cases, communication link 225-a may be associated with a first set\nof wireless resources.\nThe ANF 205 and the UEFs 210 may be assigned the same functionalities and signaling protocols for resource allocation as defined by a RAT.  That is, resource coordination of a backhaul star can be managed via the RAT, such as a mmW RAT. \nFurthermore, wireless resource use among access nodes 105 within a star may be coordinated via a large-scale (e.g., network-wide) schedule.  Within each star, signaling and resource management may be regulated by the RAT and a resource sub-schedule may\nbe generated by a star's ANF 205 (such as the ANF 205 instantiated at access node 105-a).\nIn some examples, access node 105-b may instantiate an ANF 205 in addition to the UEF 210.  Access node 105-b may accordingly communicate with access node 105-c using communication link 225-b according to an active or a suspended mode using the\nnode functions.  In some cases, communication link 225-b may be associated with a second set of wireless resources.\nIn another example, access node 105-d may instantiate an ANF 205 and communicate with a UEF 210 at access node 105-a over communication link 225-c. In some examples, communication link 225-c may be associated with the second set of resources. \nThat is, communication link 225-c may use the same resources as communication link 225-b. Additionally, the ANF 205 at access node 105-d may be used for mobile access, where access node 105-d may communicate with one or more UEs 115 over communication\nlink 225-d. As a result, access node 105-d may forward data between the one or more UEs 115 and access node 105-a. Accordingly, IAB may be accomplished by including the additional star with access node 105-d at the center and the UEs 115 at the leaves of\nthe star.\nIn some cases, ANFs 205 may support transmission of a downlink control channel, reception of an uplink control channel, scheduling of downlink and uplink data transmission within a resource space assigned to a link or to a set of links,\ntransmission of synchronization signals and cell reference signals (e.g., as a primary synchronization symbol (PSS) or secondary synchronization symbol (SSS) on a synchronization channel), transmitting beam sweeps, and transmitting downlink beam change\nrequests.  Additionally, UEFs 210 may support reception of a downlink control channel, transmission of a uplink control channel, requesting scheduling of uplink data transmissions, transmission of random access preambles on a random access channel,\nlistening to beam sweeps and reporting beam indexes and beam signal strength detected, and executing downlink beam change requests.  In some cases, there may be other features that differentiate the ANF and the UEF implemented at a node.  As described\nabove, an access node 105 node may implement a combination of one or more node functions, such as multiple ANFs 205, multiple UEFs 210, or combinations thereof.\nFIG. 3 illustrates an example of a backhaul network 300 for dynamic resource allocation in a wireless network.  In some cases, the backhaul network 300 may be an example of a wireless communications network that communicates using mmW frequency\nranges.  The backhaul network 300 may include a number of access nodes 105 that communicate over a number of different communication links 325, where the communication links 325 may be associated with a same or different set of wireless resources.  The\naccess nodes 105 may be examples of the access nodes 105 described in reference to FIGS. 1 and/or 2.  The backhaul network 300 may be an example of a wireless communications system that supports multiple hops and topological redundancy for backhaul links\nusing a RAT.\nIn some examples, complex backhaul topologies may be handled by composing the topology from multiple stars that mutually overlap.  For instance, backhaul network 300 may comprise a mesh topology with two interfaces to a wireline network (e.g.,\naccess nodes 105-e and 105-g coupled with wireline backhaul links 330-a and 330-b).  Such a topology may comprise multiple stars, where some stars mutually overlap.  An ANF may be allocated to an access node 105 at the center of each star (e.g., access\nnodes 105-e, 105-g, 105-h, etc.,) and further includes a UEF at the access node 105 at each of the leaves.  As a result, a backhaul node may include multiple ANFs and UEFs.\nFor example, access node 105-f may include multiple instances of a UEF, where it may communicate with the ANFs at access nodes 105-e, 105-g, and 105-h. Additionally, access nodes 105-g and 105-h may each communicate with each other using at\nleast one ANF and at least one UEF, and may form overlapping stars.  In such cases, access node 105-g and access node 105-h may communicate over communication links 325-e and 325-f that provide topological redundancy for the backhaul network 300.  In\nsome cases, communication links 325-e and 325-f may be associated with different sets of resources, where the resources are cooperatively allocated according to a schedule established by the ANFs.  Multiple stars may use techniques to coordinate wireless\nresources, which may efficiently handle any system constraints (e.g., half-duplexed communications, inter-link interference, etc.).  For instance, inter-link interference may be managed using spatial division multiple access (SDMA) techniques (e.g.,\nthrough the use of narrow beams), and inter-node beam coordination may account for any remaining interference.\nAdditionally or alternatively, mobile access may be integrated into such a star topology through additional stars with UEs 115 at their leaves and an access node 105 at their center.  In some examples, mobile access links may also be added to\nexisting stars.  In an example, access node 105-i may communicate with access node 105-j using communication link 325-g. Access node 105-i may further communicate with one or more UEs 115 over communication links 325-h. In this example, communication\nlinks 325-g and 325-h both share the same set of wireless resources to provide integrated access and backhaul.  As can be seen in FIG. 3, a range of ANF and UEF combinations may be instantiated in an access node 105.  Additional or different combinations\nof UEF and ANF instances in access nodes 105, as well as different topologies not shown in FIG. 3, may be possible.\nTo coordinate timing of transmission and reception, all links may be coordinated using time synchronization, where a frame structure supported by a cellular RAT may be used.  For instance, time synchronization may be achieved through a\ndetermination of timing parameters associated with another wireless node, e.g., another wireless node may transmit an indication of synchronization signal transmission timing.  In some examples, further coordination between wireless nodes may be used\nsince different wireless nodes may implement multiple ANFs and/or UEFs.\nIn some examples, access node 105 may include multiple node function instances, which may further use a routing function that makes decisions on forwarding of data among node functions residing on the same node.  The routing function may be\nexecuted or instantiated, for example, on any one of a number of protocol layers (e.g., the routing function may be executed on an Internet Protocol (IP) layer).  In some cases, the access node 105 may access a routing table, and may forward data between\nnode functions based on the routing table.  Additionally or alternatively, a routing function or a routing table may be used to forward data between different access nodes 105.\nIn some examples, a large-scale or network-wide time division multiplexed (TDM) schedule (e.g., a super schedule) may be used to assign resources to the various access nodes 105 within in a coordinated manner.  For example, adjacent stars (e.g.,\ndifferent stars with leaves that share at least one node) or overlapping stars (e.g., stars with one common leaves) may use different wireless resources.  At the same time, disjoint stars (e.g., stars that are neither adjacent nor overlapping) may reuse\nthe same wireless resources.  The schedule may be followed by all participating wireless nodes through a mutual time synchronization and the frame structure, which may be defined by the RAT.\nFIG. 4 illustrates an example of a schedule 400 for dynamic resource allocation in a wireless network.  Schedule 400 may illustrate an example of time-multiplexing wireless resources for multiple access nodes that form a mesh topology.  For\nexample, schedule 400 may illustrate a schedule used by a backhaul network 401 that comprises seven access nodes 105 that form three different stars.  In some examples, the backhaul network 401 includes both wireless and wired communication links.\nSchedule 400 may be aligned to a synchronized and slotted frame structure for uplink and downlink transmissions.  For example, a frame structure 405 may be supported by a RAT (e.g., a mmW RAT) and be used to coordinate signaling and resource\nallocation in a wireless backhaul network, such as backhaul network 200 or backhaul network 300 as described with reference to FIGS. 2 and 3, or backhaul network 401.  Within frame structure 405, a frame may occupy a time slot 410, and each frame may\ninclude control portions 415 (e.g., a portion that includes downlink control and a portion that includes uplink control at the beginning and end of the frame, respectively) and a data portion 420 (e.g., a portion that is used for the transmission of\nuplink and downlink data).  For instance, each time slot 410 may include a downlink control channel in a first sub-slot, a downlink or uplink data channel in a second sub-slot, and an uplink control channel in a third sub-slot.  In some examples, a time\nslot 410 may represent a frame or a subframe.  In the example of the schedule 400, downlink data and control may be transmitted between an ANF and a UEF, and uplink data and control may be transmitted between a UEF and an ANF.\nIn some examples, the schedule 400 may be based on a network topology (e.g., the topology of the backhaul network 401), where the schedule 400 divides resources into multiple groups (e.g., respective groups for ANF and UEFs).  The schedule 400\nmay assign alternating time slots 410 to these resources, where a first time slot 410-a is associated with a first set of wireless resources and a second time slot 410-b is associated with a second set of wireless resources.\nAn access node 105 within backhaul network 401 may communicate with one or more other access node 105 using one or more node functions, such as an ANF and a UEF, as described with reference to FIGS. 2 and 3.  Accordingly, communication may take\nplace between a first access node 105 using an ANF and one or more access nodes 105 using a UEF over communication links 225-a. Similarly, a second access node 105 and a third access node 105 using an ANF may respectively communicate with one or more\nother access nodes 105 over communication links 425-b and 425-c. In some examples, the communication links 425 may be associated with respective sets of resources.  That is, communication links 425-a and communication links 425-c may use the first set of\nwireless resources and communication link 425-b may use the second set of wireless resources.  The access nodes 105 may be examples of the access nodes 105 described in reference to FIGS. 1, 2 and/or 3.\nA node function may operate according to an active mode and a suspended mode based on schedule 400.  That is, a node function may be active or suspended in respective time slots 410 according to a resource schedule 430.  As an example, access\nnodes 105 using communication links 425-a and 425-c (and using the same set of resources) may communicate using resource schedule 430-a and resource schedule 430-c, respectively.  In such cases, the access nodes 105 may communicate using control portions\n415 and data portions 420 in the first time slot 410-a during an ANF active mode, and may refrain from transmitting during the second time slot 410-b during an ANF suspended mode.  Further, an ANF at an access node 105 using communication links 425-b may\nrefrain from communicating during the first time slot 410-a, but communicate according to resource schedule 430-b during an ANF active mode in second time slot 410-b.\nIn some cases, an ANF may use every resource schedule 430 for designated time slots 410 for communication between the UEFs that the ANF controls.  In some cases, within each resource allocation established by the schedule 400, each ANF may\nschedule resources among one or more UEFs, and an access node 105 may further sub-schedule resources among multiple UEs 115 (not shown).  In some examples, other resource allocation schemes for the schedule may be possible.  That is, more time slots 410\nmay be allocated to a wireless resource than for another wireless resource.  Additionally or alternatively, a greater number of resources may be scheduled based on a network topology.\nFIG. 5 illustrates an example of a communication swim diagram 500 for dynamic resource allocation in a wireless network.  The diagram 500 shows communications for a communications network 505.  In some examples, the communications network 505\nmay be a wireless backhaul network.  The communications network 505 includes a first communication link 510 established between a first access node 515 and a second access node 520 and a second communication link 525 established between the first access\nnode 515 and a third access node 530.  In some examples, the access nodes 515, 520, 530 may be embodied as access nodes 105 or base stations for a 5G or LTE access network.\nResources of the communications network 505 may be allocated to the first communication link 510 and the second communication link 525 according to the schedule 400.  In some examples, the schedule 400 may be the synchronized frame structure 800\nor the synchronized frame structure 900 described with reference to FIGS. 8 and 9.  The resources defined by the schedule 400 may be time slots, time intervals, frequency bands, codes, antenna beams, antenna ports, or any combination thereof.  In the\nillustrative example, the schedule 400 assigns time intervals to each communication link such that a first subset of resources are dedicated to the first communication link 510 and a second subset of resources are dedicated to the second communication\nlink 525.  In other examples, when there are additional communication links, the synchronized subframe may define additional subsets of resources.\nThe access node 515 may implement an access node function for the first communication link 510 that allows the access node 515 to schedule transmissions on the first communication link 510.  The access node 520 may implement a user equipment\nfunction for the first communication link 510 that allows the access node 520 to communicate via the first communication link 510.  However, the UEF implemented by the access node 520 may request that the access node 520 receive permission from the\ncorresponding ANF (i.e., access node 515) of the first communication link 510 before transmitting on the first communication link 510.  Similarly, the access node 530 may implement an ANF for the second communication link 525 and the access node 515 may\nimplement a UEF for the second communication link 525.  In this manner, the access node 515 may be able to communicate via both the first communication link 510 and the second communication link 525.\nThe swim diagram 500 illustrates an example of occupancy/availability signaling between access nodes 515, 520, 530.  The occupancy/availability signaling may be used to dynamically allocate local resources between the access nodes.  The swim\ndiagram 500 is presented for illustrative purposes of the basic principles of occupancy/availability signaling.\nAt block 535, the access node 515 may determine a demand for the resources, or scheduled resources, dedicated to the first communication link 510.  The access node 515 may predict a pending need for a particular resources, such as time slots\ndefined by the schedule 400 that are dedicated to the first communication link 510.  The determination of demand may include link quality measurements to gauge the channel conditions and the corresponding transmission rate.  The determination of demand\nmay also be based on queuing load, for instance.  The access node 515 may evaluate these properties on one single link or on multiple links (e.g. if the ANF multiplexes its resource over these multiple links).  This evaluation may include measurements of\nDL link quality and load and UL link quality and load.  The UL load may, for instance, be signaled to the ANF via UL transmission requests 540 (e.g., scheduling requests).  In some examples, the access node 515 may use machine learning, past historical\ndata, or other more global data to estimate demand for resources on the first communication link 510.\nBased on the determination of demand, the access node 515 may also determine whether one or more resources dedicated to the first communication link 510 are occupied or available at a future time.  For example, the access node 515 may determine\nthat based on requests for transmissions by either the access node 515 or the access node 520 the dedicated resources of the first communication link 510 are occupied for the foreseeable future.  In other examples, the access node 515 may determine that\nat least some of the resources dedicated to the first communication link 510 at certain time intervals in the future.\nAt block 545, the access node 530 may determine the demand for resources dedicated to the second communication link 525.  It should also be noted that the access node 515 may send UL transmission requests 540 to the access node 530.  If such is\nthe case, the access node 530 would determine that the resources dedicated to the second communication link 525 are occupied.  However, if the resources of the second communication link 525 are unoccupied, or are available, the access node 530 may\nrelease its available resources to other neighboring communication links and neighboring access nodes (e.g., first communication link 510).  It should be appreciated that the determinations for demand of resources dedicated to the first communication\nlink 510 and the second communication link 525 are performed by the communication link's respective ANF.  In the illustrative example, the ANF for the first communication link 510 is access node 515 and the ANF for the second communication link 525 is\naccess node 530.\nBased upon determining that at least some resources dedicated to the second communication link 525 are available for use by neighboring communication links, the access node 530 may generate an indication message 550 and send that indication\nmessage 550 to neighboring access nodes (e.g., access node 515).  In some examples, the indication message 550 is sent when one state is determined to be true (e.g., if the resources are available).  In such examples, a non-receipt of an indication\nmessage 550 may indicate that the other state is true (e.g., if the resources are occupied).  In this way, a receiver may derive information from the reception as well as the absence of reception of the indication message 550.  In the illustrative\nexample, the indication message 550 indicates that resources are available.  However, in other examples, the indication message 550 may indicate that resources are occupied.  In some examples, a neighboring access node may conclude that a resource is\navailable if no indication message is received, which is sent to flag occupancy.\nThe communication links and the access nodes that receive the indication message 550 about the available resources of the second communication link 525 may depend on the local network topology and resource allocation.  Additional examples of\nmore complex network topologies are shown in FIG. 3.  The indication message 550 may be sent on different links at different times.  The indication message 550 may include additional information about the available resources.  Such additional information\nmay include, what type of resources are available/occupied (e.g., time slots, frequencies, etc.), when the resources are available/occupied (e.g., at the next time interval or at a time interval several slots in the future), whether a fraction of the\nresource is available/occupied, other resource specific information, or any combination thereof.  A neighboring ANF may listen for indication messages during the control portions 415 of selected communication links.  For example, the access node 515 may\nlisten for the indication message 550 during the control portions 415 of scheduled resource of the second communication link 525.\nAt block 555, the access node 515 (e.g., the neighboring ANF) may determine that it has received an indication message 550 from the first communication link 510.  The access node 515 may then process the indication message 550, determine what\ninformation it includes, and identify what resources are available to be used by the access node 515.  If the first communication link 510 has a demand for resources that exceeds its dedicated resources, the access node 515 may determine to use the\navailable resources of the second communication link 525 to send additional data traffic across the first communication link 510.\nThe access node 515 may transmit a scheduling message 560 to the relevant access nodes running UEFs for the first communication link 510.  The scheduling message 560 may indicate which entity using the first communication link 510 (e.g., access\nnode 515 or access node 520) has permission to transmit using a particular scheduled resource.  The scheduling message 560 may be transmitted during the control portion 415 of the scheduled resource dedicated to the first communication link 510.  In some\nexamples, the scheduling message 560 only indicates how one scheduled resource should be used.  For example, the scheduling message 560 may indicate how the next resource dedicated to the first communication link 510 is to be used.  In other examples,\nthe scheduling message 560 may be used to indicate how multiple scheduled resources are to be used.  For example, the scheduling message 560 may indicate how a first scheduled resource dedicated to the first communication link 510 is to be used and how a\nsubsequent second scheduled resource dedicated to the second communication link 525 is also to be used.\nAt message 565, data may be communicated via the first communication link 510 based upon the scheduling message 560.  The data may be transmitted by either the access node 515 or the access node 520.  The data may be transmitted during the data\nportion 420 of the scheduled resource.\nA scheduling message 570 may sent prior to communicating data using a scheduled resource dedicated to the second communication link 525 but made available by the ANF of the second communication link 525.  The scheduling message 570 may be\nsimilarly embodied as the scheduling message 560.  In some examples, the scheduling message 570 is not transmitted because the scheduling message 560 includes the information to schedule the newly available resources dedicated to the second communication\nlink 525 as shown in FIG. 8.  In some examples, the scheduling message 570 may be transmitted during the data portion of the resources dedicated to second communication link 525 as shown in FIG. 9.  In such a manner, the control portions 415 of the\nresources dedicated to the second communication link 525 are still preserved to network purposes.  For example, even though the second communication link 525 data resources are available for other communication links to use, indication messages may still\nbe transmitted across the second communication link 525 during a particular time interval or time slot.\nAt message 575, data may be communicated via the first communication link 510 using the resources made available by the ANF of the second communication link 525.  Generally, the schedule 400 would not permit the first communication link 510 to\ntransmit using these resources, except that the resources have been released by the relevant ANF (e.g., access node 530 running the ANF).  The data may be transmitted by either the access node 515 or the access node 520.  The data may be transmitted\nduring the data portion 420 of the scheduled resource.\nWhile the swim diagram 500 shows communications across the communication links 510, 525 and three access nodes 515, 520, 530, the methods described herein may be expanded to include additional communication links and additional access nodes.\nThe access nodes 515, 520, 530 may be examples of access nodes 105 described with reference to FIGS. 1-4.  The ANFs may be examples of the ANFs described with reference to FIGS. 2-4.  The UEFs may be examples of the UEFs described with reference\nto FIGS. 2-4.  The communication links 510, 525 may be examples of the communication links described with reference to FIGS. 2-4.\nFIGS. 6A-6C illustrate examples of network connection diagrams of a communications network 600 for dynamic resource allocation in a wireless network.  FIGS. 6A-6C illustrate examples of occupancy/availability signaling as it occurs in different\nnetwork topologies.\nFIG. 6A shows a local network topology for a communications network 600.  The communications network 600 may be a wireless backhaul network.  The communications network 600 includes a primary communication link 605, a first auxiliary\ncommunication link 610, and a second auxiliary communication link 615.  The communication links 605, 610, 615 may be similar to the other communication links discussed above (e.g., communication links 510, 525).  The primary communication link 605 may be\nassigned a first subset of resources by the schedule 400.  The auxiliary communication links 610, 615 may be assigned a second subset of resources by the schedule 400.  The auxiliary communication links 610, 615 may utilize the same subset of resources\nbecause they may be separated enough spatially that they will not interfere with one another.\nAn access node 620 may communicate with an access node 625 via the primary communication link 605.  In addition, the access node 620 may communication with other access nodes via the first auxiliary communication link 610 and the access node 625\nmay communicate with other access nodes via the second auxiliary communication link 615.  In the illustrative example, the access node 620 implements an ANF 630 for the primary communication link 605 and the access node 625 implements a UEF 635 for the\nprimary communication link 605.  In other examples, however, the functions of the access nodes 620, 625 may be reversed.  As used in this disclosure, the terms primary and auxiliary are not meant to denote differences in technical features, importance,\npriority but to denote that different resources are dedicated for each link by the resource plan (e.g., schedule 400).\nThe ANF 630 of the primary communication link 605 (e.g., access node 620) may receive a first indication message 640 regarding the occupancy/availability of resources on the first auxiliary communication link 610 and a second indication message\n645 regarding the occupancy/availability of resources on the second auxiliary communication link 615.  Due to high resource demand, the ANF 630 may wish to also use resources from its adjacent links (e.g., links 610, 615).  If both of the indication\nmessages 640, 645 indicate that both the resources of both neighboring communication links (e.g., links 610, 615) are available during a particular time interval, the ANF 630 may then utilize the available resource of the auxiliary communication links\nfor its own traffic.\nFIG. 6B shows a similar local network topology for the communications network 600 as shown in FIG. 6A, except an additional access node 650 is added to the communications network 600.  The access node 650 is coupled to the access node 620 via a\nsecond primary communication link 655.  The second primary communication link 655 utilizes the same subset of resources defined by the schedule 400 as the first primary communication link 605.  The access node 650 may be coupled to other access nodes via\na third auxiliary communication link 660, which uses the same subset of resources defined by the schedule 400 as the other auxiliary communication links 610, 615.  The access node 650 may implement a UEF 635 for the second primary communication link 655. The access node 650 may transmit a third indication message 665 regarding the occupancy/availability of resources for the third auxiliary communication link 660 to the ANF 630.\nBefore using any resources dedicated to any of the auxiliary communication links 610, 615, 660, the ANF 630 (e.g., access node 620) may determine whether each of the auxiliary communication links 610, 615, 660 are available.  For example, before\ntransmitting via the second primary communication link 655, the ANF 630 may determine whether all auxiliary communication links 610, 615, 660 have made resources available to the ANF 630 during the particular time interval.  In some examples, however, to\ntransmit using the second primary communication link 655, resources from the first auxiliary communication link 610 and the third auxiliary communication link 660 are made available.  In some instances of these examples, no other resources are made\navailable.  In some examples, to transmit using the first primary communication link 605, resources from the first auxiliary communication link 610 and the second auxiliary communication link 615 are made available.  In some instances of these examples,\nno other resources are made available.\nFIG. 6C shows a similar local network topology for the communications network 600 as shown in FIG. 6A, except that the access node 625 is coupled to other access nodes via a tertiary communication link 670 instead of the second auxiliary\ncommunication link 615.  The tertiary communication link 670 may utilize a third subset of resources defined by the schedule 400 different from the first subset and the second subset of resources utilized by the primary communication links 605, 655 and\nthe auxiliary communication links 610, 615, 660 respectively.\nThe ANF 630 may utilize the resources of the auxiliary communication link 610 or the tertiary communication link 670 based at least in part on their respective occupancy/availability.  For example, if the first indication message 640 indicates\nthat resources dedicated to the auxiliary communication link 610 are available, the ANF 630 may decide to transmit data across the primary communication link 605 using those resources.  In another example, if the second indication message 645 indicates\nthat resources dedicated to the tertiary communication link 670 are available, the ANF 630 may decide to transmit data across the primary communication link 605 using those resources.\nIt should be appreciated that different network topologies may include any combination of the topologies discussed above.  For example, a network topology may include four different communication links using four different subsets of resources. \nIn other examples, each communication link may have any number of connections that an ANF may check before utilizing resources not dedicated to a particular communication link.\nThe access nodes 620, 625, 650 may be examples of access nodes 105 described with reference to FIGS. 1-5.  The ANF 630 may be examples of the ANFs described with reference to FIGS. 2-5.  The UEF 635 may be examples of the UEFs described with\nreference to FIGS. 2-5.  The communication links 605, 610, 615, 660, 670 may be examples of the communication links described with reference to FIGS. 2-5.\nFIGS. 7A-7C illustrate examples of indication message transmission diagrams for dynamic resource allocation in a wireless network 700.  Depending on the network topology of the wireless network 700, indication messages may be forwarded,\ntransmitted, and/or relied in different ways.  FIGS. 7A-7C are intended to demonstrate basic principles of indication message travel and communication.  Various network topologies may deviate from these principle are may use any combination of these\nprinciples.\nFIG. 7A illustrates a network topology that includes an access node 705 coupled with other access nodes via a first communication link 710 and a second communication link 715.  The communication links 710, 715 utilize different subsets of\nresources defined by the schedule 400.  In the illustrative example of FIG. 7A, the access node 705 implements an ANF 720 for the first communication link 710 and implements an ANF 725 for a second communication link 715.  The ANFs communicate with\nassociated UEFs 730, 735.  The UEFs 730, 735 may be implemented by other access nodes that are not depicted here.\nIt may be determined that resources dedicated to the second communication link 715 are available to be used by other communication links.  For example, the ANF 725 of the second communication link 715 may determine that certain resources of the\nsecond communication link 715 may be available for a certain time interval.  Because both the ANF 720 of the first communication link 710 and the ANF 725 of the second communication link 715 are implemented on the access node 705, the ANF 725 may send an\nindication message 740 directly to the ANF 720 internal to the access node 705.  In such a situation, no signaling across the wireless network may be performed.\nFIG. 7B illustrates a similar network topology to that shown in FIG. 7A except that the access node 705 implements the UEF 735 of the second communication link 715 instead of the ANF 725 of the second communication link 715.  In order to make\nresources dedicated to the second communication link 715 available to the first communication link 710, two indication messages may be used.  The ANF 725 may first determine that certain resources are available to be used by other communication links. \nAt which time, the ANF 725 may generate and transmit an indication message 745 to its related UEFs being implemented by neighboring access nodes (e.g., access node 705).  Once the UEF 735 receives the indication message 745, the UEF 735 may generate the\nintra-access node indication message 740 and send it to the ANF 720 for the first communication link 710.  In this manner, indication messages may be received by ANFs of the relevant communication links.\nFIG. 7C illustrates a similar network topology to that shown in FIG. 7B except that the access node 705 implements the UEF 730 of the first communication link 710 instead of the ANF 720 of the first communication link 710.  In order to make\nresources dedicated to the second communication link 715 available to the first communication link 710, three indication messages may be used.  The ANF 725 may determine that certain resources are available to be used by other communication links.  At\nwhich time, the ANF 725 may generate and transmit the indication message 745 to its related UEFs being implemented by neighboring access nodes (e.g., access node 705).  Once the UEF 735 receives the indication message 745, the UEF 735 may generate the\nintra-access node indication message 740 and send it to the UEF 730 for the first communication link 710.  The UEF 730 may then generate another indication message 750 to transmit via the first communication link 710 to the ANF 720.  In this manner, the\naccess node 705 becomes effectively a pass-through entity for the indication messages between the ANFs 720, 725 of the first communication link 710 and the second communication link 715.  Upon receiving the indication message 745, the access node 705 may\nforward the indication message 745 as indication message 750 to the appropriate ANF (e.g., ANF 720).  In this manner, an access node 705 may forward indication messages to other access nodes.\nThe access node 705 may be an example of access nodes 105 described with reference to FIGS. 1-6.  The ANFs 720, 725 may be examples of the ANFs described with reference to FIGS. 2-6.  The UEFs 730, 735 may be examples of the UEFs described with\nreference to FIGS. 2-6.  The communication links 710, 715 may be examples of the communication links described with reference to FIGS. 2-6.\nFIG. 8 illustrates an example of a resource allocation in a synchronized frame structure 800 for dynamic resource allocation in a wireless network.  The resource allocation scheme shown FIG. 8 illustrates how resources can be shifted among nodes\nwithin a synchronized frame structure 800.  A communications network 810 may include a chain of three communication links.  The communications network 810 may be a wireless backhaul network.  The synchronized frame structure 800 may be an example of the\nschedule 400.\nThe communications network 810 includes a primary communication link 812, a first auxiliary communication link 814, and a second auxiliary communication link 816.  The communications network 810 includes four access nodes, a first access node\n820, a second access node 822, a third access node 824, and a fourth access node 826.  The first access node 820 may implement an ANF 828 for the first auxiliary communication link 814.  The second access node 822 may implement a UEF 830 for the first\nauxiliary communication link 814 and an ANF 832 for the primary communication link 812.  The third access node 824 may implement a UEF 834 for the primary communication link 812 and a UEF 836 for the second auxiliary communication link 816.  The fourth\naccess node 826 may implement an ANF 838 for the second auxiliary communication link 816.\nThe access nodes 820, 822, 824, 826 may be examples of access nodes 105 described with reference to FIGS. 1-7.  The ANFs 828, 832, 838 may be examples of the ANFs described with reference to FIGS. 2-7.  The UEFs 830, 834, 836 may be examples of\nthe UEFs described with reference to FIGS. 2-7.  The communication links 812, 814, 816 may be examples of the communication links described with reference to FIGS. 2-7.\nThe synchronized frame structure 800 may include a number of resources 840 that include a control portion 842 and a data portion 844.  In some examples, a resource 840 includes two control portions 842, a downlink control portion (or a DL\ncontrol channel) and an uplink control portion (or a UL control channel).  In the illustrative example, the resources 840 are embodied as frames spanning a unique time interval.  A first subset 850 of resources 840 are dedicated for the primary\ncommunication link 812.  A second subset 855 of resources 840 are dedicated for the auxiliary communication links 814, 816.  For example, the first subset 850 includes resources at time intervals n+1, n+3, etc., and the second subset 855 includes\nresources at time intervals n, n+2, n+4, etc. In other examples, the synchronized frame structure 800 may be divided into additional subsets of resources 840 depending on the network topology.\nThe following disclosure relate to how indication messages may move between access nodes through the communications network 810 over time.  At slot n (i.e., the time interval represented by n), the ANF 838 of the second auxiliary communication\nlink 816 may transmit an indication message 860 on a control portion 842 of a resource 855-d, which is received by the UEF 836 of the second auxiliary communication link 816 and passed on to the UEF 834 of the primary communication link 812.  The\nindication message 860 may include information indicating that the resource 855-f (at time interval n+4) is available for the second auxiliary communication link 816.  In some examples, the information states that the second auxiliary communication link\n816 may be available in four time slots.\nAt time slot n+1, the ANF 832 of the primary communication link 812 may transmit a scheduling message 872 during a control portion 842 to schedule data for transmission via the primary communication link 812 during this time slot.  Also during\nthe time slot n+1, the UEF 834 of the primary communication link 812 may send an indication message 874 to its ANF 832 on the UL control portion of resource 850-a. The indication message may include information that resource 855-f for the second\nauxiliary communication link 816 is available.\nAt time slot n+2, the ANF 828 of the first auxiliary communication link 814 may transmit an indication message 876 on the DL control portion 842 of resource 855-b of the first auxiliary communication link 814.  The indication message 876 may\ninclude information that resource 855-c for the first auxiliary communication link 814 is available.  The indication message may be received by the UEF 830 for the first auxiliary communication link 814 and passed on to the ANF 832 for the primary\ncommunication link 812.\nAt time slot n+3, the ANF 832 of the primary communication link 812 may transmit a scheduling message 878 to schedule data to be transmitted using resource 850-b via the primary communication link 812.  In addition, the scheduling message 878\nmay also schedule data to be transmitted during the time slot n+4 because both resource 855-c and resource 855-f are not being used by their respective auxiliary communication links 814, 816.  The scheduling message may be transmitted during a control\nportion of the resource 850-b.\nAt time slot n+4, a data message 880 may be transmitted via the primary communication link 812 using resources usually dedicated to the auxiliary communication links 814, 816.  The data message 880 may be transmitted during the data portion of\nresources 855-c, 855-f, thereby allowing the auxiliary communication links 814, 816 to be used to transmit network messages, such as indication messages for indicating resources that may be available in the future.  In some examples, the data message 880\nis transmitted exclusively during the data portion.\nFIG. 9 illustrates an example of a resource allocation in a synchronized frame structure 900 for dynamic resource allocation in a wireless network.  Another example of a resource allocation scheme is shown in FIG. 9.  The features of the\nwireless network 905 and the synchronized frame structure 900 are similarly embodied as those described in FIG. 8 and thus descriptions of those features are not repeated here.  It should be appreciated that features having similar or identical numbers\nmay be embodied similarly.  The synchronized frame structure 900 may be an example of the schedule 400.  In the synchronized frame structure 900, a data message 918 transmitted using resources not dedicated to the relevant communication link may include\na control portion 920 and a data portion 922 and a scheduling message 924 may be transmitted during the control portion 920.\nAt slot n (i.e., the time interval represented by n), the ANF 838 of the second auxiliary communication link 816 may transmit an indication message 910 on a control portion 842 of a resource 855-d, which is received by the UEF 836 of the second\nauxiliary communication link 816 and passed on to the UEF 834 of the primary communication link 812.  The indication message 910 may include information indicating that the resource 855-e (at time interval n+2) is available for the second auxiliary\ncommunication link 816.  In some examples, the information states that the second auxiliary communication link 816 may be available in two time slots.\nAt time slot n+1, the ANF 832 of the primary communication link 812 may transmit a scheduling message 912 during a control portion 842 of resource 850-a to schedule data for transmission via the primary communication link 812 during this time\nslot.  Also during the time slot n+1, the UEF 834 of the primary communication link 812 may send an indication message 914 to its ANF 832 on the UL control portion of resource 850-a. The indication message 914 may include information that resource 855-f\nfor the second auxiliary communication link 816 is available.\nAt time slot n+2, the ANF 828 of the first auxiliary communication link 814 may transmit an indication message 916 on the DL control portion 842 of resource 855-b of the first auxiliary communication link 814.  The indication message 916 may\ninclude information that resource 855-b for the first auxiliary communication link 814 is available at the next time slot.  The indication message 916 may be received by the UEF 830 for the first auxiliary communication link 814 and passed on to the ANF\n832 for the primary communication link 812.\nUpon receiving indication messages 910, 916, the ANF 832 for the primary communication link 812 may determine that it may utilize the resources normally dedicated to the auxiliary communication links 814, 816.  As such, the ANF 832 may generate\na data message 918 to be sent via the primary communication link 812 during the time slot n+2.  The data message 918 may include both a control portion 920 and a data portion 922.  The data message 918 is also configured to be transmitted during the data\nportion 844 of resources 855-b, 855-e, thereby reserving the control portions 842 of those resources to be used by the auxiliary communication links 814, 816.  A scheduling message 924 may be transmitted during the control portion 920 of the data message\n918.  The scheduling message 924 may be embodied similarly to other scheduling messages (e.g., scheduling message 912) and may schedule data for transmission via the primary communication link during the time slot n+2.\nTime slots n+3 and n+4 illustrate another scenario that may occur using the data message 918.  At time slot n+3, the ANF 832 of the primary communication link 812 may transmit a scheduling message 926 to schedule data to be transmitted using\nresource 850-b via the primary communication link 812.\nAt time slot n+4, the ANF 828 of the first auxiliary communication link 814 may transmit an indication message 928 on the DL control portion 842 of resource 855-c of the first auxiliary communication link 814.  The indication message 928 may\ninclude information that resource 855-c for the first auxiliary communication link 814 is available at the next time slot.  The indication message 928 may be received by the UEF 830 for the first auxiliary communication link 814 and passed on to the ANF\n832 for the primary communication link 812.  In addition, at time slot n+4, the ANF 838 of the second auxiliary communication link 816 may transmit an indication message 930 on a control portion 842 of a resource 855-f, which is received by the UEF 836\nof the second auxiliary communication link 816 and passed on to the UEF 834 of the primary communication link 812.  The indication message 928 may include information indicating that the resource 855-f is available for the second auxiliary communication\nlink 816.\nUpon receiving indication messages 928, 930, the ANF 832 for the primary communication link 812 may determine that it may utilize the resources normally dedicated to the auxiliary communication links 814, 816.  As such, the ANF 832 may generate\na data message 932 to be sent via the primary communication link 812 during the time slot n+4.  The data message 932 may be similarly embodied as data message 918.  The data message 932 may be also configured to be transmitted during the data portion 844\nof resources 855-c, 855-f, thereby reserving the control portions 842 of those resources to be used by the auxiliary communication links 814, 816.  A scheduling message 934 may be transmitted during the control portion 920 of the data message 932.\nIt should be appreciated from FIGS. 8 and 9 that indication messages may be transmitted on control portions of resources 840.  For example, an indication message may be included in a DL control channel or an UL control channel.  These control\nchannels may use resources that are pre-allocated by a (network-wide) resource plan (e.g., the synchronized frame structure).  Furthermore, resource shifting via availability/occupancy signaling may be restricted to the data portions 844 of resources\n840, while control portions 842 of resources 840 are typically reserved for the assigned communication link.  This allows a communication link to sustain regular control signaling even if it has released its resource to adjacent links.  Resource shifting\nmay apply to time-multiplexed resources or frequency-multiplexed resources.  The type of resource that is indicated to be available or occupied may be explicitly included in the indication message.  It may also be implicitly derived from information an\naccess node holds about the local network topology (e.g., such as the resource plan and the local connectivity).  Such information may be provisioned to the access node or configured via additional signaling channels.\nFIG. 10 shows a block diagram 1000 of a wireless device 1005 that supports dynamic resource allocation in a wireless network in accordance with various aspects of the present disclosure.  Wireless device 1005 may be an example of aspects of an\naccess node 105 as described with reference to FIG. 1.  Wireless device 1005 may include receiver 1010, communications manager 1015, and transmitter 1020.  Wireless device 1005 may also include a processor.  Each of these components may be in\ncommunication with one another (e.g., via one or more buses).\nReceiver 1010 may receive information such as packets, user data, or control information associated with various information channels (e.g., control channels, data channels, and information related to dynamic resource allocation in a wireless\nnetwork, etc.).  Information may be passed on to other components of the device.  The receiver 1010 may be an example of aspects of the transceiver 1335 described with reference to FIG. 13.\nCommunications manager 1015 may be an example of aspects of the communications manager 1315 described with reference to FIG. 13.\nCommunications manager 1015 may determine whether data is available to be transmitted via a first communication link using a radio access technology (RAT) that supports a synchronized frame structure, receive an indication whether a scheduled\nresource of a second communication link is available during a time interval defined by the synchronized frame structure, and transmit the data via the first communication link using the scheduled resource during the time interval based on the indication\nindicating that the scheduled resource of the second communication link is available during the time interval.\nTransmitter 1020 may transmit signals generated by other components of the device.  In some examples, the transmitter 1020 may be collocated with a receiver 1010 in a transceiver module.  For example, the transmitter 1020 may be an example of\naspects of the transceiver 1335 described with reference to FIG. 13.  The transmitter 1020 may include a single antenna, or it may include a set of antennas.\nFIG. 11 shows a block diagram 1100 of a wireless device 1105 that supports dynamic resource allocation in a wireless network in accordance with various aspects of the present disclosure.  Wireless device 1105 may be an example of aspects of a\nwireless device 1005 or an access node 105 as described with reference to FIGS. 1 and 10.  Wireless device 1105 may include receiver 1110, communications manager 1115, and transmitter 1120.  Wireless device 1105 may also include a processor.  Each of\nthese components may be in communication with one another (e.g., via one or more buses).\nReceiver 1110 may receive information such as packets, user data, or control information associated with various information channels (e.g., control channels, data channels, and information related to dynamic resource allocation in a wireless\nnetwork, etc.).  Information may be passed on to other components of the device.  The receiver 1110 may be an example of aspects of the transceiver 1335 described with reference to FIG. 13.\nCommunications manager 1115 may be an example of aspects of the communications manager 1315 described with reference to FIG. 13.  Communications manager 1115 may also include data manager 1125 and resource availability manager 1130.\nData manager 1125 may determine whether data is available to be transmitted via a first communication link using a RAT that supports a synchronized frame structure, transmit the data via the first communication link during the time interval is\nfurther based on the second communication link and the third communication link being available during the time interval, transmit the data via the first communication link during the second time interval based on the third communication link being\navailable during the second time interval, transmit the data via the first communication link using the scheduled resource during the time interval based on the indication indicating that the scheduled resource of the second communication link is\navailable during the time interval, transmit a second indication based on a scheduled resource of the first communication link being available, transmit the data via the first communication link during the time interval further includes: transmitting via\nthe first communication link during the data portion of the scheduled resource of the time interval, and transmit the data during a second portion of the data portion of the scheduled resource of the time interval.  In some cases, transmitting the data\nvia the first communication link during the time interval further includes: transmitting uplink control data during a first portion of the data portion of the scheduled resource of the time interval.\nResource availability manager 1130 may receive an indication whether a scheduled resource of a second communication link is available during a time interval defined by the synchronized frame structure, receive a second indication being\nindicative of whether a schedule resource of a third communication link is available during the time interval, receive a second indication being indicative of whether a scheduled resource of a third communication link is available during a second time\ninterval, forward the indication based on what entity schedules transmissions on the first communication link, determine an availability of a scheduled resource for transmission of data on the first communication link, and transmit, by the first wireless\nnode, a second indication received via the first communication link to the second wireless node via the second communication link.  In some cases, a first wireless node receives the indication from a second wireless node.  In some cases, the second\nwireless node transmits the indication if the scheduled resource of the second communication link is available during the time interval.  In some cases, the indication includes identifying information about a scheduled resource defined by the\nsynchronized frame structure.  In some cases, the indication is transmitted via the control portion.\nTransmitter 1120 may transmit signals generated by other components of the device.  In some examples, the transmitter 1120 may be collocated with a receiver 1110 in a transceiver module.  For example, the transmitter 1120 may be an example of\naspects of the transceiver 1335 described with reference to FIG. 13.  The transmitter 1120 may include a single antenna, or it may include a set of antennas.\nFIG. 12 shows a block diagram 1200 of a communications manager 1215 that supports dynamic resource allocation in a wireless network in accordance with various aspects of the present disclosure.  The communications manager 1215 may be an example\nof aspects of a communications manager 1015, a communications manager 1115, or a communications manager 1315 described with reference to FIGS. 10, 11, and 13.  The communications manager 1215 may include data manager 1220, resource availability manager\n1225, backhaul function manager 1230, and frame structure manager 1235.  Each of these modules may communicate, directly or indirectly, with one another (e.g., via one or more buses).\nData manager 1220 may determine whether data is available to be transmitted via a first communication link using a RAT that supports a synchronized frame structure, transmit the data via the first communication link during the time interval is\nfurther based on the second communication link and the third communication link being available during the time interval, transmit the data via the first communication link during the second time interval based on the third communication link being\navailable during the second time interval, transmit the data via the first communication link using the scheduled resource during the time interval based on the indication indicating that the scheduled resource of the second communication link is\navailable during the time interval, transmit a second indication based on a scheduled resource of the first communication link being available, transmit the data via the first communication link during the time interval further includes: transmitting via\nthe first communication link during the data portion of the scheduled resource of the time interval, and transmit the data during a second portion of the data portion of the scheduled resource of the time interval.  In some cases, transmitting the data\nvia the first communication link during the time interval further includes: transmitting uplink control data during a first portion of the data portion of the scheduled resource of the time interval.\nResource availability manager 1225 may receive an indication whether a scheduled resource of a second communication link is available during a time interval defined by the synchronized frame structure, receive a second indication being\nindicative of whether a schedule resource of a third communication link is available during the time interval, receive a second indication being indicative of whether a scheduled resource of a third communication link is available during a second time\ninterval, forward the indication based on what entity schedules transmissions on the first communication link, determine an availability of a scheduled resource for transmission of data on the first communication link, and transmit, by the first wireless\nnode, a second indication received via the first communication link to the second wireless node via the second communication link.  In some cases, a first wireless node receives the indication from a second wireless node.  In some cases, the second\nwireless node transmits the indication if the scheduled resource of the second communication link is available during the time interval.  In some cases, the indication includes identifying information about a scheduled resource defined by the\nsynchronized frame structure.  In some cases, the indication is transmitted via the control portion.\nBackhaul function manager 1230 may implement, at the first wireless node, an access node function for the first communication link that allows the first wireless node to schedule transmissions on the first communication link, implement, at the\nfirst wireless node, a user equipment function for the second communication link, the UEF allowing the first wireless node to communicate via the second communication link, where the first wireless node implements simultaneously the ANF for the first\ncommunication link and a UEF for the second communication link, implement, at the first wireless node, a UEF for the first communication link, and implement, at the first wireless node, a UEF for the second communication link.  In some cases, the second\nwireless node implements an ANF for the second communication link that allows the second wireless node to schedule transmissions on the second communication link.  In some cases, one of the first communication link or the second communication link is a\nwireless backhaul link.  In some cases, the RAT includes a millimeter wave RAT.\nFrame structure manager 1235 may include information related to the synchronized frame structure.  In some cases, the synchronized frame structure is for uplink transmissions and downlink transmissions and defines the scheduled resources, each\nscheduled resource including a control portion and a data portion.  In some cases, a first subset of scheduled resources defined by the synchronized frame structure are assigned to be used by the first communication link and a second subset of scheduled\nresources defined by the synchronized frame structure are assigned to be used by the second communication link, the first subset being different than the second subset.  In some cases, the first subset or the second subset of scheduled resources includes\nat least one of a time interval, a frequency band, a code, an antenna beam, or a combination thereof.\nFIG. 13 shows a diagram of a system 1300 including a device 1305 that supports dynamic resource allocation in a wireless network in accordance with various aspects of the present disclosure.  Device 1305 may be an example of or include the\ncomponents of wireless device 1005, wireless device 1105, or an access node 105 as described above, e.g., with reference to FIGS. 1, 10 and 11.  Device 1305 may include components for bi-directional voice and data communications including components for\ntransmitting and receiving communications, including communications manager 1315, processor 1320, memory 1325, software 1330, transceiver 1335, and I/O controller 1340.  These components may be in electronic communication via one or more busses (e.g.,\nbus 1310).\nProcessor 1320 may include an intelligent hardware device, (e.g., a general-purpose processor, a digital signal processor (DSP), a central processing unit (CPU), a microcontroller, an application-specific integrated circuit (ASIC), an\nfield-programmable gate array (FPGA), a programmable logic device, a discrete gate or transistor logic component, a discrete hardware component, or any combination thereof).  In some cases, processor 1320 may be configured to operate a memory array using\na memory controller.  In other cases, a memory controller may be integrated into processor 1320.  Processor 1320 may be configured to execute computer-readable instructions stored in a memory to perform various functions (e.g., functions or tasks\nsupporting dynamic resource allocation in a wireless network).  1320.\nMemory 1325 may include random access memory (RAM) and read only memory (ROM).  The memory 1325 may store computer-readable, computer-executable software 1330 including instructions that, when executed, cause the processor to perform various\nfunctions described herein.  In some cases, the memory 1325 may contain, among other things, a basic input/output system (BIOS) which may control basic hardware and/or software operation such as the interaction with peripheral components or devices.\nSoftware 1330 may include code to implement aspects of the present disclosure, including code to support dynamic resource allocation in a wireless network.  Software 1330 may be stored in a non-transitory computer-readable medium such as system\nmemory or other memory.  In some cases, the software 1330 may not be directly executable by the processor but may cause a computer (e.g., when compiled and executed) to perform functions described herein.\nTransceiver 1335 may communicate bi-directionally, via one or more antennas, wired, or wireless links as described above.  For example, the transceiver 1335 may represent a wireless transceiver and may communicate bi-directionally with another\nwireless transceiver.  The transceiver 1335 may also include a modem to modulate the packets and provide the modulated packets to the antennas for transmission, and to demodulate packets received from the antennas.\nI/O controller 1340 may manage input and output signals for device 1305.  I/O controller 1340 may also manage peripherals not integrated into device 1305.  In some cases, I/O controller 1340 may represent a physical connection or port to an\nexternal peripheral.  In some cases, I/O controller 1340 may utilize an operating system such as iOS.RTM., ANDROID.RTM., MS-DOS.RTM., MS-WINDOWS.RTM., OS/2.RTM., UNIX.RTM., LINUX.RTM., or another known operating system.\nFIG. 14 shows a flowchart illustrating a method 1400 for dynamic resource allocation in a wireless network in accordance with various aspects of the present disclosure.  The operations of method 1400 may be implemented by an access node 105 or\nits components as described herein.  For example, the operations of method 1400 may be performed by a communications manager as described with reference to FIGS. 10 through 13.  In some examples, an access node 105 may execute a set of codes to control\nthe functional elements of the device to perform the functions described below.  Additionally or alternatively, the access node 105 may perform aspects the functions described below using special-purpose hardware.\nAt block 1405 the access node 105 may determine whether data is available to be transmitted via a first communication link using a radio access technology (RAT) that supports a synchronized frame structure.  The operations of block 1405 may be\nperformed according to the methods described with reference to FIGS. 1 through 9.  In certain examples, aspects of the operations of block 1405 may be performed by a data manager as described with reference to FIGS. 10 through 13.\nAt block 1410 the access node 105 may receive an indication whether a scheduled resource of a second communication link is available during a time interval defined by the synchronized frame structure.  The operations of block 1410 may be\nperformed according to the methods described with reference to FIGS. 1 through 9.  In certain examples, aspects of the operations of block 1410 may be performed by a resource availability manager as described with reference to FIGS. 10 through 13.\nAt block 1415 the access node 105 may transmit the data via the first communication link using the scheduled resource during the time interval based at least in part on the indication indicating that the scheduled resource of the second\ncommunication link is available during the time interval.  The operations of block 1415 may be performed according to the methods described with reference to FIGS. 1 through 9.  In certain examples, aspects of the operations of block 1415 may be\nperformed by a data manager as described with reference to FIGS. 10 through 13.\nFIG. 15 shows a flowchart illustrating a method 1500 for dynamic resource allocation in a wireless network in accordance with various aspects of the present disclosure.  The operations of method 1500 may be implemented by an access node 105 or\nits components as described herein.  For example, the operations of method 1500 may be performed by a communications manager as described with reference to FIGS. 10 through 13.  In some examples, an access node 105 may execute a set of codes to control\nthe functional elements of the device to perform the functions described below.  Additionally or alternatively, the access node 105 may perform aspects the functions described below using special-purpose hardware.\nAt block 1505 the access node 105 may determine whether data is available to be transmitted via a first communication link using a radio access technology (RAT) that supports a synchronized frame structure.  The operations of block 1505 may be\nperformed according to the methods described with reference to FIGS. 1 through 9.  In certain examples, aspects of the operations of block 1505 may be performed by a data manager as described with reference to FIGS. 10 through 13.\nAt block 1510 the access node 105 may receive an indication whether a scheduled resource of a second communication link is available during a time interval defined by the synchronized frame structure.  The operations of block 1510 may be\nperformed according to the methods described with reference to FIGS. 1 through 9.  In certain examples, aspects of the operations of block 1510 may be performed by a resource availability manager as described with reference to FIGS. 10 through 13.\nAt block 1515 the access node 105 may receive a second indication being indicative of whether a schedule resource of a third communication link is available during the time interval.  The operations of block 1515 may be performed according to\nthe methods described with reference to FIGS. 1 through 9.  In certain examples, aspects of the operations of block 1515 may be performed by a resource availability manager as described with reference to FIGS. 10 through 13.\nAt block 1520 the access node 105 may transmit the data via the first communication link during the time interval is further based at least in part on the second communication link and the third communication link being available during the time\ninterval.  The operations of block 1520 may be performed according to the methods described with reference to FIGS. 1 through 9.  In certain examples, aspects of the operations of block 1520 may be performed by a data manager as described with reference\nto FIGS. 10 through 13.\nAt block 1525 the access node 105 may transmit the data via the first communication link using the scheduled resource during the time interval based at least in part on the indication indicating that the scheduled resource of the second\ncommunication link is available during the time interval.  The operations of block 1525 may be performed according to the methods described with reference to FIGS. 1 through 9.  In certain examples, aspects of the operations of block 1525 may be\nperformed by a data manager as described with reference to FIGS. 10 through 13.\nFIG. 16 shows a flowchart illustrating a method 1600 for dynamic resource allocation in a wireless network in accordance with various aspects of the present disclosure.  The operations of method 1600 may be implemented by an access node 105 or\nits components as described herein.  For example, the operations of method 1600 may be performed by a communications manager as described with reference to FIGS. 10 through 13.  In some examples, an access node 105 may execute a set of codes to control\nthe functional elements of the device to perform the functions described below.  Additionally or alternatively, the access node 105 may perform aspects the functions described below using special-purpose hardware.\nAt block 1605 the access node 105 may determine whether data is available to be transmitted via a first communication link using a radio access technology (RAT) that supports a synchronized frame structure.  The operations of block 1605 may be\nperformed according to the methods described with reference to FIGS. 1 through 9.  In certain examples, aspects of the operations of block 1605 may be performed by a data manager as described with reference to FIGS. 10 through 13.\nAt block 1610 the access node 105 may receive an indication whether a scheduled resource of a second communication link is available during a time interval defined by the synchronized frame structure.  The operations of block 1610 may be\nperformed according to the methods described with reference to FIGS. 1 through 9.  In certain examples, aspects of the operations of block 1610 may be performed by a resource availability manager as described with reference to FIGS. 10 through 13.\nAt block 1615 the access node 105 may receive a second indication being indicative of whether a scheduled resource of a third communication link is available during a second time interval.  The operations of block 1615 may be performed according\nto the methods described with reference to FIGS. 1 through 9.  In certain examples, aspects of the operations of block 1615 may be performed by a resource availability manager as described with reference to FIGS. 10 through 13.\nAt block 1620 the access node 105 may transmit the data via the first communication link during the second time interval based at least in part on the third communication link being available during the second time interval.  The operations of\nblock 1620 may be performed according to the methods described with reference to FIGS. 1 through 9.  In certain examples, aspects of the operations of block 1620 may be performed by a data manager as described with reference to FIGS. 10 through 13.\nAt block 1625 the access node 105 may transmit the data via the first communication link using the scheduled resource during the time interval based at least in part on the indication indicating that the scheduled resource of the second\ncommunication link is available during the time interval.  The operations of block 1625 may be performed according to the methods described with reference to FIGS. 1 through 9.  In certain examples, aspects of the operations of block 1625 may be\nperformed by a data manager as described with reference to FIGS. 10 through 13.\nIt should be noted that the methods described above describe possible implementations, and that the operations and the steps may be rearranged or otherwise modified and that other implementations are possible.  Furthermore, aspects from two or\nmore of the methods may be combined.\nTechniques described herein may be used for various wireless communications systems such as code division multiple access (CDMA), time division multiple access (TDMA), frequency division multiple access (FDMA), orthogonal frequency division\nmultiple access (OFDMA), single carrier frequency division multiple access (SC-FDMA), and other systems.  The terms \"system\" and \"network\" are often used interchangeably.  A code division multiple access (CDMA) system may implement a radio technology\nsuch as CDMA2000, Universal Terrestrial Radio Access (UTRA), etc. CDMA2000 covers IS-2000, IS-95, and IS-856 standards.  IS-2000 Releases may be commonly referred to as CDMA2000 1.times., 1.times., etc. IS-856 (TIA-856) is commonly referred to as\nCDMA2000 1.times.EV-DO, High Rate Packet Data (HRPD), etc. UTRA includes Wideband CDMA (WCDMA) and other variants of CDMA.  A time division multiple access (TDMA) system may implement a radio technology such as Global System for Mobile Communications\n(GSM).\nAn orthogonal frequency division multiple access (OFDMA) system may implement a radio technology such as Ultra Mobile Broadband (UMB), Evolved UTRA (E-UTRA), Institute of Electrical and Electronics Engineers (IEEE) 802.11 (Wi-Fi), IEEE 802.16\n(WiMAX), IEEE 802.20, Flash-OFDM, etc. UTRA and E-UTRA are part of Universal Mobile Telecommunications system (UMTS).  3GPP Long Term Evolution (LTE) and LTE-Advanced (LTE-A) are new releases of Universal Mobile Telecommunications System (UMTS) that use\nE-UTRA.  UTRA, E-UTRA, UMTS, LTE, LTE-A, and Global System for Mobile communications (GSM) are described in documents from the organization named \"3rd Generation Partnership Project\" (3GPP).  CDMA2000 and UMB are described in documents from an\norganization named \"3rd Generation Partnership Project 2\" (3GPP2).  The techniques described herein may be used for the systems and radio technologies mentioned above as well as other systems and radio technologies.  While aspects an LTE system may be\ndescribed for purposes of example, and LTE terminology may be used in much of the description, the techniques described herein are applicable beyond LTE applications.\nIn LTE/LTE-A networks, including such networks described herein, the term evolved node B (eNB) may be generally used to describe the access nodes.  In other examples, the term base station may be used to describe the access nodes.  The wireless\ncommunications system or systems described herein may include a heterogeneous LTE/LTE-A network in which different types of evolved node B (eNBs) provide coverage for various geographical regions.  For example, each eNB or access node may provide\ncommunication coverage for a macro cell, a small cell, or other types of cell.  The term \"cell\" may be used to describe an access node, a carrier or component carrier associated with an access node, or a coverage area (e.g., sector, etc.) of a carrier or\naccess node, depending on context.\nAccess nodes may include or may be referred to by those skilled in the art as a base transceiver station, a radio access node, an access point, a radio transceiver, a NodeB, eNodeB (eNB), Home NodeB, a Home eNodeB, or some other suitable\nterminology.  The geographic coverage area for an access node may be divided into sectors making up only a portion of the coverage area.  The wireless communications system or systems described herein may include access nodes of different types (e.g.,\nmacro or small cell access nodes).  The UEs described herein may be able to communicate with various types of access nodes and network equipment including macro eNBs, small cell eNBs, relay access nodes, and the like.  There may be overlapping geographic\ncoverage areas for different technologies.\nA macro cell generally covers a relatively large geographic area (e.g., several kilometers in radius) and may allow unrestricted access by UEs with service subscriptions with the network provider.  A small cell is a lower-powered access node, as\ncompared with a macro cell, that may operate in the same or different (e.g., licensed, unlicensed, etc.) frequency bands as macro cells.  Small cells may include pico cells, femto cells, and micro cells according to various examples.  A pico cell, for\nexample, may cover a small geographic area and may allow unrestricted access by UEs with service subscriptions with the network provider.  A femto cell may also cover a small geographic area (e.g., a home) and may provide restricted access by UEs having\nan association with the femto cell (e.g., UEs in a closed subscriber group (CSG), UEs for users in the home, and the like).  An eNB for a macro cell may be referred to as a macro eNB.  An eNB for a small cell may be referred to as a small cell eNB, a\npico eNB, a femto eNB, or a home eNB.  An eNB may support one or multiple (e.g., two, three, four, and the like) cells (e.g., component carriers).  A UE may be able to communicate with various types of access nodes and network equipment including macro\neNBs, small cell eNBs, relay access nodes, and the like.\nThe wireless communications system or systems described herein may support synchronous or asynchronous operation.  For synchronous operation, the access nodes may have similar frame timing, and transmissions from different access nodes may be\napproximately aligned in time.  For asynchronous operation, the access nodes may have different frame timing, and transmissions from different access nodes may not be aligned in time.  The techniques described herein may be used for either synchronous or\nasynchronous operations.\nThe downlink transmissions described herein may also be called forward link transmissions while the uplink transmissions may also be called reverse link transmissions.  Each communication link described herein--including, for example, wireless\ncommunications system 100 of FIG. 1--may include one or more carriers, where each carrier may be a signal made up of multiple sub-carriers (e.g., waveform signals of different frequencies).\nThe description set forth herein, in connection with the appended drawings, describes example configurations and does not represent all the examples that may be implemented or that are within the scope of the claims.  The term \"exemplary\" used\nherein means \"serving as an example, instance, or illustration,\" and not \"preferred\" or \"advantageous over other examples.\" The detailed description includes specific details for the purpose of providing an understanding of the described techniques. \nThese techniques, however, may be practiced without these specific details.  In some instances, well-known structures and devices are shown in block diagram form in order to avoid obscuring the concepts of the described examples.\nIn the appended figures, similar components or features may have the same reference label.  Further, various components of the same type may be distinguished by following the reference label by a dash and a second label that distinguishes among\nthe similar components.  If just the first reference label is used in the specification, the description is applicable to any one of the similar components having the same first reference label irrespective of the second reference label.\nInformation and signals described herein may be represented using any of a variety of different technologies and techniques.  For example, data, instructions, commands, information, signals, bits, symbols, and chips that may be referenced\nthroughout the above description may be represented by voltages, currents, electromagnetic waves, magnetic fields or particles, optical fields or particles, or any combination thereof.\nThe various illustrative blocks and modules described in connection with the disclosure herein may be implemented or performed with a general-purpose processor, a DSP, an ASIC, an FPGA or other programmable logic device, discrete gate or\ntransistor logic, discrete hardware components, or any combination thereof designed to perform the functions described herein.  A general-purpose processor may be a microprocessor, but in the alternative, the processor may be any conventional processor,\ncontroller, microcontroller, or state machine.  A processor may also be implemented as a combination of computing devices (e.g., a combination of a DSP and a microprocessor, multiple microprocessors, one or more microprocessors in conjunction with a DSP\ncore, or any other such configuration).\nThe functions described herein may be implemented in hardware, software executed by a processor, firmware, or any combination thereof.  If implemented in software executed by a processor, the functions may be stored on or transmitted over as one\nor more instructions or code on a non-transitory computer-readable medium.  Other examples and implementations are within the scope of the disclosure and appended claims.  For example, due to the nature of software, functions described above can be\nimplemented using software executed by a processor, hardware, firmware, hardwiring, or combinations of any of these.  Features implementing functions may also be physically located at various positions, including being distributed such that portions of\nfunctions are implemented at different physical locations.  Also, as used herein, including in the claims, \"or\" as used in a list of items (for example, a list of items prefaced by a phrase such as \"at least one of\" or \"one or more of\") indicates an\ninclusive list such that, for example, a list of at least one of A, B, or C means A or B or C or AB or AC or BC or ABC (i.e., A and B and C).  Also, as used herein, the phrase \"based on\" shall not be construed as a reference to a closed set of\nconditions.  For example, an exemplary step that is described as \"based on condition A\" may be based on both a condition A and a condition B without departing from the scope of the present disclosure.  In other words, as used herein, the phrase \"based\non\" shall be construed in the same manner as the phrase \"based at least in part on.\"\nComputer-readable media includes both non-transitory computer storage media and communication media including any medium that facilitates transfer of a computer program from one place to another.  A non-transitory storage medium may be any\navailable medium that can be accessed by a general purpose or special purpose computer.  By way of example, and not limitation, non-transitory computer-readable media may comprise RAM, ROM, electrically erasable programmable read only memory (EEPROM),\ncompact disk (CD) ROM or other optical disk storage, magnetic disk storage or other magnetic storage devices, or any other non-transitory medium that can be used to carry or store desired program code means in the form of instructions or data structures\nand that can be accessed by a general-purpose or special-purpose computer, or a general-purpose or special-purpose processor.  Also, any connection is properly termed a non-transitory computer-readable medium.  For example, if the software is transmitted\nfrom a website, server, or other remote source using a coaxial cable, fiber optic cable, twisted pair, digital subscriber line (DSL), or wireless technologies such as infrared, radio, and microwave, then the coaxial cable, fiber optic cable, twisted\npair, digital subscriber line (DSL), or wireless technologies such as infrared, radio, and microwave are included in the definition of medium.  Disk and disc, as used herein, include CD, laser disc, optical disc, digital versatile disc (DVD), floppy disk\nand Blu-ray disc where disks usually reproduce data magnetically, while discs reproduce data optically with lasers.  Combinations of the above are also included within the scope of computer-readable media.\nThe description herein is provided to enable a person skilled in the art to make or use the disclosure.  Various modifications to the disclosure will be readily apparent to those skilled in the art, and the generic principles defined herein may\nbe applied to other variations without departing from the scope of the disclosure.  Thus, the disclosure is not limited to the examples and designs described herein, but is to be accorded the broadest scope consistent with the principles and novel\nfeatures disclosed herein.", "application_number": "15412367", "abstract": " Methods, systems, and devices for wireless communication are described.\n     In particular, a backhaul network that may be established between access\n     nodes and/or base stations is shown and described. To support\n     communications via the backhaul network, a synchronized frame structure\n     and unique network topologies may be established. Resources may be\n     allocated to different wireless communication links based on the\n     synchronized frame structure. Occupancy/availability indications are\n     shown and described, which enable the local redistribution of resources\n     to account for variations in signal quality and/or variations in traffic\n     experienced by the backhaul network.\n", "citations": ["7948942", "8514768", "8634842", "8737289", "8755324", "8792411", "8811262", "8848596", "8964626", "9007992", "9084246", "9215057", "9749099", "20130142136", "20130315109", "20150304014", "20160219584", "20170006627", "20170353863", "20180042031"], "related": ["62370860"]}, {"id": "20180048829", "patent_code": "10375317", "patent_name": "Low complexity auto-exposure control for computer vision and imaging\n     systems", "year": "2019", "inventor_and_country_data": " Inventors: \nChan; Victor (Del Mar, CA)  ", "description": "<BR><BR>SUMMARY\nVarious embodiments are presented for performing automatic exposure control (AEC).  According to one embodiment for performing computer vision based automatic exposure control (CV AEC), a first digital image of a scene is captured using an image\nsensor while applying a first set of values to one or more exposure control parameters.  At least one computer vision (CV) operation is performed using image data from the first digital image, thereby generating a first set of CV features extracted from\nthe first digital image, wherein the first set of CV features are from a set of possible CV features.  A mask is obtained comprising a value for each feature of the set of possible CV features.  Using the mask, a first measure of abundance is obtained of\nrelevant CV features among the first set of CV features extracted from the first digital image.  Based on the first measure of abundance of relevant CV features, an updated set of values is generated for applying to the one or more exposure control\nparameters for capturing a subsequent digital image of the scene.\nAccording to another embodiment for performing CV AEC, further steps are performed.  A second digital image of the scene is captured using the image sensor while applying a second set of values to the one or more exposure control parameters.  A\nthird digital image of the scene is captured using the image sensor while applying a third set of values to the one or more exposure control parameters.  The first set of values may correspond to a nominal level of exposure, the second set of values\ncorresponds to a level of exposure greater than the nominal level, and the third set of values corresponds to a level of exposure less than the nominal level.  The at least one CV operation is performed using image data from the second digital image\nthereby generating a second set of CV features extracted from the second digital image, wherein the second set of CV features are also from the set of possible CV features.  Using the mask, a second measure of abundance is obtained of relevant CV among\nthe second set of CV features extracted from the second digital image.  The at least one CV operation is performed using image data from the third digital image thereby generating a third set of CV features extracted from the third digital image, wherein\nthe third set of CV features are also from the set of possible CV features.  Using the mask, a third measure of abundance is obtained of relevant CV features among the third set of CV features extracted from the third digital image.  Generating the\nupdated set of values may comprise: (a) comparing the first measure of abundance of relevant CV features, the second measure of abundance of relevant CV features, and the third measure of abundance of relevant CV features to determine a greatest measure\nof abundance of relevant CV features and (b) selecting one of the first set of values, second set of values, or third set of values corresponding to the greatest measure of abundance of relevant CV features as the updated set of values.\nAccording to one embodiment for performing combined CV AEC and non-computer vision based automatic exposure control (non-CV AEC).  In the CV AEC loop, a first digital image of a scene is captured using an image sensor while applying a first set\nof values to one or more exposure control parameters.  At least one computer vision (CV) operation is performed using image data from the first digital image, thereby generating a first set of CV features extracted from the first digital image, wherein\nthe first set of CV features are from a set of possible CV features.  A mask is obtained comprising a value for each feature of the set of possible CV features.  Using the mask, a first measure of abundance is obtained of relevant CV features among the\nfirst set of CV features extracted from the first digital image.  Based on the first measure of abundance of relevant CV features, an updated set of values is generated for applying to the one or more exposure control parameters for capturing a\nsubsequent digital image of the scene.\nIn the non-CV AEC loop a digital image of the scene is captured using the image sensor while applying a non-CV AEC set of values to the one or more exposure control parameters.  A histogram of pixel brightness is generated using image data from\nthe digital image.  A desired exposure adjustment is determined based on the histogram of pixel brightness.  Based on the desired exposure adjustment, an updated non-CV AEC set of values is generated for applying to the one or more exposure control\nparameters for capturing the subsequent digital image of the scene.\nAccording to one embodiment, the histogram of pixels may comprise a first bin associated with a first range of brightness values and a second bin associated with a second range of brightness values.  The desired exposure adjustment may be\ndetermined based on at least one brightness ratio based on (a) a dark-bin value representing a pixel count of the first bin and (b) a bright-bin value representing a pixel count of the second bin.  An example of such an embodiment is referred to as\n\"Frame Average AEC.\" According to another embodiment, the histogram of pixel brightness may further comprise a third bin associated with a third range of brightness values and a fourth bin associated with a fourth range of brightness value.  The at least\none brightness ratio may comprise a limited brightness ratio based on (a) the dark-bin value and (b) the bright-bin value, as well as an expanded brightness ratio based on (a) the dark-bin value and a black-bin value representing a pixel count of the\nthird bin and (b) the bright-bin value and a white-bin value representing a pixel count of the fourth bin.  An example of such an embodiment is referred to \"Filtered Frame Average AEC.\" Determining the desired exposure adjustment may further comprise\nselecting either the limited brightness ratio or the expanded brightness ratio, based on a comparison of (a) a value including the dark-bin value and the bright-bin value and (b) a value including the black-bin value and the white-bin value, as a\nselected brightness ratio, as well as using the selected brightness ratio to determine the desired exposure adjustment.\n<BR><BR>BACKGROUND\nAspects of the disclosure relate to automatic exposure control.  For an image sensor to produce a useable signal, the sensor must adjust its gains and exposure parameters such that output signals lie within the dynamic range of the sensor. \nOverexposure leads to over-saturated pixels at the high end of the dynamic range.  FIG. 1A illustrates an example of an over-exposed image.  FIG. 1B is a luminance histogram associated with the over-exposed image shown in FIG. 1A.  By contrast,\nunderexposure leads to under-saturated pixels at the low end of the dynamic range.  FIG. 2A illustrates an example of an under-exposure image.  FIG. 2B is a luminance histogram associated with the under-exposure image shown in FIG. 2A.  In both the\nover-exposed and under-exposure cases, the resultant image's dynamic range is reduced.  Therefore, in order to consistently produce images with the widest dynamic range, it is desirable for a sensor to have a control system that automatically adjusts its\nexposure settings such that both over- or under-exposures are avoided.  This control algorithm is commonly referred to as the Automatic Exposure Control, or AEC.  Existing AEC techniques mainly rely on balancing of the pixel brightness distribution of an\nimage.  Without more, existing AEC technique have significant limitations. <BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nAspects of the disclosure are illustrated by way of example.  In the accompanying figures, like reference numbers indicate similar elements.  The figures are briefly described below:\nFIG. 1A illustrates an example of an over-exposed image;\nFIG. 1B is a luminance histogram associated with the over-exposed image shown in FIG. 1A;\nFIG. 2A illustrates an example of an under-exposed image;\nFIG. 2B is a luminance histogram associated with the under-exposure image shown in FIG. 2A;\nFIG. 3A depicts an example of center-weighted metering AEC;\nFIG. 3B depicts an example of spot-metering AEC;\nFIG. 4 shows an example of an exposure compensation dial on a camera with AEC;\nFIG. 5A illustrates an example of an image that has a large amount of black content in the background;\nFIG. 5B is a luminance histogram associated with the black content-dominated image shown in FIG. 5A;\nFIG. 6A illustrates the same black content-dominated image with Frame Average AEC applied, which leads to balanced pixel energy but over-saturation of the image;\nFIG. 6B is a luminance histogram associated with the Frame Average AEC corrected image shown in FIG. 6A;\nFIG. 7A illustrates the same black content-dominated image with Filtered Frame Average AEC applied, which leads to balancing of only the \"grey\" pixel energy;\nFIG. 7B is a luminance histogram associated with the Filtered Frame Average AEC corrected image shown in FIG. 7A;\nFIG. 8 is a flow chart illustrating an example of a non-CV AEC control loop;\nFIG. 9A is a flow chart 900 showing how the pixel ratio, R (whether R.sub.l or R.sub.f), may be used to determine whether to increase or decrease exposure, e.g., through exposure time or gain, according to embodiments of the disclosure;\nFIG. 9B illustrates a mechanism for exposure change in accordance with the flow chart shown in FIG. 9A.\nFIG. 10 illustrates a data flow for the computation of the gain for the next image frame, in logarithmic form;\nFIG. 11 is a flow chart showing the control flow of the AEC analog gain and exposure time adjustments from one frame to the next, according to certain embodiments;\nFIG. 12A shows an image on which computer vision based automatic exposure control (CV AEC) may be performed;\nFIG. 12B illustrates the distribution of CV features computed from the image shown in FIG. 12A;\nFIG. 13A shows an image having +1/2 exposure value (EV) of exposure control applied;\nFIG. 13B illustrates the distribution of CV features computed from the +1/2 EV image of FIG. 13A;\nFIG. 14A shows the same image having no exposure control applied, i.e., a nominal 0 EV applied;\nFIG. 14B illustrates the distribution of CV features computed from the 0 EV image of FIG. 14A;\nFIG. 15A shows an image having -1/2 EV of exposure control applied;\nFIG. 15B illustrates the distribution of CV features computed from the -1/2 EV image of FIG. 15A;\nFIG. 16A shows an image on which computer vision based automatic exposure control (CV AEC) may be performed;\nFIG. 16B shows a mask comprised of a binary value (i.e., \"1\" or \"0\") for each possible CV feature that may be computed from a particular CV operation or set of operations based on the image of FIG. 16A;\nFIG. 16C illustrates the distribution of CV features computed from the image shown in FIG. 16A;\nFIG. 17 is a flow chart illustrating an example of a combined AEC control loop comprising (1) a non-CV AEC control loop and (2) a CV AEC control loop;\nFIG. 18A is a flow chart 1800 showing illustrative steps in a process for performing CV AEC according to at least one embodiment of the disclosure;\nFIG. 18B is a flow chart 1820 showing illustrative steps in a process for performing CV AEC according to an embodiment of the disclosure using three digital images;\nFIG. 18C is a flow chart 1840 showing illustrative steps in a process for performing combined CV AEC and non-CV AEC according to an embodiment of the disclosure; and\nFIG. 19 illustrates an example computer system 1900 that can be used to implement features of the disclosure.\n<BR><BR>DETAILED DESCRIPTION\nSeveral illustrative embodiments will now be described with respect to the accompanying drawings, which form a part hereof.  While particular embodiments, in which one or more aspects of the disclosure may be implemented, are described below,\nother embodiments may be used and various modifications may be made without departing from the scope of the disclosure or the spirit of the appended claims.\nNon-Computer Vision Based Automatic Exposure Control (Non-CV AEC)\nVarious embodiments of the disclosure implement non-computer vision based automatic exposure control (non-CV AEC).  These are techniques that accomplish automatic exposure control without taking into account effects of exposure control on\ncomputer vision (CV) operations.  Computer vision (CV) operations and automatic exposure control techniques that take into account CV operations are discussed in later sections of the present disclosure.  For now, attention is directed to novel\ntechniques for non-CV AEC.\nMany basic non-CV AEC algorithms target the \"middle gray\" or commonly called \"18% gray\" to expose an image.  The basic assumption is that an image of a typical natural scene is composed of, on average, pixels of \"middle gray\" levels of luminance\n(there are exceptions to such an assumption, as discussed in subsequent sections).  Therefore, if a sensor is exposed for creating an image of \"middle gray\" pixels, then the image may be considered properly exposed.  In practice, in the case of a color\nRed-Green-Blue (RGB) sensor, the luminance of an image can be computed by converting the RGB values into an \"L-a-b\" color space so that the luminance value, L, can be extracted.  As is known to one of ordinary skill in the art, the \"L-a-b\" color space\ngenerally refers to a color-opponent space with dimension \"L\" for lightness/luminance and \"a\" and \"b\" for the color-opponent dimensions.  Then with the application of a nonlinear gain--gamma correction--the average luminance value of an image should\nideally lie somewhere near the middle of the dynamic range, which would be 50 if L has a range of 0 to 100, for example.  However, if the average luminance value is not in the middle (50), then there may be a number of control parameters that can be\nadjusted, including: (1) exposure time, (2) gain, and (3) aperture.  With regard to exposure time, if L is too low, then the exposure time can be lengthened to increase the luminance in an image.  Conversely, if L is too high, then the exposure time can\nbe shortened.  Since exposure time adjustment has its limit depending on the sensor's available frame rates, if adjusting exposure time alone cannot sufficiently achieve the targeted luminance value, then the gain (e.g., the AEC gain) associated with the\nimage sensor can be adjusted to scale the luminance value in the image.  The gain is sometimes referred to as the \"analog gain\" because it is applied before the image is captured and digitized.  This is in contrast to \"digital gain\" which generally\nrefers to the scaling of pixel values after the image has been captured.  With regard to aperture, if L is too high, then the aperture can be reduced.  Conversely, if L is too low, the aperture can be enlarged.  Control of parameters such as exposure\ntime, gain, and aperture in response to luminance values can form the basic feedback control of an AEC algorithm in accordance with embodiments of the present disclosure.\nAccording to some embodiments, more sophisticated non-CV AEC algorithms introduce regional biases instead of using a global pixel average.  For example, FIG. 3A depicts an example of center-weighted metering AEC.  Here, center-weighted metering\nAEC uses multiple concentric regions instead of a single global region to compute the luminance value, with higher weights in the central regions than the peripheral regions.  FIG. 3B depicts an example of spot-metering AEC.  Here, spot-metering AEC uses\nonly the single most central region to compute the luminance value and ignores the rest of the image/frame.\nAccording to further embodiments, more advanced non-CV AEC algorithms recognize that many scenes are not necessarily composed of \"middle gray\" in luminance.  For example, a \"snow\" or a \"beach\" scene will typically have a higher level of\nbrightness beyond the middle gray.  Therefore, in those cases, alternative or additional exposure compensation may be applied instead of using the middle gray as the targeted exposure.  Likewise, in a \"sunset\" scene, the exposure of the sensor needs to\nbe reduced from middle gray so that the colors of the image are not washed out.\nIn photography, the measure of exposure is often expressed in Exposure Value, or EV, due to its ease of use.  EV may be defined as:\n.function.  ##EQU00001##\nwhere F is the f-number (a measure of relative aperture commonly found in lenses), T is the exposure duration.  For example, a relative aperture of f/1.0 exposed for 1 second yields an EV of 0, while f/5.6 exposed for 1/60 second yields an EV of\n11.  As a result, the smaller the EV, the more light is let into the camera, and vice versa.  In practice, a photographer can target an exposure by fixing the EV appropriate for the scene and then adjust the associated aperture (in f-number) and exposure\ntime to create the desired artistic effects such as the field of view (FOV) or motion blur and the lack of.  Because the light sources of many visual scenes are predictable, the appropriate EV for various lighting conditions is also predictable.  Camera\nsensitivity is often described with reference, for example, to the International Organization for Standardization (ISO) 12232:2006 standard for digital still images.  For example, the lower the ISO number, the less sensitive a camera is to the light,\nwhile a higher ISO number reflects an increased sensitivity of the camera.  For example, given a sensor sensitivity of ISO 100, a daylight snow scene might use an EV of 16, while an outdoor overcast scene might use an EV of 12.  Conversely, if the light\nsource is dim such as photographing the Milky Way in a night sky, then a long exposure may be needed for the camera, and an EV of -10 might be used.\nFurthermore, EV offers a straightforward relationship in adjusting relative exposure.  An increase of one EV corresponds to halving the amount of light, while the decrease of one EV the doubling of the amount of light.  Interestingly, for\nexposure compensation nomenclature or the exposure compensation dial in a camera, \"+1 EV\" actually means to increase exposure which produces a smaller resultant EV, while \"-1 EV\" results in less exposure and a larger resultant EV in an image.  FIG. 4\nshows an example of an exposure compensation dial on a camera with AEC.\nTwo examples of non-CV AEC approaches are referred to here as Frame Average AEC and Filtered Frame Average AEC.  A typical Frame Average AEC technique targets the middle gray as the average luminance for the image.  For example, if the image\nsensor is monochromatic, then middle gray may be represented by the gray scale pixel value of 128 (out of 255).  However, Frame Average AEC algorithm may not perform well when an image is back-lit (e.g. when the sun or a bright window is behind the\nsubject) or when an image subject has a large amount of black content (e.g. when someone is wearing a black outfit).  FIG. 5A illustrates an example of an image that has a large amount of black content in the background.  FIG. 5B is a luminance histogram\nassociated with the black content-dominated image shown in FIG. 5A.\nThe typical results from a Frame Average AEC under such situations are often either over-exposure or under-exposure.  Specific image content can skew the resultant image into undesirable outcome--pixel energy is balanced but image is\nover-saturated.  FIG. 6A illustrates the same black content-dominated image with Frame Average AEC applied, which leads to balanced pixel energy but over-saturation of the image.  FIG. 6B is a luminance histogram associated with the Frame Average AEC\ncorrected image shown in FIG. 6A.\nTo address these issues, some embodiments of the present disclosure adopt a Filtered Frame Average AEC algorithm for image sensors used for extracting CV features.  FIG. 7A illustrates the same black content-dominated image with Filtered Frame\nAverage AEC applied, which leads to balancing of only the \"grey\" pixel energy--only over certain bins that represent \"grey\" pixels.  This achieves an image that is neither over-exposed nor under-exposed and is more visually appealing.  FIG. 7B is a\nluminance histogram associated with the Filtered Frame Average AEC corrected image shown in FIG. 7A.  In the luminance histograms of FIGS. 5B, 6B, and 7B, the x-axis represents an example luminance value from 0 to 256, and a y-axis of a pixel count\nranging from 0 to 1510.sup.4, 18000, 9000 respectively for each of FIGS. 5B, 6B, and 7B.\nFIG. 8 is a flow chart illustrating an example of a non-CV AEC control loop 800.  Here, the non-CV AEC control loop implements a Filtered Frame Average AEC algorithm as described herein.  The Filtered Frame Average ACE approach is explained in\nmore detail below.  In one embodiment, as an image is captured and an optional subsampling step is performed, pixels are binned into a number of categories or bins.  In the example presented below, four categories are used: Blackpixels, Darkpixels,\nBright pixels, and White pixels.  The definition of the type of a pixel, x, may be described as follows: x.fwdarw.Black Pixel, if x&lt;T.sub.k x.fwdarw.Dark Pixel, if T.sub.k.ltoreq.x&lt;T.sub.d x.fwdarw.Bright Pixel, if T.sub.d.ltoreq.x&lt;T.sub.b\nx.fwdarw.White Pixel, if x.gtoreq.T.sub.b where 0.ltoreq.T.sub.k&lt;T.sub.d&lt;T.sub.b.ltoreq.Max Pixel Value\nHere, Black Pixel Threshold is denoted by, T.sub.k; Dark Pixel Threshold is denoted by, T.sub.d; and Bright Pixel Threshold is denoted by, T.sub.b.  Max Pixel Value is the maximum value for the luminance channel, such as 255 for an unsigned,\n8-bit channel.\nIn this example, four counters, Black Pixel Count, denoted by P.sub.k, Dark Pixel Count, denoted by P.sub.d, Bright Pixel Count, denoted by P.sub.b or P.sub.br, and White Pixel Count, denoted by P.sub.w, are used to track the number of pixels\nbelonging to the four pixel types of an image.  When all the pixels in the AEC input image have been categorized, the distribution of the pixel values is computed in the form of the relative ratios between different types of pixels.  Specifically two\ntypes of AEC statistics in the form of pixel ratios are tracked: (1) an Expanded Pixel Ratio and (2) a Limited Pixel Ratio.  The Expanded Pixel Ratio takes into account more pixel categories, or pixel bins, than the Limited Pixel Ratio.  One type of\nExpanded Pixel Ratio may be a Full Pixel Ratio, which takes into account all pixel categories and is used as an illustrative example below.  The Full Pixel Ratio, R.sub.f, divides the image into two bins: Black and Dark pixels in one, and Bright and\nWhite in the other.  The LimitedPixel Ratio, R.sub.l, on the other hand, excludes the White pixels and the Black pixels and only computes the pixel ratio based on the Dark and Bright pixels.  Here, in order to avoid the potential division or\nmultiplication by zero, both the numerator and the denominator of the Full Pixel Ratio and Limited Pixel Ratio start with the value of 1:\n##EQU00002## ##EQU00002.2##\nAlternatively in the log 2 domain: Log(R.sub.f)=Log(P.sub.k+P.sub.d+1)-Log(P.sub.br+P.sub.w+1) Log(R.sub.l)=Log(P.sub.d+1)-Log(P.sub.br+1)\nIn other embodiments, the Pixel Ratio can use the average value of the pixels instead of the count of the pixels for each pixel type.  For example:\n.times..times..times..times..times..times..times..times.  ##EQU00003## .times..times..times..times..times..times..times..times.  ##EQU00003.2## where V is the pixel value.  Compared to the use of pixel count, the use of the average pixel value\nmay produce a more accurate measurement of pixel ratios, though at a higher computational complexity.\nIn this example, the motivation for using Limited Pixel Ratio is that under back-lit lighting conditions, the light source in the image may appear as White pixels, thus it makes sense to exclude any pixels that are too bright so as not to skew\nthe pixel ratio of the rest of the image.  Likewise, Black pixels are to be excluded in the Limited Pixel Ratio so that any objects that are black will not be skewing the statistics of the overall image.  As used herein, Full Pixel Ratio and Limited\nPixel Ratio are two different AEC Statistics Types, and how to decide which ratio or statistics type to use is described below.\nAccording to certain embodiments, the decision of which AEC Statistics Type to use can be based on the pixel count of the four pixel types.  For example, if the sum of Black and White pixels outnumbers that of Dark and Bright pixels, then Full\nPixel Ratio may be used, since it is more reliable, otherwise, Limited Pixel Ratio may be used.\n.function..times..times.&gt; ##EQU00004##\nIn order to avoid frequent switching between the two statistics type when the pixel counts are similar, an AEC inertia term can be used to guarantee a minimum number of consecutive captured frames yielding the new statistics type are met before\nit is adopted.\nFIG. 9A is a flow chart 900 showing how the pixel ratio, R (whether R1 or R1), may be used to determine whether to increase or decrease exposure, e.g., through exposure time or gain, according to embodiments of the disclosure.  Once the AEC\nStatistics Type is determined, the resultant pixel ratio can be used to determine whether exposure adjustments in the next frame are to be made and by how much.  In this example, if the pixel ratio, R, is greater than the Upper AEC Trigger threshold, Tu,\nthen the exposure will be increased.  On the other hand, if the pixel ratio, R, is less than 1/Lower AEC Trigger, 1/Tz, the exposure will be decreased.  In one implementations, if the pixel ratio is in-between, then no change in the exposure time or gain\nis made.  FIG. 9B illustrates a mechanism for exposure change in accordance with flow chart 1900 shown in FIG. 9A.\nThe magnitude of the exposure adjustment, Exposure Adjustment Factor, denoted as F, is a function of the selected pixel ratio, R. This factor is bounded by Maximum Adjustment Factor, F.sub.max, on the upper end, and the inverse of Minimum\nAdjustment Factor, denoted by 1/F.sub.min, on the lower end.\n.function..times..times.&gt;.times..times.&lt; ##EQU00005##\nFor example, an exposure increase by a factor of 4 will quadruple the amount of light going into the sensor, which corresponds to an increase of 2 f-stops.  Similarly, 1/4 factor will decreases the incident light by 75%, which corresponds to a\ndecrease of 2 f-stops in exposure.  By setting F.sub.max and F.sub.min, one can bound the percentage of exposure change, or f-stops, possible from one frame to the next.\nAccording to certain embodiments, the dampening is further applied to the AEC process.  Camera AEC algorithms are typically in the form of a delay feedback loop to allow adjustments to the analog gain or exposure time to take effect over time,\nfor example, over multiple frames.  As a result, if the magnitude of the changes is large, it can result in over-compensation due to the time delay, and exposure oscillations can occur.  To dampen the potential oscillations, the exposure change can be\nput into effect as an infinite impulse response (IIR) system, where the gain change is the difference between the target exposure and the current exposure, and the change is being divided and spread out over multiple frames.  The dampening operation uses\na scaling factor to scale the gain at each frame.  The scaling factor may be determined based on a desired number of frames over which the dampening operation distributes the effects of dampening.  Here, e.g., in an IIR system, the desired number of\nframes may correspond to the minimum number of frames that it would take for the change in the gain (or other exposure control input) to be fully applied.  An illustrative formula for an AEC gain algorithm employing dampening is as follows:\n'.function.  ##EQU00006## ##EQU00006.2## '.function..times.  ##EQU00006.3## ##EQU00006.4## ' ##EQU00006.5##\nwhere G.sub.t=n is the current gain value, R.sub.t=n is the current image's selected pixel ratio, F.sub.t=n .ident.F(R.sub.t=n) is the current exposure adjustment factor, {acute over (G)}.sub.t=n+1 is the proposed new gain for the next frame,\nI.sub.t=n is the AEC Inertia term which serves to gradually incorporate the gain change over multiple frames.  As can be seen, G.sub.t=n*F.sub.t=n is the feedback term in the AEC gain change algorithm, such that if F=1, then the observed pixel ratio is\nbalanced and there will be no need for any further gain change, such that G.sub.t=n+1=G.sub.t=n. Furthermore, if I=1 then the full amount of the gain change will be rolled into the immediate next frame, however, if I&gt;1 then it will take at least I\nframes.  For simplification the term,\n##EQU00007## can be considered as the Gain Multiplier, M.sub.t=n, such that the proposed gain is the product of the current gain and the gain multiplier.  In order to control the AEC convergence time, T, the value of I.sub.t=n can be defined as\na function of the frame rate, F.sub.t=n, such that: I.sub.t=n.ltoreq.TF.sub.t=n\nFor example, if it is desired for the exposure to converge within 0.5 s, then at 30 fps, then I should be set to no more than 15.  If the frame rate now changes to 10 fps, then I should be adjusted to no more than 5.\nFor implementation efficiency in hardware, the above formulation can be written in the log 2 domain:\n'.function.  ##EQU00008## '.function.  ##EQU00008.2## .function.'.function..function..function.  ##EQU00008.3##\nFIG. 10 illustrates a data flow 1000 for the computation of the gain log({acute over (G)}.sub.t=n+1) for the next image frame, in logarithmic form.\nAccording to certain embodiments, depending on the value of proposed new gain, G.sub.t=n+1, and the value of the current analog gain value, G.sub.t=n, and current exposure time, E.sub.t=n, the exposure time and/or analog gain for the next frame\nmay be adjusted.  Specifically, all vision sensors have a specific dynamic range in its analog gain, bounded by Min Analog Gain, denoted by G.sub.min, and Max Analog Gain, denoted by G.sub.max.  Likewise, for exposure time, there is a Min Exposure Time,\nE.sub.min, and a Max Exposure Time, E.sub.max, that bound its dynamic range.  And there is an inter-dependency between the analog gain value and the exposure time value such that changing one will change the other, if everything else stays the same.  As\na result, the computation of the new analog gain, G.sub.t=n+1, and new exposure time, E.sub.t=n+1, for the next frame can be divided into the following 5 scenarios, for example: 1.  G.sub.min.ltoreq.{acute over\n(G)}.sub.t=n+1.ltoreq.G.sub.max.fwdarw.G.sub.t=n+1={acute over (G)}.sub.t=n+1 If the new gain falls within the range of maximum and minimum analog gain, then only the analog gain is changed but the exposure time stays the same.  In the log 2 domain:\nlog(G.sub.t=n+1)=log({acute over (G)}.sub.t=n+1) 2.  {acute over (G)}.sub.t=n+1&gt;G.sub.max and E.sub.t=n&lt;E.sub.max.fwdarw.E.sub.t=n+1=M.sub.t=n*E.sub.t=n If the new gain is greater than the maximum analog gain but the maximum exposure time has not\nbeen reached, then change the exposure time.  In the log 2 domain: log(E.sub.t=n+1)=log(M.sub.t=n)+log(E.sub.t=n) 3.  {acute over (G)}.sub.t=n+1&lt;G.sub.min and E.sub.t=n&gt;E.sub.min.fwdarw.E.sub.t=n+1=M.sub.t=n*E.sub.t=n If the new gain is smaller\nthan the minimum analog gain but the minimum exposure time has not been reached, then change the exposure time.  In the log 2 domain: log(E.sub.t=n+1)=log(M.sub.t=n)+log(E.sub.t=n) 4.  {acute over (G)}.sub.t=n+1&gt;G.sub.max and\nE.sub.t=n=E.sub.max.fwdarw.G.sub.t=n+1=G.sub.max If the new gain is greater than the maximum analog gain but the maximum exposure time has been reached, then set the new gain to the maximum analog gain.  In the log 2 domain:\nlog(G.sub.t=n+1)=log(G.sub.max) 5.  {acute over (G)}.sub.t=n+1&lt;G.sub.min and E.sub.t=n=E.sub.min.fwdarw.G.sub.t=n+1=G.sub.min If the new gain is smaller than the minimum analog gain but the minimum exposure time has been reached, then set the new gain\nto the minimum analog gain.  In the log 2 domain: log(G.sub.t=n+1)=log(G.sub.min)\nFIG. 11 is a flow chart 1100 showing the control flow of the AEC analog gain and exposure time adjustments from one frame to the next, according to certain embodiments as described in the five scenarios above.\nAccording to further embodiments, in the case where the AEC Statistics Type changes from Full Pixel Ratio to Limited Pixel Ratio, a transition in the respective gain and exposure values may help ensure a smooth transition.  For example, the\nsimple average of the gain and exposure time between the can be used for the transitional frame: G.sub.t=n+1=(G.sub.t=n+1+G.sub.t=n)/2 E.sub.t=n+1=(E.sub.t=n+1+E.sub.t=n)/2\nAdditionally, under specific lighting situations, the user might desire to set a bias in the brightness of the image frame by adding an exposure compensation.  This can be achieved by simply setting the appropriate value of the Dark Pixel\nThreshold, T.sub.d, according to embodiments of the disclosure.  Specifically, if no exposure compensation is desired, then the relationship between the three pixel thresholds may be as follows: T.sub.d=(T.sub.k+T.sub.b)/2 such that the value of\nT.sub.d-T.sub.k is the same as that of T.sub.b-T.sub.d.  As a result, in a uniformly distributed pixel frame, the amount of pixels being counted as Bright Pixels may be the same as that of Dark Pixels, and the resultant Pixel Ratio may be 1.  In the\ngeneral case, the amount of Exposure Compensation, as measured by Exposure Value, EV, can be set using the following formula:\n##EQU00009## ##EQU00009.2## ##EQU00009.3##\nFor example, say T.sub.k=5; T.sub.b=205, and we want to under-expose the frame by 1 f-stop such that EV=-1, then we can compute T.sub.d as follow:\n##EQU00010## .apprxeq.  ##EQU00010.2## Computer Vision Based Automatic Exposure Control (CV AEC)\nAlternatively or additionally, various embodiments of the disclosure implement computer vision based automatic exposure control (CV AEC).  CV AEC refers to techniques that accomplish automatic exposure control by taking into account effects of\nexposure control on computer vision (CV) operations.  Here, the purpose of sensing an image may not necessarily be to reconstruct or faithfully transmit an image.  Rather, the purpose may be to transform an input image such that reliable, high quality\ncomputer vision (CV) features can be extracted.  In other words, instead of emphasizing on image quality based on human aesthetics as in conventional cameras, the system can focus on the quality or quantity of the extracted CV feature to be processed\nusing the image.  Many such CV features are primarily extracting relative pixel gradients.  For example, local binary pattern (LBP) and multi-block LBP (MB-LBP) are local gradient extractions relative to the center pixel.  Center symmetric LBP (CS-LBP)\nis local gradient extractions relative to spatially opposing pixels.  Patch symmetric LBP (PS-LBP) and its variants are local gradient extractions relative to a collection of local pixels.  Haar-like features consider adjacent rectangular regions at a\nspecific location in a detection window, sums up the pixel intensities in each region and calculates the difference between these sums.  LBP, MB-LBP, CS-LBP, PS-LBP, Harr-like features, etc., are just some examples of CV features that may be used in\ncomputer-based image analysis/detection techniques.  Here, so long as the relative value of the extracted pixels within the CV feature is preserved, then it may be sufficient to produce consistent CV features.  As a result, application of global image\nnormalization or digital gain may not be necessary for various types of CV feature extraction, since such application does not necessarily impact the relative values among pixels.\nFIG. 12A shows an image on which computer vision based automatic exposure control (CV AEC) may be performed.  FIG. 12B illustrates the distribution of CV features computed from the image shown in FIG. 12A.  Unlike pixel luminance distribution\n(e.g., the histograms shown in FIGS. 5B, 6B, and 7B), CV feature distribution is spiky.  Adjusting gain/exposure does not necessarily result in a monolithic shift in CV feature distribution, as in non CV-AEC.  Furthermore, depending on the CV task at\nhand, only a subset of all possible CV features are relevant to a particular classifier, e.g., a machine learning (ML) classifier that takes CV features as input and is trained to detect the presence of a given target object, in the illustrated example,\na Minion character.  In the histograms showing the distribution of CV features of FIG. 12B, and the subsequent FIGS. 13B, 14B, 15B, and 16C, the x-axis represents a given LBP computer vision feature which corresponds to an 8-bit number, and hence the\ndifferent computer vision features range from 0 (for LBP feature 00000000) to 256 (for LBP feature 11111111), and a y-axis of a CV feature count ranging from 0 to 3.510.sup.4, 3.510.sup.4, 3.510.sup.4, 910.sup.4, and 3.510.sup.4 respectively for each of\nFIGS. 12B, 13B, 14B, 15B, and 16C.\nAccording to various embodiments, CV AEC adjusts exposure control parameters, such as gain, exposure time, aperture, etc., to maximize a targeted set of CV feature extraction.  FIG. 13A shows an image having +1/2 EV of exposure adjustment\napplied.  FIG. 13B illustrates the distribution of CV features computed from the +1/2 EV image of FIG. 13A.  FIG. 14A shows the same image having no exposure adjustment applied, i.e., a nominal 0 EV applied.  FIG. 14B illustrates the distribution of CV\nfeatures computed from the 0 EV image of FIG. 14A.  FIG. 15A shows an image having -1/2 EV of exposure adjustment applied.  FIG. 15B illustrates the distribution of CV features computed from the -1/2 EV image of FIG. 15A.  As can be seen from these\nfigures, different levels of exposure control may produce different distributions of CV features.  Here, the +1/2 EV exposure control produces most CV features that are relevant to detection of the Minion character.  That is, out of the three exposure\nadjustment levels, +1/2 EV produces the most abundant CV features from within the subset of CV features relevant to the Minion classifier.  In response, the CV AEC system chooses the +1/2 EV as the exposure control level for a subsequent image to be\nacquired.  The process may be repeatedly performed to settle on an optimal exposure control level.\nFIG. 16A shows an image on which computer vision based automatic exposure control (CV AEC) may be performed.  FIG. 16B shows a mask comprised of a binary value (i.e., \"1\" or \"0\") for each possible CV feature that may be generated from a\nparticular CV operation or set of operations based on the image of FIG. 16A.  In the illustrated implementation of a binary mask, the mask identifies only those CV features that are relevant--in the illustrated example, that are relevant to a particular\ntrained classifier for detecting a given object, in the illustrated example, the classifier trained to detect the Minion character mentioned above.  Within the mask, each position marked with a non-zero value (i.e., \"1\") corresponds to a CV feature that\nis relevant to the particular classifier.  Each position marked with a zero value (i.e., \"0\") corresponds to a CV feature that is not relevant to the particular classifier.  Thus, the mask identifies the subset of the set of possible CV features, where\nthe set of possible CV features represent the CV features capable of being generated by the CV operation (or set of operations), that are relevant to the classifier.  According to certain embodiments, the mask may be generated as an output of the\ntraining process associated with training the classifier for detection of a particular object.  According to other embodiments, the mask may be generated without training a classifier.  For example, it may be recognized that particular types of CV\nfeatures are associated with certain image feature and are therefore worth emphasizing.  Just as an example, where the CV operation comprises a local binary pattern (LBP), one type of CV feature that may be emphasized is one that contains only one pair\nof transitions in the binary pattern.  Such an occurrence may be referred to as \"transition-LBPn\" where n=1 (e.g., \" .  . . 00100 .  . . \" or \" . . . 11011 .  . . \"). Such CV features may be emphasized by setting the corresponding positions to a non-zero\nvalue (i.e., \"1\") within the mask and setting other position to a zero value (i.e., \"0\") within the mask.  Qualitatively, LBP patterns containing a higher number of transitions, e.g., \"transition-LPBn\" where n is a larger number, tend to contain texture\ninformation.  Accordingly, if texture details are not necessary for the objective of the system, e.g., detection of a particular object or type of object in the scene, then such CV features may be de-emphasized by setting the corresponding positions to a\nzero value (i.e., \"0\") within the mask and setting other positions to a non-zero value (i.e., \"1\").\nWhile the mask shown in FIG. 16B comprises binary values, in other embodiments, the mask may comprise other types of values.  In one embodiment, the mask comprises real values that represent different weights to be applied.  That is, instead of\ndividing CV features into either \"relevant\" or \"not relevant\" as in the case of the binary mask, a weighted mask having real values may assign a different relevance weight to each of the possible CV features resulting from a CV operation.  Each relevant\nvalue is represented by a real number within the mask.  The real numbers may be implemented in various ways, for example, as integers within a range (e.g., -10 to +10, 0 to 15, etc.).  In another example, the real numbers may be implemented as decimal\nnumbers or fractions.  In such an implementation, a measure of abundance of relevant CV features could represent a score, such as a weighted sum, indicating, for example, how many relevant or important CV features a set of CV features extracted from an\nimage includes.  Such a score can allow the comparison of two or more images to determine which of the two images has a higher score.  In addition to a weighted sum, different kinds of scores can additionally or alternatively indicate whether a set of CV\nfeatures extracted from an image includes a diversity of relevant or important CV features as compared to a multitude of the same single relevant or important CV feature or small group of relevant or important CV features.  In some implementations,\nmultiple scores can be computed that reflect different aspects of a measure of abundance of relevant CV features, and the multiple scores can be combined to prefer one image (or set of exposure parameters) over others.\nFIG. 16C illustrates the distribution of CV features computed from the image shown in FIG. 16A.  The X-axes of FIGS. 16B and 16C align, i.e., they have a one-to-one correspondence.  The mask shown in FIG. 16B, once applied to the CV\ndistribution, identifies which CV features shown in FIG. 1 6C would be relevant to this particular classifier.  Using the mask, the CV AEC technique may select a particular exposure control level (e.g., +1/2 EV) out of a plurality of possible exposure\ncontrol levels (e.g., +1/2 EV, 0 EV, -1/2 EV), that maximizes the abundance of CV features relevant to detection of a particular object by a classifier.  The abundance of relevant CV features generated by the image exposed using an exposure control level\nthat increases the reliability of subsequent classifier decision compared to a different exposure control level.  As discussed previously, the technique may be repeatedly performed, such as in a loop, until an optimal level of exposure is reached.\nThus, using the mask, a measure of abundance of relevant CV features may be obtained, among the set of CV features extracted from each digital image.  For example, in a case of a binary mask, the measure of abundance may be a basic count of\nnumber of CV features (e.g., FIG. 16B) generated from the digital image (e.g., FIG. 16A) that align with the non-zero (e.g., \"1\") values of the mask (e.g., FIG. 16C).  In another example, in a case of a weighted mask, the measure of abundance may be a\nweighted count or score resulting from multiplying each CV feature generated from the digital image with the corresponding weight (e.g., real number) from the weighted mask, then adding all the multiplication results together.  Thus, one manner of\ngenerating the measure of abundance, such as the count, weighted count, or score, is taking the dot product of the mask with the CV features generated from the digital image.\nNote that the CV AEC technique, including use of the mask, iteration within one or more loops, etc., can be performed prior to operation of the classifier.  In other words, CV AEC can be performed to adjust exposure for capturing an image, in a\nmanner that maximizes the abundance of CV features relevant to a classifier, prior to actually providing the CV features to the classifier to classify the image.\nFIG. 17 is a flow chart illustrating an example of a combined AEC control loop 1700 comprising (1) a non-CV AEC control loop 1710 and (2) a CV AEC control loop 1720.  According to various embodiments of the disclosure, a combined technique\ninvolves first running the non-CV AEC control loop to settle on a good exposure setting with respect to visual quality (based on pixel brightness distribution), then running the CV AEC loop to refine the exposure settings to maximize the count of\nrelevant CV features (using a CV feature relevance mask) for detecting a particular object by a classifier.\nFor example, in the non-CV AEC control loop 1710, the process iterates in the loop in a manner similar to that described with respect to FIG. 8, until a good exposure setting with respect to visual quality is reached.  The exit condition is\nreflected by the \"AEC Triggered?\" block.  As discussed previously, the pixel ratio (e.g., either the Limited Pixel Ratio or the Full Pixel Ratio) is evaluated against a threshold, e.g., the AEC Trigger.  If AEC is triggered (\"Yes\"), it means that the\nnon-CV AEC control loop 1710 has not converged, so the loop is iterated.  If AEC is not triggered (\"No\"), it means that the non-CV AEC control loop 1710 has converged.  This is an example of reaching a non-CV AEC target condition.  At this point, the CV\nAEC control loop 1720 may be initiated.  The CV AEC control loop 1720 acquires three images at different exposure levels, for example+1/2 EV, 0 EV, and -1/2 EV.  In each case, at least one CV operation(s) may be performed on the acquired image to\ngenerate a set of CV features extracted from the acquired image, wherein the generated set of CV features are from the set of possible CV features.  The mask for determining a measure of abundance of relevant CV features extracted from a given image\n(taken using a given exposure level), where, in some examples, the relevant CV features may be relevant for a particular classifier (e.g., for detecting the Minion character) may be applied.  The exposure level that results in the most abundant CV\nfeatures relevant to the classifier may be chosen as the \"winning\" exposure level, out of the three levels+1/2 EV, 0 EV, and -1/2 EV.  The \"winning\" exposure level may be set as the new nominal exposure level in the next iteration of the loop.  For\nexample, if in iteration N, the \"winning\" exposure level is +1/2 EV, then that exposure level is set as the nominal exposure level (i.e., 0 EV) in the next iteration of the CV AEC control loop 1720.  The CV AEC control loop 1720 may iterate until it\nsettles and an optimal exposure level is reached with respect to maximizing relevant CV features for the classifier.\nFIG. 18A is a flow chart 1800 showing illustrative steps in a process for performing CV AEC according to at least one embodiment of the disclosure.  For example, in a step 1802, a first digital image of a scene is captured using an image sensor\nwhile applying a first set of values to one or more exposure control parameters.  Step 1802 may be carried out, for example, by using camera(s) 1920, which is described later in connection with FIG. 19.  In a step 1804, at least one computer vision (CV)\noperation is performed using image data from the first digital image, thereby generating a first set of CV features extracted from the first digital image, wherein the first set of CV features are from a set of possible CV features.  In a step 1806, a\nmask is obtained comprising a value for each feature of the set of possible CV features.  One example of such a mask is described with reference to FIG. 16B.  In a step 1808, using the mask, a first measure of abundance is obtained of relevant CV\nfeatures among the first set of CV features extracted from the first digital image.  Examples of measures of abundance are described with reference to FIGS. 16A, 16B, and 16C.  In a step 1810, based on the first measure of abundance of relevant CV\nfeatures, an updated set of values is generated for applying to the one or more exposure control parameters for capturing a subsequent digital image of the scene.  Steps 1804 through 1810 may be carried out, for example by using processor(s) 1904 or\nother circuitry, which are described later in connection with FIG. 19.\nAccording to one embodiment, obtaining the first measure of abundance of relevant CV features comprises (a) determining a count of relevant CV features among the first set of CV features extracted from the first digital image using the mask and\n(b) using the count of relevant CV features among the first set of CV features extracted from the first digital image as the first measure of abundance of relevant CV features.  In one embodiment, the mask comprises binary values.  In another embodiment,\nthe mask comprises real values representing different weights.  In one embodiment, the subset of CV features identified by the mask represent CV features, from a set of possible CV features, relevant to a particular classifier, as discussed previously.\nFIG. 18B is a flow chart 1820 showing illustrative steps in a process for performing CV AEC according to an embodiment of the disclosure using three digital images.  Here, the steps shown in FIG. 18B may be combined with the steps shown in FIG.\n18A.  Indeed, various features already described in FIG. 18A, such as the first digital image, the mask of relevant CV features, and the updated set of values (to the one ore more exposure control parameters) are referred to again in FIG. 18B.  Referring\nto FIG. 18B, in a step 1822, a second digital image of the scene is captured using the image sensor while applying a second set of values to the one or more exposure control parameters.  In a step 1824, a third digital image of the scene is captured\nusing the image sensor while applying a third set of values to the one or more exposure control parameters.  Steps 1822 and 1824 may be carried out, for example, by using camera(s) 1920, which is described later in connection with FIG. 19.  Here, the\nfirst set of values may correspond to a nominal level of exposure.  The second set of values may correspond to a level of exposure greater than the nominal level.  The third set of values may correspond to a level of exposure less than the nominal level. In a step 1826, the at least one CV operation is performed using image data from the second digital image thereby generating a second set of CV features extracted from the second digital image, wherein the second set of CV features are also from the set\nof possible CV features.  In a step 1828, using the mask, a second measure of abundance of relevant CV is obtained among the second set of CV features extracted from the second digital image.  In a step 1830, the at least one CV operation is performed\nusing image data from the third digital image thereby generating a third set of CV features extracted from the third digital image, wherein the third set of CV features are also from the set of possible CV features.  In a step 1832, using the mask, a\nsecond measure of abundance of relevant CV is obtained among the second set of CV features extracted from the second digital image.  In a step 1834, more detailed steps for generating the updated set of values to the one or more exposure control\nparameters are specified.  These include (a) comparing the first measure of abundance of relevant CV features, the second measure of abundance of relevant CV features, and the third measure of abundance of relevant CV features to determine a greatest\nmeasure of abundance of relevant CV features and (b) selecting one of the first set of values, second set of values, or third set of values corresponding to the greatest measure of abundance of relevant CV features as the updated set of values.  Steps\n1826 through 1834 may be carried out, for example by using processor(s) 1904 or other circuitry, which are described later in connection with FIG. 19.\nWhile not explicitly shown in the figures, CV AEC using two digital images may also be implemented according to another embodiment of the disclosure.  Steps for CV AEC using a first digital image according to FIG. 18A may be performed.  In\naddition, a second digital image of the scene may be captured using the image sensor while applying a second set of values to the one or more exposure control parameters.  The at least one CV operation may be performed using image data from the second\ndigital image thereby generating a second set of CV features extracted from the second digital image, wherein the second set of CV features are also from the set of possible CV features.  Using the mask, a second measure of abundance of relevant CV\nfeatures may be obtained among the second set of CV features extracted from the second digital image.  Here, generating the updated set of values may comprise (a) comparing the first measure of abundance of relevant CV features to the second measure of\nabundance of relevant CV features and (b) in response to the second measure of abundance being greater than the first measure of abundance, selecting the second set of values as the updated set of values.\nFIG. 18C is a flow chart 1840 showing illustrative steps in a process for performing combined CV AEC and non-CV AEC according to an embodiment of the disclosure.  Here the process comprises two loops, a CV automatic exposure control (CV AEC)\nloop 1850 and a non-CV automatic exposure control (non-CV AEC) loop 1870.  An example of CV AEC loop 1850 is shown in FIG. 17 as the CV AEC control loop 1720.  An example of non-CV AEC loop 1870 is shown in FIG. 17 as the non-CV AEC control loop 1710. \nWhile in FIG. 17, the non-CV AEC control loop 1710 is performed prior to the CV AEC control loop 1720, the embodiment shown in FIG. 18C does not specify a particular order of operation.  Nevertheless, in one specific embodiment, the steps for non-CV AEC\nloop 1870 are performed prior to the steps for CV AEC loop 1850.  In a specific embodiment, non-CV AEC loop 1870 is repeatedly performed until a non-CV AEC target condition is met, and upon meeting the non-CV AEC target condition, CV AEC loop 1850 is\nperformed.\nReferring back to FIG. 18C, the steps for CV AEC loop 1850 may be similar to the steps already described in FIG. 18A for performing CV AEC.  Specifically, in a step 1852, a first digital image of a scene is captured using an image sensor while\napplying a first set of values to one or more exposure control parameters.  Step 1852 may be carried out, for example, by using camera(s) 1920, which is described later in connection with FIG. 19.  In a step 1854, at least one computer vision (CV)\noperation is performed using image data from the first digital image, thereby generating a first set of CV features extracted from the first digital image, wherein the first set of CV features are from a set of possible CV features.  In a step 1856, a\nmask is obtained comprising a value for each feature of the set of possible CV features.  In a step 1858, using the mask, a first measure of abundance is obtained of relevant CV features among the first set of CV features extracted from the first digital\nimage.  In a step 1860, based on the first measure of abundance of relevant CV features, an updated set of values is generated for applying to the one or more exposure control parameters for capturing a subsequent digital image of the scene.  Steps 1854\nthrough 1860 may be carried out, for example by using processor(s) 1904 or other circuitry, which are described later in connection with FIG. 19.\nReferring again to FIG. 18C, non-CV AEC loop 1870 may comprise a number of steps.  For example, in a step 1872, a digital image of the scene is captured using the image sensor while applying a non-CV AEC set of values to the one or more exposure\ncontrol parameters.  Step 1872 may be carried out, for example, by using camera(s) 1920, which is described later in connection with FIG. 19.  In a step 1874, a histogram of pixel brightness is generated using image data from the digital image.  In a\nstep 1876, a desired exposure adjustment is determined based on the histogram of pixel brightness.  In a step 1878, based on the desired exposure adjustment, an updated non-CV AEC set of values is generated for applying to the one or more exposure\ncontrol parameters for capturing the subsequent digital image of the scene.  Steps 1874 through 1878 may be carried out, for example by using processor(s) 1904 or other circuitry, which are described later in connection with FIG. 19.\nAccording to one embodiment, the histogram of pixels may comprise a first bin associated with a first range of brightness values and a second bin associated with a second range of brightness values.  The desired exposure adjustment may be\ndetermined based on at least one brightness ratio based on (a) a dark-bin value representing a pixel count of the first bin and (b) a bright-bin value representing a pixel count of the second bin.  An example of such an embodiment is referred to as\n\"Frame Average AEC\" which is described previously.\nAccording to another embodiment, the histogram of pixel brightness may further comprise a third bin associated with a third range of brightness values and a fourth bin associated with a fourth range of brightness value.  The at least one\nbrightness ratio may comprise a limited brightness ratio based on (a) the dark-bin value and (b) the bright-bin value, as well as an expanded brightness ratio based on (a) the dark-bin value and a black-bin value representing a pixel count of the third\nbin and (b) the bright-bin value and a white-bin value representing a pixel count of the fourth bin.  An example of such an embodiment is referred to \"Filtered Frame Average AEC\" which is described previously, e.g., with respect to FIGS. 7A, 7B, and 8.\nDetermining the desired exposure adjustment may further comprise selecting either the limited brightness ratio or the expanded brightness ratio, based on a comparison of (a) a value including the dark-bin value and the bright-bin value and (b) a\nvalue including the black-bin value and the white-bin value, as a selected brightness ratio, as well as using the selected brightness ratio to determine the desired exposure adjustment.\nFIG. 19 illustrates an example computer system 1900 that can be used to implement features of the disclosure.  Computer system 1900 is shown comprising hardware elements that can be electrically coupled via a bus 1902 (or may otherwise be in\ncommunication, as appropriate).  The hardware elements may include one or more processors 1904, including without limitation one or more general-purpose processors and/or one or more special-purpose processors (such as digital signal processing chips,\ngraphics processing units 1922, and/or the like); one or more input devices 1908, which can include without limitation one or more cameras, sensors, a mouse, a keyboard, a microphone configured to detect ultrasound or other sounds, and/or the like; and\none or more output devices 1910, which can include without limitation a display unit such as the device used in implementations of the invention, a printer and/or the like.  Additional cameras 1920 may be employed for detection of user's extremities and\ngestures.  In some implementations, input devices 1908 may include one or more sensors such as infrared, depth, and/or ultrasound sensors.  The graphics processing unit 1922 may be used to carry out the method for real-time wiping and replacement of\nobjects described above.  According to certain embodiments of the disclosure, an image sensor for capturing a digital image on which automatic exposure control (AEC) is performed may be part of camera(s) 1920.  Additional digital images captured for\nperforming AEC may also be captured using the same image sensor that is part of camera(s) 1920.\nIn some implementations of the implementations of the invention, various input devices 1908 and output devices 1910 may be embedded into interfaces such as display devices, tables, floors, walls, and window screens.  Furthermore, input devices\n1908 and output devices 1910 coupled to the processors may form multi-dimensional tracking systems.\nThe computer system 1900 may further include (and/or be in communication with) one or more non-transitory storage devices 1906, which can comprise, without limitation, local and/or network accessible storage, and/or can include, without\nlimitation, a disk drive, a drive array, an optical storage device, a solid-state storage device such as a random access memory (RAM) and/or a read-only memory (ROM), which can be programmable, flash-updateable and/or the like.  Such storage devices may\nbe configured to implement any appropriate data storage, including without limitation, various file systems, database structures, and/or the like.\nThe computer system 1900 might also include a communications subsystem 1912, which can include without limitation a modem, a network card (wireless or wired), an infrared communication device, a wireless communication device and/or chipset (such\nas a Bluetooth device, an 802.11 device, a WiFi device, a WiMax device, cellular communication facilities, etc.), and/or the like.  The communications subsystem 1912 may permit data to be exchanged with a network, other computer systems, and/or any other\ndevices described herein.  In many implementations, the computer system 1900 will further comprise a non-transitory working memory 1920, which can include a RAM or ROM device, as described above.\nThe computer system 1900 also can comprise software elements, shown as being currently located within the working memory 1920, including an operating system 1914, device drivers, executable libraries, and/or other code, such as one or more\napplication programs 1916, which may comprise computer programs provided by various implementations, and/or may be designed to implement methods, and/or configure systems, provided by other implementations, as described herein.  Merely by way of example,\none or more procedures described with respect to the method(s) discussed above might be implemented as code and/or instructions executable by a computer (and/or a processor within a computer); in an aspect, then, such code and/or instructions can be used\nto configure and/or adapt a general purpose computer (or other device) to perform one or more operations in accordance with the described methods.\nA set of these instructions and/or code might be stored on a computer-readable storage medium, such as the storage device(s) 1906 described above.  In some cases, the storage medium might be incorporated within a computer system, such as\ncomputer system 1900.  In other implementations, the storage medium might be separate from a computer system (e.g., a removable medium, such as a compact disc), and/or provided in an installation package, such that the storage medium can be used to\nprogram, configure and/or adapt a general purpose computer with the instructions/code stored thereon.  These instructions might take the form of executable code, which may be executable by the computer system 1900 and/or might take the form of source\nand/or installable code, which, upon compilation and/or installation on the computer system 1900 (e.g., using any of a variety of generally available compilers, installation programs, compression/decompression utilities, etc.) then takes the form of\nexecutable code.\nSubstantial variations may be made in accordance with specific requirements.  For example, customized hardware might also be used, and/or particular elements might be implemented in hardware, software (including portable software, such as\napplets, etc.), or both.  Further, connection to other computing devices such as network input/output devices may be employed.  In some implementations, one or more elements of the computer system 1900 may be omitted or may be implemented separate from\nthe illustrated system.  For example, the processor 1904 and/or other elements may be implemented separate from the input device 1908.  In one implementation, the processor may be configured to receive images from one or more cameras that are separately\nimplemented.  In some implementations, elements in addition to those illustrated in FIG. 4 may be included in the computer system 1900.\nSome implementations may employ a computer system (such as the computer system 1900) to perform methods in accordance with the disclosure.  For example, some or all of the procedures of the described methods may be performed by the computer\nsystem 1900 in response to processor 1904 executing one or more sequences of one or more instructions (which might be incorporated into the operating system 1914 and/or other code, such as an application program 1916) contained in the working memory\n1920.  Such instructions may be read into the working memory 1920 from another computer-readable medium, such as one or more of the storage device(s) 1906.  Merely by way of example, execution of the sequences of instructions contained in the working\nmemory 1920 might cause the processor(s) 1904 to perform one or more procedures of the methods described herein.\nFor example, according to various embodiments of the disclosure, at least one processor such as processor(s) 1904, while executing one or more sequences of one or more instructions contained in the working memory 1920, may carry out various\nsteps for implementing AEC, including CV AEC and non-CV AEC.  Alternatively or additionally, circuitry not part of processor(s) 1904 may also be used to perform various steps for implementing AES.  Such circuitry may include peripheral circuitry disposed\naround or near the image sensor that is part of camera(s) 1920.  Such peripheral circuitry may be configured to perform CV operations and generate CV features in a manner that is more power-efficient than processor(s) 1904.  For example, processor(s)\n1904 or other circuitry may be configured to (a) apply a first set of values to one or more exposure control parameters while a first digital image is captured, (b) perform at least one computer vision (CV) operation using image data from the first\ndigital image, thereby generating a first set of CV features extracted from the first digital image, wherein the first set of CV features are from a set of possible CV features, (c) obtain a mask comprising a value for each feature of the set of possible\nCV features; (d) using the mask, obtain a first measure of abundance of relevant CV features among the first set of CV features extracted from the first digital image; and (e) based on the first measure of abundance of relevant CV features, generate an\nupdated set of values for applying to the one or more exposure control parameters for capturing a subsequent digital image of the scene.\nThe terms \"machine-readable medium\" and \"computer-readable medium,\" as used herein, refer to any medium that participates in providing data that causes a machine to operate in a specific fashion.  In some implementations implemented using the\ncomputer system 1900, various computer-readable media might be involved in providing instructions/code to processor(s) 1904 for execution and/or might be used to store and/or carry such instructions/code (e.g., as signals).  In many implementations, a\ncomputer-readable medium may be a physical and/or tangible storage medium.  Such a medium may take many forms, including but not limited to, non-volatile media, volatile media, and transmission media.  Non-volatile media include, for example, optical\nand/or magnetic disks, such as the storage device(s) 1906.  Volatile media include, without limitation, dynamic memory, such as the working memory 1920.  Transmission media include, without limitation, coaxial cables, copper wire and fiber optics,\nincluding the wires that comprise the bus 1902, as well as the various components of the communications subsystem 1912 (and/or the media by which the communications subsystem 1912 provides communication with other devices).  Hence, transmission media can\nalso take the form of waves (including without limitation radio, acoustic and/or light waves, such as those generated during radio-wave and infrared data communications).\nCommon forms of physical and/or tangible computer-readable media include, for example, a floppy disk, a flexible disk, hard disk, magnetic tape, or any other magnetic medium, a CD-ROM, any other optical medium, a RAM, a PROM, EPROM, a\nFLASH-EPROM, any other memory chip or cartridge, a carrier wave as described hereinafter, or any other medium from which a computer can read instructions and/or code.\nVarious forms of computer-readable media may be involved in carrying one or more sequences of one or more instructions to the processor(s) 1904 for execution.  Merely by way of example, the instructions may initially be carried on a magnetic\ndisk and/or optical disc of a remote computer.  A remote computer might load the instructions into its dynamic memory and send the instructions as signals over a transmission medium to be received and/or executed by the computer system 1900.  These\nsignals, which might be in the form of electromagnetic signals, acoustic signals, optical signals and/or the like, are all examples of carrier waves on which instructions can be encoded, in accordance with various implementations of the invention.\nThe communications subsystem 1912 (and/or components thereof) generally will receive the signals, and the bus 1902 then might carry the signals (and/or the data, instructions, etc. carried by the signals) to the working memory 1920, from which\nthe processor(s) 1904 retrieves and executes the instructions.  The instructions received by the working memory 1920 may optionally be stored on a non-transitory storage device either before or after execution by the processor(s) 1904.\nIt is understood that the specific order or hierarchy of steps in the processes disclosed is an illustration of exemplary approaches.  Based upon design preferences, it is understood that the specific order or hierarchy of steps in the processes\nmay be rearranged.  Further, some steps may be combined or omitted.  The accompanying method claims present elements of the various steps in a sample order, and are not meant to be limited to the specific order or hierarchy presented.\nThe previous description is provided to enable any person skilled in the art to practice the various aspects described herein.  Various modifications to these aspects will be readily apparent to those skilled in the art, and the generic\nprinciples defined herein may be applied to other aspects.  Moreover, nothing disclosed herein is intended to be dedicated to the public.\nWhile some examples of methods and systems herein are described in terms of software executing on various machines, the methods and systems may also be implemented as specifically-configured hardware, such as field-programmable gate array (FPGA)\nspecifically to execute the various methods.  For example, examples can be implemented in digital electronic circuitry, or in computer hardware, firmware, software, or in a combination thereof.  In one example, a device may include a processor or\nprocessors.  The processor comprises a computer-readable medium, such as a random access memory (RAM) coupled to the processor.  The processor executes computer-executable program instructions stored in memory, such as executing one or more computer\nprograms.  Such processors may comprise a microprocessor, a digital signal processor (DSP), an application-specific integrated circuit (ASIC), field programmable gate arrays (FPGAs), and state machines.  Such processors may further comprise programmable\nelectronic devices such as PLCs, programmable interrupt controllers (PICs), programmable logic devices (PLDs), programmable read-only memories (PROMs), electronically programmable read-only memories (EPROMs or EEPROMs), or other similar devices.\nSuch processors may comprise, or may be in communication with, media, for example computer-readable storage media, that may store instructions that, when executed by the processor, can cause the processor to perform the steps described herein as\ncarried out, or assisted, by a processor.  Examples of computer-readable media may include, but are not limited to, an electronic, optical, magnetic, or other storage device capable of providing a processor, such as the processor in a web server, with\ncomputer-readable instructions.  Other examples of media comprise, but are not limited to, a floppy disk, CD-ROM, magnetic disk, memory chip, ROM, RAM, ASIC, configured processor, all optical media, all magnetic tape or other magnetic media, or any other\nmedium from which a computer processor can read.  The processor, and the processing, described may be in one or more structures, and may be dispersed through one or more structures.  The processor may comprise code for carrying out one or more of the\nmethods (or parts of methods) described herein.\nThe foregoing description, for purposes of explanation, used specific nomenclature to provide a thorough understanding of the described embodiments.  However, it will be apparent to one skilled in the art that the specific details are not\nrequired in order to practice the described embodiments.  Thus, the foregoing descriptions of specific embodiments are presented for purposes of illustration and description.  They are not intended to be exhaustive or to limit the described embodiments\nto the precise forms disclosed.  It will be apparent to one of ordinary skill in the art that many modifications and variations are possible in view of the above teachings.\nThe foregoing description of some examples has been presented only for the purpose of illustration and description and is not intended to be exhaustive or to limit the disclosure to the precise forms disclosed.  Numerous modifications and\nadaptations thereof will be apparent to those skilled in the art without departing from the spirit and scope of the disclosure.\nReference herein to an example or implementation means that a particular feature, structure, operation, or other characteristic described in connection with the example may be included in at least one implementation of the disclosure.  The\ndisclosure is not restricted to the particular examples or implementations described as such.  The appearance of the phrases \"in one example,\" \"in an example,\" \"in one implementation,\" or \"in an implementation,\" or variations of the same in various\nplaces in the specification does not necessarily refer to the same example or implementation.  Any particular feature, structure, operation, or other characteristic described in this specification in relation to one example or implementation may be\ncombined with other features, structures, operations, or other characteristics described in respect of any other example or implementation.\nUse herein of the word \"or\" is intended to cover inclusive and exclusive OR conditions.  In other words, A or B or C includes any or all of the following alternative combinations as appropriate for a particular usage: A alone; B alone; C alone;\nA and B only; A and C only; B and C only; and A and B and C.", "application_number": "15642303", "abstract": " Methods, apparatuses, computer-readable medium, and systems are disclosed\n     for performing automatic exposure control (AEC). In one embodiment, a\n     first digital image is captured while applying a first set of values to\n     one or more exposure control parameters. At least one computer vision\n     (CV) operation is performed using image data from the first digital\n     image, thereby generating a first set of CV features from a set of\n     possible CV features. A mask is obtained comprising a value for each\n     feature of the set of possible CV features. Using the mask, a first\n     measure of abundance is obtained of relevant CV features among the first\n     set of CV features extracted from the first digital image. Based on the\n     first measure of abundance of relevant CV features, an updated set of\n     values is generated for applying to the one or more exposure control\n     parameters for capturing a subsequent digital image of the scene.\n", "citations": ["20080024616", "20090021602", "20090136152", "20100165178", "20100201848", "20110096085", "20110211732", "20120081579", "20120212642", "20140003663", "20140176789", "20140184852", "20140218559", "20140347511", "20150199559", "20160092735", "20160094814", "20170064211"], "related": ["62359624"]}, {"id": "20180063163", "patent_code": "10375143", "patent_name": "Learning indicators of compromise with hierarchical models", "year": "2019", "inventor_and_country_data": " Inventors: \nPevny; Tomas (Praha-Modrany, CZ), Somol; Petr (Marianske Lazne, CZ)  ", "description": "<BR><BR>TECHNICAL FIELD\nThe present disclosure relates to network security, and more particularly to the use of neural networks to identify indicators of compromise (IOCs) in connection with discovery of malware.\n<BR><BR>BACKGROUND\nEnterprise networks can easily become infected with viruses and malware, particularly as the types and number of applications proliferate over the Internet.  Keeping track of and preventing viruses and malware has, accordingly, become\nincreasingly difficult.\nTraditionally, signature-based security devices, firewalls, or anti-viruses are deployed to detect such threats.  However, signature-based algorithms simply compare a byte sequence that has been detected to stored byte-sequences corresponding to\nknown threats, which may be in a database.  Thus, if a new threat has not yet been analyzed and recorded into the database, the signature based algorithm may not identify the new threat.  Furthermore, if a threat has the ability to change, the\nsignature-based algorithms may again fail to identify the threat because a current signature of the threat may be different from a stored signature of the same threat that was recorded earlier.  Thus, polymorphic malware, zero-day attacks by threats that\nare novel or previously unseen, or other types of advanced persistent network threats are usually not detected or blocked by signature-based security algorithms. <BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nFIG. 1 illustrates an example operating environment in which an embodiment may be implemented.\nFIG. 2 is a representation of traffic flowing between computers within an enterprise and multiple external domains, with individual flows organized as bags in accordance with an example embodiment.\nFIG. 3 depicts the aggregation of traffic through a neural network in accordance with an example embodiment.\nFIG. 4 is a flowchart depicting a series of operations in accordance with an example embodiment.\nFIG. 5 is a block diagram that illustrates a computer system or apparatus upon which an embodiment of the disclosed malware detection system may be implemented.\n<BR><BR>DESCRIPTION OF EXAMPLE EMBODIMENTS\nOverview\nPresented herein are techniques for classifying devices as being infected with malware based on learned indicators of compromise.  A method includes receiving at a security analysis device, traffic flows from a plurality of entities destined for\na plurality of users, aggregating the traffic flows into discrete bags of traffic, wherein the bags of traffic comprise a plurality of flows of traffic for a given user over a predetermined period of time, extracting features from the bags of traffic and\naggregating the features into per-flow feature vectors, aggregating the per-flow feature vectors into per-destination domain aggregated vectors, combining the per-destination-domain aggregated vectors into a per-user aggregated vector, and classifying a\ncomputing device used by a given user as infected with malware when indicators of compromise detected in the bags of traffic indicate that the per-user aggregated vector for the given user includes suspicious features among the extracted features.\nAn apparatus is also presented and includes a network interface unit configured to enable communications via a network, a memory configured to store logic instructions, and a processor, when executing the logic instructions, configured to\nreceive traffic flows from a plurality of entities destined for a plurality of users, aggregate the traffic flows into discrete bags of traffic, wherein the bags of traffic comprise a plurality of flows of traffic for a given user over a predetermined\nperiod of time, extract features from the bags of traffic and aggregating the features into per-flow feature vectors, aggregate the per-flow feature vectors into per-destination domain aggregated vectors, combine the per-destination-domain aggregated\nvectors into a per-user aggregated vector, and classify a computing device used by a given user as infected with malware when indicators of compromise detected in the bags of traffic indicate that the per-user aggregated vector for the given user\nincludes suspicious features among the extracted features.\nExample Embodiments\nA problem preventing a wider use of machine learning in network intrusion detection is the difficulty of obtaining accurate labels on the level of individual network connections (Terminal Control Protocol (TCP) flow, Hypertext Transfer Protocol\n(HTTP) request, etc.).  Even for an experienced security officer it is almost impossible to determine which network connections are caused by malware and which by a benign user or an application.  Moreover, malware often exhibits itself by performing\nseemingly innocent connections.  For example, it might connect to google.com to verify if the computer is connected to the network (connection check), or it may display advertisements to render money for malware authors, etc. These problems in obtaining\ntrue labels on level of individual connections makes automatic and large-scale training of accurate classifiers for a given type of network traffic very difficult.\nTo address the foregoing, described herein is a hierarchical classifier where lower layers detect a type of traffic typical for the malware, and upper layers learn that certain combinations of the traffic are very typical for infected hosts. \nThe advantage of this hierarchy is that accuracy of classifiers on lower layers can be relatively low, but their combination performed in upper layers tends to make the overall classifier (detector) very accurate.  Further, in an embodiment, the\nclassifier may be trained with labels provided on the level of user, i.e., it can be determined that a given computer is clean, or another one is infected, all while also determining which connections were more likely caused by malware and which ones\nwere more likely caused by a user or a legitimate system process.\nMore specifically, the embodiments described herein train a Machine Learning (ML) system (e.g., a neural network) based on collectable data from an arbitrarily large computer network where the human analyst input merely includes verdicts on a\nnetwork node level--i.e., an analyst identifies nodes (corresponding to users) that are likely infected, without providing any further detail.  The instant embodiments analyze traffic logs, sys logs and possibly other information, and discover patterns\nthat are significant in distinguishing the suspicious nodes from the presumed benign ones.  Subsequent to this training stage, the ML system uses the discovered patterns to make verdicts about future infections in network nodes.\nFIG. 1 illustrates an example operating environment in which an embodiment may be implemented.  In FIG. 1, network traffic 152 between an enterprise 110 and external domains 120, 130 is depicted.  Domains 120, 130, may be devices such as, as\nservers (e.g., web servers, and the like) with which computers within the enterprise 110 communicate.\nThe computer system 113 in enterprise 110 may initiate communication with, e.g., computer system 123 of domain 120 through a gateway or proxy device 115 that is connected to the Internet 150.  That same traffic 152, in the form of packet flows\n154, may also be provided to a central server computer 117.\nIn an example of a network threat incident, computer system 140, also connected to Internet 150, may be a source of a network attack.  That is, computer system 140 may be configured to spoof domains 120 and 130.  Those skilled in the art will\nappreciate that domains 120 and 130 may, however, themselves, be a source of a network attack, which may have been able to penetrate enterprise 110.  Thus, the packet flows from computer system 123 (or 140) to enterprise computer system 113 may contain\nmalicious network attack packets.  In order to detect the network threat, malware detection logic 200 may be part of gateway system 115, in an embodiment.  As the traffic 152 is received by gateway system 115, the malware detection logic 200 may analyze\nthe traffic using the techniques described herein.\nIn another embodiment, the malware detection logic 200 may be hosted in a separate computer system such as central server computer 117.  In this configuration, the malware detection logic 200 may import packet flow log data or files (\"logs\") and\nanalyze them rather than receiving packet flows 154 directly from the network.  For example, the malware detection logic 200 may receive logs from gateway system 115.  Thus, using techniques described herein, the malware detection logic 200 may detect\nthe network threat incident on computer system 113 and also may trace the incident to the originating computer system 140 or 123.  In a specific implementation, malware detection logic 200 may be configured to learn indicators of compromise (IOCs) via a\nneural network and thereby determine whether a given computer system (e.g., computer system 113) is infected with malware or not.\nIn an embodiment, malware detection logic 200 is configured to learn a security breach from training data.  Notably, the number of needed labels in training data to be provided by humans is effectively much lower than usual in standard ML\ntechniques due to the label coverage of groups of samples (\"bags\") instead of single samples, i.e., groups of (groups of .  . . ) samples.  Each bag can consist of an arbitrary number of samples and thus vectors arranged with such samples may be of\narbitrary size.  This enables automatic learning from large data sets.  Malware detection logic 200 is further configured to automatically reveal to human analysts previously unknown patterns in data that signify malicious activity (i.e., automatically\ndiscover previously unknown IOCs).\nMalware detection logic 200 may be implemented in the form of a multi-layer feed-forward neural network (NN) trained by back-propagation.  The neural network according to the instant embodiments, consumes traffic data in a hierarchy of \"bags.\"\nThat is, individual samples are represented by vectors obtained by extracting numeric features from source network data (e.g., logs).  Which features are employed depends on the type of domain/network being analyzed.  Thus, for example, in the\ncase of telemetry, features may include, e.g., n-gram statistics from URL strings, bytes received, bytes sent, etc. In the case of a sandbox, captured features may include, e.g., timings of system calls, size of allocated resources, etc.\nNo individual labels (benign/malicious) need to be known for individual samples in the training set.  Only coarser labels are needed on the top-most bag level, marking bags as presumed benign or presumed malicious; here benign or malicious does\nnot mean that all samples in a bag are of the same type--it is only assumed that the distribution of samples in malicious bags is in some (unknown) way different from the distribution of samples in benign bags.\nHidden NN layers are designed to model patterns inside bags.  The hierarchy of NN layers reflects the hierarchy of bags.  Note that the number of neurons in hidden NN layers may be manually selected, keeping in mind that it should be high enough\nto cover the possible variety of patterns that can be expected to appear in input data.\nA notable feature of malware detection logic 200 is in its approach to learning.  In standard NNs every sample consists of one input vector.  As such, every neuron reads the input vector representing one sample and immediately computes and\npasses one output value to the next layer.  In the instant embodiments, each sample consists of multiple input vectors.  Every neuron processes all input vectors of one sample, computes their respective output values, and only when all input vectors from\na sample have been read the neuron aggregates all computed output values to a single output value representing the whole sample.  This is then passed to the next layer as the neuron's output.  Note that the aggregation function may be selected so as to\nenable calculation of partial derivatives with respect to its input and possibly with respect to internal parameters.  This ensures proper operation of the back-propagation operations.\n<BR><BR>User-Centric System Based on Telemetry Data\nThe ultimate goal of the instant embodiments is to achieve high accuracy of detecting users infected by malware where the detection is based on traffic logs (e.g., HTTP proxy logs, statistics provided by network devices through NetFlow protocol)\nof user's network activity.  In the tested embodiment, individual samples are HTTP(s) requests.  Lower level bags are viewed here as collections of requests from one user to one domain.  Higher level bags then represent all traffic of one user.\nFIG. 2 is a representation of traffic flowing between computers 113a, 113b, 113c within enterprise 110 and multiple external domains, with individual flows.  The individual flows from multiple domains to a given single user can be aggregated or\norganized as a bag in accordance with an example embodiment.\nReference is now made to FIG. 3, which shows how traffic 350 (e.g., traffic 152 in FIG. 1, and the individual flows shown in FIG. 2, arranged into bags by user (e.g., user_j_smith, user_s_dawn), is broken down into feature vectors 360 per flow,\nwhich are aggregated into per-destination-domain vectors 370, which are then further aggregated to obtain a per-user aggregated vector 380.\nThe object of these aggregations is to learn the difference in behavior of clean versus infected users.  Observations confirm that discovery of high-end malware improves greatly with structural and context-sensitive IOCs.  Examples of IOCs\ninclude, as indicated at the bottom of FIG. 3, \"flow active as connection check,\" \"flow representing search request,\" \"communication to this domain is mostly API based,\" \"communication to this domain has high number of connection checks,\" \"communication\nto this domain contained empty path,\" user accesses search engines through API, \" or \"user often reads mail and news,\" among other possibilities.\nThe methodology described below aims to learn IOCs automatically from labels available only on the level of users.  To create training data, an analyst need only mark each network node (user) as infected or clean, instead of tediously analyzing\nand labeling an excessive number of individual HTTP(s) requests.  The embodiments described herein then find how the users' traffic differs, and finds the common as well as distinct traffic patterns.  Thus, the approach described herein effectively\nlearns IOCs and also their weighted combination leading to accurate detection of infected users.\nIn one implementation, the neural network of the malware detection logic 200 comprises two hidden NN layers, that are supplied initial per-flow feature vectors 360 via inputs 375.\nThe first hidden NN layer, domain model, shown as 385 in FIG. 3, is intended to model types of user's traffic to domains (or destination IP addresses).  Each neuron in the layer 385 learns to recognize particular (unknown) types of connections. \nThus, neurons in the domain model 385 receive, via inputs 375, bags of connections between a single user and single domain, and output their decision, which is a single real number.  Since the domain model 385 is composed of multiple neurons, its output\nis one real vector for every domain the user has visited (i.e., for each bag entering layer 385).\nThe second hidden NN layer, user model, shown as 390 in FIG. 3, is intended to model types of users.  Its neurons learn the particular patterns in traffic that are specific for infected users.  The neurons consume the output of the domain model. Since the user has probably visited many domains, the domain model can produce a varying number of output vectors, forming a higher-level \"bag of domain connection behaviors\".  Thus, neurons in the user model 390 take such higher-level bags and output\ntheir decision which is again a single real number.\nNote that while processing samples in a bag, both the domain model 385 and user model 390 aggregate individual-sample outputs per neuron to produce a single output value.  For this function, the use of a maximum or average aggregator, among\nothers, may be employed to produce or generate the single output value.\nIn one implementation, input vectors are representative of individual HTTP(s) requests.  Referring to FIG. 2, the black dots represent a plurality of HTTP sessions, which are sampled to produce the input vector.  In one example, individual\nsamples were transformed into vectors of 550 features.  Feature examples include: 1) HTTP protocol used, 2) number of subdomains present in URL, 3) length of User-Agent string (indicates also whether the user agent is present), 4) domain popularity\n(e.g., taken from Alexa database), 5) (logarithm of the number of) bytes transferred from client to server, 6) bytes transferred from server to client, 7) presence of a specific \"xyz\" n-gram in the domain name (e.g., from all possible n-grams use those\nwith below-average frequency), 8) presence of self-signed certificate, 9) referrer for the request is available?, 10) HTTP status of the connection (e.g., connection is a redirection/success/error/ .  . . ), 11) maximum length of continuous lower case\ncharacters in URL, 12) maximum length of continuous upper case characters in URL, 13) duration of TCP connection, 14) length of URL path, 15) length of URL query, 16) status of the HTTP request, 17) client port, 18) server port, 19) is connection to raw\nIP address, among others.\nInput bags enter the first hidden layer which outputs for each bag one output vector; a collection of such vectors then enters as higher-level bag the second hidden layer to eventually produce one final output vector.  See FIG. 3, vector 380. \nThis vector is then transformed to a single value 295 that expresses the classifier decision.  Back-propagation then propagates back weight changes depending on how the classifier output corresponded to the known user-level label in the training set.\nNote that while the classifier is learned using the user-level labels and is thus learned in a supervised way, the underlying structure of neuron weights in lower-level hidden layers is effectively trained in an unsupervised way; the top-level\nlabel does affect the weight updates but does not determine the model's ability to reveal the patterns in underlying data distribution.\nA more formal description of the foregoing follows.\nAs explained, the goal is to classify users on the basis of their connections into two or more classes (in this case, just two classes: clean and infected).  This means that one sample (bag), b; consists of all network connection of one user and\nit is not known which connections within are caused by the malware and which are of the user or the operating system.\nThe set of all bags (also users or samples), is denoted by b.di-elect cons..  Set of all observed connections of all users will be denoted by x.di-elect cons.X,x.di-elect cons..sup.m, with X.sub.b being set of connections of user b; and\nx.sub.b:d being set of connections of a user b going to domain d. The set .sub.b is the set of domains contacted by the user b. Finally y.sub.b is a label of a user b (infected/clean).  In this work y.sub.b.di-elect cons.{-1, +1}\nThe essence of the approach described herein is to model the classification system in two layers.  The first layer, domain model, is intended to model traffic to domains (or destination IP addresses), d. Its detectors (neurons) within should\nrecognize particular type of connections, for example download of advertisements, the domain name in the request was generated by a domain generating algorithm (DGA), etc. The second layer, user model, is intended to model users and its detectors learn\nthat particular combinations of traffic are unique for infected users.\nBoth layers can be described by functions f.sub.d: .orgate..sub.k&gt;1.sup.k,m.fwdarw..sup.m.sup.d and f.sub.b:.orgate..sub.k&gt;1.sup.k,m.sup.d.fwdarw..sup.m.sup.b, respectively.  Since the number of user's connections and number of contacted\ndomains by user varies across users, both functions f.sub.d and f.sub.b are be configured to accept unknown number of input vectors of size .sup.m, .sup.m.sup.d respectively.  This is symbolized in the above notation by domain of functions being\nrectangular matrices with any number of rows.  The composite function f.sub.b (.orgate..sub.d.di-elect cons..sub.bf.sub.d(x.sub.b:d)) models the user by a single m.sub.d-dimensional vector.  Thus, there is one vector of fixed dimension to one label,\nwhich enables the use any machine-learning algorithms.  The challenge is, how to choose/learn functions (models) f.sub.d and f.sub.b.\nThe problem can be elegantly solved if the function composition is viewed as a neural network with aggregation functions of choice, e.g., minimum, maximum, applied in two separate layers and learning f.sub.d and f.sub.b using a backpropagation\nalgorithm.  The approach is outlined in FIG. 3.  Units (neurons) i.sub.1, .  . . , i.sub.m in the input layer 375 do not have any weights as they simply distribute the input feature vector, x.di-elect cons.  (describing one connection) to neurons in the\nnext layer 385 modeling the domains (domain models denoted as h.sub.d:1, .  . . , h.sub.d:m.sub.d).  The domain models aggregate feature vectors of all user's connections to each domain, d; and for each domain outputs a single vector x.sub.b:d.di-elect\ncons..sup.m.sup.b.  This set of vectors (one for each domain contacted by the user) is forwarded to user models (users models denoted as h.sub.b:1, .  . . , h.sub.b:m.sub.b), which aggregate them and output a single vector x.sub.b:d.di-elect\ncons..sup.m.sup.b describing the user.  This vector is used in the final classifier to provide the decision.  As noted, both layers f.sub.d, f.sub.b and the final classifier can be implemented as a multi-layer neural network.  Moreover, the model can be\nextended to have more aggregation layers, if for example there is meta-information that allows further sub-grouping of connections or connections can be broken down to simpler entities.\nExample of an Implementation\nIn an example implementation, neurons in the domain and user models layer were implemented by reluMax units as f.sub.d(x.sub.b:d)=(max{{0,max.sub.x.di-elect cons.X.sub.b:d{w.sub.d,1.sup.Tx+v.sub.d,1}}, .  . . , max{{0,max.sub.x.di-elect\ncons.X.sub.b:d{w.sub.d,m.sub.d.sup.Tx+v.sub.d,m.sub.d}}) (1) and f.sub.d(x.sub.b)=(max{{0,max.sub.d.di-elect cons.D.sub.b{w.sub.b,1.sup.Tf.sub.d(x.sub.b:d)+v.sub.b,1}}, .  . . , max{{0,max.sub.d.di-elect\ncons.D.sub.b{w.sub.b,1.sup.Tf.sub.d(x.sub.b:d)+v.sub.b,m.sub.d}}) (2)\nInner maximums max.sub.x.di-elect cons.X.sub.b:d (1) aggregate multiple connections to single domain (domain models) and max.sub.d.di-elect cons..sub.b in (2) aggregates user's connections to multiple domains.  The output classifier o; is a\nsimple linear classifier sign(w.sub.o.sup.Tf.sub.d(X.sub.b)+v.sub.o) with sign being the signum function.  The whole model is parametrized by weights w.sub.d.di-elect cons..sup.k,m.sup.d, w.sub.b.di-elect cons..sup.m.sup.d.sup.,m.sup.b, w.sub.o.di-elect\ncons..sup.m.sup.b, v.sub.d.di-elect cons..sup.m.sup.d, v.sub.b.di-elect cons..sup.m.sup.b, v.sub.o.di-elect cons.  which are optimized with a gradient calculated by a back-propagation algorithm.  The error function is a simple hinge-loss of the form\nl(y.sub.b,o.sub.b)=max{0,1-y.sub.bo.sub.b}.  The ADAM algorithm (Kingma, D. et al., \"Adam: A Method for Stochastic Optimization,\" 2015) was used with default settings and with gradient estimated from 50 randomly selected clean bags and 50 randomly\nselected infected bags.\nThe above neural network was evaluated in two scenarios in network intrusion detection differing by the source of the data.  In the first scenario the data included HTTP proxy logs obtained from a cloud web security system.  In the second\nscenario the data included NetFlow records enhanced by the information about length and timings of individual packets.\nFirst Embodiment--HTTP Proxy Logs\nOne HTTP(s) connection is described by a 287 dimensional vector developed for the classification layer of a Cognitive Threat Analytics (CTA) engine.  One sample (bag) describing user's traffic from a five-minute long observation window consists\nof a set of 287-dimensional vectors, each describing one HTTP(s) connection.  The CTA engine identified users infected by the malware using DGA algorithm, and this labeling was used as a ground truth.  Although this labeling is not perfect, the goal here\nwas to demonstrate that the instant approach can learn from labels provided on the level of bags rather than on level of individual HTTP requests.  The dataset contained 15,055,979 HTTP requests from 24,627 users out of which 1,471 were labeled as\ninfected.  Note that some bags contain as many as 50,000 requests which demonstrates the difficulty of labeling requests within.  The data were split into training and testing sets on the level of users, such that 50% of users were used for training and\nthe rest for testing.  Both domain and user models consisted of single layer of 20 reluMax neurons as described in (1) and (2).  The ADAM algorithm was allowed to run for 210.sup.4 iterations with gradient estimated from 100 randomly selected bags in\neach iteration (50 clean, 50 infected).\nAfter the training, the error was 3.2910.sup.-4 (P.sub.FP=3.2910.sup.-4, P.sub.FN=0.0) on the training set and 0.0048 (P.sub.FP=0.0021, P.sub.FN=0.0506) on the testing set.  The fact that the described embodiment achieved nearly zero error on\nthe training set and optimistic 0.2% error on the testing set demonstrates that it has learned to identify infected users.  Moreover, notice that the false positive rate on the testing set is about 0.1% which is very favorable.\nRecall that one of the goals of the instant embodiments is to learn the type of traffic typical for malware that indicate infected users (indicators of compromise, IOC).  The learnt IOCs can be very weak if used independently of each other, but\ntheir combination enabled by user model layer yields a very strong classifier, since the error probability is less than 0.5%.\nIn this regard, investigation was undertaken related to the types of HTTP connections to which neurons in the domain model layer are the most sensitive.  The sensitivity of i.sup.th-neuron to HTTP request with feature vector x was measured as\nmax{0,w.sub.d,i.sup.Tx+v.sub.d,i}.  By looking at connections with the highest score, it was possible to identify which connections the neuron recognizes.  Listed below are several connections of interest together with the assumed type of learnt traffic. HTTPs connections to raw IP addresses like hxxps://62.249.33.21/ DGA domains like hxxp://ivdyxgqtwqsztopjrijlnhqwcnbtk.com, hxxp://pojxofukqskfhajvizdhmdxwwghq.biz, and hxxp://twwkgihmmvspblrnzpnjnhexcqgtkrk.com HTTPs connections to live.com domain like\nhxxps://roaming.officeapps.live.com/[ow] Download of images like http://www.biglots.com/images/aprimo/common/holiday_header/110714-04.gif Seemingly legitimate traffic like hxxp://banners.itunes.apple.com/js/banner-main-built.j s or\nhxxp://www.slfn.co.uk/today_matchsheet.php.\nThe first two types are well known to be related to malware.  The third one is interesting, since live.com is a legitimate domain belonging to Microsoft.  Nevertheless, search on malware blogs revealed that it is indeed used by malware.  The\nlast two types seem to be related to advertisements, which would suggest that the system has learnt that advertisement is a sign of malware infection.  This also makes sense as the malware monetize infections by doing click-fraud.  The learnt indicators\nof compromise seem to make sense, but they are not the type security researchers would create manually, because using them individually would lead to very high false positive rate.  Notably, the system learned DGA indicator of compromise without knowing\nwhat it is, which demonstrates its practical utility.\nSecond Embodiment--NetFlow Records\nEnhanced Threat Telemetry and Analytics (ETTA) is a project whose goal is to investigate how statistics exported in NetFlow and IPFIX about network connections can be enriched to get better visibility into the flows and to improve the accuracy\nin detecting infected users/malware.  An experimental database was created by using traffic of few users captured during ETTA.  This traffic was used as a background (considered clean), since all computers came from a protected network.  The bags were\ncreated by grouping the flows by an IP address, which means that bags contained mixture of users and servers.  Infected users were simulated by mixing the traffic captured during 5-minute long execution of the malware within ThreatGRID sandbox.  The\nmixing was implemented by changing the IP address, which was sufficient.  Note that in this case, we are effectively trying to detect an event, when user has just downloaded the malicious payload which is executed.  As in the previous case, the time\nwindow of one bag was set to five minutes, which is also the length of malware's execution in ThreatGRID sandbox.  A notable difference to experiment with HTTP logs is the replacement of domains by destination IP addresses, since not all flows were HTTP\nconnection.  The training set contained traffic from 6132 users collected during 11 days (with a 14 day break after the fifth day), which after division into batches yielded to 2,132,446 samples.  Thus, the training set contained traffic from five days,\nand the testing set contained traffic from six days, such that the time-difference between both sets was 14 days.  To simulate infected users, traffic collected in sandbox was mixed into randomly selected users.  Samples of 237,100 binaries were taken\nfrom the sandbox.\nEach connection was described by an 80-dimensional vector, consisting of sizes and inter-packet times of first twenty incoming and first twenty outgoing packets.  These features are very simple and used as is, without any other processing except\nnormalizing by log(1+x) to reduce their dynamic range.  All settings as the configuration of the neural network, the training algorithm, its number of allowed connections, etc. were the same as in the previous HTTP case.\nThe error of the classifier was 0.0093 (P.sub.FP=0.0061, P.sub.FN=0.0124) on the training set and 0.0111 (P.sub.FP=0.0083, P.sub.FN=0.0139) on the testing set.  As in the previous case, investigation was made as to the types of connections the\ndomain (destination IP) models have learned, though this was more difficult, since there was no information about the URLs.  Nevertheless, it could be surmised according to the information about destination IP address and if the flow came from the\nsandbox or from a user.  Based on this information, neurons have been found to be sensitive to connection checks or pings, with only one incoming and outgoing packet typically to google servers, to connections to sinkhole server located to Poland, but\nalso neurons sensitive web conferencing (this is due to the nature of our background data).  These again demonstrate that the neurons in the domain (destination IP) modeling layer learn useful types of traffic, albeit not being always individually usable\nIOC.\nThird Embodiment--Domain-centric System Based on Telemetry Data\nThe first embodiment assumed availability of user-level labels.  A \"shallower\" system can be defined with only one hidden NN layer, taking use of domain-centric labels, e.g., from blacklists.  Such a neural network may be trained only from bags\nrepresenting user-domain connections.  The resulting IOCs would represent patterns extracted from traffic per domain.\nFourth Embodiment--Binary-hash-centric System Based on Sandbox Data\nA version of the approach from the previously described embodiments can be modified for analyzing behaviors of executable files.  The training data would come from a sandbox or any other network sensor capable attaching a hash of the process to\neach log entry (e.g., ThreatGRID or Advanced Malware Protection (AMP)), covering logs of respective executable's system calls, network traffic, caused changes to file system, etc. Labels are available per executable (hash), highest level bags would cover\ncomplete executable activity capture.  Lower-level bags can be certainly defined taking use of inherent data structure (activity time windows, sys calls of various types, structuring of file system access according to destination, etc.).\nThe result would be a classification system as well as IOCs describing in previously unknown detail the inner patterns in executable activities.\n<BR><BR>Other Possible Implementation and Enhancements\nMore Descriptive Power by Extended Aggregators\nIn the first embodiment all neurons employed the same aggregator function: maximum.  The intuition is to let the system emphasize such individual detailed pattern(s) in the bag traffic that have the most striking effect on overall efficacy.  It\nis fast and well interpretable.  The maximum function, however, does not take into the account the volume of the type of the IOCs, which can be undesirable (e.g., IOC of showing advertisement).\nIf maximum function is replaced by average, the system would put emphasis on less detail but more on prevailing characteristics over whole bags.  This would improve robustness against noise but reduce the attention to detail--will not trigger on\nspikes in traffic.  Replacing maximum by average in all neurons can lead to loss of detection power according to experiments.\nHowever, there are other ways to modify neural networks where one or multiple of the following are implemented to enrich the model's expressive power:\nIn one approach, the aggregator function is defined as a parametrized generic function where the parameter gets optimized as part of network learning.  A good example is q-th root of a sum of q-th powers.  Higher q moves the function closer to\nmaximum, lower q&gt;1 moves it closer to average.\nIn another approach, multiple aggregator functions, maximum, average and/or the generic aggregator are performed in parallel, increasing the number of neurons per layer.\nDeepening the Neural Network to Learn a Hierarchy of IOCs\nIn the foregoing embodiments, a small number of NN layers are implemented, and can be described as \"shallow\".  They partially emulate the power of deep networks by modeling the structure in bags; however, the hierarchy of bags is pre-specified,\ne.g., flows over a predetermined amount of time.  The power of deep neural networks in image analysis consists in the ability to model arbitrarily deep hierarchies of patterns in data.  The embodiments described herein can also be extended to allow\nautomatic learning of hierarchy depth as well.\nSpecifically, an equivalent to the \"convolution kernel\" trick from classical Deep Learning is employed, where a parameterized function reflects context in data to various extent while the learning algorithms optimizes the parameter.  This is\nmore difficult in network security than in image analysis due to less inherent structure in data--unlike the regular matrix of pixels in images the network data are highly irregular and diverse.\nHowever, a \"convolution\" can be defined over network events in time.  It is known that context in traffic matters (e.g., if certain signals are preceded by connection check, their maliciousness probability is higher).  Hence it is possible to\ndefine time context windows of parameterized size (the window would aggregate information from network events within its reach), allowing for both automated window parameter optimization and hierarchical stacking of such windows in varying levels of\ncoarseness.\nA difference from the first embodiment is in the definition of what is a bag and consequently how NN layers are constructed.  In the first embodiment bags have pre-specified types, i.e., humans have defined how to group samples to bags, and the\nNN has consequently a corresponding number of levels.  In contrast, a deep architecture can decide itself as part of the optimization process.  Hence a parameterized definition of a bag is employed with optimizable parameters.  The time window is a\nviable installment of parameterized bag.  The number of NN layers would then be set significantly higher to let the optimizer assign weights to neurons as needed for the accuracy of the model.\nCorrelating Multiple Types of Data for More Complex IOC Discovery\nIn the foregoing embodiments it is assumed that the network is trained on a single type of data.  Experience confirms that correlating various sources of data often leads to better efficacy.\nThus, the embodiments described herein can be extended to build joint models over multiple types of data.  This can be achieved in multiple ways, including: connecting multiple input vectors to the first network layer; or using supplemental data\nto define bags in primary data.\nReference is now made to FIG. 4, which is a flowchart depicting a series of operations in accordance with an example embodiment.  These operations may be performed by malware detection logic 200 that is loaded on, e.g., gateway 115 or separate\nserver 117, either of which can function as a security analysis device.  The operations include, at 410, receiving at a security analysis device, traffic flows from a plurality of entities destined for a plurality of users, at 412, aggregating the\ntraffic flows into discrete bags of traffic, wherein the bags of traffic comprise a plurality of flows of traffic for a given user over a predetermined period of time, at 414, extracting features from the bags of traffic and aggregating the features into\nper-flow feature vectors, at 416, aggregating the per-flow feature vectors into per-destination domain aggregated vectors, at 418, combining the per-destination-domain aggregated vectors into a per-user aggregated vector, and, at 420, classifying a\ncomputing device used by the given user as infected with malware when indicators of compromise detected in the bags of traffic indicate that the per-user aggregated vector includes suspicious features among the extracted features\nIn sum, described is a hierarchical Neural Network system capable of detecting network threats.  The system has unique practical properties.  I performs as high-precision detector of threats.  It is learned from training data with dramatically\nlower requirements on labels provided by human analysts--this makes it practical for Big Data analytics.  It is capable of learning and revealing novel Indicators of Compromise (IOCs) that are unlikely to be discovered by human analysts.  The system can\nbe implemented to consume various types of data.  An important feature underlying the ability to accomplish the foregoing, is to treat data samples in a hierarchy of groups (\"bags\"); individual hidden Neural Network layers then represent the respective\nbag granularity and learn otherwise invisible structures inside bags on the given level of granularity.  Only the last output layer uses labels.\nThe methodology described herein is capable of learning from Big Data due to its ability to learn from very small number of labels describing very grossly a large amount of training data.  This removes a significant bottleneck caused by cost of\nhuman analysis.\nThe methodology further learns to classify entities on various levels of abstraction; in the tested installment it provides verdicts about users (infected/clean); the verdict can be analyzed and explained.\nThe methodology also automatically learns Indicators of Compromise (IOCs) that would otherwise be difficult or impossible to define by human analysts.\nThe methodology is general, and has been verified on two types of input data.  In both cases its efficacy was proven.\nFinally, the methodology has the potential to increase accuracy of many products classifying users based on observed network connection.  In other embodiments, it has the potential to improve efficacy of sandbox solutions as well as boost the\nefficacy of security products as a whole when utilizing correlations between data of various type.\nFIG. 5 depicts an apparatus that is configured to operate as a security analysis device or apparatus that hosts malware detection logic 200 according to an example embodiment.  The apparatus may be implemented on a computer system 501.  The\ncomputer system 501 may be programmed to implement a computer based device.  The computer system 501 includes a bus 502 or other communication mechanism for communicating information, and a processor 503 coupled with the bus 502 for processing the\ninformation.  While the figure shows a single block 503 for a processor, it should be understood that the processor 503 represents a plurality of processors or processing cores, each of which can perform separate processing.  The computer system 501 may\nalso include a main memory 504, such as a random access memory (RAM) or other dynamic storage device (e.g., dynamic RAM (DRAM), static RAM (SRAM), and synchronous DRAM (SD RAM)), coupled to the bus 502 for storing information and instructions to be\nexecuted by processor 503.  In addition, the main memory 504 may be used for storing temporary variables or other intermediate information during the execution of instructions by the processor 503.  Main memory may also be used to store logic\ninstructions or software for performing the operations shown in FIG. 4.\nThe computer system 501 may further include a read only memory (ROM) 505 or other static storage device (e.g., programmable ROM (PROM), erasable PROM (EPROM), and electrically erasable PROM (EEPROM)) coupled to the bus 502 for storing static\ninformation and instructions for the processor 503.\nThe computer system 501 may also include a disk controller 506 coupled to the bus 502 to control one or more storage devices for storing information and instructions, such as a magnetic hard disk 507, and a removable media drive 508 (e.g.,\nfloppy disk drive, read-only compact disc drive, read/write compact disc drive, compact disc jukebox, tape drive, and removable magneto-optical drive).  The storage devices may be added to the computer system 701 using an appropriate device interface\n(e.g., small computer system interface (SCSI), integrated device electronics (IDE), enhanced-IDE (E-IDE), direct memory access (DMA), or ultra-DMA).\nThe computer system 501 may also include special purpose logic devices (e.g., application specific integrated circuits (ASICs)) or configurable logic devices (e.g., simple programmable logic devices (SPLDs), complex programmable logic devices\n(CPLDs), and field programmable gate arrays (FPGAs)), that, in addition to microprocessors and digital signal processors may individually, or collectively, are types of processing circuitry.  The processing circuitry may be located in one device or\ndistributed across multiple devices.\nThe computer system 501 may also include a display controller 509 coupled to the bus 502 to control a display 510, such as a cathode ray tube (CRT) or liquid crystal display (LCD), for displaying information to a computer user.  The computer\nsystem 501 may include input devices, such as a keyboard 511 and a pointing device 512, for interacting with a computer user and providing information to the processor 503.  The pointing device 512, for example, may be a mouse, a trackball, or a pointing\nstick for communicating direction information and command selections to the processor 503 and for controlling cursor movement on the display 510.  In addition, a printer may provide printed listings of data stored and/or generated by the computer system\n501.\nThe computer system 501 performs a portion or all of the processing operations of the embodiments described herein in response to the processor 503 executing one or more sequences of one or more instructions contained in a memory, such as the\nmain memory 504.  Such instructions may be read into the main memory 504 from another computer readable medium, such as a hard disk 507 or a removable media drive 508.  One or more processors in a multi-processing arrangement may also be employed to\nexecute the sequences of instructions contained in main memory 504.  In alternative embodiments, hard-wired circuitry may be used in place of or in combination with software instructions.  Thus, embodiments are not limited to any specific combination of\nhardware circuitry and software.\nAs stated above, the computer system 501 includes at least one computer readable medium or memory for holding instructions programmed according to the embodiments presented, for containing data structures, tables, records, or other data\ndescribed herein.  Examples of computer readable media are compact discs, hard disks, floppy disks, tape, magneto-optical disks, PROMs (EPROM, EEPROM, flash EPROM), DRAM, SRAM, SD RAM, or any other magnetic medium, compact discs (e.g., CD-ROM), or any\nother optical medium, punch cards, paper tape, or other physical medium with patterns of holes, or any other medium from which a computer can read.\nStored on any one or on a combination of non-transitory computer readable storage media, embodiments presented herein include software for controlling the computer system 501, for driving a device or devices for implementing the described\nembodiments, and for enabling the computer system 501 to interact with a human user (e.g., print production personnel).  Such software may include, but is not limited to, device drivers, operating systems, development tools, and applications software. \nSuch computer readable storage media further includes a computer program product for performing all or a portion (if processing is distributed) of the processing presented herein.\nThe computer code may be any interpretable or executable code mechanism, including but not limited to scripts, interpretable programs, dynamic link libraries (DLLs), Java classes, and complete executable programs.  Moreover, parts of the\nprocessing may be distributed for better performance, reliability, and/or cost.\nThe computer system 501 also includes a communication interface 513 coupled to the bus 502.  The communication interface 513 provides a two-way data communication coupling to a network link 514 that is connected to, for example, a local area\nnetwork (LAN) 515, or to another communications network 516.  For example, the communication interface 513 may be a wired or wireless network interface card or modem (e.g., with SIM card) configured to attach to any packet switched (wired or wireless)\nLAN or WWAN.  As another example, the communication interface 513 may be an asymmetrical digital subscriber line (ADSL) card, an integrated services digital network (ISDN) card or a modem to provide a data communication connection to a corresponding type\nof communications line.  Wireless links may also be implemented.  In any such implementation, the communication interface 513 sends and receives electrical, electromagnetic or optical signals that carry digital data streams representing various types of\ninformation.\nThe network link 514 typically provides data communication through one or more networks to other data devices.  For example, the network link 514 may provide a connection to another computer through a local are network 515 (e.g., a LAN) or\nthrough equipment operated by a service provider, which provides communication services through a communications network 516.  The local network 514 and the communications network 516 use, for example, electrical, electromagnetic, or optical signals that\ncarry digital data streams, and the associated physical layer (e.g., CAT 5 cable, coaxial cable, optical fiber, etc.).  The signals through the various networks and the signals on the network link 514 and through the communication interface 513, which\ncarry the digital data to and from the computer system 501 may be implemented in baseband signals, or carrier wave based signals.  The baseband signals convey the digital data as unmodulated electrical pulses that are descriptive of a stream of digital\ndata bits, where the term \"bits\" is to be construed broadly to mean symbol, where each symbol conveys at least one or more information bits.  The digital data may also be used to modulate a carrier wave, such as with amplitude, phase and/or frequency\nshift keyed signals that are propagated over a conductive media, or transmitted as electromagnetic waves through a propagation medium.  Thus, the digital data may be sent as unmodulated baseband data through a \"wired\" communication channel and/or sent\nwithin a predetermined frequency band, different than baseband, by modulating a carrier wave.  The computer system 501 can transmit and receive data, including program code, through the network(s) 515 and 516, the network link 514 and the communication\ninterface 513.  Moreover, the network link 514 may provide a connection to a mobile device 517 such as a personal digital assistant (PDA) laptop computer, cellular telephone, or modem and SIM card integrated with a given device.\nIn summary, in one form, a method is provided comprising: receiving, at a security analysis device, traffic flows from a plurality of entities destined for a plurality of users; aggregating the traffic flows into discrete bags of traffic,\nwherein the bags of traffic comprise a plurality of flows of traffic for a given user over a predetermined period of time; extracting features from the bags of traffic and aggregating the features into per-flow feature vectors; aggregating the per-flow\nfeature vectors into per-destination domain aggregated vectors; combining the per-destination-domain aggregated vectors into a per-user aggregated vector; and classifying a computing device used by a given user as infected with malware when indicators of\ncompromise detected in the bags of traffic indicate that the per-user aggregated vector for the given user includes suspicious features among the extracted features.\nIn another form, an apparatus is provided comprising: a network interface unit configured to enable communications via a network; a memory configured to store logic instructions; and a processor, when executing the logic instructions, configured\nto receive traffic flows from a plurality of entities destined for a plurality of users; aggregate the traffic flows into discrete bags of traffic, wherein the bags of traffic comprise a plurality of flows of traffic for a given user over a predetermined\nperiod of time; extract features from the bags of traffic and aggregating the features into per-flow feature vectors; aggregate the per-flow feature vectors into per-destination domain aggregated vectors; combine the per-destination-domain aggregated\nvectors into a per-user aggregated vector; and classify a computing device used by a given user as infected with malware when indicators of compromise detected in the bags of traffic indicate that the per-user aggregated vector for the given user\nincludes suspicious features among the extracted features.\nIn still another form, one or more non-transitory computer readable storage media are provided encoded with software comprising computer executable instructions and when the software is executed operable to: receive traffic flows from a\nplurality of entities destined for a plurality of users; aggregate the traffic flows into discrete bags of traffic, wherein the bags of traffic comprise a plurality of flows of traffic for a given user over a predetermined period of time; extract\nfeatures from the bags of traffic and aggregating the features into per-flow feature vectors; aggregate the per-flow feature vectors into per-destination domain aggregated vectors; combine the per-destination-domain aggregated vectors into a per-user\naggregated vector; and classify a computing device used by a given user as infected with malware when indicators of compromise detected in the bags of traffic indicate that the per-user aggregated vector for the given user includes suspicious features\namong the extracted features.\nThe above description is intended by way of example only.  Various modifications and structural changes may be made therein without departing from the scope of the concepts described herein and within the scope and range of equivalents of the\nclaims.", "application_number": "15248252", "abstract": " Presented herein are techniques for classifying devices as being infected\n     with malware based on learned indicators of compromise. A method includes\n     receiving at a security analysis device, traffic flows from a plurality\n     of entities destined for a plurality of users, aggregating the traffic\n     flows into discrete bags of traffic, wherein the bags of traffic comprise\n     a plurality of flows of traffic for a given user over a predetermined\n     period of time, extracting features from the bags of traffic and\n     aggregating the features into per-flow feature vectors, aggregating the\n     per-flow feature vectors into per-destination domain aggregated vectors,\n     combining the per-destination-domain aggregated vectors into a per-user\n     aggregated vector, and classifying a computing device used by a given\n     user as infected with malware when indicators of compromise detected in\n     the bags of traffic indicate that the per-user aggregated vector for the\n     given user includes suspicious features among the extracted features.\n", "citations": ["7296288", "8555383", "20100284283", "20120210423", "20140280898", "20170099314"], "related": []}, {"id": "20180077483", "patent_code": "10375465", "patent_name": "System and method for alerting a user of preference-based external sounds\n     when listening to audio through headphones", "year": "2019", "inventor_and_country_data": " Inventors: \nSahay; Pratyush (Karnataka, IN)  ", "description": "<BR><BR>TECHNICAL FIELD\nAspects disclosed herein generally relate to a system and method for alerting a user of preference-based external sounds (or user-defined audio preferences) when listening to audio through headphones.  These aspects and others will be discussed\nin more detail herein.\n<BR><BR>BACKGROUND\nCurrent headphones may be considered to be \"blind systems.\" For example, when a listener listens to audio above a certain volume that is played back through the headphones, the user is generally not able to hear any external or ambient sound\nirrespective of the type and importance of the external sound that is present in the environment-context (i.e., user's current environment--public transport, shopping mall, etc.)/social-context (i.e., type of callouts may vary depending upon the user's\ncurrent role--office, home, etc.) in which the user is situated during audio playback.\nThere may be certain external sounds such as announcements that are transmitted as ambient sounds such as in public places, or during emergency scenarios or call-outs from certain individuals that the user may be interested in hearing even when\nexperiencing media through the headphones.  Further, the preference of \"the external sound of interest\" may vary from person to person based on that respective person's (i) preference, (ii) environmental-context, or (iii) social-context.  Alerting the\nuser of the presence of such preferred external sounds may not be possible when listening to media through current headphones.\n<BR><BR>SUMMARY\nA computer-program product embodied in a non-transitory computer readable medium that is programmed to communicate with a listener of headphones is provided.  The computer-program product includes instructions to receive ambient noise indicative\nof external noise to a listener's headphones and to extract a speech component from the ambient noise.  The computer-program product further includes instructions to derive an intent from the speech component of the ambient noise and compare the intent\nto at least one user defined preference.  The computer-program product further includes instructions to transmit an alert to notify a listener that the intent of the speech component matches the at least one user defined preference.\nAn apparatus for alerting a listener of an external sound of interest while listening to headphones is provided.  The apparatus includes a server programmed to receive ambient noise indicative of external noise to headphones from a mobile device\nand to extract a speech component from the ambient noise.  The server is programmed to derive an intent from the speech component of the ambient noise and to compare the intent to at least one user defined preference.  The server is programmed to\ntransmit an alert to notify a listener that the intent of the speech component matches the at least one user defined preference.\nAn apparatus for alerting a listener of an external sound of interest while listening to headphones.  The apparatus includes a mobile device programmed to transmit ambient noise indicative of external noise to headphones to a server and to\nreceive an alert from the server in response to extracted speech on the ambient noise providing an intent that matches at least one user defined preference.  The mobile device is programmed to audibly or visually notify a user of the headphones that the\nintent of the extracted speech matches the at least one user defined preference in response to the alert. <BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nThe embodiments of the present disclosure are pointed out with particularity in the appended claims.  However, other features of the various embodiments will become more apparent and will be best understood by referring to the following detailed\ndescription in conjunction with the accompanying drawings in which:\nFIG. 1 generally depicts an example of active noise-cancelling headphones that may capture external sounds of interest and other aspects disclosed in connection with the system as illustrated in FIG. 3;\nFIG. 2 generally depicts a plot including waveforms utilized in connection with the active noise-cancelling headphone of FIG. 1;\nFIG. 3 depicts a system for alerting a user of preference-based external sound(s) when listening to audio;\nFIG. 4 generally depicts a more detailed implementation of the headphones and the mobile device in accordance to one embodiment;\nFIG. 5 generally depicts a method for alerting a user of preference-based external sounds when listening to audio through headphones in accordance to one embodiment; and\nFIG. 6 generally depicts a method for generating a user preference model based on user aspects in accordance to one embodiment.\n<BR><BR>DETAILED DESCRIPTION\nAs required, detailed embodiments of the present invention are disclosed herein; however, it is to be understood that the disclosed embodiments are merely exemplary of the invention that may be embodied in various and alternative forms.  The\nfigures are not necessarily to scale; some features may be exaggerated or minimized to show details of particular components.  Therefore, specific structural and functional details disclosed herein are not to be interpreted as limiting, but merely as a\nrepresentative basis for teaching one skilled in the art to variously employ the present invention.\nThe embodiments of the present disclosure generally provide for a plurality of circuits or other electrical devices.  All references to the circuits and other electrical devices and the functionality provided by each are not intended to be\nlimited to encompassing only what is illustrated and described herein.  While particular labels may be assigned to the various circuits or other electrical devices disclosed, such labels are not intended to limit the scope of operation for the circuits\nand the other electrical devices.  Such circuits and other electrical devices may be combined with each other and/or separated in any manner based on the particular type of electrical implementation that is desired.  It is recognized that any circuit or\nother electrical device disclosed herein may include any number of microcontrollers, a graphics processor unit (GPU), integrated circuits, memory devices (e.g., FLASH, random access memory (RAM), read only memory (ROM), electrically programmable read\nonly memory (EPROM), electrically erasable programmable read only memory (EEPROM), or other suitable variants thereof) and software which co-act with one another to perform operation(s) disclosed herein.  In addition, any one or more of the electrical\ndevices may be configured to execute a computer-program that is embodied in a non-transitory computer readable medium programmed to perform any number of the functions as disclosed.\nAspects disclosed herein provide for a system that captures an \"external sound of interest\" that is preferred by a particular user when listening to media through headphones.  The system further provides an alert to the user regarding the\npreferred external sound in the user's preferred format (e.g., an audible alert or a text notification).  Further, the system incorporates a machine learning block that, among other things, learns a user-preference model regarding the various types of\n\"external sounds of interest\" that a particular user shows interest in and recommends adding a new rule based on the user's environmental-context, social-context or location.  The system may be incorporated in connection with active noise-cancelling\nheadphones.  However, the system may also be extended to any headphone that has an active microphone (e.g., which is in an `on` state while media is being streamed into the headphones) to capture the external sounds.\nFIG. 1 discloses an example of headphones 10 that may capture external sounds of interest and other aspects disclosed in connection with the system as illustrated in FIG. 3.  In one example, the headphones 10 may be implemented as active noise\ncancelling headphones.  The headphones 10 generally include at least one microphone 12 for sensing ambient noise 14 exterior to the headphones 10 during audio playback of media (e.g., see streaming media signal 15 that is received at the headphones 10). \nAdditionally, the headphones 10 generally include noise-cancelling circuitry 16 for generating and emitting a mirrored signal 18 (or inverse signal, or out of phase signal) (see FIG. 2)) to cancel the sensed ambient noise 14.  As shown, the mirrored\nsignal 18 is generally 180 degrees out of phase with the ambient noise 14.  The headphones 10 transmit the mirrored signal 18 to the user's headphone 10 to cancel the external ambient noise 14 that is sensed at the microphones 12.  A resultant signal 20\n(see FIG. 2) is provided which illustrates that the mirrored signal 18 essentially cancels out the ambient noise 14 (e.g., the resultant signal has no amplitude or phase) such that the user listens to the media free of any disturbance of the ambient\nnoise 14.  While this is generally preferable, certain external sounds within the ambient noise 14 may be of interest to the user.\nFIG. 3 depicts a system 30 for alerting a user of preference-based external sound(s) when listening to audio (or the streaming media 15).  The system 30 generally includes the headphones 10, a mobile device 32, and an alert generation block 34. \nThe mobile device 32 may be implemented as a cellular telephone, laptop, computer, tablet computer, etc. The mobile device 32 may be arranged as an audio source and provide the streaming media signal 15 to the headphones 10.  The headphones 10 may then\naudibly playback the media in response to receiving the streaming media signal 15.  The headphones 10 and the mobile device 32 may be hardwired coupled via an audio cable (not shown).  Alternatively, the headphones 10 and the mobile device 32 may be\nwirelessly coupled to one another and engage in data transfer via Bluetooth.RTM., WiFi, etc.\nAs noted above, the headphones 10 include a microphone 12 for receiving the ambient noise 14 from the environment external to the headphones 10.  The headphones 10 may transmit the sensed or received ambient noise 14' to the mobile device 32. \nWhile in the noted example, the mobile device 32 may transmit the streaming media signal 15 and the received ambient noise 14', it is recognized that the mobile device 32 may not necessarily be the device that provides the streaming media signal 15 as\nthe user may use a separate audio source (not shown) to provide streaming media signal 15.\nThe alert generation block 34 is generally configured to process the received ambient noise 14' and transmit an alert to the user via the mobile device 32 in the event the received ambient noise 14' includes an audible signature that is of\ninterest to the user (i.e., an external sound of interest).  The external sounds of interest may correspond to various rules as set up by the user.  For example, the external sounds of interest (or \"user defined preferences\") may include, but are not\nlimited to, detected words such as \"bus, train, bus stop, train station, timings, emergency, alert, public, \"user name,\" nick-name, father, son, mother, daughter, etc. The mobile device 32 enables the user to input (voice or touch input) and to update\nany of the external sounds of interest in a database 39 (or memory) (not shown).  The external sounds of interest may be generally defined as user-defined preferences and may be modified as the user desires.  This aspect may enable the user to change the\nuser-defined preferences based on any dynamic change in the user's location, environmental-context or social-context.  It is recognized that the alert generation block 34 may be located on the mobile device 32.  Alternatively, the alert generation block\n34 may be located on a server 36 that is remote from the mobile device 32.  Assuming for purposes of explanation that the alert generation block 34 is on the server 36, the mobile device 32 wirelessly transmits the received ambient noise 14' to the alert\ngeneration block 34.  Likewise, the mobile device 32 is configured to transmit the user-defined preferences to the alert generation block 34 for storage on the database 39.\nThe alert generation block 34 generally includes at least one controller 38 (or at least one microprocessor) (hereafter controller 38), a speech extraction block 40, a speech-to-text (STT) block 42, a natural language processing (NLP) engine 44,\na comparator 46, a text-to-speech (TTS) block 48, a notification block 50, a locator block 52, and a machine learning block 54.  The speech extraction block 40 receives the ambient noise 14' from the mobile device 32 and is configured to extract a\ndominant voice (or speech) signal that may be present in the received ambient noise 14'.  One example of the manner in which speech may be extracted from an audio signal is set forth in \"SPEECH IN NOISY ENVIRONMENTS: ROBUST AUTOMATIC SEGMENTATION,\nFEATURE EXTRACTION, AND HYPOTHESIS COMBINATION\" May 7, 2001 IEEE International Conference on Acoustics, Speech and Signal Processing, by Rita Singh, Michael L. Seltzer, Bhiksa Raj, and Richard M. Stern, Department of Electrical and Computer Engineering\nand School of Computer Science, Carnegie Mellon University which is hereby incorporated by reference in its entirety.  If the speech extraction block 40 detects the presence of speech in the ambient noise 14', the dominant voice component (i.e.,\ndetected) speech is transmitted to the STT block 42.  The STT block 42 converts the detected speech into text and the text is then transmitted to the NLP engine 44.  The NLP engine 44 then determines the intent (i.e., basic purpose or intention) of the\ntext and outputs the same to the comparator 46.  Since there exist humongous ways in which a `purpose` or `intent` could be spoken, the NLP engine 44 may extract the base purpose included in the spoken sentence.  As an example, consider the different\nvariations surrounding the intent behind \"How are you?\".  Different ways to speak this could include--\"How are you doing?\", \"What's going on with you?\", \"Howdy?\", etc. The NPL engine 44 is arranged to identify the same base purpose for the noted\ndifferent ways of how these questions are spoken.\nThe comparator 46 compares the intent against the various user-defined preferences received from the mobile device 32.  As noted above, the user-defined preferences are generally defined as preferences that are set up by the user of the\nheadphones 10 that correspond to detected speech in the ambient sound 14' at the headphones 10 that warrants generating an alert to notify the user of the headphones 10 while the user is engaged with audio playback at the headphones 10.  The user may\ninput, establish and/or store the user-defined preferences at any time on the alert generation block 34 via the mobile device 32.  If the comparator 46 detects a match between the intent and the available user defined preferences, then the comparator 46\noutputs a signal to control the notification block 50 to provide an alert (e.g., visual and/or audible) to the mobile device 32 and/or the headphones 10.  The user has the option of transmitting the alert in either a visual format or an audible format. \nThe alert that is transmitted to the mobile device 32 is as per what the user added while entering the user-preference rules in the audible format assuming the audible format is elected.  For example, the user may have added a rule (a) if text \"contains\nmy name\", then play \"You were called-out\", (b) if text contains \"XYZ bus stop\", then play \"you have reached your destination\", etc. In this case, the TTS block 48 may provide an audible format of \"You were called-out\" or \"You have reached your\ndestination\" if this is setup by the user as a user-defined preference.  Alternatively, if the user elects to transmit the alert in a visual format, the notification block 50 receives the text (in the digitized format) from the STT block 42 and transmits\nthe same to the mobile device 32 where the mobile device 32 then visually presents the text to the user on a display thereof.  For example, the notification block 50 transmits the alert \"You were called-out\" or \"You have reached your destination\" if this\nis the selected video (or text notification) to be displayed to the user as defined by the user-defined preferences.\nIn general, the alert generation block 34 is configured to transmit a plurality of alerts over time to the mobile device 32.  The mobile device 32 is configured to arrange a listing of alerts and corresponding dates/time in which such alerts\nwere received and is further configured to enable the user to read/listen to the alerts at his/her preference.  The locator block 52 is generally configured to determine the location, activity, and/or upcoming tasks from the mobile device 32.  In one\nexample, the locator block 52 may determine the location of the mobile device 32 (and the user) based on global positioning satellite (GPS) coordinates and/or mobile base station coordinates that are provided by the mobile device 32 to the alert\ngeneration block 34.  The locator block 52 may include various hardware sensors (not shown) such as, but not limited to, a GPS sensor, an accelerometer, a gyroscope and/or a magnetic sensor.  The data from the noted hardware sensors may aid in\nidentifying a particular activity that the user is engaged in. For example, the user may be involved in a physical activity such as sitting, walking, moving in a vehicle, etc. Knowledge of the user's activity may aid in correlating the user defined\npreferences and the type of physical activity that the user is involved with.  The database 39 receives and stores information pertaining to the user's calendar and e-mails that are present on the mobile device 32.  The data from the calendar and emails\nmay aid in detecting a correlation between upcoming tasks or appointments and the user-defined preferences.  The information pertaining to the location of the mobile device 32 may aid in identifying at which locations the user prefers to add user-defined\npreferences and for the system 30 to provide additional external sounds of interest in the future at such locations.  Thus, over time, the machine learning block 54 can build a user-preference model and the alert generation block 34 may then transmit\nsignals corresponding to recommendations provided by the machine learning block 54 to add more user-defined preferences based on the location, upcoming events, environmental-context, social-context and/or user's current activity.  In general, the\ncontroller 38 is programmable to execute instructions for the operations performed by the speech extraction block 40, the STT block 42, the NLP engine 44, the comparator 46, the TTS block 48, the notification block 50, the locator block 52, and the\nmachine learning block 54.\nFIG. 4 generally depicts a more detailed implementation of the headphones 10 and the mobile device 32 in accordance to one embodiment.  The headphones 10 generally include at least one controller 70 (or at least one microprocessor) (hereafter\ncontroller 70), a power/battery supply 72, a microphone 74, a transceiver 76, active noise cancellation circuitry 78, and speaker(s) 80.  The power supply 72 powers the headphones 10 (e.g., the electrical devices located within the headphones 10).  The\nmicrophone 74 receives ambient noise 14 external to the headphones 10 and the active noise cancellation circuitry 78 generates the mirrored signal 18 (see FIG. 2) that is out of phase with respect to the received ambient noise 14.  The controller 70 and\nthe active noise cancellation circuitry 78 transmit the mirrored signal 18 to the speakers 80 to cancel the noise present external to the headphones 10.  The transceiver 76 is configured to receive the streaming media 15 such that the speakers 80\nplayback the same for the user.  In addition, the transceiver 76 is configured to transmit the received ambient noise 14' to the mobile device 32.  It is recognized that there may be any number of transceivers 76 positioned within headphone 10.  The\ntransceiver 76 is also configured to receive the alert from the mobile device 32 assuming the alert is to be audibly played back to the user when a preferred external sound of interest is detected.\nThe mobile device 32 generally includes at least one controller 82 (or at least one microprocessor) (hereafter controller 82), a power/battery supply 84 (hereafter power supply 84), a first transceiver 86, memory 88, a user interface 90,\nspeakers 92, a display 94, and a second transceiver 96.  The power supply 84 powers the mobile device 32 (e.g., the electrical devices located within the mobile device 32).  The first transceiver 86 is configured to receive the ambient noise 14' from the\nheadphones 10.  It is recognized that the headphone 10 and the mobile device 32 may engage in communication with one another via an audio cable, Bluetooth.RTM., Wifi, or other suitable communication mechanism/protocol.  The mobile device 32 is configured\nto communicate with the alert generator block 34 on the server 36 via the second transceiver 96 in the event the alert generator block 34 is not implemented within the mobile device 32.  In this case, the mobile device 32 and the alert generator block 34\nmay engage in communication with one another also via Bluetooth.RTM., Wifi, or other suitable communication mechanism/protocol.  The mobile device 32 transmits the user-defined preferences as input by the user and stored on the memory 88 to the alert\ngeneration block 34.\nThe user interface 90 enables the user to enter the various user-defined preferences that are used to trigger the generation of an alert to notify the user of an external sound of interest for the user during audio playback of the headphones 10. The display 94 is configured to visually notify the user of the alert when the comparator 46 detects a match between data on the received ambient noise 14' and the user defined preference on the server 36 in the event the user selects to have the alert\nvisually displayed.\nFIG. 5 generally depicts a method 110 for alerting a user of external sounds of interest when listening to audio through the headphones 10 in accordance to one embodiment.\nIn operation 112, the alert generation block 34 receives the user-defined preferences from the mobile device 32.  The alert generation block 34 stores the same on memory thereof.\nIn operation 114, the alert generation block 34 receives the ambient noise 14' from the mobile device 32.\nIn operation 116, the STT block 42 extracts the voice component (if applicable) from the received ambient noise 14' and converts this information into text.  In general, the user-defined preferences as established or added as text may generally\nfollow the format of if-then-statements.  As noted above, in one example the user may add the rule if text \"contains my name,\" then provide alert \"You were called out.\" In another example, the user may add a rule--\"if the external sound contains a sound\nsimilar to\" and point this rule to a recording of a siren sound using the user interface 90 available to the user.  A match between the external sound and the recorded sound results in the system 30 providing a notification/audible alert to the user.\nIn operation 118, the NLP engine 44 extracts the intent from the text.\nIn operation 120, the comparator 46 compares the intent against the various user-defined preferences and determines whether there is a match.  If there is a match, then the method 110 proceeds to operation 122.  If no match is found, the system\n30 performs no action.  The detected intent is noted or tagged by the alert generation block 34 as a rejected intent.  The rejected intent is stored in the database 39.  If the alert generation block 34 detects the rejected intent two or more times, then\nthe alert generation block 34 via the machine learning block 54 flags this case and the alert generation block 34 transmits a signal to the mobile device 32 to provide an inquiry or recommendation to the user regarding interest in the rejected intent.\nIn operation 122, the notification block 50 transmits an alert in either an audio based format or a visual based format to the mobile device 32.  The alert generation block 34 receives a signal from the mobile device 32 that indicates whether\nthe alert generation block 34 is to transmit the alert in either the audio based format or in the visual based format.  As noted above, in the event the alert generation block 34 is controlled to transmit the alert audibly, the TTS block 48 transmits the\nresponse as desired by the user or as setup within the user-defined rule to mobile device 32.  In the event the alert generation block 34 is controlled to transmit a visual alert, the alert generation block 34 transmits the text as provided by the STT\nblock 42 to the mobile device 32.\nFIG. 6 depicts a method 130 for generating a user preference model based on user aspects in accordance to one embodiment.\nIn operation 132, the alert generation block 34 receives information corresponding to the location of the mobile device 32, activity of the user (e.g., sitting, walking, moving in a vehicle, etc), and upcoming events for the user from the mobile\ndevice 32.  The alert generation block 34 stores this information on memory (not shown).  The alert generation block 34 is configured to continuously receive this information from mobile device 32 for purposes of generating and updating the user\npreference model.\nIn operation 134, the machine learning block 54 dynamically generates and refines the user preference model over time based on the information.  Thus, over time, the machine learning block 54 can build the user preference model and the alert\ngeneration block 34 may then transmit signals corresponding to recommendations to the mobile device 32 to add more user-defined preferences based on location, upcoming events, environmental-context, social-context and/or current user activity.\nIn operation 136, the alert generation block 34 may communicate various recommendations to the mobile device 32 of the user to add/select additional user-defined preferences based on the location, upcoming events, environmental-context,\nsocial-context and/or current user activity.  For example, (a) if a user has added a few rules based on his/her location being office, and if the machine learning block 54 detects several but similar rejected intents while the location corresponds to an\noffice, a recommendation is presented to the user to add the case of the rejected intent as a rule the user maybe interested in; and (b) a user has historically added a few rules based on their location being at \"XYZ shopping complex\", the user has a\nto-do list on their phone, and it shows grocery buying at the \"XYZ shopping complex,\" then the machine learning block 54 may dynamically add \"if user is near a grocery store in XYZ shopping complex\", and prompt the user to complete the task if any\nannouncement regarding grocery store is played out in \"XYZ shopping complex\".\nWhile various embodiments are described above, it is not intended that these embodiments describe all possible forms of the invention.  Rather, the words used in the specification are words of description rather than limitation, and it is\nunderstood that various changes may be made without departing from the spirit and scope of the invention.  Additionally, the features of various implementing embodiments may be combined to form further embodiments of the invention.", "application_number": "15265092", "abstract": " A computer-program product embodied in a non-transitory computer readable\n     medium that is programmed to communicate with a listener of headphones is\n     provided. The computer-program product includes instructions to receive\n     ambient noise indicative of external noise to a listener's headphone and\n     to extract a speech component from the ambient noise. The\n     computer-program product further includes instructions to derive an\n     intent from the speech component of the ambient noise and compare the\n     intent to at least one user defined preference. The computer-program\n     product further including instructions to transmit an alert to notify a\n     listener that the intent of the speech component matches the at least one\n     user defined preference.\n", "citations": ["5647011", "8326635", "9357320", "9398367", "20070092087", "20070116316", "20080101627", "20100046767", "20100076793", "20140064511", "20140376728", "20150170645", "20150222977", "20150332564", "20160381450"], "related": []}, {"id": "20180089573", "patent_code": "10375200", "patent_name": "Recommender engine and user model for transmedia content data", "year": "2019", "inventor_and_country_data": " Inventors: \nSolenthaler; Barbara (Zurich, CH), Kaeser; Tanja (Zurich, CH), Klingler; Severin (Zurich, CH), Galati; Adriano (Burbank, CA), Gross; Markus (Burbank, CA)  ", "description": "<BR><BR>FIELD OF DISCLOSURE\nThe present disclosure relates to an apparatus, system and method for processing transmedia content data.  For example, the disclosure provides for identifying and surfacing recommendations of transmedia content to users for processing of\ntransmedia content, and additionally the disclosure also provides for user modelling of users accessing transmedia content data.\n<BR><BR>BACKGROUND\nInfluenced by a variety of different multimedia content types, new digital distribution channels, mobile communication devices and an ever increasing use of social media, industry is currently experiencing a disruption in how media is created,\ndistributed and consumed.  Classical production pipelines have become less effective as audiences move towards anytime, anywhere, personalized consumption, substituting TV-centric models with multi-device, multichannel models.  Individual customers and\ngroups of customers have also become more interactive and participatory, contributing significantly to the creation of new media.  The cycles in the traditional creation-distribution-consumption loop become much shorter as consumers constantly provide\nfeedback, resulting in a trend towards ultrashort form content.\nExisting delivery platforms, for example YouTube and Facebook, allow for the creation and editing of simple channel based content, using a basic model whereby content creators can upload content such as video, text or images, and users can\nconsume the content in an isolated, linear and mono-medial manner.  This can often be done in conjunction with media presented via other platforms such as television or print media.\nAt the same time, functionality provided by existing multimedia platforms allows the sharing of user-generated content, which, along with social networking, is transforming the media ecosystem.  Mobile phones, digital cameras and other pervasive\ndevices produce huge amounts of data that users can continuously distribute in real time.  Consequently, content sharing and distribution needs will continue to increase.  The content can be of many different forms, known collectively as \"transmedia\"\ncontent.\nExisting systems that allow users to generate, organize and share content are generally hard to control: these systems do not offer adequate tools for predicting what the next big trend will be, and which groupings of time-ordered content\nresonate with particular audiences.  Furthermore, visualising the large amount of multimedia information in a way which users can explore and consume is challenging.  In particular, visualisation of such large data sets is challenging in terms of\nperformance, especially on lower-power devices such as smartphones or tablets.  It is desirable that any visualisation of the data could be rendered in real time such that immediate visual feedback is provided to a user exploring the data.\nThe ever-growing availability of content to multiple users and the ever-increasing power of computing resources available to individual users is driving users towards their own individual creation of content, with such content being in multiple\nformats.  This progression can be seen in FIG. 1.  User 10 consumes single content items 15.  With increasing computing resources, user 20 has developed into an interactive user making choices which affect the flow of individual content items 25 to the\nuser 20.  Further, user 30 has recently become more common by generating multiple personalised, individual content items 35 which can be accessed over the Internet 50 by other users.  A problem now exists with a current user 40 who can access a\nconsiderable amount of different types of content items 35 over the Internet 50 and desires to utilise such content.  It would be desirable for user 40 to be able to contribute to and generate new structured groups 46 of linked content items 45.\nIn utilising the content items 35, the amount of which can be considerable, it can be problematic for the user 40 to be able to readily identify relevant and appropriate content for placing into the new structured groups 46.  Such content can\nadditionally be identified by considering the content generated from other users.  However, it can be equally problematic to identify the users who might be similar to user 40, and thereby themselves be generating relevant and appropriate content.\n<BR><BR>SUMMARY OF THE DISCLOSURE\nIn a first aspect of this disclosure, there is provided an apparatus for surfacing transmedia content to a given user of a plurality of users comprising: a memory configured to store: a plurality of transmedia content items, each content item\nbeing associated with at least one of the plurality of users; and linking data which define time-ordered content links between the plurality of transmedia content items, the plurality of transmedia content items being arranged into linked transmedia\ncontent subsets comprising different groups of the transmedia content items and different content links therebetween; a transmedia content linking engine configured to receive user input indicative of a time-ordering between at least two user-selected\ntransmedia content items and generate the content link data for storage in the memory, thereby defining a linked transmedia content subset including the at least two user-selected transmedia content items; and a recommender engine configured to access\nthe memory and surface to the given user: one or more content items of the plurality of transmedia content items; and/or one or more of the linked transmedia content subsets of the linked transmedia content subsets; and/or one or more identifications of\nidentified users other than the given user; and/or the content items of the plurality of transmedia content items associated with at least one identified user other than the given user, wherein the one or more surfaced content items and/or the one or\nmore surfaced linked transmedia content subsets are surfaced for selection by the given user via the transmedia content linking engine as one or more user-selectable transmedia content items.\nThe apparatus thus provides for transmedia content, whether individual items, subset groups thereof, or owners associated thereto, to be readily identified and surfaced so that a given user can utilise such content within their apparatus, even\nif it is associated with other users, for example by linking it to other content in a time-ordered fashion.\nThe apparatus may be implemented as a computing device, for example a server computing device.\nThe memory may further include or be configured to store user ownership data which associates each transmedia content item to a user of the plurality of users, and each transmedia content subset of the multiple content subsets with a user of the\nplurality of users.\nThe transmedia content linking engine may be configured to identify a current user-selected time-ordered location of the given user within a given linked transmedia content subset for insertion of one of the transmedia content items, and to\ngenerate corresponding content linking data therefor upon insertion by the given user of the one or more content items into the given linked transmedia content subset.\nThe recommender engine may be further configured to surface the one or more transmedia content items and/or surface the one or more transmedia content subsets based on the current user-selected time-ordered location.\nThe apparatus may further comprise a preference model, wherein the preference model is configured to generate a user-specific rating (or score) for a given user for a given transmedia content data item.\nThe preference model can be configured to:\nremove global effects for one or more rated items that have been previously rated by the given user to generate explicit ratings (user-assigned ratings) and storing the removed global effects;\nprovide the explicit ratings and metadata associated with the given transmedia content data item to a plurality of recommender algorithms to generate a plurality of predicted ratings for the given transmedia content data item;\ncombine the plurality of predicted ratings to build an ensemble prediction of the predicted ratings for the given transmedia content data item;\nadd the stored global effects to the ensemble prediction to produce a predicted user rating for the given transmedia content data item; and\noutput the predicted user rating for the given transmedia content data item.\nThe user assigned ratings previously assigned by individual users may be stored as metadata alongside each content item, or linked content subset.  The user assigned ratings may be extracted by a metadata extraction component and fed to the\npreference model.\nThe apparatus may further comprise a user-brand match component configured to provide for a given user a prediction of a preference for a given branded content data item.\nThe apparatus may further comprise a branded content model configured to provide for a given transmedia content item a prediction of the suitability of a given branded content data item.\nThe recommender engine may be further configured to surface transmedia content data items by querying the preference model, user-brand match component and brand model component by providing the preference model, user-brand match component and\nbrand model with a transmedia content parameter, user data for the given user and a given branded content data item, and to maximise the sum of the output for the preference model, user-brand match component and brand model over the transmedia content\nparameter.\nThe recommender engine may be configured to surface the transmedia content data item with the maximum output.\nThe plurality of transmedia content items may comprise items of different content types.\nThe transmedia content data items may relate to narrative elements of the transmedia content data items.  The time-ordered content links can define a narrative order of the transmedia content data items.  Each time-ordered content link can\ndefine a directional link from a first transmedia content data item to a second transmedia content data item of the plurality of transmedia content data items.\nThe first transmedia content data item may have a plurality of outgoing time-ordered content links.  The second transmedia content data item may have a plurality of incoming time-ordered content links.\nThe memory can be further configured to store a plurality of subset entry points for the plurality of transmedia content subsets.  Each subset entry point may be a flag indicating a transmedia content data item that has at least one outgoing\ntime-ordered link and no incoming time-ordered links.  Each linked transmedia content subset can define a linear path, wherein the linear path comprises a subset entry point, one or more transmedia content data items and one or more time ordered links\nbetween the subset entry point and the transmedia content data items.  Two or more transmedia content subsets can share one or more subset entry points, one or more transmedia content data items and/or one or more time ordered content links.\nThe recommender engine can be further configured to surface one or more group's transmedia content subsets to the given user, the one or more surfaced groups being surfaced for selection by the given user via the transmedia content linking\nengine.\nThe apparatus may further comprise a user model, wherein the user model can be configured to provide predictions of the state and/or behaviour of a given user to the recommender engine.\nThe memory may be further configured to store a plurality of historical user interaction data items which define the historical interaction of a given user with the apparatus for surfacing transmedia content to a given user of a plurality of\nusers, and wherein the user model may be configured to: receive one or more historical user interaction data items associated with the given user; provide the one or more historical interaction data items to a statistical model to produce a prediction of\na mental state of the user; retrieve from the statistical model the predicted mental state of the user and/or a predicted behaviour of the user; and output the predicted mental state of the user and/or predicted behaviour of the user.\nThe statistical model may be a hidden Markov model.\nThe transmedia content recommender engine may be further configured to surface the one or more individual content items to the given user and/or surface one or more of the linked transmedia content subsets to the given user when the output of\nthe user model indicates that the user is in a mental state conducive to consuming transmedia content data items.\nThe transmedia content recommender engine can be further configured to surface the one or more individual content items to the given user and/or surface one or more of the linked transmedia content subsets to the given user when the output of\nthe user model indicates the predicted user behaviour is to consume transmedia content data items.\nIn a second aspect of the present disclosure, there is provided a system for surfacing transmedia content to a given user of a plurality of users comprising:\nthe aforementioned apparatus;\nan electronic device configured to be in communication with the apparatus and receive and display to the given user: the surfaced one or more content items of the plurality of transmedia content items; and/or the surfaced one or more linked\ntransmedia content subsets of the linked transmedia content subsets; and/or the surfaced one or more identifications of identified users other than the given user; and/or the surfaced content items of the plurality of transmedia content items associated\nwith at least one identified user other than the given user.\nIn a third aspect of the present invention, there is provided A method for surfacing transmedia content to a given user of a plurality of users from a memory configured to store a plurality of transmedia content items, each content item being\nassociated with at least one of the plurality of users, and linking data which define time-ordered content links between the plurality of transmedia content items, the plurality of transmedia content items being arranged into linked transmedia content\nsubsets comprising different groups of the transmedia content items and different content links therebetween, the method comprising: receiving, at a transmedia content linking engine, user input indicative of a time-ordering between at least two\nuser-selected transmedia content items; generating with the transmedia content linking engine the content linking data for storage in the memory, thereby defining a linked transmedia content subset including the at least two user-selected transmedia\ncontent items; and accessing the memory and surfacing to the given user with a recommender engine: one or more content items of the plurality of transmedia content items; and/or one or more of the linked transmedia content subsets of the linked\ntransmedia content subsets; and/or one or more identifications of identified users other than the given user; and/or the content items of the plurality of transmedia content items associated with at least one identified user other than the given user,\nwherein the one or more surfaced content items and/or the one or more surfaced linked transmedia content subsets are surfaced for selection by the given user via the transmedia content linking engine as one or more user-selectable transmedia content\nitems.\nThe electronic device may be implemented as user equipment, which may be a computing device, for example a client computing device, such as personal computer, laptop or a handheld computing device.  For example, the handheld computing device may\nbe a mobile phone or smartphone.  The electronic device may include a user input interface configured to provide user input to the processing circuitry, such as for example the instructions to create a new time-ordered content link.  This may be one or\nmore of a keypad or touch-sensitive display.  The electronic device may include a display (which may include the touch-sensitive display) configured to output data to a user interacting with the electronic device, including one or more of the content\ndata items.\nIn a fourth aspect of the present invention, there is provided a computer readable medium comprising computer executable instructions, which when executed by a computer, cause the computer to perform the steps of the aforementioned method.\n<BR><BR>BRIEF DESCRIPTION OF DRAWINGS\nThe present invention is described in exemplary embodiments below with reference to the accompanying drawings in which:\nFIG. 1 depicts how users interact with content items according to the present disclosure.\nFIGS. 2A, 2B and 2C depict a linear transmedia content subset, grouped non-linear transmedia content subsets, and a subset universe respectively, each formed of multiple transmedia content data items and time-ordered content links.\nFIG. 3 depicts the architecture of the system of the present disclosure.\nFIG. 4 depicts an exemplary apparatus on which the backend of the present disclosure operates.\nFIG. 5 depicts an exemplary apparatus on which the frontend of the present disclosure operates.\nFIG. 6 depicts exemplary non-linear networks of transmedia content data items according to the present disclosure.\nFIG. 7 depicts a recommender engine of the apparatus of the present disclosure.\nFIG. 8 depicts an exemplary preference model component of the recommender engine component according to the present disclosure.\nFIG. 9 depicts a user model component of the apparatus of the present disclosure.\nFIGS. 10A and 10B depict exemplary user interfaces that are rendered and output by the system.\nFIG. 11 depicts how user state and actions can be modelled according to the system.\nFIG. 12 depicts a method for surfacing transmedia content to a user according to the present disclosure.\n<BR><BR>DETAILED DISCLOSURE\nThe present disclosure describes a new apparatus, system and method for managing transmedia content.  In one embodiment, there is disclosed a platform for the creation, distribution and consumption of transmedia content.  The content may be\narranged in a time-ordered manner for consumption, thereby defining so-called \"story\" based content.\nIn the context of the present disclosure, groups of time-ordered content, for example in the form of stories, are made up of multiple elements of transmedia content, each being referred to herein as a transmedia content data items.  Each item\npertains to a narrative element of the story.  Each transmedia content data item may be linked, and thus connected, to one or more other transmedia content data items in an ordered fashion such that a user can navigate through subsets of the transmedia\ncontent data items (also referred to as transmedia content subsets) in a time-ordered fashion to consume some or all of an entire story.\nThe term \"transmedia\" means that the grouped content data items (which are linked within the content subsets) comprise a plurality of different multimedia types, e.g. at least two different types of multimedia content.  For example, the\ndifferent types of transmedia content data of each content data item within the subset can comprise at least two different types from one or more of the following: textual data, image data, video data, audio data, animation data, graphical visualization\nor UI data, hypertext data, gaming data, interactive experience data, virtual reality (VR) data, augmented reality data, and multisensory experience data.  Each transmedia content data item may itself comprise multiple media types, e.g. video and audio\ndata may be present within a single item such that the audio is time-associated associated with the video.\nThe transmedia content data items can be grouped into transmedia content subsets.  Each subset may be grouped based on one or more non-linear network of the content data items.\nWithin each transmedia content subset, transmedia content data items are linked to one another, directly or indirectly, by time-ordered links between each data item.  Typically, each time ordered-content link links two transmedia content data\nitems.  An exception exists for a time ordered link which connects a subset entry point and a transmedia content data item as explained below.  The time-ordered link also defines a direction between the two transmedia content data items.  The direction\nindicates an order in which linked transmedia content data items should be presented to a user of the system.  For example, when a first transmedia content data item is presented to a user, at least one outgoing time-ordered link (i.e. the direction\ndefined by the link is away from the first transmedia content data item) indicates a second transmedia content item that should be presented to the user next.\nThe term \"subset\" is used to denote a subset of transmedia content data items within the set of all transmedia content data items stored by the system.  The transmedia content data items that are part of a transmedia content subset are all\ndirectly or indirectly connected to each via time-ordered content links between the transmedia content data items.  A transmedia content subset may be a linear, i.e. each transmedia content data item in the subset has at most one incoming time-ordered\ncontent link and one outgoing time-ordered content link, or a non-linear network, i.e. one or more of the constituent transmedia content data items has more than one incoming or outgoing time-ordered content link.  It will also be appreciated a\nnon-linear network of transmedia content data items can be considered to be made up of multiple overlapping linear paths from start-point to end-point through that network, and that each linear path may also be considered to be a transmedia content\nsubset.  Furthermore, the term \"group\" has been used to denote a collection of transmedia content data items that are not necessarily connected, directly or indirectly, via time-ordered content links.  However, where the term \"group\" has been used, a\n\"subset\" of transmedia content data items, as defined above, may additionally be formed and utilised.\nEach time-ordered link is stored in memory as link data comprising: the input content data item; and the output content data item and thus implicitly a direction between two content data items.  Directional data for the links is also stored\nwhich defines the path the links should be processed, and thus the order for processing of transmedia content items.  The order can also be dependent on user interaction with each item as it is surfaced to a user.\nExamples of transmedia content subsets are depicted in FIGS. 2A, 2B and 2C.  As mentioned above, the transmedia content data items are grouped into transmedia content subsets.\nIn FIG. 2A, subset 100 defines a linear path of transmedia content data items 104a-c. The subset 100 also comprises a subset entry point 102, which defines a starting point in the subset from which the system can commence presenting the\ntransmedia content data items within the subset.  The subset entry point 102 may be linked to the first transmedia content data item 104a by a time-ordered link, or may be a flag associated with the first transmedia content data item 104a which indicates\nthat the transmedia content data item 104a is the first in the subset.\nIn the context of the present disclosure, the term \"linear\" means that each transmedia content data item has, at most, only one incoming time-ordered line (i.e. the direction defined by the link is inwards towards the transmedia content data\nitem) and only one outgoing time-ordered link.  The path defined by the transmedia content subset 100 is unidirectional and there is only one possible route from the subset entry point 102 and the final transmedia content data item of the path (i.e. a\ntransmedia content data item with an incoming time-ordered link but no outgoing time ordered link).\nThe transmedia content data items may also be grouped based on multiple content subsets in a non-linear network, such as non-linear network 110 depicted in FIG. 2B.  In this context, the term \"non-linear\" means that time-ordered links between\nthe data items of each network may form a plurality of different paths through the network 110 which start in different places, end in different places, branch, split, diverge, leave some data items out of the path and/or overlap with other paths.  Such\na non-linear network 110 can also be considered to be a group of transmedia content subsets which share one or more transmedia content data items and/or time-ordered content links.\nIn the depicted non-linear network 110, each transmedia content data item 104a-c, 112a-b can have one or more incoming time-ordered links and one or more outgoing time-ordered links.  The data items 112a, 104b and 112b form a second transmedia\ncontent subset which shares the data item 104b with the first transmedia content subset 100.\nFIG. 2C depicts a story universe 120, in which multiple, related non-linear networks are grouped or clustered together.  In one embodiment, the non-linear networks of a story universe do not share transmedia content data items and/or\ntime-ordered links with the non-linear networks of another, different story universe.  However, in an alternative embodiment, the non-linear networks of a story universe do share one or more transmedia content data items and/or time-ordered links with\nthe non-linear networks of another, different story universe.\nThe system of the present disclosure manages the transmedia content data items, transmedia content subsets and one or more non-linear networks, facilitates the generation and manipulation of items between and within subsets and networks so that\nstorylines can be formed.  Accordingly, the creation of transmedia content subsets and non-linear networks by a user of the system enables collaboration between users of the system and allows consumption of the created storylines.  The architecture of\nthe system is depicted in FIG. 3.\nFIG. 3 depicts the overall architecture of the system 200.  The system 200 includes a frontend device 210, which is typically located on a user device such as a smartphone, tablet or PC that is operated directly by the user of the system 200,\nand a backend device 230, which is typically located on one or more servers that are connected to the user device via a network such as the Internet.\nThe backend 230 contains global resources and processes that are managed, stored and executed at a central location or several distributed locations.  The frontend 210 contains resources and processes that are stored and executed on an\nindividual user device.  The backend 230 is responsible for tasks that operate on large amounts of data and across multiple users and stories, while the frontend 210 only has access to the resources of a particular user (or a group of users) and focuses\non presentation and interaction.\nThe frontend 210 communicates with the backend 230 via the network, the communication layer 212 that is part of the frontend 210 and the experience control layer 232 that is part of the backend 230.  The experience control layer 232 is\nresponsible for handling the distribution of transmedia content data items, access limitations, security and privacy aspects, handling of inappropriate content data items, and user-specific limitations such as age group restrictions.  It ensures that\ninappropriate, illegal, unlicensed or IP-violating content is flagged and/or removed, either automatically, semi-automatically or manually.  It also handles sessions as the user interacts with the system and provides session specific contextual\ninformation, including the user's geolocation, consumption environment and consumption device, which can then be used by the frontend 210 to adapt the consumption experience accordingly.  The experience control layer 232 also acts as a checkpoint for\ncontent validation, story verification, and story logic, in order to provide users with a consistent story experience.\nThe communication layer 212 performs client-side checks on permissions, content validation, and session management.  While the final checks happen at the experience control layer 232 of the backend 230, the additional checks carried out in the\ncommunication layer 212 help in providing a consistent experience to the user (e.g. not displaying content or features that cannot be accessed).\nThe frontend 210 also includes the user interface (UI) component 220, which is responsible for displaying and presenting the transmedia content data items to users, including visual, auditory and textual representations, and is also responsible\nfor receiving the user's input through pointing devices, touch events, text input devices, audio commands, live video, or any other kind of interaction.  The UI component 218 can adapt to the user's location, environment, or current user state in order\nto provide an optimized experience.\nThe visual navigation component 214 is also included in the frontend 210, and allows a user to explore, browse, filter and search the transmedia content data items, transmedia content subsets and non-linear networks, and any other content\nprovided by the platform.  For navigation in the context of transmedia content and stories, the visual navigation component 214 provides intelligent abstractions and higher-level clusterings of transmedia content data items, transmedia content subsets\nand non-linear networks, providing the user with an interface for interactive visual exploration of the transmedia content, which enables the user to make navigation choices at a higher-level abstraction before exploring lower levels, down to single\nstories, i.e. transmedia content subsets, and individual transmedia content data items.  The structure of transmedia content subsets and non-linear network and the time-ordered links between transmedia content data items data items is visualized as well,\nproviding the user with information on how these data items are related to each other.  In one embodiment of this visualisation, a graph structure is employed, with nodes representing transmedia content data items, and connections representing the\ntime-ordered content links.  In the main, the evolution of the transmedia content subsets and non-linear networks is rendered in real-time as the subsets and non-linear networks are created and modified by all users of the system.  In addition, in a\nparticular embodiment which is user initiated, for example via user selection or automatically based on user interaction, e.g. immediately or shortly after a given user logs in to the system, the recent past evolution of the transmedia content subsets\nand non-linear networks, e.g. the evolution since last login of the given user can be displayed graphically, e.g. in a time lapse rendering of the changes of the transmedia content subsets and non-linear networks in the order in which they occurred.\nThe social interaction component 216 handles visualisations and user input related to interactions between individual users of the system 200.  It provides tools for editing a user's public information, enabling notifications on other users\ncreating content (following), endorsing and rating other users' content, and directly interacting with other users through real-time messaging systems, discussion boards, and video conferencing.  These tools also allow users to collaboratively create new\ncontent (i.e. transmedia content data items) and author and edit stories (i.e. transmedia content subsets and/or non-linear networks), as well as define the access rights and permissions associated with such collaborative work.  The enforcement of these\nrights is handled by the experience control layer 232, as mentioned above.\nIn addition to the experience control layer 232, the backend 230 comprises a user model component 234, a story model component 236, a recommender engine 238, and a metadata extraction component 240, in addition to one or more data stores or\ncomputer memory for storing data related to the transmedia content such as the transmedia content data items, linking data, transmedia content subsets, non-linear networks, and metadata relating to the individual data items and linking data as well as to\nthe subsets and non-linear networks.\nThe user model component 234 represents user behaviour and properties.  It is driven by a suite of algorithms and data collections, including but not limited to statistics, analytics and machine learning algorithms operating on user interaction\npatterns, consumption behaviour and social interactions.  The analytics happens in real-time and the user model component 234 is continuously updated as the user interacts with the system 200.  Additional information such as the user's geolocation can be\ntaken into account as well.  The corresponding data is stored in a user database.  The user model component 234 also comprises models for groups of several users, which for example emerge during multiuser collaboration, consumption, geolocation, or\nsocial interactions.  As part of the user model component 234, users and/or groups of users are profiled and characterized according to their personality, productivity stage and other criteria.  The user model component allows the system to make\npredictions of user behaviour under real or hypothetical conditions, which then feed into the recommender engine component 238.  The methods employed by the user model component for prediction user behaviour and creative state are described below in more\ndetail with respect to FIG. 9.  The user model component 234 also permits the correlation of interaction patterns of users not identified to the system so as to re-identify users probabilistically.\nThe user model component 234 is also connected to the talent harvest component, which, based on user behaviour, identifies individual users or groups of users that fulfil certain criteria such as, for example, having a large amount of users\nconsuming or endorsing their work, having significant influence on other users' behaviours and opinions, or being highly popular personalities.  The talent harvest component, in concert with the recommender engine component 238, then influences the\nbehaviour of such users of the system 200.\nThe story model component 236 characterises the content of single transmedia content data items, transmedia content subsets, non-linear networks and whole story universe, and stores the corresponding data in a story world database.  The\ncharacterisations are found through algorithms such as, but not limited to, metadata extraction, analytics, graph analysis, or any other algorithms operating on connections between content in general.  Metadata extraction extends to include visual,\nauditory or textual elements, as well as higher-level concepts like characters, character personality traits, actions, settings, and environments.  The characterisation also takes into account how users interact with the content, including the analysis\nof consumption patterns, content ratings and content-related social interaction behaviour.  The corresponding updates of the story model component 236 happen in real-time as users interact with the system 200 or as new content is created or existing\ncontent is modified.  Additionally, the story model component 236 makes use of story, characterisations (including metadata) to model story logic.  Using story reasoning, the consistency of individual stories can be verified and logical inconsistencies\ncan be prevented either when a story is created or at the time of consumption.  The story model component 236 also communication with the story harvest component, which uses the data provided by the story model component 236 in order to identify and\nextract content (transmedia media content data items, transmedia content subsets, non-linear networks or higher-level abstractions).\nThe recommender engine component 238 is in communication with both the story model component 236 and the user model component 234 and provides conceptual connections between the story model component 236 and the user model component 234.  The\nrecommender engine component 238 uses data analytics to match content (transmedia content data items, transmedia content subsets, non-linear networks) with users, suggesting users for collaboration, suggesting content to be consumed, or suggesting\nstories, story arcs or story systems to be extended with new content.  Recommendations can be explicit, with recommendations being explicitly labelled as such to the user, guiding the user through a transmedia content subset or non-linear by providing an\noptimal consumption path or suggesting other users to collaborate with, or they can be implicit, meaning the user's choice is biased towards certain elements of content (including transmedia content, advertisement, users), without making the bias\nexplicitly visible to the user.\nThe metadata extraction component extracts metadata from transmedia content (i.e. transmedia content data items, transmedia content subsets and/or non-linear networks) automatically, semi-automatically, or manually.  The metadata extraction\ncomponent 240 tags and annotates transmedia content, providing a semantic abstraction not only of the content of individual transmedia content data items, but also of the time-ordered links, transmedia content subsets, non-linear networks, and story\nsystems.  The derived metadata thus spans a horizontal dimension (cross-domain, covering different types of media) as well as a vertical one (from single transmedia content data items to whole story systems).\nAlso depicted in FIG. 3 are story world database 280, media (content) database 282 and user database 284.  The databases 280-284 are stored in memory 301 of server device 230.  The story world database 280 stores data characterising and defining\nthe structure of the transmedia content subsets, non-linear networks and whole story systems, for example by way of linking data defining the subset structure.  Additionally, metadata and graph analytics pertaining to the subsets and networks may also be\nstored in the story world database 280.  The media database 282 stores individual content items, and data characterising the content of individual transmedia content data items, e.g. metadata and graph analytics for the individual content items.  The\nuser database 284 stores user data pertaining to users of the system 200, including user behaviour data defining how users have interacted with individual content items and subsets, and user preference data defining user indicated or derived preferences\nfor content items and subsets.\nFIG. 4 depicts an exemplary backend server device 230 on which the backend of the system 200 is implemented.  It will be appreciated that the backend 230 or functional components thereof may be implemented across several servers or other\ndevices.  The server device 230 includes the memory 301, processing circuitry 302 and a network interface 303.  The memory may be any combination of one or more databases, other long-term storage such as a hard disk drive or solid state drive, or RAM. \nAs described above, the memory 301 stores the transmedia content data items and associated linking data, which define time-ordered content links between the plurality of transmedia content data items.  The plurality of transmedia content data items are\narranged into linked transmedia content subsets comprising different groups of the transmedia content data items and different content links therebetween.  The processing circuitry 302 is in communication with the memory 301 and is configured to receive\ninstructions from a user device via the network interface to create new time-ordered content links between at least two of the plurality of transmedia content data items and modify 301 the linking data stored in the memory to include the new time-ordered\ncontent link.\nFIG. 5 depicts an exemplary frontend electronic user device 210 on which the frontend of system 200 is provisioned.  The user device 210 includes a memory 401, processing circuitry 402, network interface 403 and a user interface 404.  The user\ninterface 404 may comprise one or more of: a touch-sensitive input, such as a touch-sensitive display, a touchscreen, pointing device, keyboard, display device, audio output device, and a tablet/stylus.  The network interface 403 may be in wired or\nwireless communication with a network such as the Internet and, ultimately, the server device 230 depicted in FIG. 4.  The electronic device 210 receives user input at the user interface 404 and thereby communicates with the server device 230 via the\nnetwork interface 403 and network interface 303, which provides the processor 302 with instructions to create new time-ordered content links between the transmedia content data items in the memory 301.  The electronic device 210 may also provide\ninstructions to the server device 230 to delete or modify existing time-ordered content links and/or transmedia content data items from the memory.\nIt will be appreciated that the system may comprise multiple frontend electronic devices 210, each configured to receive user input and thereby communicate with the server device 230 and provide instructions to create, delete or modify\ntime-ordered content links between the transmedia content data items.  Thus, multiple electronic devices 210, each being accessed by a different user, are adapted to process common content links and content data items which are accessible to multiple\nusers.\nThe memory 301 of the server device 230 may also store user data items, which are associated with users of the system 200 and comprise user identification data, such as a username, password, email address, telephone number and other profile\ninformation.  The user data items may also comprise, for each user of the system user preference data pertaining to each user's preferences, user behaviour data pertaining to the each user's online behaviours, user interaction data pertaining to the each\nuser's interaction with other users, and/or user location data pertaining to the current determined and/or past determined location of the each user.\nThe server device 230 may also be configured to implement the user model 234 of the system 200 as mentioned above as described below in further detail with respect to FIG. 9.  The processing circuitry 302 of the device 230 can use the user model\n234 to identify user interactions of the users of the system 200 with the transmedia content data items and subsequently update the user interaction data stored in the memory 301 in accordance with the user interaction.\nThe memory 301 may also store content characterisation data items, which characterise one or more of the transmedia content data items.  In particular, the memory 301 may store auditory characterisation data which characterises auditory\ncomponents the transmedia content data items, visual characterisation data which characterises visual components of the transmedia content data items, textual characterisation data which characterises textual components of the transmedia content data\nitems and/or interaction characterisation data which characterises interactive components, such as games, or quizzes, or puzzles, of the one or more transmedia content data items.  The processing circuitry 303 of the server device 230 can be further\nconfigured to provide a content subset modelling engine that processes the content characterisation data items for each transmedia content data item in a given transmedia content subset and generates unique subset characterisation data for the transmedia\ncontent subset based on the processed characterisation data.  The content subset modelling engine may be provided by the story model component 236 mentioned above.\nThe processing circuitry 302 may also implement the transmedia content recommender engine 238, mentioned above and described below in more detail with respect to FIG. 7.  The recommender engine 238 is configured to process the characterisation\ndata items and the user data items for a given user and identify transmedia content data items and surface identification(s) of one or more of the transmedia content data items that are predicted to be matched to users, and additionally can surface\nidentification(s) of other matched users of the system 200.\nThe processing circuitry 302 of the server device is also configured to implement the experience control layer 232 mentioned above.  The experience control layer 232 implements a permission control system which is used to determine whether a\ngiven user has permission to view, edit, modify or delete a given transmedia content data item, time-ordered content like, transmedia content subset or non-linear network.  Collaboration is a challenge in itself; however, authorship attribution and\nconsistency in particular are supported.  A balance is then provided between a very rigid and tight permission system, which might hinder collaboration and discourage potential contributors from sharing their ideas, and an open system which allows any\nuser to modify or delete the content contributed by other users.\nFor a given transmedia content subset or non-linear network, created by an original first user (referenced hereinafter as \"Alice\"), and which consists of individual transmedia content data items connected by time-ordered content links\ntherebetween, this transmedia content subset or non-linear network is attributed to and owned by Alice in metadata associated with the transmedia content data items, linking data and the original transmedia content subset and/or non-linear network\nexclusively.  The experience control layer 232 only allows modifications to the original transmedia content subset or non-linear network by Alice.  The system 200 is also configured such that a system administrator user or moderator user can provide or\nchange permissions for and between all individual users, regardless of permissions assigned by individual users.\nA second user (referenced hereinafter as \"Bob\") may wish to contribute to the transmedia content subset or non-linear network.  Bob may wish to insert a new transmedia content data item into the transmedia content subset or non-linear network,\nand/or Bob may wish to create new linking data that defines an alternative path through the non-linear network or transmedia content subset.\nThe experience control layer 232 does not permit Bob to modify the original transmedia content subset or non-linear network that are attributed to and owned by Alice in the metadata.  Instead, the experience control layer 232 instructs the\nprocessor 302 to create a copy of the original transmedia content subset or non-linear network in the memory 301, which includes the changes to the transmedia content data items and/or linking data input by Bob.\nThe copy will be exclusively owned by Bob in the metadata associated with the copied transmedia content subset or non-linear network, and as such the experience control layer 232 will permit only Bob to edit or modify the copied transmedia\ncontent subset or non-linear network.  However, the original transmedia content data items and linking data contributed by Alice remain attributed to Alice in the metadata, and only the new transmedia content data items and linking data are attributed to\nBob in the metadata.  The copied transmedia content subset or non-linear network maintains a reference to the original transmedia content subset or non-linear network.\nAs Bob interacts with the content by creating, modifying and editing content items, subsets and non-linear networks, it updates in real time such that all other users can see the changes as they happen, including Alice.\nIn an alternative embodiment, Bob can interact with the content by creating, modifying and editing content items, subsets and non-linear networks so that the changes at this stage can only be seen by Bob.  When Bob is finished and no longer\nwishes to modify the content, subsets or non-linear networks, he can formally submit his changes and the experience control layer 232 provides the copied transmedia content subset or non-linear network to the user, more particularly to the user device of\nthe user that is indicated by the metadata of the original transmedia content subset or non-linear network as the owner, e.g. Alice's user device, for review.  Alice may then choose to \"merge\" the copied transmedia content subset or non-linear network\nwith the original.  The experience control layer 232 will delete the original and modify the metadata of the copy to indicate that Alice is owner of the copied transmedia content subset or non-linear network, since Bob will have previously been\ndesignated in the metadata as owner of any items, subsets or networks which were created or modified by him.  The metadata of the individual transmedia content items and linking data, other than the owner, is left unchanged.\nAlice may approve of the modifications made by Bob, but may wish keep the modifications as an optional alternative.  In this case, she will choose to \"branch\" the original transmedia content subset or non-linear network.  The experience control\nlayer 232 will modify the original transmedia content subset or non-linear network to include any new transmedia content data items and linking data contributed by Bob.  The metadata of the individual transmedia content items and linking data is\nunchanged, and the metadata for the modified original transmedia content subset or non-linear network still identifies Alice as the owner.\nFinally, Alice may disapprove of the modification made by Bob, and can choose to \"push out\" Bob's version.  This causes the experience control layer 232 to remove the reference link from the copy to the original.  Again, the metadata of the\nindividual transmedia content items and linking data is unchanged.  This means that Bob's version of the content items, subset and non-linear networks as a result of his creation and/or modifications are now distinct from Alice's, and exists separately\nin memory with corresponding metadata to indicate that he is the owner of this version.\nThe system 200 and experience control layer 232 may allow Alice to fully delete Bob's edit, or force the modification to be hidden from other users of the system.  Allowing for this option might be required in some cases, for example when\ncopyright is infringed or the content contributed by Bob is inappropriate.\nAs mentioned above, the system 200 structures groups of content subsets (\"storyworlds\"), i.e. non-linear networks, as directed graphs of connected transmedia story data items and storylines, i.e. transmedia content subsets, as sequences of\nconnected transmedia content data items out of the generated non-linear networks graphs.\nThe story model component 236 of the system 200 arranges the stories and storyworlds at the transmedia content level based on complex graph metrics.  The transmedia content data items are nodes.  Edges of the graph define the time-ordered\ncontent links between transmedia content data items.  The edges and nodes of the graph may be assigned with weights derived from story attributes, e.g. number of likes received by users consuming the story.  The graph-based model defines all of the\npossible consumption flows throughout a given graph and allows identification of transmedia content data items which play key roles within the storyworld.  FIG. 6 depicts a graph-model of two non-linear networks of transmedia content data items.\nEach depicted non-linear network, 510 and 520 includes at least two subset entry points 511 and 521 which define starting points in the subset (and also any non-linear networks that the subsets are part of) from which the system should begin\npresenting the transmedia content data items.  Non-linear network 511 has two distinct end points 513, 516, which are transmedia content data items that have only incoming time-ordered content links.  End point 513 is preceded by a branch 512 of the\nnon-linear network, which shares only its first data item in common with a second brand 514.  Branch 514 has an alternate path 515 which skips several of the transmedia content data items of the rest of the branch and then re-joins the branch 515 to\nterminate at the end point 516.  In contrast, non-linear network 520 has four branches with four distinct end points 523, 525, 526 and 527, which share varying numbers of transmedia content data items in common with one another.  The data thus generated\nand stored which is representative of the non-linear network is structural data indicative of the nodes and edges and links therebetween, thereby defining the time-ordered structure of content data items in for the non-linear network.\nIn a hierarchical system containing a lot of changing content at different levels (story systems, non-linear networks, transmedia content subsets and individual transmedia content data items) users can easily get lost in irrelevant content (for\nthe user) or unappealing content (for the user).  An engine guiding the user and fostering the creation of high quality content is thus provided.\nAs mentioned above, the system 200 further includes a recommender engine component 238 in the backend device 230, which principally provides one or both of two functions namely: (i) surface relevant transmedia content data items, whether as\nindividual items, or as linked content subsets to a given users of the system; and (ii) surface relevant users and their content to the given user.  References herein to the \"given\" user mean a user who is logged into the system 200 via their frontend\ndevice 210 and is accessing data, including content items and user data from the backend device 230.\nFIG. 7 shows data flows between some components of the backend device 230, in particular, the inputs and outputs of the recommender engine component 234 implemented in the backend device 230.\nAs mentioned above, the recommender engine component 238 permits users via the frontend device 210 to receive suggestions about possible story elements, i.e. transmedia content data items, to be consumed and/or extended with new content by the\ngiven user.  Due to the hierarchic nature of the system, recommendations can be issued at one or more different levels of granularity, e.g. story system, non-linear networks, transmedia content subsets and individual transmedia content data items. \nFurthermore, recommendations are dynamic, i.e. they change with continuously evolving content.  Recommendations also take into account preferences of the given user to keep the user engaged with processing the time-arranged content items.  This can mean\nthat individual content items from the same or other users with sufficiently similar or identical content characteristic metadata, e.g. specifying content with sufficiently similar characters, occurring at sufficiently similar or identical factual or\nfictional times, or in sufficiently similar or identical factual or fictional times, as the characteristic metadata of the content which has already been utilised (consumed) by the user can be surfaced by the recommender engine component 238.\nIn general terms, the recommender engine component 238 is configured to access the memory 301 of the backend device 230 and surface one or more identifications of individual content items and/or linked transmedia content subsets to a given user\nof the system.  This is achieved with the identifications of such content being passed to experience control layer 232, as can be seen in FIG. 3 and FIG. 7.  In addition, the recommender engine component 238 can be configured to surface users relevant to\nthe given user with one or more identifications of other recommended users being passed to the experience control layer 232.  The experience control layer 232 obtains the received identifications and passes them to frontend device 210 for display in the\nuser interface 218 via the visual navigation component 214.\nThe surfaced content items or linked transmedia content subsets are selected by the recommender engine component 238 based on data about the individual transmedia content data items and transmedia content subsets already stored in the memory\n301.  In the present context, \"surface\" means that the selected one or more items of content or users are isolated from other items of content or users and provided as one or more identifications to the given user, e.g. as a notification on the user\ndevice, or as a special flag associated with the surfaced item.  This way, the given user can readily access the content and/or information and content associated with other users.\nThe recommender engine component 238 includes a preference model 242, the function of which is to provide a score of a given transmedia content data item or transmedia content subset for a specific user of the system 200.  The preference model\n242 is depicted in FIG. 8.\nReferring to FIG. 8, the preference model 242 takes as input, data about one or more transmedia content data items or transmedia content subsets and provides as output to an optimisation component 248 a score for each input item for a given\nuser.  The preference model 242 achieves this by, at a first step 601, removing global effects.  Some users might, for example, tend to constantly give lower ratings (user-assigned scores) than others.  Removing this bias before processing the input\nitems improves prediction accuracy.  In a second step 602, the model collects the predictions of n independent state of the art algorithms (such as Rank-based SVD).  The system then builds an ensemble prediction at step 603 by using a Lasso Regression. \nIn the last step 604, the global effects are added back to the ensemble prediction to obtain and output a final suitability score 605 of all input content and users in respect of the given user.\nThe recommender engine component 238 may also include a user-brand match component 244, which is configured to provide, for the given user, at least one prediction of a preference for a given branded content data item, and a branded content\nmodel 246 that provides, for a given transmedia content data item, a prediction of the suitability of a given branded content data item, e.g. an advertisement.\nAs shown in FIG. 7, the transmedia content recommender engine 238 additionally implements an optimisation component 248 which is configured to query at least the preference model component 242, and optionally the user-brand match component 244\nand brand model component 246 by providing them with a transmedia content parameter identifying items of content in use and/or a user identification for the given user.  The optimisation component 248 acts on the suitability scores it receives to\nmaximise content and/or user recommendations.  Reference here to \"content in use\" generally means the content currently selected and/or being manipulated by the given user in the user interface 218.\nFor the case of preference model data only, the optimisation component 248 outputs to the control layer 232 all identifications for recommended content (or linked subsets) and/or users which have a determined suitability score which sufficiently\nmeets a satisfactory threshold particular to the given user or content currently being utilised by the given user.  By meeting a particular threshold, this may mean that the suitability score has to be above, or below a particular fixed vale, dependant\non whether the score is arranged to be ascending or descending respectively in respect of suitability.  Alternatively, the optimisation component 248 can rank suitability scores (either for all content or all content which sufficiently meets a threshold)\nand then output a predetermined number of different content or users identifications down from the top-ranked scores.  For example, the optimisation component 248 may output identifications for transmedia content items (or linked content subsets) which\nbeen determined in respect of the given user to have the top X highest suitability scores (or lowest depending on whether scores are ascending or descending), as determined by the preference model 242, where X may be 5, 10, 20, 30, 40, 50 or 100.  This\none-dimensional optimisation ensures that the given user is engaged by relevant content, particularly in respect of linked content subsets.\nIn a further embodiment, a three-dimensional optimisation can additionally take into account user-brand matching and brand modelling.  In this embodiment, the optimisation component 248 maximises the sum of the output for the preference model\ncomponent 242, user-brand match component 244 and brand model component 246 over the transmedia content parameter.  This three-dimensional optimisation can ensure that users are both engaged by relevant content, and the content being consumed content\nwhich is pertinent a brand relevant to the particular user or existing utilised content.\nThe recommender engine component 238 may also take into account a given user's defined preferences, historical behaviour and other predicted properties such as future user behaviour, or inferred properties such as an emotional state.  In order\nto achieve this, the recommender engine component 238 communicates with the user model component 234 depicted in FIG. 3 and shown in more detail in FIG. 9.\nFIG. 9 depicts data flows between some components of the backend device 230 in the context of the user model component 234, in particular, the input and output components of the user model component 234, many of which are also the input and\noutput components of the recommender engine component 238.  Referring to FIG. 9, the user model component 234, 234 includes a state model 701, behaviour model 702, user profile 703, interaction model 704 and talent harvest component 705.\nModelling the state of the user, using state model component 701 permits personalised recommendations to be provided by the recommender engine component 238, and also provides accurate predictions of user behaviour by the behaviour model\ncomponent 702.  The state model component 701 may also be used to customise the user interface 218 and encourage users to create new content, i.e. new transmedia content data items and time-ordered content links, within the system 200.  The state model\ncomponent 701 represents and predicts the continuous creational, emotional and affective state of users of the system 200.  The creational state describes if the user is in the mood for consuming transmedia content subsets or non-linear networks or\ncontributing their own transmedia content data items and time-ordered content links.  The affective state indicates whether the user is, for example, motivated or bored.  The emotional state describes the emotions that individual transmedia content data\nitems or transmedia content subsets/non-linear networks trigger in the user.\nDue to the hierarchy of the system, i.e. the logical separation of transmedia content into levels of the individual transmedia content data items, transmedia content subsets, non-linear networks and story universe, user behaviour is predicted at\ndifferent levels of granularity in two main dimensions, namely: (1) the transmedia content hierarchy; and (2) the interaction of users with other users.  User dynamics are of interest at different levels of user granularity, for example in respect of:\nsingle users, small groups, audiences.  The behaviour model component 702 predicts the user behaviour in both of these dimensions and provides insights into dynamic behaviour for load balancing, information about the most likely path of a user through a\ngiven non-linear network and predicts the evolution of the whole system 200.\nThe user preference component 703 provides a detailed profiling for each user of the system 200, including, for example, demographics (e.g. age, gender, etc.), personality traits (e.g. Big 5) and location data (e.g. GPS).\nThe interaction model component 704 monitors and predicts social interactions of users.  The analysis and prediction of social interactions of groups of contributors can be used by the system 200 to encourage collaboration across all levels of\nthe system, predict the quality of stories and influence user community building.\nTalent harvest component 705 identifies users of the system 200 with exceptional talent at creating transmedia content, and categorises these users according to their type of talent, e.g. a Star Wars expert, an artist doing funny comic strips or\na user resonating with a specific audience.\nAs shown in FIG. 9, the user model component 234 is in communication with the story model component 236, the recommender engine component 238 and the metadata extraction component 240, with these components being both inputs and outputs to the\nuser model component 234, allowing data exchange therebetween.\nThe state model component 701 within the user model predicts or infers the state of a given user from (noisy) observations of the user interacting with the system 200.  The state model component 701 uses a statistical model, such as a hidden\nMarkov model to infer a creational state of the user (i.e. on a scale from creative to consumptive; see FIG. 1), the user's emotional state (e.g. sad, happy, excited, etc.) and the affective state of the user (e.g. engaged with the system 200, bored,\netc.).  The state model component 701 further classifies these continuous states into a set of discrete states for further processing (or similar).  The observed variables include the profile of the user (e.g. personality), the history of interactions of\nthe user with the system, properties of stories/story elements the user is actually consuming or creating, the environment of the user (e.g. office) and temporal metrics (e.g. working day versus weekends).\nThe state model component 701 represents user behaviour across different scales/levels of the system (i.e. individual transmedia content data items, transmedia content sub-sets or non-linear networks) by creating a hierarchical, graph-based user\nmodel that refines modelled action and states with every level.  User behaviour is modelled across different levels of granularity: crowd (e.g. predict behaviour of large numbers of users), community (e.g. defined, sorted groups of users), and at an\nindividual level.\nIn one embodiment of the invention, the user state and actions are represented by a Hidden Markov Model 900, depicted in FIG. 11.  In this case, the latent variable 901 represents the creational state of the user.  The observed variable 902\nrepresents the actions of the user, such as \"consuming a transmedia content data item\", \"rating a transmedia content subset\", \"creating a new transmedia content data item\", etc. The creational state changes dynamically over time and can be inferred from\nthese observed user actions.  As a Dynamic Bayesian Network (DBN), this representation can be extended to include the engagement and emotional state of the user as well as the interactions between users.\nNavigating through a large quantity of transmedia content data items, transmedia content subsets and non-linear networks that are provided to users by the system 200, in a way that users and user groups can create and consume the data quickly on\na range of devices, including personal computers, laptops, tablets, mobile devices etc. is challenging.  The user interface component 218 guides the user in a non-disruptive way, whilst also avoiding repetitive meandering and visual overload of content\ncreation and navigation tools on the multiple, hierarchical levels of the transmedia content.\nThe user interface component 218 presents the transmedia content data items, transmedia content subsets and non-linear networks as follows.  A three-dimensional representation can be utilised based on one or more three-dimensional shapes which\ncan be manipulated by the user.  In a two-dimensional system involving a two-dimensional display screen, a two-dimensional representation of the three-dimensional shape(s) is/are generated, and the shape(s) utilised may be one or more spheres or\nellipsoids, which use hierarchically-connected visual metaphors to provide further information on the transmedia content data items and how they are related and connected in a time-based manner for users and groups of users.  This is achieved in a\nnon-disruptive and non-distracting manner.  It will be appreciated that any three-dimensional object may be used to present the transmedia content.  In one embodiment of the present invention, the visual metaphors can equate to features of a planet, such\nas continental landmasses, oceans, clouds, mountain ranges and coastlines.\nFIGS. 10A and 10B depict an example of the user interface that is presented to a user of the system 200 at different levels of the hierarchical tree structure.  FIG. 10A depicts a higher-level view of the transmedia content 800 which depicts\nseveral spheres 801, 802.  Each sphere 801, 802 represents a storyworld, i.e. groups of transmedia content subsets and non-linear networks that are semantically similar, e.g. the constituent transmedia content data items relate to the same story\ncharacters or story universe.  The spheres themselves may be visually clustered together in the displayed three-dimensional representation according to semantic similarity between the storyworlds.\nA user may select one of the spheres 801, 802, which causes the user interface 800 to transition to a modified user interface 810, which depicts the selected single sphere 811 with additional detail.  Additional surface features of the selected\nsphere 811 are displayed in user interface 810, such as individual transmedia content subsets or non-linear networks indicated by icons 813, and representative images of the content 814.  The visual metaphors are provided such that semantically similar\ntransmedia content subsets and non-linear networks are depicted on the same continents 812 on the surface of the planet 811.  When a user wishes to consume or edit an individual transmedia content subset or non-linear network, the user can select one of\nthe icons 813 or images 814 and the user interface 810 transitions to show a graph-structure of the subset/network and/or the individual transmedia content data items.\nFIG. 12 depicts a method for surfacing transmedia content to a given user of the system 200 from the memory 301 of the server device 230.  At step 1201, the server device 230 receives, e.g. via the network interface, user input indicative of a\ntime-ordering between at least two user-selected transmedia content items.  At step 1202, the processing circuitry 302 of the server device 230 generates, using a transmedia content linking engine, the content linking data for storage in the memory 301\nthereby defining a linked transmedia content subset including the at least two user-selected transmedia content items.  At step 1203, the processing circuitry 302 accesses the memory 301 and surfaces transmedia content items, linked transmedia content,\nidentifications of identified users other than the given user and/or the content items associated with at least one identified user other than the given user.  The surfaced content items and/or the surfaced linked transmedia content subsets are surfaced\nfor selection by the given user via the transmedia content linking engine as one or more user-selectable transmedia content items.\nWhile some exemplary embodiments of the present disclosure have been shown in the drawings and described herein, it will be appreciated that the methods described herein may be deployed in part or in whole through a computing apparatus that\nexecutes computer software, program codes, and/or instructions on processing circuitry, which may be implemented by or on one or more discrete processors.  As a result, the claimed electronic device, apparatus and system can be implemented via computer\nsoftware being executed by the processing circuitry.  The present disclosure may be implemented as a method in a system, or on an apparatus or electronic device, as part of or in relation to the apparatus or device, or as a computer program product\nembodied in a computer readable medium executable on one or more apparatuses or electronic devices.\nA processor as disclosed herein may be any kind of computational or processing device capable of executing program instructions, codes, binary instructions and the like.  The processor may be or may include a signal processor, digital processor,\nembedded processor, microprocessor or any variant such as a co-processor (math co-processor, graphic co-processor, communication co-processor and the like) and the like that may directly or indirectly facilitate execution of program code or program\ninstructions stored thereon.  Each processor may be realized as one or more microprocessors, microcontrollers, embedded microcontrollers, programmable digital signal processors or other programmable device, along with internal and/or external memory.  A\ngiven processor may also, or instead, be embodied as an application specific integrated circuit, a programmable gate array, programmable array logic, or any other device or combination of devices that may be configured to process electronic signals.  In\naddition, each processor may enable execution of multiple programs, threads, and codes.  The threads may be executed simultaneously to enhance the performance of the processor and to facilitate simultaneous operations of the application.  By way of\nimplementation, the methods, program codes, program instructions and the like described herein may be implemented in one or more thread.  The thread may spawn other threads that may have assigned priorities associated with them; the processor may execute\nthese threads based on priority or any other order based on instructions provided in the program code.\nEach processor may access one or more memories, for example one or more non-transitory storage media which store the software, executable code, and instructions as described and claimed herein.  A storage medium associated with the processor for\nstoring methods, programs, codes, program instructions or other type of instructions capable of being executed by the computing or processing device may include but may not be limited to one or more of a CD-ROM, DVD, memory, hard disk, flash drive, RAM,\nROM, cache and the like.\nThe methods and/or processes disclosed herein, and steps associated therewith, may be realized in hardware, software or a combination of hardware and software suitable for a particular application.  The hardware may include a general purpose\ncomputer and/or dedicated computing device or specific computing device or particular aspect or component of a specific computing device.\nThe computer executable code may be created using a structured programming language such as C, an object oriented programming language such as C++, or any other high-level or low-level programming language (including assembly languages, hardware\ndescription languages, and database programming languages and technologies) that may be stored, compiled or interpreted to run on one of the above devices, as well as heterogeneous combinations of processors, processor architectures, or combinations of\ndifferent hardware and software, or any other machine capable of executing program instructions.\nThus, in one aspect, the methods described above and combinations thereof may be embodied in computer executable code that, when executing on one or more computing devices, performs the steps thereof.  In another aspect, the methods may be\nembodied in a system that performs the steps thereof, and may be distributed across electronic devices in a number of ways, or all of the functionality may be integrated into a dedicated, standalone electronic device or other hardware.  In another\naspect, the means for performing the steps associated with the processes described above may include any of the hardware and/or software described above.\nThe use of the terms \"a\" and \"an\" and \"the\" and similar referents in the context of describing the disclosure (especially in the context of the following claims) is to be construed to cover both the singular and the plural, unless otherwise\nindicated herein or clearly contradicted by context.  The terms \"comprising\", \"having\", \"including\", and \"containing\" are to be construed as open-ended terms (i.e. meaning \"including, but not limited to\") unless otherwise noted.  Recitation of ranges of\nvalues herein are merely intended to serve as a shorthand method of referring individually to each separate value falling within the range, unless otherwise indicated herein, and each separate value is incorporated into the specification as if it were\nindividually recited herein.\nThe use of any and all examples, or exemplary language (e.g. \"such as\") provided herein, is intended merely to better illuminate the disclosure and does not pose a limitation on the scope of the disclosure unless otherwise claimed.\nThe present disclosure has been provided above by way of example only, and it will be appreciated that modifications of detail can be made within the scope of the claims which define aspects of the invention.", "application_number": "15276493", "abstract": " A recommender engine is configured to access memory and surface\n     transmedia content items; and/or linked transmedia content subsets;\n     and/or one or more identifications of identified users; and/or content\n     items of the plurality of transmedia content items associated with at\n     least one identified user. The surfaced items are presented for selection\n     by the given user via the transmedia content linking engine as one or\n     more user-selected transmedia content items.\n", "citations": ["8918404", "20100293576", "20120290950", "20150081695", "20160050446", "20160065577", "20170091336"], "related": []}, {"id": "20180114329", "patent_code": "10375266", "patent_name": "Systems and methods for selecting an action based on a detected person", "year": "2019", "inventor_and_country_data": " Inventors: \nWexler; Yonatan (Jerusalem, IL), Shashua; Amnon (Mevaseret Zion, IL)  ", "description": "<BR><BR>BACKGROUND\nTechnical Field\nThis disclosure generally relates to devices and methods for capturing and processing images from an environment of a user, and using information derived from captured images.  More particularly, this disclosure relates to devices and methods\nfor selection of a device action based on a detected person.\nBackground Information\nToday, technological advancements make it possible for wearable devices to automatically capture images and store information that is associated with the captured images.  Certain devices have been used to digitally record aspects and personal\nexperiences of one's life in an exercise typically called \"lifelogging.\" Some individuals log their life so they can retrieve moments from past activities, for example, social events, trips, etc. Lifelogging may also have significant benefits in other\nfields (e.g., business, fitness and healthcare, and social research).  Lifelogging devices, while useful for tracking daily activities, may be improved with capability to enhance one's interaction in his environment with feedback and other advanced\nfunctionality based on the analysis of captured image data.\nEven though users can capture images with their smartphones and some smartphone applications can process the captured images, smartphones may not be the best platform for serving as lifelogging apparatuses in view of their size and design. \nLifelogging apparatuses should be small and light, so they can be easily worn.  Moreover, with improvements in image capture devices, including wearable apparatuses, additional functionality may be provided to assist users in navigating in and around an\nenvironment, identifying persons and objects they encounter, and providing feedback to the users about their surroundings and activities.  Therefore, there is a need for apparatuses and methods for automatically capturing and processing images to provide\nuseful information to users of the apparatuses, and for systems and methods to process and leverage information gathered by the apparatuses.\n<BR><BR>SUMMARY\nEmbodiments consistent with the present disclosure provide devices and methods for automatically capturing and processing images from an environment of a user, and systems and methods for processing information related to images captured from\nthe environment of the user.\nIn one embodiment, a wearable apparatus for causing an action to be executed based on a person being present in an environment of a user of the wearable apparatus may comprise a wearable image sensor configured to capture a plurality of images\nfrom the environment of the user of the wearable apparatus and at least one processing device.  The at least one processing device may be programmed to analyze at least one of the plurality of images to detect the person; analyze at least one of the\nplurality of images to identify an attribute of the detected person; and select at least one category for the detected person based on the identified attribute.  The at least one processing device may be further programmed to select at least one action\nbased on the at least one category and cause the at least one selected action to be executed.\nIn one embodiment, a method is provided for causing an action to be executed based on a person being present in an environment of a user of a wearable apparatus.  The method includes receiving a plurality of images captured by an image sensor of\nthe wearable apparatus from the environment of the user of the wearable apparatus; analyzing at least one of the plurality of images to detect the person; analyzing at least one of the plurality of images to identify an attribute of the detected person;\nselecting at least one category for the detected person based on the identified attribute; selecting at least one action based on the at least one category; and causing the at least one selected action to be executed.\nIn one embodiment, a wearable apparatus for causing an action to be executed based on whether a person is physically present in an environment of a user of the wearable apparatus may comprise a wearable image sensor configured to capture a\nplurality of images from the environment of the user of the wearable apparatus and at least one processing device.  The at least one processing device may be programmed to analyze at least one of the plurality of images to detect the person and analyze\nat least one of the plurality of images to determine whether the detected person is physically present in the environment of the user.  The at least one processing device may be further programmed to select at least one action based on whether the\ndetected person is physically present in the environment of the user and cause the selected at least one action to be executed.\nIn one embodiment, a wearable apparatus for causing an action to be executed based on whether a person is visible on a display of a device in an environment of a user of the wearable apparatus may comprise a wearable image sensor configured to\ncapture a plurality of images from the environment of the user of the wearable apparatus and at least one processing device.  The at least one processing device may be programmed to analyze at least one of the plurality of images to detect the person and\nanalyze at least one of the plurality of images to determine whether the detected person is visible on the display of the device.  The at least one processing device may be further programmed to select at least one action based on whether the detected\nperson is visible on the display of the device and cause the at least one action to be executed.\nIn one embodiment, a wearable apparatus for causing an action to be executed based on whether a person is physically present in an environment of a user of the wearable apparatus may comprise a wearable image sensor configured to capture a\nplurality of images from the environment of the user of the wearable apparatus and at least one processing device.\nThe at least one processing device may be programmed to analyze at least one of the plurality of images to detect the person and analyze at least one of the plurality of images to determine whether the detected person is physically present in\nthe environment of the user or whether a graphical representation of the detected person appears in the environment of the user.  The at least one processing device may be further programmed to select a first action after the determination is made that\nthe detected person is physically present in the environment of the user, select a second action different from the first action after the determination is made that the graphical representation of the detected person appears in the environment of the\nuser, and cause the first action or the second action to be executed.\nIn one embodiment, a method for causing an action to be executed based on whether a person is physically present in an environment of a user of the wearable apparatus may comprise receiving a plurality of images of the environment of the user\nfrom an image sensor of the wearable apparatus, analyzing at least one of the plurality of images to detect the person, and analyzing at least one of the plurality of images to determine whether the detected person is physically present in the\nenvironment of the user.  The method may further comprise selecting at least one action based on whether the detected person is physically present in the environment of the user and causing the selected at least one action to be executed.\nIn one embodiment, a method for causing an action to be executed based on whether a person is visible on a display of a device in an environment of a user of the wearable apparatus may comprise receiving a plurality of images of the environment\nof the user from an image sensor of the wearable apparatus, analyzing at least one of the plurality of images to detect the person, and analyzing at least one of the plurality of images to determine whether the detected person is visible on the display\nof the device.  The method may further comprise selecting at least one action based on whether the detected person is visible on the display of the device and causing the at least one action to be executed.\nIn one embodiment, a method for causing an action to be executed based on whether a person is physically present in an environment of a user of the wearable apparatus may receiving a plurality of images of the environment of the user from an\nimage sensor of the wearable apparatus, analyzing at least one of the plurality of images to detect the person, and analyzing at least one of the plurality of images to determine whether the detected person is physically present in the environment of the\nuser or whether a graphical representation of the detected person appears in the environment of the user.  The method may further comprise selecting a first action after the determination is made that the detected person is physically present in the\nenvironment of the user, selecting a second action different from the first action after the determination is made that the graphical representation of the detected person appears in the environment of the user, and causing the first action or the second\naction to be executed.\nIn one embodiment, a system for updating profile information based on data collected by a wearable apparatus may comprise a database storing a plurality of profiles and at least one processing device.  The at least one processing device may be\nprogrammed to obtain identification information associated with a person detected in one or more images captured by a wearable image sensor included in the wearable apparatus and obtain, from the wearable apparatus, auxiliary information associated with\nthe detected person.  The at least one processing device may be further programmed to identify, in the database, a profile associated with the detected person based on the identification information and update the identified profile based on the\nauxiliary information.\nIn one embodiment, a method is provided for updating profile information based on data collected by a wearable apparatus.  The method may comprise obtaining identification information associated with a person detected in one or more images\ncaptured by a wearable image sensor included in the wearable apparatus and obtaining, from the wearable apparatus, auxiliary information associated with the detected person.  The method may further comprise identifying, in a database storing a plurality\nof profiles, a profile associated with the detected person based on the identification information and updating the identified profile based on the auxiliary information.\nIn one embodiment, a system is provided for providing information to a user of a wearable apparatus.  The system includes at least one processing device programmed to identify a person represented in at least one image captured by a wearable\nimage sensor included in the wearable apparatus, obtain information associated with the person represented in the at least one captured image, and obtain at least one affinity measurement representing a degree of a relationship between the user and the\nperson.  The at least one processing device is further programmed to determine, based on the at least one affinity measurement, an information level to be disclosed to the user of the wearable apparatus and provide, to the user of the wearable apparatus,\nthe information based on the information associated with the person and on the information level.\nIn one embodiment, a method is provided for providing information to a user of a wearable apparatus.  The method includes identifying a person represented in at least one image captured by a wearable image sensor included in the wearable\napparatus, obtaining information associated with the person represented in the at least one captured image, and obtaining at least one affinity measurement representing a degree of a relationship between the user and the person.  The method further\nincludes determining, based on the at least one affinity measurement, an information level to be disclosed to the user of the wearable apparatus and providing, to the user of the wearable apparatus, the information based on the information associated\nwith the person and on the information level.\nIn one embodiment, a wearable apparatus is provided for registering a verbal contract.  The wearable apparatus includes at least one image sensor configured to capture a plurality of images from an environment of a user of the wearable apparatus\nand at least one audio sensor configured to capture audio data from the environment of the user of the wearable apparatus.  The wearable apparatus also includes at least one processing device programmed to analyze the plurality of images to detect a\nperson in the environment of the user of the wearable apparatus, obtain identification information associated with the detected person, analyze at least a portion of the audio data to identify one or more words associated with the verbal contract and\nspoken by the user of the wearable apparatus or the detected person, and obtain at least one profile of the user.  The at least one processing device is also programmed to authenticate an identity of the user based on the at least one profile of the user\nand, based on the authentication of the identity of the user, register the verbal contract and the identification information associated with the detected person.\nIn another embodiment, a method is provided for registering a verbal contract.  The method includes analyzing a plurality of images captured by at least one image sensor from an environment of a user of a wearable apparatus to detect a person in\nthe environment of the user of the wearable apparatus, obtaining identification information associated with the detected person, and analyzing at least a portion of audio data captured by at least one audio sensor from the environment of the user of the\nwearable apparatus to identify one or more words associated with the verbal contract and spoken by the user of the wearable apparatus or the detected person.  The method also includes obtaining at least one profile of the user, authenticating an identity\nof the user based on the at least one profile of the user and, based on the authentication of the identity of the user, registering the verbal contract and the identification information associated with the detected person.\nIn one embodiment, a wearable apparatus for providing information to a user of the wearable apparatus is disclosed.  The apparatus may include at least one image sensor configured to capture a plurality of images from an environment of the user\nof the wearable apparatus, at least one communication device, and at least one processing device.  The processing device may be programmed to analyze at least one of the plurality of images to detect an object in the environment of the user of the\nwearable apparatus, determine a measurement of an estimated physical distance from the user to the object, and transmit, based on the measurement and using the at least one communication device, information related to the detected object.\nIn one embodiment, a method provides information to a user of a wearable apparatus.  The method may be performed by at least one image sensor, at least one communication device, and at least one processing device.  The method may include\ncapturing, via the at least one image sensor, a plurality of images from an environment of the user of the wearable apparatus, analyzing, via the at least one processing device, at least one of the plurality of images to detect an object in the\nenvironment of the user of the wearable apparatus, determining a measurement of an estimated physical distance from the user to the object, and transmitting based on the measurement and using the at least one communication device, information related to\nthe detected object.\nIn one embodiment, a system for providing recommendations based on images captured by a wearable apparatus is disclosed.  The system may include a wearable image sensor and at least one processing device.  The processing device may be programmed\nto analyze at least one image captured by the wearable image sensor included in the wearable apparatus from an environment of a user of the wearable apparatus, obtain information based on a result of the analysis of the at least one captured image,\ngenerate at least one contact recommendation for at least one new social network contact based on the obtained information, and provide the at least one contact recommendation to at least one of the user and at least one other person.\nIn one embodiment, a method provides recommendations based on images captured by a wearable apparatus.  The method may be performed by a wearable image sensor and at least one processing device.  The method may include analyzing at least one\nimage captured by the wearable image sensor included in the wearable apparatus from an environment of a user of the wearable apparatus, obtaining information based on a result of the analysis of the at least one captured image, generating at least one\ncontact recommendation for at least one new social network contact based on the obtained information, and providing the at least one contact recommendation to at least one of the user and at least one other person.\nIn one embodiment, a wearable apparatus is disclosed.  The apparatus may include at least one image sensor configured to capture a plurality of images from an environment of a user of the wearable apparatus, and at least one projector configured\nto emit a light pattern configured to visually indicate to the user of the wearable apparatus an active field of view of the at least one image sensor.\nIn one embodiment, a method provides visual feedback to a user of a wearable apparatus.  The method may include capturing, via at least one image sensor included in the wearable apparatus, a plurality of images from an environment of the user of\nthe wearable apparatus, activating at least one projector included in the wearable apparatus based at least on a visual trigger appearing in at least one of the plurality of images, and emitting, via the at least one projector, a light pattern configured\nto visually indicate to the user of the wearable apparatus an active field of view of the at least one image sensor.\nConsistent with other disclosed embodiments, non-transitory computer-readable storage media may store program instructions, which are executed by at least one processor and perform any of the methods described herein.\nThe foregoing general description and the following detailed description are exemplary and explanatory only and are not restrictive of the claims. <BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nThe accompanying drawings, which are incorporated in and constitute a part of this disclosure, illustrate various disclosed embodiments.  In the drawings:\nFIG. 1A is a schematic illustration of an example of a user wearing a wearable apparatus according to a disclosed embodiment.\nFIG. 1B is a schematic illustration of an example of the user wearing a wearable apparatus according to a disclosed embodiment.\nFIG. 1C is a schematic illustration of an example of the user wearing a wearable apparatus according to a disclosed embodiment.\nFIG. 1D is a schematic illustration of an example of the user wearing a wearable apparatus according to a disclosed embodiment.\nFIG. 2 is a schematic illustration of an example system consistent with the disclosed embodiments.\nFIG. 3A is a schematic illustration of an example of the wearable apparatus shown in FIG. 1A.\nFIG. 3B is an exploded view of the example of the wearable apparatus shown in FIG. 3A.\nFIG. 4A is a schematic illustration of an example of the wearable apparatus shown in FIG. 1B from a first viewpoint.\nFIG. 4B is a schematic illustration of the example of the wearable apparatus shown in FIG. 1B from a second viewpoint.\nFIG. 5A is a block diagram illustrating an example of the components of a wearable apparatus according to a first embodiment.\nFIG. 5B is a block diagram illustrating an example of the components of a wearable apparatus according to a second embodiment.\nFIG. 5C is a block diagram illustrating an example of the components of a wearable apparatus according to a third embodiment.\nFIG. 6 illustrates an exemplary embodiment of a memory containing software modules consistent with the present disclosure.\nFIG. 7 is a schematic illustration of an embodiment of a wearable apparatus including an orientable image capture unit.\nFIG. 8 is a schematic illustration of an embodiment of a wearable apparatus securable to an article of clothing consistent with the present disclosure.\nFIG. 9 is a schematic illustration of a user wearing a wearable apparatus consistent with an embodiment of the present disclosure.\nFIG. 10 is a schematic illustration of an embodiment of a wearable apparatus securable to an article of clothing consistent with the present disclosure.\nFIG. 11 is a schematic illustration of an embodiment of a wearable apparatus securable to an article of clothing consistent with the present disclosure.\nFIG. 12 is a schematic illustration of an embodiment of a wearable apparatus securable to an article of clothing consistent with the present disclosure.\nFIG. 13 is a schematic illustration of an embodiment of a wearable apparatus securable to an article of clothing consistent with the present disclosure.\nFIG. 14 is a schematic illustration of an embodiment of a wearable apparatus securable to an article of clothing consistent with the present disclosure.\nFIG. 15 is a schematic illustration of an embodiment of a wearable apparatus power unit including a power source.\nFIG. 16 is a schematic illustration of an exemplary embodiment of a wearable apparatus including protective circuitry.\nFIG. 17 illustrates an exemplary embodiment of a memory containing software modules for selecting an action based on a detected person consistent with the present disclosure.\nFIG. 18A is a schematic illustration of an example image captured by a wearable apparatus consistent with an embodiment of the present disclosure.\nFIG. 18B is a schematic illustration of another example image captured by a wearable apparatus consistent with an embodiment of the present disclosure.\nFIG. 19 is a flowchart of an example method for selecting an action based on a detected person consistent with the present disclosure.\nFIG. 20 illustrates an exemplary embodiment of a memory containing software modules for selecting an action based on a detected person consistent with the present disclosure.\nFIG. 21A is a schematic illustration of an example image captured by a wearable apparatus consistent with an embodiment of the present disclosure.\nFIG. 21B is a schematic illustration of another example image captured by a wearable apparatus consistent with an embodiment of the present disclosure.\nFIG. 22A is a flowchart of an example method for causing execution of an action based on physical presence of a detected person consistent with the present disclosure.\nFIG. 22B is a flowchart of an example method for causing execution of an action based on whether a detected person is visible on a display consistent with the present disclosure.\nFIG. 22C is a flowchart of another example method for causing execution of an action based on physical presence of a detected person consistent with the present disclosure.\nFIG. 23 illustrates an exemplary embodiment of a memory containing software modules for updating profile information based on data collected by a wearable apparatus consistent with the present disclosure.\nFIG. 24 is a schematic illustration of a profile stored in a database, consistent with an embodiment of the present disclosure.\nFIG. 25 is a flowchart of an example method for updating profile information based on data collected by a wearable apparatus consistent with the present disclosure.\nFIG. 26 illustrates an exemplary embodiment of a memory containing software modules consistent with the present disclosure.\nFIG. 27 shows an example environment including a wearable apparatus for capturing and processing images.\nFIG. 28A is a flowchart illustrating an exemplary method for identifying a person and information associated with the person.\nFIG. 28B is a flowchart illustrating an exemplary method for determining a level of detail of information relating to the identified person and provided to the user.\nFIG. 29 illustrates an exemplary embodiment of a memory containing software modules consistent with the present disclosure.\nFIG. 30 shows an example environment including a wearable apparatus for capturing and processing images and audio data.\nFIG. 31A is a flowchart illustrating an exemplary method for analyzing image and audio data captured by a wearable device.\nFIG. 31B is a flowchart illustrating an exemplary method for registering a verbal contract based on an analysis of captured image and audio data.\nFIG. 31C is a flowchart illustrating an exemplary method for registering information related to a witness to a verbal contract.\nFIG. 32 illustrates an exemplary embodiment of a memory containing software modules consistent with the present disclosure.\nFIG. 33A is a schematic illustration of an example of a user wearing a wearable apparatus and capturing an image of a person according to a disclosed embodiment.\nFIG. 33B is a schematic illustration of an example of an update of a social network profile according to a disclosed embodiment.\nFIG. 33C is a schematic illustration of an example of a user wearing a wearable apparatus and capturing an image of an object according to a disclosed embodiment.\nFIG. 33D is a schematic illustration of an example of an update of a social network profile according to a disclosed embodiment.\nFIG. 34 is a flowchart of an example of a method for providing information to a user of a wearable apparatus.\nFIG. 35 illustrates an exemplary embodiment of a memory containing software modules consistent with the present disclosure.\nFIG. 36A is a schematic illustration of an example of a user wearing a wearable apparatus capturing an image of a person according to a disclosed embodiment.\nFIG. 36B is a schematic illustration of an example of a contact recommendation according to a disclosed embodiment.\nFIG. 36C is a schematic illustration of an example of a user wearing a wearable apparatus capturing an image of a person according to a disclosed embodiment.\nFIG. 36D is a schematic illustration of an example of a contact recommendation according to a disclosed embodiment.\nFIG. 37 is a flowchart of an example of a method for providing contact recommendations based on captured images to a user of a wearable apparatus.\nFIG. 38A is a diagrammatic view of an apparatus including a light projector.\nFIG. 38B is a diagrammatic view of a wearable apparatus securable to an article of clothing that includes a light projector consistent with the present disclosure.\nFIG. 39 is a diagrammatic illustration of one example of a type of visual feedback that the light projector shown in FIG. 38A may provide to a user.\nFIGS. 40A-40H are examples of various patterns that can be generated by the light projector of FIG. 38A and/or by the light projector of FIG. 38B.\nFIG. 41 is a flowchart of an example of a method for providing visual feedback to a user of a wearable apparatus.\n<BR><BR>DETAILED DESCRIPTION\nThe following detailed description refers to the accompanying drawings.  Wherever possible, the same reference numbers are used in the drawings and the following description to refer to the same or similar parts.  While several illustrative\nembodiments are described herein, modifications, adaptations and other implementations are possible.  For example, substitutions, additions or modifications may be made to the components illustrated in the drawings, and the illustrative methods described\nherein may be modified by substituting, reordering, removing, or adding steps to the disclosed methods.  Accordingly, the following detailed description is not limited to the disclosed embodiments and examples.  Instead, the proper scope is defined by\nthe appended claims.\nFIG. 1A illustrates a user 100 wearing an apparatus 110 that is physically connected (or integral) to glasses 130, consistent with the disclosed embodiments.  Glasses 130 may be prescription glasses, magnifying glasses, non-prescription glasses,\nsafety glasses, sunglasses, etc. Additionally, in some embodiments, glasses 130 may include parts of a frame and earpieces, nosepieces, etc., and one or no lenses.  Thus, in some embodiments, glasses 130 may function primarily to support apparatus 110,\nand/or an augmented reality display device or other optical display device.  In some embodiments, apparatus 110 may include an image sensor (not shown in FIG. 1A) for capturing real-time image data of the field-of-view of user 100.  The term \"image data\"\nincludes any form of data retrieved from optical signals in the near-infrared, infrared, visible, and ultraviolet spectrums.  The image data may include video clips and/or photographs.\nIn some embodiments, apparatus 110 may communicate wirelessly or via a wire with a computing device 120.  In some embodiments, computing device 120 may include, for example, a smartphone, or a tablet, or a dedicated processing unit, which may be\nportable (e.g., can be carried in a pocket of user 100).  Although shown in FIG. 1A as an external device, in some embodiments, computing device 120 may be provided as part of wearable apparatus 110 or glasses 130, whether integral thereto or mounted\nthereon.  In some embodiments, computing device 120 may be included in an augmented reality display device or optical head mounted display provided integrally or mounted to glasses 130.  In other embodiments, computing device 120 may be provided as part\nof another wearable or portable apparatus of user 100 including a wrist-strap, a multifunctional watch, a button, a clip-on, etc. And in other embodiments, computing device 120 may be provided as part of another system, such as an on-board automobile\ncomputing or navigation system.  A person skilled in the art can appreciate that different types of computing devices and arrangements of devices may implement the functionality of the disclosed embodiments.  Accordingly, in other implementations,\ncomputing device 120 may include a Personal Computer (PC), laptop, an Internet server, etc.\nFIG. 1B illustrates user 100 wearing apparatus 110 that is physically connected to a necklace 140, consistent with a disclosed embodiment.  Such a configuration of apparatus 110 may be suitable for users that do not wear glasses some or all of\nthe time.  In this embodiment, user 100 can easily wear apparatus 110, and take it off.\nFIG. 1C illustrates user 100 wearing apparatus 110 that is physically connected to a belt 150, consistent with a disclosed embodiment.  Such a configuration of apparatus 110 may be designed as a belt buckle.  Alternatively, apparatus 110 may\ninclude a clip for attaching to various clothing articles, such as belt 150, or a vest, a pocket, a collar, a cap or hat or other portion of a clothing article.\nFIG. 1D illustrates user 100 wearing apparatus 110 that is physically connected to a wrist strap 160, consistent with a disclosed embodiment.  Although the aiming direction of apparatus 110, according to this embodiment, may not match the\nfield-of-view of user 100, apparatus 110 may include the ability to identify a hand-related trigger based on the tracked eye movement of a user 100 indicating that user 100 is looking in the direction of the wrist strap 160.  Wrist strap 160 may also\ninclude an accelerometer, a gyroscope, or other sensor for determining movement or orientation of a user's 100 hand for identifying a hand-related trigger.\nFIG. 2 is a schematic illustration of an exemplary system 200 including a wearable apparatus 110, worn by user 100, and an optional computing device 120 and/or a server 250 capable of communicating with apparatus 110 via a network 240,\nconsistent with disclosed embodiments.  In some embodiments, apparatus 110 may capture and analyze image data, identify a hand-related trigger present in the image data, and perform an action and/or provide feedback to a user 100, based at least in part\non the identification of the hand-related trigger.  In some embodiments, optional computing device 120 and/or server 250 may provide additional functionality to enhance interactions of user 100 with his or her environment, as described in greater detail\nbelow.\nAccording to the disclosed embodiments, apparatus 110 may include an image sensor system 220 for capturing real-time image data of the field-of-view of user 100.  In some embodiments, apparatus 110 may also include a processing unit 210 for\ncontrolling and performing the disclosed functionality of apparatus 110, such as to control the capture of image data, analyze the image data, and perform an action and/or output a feedback based on a hand-related trigger identified in the image data. \nAccording to the disclosed embodiments, a hand-related trigger may include a gesture performed by user 100 involving a portion of a hand of user 100.  Further, consistent with some embodiments, a hand-related trigger may include a wrist-related trigger. \nAdditionally, in some embodiments, apparatus 110 may include a feedback outputting unit 230 for producing an output of information to user 100.\nAs discussed above, apparatus 110 may include an image sensor 220 for capturing image data.  The term \"image sensor\" refers to a device capable of detecting and converting optical signals in the near-infrared, infrared, visible, and ultraviolet\nspectrums into electrical signals.  The electrical signals may be used to form an image or a video stream (i.e. image data) based on the detected signal.  The term \"image data\" includes any form of data retrieved from optical signals in the\nnear-infrared, infrared, visible, and ultraviolet spectrums.  Examples of image sensors may include semiconductor charge-coupled devices (CCD), active pixel sensors in complementary metal-oxide-semiconductor (CMOS), or N-type metal-oxide-semiconductor\n(NMOS, Live MOS).  In some cases, image sensor 220 may be part of a camera included in apparatus 110.\nApparatus 110 may also include a processor 210 for controlling image sensor 220 to capture image data and for analyzing the image data according to the disclosed embodiments.  As discussed in further detail below with respect to FIG. 5A,\nprocessor 210 may include a \"processing device\" for performing logic operations on one or more inputs of image data and other data according to stored or accessible software instructions providing desired functionality.  In some embodiments, processor\n210 may also control feedback outputting unit 230 to provide feedback to user 100 including information based on the analyzed image data and the stored software instructions.  As the term is used herein, a \"processing device\" may access memory where\nexecutable instructions are stored or, in some embodiments, a \"processing device\" itself may include executable instructions (e.g., stored in memory included in the processing device).\nIn some embodiments, the information or feedback information provided to user 100 may include time information.  The time information may include any information related to a current time of day and, as described further below, may be presented\nin any sensory perceptive manner.  In some embodiments, time information may include a current time of day in a preconfigured format (e.g., 2:30 pm or 14:30).  Time information may include the time in the user's current time zone (e.g., based on a\ndetermined location of user 100), as well as an indication of the time zone and/or a time of day in another desired location.  In some embodiments, time information may include a number of hours or minutes relative to one or more predetermined times of\nday.  For example, in some embodiments, time information may include an indication that three hours and fifteen minutes remain until a particular hour (e.g., until 6:00 pm), or some other predetermined time.  Time information may also include a duration\nof time passed since the beginning of a particular activity, such as the start of a meeting or the start of a jog, or any other activity.  In some embodiments, the activity may be determined based on analyzed image data.  In other embodiments, time\ninformation may also include additional information related to a current time and one or more other routine, periodic, or scheduled events.  For example, time information may include an indication of the number of minutes remaining until the next\nscheduled event, as may be determined from a calendar function or other information retrieved from computing device 120 or server 250, as discussed in further detail below.\nFeedback outputting unit 230 may include one or more feedback systems for providing the output of information to user 100.  In the disclosed embodiments, the audible or visual feedback may be provided via any type of connected audible or visual\nsystem or both.  Feedback of information according to the disclosed embodiments may include audible feedback to user 100 (e.g., using a Bluetooth.TM.  or other wired or wirelessly connected speaker, or a bone conduction headphone).  Feedback outputting\nunit 230 of some embodiments may additionally or alternatively produce a visible output of information to user 100, for example, as part of an augmented reality display projected onto a lens of glasses 130 or provided via a separate heads up display in\ncommunication with apparatus 110, such as a display 260 provided as part of computing device 120, which may include an onboard automobile heads up display, an augmented reality device, a virtual reality device, a smartphone, PC, table, etc.\nThe term \"computing device\" refers to a device including a processing unit and having computing capabilities.  Some examples of computing device 120 include a PC, laptop, tablet, or other computing systems such as an on-board computing system of\nan automobile, for example, each configured to communicate directly with apparatus 110 or server 250 over network 240.  Another example of computing device 120 includes a smartphone having a display 260.  In some embodiments, computing device 120 may be\na computing system configured particularly for apparatus 110, and may be provided integral to apparatus 110 or tethered thereto.  Apparatus 110 can also connect to computing device 120 over network 240 via any known wireless standard (e.g., Wi-Fi,\nBluetooth.RTM., etc.), as well as near-filed capacitive coupling, and other short range wireless techniques, or via a wired connection.  In an embodiment in which computing device 120 is a smartphone, computing device 120 may have a dedicated application\ninstalled therein.  For example, user 100 may view on display 260 data (e.g., images, video clips, extracted information, feedback information, etc.) that originate from or are triggered by apparatus 110.  In addition, user 100 may select part of the\ndata for storage in server 250.\nNetwork 240 may be a shared, public, or private network, may encompass a wide area or local area, and may be implemented through any suitable combination of wired and/or wireless communication networks.  Network 240 may further comprise an\nintranet or the Internet.  In some embodiments, network 240 may include short range or near-field wireless communication systems for enabling communication between apparatus 110 and computing device 120 provided in close proximity to each other, such as\non or near a user's person, for example.  Apparatus 110 may establish a connection to network 240 autonomously, for example, using a wireless module (e.g., Wi-Fi, cellular).  In some embodiments, apparatus 110 may use the wireless module when being\nconnected to an external power source, to prolong battery life.  Further, communication between apparatus 110 and server 250 may be accomplished through any suitable communication channels, such as, for example, a telephone network, an extranet, an\nintranet, the Internet, satellite communications, off-line communications, wireless communications, transponder communications, a local area network (LAN), a wide area network (WAN), and a virtual private network (VPN).\nAs shown in FIG. 2, apparatus 110 may transfer or receive data to/from server 250 via network 240.  In the disclosed embodiments, the data being received from server 250 and/or computing device 120 may include numerous different types of\ninformation based on the analyzed image data, including information related to a commercial product, or a person's identity, an identified landmark, and any other information capable of being stored in or accessed by server 250.  In some embodiments,\ndata may be received and transferred via computing device 120.  Server 250 and/or computing device 120 may retrieve information from different data sources (e.g., a user specific database or a user's social network account or other account, the Internet,\nand other managed or accessible databases) and provide information to apparatus 110 related to the analyzed image data and a recognized trigger according to the disclosed embodiments.  In some embodiments, calendar-related information retrieved from the\ndifferent data sources may be analyzed to provide certain time information or a time-based context for providing certain information based on the analyzed image data.\nAn example of wearable apparatus 110 incorporated with glasses 130 according to some embodiments (as discussed in connection with FIG. 1A) is shown in greater detail in FIG. 3A.  In some embodiments, apparatus 110 may be associated with a\nstructure (not shown in FIG. 3A) that enables easy detaching and reattaching of apparatus 110 to glasses 130.  In some embodiments, when apparatus 110 attaches to glasses 130, image sensor 220 acquires a set aiming direction without the need for\ndirectional calibration.  The set aiming direction of image sensor 220 may substantially coincide with the field-of-view of user 100.  For example, a camera associated with image sensor 220 may be installed within apparatus 110 in a predetermined angle\nin a position facing slightly downwards (e.g., 5-15 degrees from the horizon).  Accordingly, the set aiming direction of image sensor 220 may substantially match the field-of-view of user 100.\nFIG. 3B is an exploded view of the components of the embodiment discussed regarding FIG. 3A.  Attaching apparatus 110 to glasses 130 may take place in the following way.  Initially, a support 310 may be mounted on glasses 130 using a screw 320,\nin the side of support 310.  Then, apparatus 110 may be clipped on support 310 such that it is aligned with the field-of-view of user 100.  The term \"support\" includes any device or structure that enables detaching and reattaching of a device including a\ncamera to a pair of glasses or to another object (e.g., a helmet).  Support 310 may be made from plastic (e.g., polycarbonate), metal (e.g., aluminum), or a combination of plastic and metal (e.g., carbon fiber graphite).  Support 310 may be mounted on\nany kind of glasses (e.g., eyeglasses, sunglasses, 3D glasses, safety glasses, etc.) using screws, bolts, snaps, or any fastening means used in the art.\nIn some embodiments, support 310 may include a quick release mechanism for disengaging and reengaging apparatus 110.  For example, support 310 and apparatus 110 may include magnetic elements.  As an alternative example, support 310 may include a\nmale latch member and apparatus 110 may include a female receptacle.  In other embodiments, support 310 can be an integral part of a pair of glasses, or sold separately and installed by an optometrist.  For example, support 310 may be configured for\nmounting on the arms of glasses 130 near the frame front, but before the hinge.  Alternatively, support 310 may be configured for mounting on the bridge of glasses 130.\nIn some embodiments, apparatus 110 may be provided as part of a glasses frame 130, with or without lenses.  Additionally, in some embodiments, apparatus 110 may be configured to provide an augmented reality display projected onto a lens of\nglasses 130 (if provided), or alternatively, may include a display for projecting time information, for example, according to the disclosed embodiments.  Apparatus 110 may include the additional display or alternatively, may be in communication with a\nseparately provided display system that may or may not be attached to glasses 130.\nIn some embodiments, apparatus 110 may be implemented in a form other than wearable glasses, as described above with respect to FIGS. 1B-1D, for example.  FIG. 4A is a schematic illustration of an example of an additional embodiment of apparatus\n110 from a first viewpoint.  The viewpoint shown in FIG. 4A is from the front of apparatus 110.  Apparatus 110 includes an image sensor 220, a clip (not shown), a function button (not shown) and a hanging ring 410 for attaching apparatus 110 to, for\nexample, necklace 140, as shown in FIG. 1B.  When apparatus 110 hangs on necklace 140, the aiming direction of image sensor 220 may not fully coincide with the field-of-view of user 100, but the aiming direction would still correlate with the\nfield-of-view of user 100.\nFIG. 4B is a schematic illustration of the example of a second embodiment of apparatus 110, from a second viewpoint.  The viewpoint shown in FIG. 4B is from a side orientation of apparatus 110.  In addition to hanging ring 410, as shown in FIG.\n4B, apparatus 110 may further include a clip 420.  User 100 can use clip 420 to attach apparatus 110 to a shirt or belt 150, as illustrated in FIG. 1C.  Clip 420 may provide an easy mechanism for disengaging and reengaging apparatus 110 from different\narticles of clothing.  In other embodiments, apparatus 110 may include a female receptacle for connecting with a male latch of a car mount or universal stand.\nIn some embodiments, apparatus 110 includes a function button 430 for enabling user 100 to provide input to apparatus 110.  Function button 430 may accept different types of tactile input (e.g., a tap, a click, a double-click, a long press, a\nright-to-left slide, a left-to-right slide).  In some embodiments, each type of input may be associated with a different action.  For example, a tap may be associated with the function of taking a picture, while a right-to-left slide may be associated\nwith the function of recording a video.\nThe example embodiments discussed above with respect to FIGS. 3A, 3B, 4A, and 4B are not limiting.  In some embodiments, apparatus 110 may be implemented in any suitable configuration for performing the disclosed methods.  For example, referring\nback to FIG. 2, the disclosed embodiments may implement an apparatus 110 according to any configuration including an image sensor 220 and a processor unit 210 to perform image analysis and for communicating with a feedback unit 230.\nFIG. 5A is a block diagram illustrating the components of apparatus 110 according to an example embodiment.  As shown in FIG. 5A, and as similarly discussed above, apparatus 110 includes an image sensor 220, a memory 550, a processor 210, a\nfeedback outputting unit 230, a wireless transceiver 530, and a mobile power source 520.  In other embodiments, apparatus 110 may also include buttons, other sensors such as a microphone, and inertial measurements devices such as accelerometers,\ngyroscopes, magnetometers, temperature sensors, color sensors, light sensors, etc. Apparatus 110 may further include a data port 570 and a power connection 510 with suitable interfaces for connecting with an external power source or an external device\n(not shown).\nProcessor 210, depicted in FIG. 5A, may include any suitable processing device.  The term \"processing device\" includes any physical device having an electric circuit that performs a logic operation on input or inputs.  For example, processing\ndevice may include one or more integrated circuits, microchips, microcontrollers, microprocessors, all or part of a central processing unit (CPU), graphics processing unit (GPU), digital signal processor (DSP), field-programmable gate array (FPGA), or\nother circuits suitable for executing instructions or performing logic operations.  The instructions executed by the processing device may, for example, be pre-loaded into a memory integrated with or embedded into the processing device or may be stored\nin a separate memory (e.g., memory 550).  Memory 550 may comprise a Random Access Memory (RAM), a Read-Only Memory (ROM), a hard disk, an optical disk, a magnetic medium, a flash memory, other permanent, fixed, or volatile memory, or any other mechanism\ncapable of storing instructions.\nAlthough, in the embodiment illustrated in FIG. 5A, apparatus 110 includes one processing device (e.g., processor 210), apparatus 110 may include more than one processing device.  Each processing device may have a similar construction, or the\nprocessing devices may be of differing constructions that are electrically connected or disconnected from each other.  For example, the processing devices may be separate circuits or integrated in a single circuit.  When more than one processing device\nis used, the processing devices may be configured to operate independently or collaboratively.  The processing devices may be coupled electrically, magnetically, optically, acoustically, mechanically or by other means that permit them to interact.\nIn some embodiments, processor 210 may process a plurality of images captured from the environment of user 100 to determine different parameters related to capturing subsequent images.  For example, processor 210 can determine, based on\ninformation derived from captured image data, a value for at least one of the following: an image resolution, a compression ratio, a cropping parameter, frame rate, a focus point, an exposure time, an aperture size, and a light sensitivity.  The\ndetermined value may be used in capturing at least one subsequent image.  Additionally, processor 210 can detect images including at least one hand-related trigger in the environment of the user and perform an action and/or provide an output of\ninformation to a user via feedback outputting unit 230.\nIn another embodiment, processor 210 can change the aiming direction of image sensor 220.  For example, when apparatus 110 is attached with clip 420, the aiming direction of image sensor 220 may not coincide with the field-of-view of user 100. \nProcessor 210 may recognize certain situations from the analyzed image data and adjust the aiming direction of image sensor 220 to capture relevant image data.  For example, in one embodiment, processor 210 may detect an interaction with another\nindividual and sense that the individual is not fully in view, because image sensor 220 is tilted down.  Responsive thereto, processor 210 may adjust the aiming direction of image sensor 220 to capture image data of the individual.  Other scenarios are\nalso contemplated where processor 210 may recognize the need to adjust an aiming direction of image sensor 220.\nIn some embodiments, processor 210 may communicate data to feedback-outputting unit 230, which may include any device configured to provide information to a user 100.  Feedback outputting unit 230 may be provided as part of apparatus 110 (as\nshown) or may be provided external to apparatus 110 and communicatively coupled thereto.  Feedback-outputting unit 230 may be configured to output visual or nonvisual feedback based on signals received from processor 210, such as when processor 210\nrecognizes a hand-related trigger in the analyzed image data.\nThe term \"feedback\" refers to any output or information provided in response to processing at least one image in an environment.  In some embodiments, as similarly described above, feedback may include an audible or visible indication of time\ninformation, detected text or numerals, the value of currency, a branded product, a person's identity, the identity of a landmark or other environmental situation or condition including the street names at an intersection or the color of a traffic light,\netc., as well as other information associated with each of these.  For example, in some embodiments, feedback may include additional information regarding the amount of currency still needed to complete a transaction, information regarding the identified\nperson, historical information or times and prices of admission etc. of a detected landmark etc. In some embodiments, feedback may include an audible tone, a tactile response, and/or information previously recorded by user 100.  Feedback-outputting unit\n230 may comprise appropriate components for outputting acoustical and tactile feedback.  For example, feedback-outputting unit 230 may comprise audio headphones, a hearing aid type device, a speaker, a bone conduction headphone, interfaces that provide\ntactile cues, vibrotactile stimulators, etc. In some embodiments, processor 210 may communicate signals with an external feedback outputting unit 230 via a wireless transceiver 530, a wired connection, or some other communication interface.  In some\nembodiments, feedback outputting unit 230 may also include any suitable display device for visually displaying information to user 100.\nAs shown in FIG. 5A, apparatus 110 includes memory 550.  Memory 550 may include one or more sets of instructions accessible to processor 210 to perform the disclosed methods, including instructions for recognizing a hand-related trigger in the\nimage data.  In some embodiments memory 550 may store image data (e.g., images, videos) captured from the environment of user 100.  In addition, memory 550 may store information specific to user 100, such as image representations of known individuals,\nfavorite products, personal items, and calendar or appointment information, etc. In some embodiments, processor 210 may determine, for example, which type of image data to store based on available storage space in memory 550.  In another embodiment,\nprocessor 210 may extract information from the image data stored in memory 550.\nAs further shown in FIG. 5A, apparatus 110 includes mobile power source 520.  The term \"mobile power source\" includes any device capable of providing electrical power, which can be easily carried by hand (e.g., mobile power source 520 may weigh\nless than a pound).  The mobility of the power source enables user 100 to use apparatus 110 in a variety of situations.  In some embodiments, mobile power source 520 may include one or more batteries (e.g., nickel-cadmium batteries, nickel-metal hydride\nbatteries, and lithium-ion batteries) or any other type of electrical power supply.  In other embodiments, mobile power source 520 may be rechargeable and contained within a casing that holds apparatus 110.  In yet other embodiments, mobile power source\n520 may include one or more energy harvesting devices for converting ambient energy into electrical energy (e.g., portable solar power units, human vibration units, etc.).\nMobile power source 520 may power one or more wireless transceivers (e.g., wireless transceiver 530 in FIG. 5A).  The term \"wireless transceiver\" refers to any device configured to exchange transmissions over an air interface by use of radio\nfrequency, infrared frequency, magnetic field, or electric field.  Wireless transceiver 530 may use any known standard to transmit and/or receive data (e.g., Wi-Fi, Bluetooth.RTM., Bluetooth Smart, 802.15.4, or ZigBee).  In some embodiments, wireless\ntransceiver 530 may transmit data (e.g., raw image data, processed image data, extracted information) from apparatus 110 to computing device 120 and/or server 250.  Wireless transceiver 530 may also receive data from computing device 120 and/or server\n250.  In other embodiments, wireless transceiver 530 may transmit data and instructions to an external feedback outputting unit 230.\nFIG. 5B is a block diagram illustrating the components of apparatus 110 according to another example embodiment.  In some embodiments, apparatus 110 includes a first image sensor 220a, a second image sensor 220b, a memory 550, a first processor\n210a, a second processor 210b, a feedback outputting unit 230, a wireless transceiver 530, a mobile power source 520, and a power connector 510.  In the arrangement shown in FIG. 5B, each of the image sensors may provide images in a different image\nresolution, or face a different direction.  Alternatively, each image sensor may be associated with a different camera (e.g., a wide angle camera, a narrow angle camera, an IR camera, etc.).  In some embodiments, apparatus 110 can select which image\nsensor to use based on various factors.  For example, processor 210a may determine, based on available storage space in memory 550, to capture subsequent images in a certain resolution.\nApparatus 110 may operate in a first processing-mode and in a second processing-mode, such that the first processing-mode may consume less power than the second processing-mode.  For example, in the first processing-mode, apparatus 110 may\ncapture images and process the captured images to make real-time decisions based on an identifying hand-related trigger, for example.  In the second processing-mode, apparatus 110 may extract information from stored images in memory 550 and delete images\nfrom memory 550.  In some embodiments, mobile power source 520 may provide more than fifteen hours of processing in the first processing-mode and about three hours of processing in the second processing-mode.  Accordingly, different processing-modes may\nallow mobile power source 520 to produce sufficient power for powering apparatus 110 for various time periods (e.g., more than two hours, more than four hours, more than ten hours, etc.).\nIn some embodiments, apparatus 110 may use first processor 210a in the first processing-mode when powered by mobile power source 520, and second processor 210b in the second processing-mode when powered by external power source 580 that is\nconnectable via power connector 510.  In other embodiments, apparatus 110 may determine, based on predefined conditions, which processors or which processing modes to use.  Apparatus 110 may operate in the second processing-mode even when apparatus 110\nis not powered by external power source 580.  For example, apparatus 110 may determine that it should operate in the second processing-mode when apparatus 110 is not powered by external power source 580, if the available storage space in memory 550 for\nstoring new image data is lower than a predefined threshold.\nAlthough one wireless transceiver is depicted in FIG. 5B, apparatus 110 may include more than one wireless transceiver (e.g., two wireless transceivers).  In an arrangement with more than one wireless transceiver, each of the wireless\ntransceivers may use a different standard to transmit and/or receive data.  In some embodiments, a first wireless transceiver may communicate with server 250 or computing device 120 using a cellular standard (e.g., LTE or GSM), and a second wireless\ntransceiver may communicate with server 250 or computing device 120 using a short-range standard (e.g., Wi-Fi or Bluetooth.RTM.).  In some embodiments, apparatus 110 may use the first wireless transceiver when the wearable apparatus is powered by a\nmobile power source included in the wearable apparatus, and use the second wireless transceiver when the wearable apparatus is powered by an external power source.\nFIG. 5C is a block diagram illustrating the components of apparatus 110 according to another example embodiment including computing device 120.  In this embodiment, apparatus 110 includes an image sensor 220, a memory 550a, a first processor\n210, a feedback-outputting unit 230, a wireless transceiver 530a, a mobile power source 520, and a power connector 510.  As further shown in FIG. 5C, computing device 120 includes a processor 540, a feedback-outputting unit 545, a memory 550b, a wireless\ntransceiver 530b, and a display 260.  One example of computing device 120 is a smartphone or tablet having a dedicated application installed therein.  In other embodiments, computing device 120 may include any configuration such as an on-board automobile\ncomputing system, a PC, a laptop, and any other system consistent with the disclosed embodiments.  In this example, user 100 may view feedback output in response to identification of a hand-related trigger on display 260.  Additionally, user 100 may view\nother data (e.g., images, video clips, object information, schedule information, extracted information, etc.) on display 260.  In addition, user 100 may communicate with server 250 via computing device 120.\nIn some embodiments, processor 210 and processor 540 are configured to extract information from captured image data.  The term \"extracting information\" includes any process by which information associated with objects, individuals, locations,\nevents, etc., is identified in the captured image data by any means known to those of ordinary skill in the art.  In some embodiments, apparatus 110 may use the extracted information to send feedback or other real-time indications to feedback outputting\nunit 230 or to computing device 120.  In some embodiments, processor 210 may identify in the image data the individual standing in front of user 100, and send computing device 120 the name of the individual and the last time user 100 met the individual. \nIn another embodiment, processor 210 may identify in the image data, one or more visible triggers, including a hand-related trigger, and determine whether the trigger is associated with a person other than the user of the wearable apparatus to\nselectively determine whether to perform an action associated with the trigger.  One such action may be to provide a feedback to user 100 via feedback-outputting unit 230 provided as part of (or in communication with) apparatus 110 or via a feedback unit\n545 provided as part of computing device 120.  For example, feedback-outputting unit 545 may be in communication with display 260 to cause the display 260 to visibly output information.  In some embodiments, processor 210 may identify in the image data a\nhand-related trigger and send computing device 120 an indication of the trigger.  Processor 540 may then process the received trigger information and provide an output via feedback outputting unit 545 or display 260 based on the hand-related trigger.  In\nother embodiments, processor 540 may determine a hand-related trigger and provide suitable feedback similar to the above, based on image data received from apparatus 110.  In some embodiments, processor 540 may provide instructions or other information,\nsuch as environmental information to apparatus 110 based on an identified hand-related trigger.\nIn some embodiments, processor 210 may identify other environmental information in the analyzed images, such as an individual standing in front user 100, and send computing device 120 information related to the analyzed information such as the\nname of the individual and the last time user 100 met the individual.  In a different embodiment, processor 540 may extract statistical information from captured image data and forward the statistical information to server 250.  For example, certain\ninformation regarding the types of items a user purchases, or the frequency a user patronizes a particular merchant, etc. may be determined by processor 540.  Based on this information, server 250 may send computing device 120 coupons and discounts\nassociated with the user's preferences.\nWhen apparatus 110 is connected or wirelessly connected to computing device 120, apparatus 110 may transmit at least part of the image data stored in memory 550a for storage in memory 550b.  In some embodiments, after computing device 120\nconfirms that transferring the part of image data was successful, processor 540 may delete the part of the image data.  The term \"delete\" means that the image is marked as `deleted` and other image data may be stored instead of it, but does not\nnecessarily mean that the image data was physically removed from the memory.\nAs will be appreciated by a person skilled in the art having the benefit of this disclosure, numerous variations and/or modifications may be made to the disclosed embodiments.  Not all components are essential for the operation of apparatus 110. Any component may be located in any appropriate apparatus and the components may be rearranged into a variety of configurations while providing the functionality of the disclosed embodiments.  For example, in some embodiments, apparatus 110 may include a\ncamera, a processor, and a wireless transceiver for sending data to another device.  Therefore, the foregoing configurations are examples and, regardless of the configurations discussed above, apparatus 110 can capture, store, and/or process images.\nFurther, the foregoing and following description refers to storing and/or processing images or image data.  In the embodiments disclosed herein, the stored and/or processed images or image data may comprise a representation of one or more images\ncaptured by image sensor 220.  As the term is used herein, a \"representation\" of an image (or image data) may include an entire image or a portion of an image.  A representation of an image (or image data) may have the same resolution or a lower\nresolution as the image (or image data), and/or a representation of an image (or image data) may be altered in some respect (e.g., be compressed, have a lower resolution, have one or more colors that are altered, etc.).\nFor example, apparatus 110 may capture an image and store a representation of the image that is compressed as a .JPG file.  As another example, apparatus 110 may capture an image in color, but store a black-and-white representation of the color\nimage.  As yet another example, apparatus 110 may capture an image and store a different representation of the image (e.g., a portion of the image).  For example, apparatus 110 may store a portion of an image that includes a face of a person who appears\nin the image, but that does not substantially include the environment surrounding the person.  Similarly, apparatus 110 may, for example, store a portion of an image that includes a product that appears in the image, but does not substantially include\nthe environment surrounding the product.  As yet another example, apparatus 110 may store a representation of an image at a reduced resolution (i.e., at a resolution that is of a lower value than that of the captured image).  Storing representations of\nimages may allow apparatus 110 to save storage space in memory 550.  Furthermore, processing representations of images may allow apparatus 110 to improve processing efficiency and/or help to preserve battery life.\nIn addition to the above, in some embodiments, any one of apparatus 110 or computing device 120, via processor 210 or 540, may further process the captured image data to provide additional functionality to recognize objects and/or gestures\nand/or other information in the captured image data.  In some embodiments, actions may be taken based on the identified objects, gestures, or other information.  In some embodiments, processor 210 or 540 may identify in the image data, one or more\nvisible triggers, including a hand-related trigger, and determine whether the trigger is associated with a person other than the user to determine whether to perform an action associated with the trigger.\nSome embodiments of the present disclosure may include an apparatus securable to an article of clothing of a user.  Such an apparatus may include two portions, connectable by a connector.  A capturing unit may be designed to be worn on the\noutside of a user's clothing, and may include an image sensor for capturing images of a user's environment.  The capturing unit may be connected to or connectable to a power unit, which may be configured to house a power source and a processing device. \nThe capturing unit may be a small device including a camera or other device for capturing images.  The capturing unit may be designed to be inconspicuous and unobtrusive, and may be configured to communicate with a power unit concealed by a user's\nclothing.  The power unit may include bulkier aspects of the system, such as transceiver antennas, at least one battery, a processing device, etc. In some embodiments, communication between the capturing unit and the power unit may be provided by a data\ncable included in the connector, while in other embodiments, communication may be wirelessly achieved between the capturing unit and the power unit.  Some embodiments may permit alteration of the orientation of an image sensor of the capture unit, for\nexample to better capture images of interest.\nFIG. 6 illustrates an exemplary embodiment of a memory containing software modules consistent with the present disclosure.  Included in memory 550 are orientation identification module 601, orientation adjustment module 602, and motion tracking\nmodule 603.  Modules 601, 602, 603 may contain software instructions for execution by at least one processing device, e.g., processor 210, included with a wearable apparatus.  Orientation identification module 601, orientation adjustment module 602, and\nmotion tracking module 603 may cooperate to provide orientation adjustment for a capturing unit incorporated into wireless apparatus 110.\nFIG. 7 illustrates an exemplary capturing unit 710 including an orientation adjustment unit 705.  Orientation adjustment unit 705 may be configured to permit the adjustment of image sensor 220.  As illustrated in FIG. 7, orientation adjustment\nunit 705 may include an eye-ball type adjustment mechanism.  In alternative embodiments, orientation adjustment unit 705 may include gimbals, adjustable stalks, pivotable mounts, and any other suitable unit for adjusting an orientation of image sensor\n220.\nImage sensor 220 may be configured to be movable with the head of user 100 in such a manner that an aiming direction of image sensor 220 substantially coincides with a field of view of user 100.  For example, as described above, a camera\nassociated with image sensor 220 may be installed within capturing unit 710 at a predetermined angle in a position facing slightly upwards or downwards, depending on an intended location of capturing unit 710.  Accordingly, the set aiming direction of\nimage sensor 220 may match the field-of-view of user 100.  In some embodiments, processor 210 may change the orientation of image sensor 220 using image data provided from image sensor 220.  For example, processor 210 may recognize that a user is reading\na book and determine that the aiming direction of image sensor 220 is offset from the text.  That is, because the words in the beginning of each line of text are not fully in view, processor 210 may determine that image sensor 220 is tilted in the wrong\ndirection.  Responsive thereto, processor 210 may adjust the aiming direction of image sensor 220.\nOrientation identification module 601 may be configured to identify an orientation of an image sensor 220 of capturing unit 710.  An orientation of an image sensor 220 may be identified, for example, by analysis of images captured by image\nsensor 220 of capturing unit 710, by tilt or attitude sensing devices within capturing unit 710, and by measuring a relative direction of orientation adjustment unit 705 with respect to the remainder of capturing unit 710.\nOrientation adjustment module 602 may be configured to adjust an orientation of image sensor 220 of capturing unit 710.  As discussed above, image sensor 220 may be mounted on an orientation adjustment unit 705 configured for movement. \nOrientation adjustment unit 705 may be configured for rotational and/or lateral movement in response to commands from orientation adjustment module 602.  In some embodiments orientation adjustment unit 705 may be adjust an orientation of image sensor 220\nvia motors, electromagnets, permanent magnets, and/or any suitable combination thereof.\nIn some embodiments, monitoring module 603 may be provided for continuous monitoring.  Such continuous monitoring may include tracking a movement of at least a portion of an object included in one or more images captured by the image sensor. \nFor example, in one embodiment, apparatus 110 may track an object as long as the object remains substantially within the field-of-view of image sensor 220.  In additional embodiments, monitoring module 603 may engage orientation adjustment module 602 to\ninstruct orientation adjustment unit 705 to continually orient image sensor 220 towards an object of interest.  For example, in one embodiment, monitoring module 603 may cause image sensor 220 to adjust an orientation to ensure that a certain designated\nobject, for example, the face of a particular person, remains within the field-of view of image sensor 220, even as that designated object moves about.  In another embodiment, monitoring module 603 may continuously monitor an area of interest included in\none or more images captured by the image sensor.  For example, a user may be occupied by a certain task, for example, typing on a laptop, while image sensor 220 remains oriented in a particular direction and continuously monitors a portion of each image\nfrom a series of images to detect a trigger or other event.  For example, image sensor 210 may be oriented towards a piece of laboratory equipment and monitoring module 603 may be configured to monitor a status light on the laboratory equipment for a\nchange in status, while the user's attention is otherwise occupied.\nIn some embodiments consistent with the present disclosure, capturing unit 710 may include a plurality of image sensors 220.  The plurality of image sensors 220 may each be configured to capture different image data.  For example, when a\nplurality of image sensors 220 are provided, the image sensors 220 may capture images having different resolutions, may capture wider or narrower fields of view, and may have different levels of magnification.  Image sensors 220 may be provided with\nvarying lenses to permit these different configurations.  In some embodiments, a plurality of image sensors 220 may include image sensors 220 having different orientations.  Thus, each of the plurality of image sensors 220 may be pointed in a different\ndirection to capture different images.  The fields of view of image sensors 220 may be overlapping in some embodiments.  The plurality of image sensors 220 may each be configured for orientation adjustment, for example, by being paired with an image\nadjustment unit 705.  In some embodiments, monitoring module 603, or another module associated with memory 550, may be configured to individually adjust the orientations of the plurality of image sensors 220 as well as to turn each of the plurality of\nimage sensors 220 on or off as may be required.  In some embodiments, monitoring an object or person captured by an image sensor 220 may include tracking movement of the object across the fields of view of the plurality of image sensors 220.\nEmbodiments consistent with the present disclosure may include connectors configured to connect a capturing unit and a power unit of a wearable apparatus.  Capturing units consistent with the present disclosure may include least one image sensor\nconfigured to capture images of an environment of a user.  Power units consistent with the present disclosure may be configured to house a power source and/or at least one processing device.  Connectors consistent with the present disclosure may be\nconfigured to connect the capturing unit and the power unit, and may be configured to secure the apparatus to an article of clothing such that the capturing unit is positioned over an outer surface of the article of clothing and the power unit is\npositioned under an inner surface of the article of clothing.  Exemplary embodiments of capturing units, connectors, and power units consistent with the disclosure are discussed in further detail with respect to FIGS. 8-14.\nFIG. 8 is a schematic illustration of an embodiment of wearable apparatus 110 securable to an article of clothing consistent with the present disclosure.  As illustrated in FIG. 8, capturing unit 710 and power unit 720 may be connected by a\nconnector 730 such that capturing unit 710 is positioned on one side of an article of clothing 750 and power unit 720 is positioned on the opposite side of the clothing 750.  In some embodiments, capturing unit 710 may be positioned over an outer surface\nof the article of clothing 750 and power unit 720 may be located under an inner surface of the article of clothing 750.  The power unit 720 may be configured to be placed against the skin of a user.\nCapturing unit 710 may include an image sensor 220 and an orientation adjustment unit 705 (as illustrated in FIG. 7).  Power unit 720 may include mobile power source 520 and processor 210.  Power unit 720 may further include any combination of\nelements previously discussed that may be a part of wearable apparatus 110, including, but not limited to, wireless transceiver 530, feedback outputting unit 230, memory 550, and data port 570.\nConnector 730 may include a clip 715 or other mechanical connection designed to clip or attach capturing unit 710 and power unit 720 to an article of clothing 750 as illustrated in FIG. 8.  As illustrated, clip 715 may connect to each of\ncapturing unit 710 and power unit 720 at a perimeter thereof, and may wrap around an edge of the article of clothing 750 to affix the capturing unit 710 and power unit 720 in place.  Connector 730 may further include a power cable 760 and a data cable\n770.  Power cable 760 may be capable of conveying power from mobile power source 520 to image sensor 220 of capturing unit 710.  Power cable 760 may also be configured to provide power to any other elements of capturing unit 710, e.g., orientation\nadjustment unit 705.  Data cable 770 may be capable of conveying captured image data from image sensor 220 in capturing unit 710 to processor 800 in the power unit 720.  Data cable 770 may be further capable of conveying additional data between capturing\nunit 710 and processor 800, e.g., control instructions for orientation adjustment unit 705.\nFIG. 9 is a schematic illustration of a user 100 wearing a wearable apparatus 110 consistent with an embodiment of the present disclosure.  As illustrated in FIG. 9, capturing unit 710 is located on an exterior surface of the clothing 750 of\nuser 100.  Capturing unit 710 is connected to power unit 720 (not seen in this illustration) via connector 730, which wraps around an edge of clothing 750.\nIn some embodiments, connector 730 may include a flexible printed circuit board (PCB).  FIG. 10 illustrates an exemplary embodiment wherein connector 730 includes a flexible printed circuit board 765.  Flexible printed circuit board 765 may\ninclude data connections and power connections between capturing unit 710 and power unit 720.  Thus, in some embodiments, flexible printed circuit board 765 may serve to replace power cable 760 and data cable 770.  In alternative embodiments, flexible\nprinted circuit board 765 may be included in addition to at least one of power cable 760 and data cable 770.  In various embodiments discussed herein, flexible printed circuit board 765 may be substituted for, or included in addition to, power cable 760\nand data cable 770.\nFIG. 11 is a schematic illustration of another embodiment of a wearable apparatus securable to an article of clothing consistent with the present disclosure.  As illustrated in FIG. 11, connector 730 may be centrally located with respect to\ncapturing unit 710 and power unit 720.  Central location of connector 730 may facilitate affixing apparatus 110 to clothing 750 through a hole in clothing 750 such as, for example, a button-hole in an existing article of clothing 750 or a specialty hole\nin an article of clothing 750 designed to accommodate wearable apparatus 110.\nFIG. 12 is a schematic illustration of still another embodiment of wearable apparatus 110 securable to an article of clothing.  As illustrated in FIG. 12, connector 730 may include a first magnet 731 and a second magnet 732.  First magnet 731\nand second magnet 732 may secure capturing unit 710 to power unit 720 with the article of clothing positioned between first magnet 731 and second magnet 732.  In embodiments including first magnet 731 and second magnet 732, power cable 760 and data cable\n770 may also be included.  In these embodiments, power cable 760 and data cable 770 may be of any length, and may provide a flexible power and data connection between capturing unit 710 and power unit 720.  Embodiments including first magnet 731 and\nsecond magnet 732 may further include a flexible PCB 765 connection in addition to or instead of power cable 760 and/or data cable 770.  In some embodiments, first magnet 731 or second magnet 732 may be replaced by an object comprising a metal material.\nFIG. 13 is a schematic illustration of yet another embodiment of a wearable apparatus 110 securable to an article of clothing.  FIG. 13 illustrates an embodiment wherein power and data may be wirelessly transferred between capturing unit 710 and\npower unit 720.  As illustrated in FIG. 13, first magnet 731 and second magnet 732 may be provided as connector 730 to secure capturing unit 710 and power unit 720 to an article of clothing 750.  Power and/or data may be transferred between capturing\nunit 710 and power unit 720 via any suitable wireless technology, for example, magnetic and/or capacitive coupling, near field communication technologies, radiofrequency transfer, and any other wireless technology suitable for transferring data and/or\npower across short distances.\nFIG. 14 illustrates still another embodiment of wearable apparatus 110 securable to an article of clothing 750 of a user.  As illustrated in FIG. 14, connector 730 may include features designed for a contact fit.  For example, capturing unit 710\nmay include a ring 733 with a hollow center having a diameter slightly larger than a disk-shaped protrusion 734 located on power unit 720.  When pressed together with fabric of an article of clothing 750 between them, disk-shaped protrusion 734 may fit\ntightly inside ring 733, securing capturing unit 710 to power unit 720.  FIG. 14 illustrates an embodiment that does not include any cabling or other physical connection between capturing unit 710 and power unit 720.  In this embodiment, capturing unit\n710 and power unit 720 may transfer power and data wirelessly.  In alternative embodiments, capturing unit 710 and power unit 720 may transfer power and data via at least one of cable 760, data cable 770, and flexible printed circuit board 765.\nFIG. 15 illustrates another aspect of power unit 720 consistent with embodiments described herein.  Power unit 720 may be configured to be positioned directly against the user's skin.  To facilitate such positioning, power unit 720 may further\ninclude at least one surface coated with a biocompatible material 740.  Biocompatible materials 740 may include materials that will not negatively react with the skin of the user when worn against the skin for extended periods of time.  Such materials\nmay include, for example, silicone, PTFE, kapton, polyimide, titanium, nitinol, platinum, and others.  Also as illustrated in FIG. 15, power unit 720 may be sized such that an inner volume of the power unit is substantially filled by mobile power source\n520.  That is, in some embodiments, the inner volume of power unit 720 may be such that the volume does not accommodate any additional components except for mobile power source 520.  In some embodiments, mobile power source 520 may take advantage of its\nclose proximity to the skin of user's skin.  For example, mobile power source 520 may use the Peltier effect to produce power and/or charge the power source.\nIn further embodiments, an apparatus securable to an article of clothing may further include protective circuitry associated with power source 520 housed in in power unit 720.  FIG. 16 illustrates an exemplary embodiment including protective\ncircuitry 775.  As illustrated in FIG. 16, protective circuitry 775 may be located remotely with respect to power unit 720.  In alternative embodiments, protective circuitry 775 may also be located in capturing unit 710, on flexible printed circuit board\n765, or in power unit 720.\nProtective circuitry 775 may be configured to protect image sensor 220 and/or other elements of capturing unit 710 from potentially dangerous currents and/or voltages produced by mobile power source 520.  Protective circuitry 775 may include\npassive components such as capacitors, resistors, diodes, inductors, etc., to provide protection to elements of capturing unit 710.  In some embodiments, protective circuitry 775 may also include active components, such as transistors, to provide\nprotection to elements of capturing unit 710.  For example, in some embodiments, protective circuitry 775 may comprise one or more resistors serving as fuses.  Each fuse may comprise a wire or strip that melts (thereby braking a connection between\ncircuitry of image capturing unit 710 and circuitry of power unit 720) when current flowing through the fuse exceeds a predetermined limit (e.g., 500 milliamps, 900 milliamps, 1 amp, 1.1 amps, 2 amp, 2.1 amps, 3 amps, etc.) Any or all of the previously\ndescribed embodiments may incorporate protective circuitry 775.\nIn some embodiments, the wearable apparatus may transmit data to a computing device (e.g., a smartphone, tablet, watch, computer, etc.) over one or more networks via any known wireless standard (e.g., cellular, Wi-Fi, Bluetooth.RTM., etc.), or\nvia near-filed capacitive coupling, other short range wireless techniques, or via a wired connection.  Similarly, the wearable apparatus may receive data from the computing device over one or more networks via any known wireless standard (e.g., cellular,\nWi-Fi, Bluetooth.RTM., etc.), or via near-filed capacitive coupling, other short range wireless techniques, or via a wired connection.  The data transmitted to the wearable apparatus and/or received by the wireless apparatus may include images, portions\nof images, identifiers related to information appearing in analyzed images or associated with analyzed audio, or any other data representing image and/or audio data.  For example, an image may be analyzed and an identifier related to an activity\noccurring in the image may be transmitted to the computing device (e.g., the \"paired device\").  In the embodiments described herein, the wearable apparatus may process images and/or audio locally (on board the wearable apparatus) and/or remotely (via a\ncomputing device).  Further, in the embodiments described herein, the wearable apparatus may transmit data related to the analysis of images and/or audio to a computing device for further analysis, display, and/or transmission to another device (e.g., a\npaired device).  Further, a paired device may execute one or more applications (apps) to process, display, and/or analyze data (e.g., identifiers, text, images, audio, etc.) received from the wearable apparatus.\nSome of the disclosed embodiments may involve systems, devices, methods, and software products for determining at least one keyword.  For example, at least one keyword may be determined based on data collected by apparatus 110.  At least one\nsearch query may be determined based on the at least one keyword.  The at least one search query may be transmitted to a search engine.\nIn some embodiments, at least one keyword may be determined based on at least one or more images captured by image sensor 220.  In some cases, the at least one keyword may be selected from a keywords pool stored in memory.  In some cases,\noptical character recognition (OCR) may be performed on at least one image captured by image sensor 220, and the at least one keyword may be determined based on the OCR result.  In some cases, at least one image captured by image sensor 220 may be\nanalyzed to recognize: a person, an object, a location, a scene, and so forth.  Further, the at least one keyword may be determined based on the recognized person, object, location, scene, etc. For example, the at least one keyword may comprise: a\nperson's name, an object's name, a place's name, a date, a sport team's name, a movie's name, a book's name, and so forth.\nIn some embodiments, at least one keyword may be determined based on the user's behavior.  The user's behavior may be determined based on an analysis of the one or more images captured by image sensor 220.  In some embodiments, at least one\nkeyword may be determined based on activities of a user and/or other person.  The one or more images captured by image sensor 220 may be analyzed to identify the activities of the user and/or the other person who appears in one or more images captured by\nimage sensor 220.  In some embodiments, at least one keyword may be determined based on at least one or more audio segments captured by apparatus 110.  In some embodiments, at least one keyword may be determined based on at least GPS information\nassociated with the user.  In some embodiments, at least one keyword may be determined based on at least the current time and/or date.\nIn some embodiments, at least one search query may be determined based on at least one keyword.  In some cases, the at least one search query may comprise the at least one keyword.  In some cases, the at least one search query may comprise the\nat least one keyword and additional keywords provided by the user.  In some cases, the at least one search query may comprise the at least one keyword and one or more images, such as images captured by image sensor 220.  In some cases, the at least one\nsearch query may comprise the at least one keyword and one or more audio segments, such as audio segments captured by apparatus 110.\nIn some embodiments, the at least one search query may be transmitted to a search engine.  In some embodiments, search results provided by the search engine in response to the at least one search query may be provided to the user.  In some\nembodiments, the at least one search query may be used to access a database.\nFor example, in one embodiment, the keywords may include a name of a type of food, such as quinoa, or a brand name of a food product; and the search will output information related to desirable quantities of consumption, facts about the\nnutritional profile, and so forth.  In another example, in one embodiment, the keywords may include a name of a restaurant, and the search will output information related to the restaurant, such as a menu, opening hours, reviews, and so forth.  The name\nof the restaurant may be obtained using OCR on an image of signage, using GPS information, and so forth.  In another example, in one embodiment, the keywords may include a name of a person, and the search will provide information from a social network\nprofile of the person.  The name of the person may be obtained using OCR on an image of a name tag attached to the person's shirt, using face recognition algorithms, and so forth.  In another example, in one embodiment, the keywords may include a name of\na book, and the search will output information related to the book, such as reviews, sales statistics, information regarding the author of the book, and so forth.  In another example, in one embodiment, the keywords may include a name of a movie, and the\nsearch will output information related to the movie, such as reviews, box office statistics, information regarding the cast of the movie, show times, and so forth.  In another example, in one embodiment, the keywords may include a name of a sport team,\nand the search will output information related to the sport team, such as statistics, latest results, future schedule, information regarding the players of the sport team, and so forth.  For example, the name of the sport team may be obtained using audio\nrecognition algorithms.\nSelecting Actions Based on a Detected Person\nIn some embodiments, wearable apparatus 110 may execute a variety of actions, such as identifying persons in captured images, uploading images of persons (e.g., to one or more social networks, to one or more cloud storage folders, etc.), tagging\nimages of persons, sending images of persons (e.g., via email, text message, or the like), updating a Gnatt chart or a calendar, sending information to one or more matchmaking services, updating one or more social network profiles, providing one or more\nstatistics, or the like.  The wearable apparatus 110 may select one or more actions to perform based on one or more attributes of a detected person, such as age, gender, weight, height, relationship with a wearer of the device (e.g., social, family,\nbusiness, etc.).  In doing so, embodiments consistent with the present disclosure may address the technical problem of extracting information from an environment of the wearer of the wearable apparatus that is relevant to the wearer and then determining\nhow to use that information in a way that is useful to the wearer and/or according to the user's preferences.  For example, the wearer may wish to track encounters with certain people who are related to them or persons with whom the wearer is associated\nwith at work, but may have different preferences as to the kinds of information that the wearer would like to store regarding different persons.  Embodiments of the present disclosure may address this problem through techniques for categorizing\ninformation extracted or determined from images of the wearer's environment and executing appropriate actions related to the extracted information.\nFIG. 17 illustrates an exemplary embodiment of a memory 1700 containing software modules consistent with the present disclosure.  Memory 1700 may be included in apparatus 110 in lieu of or in combination with memory 550.  In some embodiments,\nthe software modules of memory 1700 may be combined with one or more software modules of memory 550 into one or more memories.  Memory 1700 may store more or fewer modules than those shown in FIG. 17.\nAs illustrated in FIG. 17, included in memory 1700 are software instructions to execute a person detection module 1701, an attribute identification module 1702, a categorization module 1703, and an action module 1704.  Modules 1701, 1702, 1703,\nand 1704 may contain software instructions for execution by at least one processing device, e.g., processor 210, included in a wearable apparatus, e.g., wearable apparatus 110.  In some embodiments, person detection module 1701, attribute identification\nmodule 1702, categorization module 1703, and action module 1704 may cooperate to execute method 1900 (or a variant thereof) of FIG. 19.\nPerson detection module 1701 may be configured to analyze one or more images captured from a wearable apparatus to detect at least one person within the images.  For example, person detection module 1701 may be configured to identify a subset of\nthe captured data that includes at least one person.  In some embodiments, person detection module 1701 may be configured to receive a plurality of images and to identify one or more of the plurality of images that include at least one person.  For\nexample, module 1701 may receive a plurality of images of an environment surrounding a user wearing the wearable apparatus 110 and identify which of the plurality of images include at least one person.\nIn some embodiments, such an analysis may be performed by employing a facial recognition algorithm designed to detect facial features (e.g., mouth, eyes, etc.), facial contours, body shape, or any other suitable identifying feature of a person. \nIn other embodiments, the at least one person may be identified using a thermal signature algorithm designed to detect the presence of at least one person based on the heat generated by the at least one person.  In such embodiments, the wearable\napparatus 110 may capture thermal images, either alone or in combination with visual images, for processing by the thermal signature algorithm.  Thermal recognition of the at least one person may be desirable in implementations in which the wearable\ndevice 110 is operating in reduced lighting situations.  In some embodiments, the at least one person may be identified through the application of one or more image classification techniques.  For example, at least one image classification technique may\nbe used to classify at least one feature of an image.  In some embodiments, an image classification technique may include one or more of image enhancement, edge detection, image analysis, and data extraction.  Specific examples of the methods for\nidentifying at least one person are exemplary only, and a person of ordinary skill in the art will recognize other methods for identifying the at least one person that remain consistent with the present disclosure.  In some examples, the at least one\nperson may be detected using a facial detection algorithm, using a neural network trained to detect faces and/or persons in images, and so forth.  In some examples, the at least one person may be identified using a facial recognition algorithm, using a\nneural network trained to identify people in images, and so forth.\nAttribute identification module 1702 may be configured to receive one or more images of detected persons and further analyze the one or more images to determine one or more attributes (e.g., age, gender, weight, height, relationship with a\nwearer of the wearable apparatus) associated with the identified persons.  In some embodiments, attribute identification module 1702 may determine more than one attribute for each detected person.\nIn some embodiments, at least one attribute of a detected person may be determined based on analysis of one or more images of the detected person.  For example, one or more algorithms may analyze one or more of detected facial features (e.g.,\nmouth, eyes, etc.), facial contours, body shape, or any other suitable identifying feature of a person to estimate or determine information such as a the detected person's age, gender, weight, height, etc. For example, a neural network trained to\nidentify attributes of a person from an image may be used.\nIn some embodiments, attribute identification module 1702 may determine an identity of a detected person (e.g., through facial recognition), and then access one or more databases (stored, e.g., locally in a memory of wearable apparatus 110\nand/or accessible remotely over a network, e.g., such as by accessing server 250) to retrieve at least one attribute of the detected person.  For example, after identifying a detected person as a particular individual (e.g., determining a match based on\nfacial recognition to a known person's image), attribute identification module 1702 may access a database to retrieve information about the detected person, such as the detected person's age, family members, etc.\nCategorization module 1703 may be configured to use associated attributes to categorize identified persons.  For example, categorization module 1703 may classify a detected person as an adult, a teenager, a child, a brother of the wearer, a\nmother-in-law of the wearer, a tall (e.g., above median) person, a short (e.g., below median) person, a male, a female, or the like.\nIn some embodiments, based on an attribute of a particular individual, categorization module 1703 may associate the detected person with one or more relevant categories.  For example, if an attribute of a person indicates he or she is more than\na predetermined height (e.g., six feet or taller), categorization module 1703 may associate the detected person a category of \"tall.\" Similarly, if an attribute of a person indicates he or she is less than a predetermined age (e.g., 18 years),\ncategorization module 1703 may associate the detected person a category of \"child.\" Categorization module 1703 may further associate more than one category with a detected person.  For example, a detected person who has an attribute of an age of fifteen\nyears old may be associated with both a category of \"child\" and a category of \"teenager.\"\nIn some embodiments, categorization module 1703 may be configured to receive one or more images of detected persons and further analyze the one or more images to categorize identified persons.  In some examples, the detected person may be\nclassified to one of a number of predefined categorizes using an image classifier that assign a category to a person based on images of a person.  The image classifier may be a result of a machine learning algorithm trained on a set of examples, where an\nexample may include images of a person along with the desired category for the person.  In some examples, a neural network trained to assign one or more categories to a person based on images of a person may be used.\nIn some embodiments, categorization module 1703 may be configured to receive one or more images and analyze the one or more images to detect persons of a select category.  In some examples, the one or more images may be analyzed using a detector\nconfigured to detect females, males, children, elderly persons, business persons, and so forth.  For example, the detector may comprise a classifier that classifies images and/or portion of images as ones that contain and ones that do not contain a\nperson matching the selected category.  The classifier may be a result of a machine learning algorithm trained using training examples, where a training example may comprise images and a desired answer.  For example, the detector may comprise a neural\nnetwork trained to detect persons that match the selected category in images.\nAction module 1704 may be configured to select one or more actions based on the categories.  For example, action module 1704 may identify the detected person, when the detected person is categorized as business contact of the wearer.  By way of\nfurther example, action module 1704 may upload and tag an image (e.g., to a cloud storage service, a social network, etc.) when, for example, the detected person is categorized as a friend or family of the wearer.  In another example, action module 1704\nmay update a timeline, calendar, Gnatt chart, or the like, when the detected person is categorized as a coworker or significant other of the wearer.  In yet another example, action module 1704 may provide information related to a potential dating match,\ne.g., when the detected person is categorized in one or more categories matching dating preferences of the wearer (e.g., gender, height, hair color, etc.).\nAlternatively or concurrently to performing one or more actions such as those above, action module 1704 may update one or more statistics based on the categorization.  In some embodiments, the statistics may be accumulated over geographical\nregions and/or time frames.\nIn some embodiments, action module 1704 may cause the selected one or more actions to be executed by providing information (e.g., one or more instructions and/or one or more parameters to an instruction) to a device paired with the wearable\napparatus (e.g., a smartphone) and/or by providing information (e.g., one more instructions and/or one or more parameters to an instruction) to a remote server (e.g., server 250) over a network.  For example, action module 1704 may transmit one or more\ninstructions to a paired device to update a calendar displayed on a screen of the paired device to indicate that the user had a meeting with a co-worker on a particular day.\nModules 1701-1704 may be implemented in software, hardware, firmware, a mix of any of those, or the like.  For example, if the modules are implemented in software, they may be stored, for example, in memory 550.  However, in some embodiments,\nany one or more of modules 1701-1704 may, for example, be stored in processor 540 and/or located on server 250, which may include one or more processing devices.  Processing devices of server 250 may be configured to execute the instructions of modules\n1701-1704.  In some embodiments, aspects of modules 1701-1704 may include software, hardware, or firmware instructions (or a combination thereof) executable by one or more processors, alone or in various combinations with each other.  For example,\nmodules 1701-1704 may be configured to interact with each other and/or other modules of server 250 and/or a wearable camera system to perform functions consistent with disclosed embodiments.  In some embodiments, any of the disclosed modules may each\ninclude dedicated sensors (e.g., IR, image sensors, etc.) and/or dedicated application processing devices to perform the functionality associated with each module.\nFIG. 18A is an illustration of an example image 1800 captured by a wearable apparatus, such as wearable apparatus 110.  In the example of FIG. 18A, image 1800 contains a person 1810.  In the example of FIG. 18A, wearable apparatus 110 may\nclassify person 1810 as a brother of the wearer of apparatus 110.  The classification may be determined using any one or more of the techniques discussed above in connection with FIG. 17.  Based on this classification, wearable apparatus 110 may take one\nor more actions.  For example, apparatus 110 may identify person 1810 as \"brother\" and/or by the name \"Ted,\" may upload and tag an image of person 1810 (e.g., to a social network account associated with the wearer), may update a social network account\nassociated with the wearer to indicate that the wearer was with person 1810, or the like.\nFIG. 18B is an illustration of another example image 1850 captured by a wearable apparatus such as wearable apparatus 110.  In the example of FIG. 18B, image 1850 contains a person 1860.  In the example of FIG. 18B, wearable apparatus 110 may\nclassify person 1860 as a child (e.g., based on a determined age of person 1860).  Based on this classification, apparatus 110 may take one or more actions.  For example, as depicted in FIG. 18B, wearable apparatus 110 may transmit an amber alert based\non the classification of person 1860 as a child and/or based on the absence of other persons, such as adults, nearby.  Alternatively or concurrently, as depicted in FIG. 18B, wearable apparatus 110 may transmit an update to a database providing\ninformation related to missing persons.  Such an update may verify if person 1860 is in the missing persons database and, if so, transmit a time and/or a location of the capture of image 1850 to the database.  In some examples, wearable apparatus 110 may\nforgo certain actions based on the category of the person, for example in FIG. 18B the apparatus may forgo uploading images of person 1860 to a social network or a public photo album based on the categorization of person 1860 as a child.\nFIGS. 18A and 18B are examples of persons being detected and categorized by wearable apparatus 110.  As would be understood by one of ordinary skill in the art, wearable apparatus 110 may capture images throughout the user's day at a variety of\nlocations as the environment surrounding the user changes.  For example, images may be captured when the user visits a restaurant for dinner, commutes to and from work, attends social events, etc. In this way, wearable apparatus 110 may be configured to\nmonitor the environment surrounding user 100 throughout the user's activities to identify exposure to one or more persons throughout the time user wears wearable apparatus 110, and then determine attributes of detected persons, base categories on the\ndetermined attributes, and take appropriate actions based on the categories.\nFIG. 19 illustrates a flowchart of an example method 1900 for selecting an action based on a detected person.  Method 1900 may be implemented by at least one processing device (e.g., processor 210 of wearable apparatus 110) and by a wearable\nimage sensor (e.g., image sensor 220 of wearable apparatus 110) configured to capture a plurality of images from the environment of the user of the wearable apparatus.\nAt step 1910, the processing device may analyze at least one of the plurality of images to detect the person.  For example, received image data may be processed by software steps executed by person detection module 1701.  In some embodiments,\nthe processing device may compare one or more regions of the at least one image against a database of known patterns and/or images to determine whether a person is included in the one or more regions.  Alternatively or concurrently, the processing device\nmay apply one or more classifiers to the one or more regions, and the one or more classifiers may output whether a person is detected in the one or more regions.  Still further, the processing device may execute one or more algorithms to detect facial\nfeatures (e.g., mouth, eyes, etc.), facial contours, body shape, or any other suitable identifying feature of a person.\nIn some embodiments, the detection may be binary.  In such embodiments, the comparison and/or classifiers may output true if a person is detected and false if a person is not detected.  In other embodiments, the detection may be threshold-based. In such embodiments, the comparison and/or classifiers may output a percentage or other measurement of confidence that the one or more regions include a person.  For example, the comparison may output a likelihood (e.g., a 58% likelihood) that the one or\nmore regions include a person.  By way of further example, the classifiers may output a score (e.g., a score of 8 out of 10) that the one or more regions include a person.  In such embodiments, the processing device may use a threshold (e.g., at least\n55%, at least 7 out of 10, etc.) to convert the output to a binary detection.\nAt step 1920, the processing device may analyze at least one of the plurality of images to identify an attribute of the detected person.  For example, received image data may be processed by software steps executed by attribute identification\nmodule 1702.  Similar to step 1910, the processing device may compare one or more regions of the at least one image against a database of known patterns and/or images to determine the attribute.  Alternatively or concurrently, the processing device may\napply one or more classifiers to the one or more regions, and the one or more classifiers may output the attribute.  For example, as discussed earlier, one or more algorithms may analyze one or more of detected facial features (e.g., mouth, eyes, etc.),\nfacial contours, body shape, or any other suitable identifying feature of a person to estimate or determine information such as a the detected person's age, gender, weight, height, etc. In some embodiments, as discussed earlier, attribute identification\nmodule 1702 may determine an identity of a detected person (e.g., through facial recognition), and then access one or more databases (stored, e.g., locally in a memory of wearable apparatus 110 and/or accessible remotely over a network, e.g., such as by\naccessing server 250) to retrieve at least one attribute of the detected person.\nIn some embodiments, a confidence score or the like may be associated with the attribute.  For example, the comparison may output an attribute with a 64% confidence score.  By way of further example, the classifiers may output an attribute with\na confidence score of 4 (out of 10).  In such embodiments, the comparison and/or classifiers may output a plurality of attributes, and the processing device may select the attribute with the highest confidence score as the associated attribute.\nAs one example, analyzing at least one of the plurality of images to identify an attribute of the detected person may include analyzing the at least one of the plurality of images to estimate an age of the detected person.  As a second example,\nanalyzing at least one of the plurality of images to identify an attribute of the detected person may include analyzing the at least one of the plurality of images to estimate a height of the detected person.  In a third example, analyzing at least one\nof the plurality of images to identify an attribute of the detected person may include analyzing the at least one of the plurality of images to estimate a weight of the detected person.  As a fourth example, analyzing at least one of the plurality of\nimages to identify an attribute of the detected person may include analyzing the at least one of the plurality of images to estimate a gender of the detected person.\nIn some embodiments, analyzing at least one of the plurality of images to identify an attribute of the detected person may include analyzing the at least one of the plurality of images to determine an identity of the detected person.  For\nexample, an identity of the detected person may comprise a name, a job title, a phone number, or other identifier of the detected person.  In some embodiments, the processing device may compare one or more regions of the at least one image against a\ndatabase of known patterns to identify the detected person.  Alternatively or concurrently, the processing device may apply one or more classifiers to the one or more regions, and the one or more classifiers may output an identity of the detected person. In some embodiments, a confidence score or the like may be associated with the identity.  For example, the classifiers may output an identity with a confidence score (e.g., a 43% confidence score).  By way of further example, the comparison may output an\nidentity with a confidence score (e.g., a confidence score of 9 out of 10).  In such embodiments, the comparison and/or classifiers may output a plurality of identities, and the processing device may select the identity with the highest confidence score\nas the associated identity.\nBased on the identity of the detected person, the processing device may determine a type of a relationship between the user and the detected person, as depicted in the example of FIG. 18A.  For example, the relationship may comprise a category\nof relationship (such as family, friend, social, business, etc.), a specific relationship (such as brother, cousin, coworker, client, etc.), or the like.  In some embodiments, the processing device may determine multiple types of relationships (e.g., the\ndetected person is a family member and, more specifically, is the wearer's brother).\nAt step 1930, the processing device may select at least one category for the detected person based on the identified attribute.  Selecting the at least one category may be facilitated by software steps executed by categorization module 1704.  In\nsome embodiments, the at least one category may be selected from a plurality of attribute categories.  For example, the processing device may categorize the detected person into the at least one category based on the estimated age of the detected person\n(such as adult, child, teenager, senior citizen, etc.).  In a second example, the processing device may categorize the detected person into the at least one category based on the estimated height of the detected person (such as tall, average, short,\netc.).  As a third example, the processing device may categorize the detected person into the at least one category based on the estimated weight of the detected person (such as heavy, average, thin, etc.).  In a fourth example, the processing device may\ncategorize the detected person into the at least one category based on the estimated gender of the detected person (such as male, female, etc.).\nIn embodiments where the processing device has determined an identity of the detected person and determined a type of a relationship between the user and the detected person, the processing device may categorize the detected person into the at\nleast one category based on the type of the relationship between the user and the detected person (such as family, friend, social, business, sibling, coworker, etc.).\nIn some embodiments, the at least one category may include a statistical category.  For example, the processing device may retain a collection of statistics on the age, height, weight, and/or gender of persons detected in images captured by the\nwearable apparatus.  Statistical categories may thus be based on the collection of statistics, for example, medians, modes, means, deciles, quartiles, quintiles, or the like.  In some embodiments, the statistical category may be associated with a\ngeographical region and/or associated with a time period.  For example, the median may be measured only for a portion of the collection associated with North America.  By way of further example, the quartiles may be measured only for a portion of the\ncollection associated with the previous week.\nIn some embodiments, method 1900 may skip steps 1910 and 1920, and step 1930 may detect persons that match a selected category in images, for example using categorization module 1704.\nAt step 1940, the processing device may select at least one action based on the at least one category.  Selection of the at least one action may be facilitated via action module 1704.  For example, the at least one action may include identifying\nthe person, at least one of uploading an image and tagging an image, at least one of updating a log and updating a timeline or a Gantt chart, providing information related to a potential dating match, updating at least one of a social graph and a social\nnetwork profile, or the like.  The at least one action may be based on the at least one category.  For example, the processing device may select updating a timeline or a Gnatt chart (or other calendar or task list) if the detected person is categorized\nas a coworker or client.  By way of further example, the processing device may provide information related to a potential dating match if the detected person is categorized in one or more categories that match a dating profile of the user.  In another\nexample, the processing device may update a social graph and/or a social network profile if the detected person is categorized as a friend.\nIn some embodiments, the at least one action may be selected from a plurality of alternative actions associated with the at least one category.  For example, the category of friend may be associated with at least one of uploading an image and\ntagging an image and/or updating at least one of a social graph and a social network profile.  In such an example, the processing device may select at least one of uploading an image and tagging an image and/or updating at least one of a social graph and\na social network profile when the detected person is categorized as a friend.  In another example, the category of coworker may be associated with at least one of updating a log and updating a timeline or a Gantt chart and/or providing information\nrelated to a potential dating match.  In such an example, the processing device may select at least one of updating a log and updating a timeline or a Gantt chart and/or providing information related to a potential dating match when the detected person\nis categorized as a coworker.\nIn embodiments with a plurality of categories, the processing device may use a second category to select at least one action from a plurality of alternative actions associated with the first category.  Accordingly, in an example where the\ncategory of friend is associated with at least one of uploading an image and tagging an image and/or providing information related to a potential dating match, the processing device may select providing information related to a potential dating match\nwhen the gender of the detected person matches a preferred gender in a dating profile of the user.\nIn embodiments where the at least one category includes a statistical category, the at least one action may include updating information related to the statistical category.  For example, updating the information related to the statistical\ncategory includes updating a count of unique persons associated with the statistical category.  Accordingly, the processing device may keep track of a total number of friends, coworkers, males, females, short persons, tall persons, heavy persons, thin\npersons, or the like that are detected by the wearable apparatus.  As explained above, this count may be associated with a geographical region and/or associated with a time period.\nAt step 1950, the processing device may cause the at least one selected action to be executed.  In some embodiments, causing the at least one selected action to be executed may include sending information to a device paired with the wearable\napparatus.  A device paired with the wearable apparatus may include a smartphone, a tablet, a laptop, or the like.  Accordingly, the processing device may send information to the paired device using a wired connection (such as a universal serial bus\n(USB), or the like), a direct wireless connection (such as Bluetooth.RTM., or the like), and/or an indirect wireless connection (such as WiFi, 4G, or the like).\nThe transmitted information may include at least one of the plurality of images, one or more regions of at least one image (e.g., one or more regions used to detect a person and/or identify an attribute), one or more attributes of detected\npersons in the plurality of images, one or more categories for detected persons, one or more identities or detected persons, or the like.  Furthermore, the transmitted information may allow the paired device to executed one or more selected actions.  For\nexample, the processing device may transmit information to the paired device such that the paired device may identify the person, upload an image, tag an image, update a log, update a timeline or a Gantt chart, provide information related to a potential\ndating match, update at least one of a social graph and a social network profile, or the like.\nMethod 1900 may further include additional steps.  For example, the processing device may analyze at least one of the plurality of images to detect a second person.  The detection of the second person may be performed as the detection of the\nfirst person in step 1910.  Furthermore, method 1900 may further include analyzing at least one of the plurality of images to identify an attribute of the second person and selecting at least one category for the second person based on the identified\nattribute of the second person.\nIn some embodiments, one or more steps regarding the second person may be performed after steps 1910 to 1950 regarding the first person.  In other embodiments, one or more steps regarding the second person may be interspersed with steps 1910 to\n1950.  For example, the processing device may detect the second person after detecting the first person but before identifying an attribute of the first person and/or selecting at least one category for the first person.  By way of further example, the\nprocessing device may identify an attribute of the second person concurrently with identifying an attribute of the first person and/or may select at least one category for the second person concurrently with selecting at least one category for the first\nperson.\nIn some embodiments, method 1900 may base the selection of the at least one action on the at least one category selected for the second person.  For example, if a first detected person is categorized as a child and a second detected person is\ncategorized as an adult, the processing device may verify if the first detected person has been reported as abducted or kidnapped by the second detected person using a database of missing persons, a database of police reports, or the like.  By way of\nfurther example, if a first detected person is categorized as a coworker and a second detected person is categorized as a client, the processing device may update a timeline or a Gantt chart (or a calendar or task list) with a log of the meeting with the\nidentified coworker and the identified client.\nIn some embodiments, method 1900 may detect and categorize a plurality of persons appearing in one or more images, for example in a fashion similar to the one described above.  In some examples, step 1940 may base the selection of the at least\none action on the categories of the plurality of persons.  For example, the selection of the at least one action may be based on a distribution of the categories of a group of persons.  For example, the entropy of the distribution of the categories of a\ngroup of persons may be calculated, and the selection of the at least one action may be based on the calculated entropy.\nExecuting Actions Based on Physical Presence of a Detected Person\nAs explained above, in some embodiments, wearable apparatus 110 of the present disclosure may execute a variety of actions.  Some actions may be associated with a physical location of the apparatus, such as transmitting information associated\nwith a physical location of the wearable apparatus, updating at least one of a database and a social network profile based on information associated with a physical location of the wearable apparatus, determining one or more statistics based, at least in\npart, on information associated with a physical location of the wearable apparatus, or the like.  Other actions may be associated with a time and date (e.g., a time and date of capture of one or more images), such as transmitting information associated\nwith at least one of a time and a date, updating a database or a social network based on information associated with at least one of a time and a date, identifying one or more statistics based, at least in part, on information associated with at least\none of a time and a date, or the like.\nIn some embodiments, wearable apparatus 110 may determine one or more actions to take based on whether a detected person is physically present in an environment of a user of the wearable apparatus or visible on a display of a device in an\nenvironment of a user of the wearable apparatus.  In doing so, embodiments consistent with the present disclosure may address the technical problem of extracting and using information from an environment of the wearer of the wearable apparatus when the\nwearer is interacting with other people and not with images or screens.  For example, the wearer may wish to track encounters with certain people but not with any images of people in picture frames and/or on television screens, tablet screens, smartphone\nscreens, or the like.  Alternatively, the wearer may wish to track encounters with images separately from tracking physical encounters with other people.  Embodiments of the present disclosure may address this problem through techniques for assessing\nphysical presence of others based on images of the wearer's environment and executing appropriate actions based on the assessment.\nFIG. 20 illustrates an exemplary embodiment of a memory 2000 containing software modules consistent with the present disclosure.  Memory 200 may be included in apparatus 110 in lieu of or in combination with memory 550.  In some embodiments, the\nsoftware modules of memory 2000 may be combined with one or more software modules of memory 550 into one or more memories.  Memory 2000 may store more or fewer modules than those shown in FIG. 20.\nAs illustrated in FIG. 20, included in memory 2000 are software instructions to execute a person detection module 2001, a physical presence identification module 2002, and action module 2003.  Modules 2001, 2002, and 2003 may contain software\ninstructions for execution by at least one processing device, e.g., processor 210, included in a wearable apparatus, e.g., wearable apparatus 110.  In some embodiments, person detection module 2001, physical presence identification module 2002, and\naction module 2003 may cooperate to execute method 2200 of FIG. 22A, method 2230 of FIG. 22B, and/or method 2260 of FIG. 22C (or variants or combinations thereof).\nPerson detection module 2001 may be configured to analyze one or more images captured from a wearable apparatus to detect at least one person within the images.  For example, person detection module 2001 may be configured to identify a subset of\nthe captured data that includes at least one person.  In some embodiments, person detection module 2001 may be configured to receive a plurality of images and to identify one or more of the plurality of images that include at least one person.  For\nexample, module 2001 may receive a plurality of images of an environment surrounding a user wearing the wearable apparatus 110 and identify which of the plurality of images, if any, include at least one person.\nIn some embodiments, such an analysis may be performed by employing a facial recognition algorithm designed to detect facial features (e.g., mouth, eyes, etc.), facial contours, body shape, or any other suitable identifying feature of a person. \nIn other embodiments, the at least one person may be identified using a thermal signature algorithm designed to detect the presence of at least one person based on the heat generated by the at least one person.  In such embodiments, the wearable\napparatus 110 may capture thermal images, either alone or in combination with visual images, for processing by the thermal signature algorithm.  In such embodiments, module 2001 (or another module not depicted) may align the thermal images with the\nvisual images such that at least one person detected on a thermal image may then be identified on a corresponding visual image.  Thermal recognition of the at least one person may be desirable in implementations in which the wearable device 110 is\noperating in reduced lighting situations.\nIn some embodiments, the at least one person may be identified through the application of one or more image classification techniques.  For example, at least one image classification technique may be used to classify at least one feature of an\nimage.  In some embodiments, an image classification technique may include one or more of image enhancement, edge detection, image analysis, and data extraction.  In some examples, the at least one person may be detected using a facial detection\nalgorithm, using a neural network trained to detect faces and/or persons in images, and so forth.  Specific examples of the methods for identifying at least one person are exemplary only, and a person of ordinary skill in the art will recognize other\nmethods for identifying the at least one person that remain consistent with the present disclosure.\nPhysical presence identification module 2002 may be configured to receive one or more images of detected persons and further analyze the one or more images to determine whether the detected persons are physically present or visible on a display\nof a device.  For example, module 2002 may receive one or more images in which person detection module 2001 has determined include at least one person and may identify if one or more persons of the at least one person are physically present and/or\nvisible on a display of a device.\nIn some embodiments, physical presence and/or visibility on a screen of a detected person may be determined based on analysis of one or more images of the detected person.  For example, one or more algorithms may analyze and/or compare one or\nmore of detected facial features (e.g., mouth, eyes, etc.), facial contours, body shape, or any other suitable identifying feature of a person to determine whether the detected person was physically present in the one or more images or was visible on a\ndisplay in the one or more images.  For example, visibility on a display may cause one or more proportions of the detected person in the one or more images to differ from one or more expected proportions if the detected person were physically present. \nIn some examples, visibility on a display may cause pixilation and/or other aberrations of the detected person in the one or more images to exceed one or more expected values of such aberrations if the detected person were physically present.\nIn some embodiments, physical presence and/or visibility on a screen of a detected person may be determined based on analysis of areas adjacent to or surrounding the detected person and/or in the background of the detected person.  Such analysis\nmay include determining whether the detected person is bordered by a frame or other surface, such as the edge of a display screen.  In such instances, the analysis may include determining that certain features or parts (e.g., arms, legs, hands, feet,\netc.) of the detected person do not continue beyond a surface or edge depicted and detected in one or more images captured by the wearable apparatus, such as for example, when only a person's face is shown on display screen, but the person's body is not\npresent on the display.  Such analysis may further or alternatively include comparing features or parts of the detected person to other areas of one or more captured images to determine whether the detected person is not fully visible in the captured\nimages.  In some examples, such analysis may indicate that a detected person is interacting with items known to be the environment of a wearer of the wearable apparatus (e.g., an item previously identified in the wearer's environment, and now associated\nwith a detected person).  For example, the wearer may have picked up a particular item (e.g., a cup of coffee) and held the item in his or her hand previously before setting the item down (e.g., on a table) or handing it to another person. \nIdentification of the item in the hand of a detected person (or near the detected person) may contribute to or constitute a determination to that the detected person is physically present in the wearer's environment.  Such a determination may further\ntaken into an amount of time that has elapsed since the wearer or detected person held the item, interacted with the item, or otherwise encountered the item in his or her environment.  For example, if the wearer held the item within a predetermined time\nperiod (e.g., within 5 seconds, within 10 seconds, within 30 seconds, within 45 seconds, within 1 minute, etc.) before or after the item was identified in association with a detected person, then physical presence identification module 2002 may determine\nthat the detected person is in fact in the environment of the wearer.\nIn some embodiments, physical presence and/or visibility on a screen of a detected person may be determined based on analysis of one or more 3D images of the detected person.  For example, a person visible on a screen may correspond to a flat\nsurface in the 3D images, while a person which is physically present may correspond to a surface with convex and/or concave curves typical to a physical human body and/or face.\nIn some embodiments, physical presence identification module 2002 may be configured to receive one or more images, and analyze the one or more images using a detector configured to detect persons that are physically present and not to detect\npersons that are visible on a screen and/or in a photo.  For example, the detector may comprise using a classifier that classifies images and/or regions of images as depicting people that are physically present.  The classifier may be a result of a\nmachine learning algorithm trained on training examples, where a training example may comprise images and a label of the desired result.  For example, the classifier may comprise a neural network trained to detect persons that are physically present in\nimages.\nIn some embodiments, physical presence identification module 2002 may be configured to receive one or more images, and analyze the one or more images using a detector configured to detect persons that are visible on a screen and/or in a photo\nand not to detect persons that are physically present.  For example, the detector may comprise using a classifier that classifies images and/or regions of images as depicting people that are visible on a screen and/or in a photo.  The classifier may be a\nresult of a machine learning algorithm trained on training examples, where a training example may comprise images and a label of the desired result.  For example, the classifier may comprise a neural network trained to detect persons that are visible on\na screen and/or in a photo in images.\nIn some embodiments, physical presence and/or visibility on a screen of a detected person may be determined based on analysis of one or more images depicting the detected person.  For example, regions of the images containing the detected person\nmay be compared with regions of the images that do not contain the detected person, and the determination may be made based on the comparison results.  For example, shadows may be detected in regions of the images containing the detected person and in\nregions of the images that do not contain the detected person, properties of the shadows, such as the angle of the shadow, may be compared, and the determination may be made based on the comparison results, for example determining that the detected\nperson is physically present if and only if the angles match.  For example, statistics about colors and/or edges may be collected from regions of the images containing the detected person and from regions of the images that do not contain the detected\nperson, the statistics may be compared, and the determination may be made based on the comparison results.\nIn some embodiments, a determination as to whether or not a detected person is physically present may be based on a scoring or weighting approach in which certain cues (e.g., indicators tending to establish or not establish physical presence)\nare considered to determine whether or not a detected person is physically present or is depicted in an image on a display screen.  As such, weights or scores may be assigned to one or more cues determined from image analysis, and physical presence\nidentification module 2002 may use a cumulative weight value or score value for the one or more cues to make a determination (e.g., the cumulative weight value or score value exceeding a predetermined threshold may result in a determination that a\ndetected person is or is not physically present with the wearer).  Any combination of weighting and or scoring any one or more cues is contemplated and consistent with the disclosed embodiments.\nIn addition or in combination, certain rules or predetermined rule combinations may contribute to such determinations by physical presence identification module 2002.  For example, if a detected person's hands and/or feet are visible in one or\nmore captured images, and at least a portion of a background of the detected person includes an item known to be in the environment of the wearer of the wearable apparatus, physical presence identification module 2002 may conclude that the detected\nperson is physically present in the wearer's environment.  In contrast, if a detected person's hands and/or feet are not visible in one or more captured images, and at least a portion of a background of the detected person has no known commonality with\nthe environment of the wearer of the wearable apparatus (e.g., no items known to be in the environment of the wearer), physical presence identification module 2002 may conclude that the detected person is not physically present in the wearer's\nenvironment and, instead, is visible on a display or in photograph.  In some embodiments, any one of these example cues individually (e.g., that the detected person's hands and/or feet are or are not visible) may constitute a sufficient determination to\narrive at a conclusion as to physical presence or a lack of physical presence.\nSpecific examples of the methods for determining whether a detected person is physically present or visible on a display are exemplary only, and a person of ordinary skill in the art will recognize other methods for identifying the at least one\nperson that remain consistent with the present disclosure.  In some examples, physical presence may be determined using an algorithm, using a neural network trained to detect physical presence in images, and so forth.\nAction module 2003 may be configured to select one or more actions based on whether the detected person is physically present in the environment of the user and/or based on whether the detected person is visible on the display of the device. \nFor example, action module 2003 may update at least one of a database and a social network profile based on information associated with a physical location of the wearable apparatus when the detected person is physically present.  In such an example, a\nsocial network profile of the wearer may be updated with a new status indicating that the wearer interacted with the detected person at the physical location.  By way of further example, action module 2003 may update a database or a social network based\non information associated with at least one of a time and a date (e.g., a time or a date of capture of one or more images) when the detected person is visible on a display.  In such an example, a social network profile of the wearer may be updated with a\nnew status indicating that the wearer watched a particular television show, movie, or other source of the detected person at the time and/or the date.\nAlternatively or concurrently to performing one or more actions such as those above, action module 2003 may identify, determine, and/or update one or more statistics based, at least in part, on whether the detected person is physically present\nin the environment of the user or visible on the display of the device.  The one or more statistics may, alternatively or concurrently, be based, at least in part, on information associated with at least one of a time and a date and/or associated with a\nphysical location of the wearable apparatus.  Accordingly, in some embodiments, the statistics may be accumulated over physical locations and/or time frames.\nIn some embodiments, action module 2003 may cause the selected one or more actions to be executed by providing information (e.g., one or more instructions) to a device paired with the wearable apparatus (e.g., a smartphone) and/or by providing\ninformation (e.g., one more instructions) to a remote server (e.g., server 250) over a network.  For example, action module 1704 may transmit one or more instructions to a paired device to update a social network profile displayed on a screen of the\npaired device to indicate that the user interacted with the detected person, watched a particular television show, or the like.\nModules 2001, 2002, and 2003 may be implemented in software, hardware, firmware, a mix of any of those, or the like.  For example, if the modules are implemented in software, they may be stored, for example, in memory 550.  However, in some\nembodiments, any one or more of modules 2001, 2002, and 2003 may, for example, be stored in processor 540 and/or located on server 250, which may include one or more processing devices.  Processing devices of server 250 may be configured to execute the\ninstructions of modules 2001, 2002, and 2003.  In some embodiments, aspects of modules 2001, 2002, and 2003 may include software, hardware, or firmware instructions (or a combination thereof) executable by one or more processors, alone or in various\ncombinations with each other.  For example, modules 2001, 2002, and 2003 may be configured to interact with each other and/or other modules of server 250 and/or a wearable camera system to perform functions consistent with disclosed embodiments.  In some\nembodiments, any of the disclosed modules may each include dedicated sensors (e.g., IR, image sensors, etc.) and/or dedicated application processing devices to perform the functionality associated with each module.\nFIG. 21A is an illustration of an example image 2100 captured by a wearable apparatus, such as wearable apparatus 110.  In the example of FIG. 21A, image 2100 contains a person 2110.  In the example of FIG. 21A, wearable apparatus 110 may\ndetermine that person 2110 is physically present in the environment of the wearer of apparatus 110.  This determination may be made using any one or more of the techniques discussed above in connection with FIG. 20.  Based on this determination, wearable\napparatus 110 may take one or more actions.  For example, apparatus 110 may update a social network account associated with the wearer to indicate that the wearer was with person 2110, or the like.  In such an example, apparatus 110 may include a\nphysical location of apparatus 110 in the update (e.g., to indicate that the wearer was with person 2110 at the Starbucks on K St NW in Washington, D.C.).\nFIG. 21B is an illustration of another example image 2150 captured by a wearable apparatus, such as wearable apparatus 110.  In the example of FIG. 21B, image 2150 contains a person 2160.  In the example of FIG. 21B, wearable apparatus 110 may\ndetermine that person 2160 is visible on a display of a device in the environment of the wearer of apparatus 110.  This determination may be made using any one or more of the techniques discussed above in connection with FIG. 20.  For example, as\ndepicted in FIG. 21B, person 2160 may be visible on a display of a television.  In other examples, person 2160 may be visible on a display of a tablet, a display of a laptop, a display of a smartphone, or the like.  In the example of FIG. 21B, wearable\napparatus 110 may determine that person 2160 is visible in a photo.\nBased on this determination, wearable apparatus 110 may take one or more actions.  For example, apparatus 110 may update a social network account associated with the wearer to indicate that the wearer was watching person 2110 (or a television\nshow or movie including person 211), or the like.  In such an example, apparatus 110 may include at least one of a time and date in the update (e.g., to indicate that the wearer was watching Casablanca at 9:05 pm on Monday, October 16).\nFIGS. 21A and 21B are examples of persons being detected and determined to be either physically present or visible on a display by wearable apparatus 110.  As would be understood by one of ordinary skill in the art, wearable apparatus 110 may\ncapture images throughout the user's day at a variety of locations as the environment surrounding the user changes.  For example, images may be captured when the user visits a coffee shop to meet a friend, commutes to and from work, relaxes in a living\nroom of the user's house, etc. In this way, wearable apparatus 110 may be configured to monitor the environment surrounding user 100 throughout the user's activities to identify exposure to one or more persons throughout the time user wears wearable\napparatus 110, and then determine whether detected persons are physically present or visible on a display and take appropriate actions based on the determinations.\nFIG. 22A illustrates a flowchart of an example method 2200 for causing execution of an action based on physical presence of a detected person.  Method 2200 may be implemented by at least one processing device (e.g., processor 210 of wearable\napparatus 110) and by a wearable image sensor (e.g., image sensor 220 of wearable apparatus 110) configured to capture a plurality of images from the environment of the user of the wearable apparatus.\nAt step 2205, the processing device may analyze at least one of the plurality of images to detect the person.  For example, received image data may be processed by software steps executed by person detection module 2001.  In some embodiments,\nthe processing device may compare one or more regions of the at least one image against a database of known patterns and/or to determine whether a person is included in the one or more regions.  Alternatively or concurrently, the processing device may\napply one or more classifiers to the one or more regions, and the one or more classifiers may output whether a person is detected in the one or more regions.  Still further, the processing device may execute one or more algorithms to detect facial\nfeatures (e.g., mouth, eyes, etc.), facial contours, body shape, or any other suitable identifying feature of a person.\nIn some embodiments, the detection may be binary.  In such embodiments, the comparison and/or classifiers may output true if a person is detected and false if a person is not detected.  In other embodiments, the detection may be threshold-based. In such embodiments, the comparison and/or classifiers may output a percentage or other measurement of confidence that the one or more regions include a person.  For example, the comparison may output a likelihood (e.g., a 58% likelihood) that the one or\nmore regions include a person.  By way of further example, the classifiers may output a score (e.g., a score of 8 out of 10) that the one or more regions include a person.  In such embodiments, the processing device may use a threshold (e.g., at least\n55%, at least 7 out of 10, etc.) to convert the output to a binary detection.\nAt step 2210, the processing device may analyze at least one of the plurality of images to determine whether the detected person is physically present in the environment of the user.  For example, received image data may be processed by software\nsteps executed by physical presence identification module 2002 executing any one or more of the techniques discussed above.  Similar to step 2205, the processing device may compare one or more regions of the at least one image against a database of known\npatterns and/or images to make the determination.  Alternatively or concurrently, the processing device may apply one or more classifiers to the one or more regions, and the one or more classifiers may output the determination.  Accordingly, in some\nembodiments, the analysis may include selecting, in the at least one of the plurality of images, one or more regions located within a threshold distance from the detected person and analyzing the selected one or more regions.  Additionally or\nalternatively, analyzing the at least one of the plurality of images to determine whether the person is physically present in the environment of the user may include analyzing whether the person is visible on a display of a device in the environment of\nthe user of the wearable apparatus.\nIn some embodiments, a confidence score or the like may be associated with the determination.  For example, the comparison may output the determination with a confidence score (e.g., a 64% confidence score).  By way of further example, the\nclassifiers may output the determination with a confidence score (e.g., a score of 4 out of 10).  In such embodiments, the processing device may use a threshold (e.g., at least 55%, at least 7 out of 10, etc.) to finalize the determination. \nAlternatively or concurrently, the comparison and/or classifiers may output a first confidence score or the like associated with a determination that the detected person is physically present in the environment of the user and a second confidence score\nor the like associated with a determination that the detected person is visible on a display of a device in the environment of the user of the wearable apparatus.  In such embodiments, the processing device may select the determination with the highest\nconfidence score.\nAt step 2215, the processing device may select at least one action based on whether the detected person is physically present in the environment of the user.  Selection of the at least one action may be facilitated via action module 2003.  For\nexample, the at least one action may include transmitting information associated with a physical location of the wearable apparatus, updating at least one of a database and a social network profile based on information associated with a physical location\nof the wearable apparatus; determining one or more statistics based, at least in part, on information associated with a physical location of the wearable apparatus; or the like.  As an example, when the detected person is physically present in the\nenvironment of the user, the processing device may update a database and/or a social network profile with a record of the interaction between the user and the detected person.  In such an example, the update may include a physical location of the\nwearable apparatus.  Alternatively or concurrently, the update may include a time and/or a date of the interaction.  In embodiments where the at least one action include determining one or more statistics, the processing device may update a count of\npersons associated with a physical location of the wearable apparatus and/or associated with a time and/or date of the interaction.\nAt step 2220, the processing device may cause the at least one selected action to be executed.  In some embodiments, causing the at least one selected action to be executed may include sending information to a device paired with the wearable\napparatus.  A device paired with the wearable apparatus may include a smartphone, a tablet, a laptop, or the like.  Accordingly, the processing device may send information to the paired device using a wired connection (such as a universal serial bus\n(USB), or the like), a direct wireless connection (such as Bluetooth.RTM., or the like), and/or an indirect wireless connection (such as WiFi, 4G, or the like).\nThe transmitted information may include at least one of the plurality of images, one or more regions of at least one image (e.g., one or more regions used to detect a person and/or identify an attribute), one or more determinations of whether\ndetected persons in the plurality of images are physically present or visible on a display, a physical location of the wearable apparatus, a time and/or a date of capture of the images, or the like.  Furthermore, the transmitted information may allow the\npaired device to execute one or more selected actions.  For example, the processing device may update at least one of a database and a social network profile based on information associated with a physical location of the wearable apparatus; determine\none or more statistics based, at least in part, on information associated with a physical location of the wearable apparatus; or the like.\nMethod 2200 may further include additional steps.  For example, the processing device may analyze at least one of the plurality of images to detect a second person.  The detection of the second person may be performed as the detection of the\nfirst person in step 2205.  Furthermore, method 2200 may further include analyzing at least one of the plurality of images to determine whether the second person is visible on a display of a device.\nIn some embodiments, one or more steps regarding the second person may be performed after steps 2205 to 2220 regarding the first person.  In other embodiments, one or more steps regarding the second person may be interspersed with steps 2205 to\n2220.  For example, the processing device may detect the second person after detecting the first person but before determining whether the first person is physically present.  By way of further example, the processing device may determine whether the\nsecond person is visible on a display of a device concurrently with determining whether the first person is physically present.\nIn some embodiments, method 2200 may include further basing the selection of the at least one action on whether the second person is visible on the display of the device.  For example, if a first detected person is determined as physically\npresent and a second detected person is determined as visible on a display, the processing device may update a database and/or a social network profile and include the first detected person in the update and omit the second detected person from the\nupdate, or the like.  By way of further example, the processing device may update a first statistic based on the determination that the first detected person is physically present and update a second statistic based on the determination that the second\ndetected person is visible on a display.  In another example, based on the determination that the first detected person is physically present and the second detected person is visible on a display, the processing device may update a database and/or a\nsocial network profile with information regarding the interaction of wearer and the first detected person with the second person, for example indicating that the wearer and the first detected person are in a video conference with the second detected\nperson.\nFIG. 22B illustrates a flowchart of an example method 2230 for causing execution of an action based on whether a detected person is visible on a display of a device.  Method 2230 may be implemented by at least one processing device (e.g.,\nprocessor 210 of wearable apparatus 110) and by a wearable image sensor (e.g., image sensor 220 of wearable apparatus 110) configured to capture a plurality of images from the environment of the user of the wearable apparatus.\nAt step 2235, the processing device may analyze at least one of the plurality of images to detect the person.  For example, received image data may be processed by software steps executed by person detection module 2001.  In some embodiments,\nthe processing device may compare one or more regions of the at least one image against a database of known patterns and/or to determine whether a person is included in the one or more regions.  Alternatively or concurrently, the processing device may\napply one or more classifiers to the one or more regions, and the one or more classifiers may output whether a person is detected in the one or more regions.  Still further, the processing device may execute one or more algorithms to detect facial\nfeatures (e.g., mouth, eyes, etc.), facial contours, body shape, or any other suitable identifying feature of a person.\nIn some embodiments, the detection may be binary.  In such embodiments, the comparison and/or classifiers may output true if a person is detected and false if a person is not detected.  In other embodiments, the detection may be threshold-based. In such embodiments, the comparison and/or classifiers may output a percentage or other measurement of confidence that the one or more regions include a person.  For example, the comparison may output a likelihood (e.g., a 58% likelihood) that the one or\nmore regions include a person.  By way of further example, the classifiers may output a score (e.g., a score of 8 out of 10) that the one or more regions include a person.  In such embodiments, the processing device may use a threshold (e.g., at least\n55%, at least 7 out of 10, etc.) to convert the output to a binary detection.\nAt step 2240, the processing device may analyze at least one of the plurality of images to determine whether the detected person is visible on the display of the device.  For example, received image data may be processed by software steps\nexecuted by physical presence identification module 2002 executing any one or more of the techniques discussed above.  Similar to step 2235, the processing device may compare one or more regions of the at least one image against a database of known\npatterns and/or images to make the determination.  Alternatively or concurrently, the processing device may apply one or more classifiers to the one or more regions, and the one or more classifiers may output the determination.  Accordingly, in some\nembodiments, the analysis may include selecting, in the at least one of the plurality of images, one or more regions located within a threshold distance from the detected person and analyzing the selected one or more regions.  Additionally or\nalternatively, analyzing the at least one of the plurality of images to determine whether the detected person is visible on the display of the device may include analyzing whether the detected person is physically present in the environment of the user\nof the wearable apparatus.\nIn some embodiments, a confidence score or the like may be associated with the determination.  For example, the comparison may output the determination with a confidence score (e.g., a 64% confidence score).  By way of further example, the\nclassifiers may output the determination with a confidence score (e.g., a score of 4 out of 10).  In such embodiments, the processing device may use a threshold (e.g., at least 55%, at least 7 out of 10, etc.) to finalize the determination. \nAlternatively or concurrently, the comparison and/or classifiers may output a first confidence score or the like associated with a determination that the detected person is visible on the display of the device and a second confidence score or the like\nassociated with a determination that the detected person is physically present in the environment of the user of the wearable apparatus.  In such embodiments, the processing device may select the determination with the highest confidence score.\nAt step 2245, the processing device may select at least one action based on whether the detected person is visible on the display of the device.  Selection of the at least one action may be facilitated via action module 2003.  For example, the\nat least one action may include transmitting information associated with at least one of a time and a date (e.g., a time and a date of capture of at least one of the plurality of images), updating a database or a social network based on information\nassociated with at least one of a time and a date (e.g., a time and a date of capture of at least one of the plurality of images); identifying one or more statistics based, at least in part, on information associated with at least one of a time and a\ndate (e.g., a time and a date of capture of at least one of the plurality of images); or the like.  As an example, when the detected person is visible on the display of the device, the processing device may update a database and/or a social network\nprofile with a television show, movie, or other media in which the detected person appears.  In such an example, the update may include a time and/or a date of capture of the at least one image with the detected person.  Alternatively or concurrently,\nthe update may include a physical location associated with the wearable apparatus.  In embodiments where the at least one action include identifying one or more statistics, the processing device may update a count of displays (or television shows or\nmovies or the like) associated with a time and/or a date of capture of the at least one image with the detected person and/or associated with a physical location of the wearable apparatus.\nAt step 2250, the processing device may cause the at least one selected action to be executed.  In some embodiments, causing the at least one selected action to be executed may include sending information to a device paired with the wearable\napparatus.  A device paired with the wearable apparatus may include a smartphone, a tablet, a laptop, or the like.  Accordingly, the processing device may send information to the paired device using a wired connection (such as a universal serial bus\n(USB), or the like), a direct wireless connection (such as Bluetooth.RTM., or the like), and/or an indirect wireless connection (such as WiFi, 4G, or the like).\nThe transmitted information may include at least one of the plurality of images, one or more regions of at least one image (e.g., one or more regions used to detect a person and/or identify an attribute), one or more determinations of whether\ndetected persons in the plurality of images are visible on the display or physically present, a physical location of the wearable apparatus, a time and/or a date of capture of the images, or the like.  Furthermore, the transmitted information may allow\nthe paired device to execute one or more selected actions.  For example, the processing device may update at least one of a database and a social network profile based on information associated with a time and/or a date; identify one or more statistics\nbased, at least in part, on information associated with a time and/or a date; or the like.\nMethod 2230 may further include additional steps.  For example, the processing device may analyze at least one of the plurality of images to determine that the detected person is visible on the display device and is taking part in a video\nconference and further base the selection of the at least one action on the determination that the detected person is visible on the display device and is taking part in the video conference.  In such an example, the processing device may update a\ndatabase and/or a social network profile with a record of the video conference in which the detected person appears.  In such an example, the update may include a time and/or a date of the videoconference.\nBy way of additional example, the processing device may analyze at least one of the plurality of images to determine that the detected person is included in at least one of a video image and a still image and further base the selection of the at\nleast one action on the determination that the detected person is included in at least one of a video image and a still image.  For example, if the detected person is included in a video image, the processing device may then analyze at least one of the\nplurality of images to determine that the detected person is visible on the display device and is taking part in a video conference.  Alternatively or concurrently, the processing device may update a database and/or a social network profile with a\ntelevision show, movie, or other media in which the detected person appears.  In such an example, the update may include a time and/or a date of capture of the at least one image with the detected person.  In other embodiments, if the detected person is\nincluded in a still image, the processing device may update a database and/or a social network profile with a record of the image in which the detected person appears.  Alternatively, the processing device may take no action.\nIn another example, the processing device may analyze at least one of the plurality of images to detect a second person.  The detection of the second person may be performed as the detection of the first person in step 2235.  Furthermore, method\n2230 may further include analyzing at least one of the plurality of images to determine whether the second person is physically present in the environment of the user.\nIn some embodiments, one or more steps regarding the second person may be performed after steps 2235 to 2250 regarding the first person.  In other embodiments, one or more steps regarding the second person may be interspersed with steps 2235 to\n2250.  For example, the processing device may detect the second person after detecting the first person but before determining whether the first person is visible on the display of the device.  By way of further example, the processing device may\ndetermine whether the second person is physically present in the environment of the user concurrently with determining whether the first person is visible on the display of the device.\nIn some embodiments, method 2230 may include further basing the selection of the at least one action on whether the second person is physically present in the environment of the user.  For example, if a first detected person is determined as\nvisible on the display and a second detected person is determined as physically present, the processing device may update a database and/or a social network profile and include the second detected person in the update and omit the first detected person\nfrom the update, or the like.  By way of further example, the processing device may update a first statistic based on the determination that the first detected person is visible on the display and update a second statistic based on the determination that\nthe second detected person is physically present.\nFIG. 22C illustrates a flowchart of an example method 2260 for causing execution of an action based on physical presence of a detected person.  Method 2260 may be implemented by at least one processing device (e.g., processor 210 of wearable\napparatus 110) and by a wearable image sensor (e.g., image sensor 220 of wearable apparatus 110) configured to capture a plurality of images from the environment of the user of the wearable apparatus.\nAt step 2265, the processing device may analyze at least one of the plurality of images to detect the person.  For example, received image data may be processed by software steps executed by person detection module 2001.  In some embodiments,\nthe processing device may compare one or more regions of the at least one image against a database of known patterns and/or to determine whether a person is included in the one or more regions.  Alternatively or concurrently, the processing device may\napply one or more classifiers to the one or more regions, and the one or more classifiers may output whether a person is detected in the one or more regions.  Still further, the processing device may execute one or more algorithms to detect facial\nfeatures (e.g., mouth, eyes, etc.), facial contours, body shape, or any other suitable identifying feature of a person.\nIn some embodiments, the detection may be binary.  In such embodiments, the comparison and/or classifiers may output true if a person is detected and false if a person is not detected.  In other embodiments, the detection may be threshold-based. In such embodiments, the comparison and/or classifiers may output a percentage or other measurement of confidence that the one or more regions include a person.  For example, the comparison may output a likelihood (e.g., a 58% likelihood) that the one or\nmore regions include a person.  By way of further example, the classifiers may output a score (e.g., a score of 8 out of 10) that the one or more regions include a person.  In such embodiments, the processing device may use a threshold (e.g., at least\n55%, at least 7 out of 10, etc.) to convert the output to a binary detection.\nAt step 2270, the processing device may analyze at least one of the plurality of images to determine whether the detected person is physically present in the environment of the user or whether a graphical representation of the detected person\nappears in the environment of the user.  For example, received image data may be processed by software steps executed by physical presence identification module 2002 executing any one or more of the techniques discussed above.  Similar to step 2265, the\nprocessing device may compare one or more regions of the at least one image against a database of known patterns and/or images to make the determination.  Alternatively or concurrently, the processing device may apply one or more classifiers to the one\nor more regions, and the one or more classifiers may output the determination.  Accordingly, in some embodiments, the analysis may include selecting, in the at least one of the plurality of images, one or more regions located within a threshold distance\nfrom the detected person and analyzing the selected one or more regions.\nIn some embodiments, a confidence score or the like may be associated with the determination.  For example, the comparison may output the determination with a confidence score (e.g., a 64% confidence score).  By way of further example, the\nclassifiers may output the determination with a confidence score (e.g., a score of 4 out of 10).  In such embodiments, the processing device may use a threshold (e.g., at least 55%, at least 7 out of 10, etc.) to finalize the determination. \nAlternatively or concurrently, the comparison and/or classifiers may output a first confidence score or the like associated with a determination that the detected person is physically present in the environment of the user and a second confidence score\nor the like associated with a determination that a graphical representation of the detected person appears in the environment of the user.  In such embodiments, the processing device may select the determination with the highest confidence score.\nAt step 2275, the processing device may select a first action after the determination is made that the detected person is physically present in the environment of the user.  Selection of the first action may be facilitated via action module\n2003.  For example, the first action may include transmitting information associated with a physical location of the wearable apparatus, updating at least one of a database and a social network profile based on information associated with a physical\nlocation of the wearable apparatus; determining one or more statistics based, at least in part, on information associated with a physical location of the wearable apparatus; or the like.  As an example, when the detected person is physically present in\nthe environment of the user, the processing device may update a database and/or a social network profile with a record of the interaction between the user and the detected person.  In such an example, the update may include a physical location of the\nwearable apparatus.  Alternatively or concurrently, the update may include a time and/or a date of the interaction.  In embodiments where the first action include determining one or more statistics, the processing device may update a count of persons\nassociated with a physical location of the wearable apparatus and/or associated with a time and/or date of the interaction.\nAt step 2280, the processing device may select a second action different from the first action after the determination is made that the graphical representation of the detected person appears in the environment of the user.  Selection of the\nsecond action may be facilitated via action module 2003.  For example, the second action may include transmitting information associated with at least one of a time and a date (e.g., a time and a date of capture of at least one of the plurality of\nimages), updating a database or a social network based on information associated with at least one of a time and a date (e.g., a time and a date of capture of at least one of the plurality of images); identifying one or more statistics based, at least in\npart, on information associated with at least one of a time and a date (e.g., a time and a date of capture of at least one of the plurality of images); or the like.  As an example, when the graphical representation of the detected person appears in the\nenvironment of the user, the processing device may update a database and/or a social network profile with a television show, movie, or other media in which the graphical representation appears.  In such an example, the update may include a time and/or a\ndate of capture of the at least one image with the detected person.  Alternatively or concurrently, the update may include a physical location associated with the wearable apparatus.  In embodiments where the second action include identifying one or more\nstatistics, the processing device may update a count of displays (or television shows or movies or the like) associated with a time and/or a date of capture of the at least one image with the detected person and/or associated with a physical location of\nthe wearable apparatus.\nIn addition, the first action and/or the second action may include taking no action.  Accordingly, the wearable apparatus 110 may take no action if the detected person is physically present in the environment of the user and/or if a graphical\nrepresentation of the detected person appears in the environment of the user.  Accordingly, wearable apparatus may only update at least one of a database and a social network profile, determine or identify one or more statistics or the like when the\ndetected person is physically present in the environment of the user and/or if a graphical representation of the detected person appears in the environment of the user.\nAt step 2285, the processing device may cause the first action or the second action to be executed.  In some embodiments, causing the first action or the second action to be executed may include sending information to a device paired with the\nwearable apparatus.  A device paired with the wearable apparatus may include a smartphone, a tablet, a laptop, or the like.  Accordingly, the processing device may send information to the paired device using a wired connection (such as a universal serial\nbus (USB), or the like), a direct wireless connection (such as Bluetooth.RTM., or the like), and/or an indirect wireless connection (such as WiFi, 4G, or the like).\nThe transmitted information may include at least one of the plurality of images, one or more regions of at least one image (e.g., one or more regions used to detect a person and/or identify an attribute), one or more determinations of whether\ndetected persons in the plurality of images are visible on the display or physically present, a physical location of the wearable apparatus, a time and/or a date of capture of the images, or the like.  Furthermore, the transmitted information may allow\nthe paired device to execute one or more selected actions.  For example, the processing device may update at least one of a database and a social network profile based on information associated with a time and/or a date; identify one or more statistics\nbased, at least in part, on information associated with a time and/or a date; or the like.\nMethod 2260 may further include additional steps.  For example, any of the additional steps of method 2230 and/or method 2200 may be included in method 2260.\nUpdating Profile Information for a Person Viewed by Multiple Wearable Apparatuses\nIn some embodiments, a plurality of persons may use a plurality of wearable apparatuses, such as wearable apparatus 110, and a profile associated with a particular person may be updated using information from one or more of the plurality of\nwearable apparatuses.  In some embodiments in which the profile is updated based on information from a plurality of wearable apparatuses, the profile may be considered to be a joint profile as the profile may reflect information collected and/or updated\nfrom the plurality of wearable apparatuses.\nIn certain aspects, the profile may be stored and updated on a server and/or on a shared storage and/or on a shared database to which the information from the one or more wearable apparatuses is sent.  In doing so, embodiments consistent with\nthe present disclosure may address the technical problem of extracting, collating, and indexing information from a plurality of wearable apparatuses and keeping the information updated such that it remains useful rather than stale, out-of-date, and/or\ninaccurate.  Further, collecting and leveraging data from multiple wearable apparatuses may contribute to an approved accuracy of a particular person's profile.  For example, updates to the profile may be more frequent and therefore are more likely to be\naccurate and complete.  Moreover, the improved accuracy and completeness of the profile may provide for the delivery or relevant news, advertisements, or the like to the person associated with the profile.  Embodiments of the present disclosure may\naddress this problem through techniques for managing information extracted or determined from images of multiple wearable devices and properly indexing and storing the extracted or determined information in a database of profiles.\nAlthough discussed herein in association with a profile related to a particular person, the disclosed systems and methods may equally apply to updating and/or managing a profile related to an object or a place.\nFIG. 23 illustrates an exemplary embodiment of a memory 2300 containing software modules consistent with the present disclosure.  Memory 2300 may be included in apparatus 110 in lieu of or in combination with memory 550.  In some embodiments,\none or more software modules of memory 2300 may be stored in a remote server, e.g., server 250.  Memory 2300 may store more or fewer modules than those shown in FIG. 23.\nAs illustrated in FIG. 23, included in memory 2300 are software instructions to execute a person detection module 2301, an identification information module 2302, an auxiliary information module 2303, and a profile module 2304.  Modules 2301,\n2302, 2303, and 2304 may contain software instructions for execution by at least one processing device, e.g., one or more processes, included in a remote server, e.g., server 250.  In some embodiments, person detection module 2301, identification\ninformation module 2302, auxiliary information module 2303, and profile module 2304 may cooperate to execute method 2500 (or a variant thereof) of FIG. 25.\nPerson detection module 2301 may be configured to analyze one or more images captured from a wearable apparatus to detect at least one person within the images.  For example, person detection module 2301 may be configured to identify a subset of\nthe captured data that includes at least one person.  In some embodiments, person detection module 2301 may be configured to receive a plurality of images and to identify one or more of the plurality of images that include at least one person.  For\nexample, module 2301 may receive a plurality of images of an environment surrounding a user wearing wearable apparatus 110 and identify which of the plurality of images include at least one person.\nIn some embodiments, such an analysis may be performed by employing a facial recognition algorithm designed to detect facial features (e.g., mouth, eyes, etc.), facial contours, body shape, or any other suitable identifying feature of a person. \nIn other embodiments, the at least one person may be identified using a thermal signature algorithm designed to detect the presence of at least one person based on the heat generated by the at least one person.  In such embodiments, the wearable\napparatus 110 may capture thermal images, either alone or in combination with visual images, for processing by the thermal signature algorithm.  Thermal recognition of the at least one person may be desirable in implementations in which the wearable\ndevice 110 is operating in reduced lighting situations.  In some embodiments, the at least one person may be identified through the application of one or more image classification techniques.  For example, at least one image classification technique may\nbe used to classify at least one feature of an image.  In some embodiments, an image classification technique may include one or more of image enhancement, edge detection, image analysis, and data extraction.  Specific examples of the methods for\nidentifying at least one person are exemplary only, and a person of ordinary skill in the art will recognize other methods for identifying the at least one person that remain consistent with the present disclosure.  In some examples, the at least one\nperson may be detected using a facial detection algorithm, using a neural network trained to detect faces and/or persons in images, and so forth.\nIdentification information module 2302 may be configured to obtain identification information associated with the detected persons.  For example, identification information may include a name, birthday, or other real life indicator of the\ndetected person and/or may include an identification number, a username, or other artificial (optionally anonymous) indicator of the detected person.  In some embodiments, the identification information may comprise a plurality of indicators.\nIn some embodiments, identification information of a detected person may be obtained based on analysis of one or more images of the detected person.  For example, one or more algorithms may analyze one or more of detected facial features (e.g.,\nmouth, eyes, etc.), facial contours, body shape, or any other suitable identifying feature of a person to determine one or more indicators associated with the detected person.  In another example, identification information module 2302 may obtain\nidentification information through facial recognition.  Identification information module 2302 may then access a database included, for example, in remote server 250 and/or a database accessible over a network in order to search for and retrieve\ninformation related to the detected person.\nIn some embodiments, identification information module 2302 may apply at least one hashing function on at least part of at least one image of the detected person captured by the wearable image sensor to obtain a hash value of the detected\nperson.  Accordingly, obtaining the identification information may include obtaining a hash value associated with the detected person and accessing a plurality of hash values to determine the existence of a hash value associated with the detected person\nbased on the obtained hash value.  In some embodiments, then, a plurality of hash values may be stored in a lookup database, which may be separate from or form a part of the database of profiles.\nIn some examples, the at least part of at least one image of the detected person may comprise at least one image of a face of the detected person.  By hashing the face (or any other portion of at least one image) of the detected person,\nidentification information module 2302 may allow for indexing and lookup of the detected person in the database without compromising the anonymity of the detected person.  Accordingly, in such embodiments, even if the hash value of the detected person\nwere intercepted, for example, while traveling across a computer network, the identity of the detected person may remain hidden from the intercepting party.\nIn some examples, the at least one hashing function may comprise obtaining an identifier of a person (such as a unique identifier, a nearly unique identifier, a name, a login name, a user name from a social network, and so forth), for example\nusing a person recognition algorithm and/or facial recognition algorithm, and evaluating a hash function on the unique identifier.  Examples of such hash functions may include a cryptographic hash function, a perfect hash function, a nonlinear table\nlookup hash function, and so forth.  In some examples, the at least one hashing function may comprise projecting the image of the detected person (or of the face of the detected person) to an n-dimensional space to obtain an n-dimensional vector,\nquantizing the n-dimensional vector, and using the resulting quantized n-dimensional vector as a hash value and/or evaluating a hash function on the quantized n-dimensional vector.  In some cases, the projection function may be the result of training a\nmachine learning dimensional reduction algorithm on training examples, where a training example may comprise an image of a person and/or a face with a desired it-dimensional vector and/or indication of desired distance and/or proximity to other\ndata-points in the n-dimensional space.\nAuxiliary information module 2303 may be configured to obtain auxiliary information associated with the detected person.  For example, auxiliary information module 2303 may obtain position information, a capture time of at least one of the one\nor more images, information associated with a user of the wearable image sensor (e.g., identification information associated with the user, one or more images of the user, or the like).\nAlternatively or concurrently, auxiliary information may be obtained from analysis of the one or more images of the detected person.  For example, auxiliary information module 2303 may analyze one or more of detected facial features, facial\ncontours, body position, or the like to determine an emotional state of the detected person and/or a facial expression of the detected person.  In another example, auxiliary information module 2303 may analyze the one or more images to determine\ninformation associated with a distance (e.g., a measurement of the distance, a direction of the distance, etc.) of the wearable image sensor to the detected person.  In some embodiments, auxiliary information may be obtained from analysis of one or more\nimages of other detected persons.  Accordingly, the auxiliary information may include information related to a second person appearing in the one or more images with the detected person.  For example, auxiliary information module 2303 may analyze one or\nmore of detected facial features, facial contours, body position, or the like to obtain information associated with at least one other person.  In certain aspects, the detected person and the at least one other detected person may be detected in the same\nimage.\nAlternatively or concurrently, auxiliary information may be obtained from input to a wearable apparatus.  For example, auxiliary information module 2303 may receive at least one of audio topics and video topics from the wearable apparatus (or\nfrom user input into the wearable apparatus).  Similarly, an emotional state of the detected person, a facial expression of the detected person, and/or information associated with at least one other person may be obtained from user input into the\nwearable apparatus.\nMoreover, the auxiliary information may directly include at least part of an image.  For example, at least part of an image of the one or more images of the detected person may be included in the auxiliary information.  Alternatively or\nconcurrently, at least part of an image of the one or more images of other detected persons may be included in the auxiliary information.\nIn some embodiments, auxiliary information module 2303 may be configured to receive one or more images of detected persons and further analyze the one or more images to obtain at least one property associated with the detected person (e.g., age,\ngender, weight, height, facial expression, emotional state, etc.) associated with the identified persons.\nIn some embodiments, the at least one property of a detected person may be determined based on further analysis of one or more images of the detected person.  For example, one or more algorithms may analyze one or more of detected facial\nfeatures (e.g., mouth, eyes, etc.), facial contours, body shape, or any other suitable identifying feature of a person to estimate or determine information such as a the detected person's age, gender, weight, height, facial expression, emotional state,\netc.\nIn some embodiments, auxiliary information module 2303 may determine an identity of a detected person (e.g., through facial recognition) or receive identification information from identification information module 2302, and then access one or\nmore databases (stored, e.g., locally in a memory of wearable apparatus 110 and/or accessible remotely over a network, e.g., such as by accessing server 250) to retrieve at least one property of the detected person.  For example, after identifying a\ndetected person as a particular individual (e.g., determining a match based on facial recognition to a known person's image), auxiliary information module 2303 may access a database to retrieve information about the detected person, such as the detected\nperson's age, gender, family members, friends, etc.\nIn some embodiments, the auxiliary information may include information related to at least one item associated with the detected person.  For example, auxiliary information module 2303 may analyze one or more regions of the one or more images to\nidentify the at least one item.  In some embodiments, the regions may include regions within a particular distance of the detected person.  In some embodiments, the at least one item may include a product appearing in the one or more images with the\ndetected person.  For example, auxiliary information module 2303 may compare at least a portion of the one or more regions to known patterns and/or images to identify the product.  Alternatively or concurrently, auxiliary information module 2303 may use\none or more classifiers on the one or more regions to identify the product.\nIn some embodiments, the auxiliary information may include information related to an action performed by the detected person.  For example, using one or more comparisons to known patterns and/or images and/or using one or more classifiers,\nauxiliary information module 2303 may identify an action being performed by the detected person.  In such an example, auxiliary information module 2303 may identify the detected person as cheering if a facial expression of the detected person matches\nknown images and/or patterns of people cheering and/or is identified as an image of cheering by one or more classifiers trained to recognize people cheering.\nProfile module 2304 may be configured to identify, in a database storing a plurality of profiles, a profile associated with the detected person based on the identification information and update the identified profile based on the auxiliary\ninformation.  For example, profile module 2304 may identify the profile using at least a portion of the identification information.  The profile may be included in a database indexed by identification information.  Accordingly, in some embodiments,\nprofile module 2304 may use an indicator determined by identification information module 2302 and query an index of indicators to identify the profile associated with the detected person.  Alternatively or concurrently, profile module 2304 may query the\nidentification information against the profiles directly (e.g., by performing a fuzzy search of images in the profiles against at least a portion of the one or more images of the detected person) to identify the profile associated with the detected\nperson.  Alternatively or concurrently, profile module 2304 may use a hash value, such as a hash value calculated by identification information module 2302, to select a profile.\nIn some embodiments, profile module 2304 may update the profile by adding at least a portion of the auxiliary information to the profile.  Additionally or alternatively, profile module 2304 may update the profile by adding at least a portion of\nthe identification information to the profile.  For example, profile module 2304 may store at least a portion of the one or more images of the detected person and/or at least a portion of one or more images of other detected persons in the profile.  In\nanother example, profile module 2304 may store an emotional state of the detected person, a facial expression of the detected person, information associated with at least one other person, position information, a capture time of at least one of the one\nor more images, information associated with a user of the wearable image sensor, or other portions of the auxiliary information (and/or identification information) in the profile.\nAlternatively or additionally, profile module 2304 may update the profile based on the auxiliary information (and/or the identification information).  For example, information associated with a user of the wearable image sensor may be used to\nupdate a network of social connections of the detected person stored within the profile.  By way of further example, an emotional state of the detected person and/or a facial expression of the detected person may be used to update an emotional timeline\nof the detected person stored within the profile.\nIn some embodiments, profile module 2304 may also provide information based on the identified profile.  For example, profile module 2304 may provide the information to a device paired with the wearable apparatus (e.g., a smartphone or tablet)\nand/or may provide the information to a remote server (e.g., server 250) over a network.  For example, profile module 2304 may transmit information from the identified profile to a paired device to alert a user of the wearable apparatus of the name of\nthe detected person.  In another example, profile module 2304 may transmit a determined emotional state of the detected person to alert the user that the detected person is feeling sad and may desire comforting.\nModules 2301, 2302, 2303, and 2304 may be implemented in software, hardware, firmware, a mix of any of those, or the like.  For example, if the modules are implemented in software, they may be stored, for example, in memory 550.  However, in\nsome embodiments, any one or more of modules 2301, 2302, 2303, and 2304 may, for example, be stored in processor 540 and/or located on server 250, which may include one or more processing devices.  Processing devices of server 250 may be configured to\nexecute the instructions of modules 2301, 2302, 2303, and 2304.  In some embodiments, aspects of modules 2301, 2302, 2303, and 2304 may include software, hardware, or firmware instructions (or a combination thereof) executable by one or more processors,\nalone or in various combinations with each other.  For example, modules 2301, 2302, 2303, and 2304 may be configured to interact with each other and/or other modules of server 250 and/or a wearable camera system to perform functions consistent with\ndisclosed embodiments.  In some embodiments, any of the disclosed modules may each include dedicated sensors (e.g., IR, image sensors, etc.) and/or dedicated application processing devices to perform the functionality associated with each module.\nFIG. 24 is an illustration of an example profile 2400 associated with a person.  In some embodiments, at least some of the information stored in profile 2400 may include or be based upon information collected by one or more wearable apparatuses,\nsuch as wearable apparatus 110.\nAs would be understood by one of ordinary skill in the art, wearable apparatus 110 may capture images throughout a user's day at a variety of locations as the environment surrounding the user changes.  In this way, wearable apparatus 110 may be\nconfigured to capture information related to the user and/or various detected persons.  Disclosed embodiments may use the captured information to construct profiles associated with the user and/or detected persons and index those profiles within one or\nmore databases.\nIn the example of FIG. 24, profile 2400 includes position information 2401.  For example, position information 2401 may include indicators of locations in which a person was detected by one or more wearable apparatuses.  For example, position\ninformation 2401 may include indicators that the person was detected in zip code 20001; at the Starbucks on New York Ave NW; at particular GPS coordinates; at a street address; in a particular country, city, state, and/or county; at a particular\nlandmark; or the like.\nAs further illustrated in FIG. 24, profile 2400 may include times and/or distances 2402 related to the capture of one or more images of the person.  For example, profile 2400 may include information such as the person being imaged at a\nparticular date and/or time, such as at 4:30 pm on Tuesday, November 9; at 21:16 on Halloween 2018; or the like.  Further, as other examples, such information may include distances between persons or between person and objects as determined from captured\nimages.\nProfile 2400 may further include emotional states 2403 and/or facial expressions 2404 of the person.  For example, emotional states may include information and/or images reflected observed emotional states (e.g., happy, sad, etc.) determined\nfrom analysis of one or more images.  In some embodiments, such information may be correlated with dates and/or times at which the emotional state was observed.  Similarly, facial expressions may include information and/or images reflecting observed\nfacial expressions (e.g., smiling, frowning, etc.) determined from analysis of one or more images.  In some embodiments, such information may be correlated with dates and/or times at which the facial expression was observed.\nAs further illustrated in FIG. 24, profile 2400 may include audio topics and/or video topics 2405 associated with the person.  In some embodiments, audio topics may have been determined through analysis of audio data recorded by a microphone\nincluded a wearable apparatus.  Similarly, in some embodiments, video topics may have been determined through analysis of video data captured by an image sensor included a wearable apparatus.\nIn addition, profile 2400 may include information about nearby items 2406.  Such information may be determined through analysis of one or more images captured by a wearable apparatus.  As explained above in reference to auxiliary information\nmodule 2303, the nearby items may include products within a vicinity of a person included in an image and/or a user of a wearable apparatus.\nIn some embodiments, profile 2400 may include demographic information 2407.  For example, estimates of age, height, weight, gender, socioeconomic status, or the like may be included in profile 2400.\nAlthough profile 2400 include the foregoing exemplary information, profile 2400 may include any information collected related to a particular person, object, or location.  In some embodiments, at least some of the information stored in profile\n2400 includes and/or is based upon information collected by one or more wearable apparatus.  Such collected information may include any combination of images, video, and audio, and/or or any information derived from analysis of any combination of images,\nvideo, and audio.\nIn some embodiments, profile 2400 may be stored in a server, in a shared storage device, in a distributed database, in a blockchain, and so forth.\nFIG. 25 illustrates a flowchart of an example method 2500 for updating profile information based on data collected by a wearable apparatus.  Method 2500 may be implemented by at least one processing device (e.g., one or more processors of server\n250) receiving information from one or more wearable image sensors (e.g., image sensor 220 of wearable apparatus 110) configured to capture a plurality of images from the environment of the user(s) of the wearable apparatus(es).\nAt step 2510, the processing device may obtain identification information associated with a person detected in one or more images captured by a wearable image sensor included in the wearable apparatus.  For example, received image data may be\nprocessed by software steps executed by person detection module 2301 and/or identification information module 2302.  In some embodiments, the detection may include comparing one or more regions of the at least one image against a database of known\npatterns and/or images to determine whether a person is included in the one or more regions.  Alternatively or concurrently, the detection may include applying one or more classifiers to the one or more regions, and the one or more classifiers may output\nwhether a person is detected in the one or more regions.  Still further, the detection may include using one or more algorithms to detect facial features (e.g., mouth, eyes, etc.), facial contours, body shape, or any other suitable identifying feature of\na person.  In some embodiments, the detection may be performed by a separate processing device.  For example, the detection may be performed by processor 210 of wearable apparatus 110, and obtaining identification information associated with the detected\nperson may be performed by one or more processors of server 250.  In some embodiments, detecting and person and obtaining identification information associated with the detected person may be performed in a single device, for example by processor 210 of\nwearable apparatus 110, by server 250, and so forth.\nIn some embodiments, the identification information may include a name, birthday, or other real life indicator of the detected person and/or may include an identification number, a username, or other artificial (optionally anonymous) indicator\nof the detected person.  In some embodiments, the identification information may comprise a plurality of indicators.  In some embodiments, the identification information may comprise a hash value, such as a hash value calculated by identification\ninformation module 2302.\nIn some embodiments, identification information of a detected person may be obtained based on analysis of one or more images of the detected person.  Similar to the detection, the processing device may compare one or more regions of the at least\none image against a database of known patterns and/or images to obtain the identification information.  Alternatively or concurrently, the processing device may apply one or more classifiers to the one or more regions, and the one or more classifiers may\noutput the identification information.  For example, as discussed earlier, one or more algorithms may extract one or more of detected facial features (e.g., mouth, eyes, etc.), facial contours, body shape, or any other suitable identifying feature of a\nperson as the identification information.\nIn some embodiments, obtaining the identification information may include searching a lookup database, which may be separate from or form a part of a database of profiles, for identification information using one or more extracted features of\nthe one or more images, as described above.  In such embodiments, obtaining the identification information may include obtaining a hash value associated with the detected person and accessing a plurality of hash values to determine the existence of a\nhash value associated with the detected person based on the obtained hash value.  Accordingly, the lookup database may include and/or be indexed by a plurality of hash values.  In one example, the at least part of at least one image of the detected\nperson may comprise at least one image of a face of the detected person.\nAt step 2520, the processing device may obtain, for example from the wearable apparatus, auxiliary information associated with the detected person.  For example, received image data may be processed by software steps executed by auxiliary\ninformation module 2303 to produce the auxiliary information.  In another example, step 2520 may comprise receiving the auxiliary information produced by auxiliary information module 2303.\nIn some embodiments, the auxiliary information may include at least one of position information, a capture time of at least one of the one or more images, and information associated with a user of the wearable image sensor (e.g., identification\ninformation associated with the user, one or more images or the user, or the like).  Alternatively or concurrently, the auxiliary information may be obtained from analysis of the one or more images of the detected person.  For example, the auxiliary\ninformation may include at least one of an emotional state of the detected person and a facial expression of the detected person.  In another example, the auxiliary information may include information associated with a distance (e.g., a measurement of\nthe distance, a qualitative label of the distance such as \"far\" or \"near,\" or the like) of the wearable image sensor to the detected person.\nIn some embodiments, the auxiliary information may be obtained from analysis of one or more images of other detected persons.  For example, the auxiliary information may include information associated with at least one other person detected in\nthe one or more images and/or information related to a second person appearing in the one or more images with the detected person (e.g., identification information associated with the at least one other person and/or the second person, one or more images\nof the at least one other person and/or the second person, or the like).  In some examples, the detected person and the at least one other detected person may be detected in the same image.\nAlternatively or concurrently, the auxiliary information may be obtained from input to the wearable apparatus.  For example, the processing device may receive at least one of audio topics and video topics from the wearable apparatus (or from\nuser input into the wearable apparatus).  Similarly, an emotional state of the detected person, a facial expression of the detected person, information associated with at least one other person, and/or information related to the second person may be\nobtained from user input into the wearable apparatus.\nMoreover, the auxiliary information may directly include at least part of an image.  For example, at least part of an image of the one or more images of the detected person may be included in the auxiliary information.  Alternatively or\nconcurrently, at least part of an image of one or more images of the at least one other person and/or the second person may be included in the auxiliary information.\nIn some embodiments, obtaining auxiliary information may include receiving one or more images of a detected person and further analyzing the one or more images to obtain at least one property associated with the detected person (e.g., age,\ngender, weight, height, facial expression, emotional state, etc.).  In some embodiments, this analysis may be performed by a separate processing device.  For example, the analysis may be performed by processor 210 of wearable apparatus 110, and the\noutput may be received (and thus obtained as the auxiliary information) by one or more processors of server 250.\nIn some embodiments, the analysis may include applying one or more algorithms to analyze one or more of detected facial features (e.g., mouth, eyes, etc.), facial contours, body shape, or any other suitable identifying feature of a person to\nestimate or determine information such as a the detected person's age, gender, weight, height, facial expression, emotional state, etc. In some embodiments, the detected person be identified (e.g., through facial recognition), and the auxiliary\ninformation including at least one property may be obtained from one or more databases (stored, e.g., locally in a memory of wearable apparatus 110 and/or accessible remotely over a network, e.g., such as by accessing server 250).\nIn some embodiments, obtaining auxiliary information may include receiving one or more images of a detected person and further analyzing the one or more images to identify the at least one item associated with the detected person.  For example,\nthe analysis may include analysis of one or more regions of the one or more images to identify the at least one item.  The regions may, for example, include regions within a particular distance of the detected person.  In some embodiments, the analysis\nmay further include identifying the at least one item as a product.  For example, the analysis may include compare at least a portion of the one or more regions to known patterns and/or images to identify the product.  Alternatively or concurrently, one\nor more classifiers may be used on the one or more regions to identify the product.  In some embodiments, these analyses may be performed by a separate processing device.  For example, the analysis to identify the at least one item and/or the analysis to\nidentify the at least one item as a product may be performed by processor 210 of wearable apparatus 110, and the output may be received (and thus obtained as the auxiliary information) by one or more processors of server 250.\nIn some embodiments, the auxiliary information may include information related to an action performed by the detected person.  For example, obtaining the auxiliary informing may include using one or more comparisons to known patterns and/or\nimages and/or using one or more classifiers to identify an action being performed by the detected person.  In some embodiments, this analysis may be performed by a separate processing device.  For example, the analysis may be performed by processor 210\nof wearable apparatus 110, and the output may be received (and thus obtained as the auxiliary information) by one or more processors of server 250.\nIn some embodiments, the auxiliary information may include at least one property associated with the detected person.  Similar to step 2510, the processing device may compare one or more regions of the at least one image against a database of\nknown patterns and/or images to determine the at least one property.  Alternatively or concurrently, the processing device may apply one or more classifiers to the one or more regions, and the one or more classifiers may output the attribute.  For\nexample, as discussed earlier, one or more algorithms may analyze one or more of detected facial features (e.g., mouth, eyes, etc.), facial contours, body shape, or any other suitable identifying feature of a person to estimate or determine information\nsuch as a the detected person's age, gender, weight, height, facial expression, emotional state, etc. In some embodiments, this analysis may be performed by a separate processing device.  For example, the analysis may be performed by processor 210 of\nwearable apparatus 110, and the output may be received (and thus obtained as the auxiliary information) by one or more processors of server 250.\nAt step 2530, the processing device may identify, in the database, a profile associated with the detected person based on the identification information.  Identifying the profile may be facilitated by software steps executed by profile module\n2304.  For example, the processing device may identify the profile using an indicator included in the identification information and query an index of indicators to identify the profile associated with the detected person.  Alternatively or concurrently,\nthe processing device may query the identification information against the profiles directly (e.g., by performing a fuzzy search of names in the profiles against one or more names and/or partial names included in the obtained identification information)\nto identify the profile associated with the detected person.\nAt step 2540, the processing device may update the identified profile based on the auxiliary information.  For example, the processing device may update the profile by adding at least a portion of the auxiliary information to the profile. \nAdditionally or alternatively, the processing device may update the profile by adding at least a portion of the identification information to the profile.  For example, the processing device may store at least a portion of one or more names associated\nwith the detected person in the profile.  In another example, the processing device may store an emotional state of the detected person, a facial expression of the detected person, information associated with a second person, position information, a\ncapture time of at least one of the one or more images, information associated with a user of the wearable image sensor, or other portions of the auxiliary information (and/or identification information) in the profile.\nAlternatively or additionally, the processing device may update the profile based on the auxiliary information (and/or the identification information).  For example, information associated with at least one other detected person may be used to\nupdate a network of social connections of the detected person stored within the profile.  By way of further example, information associated with at least one other detected person may be used to update a calendar of social interactions, business\ninteractions, or the like stored within profile.\nIn some embodiments, the processing device may provide information based on the identified profile.  For example, the processing device may provide the information to a device (e.g., a server, a computer, a smartphone, a tablet, etc.) or may\ntransmit the information to an address (e.g., an email address) or a network address.  In some embodiments, the device to which the information is provided may be paired with a wearable apparatus.  A device paired with the wearable apparatus may include\na smartphone, a tablet, a laptop, or the like.  Accordingly, the processing device may send information to a device using a wired connection (such as a universal serial bus (USB), or the like), a direct wireless connection (such as Bluetooth.RTM., or the\nlike), and/or an indirect wireless connection (such as WiFi, 4G, or the like).\nIn some embodiments, the transmitted information may include at least a portion of the identification information, at least a portion of the auxiliary information, at least a portion of the profile, or the like.\nMethod 2500 may further include additional steps.  For example, the processing device may obtain, from a second wearable apparatus, additional auxiliary information associated with the detected person, the additional auxiliary information being\nbased on a second set of one or more images captured by the second wearable image sensor.  The obtaining of additional auxiliary information associated with the detected person may be performed similar to the obtaining of the auxiliary information in\nstep 2520.  Furthermore, method 2500 may further include updating the profile of the detected person based on the additional auxiliary information.  The updating of the profile based on the additional auxiliary information may be performed similar to the\nupdating of the profile in step 2540.\nDetermining a Level of Information Detail Provided to a User\nIn some embodiments, wearable apparatus 110 may collect information related to at least one person detected in an environment of the user of the wearable apparatus 110.  The wearable apparatus 101 may then determine what level of detail (e.g.,\nhow much) of information to provide to the user about the detected person.  Some existing wearable device systems may encounter the technical problem of how to process the information collected by the wearable device and use that information to provide\nuseful feedback to the user.  For example, certain existing systems may capture images that include people in the user's environment, but given the increasing quantity of publicly available data about individuals and the likelihood that the majority of\nthe available data is not of interest to the user, fail to provide information pertinent to the user (e.g., information that the user finds useful or of interest).  Some of the presently disclosed embodiments, on the other hand, may address this problem\nby assigning affinity measurements to a degree of a relationship between the identified person and the user to determine what information and/or level of information to provide to the user.\nFor example, in one embodiment, the affinity measurement may be based on a relationship in a social network between the user of the wearable apparatus 110 and the detected person in the captured image.  As a further example, in one embodiment,\nthe user may not receive any information related to a stranger with whom the user has no social relationship, but may receive a name and affiliation for a person in a common social group, and a name and common friends for second degree connections.  In\nthis way, presently disclosed embodiments may address the technical problems associated with parsing out information useful to a user in view of the large quantity of data acquired by the wearable device and publicly available.\nFIG. 26 is a diagram illustrating an example of memory 550 storing a plurality of modules, consistent with the disclosed embodiments.  The modules may be executable by at least one processing device to perform various methods and processes\ndisclosed herein.  Memory 550 may store more or fewer modules than those shown in FIG. 26.\nAs illustrated in FIG. 26, memory 550 may store software instructions to execute a data capture module 2601, a person identification module 2602, an action execution module 2603, a database access module 2604, and may also include database(s)\n2605.  Data capture module 2601 may include software instructions for receiving data from wearable apparatus 110.  Person identification module 2602 may include software instructions for analyzing data obtained by wearable apparatus 110 to identify\nsubsets of the captured data including at least one person and information associated with the at least one person.  Action execution module 2603 may include software instructions to cause the occurrence of an action based on the information associated\nwith the at least one person identified in the acquired data.  Database access module 2604 may include software instructions executable to interact with database(s) 2605, to store and/or retrieve information.\nData capture module 2601 may include software instructions for receiving data from a wearable apparatus, such as a wearable camera system.  Data received from a wearable camera system may include audio and image data, captured, by, for example,\nan image sensor or microphone associated with the wearable camera system.  Image data may include raw images and may include image data that has been processed.  Raw images may be provided, for example, in the form of still images and video data, either\nwith or without embedded metadata.  In some embodiments, image data and audio data may be preprocessed prior to capture by data capture module 2601.  Preprocessing may include, for example, noise reduction, artifact removal, compression, and other image\npre-processing techniques.\nPerson identification module 2602 may be configured to analyze data captured by data capture module 2601 to detect or identify a subset of the captured data that includes at least one person.  In some embodiments, module 2602 may be configured\nto receive a plurality of images and to identify one or more of the plurality of images that include at least one person.  For example, module 2602 may receive a plurality of images of an environment surrounding a user wearing the wearable device 110 and\nidentify which of the plurality of images include at least one person.\nIn some embodiments, such an analysis may be performed by employing a facial recognition algorithm designed to detect facial features (e.g., mouth, eyes, etc.), facial contours, body shape, or any other suitable identifying feature of a person. \nIn other embodiments, the at least one person may be identified using a thermal signature algorithm designed to detect the presence of at least one person based on the heat generated by the at least one person.  In such embodiments, the wearable device\n110 may capture thermal images, either alone or in combination with visual images, for processing by the thermal signature algorithm.  Thermal recognition of the at least one person may be desirable in implementations in which the wearable device 110 is\noperating in reduced lighting situations.  In some embodiments, the at least one person may be identified through the application of one or more image classification techniques.  For example, at least one image classification technique may be used to\nclassify at least one feature of an image.  In some embodiments, an image classification technique may include one or more of image enhancement, edge detection, image analysis, and data extraction.  Specific examples of the methods for identifying at\nleast one person are exemplary only, and a person of ordinary skill in the art will recognize other methods for identifying the at least one person that remain consistent with the present disclosure.  In some examples, the at least one person may be\ndetected using a face detection algorithm, using a neural network trained to detect faces and/or persons in images, and so forth.  In some examples, the at least one person may be identified using a face recognition algorithm, using a neural network\ntrained to identify people in images, and so forth.\nPerson identification module 2602 may further be configured to determine or obtain information associated with the at least one person identified in the image(s).  The information associated with the at least one person may include a name,\nnickname, social security number, account number, or any other identifier of the at least one person.  Once identifying information is obtained for the at least one person, the person identification module 2602 may obtain or determine at least one\naffinity measurement representing a degree of a relationship between the user of the wearable device 110 and the identified at least one person.\nThe affinity measurement may be represented in any suitable manner.  For example, the affinity measurement may be a numerical value assigned on a given scale, such as 0-1 or 1-100, with larger numbers representing a higher or closer degree of\nrelationship between the user and the at least one person and lower numbers representing a lower or more distant degree of relationship between the user and the at least one person (or vice versa, with lower numbers indicating a higher degree).  In other\nembodiments, the affinity measurement may be a level selected from a finite number of levels, for example with each level representing a range capturing the number of years the user has been associated with the at least one person.  For example, a\nchildhood friend whom the user has known for 15 years may be an affinity 1, which is assigned to people known by the user for 15 or more years.  In this example, a friend for two months may be assigned an affinity 3, which is assigned to people known by\nthe user for less than a year.\nIn some embodiments, the affinity measurement may be based on a category that indicates a relationship degree.  For example, some affinity categories may include family, friends, acquaintances, co-workers, strangers, etc. In other embodiments,\nthe affinity measurement may capture the emotional and/or social closeness of the user to the at least one person.  For example, close friends and family (e.g., childhood friends and siblings) may be assigned a similar affinity measurement, while more\ndistant friends and family (e.g., co-workers and second cousins) may be assigned a different affinity measurement.  Indeed, the affinity measurement may be assigned in any suitable manner, not limited to the examples herein, depending on\nimplementation-specific considerations.\nIn some embodiments, the affinity measurement may be based on a social network.  In some examples, the distance between two persons in social network may calculated as the minimal number of social network connections required to connect the two\npersons, and the affinity measurement may be based on the distance.  For example, let the number of social network connections required to connect the two persons be N, the affinity measure may be calculated as 1/N, as exp(-(N-A)/S) for some constants A\nand S, as f(N) for some monotonically decreasing function f, and so forth.  In some examples, the affinity measurement of two persons may be calculated based on the number of shared connections the two persons have in the social network.  For example,\nlet the number of shared connections the two persons have in the social network be M, the affinity measure may be calculated as M, log(A), as exp((M-A)/S) for some constants A and S, as f(M) for some monotonically increasing function f, and so forth. \nFor example, let the number of shared connections the two persons have in the social network be M, and let the number of connections the first person and second person have in the social network be M1 and M2 correspondingly, the affinity measure may be\ncalculated as M/sqrt(M1*M2), as f(M,M1,M2) for some function f that is monotonically increasing in M and monotonically decreasing in M1 and M2, and so forth.  In some examples, the affinity measurement of two persons may be calculated based on the number\nand/or length of interaction (e.g., conversations) the two persons conducted in the social network.  For example, let the overall length of interaction the two persons conducted in the social network be M, the affinity measure may be calculated as M,\nlog(M), exp((M-A)/S) for some constants A and S, f(M) for some monotonically increasing function f, and so forth.\nAction execution module 2603 may be configured to perform a specific action in response to the identification of one or more images including the at least one person.  For example, action execution module 2603 may determine, based on the at\nleast one affinity measurement, an information level to be disclosed to the user of the wearable apparatus.  In certain embodiments, the information level may be an amount, quantity, extent, type, and/or quality of information.  For example, a higher\ninformation level may provide the user a greater amount of information (e.g., name, athletic interests, and job title) than a lower information level (e.g., only name).  In other embodiments, the information level may provide matchmaking information if\nthe affinity measurement indicates a non-familial connection between the user and the at least one person.\nAction execution module 2603 may further be configured to provide information to the user of the wearable apparatus based on the information associated with the at least one person identified in the captured images and the determined information\nlevel.  The information provided to the user may include, for example, a person's name, job title, gender, interests, hobbies, political affiliation, work related information (e.g., whether the user and the at least one person have worked together in the\npast), leisure related information (e.g., whether the user and the at least one person have played sports together in the past, whether the user and the at least one person are predicted to be a successful match, whether the at least one person is\nsingle, etc.), matchmaking information (e.g., whether the user and the at least one person have dated in the past), or any other information about the at least one person that is available to the wearable device 110.\nDatabase 2605 may be configured to store any type of information of use to modules 2601-2604, depending on implementation-specific considerations.  For example, in embodiments in which action execution module 2603 is configured to provide the\ninformation about the identified at least one person to the user of the wearable apparatus 110, database 2605 may store prior-collected information about the user's social, familial, or other contacts.  Further, the database 2605 may store the metadata\nassociated with the captured images.  In some embodiments, database 2605 may store the one or more images of the plurality of captured images that include the at least one person.  Indeed, database 2605 may be configured to store any information\nassociated with the functions of modules 2601-2604.\nModules 2601-2604 may be implemented in software, hardware, firmware, a mix of any of those, or the like.  For example, if the modules are implemented in software, they may be stored in memory 550, as shown in FIG. 26.  However, in some\nembodiments, any one or more of modules 2601-2604 and data associated with database 2605, may, for example, be stored in processor 540 and/or located on server 250, which may include one or more processing devices.  Processing devices of server 250 may\nbe configured to execute the instructions of modules 2601-2604.  In some embodiments, aspects of modules 2601-2604 may include software, hardware, or firmware instructions (or a combination thereof) executable by one or more processors, alone or in\nvarious combinations with each other.  For example, modules 2601-2604 may be configured to interact with each other and/or other modules of server 250 and/or a wearable camera system to perform functions consistent with disclosed embodiments.  In some\nembodiments, any of the disclosed modules may each include dedicated sensors (e.g., IR, image sensors, etc.) and/or dedicated application processing devices to perform the functionality associated with each module.\nFIG. 27 shows an example environment including wearable apparatus 110 for capturing and processing images.  In the depicted embodiment, user 100 may wear wearable apparatus 110 on his or her neck.  However, in other embodiments, wearable\napparatus 110 may be differently positioned in any suitable location to enable capture of images of the user's environment, such as the locations explained in detail above.  User 100 may be in any location and engaging in any interaction encountered\nduring user's daily activities.  For example, user 100 may be at a convenience store, grocery store, sports event, social event, work-related event, movie theater, concert, etc. Wearable apparatus 110 may capture a plurality of images depicting the\nenvironment to which the user is exposed while user 100 is engaging in his/her chosen activity.  For example, in the illustrated embodiment, wearable apparatus 110 may capture images that include a first person 2702 and/or a second person 2704\ninteracting with user 100.  Further, wearable apparatus 110 may also capture, for example, when user 100 turns, one or more additional persons 2706 located at a distance from the area in which the conversation with persons 2702 and 2704 is occurring.  As\nsuch, the images may show that the user 100 is exposed to persons 2702, 2704, and 2706.  The images depicting the exposure of user 100 to particular persons 2702, 2704, and 2706 may be included in a log or otherwise saved in database 2605.\nFIG. 27 shows user 100 being exposed to persons 2702, 2704, and 2706 while standing.  However, as would be understood by one of ordinary skill in the art, wearable apparatus 110 may capture images throughout the user's day at a variety of\nlocations with the user in a variety of positions as the environment surrounding the user changes.  For example, images may be captured when the user visits a restaurant for dinner, commutes to and from work, attends social events, etc. In this way,\nwearable apparatus 110 may be configured to monitor the environment surrounding user 100 throughout the user's activities to identify exposure to one or more persons throughout the time user wears wearable apparatus 110.\nIn some embodiments, the one or more affinity measurements assigned to the identified persons 2702, 2704, and/or 2706 may depend on a type of interaction of the user 100 with the given identified person.  For example, in the illustrated\nembodiment, the user 100 is shaking the hand of person 2702 while standing in a conversational position with respect to persons 2702 and 2704.  As such, the user 100 and the person 2704 may be previously acquainted, while the user 100 and the person 2702\nmay have just met.  Based on this interaction, a higher affinity measurement may be assigned to person 2704 than person 2702.  For further example, while persons 2706 may be captured in images, user 100 is at a distance from persons 2706, thus indicating\nlack of a current interaction.  As such, a lower affinity measurement may be assigned to persons 2706 than persons 2702 and 2704.\nFurther, in some embodiments, the affinity measurement(s) assigned to the identified person(s) may be based in whole or in part on a physical distance between user 100 and the person(s) identified.  For example, in the illustrated embodiment,\nuser 100 may be at a distance 2708 from person 2702.  The distance 2708 may be determined, for example, by analyzing the size and/or location of the features of the captured person(s), using 3D and/or range imaging, and so forth.  For example, let the\nphysical distance between two persons be L, the affinity measure may be calculated as 1/L, as exp(-(L-A)/S) for some constants A and S, as f(L) for some monotonically decreasing function f, and so forth.  Still further, in some embodiments, wearable\ndevice 110 may be programmed to determine the distance 2708 or interaction between, for example, user 100 and person 2702, by analyzing the captured image.  For example, wearable device 110 may recognize that the chest of person 2702 is in the direct\nline of capture of the camera in wearable device 110, which may in turn indicate the relative position between user 100 and person 2702.\nFIG. 28A illustrates a flowchart of an exemplary method 2800 for identifying exposure to at least one person, consistent with embodiments of the present disclosure.  The method 2800 may be carried out, for example, by a processing device\nintegrated with and/or associated with wearable apparatus 110.  In such an embodiment, wearable apparatus 110 may include a wearable image sensor, e.g., image sensor 220, configured to capture a plurality of images from the environment of the user.  For\nexemplary purposes only, method 2800 for identifying exposure to at least one person is described herein with respect to processing device 210 cooperating with memory 550 to execute modules 2601-2604.\nIn accordance with method 2800, processor 210 may receive image data captured by a wearable image sensor at block 2802.  Block 2802 may be facilitated by software instructions of data capture module 2601.  Data capture module 2601 may be\nconfigured to execute instructions to receive image data from a wearable image sensor, and may also be configured to execute instructions to control the wearable image sensor.  Controlling the wearable image sensor may include issuing commands to record\nimages and/or videos, and may also include issuing commands to control an orientation or direction of viewing of the image sensor.\nReceived image data may be processed via software steps executed by person identification module 2602.  For example, at block 2804, person identification module 2602 may identify one or more images including the at least one person from a\nplurality of captured images.  For example, the at least one person may be person 2702, and the module 2602 may analyze the plurality of images to identify a subset of the captured images that include features sized, shaped, or otherwise resembling a\nperson.  Further, at block 2806, the one or more identified images may be processed to identify the particular person(s) depicted in the images flagged as including person(s).  In another example, at block 2804 and/or 2806, person identification module\n2602 may identify unique instances of the at least one person appearing in the plurality of captured images.\nAt block 2808, person identification module 2602 may further analyze the subset of the captured images including the at least one person to determine information associated with the at least one person.  The information associated with the at\nleast one person may include a name, nickname, hobby, interest, like, dislike, places frequently visited, sexual orientation, gender, political affiliation, nationality, sporting events attended, or any other identifier or information available about the\ngiven person.  Further, the information about the at least one person may be sourced from any desired location.  For example, the information may be sourced from prior-sourced information stored in database(s) 2605 via database access module 2604.  For\nfurther example, the information may be sourced from one or more social media accounts.  In such embodiments, wearable device 110 may source the information publicly available via a web browser and/or through a private account of user 100.  In other\nembodiments, the information may be sourced from records of email, text, voicemail, or telephone communications between user 100 and the at least one identified person.  Indeed, the information may be sourced from any suitable location, not limited to\nthose described herein, depending on implementation-specific considerations.\nAt block 2810, person identification module 2602 may determine or obtain at least one affinity measurement representing a degree of a relationship between the user 100 and the at least one identified person.  The at least one affinity\nmeasurement may take any of the forms described above.  Further, the at least one affinity measurement may be based on one or more factors indicative of the degree of the relationship between the user 100 and the identified at least one person.  For\nexample, in some embodiments, the affinity measurement may be based on a social graph, a social network, a type of interaction between the user 100 and the at least one person, a physical distance between the user 100 and the at least one person, or any\nother suitable factor.  In one embodiment, the affinity measurement may capture the relationship between the user 100 and the at least one person person in a social network (e.g., a list of contacts on a social media site) to ensure that an appropriate\nlevel of information is provided to the user 100.  For example, in one embodiment, a low affinity measurement may ensure that the user 100 would not receive any information related to a stranger, but a higher affinity measurement would ensure that the\nuser 100 would receive a name and affiliation for person in a common group, or a name and common friends for second degree connections.  In some embodiments, an appropriate affinity measurement could ensure that for a detected person that is a first\ndegree connection, even more information could be provided, such as the time of a last meeting, a last email, etc.\nIn the embodiment illustrated in FIG. 28A, at block 2812, the data including the information associated with the at least one person and/or the affinity measurement(s) is transmitted to an external device.  The external device may be a\nsmartphone, tablet, smartwatch, laptop, server, or any other suitable device configured to process the transmitted data.  To that end, the external device and wearable apparatus 110 may include suitable components to enable data transfer between wearable\napparatus 110 and the external device.  For example, in one embodiment, the wearable apparatus may include a transmitter configured to enable wireless pairing with a receiver located in the external device.  In such embodiments, the wearable apparatus\nand the external device may be reversibly or irreversibly paired to enable exclusive data transfer between the two devices.  The pairing may be established by the user of the wearable apparatus and external device, or may be automatically performed when\nthe wearable apparatus and the external device are within a given distance from one another (e.g., within a range such that the transmitter and receiver are capable of exchanging data).\nHowever, in other embodiments, the transfer of data to the external device at block 2812 may be omitted from method 2800, and further processing of the data may be performed by the wearable apparatus 110, for example, in action execution module\n2603 or using blocks 2824, 2826 and 2828.  In the exemplary illustrated embodiment, however, the information associated with the at least one person and the affinity measurement are received by an external device at block 2822 in method 2820.  At block\n2824, method 2820 includes determining an information level to be disclosed to the user 100 of the wearable apparatus 110.\nIn some embodiments, the information level may be selected from a plurality of alternate information levels, as discussed above.  For example, a first information level may be for a close social connection, and a second information level may be\nfor a more distant connection.  More specifically, with respect to FIG. 27, the first information level may be assigned to person 2704, with whom user 100 is already acquainted, and the second information level may be assigned to person 2702, whom the\nuser 100 just met.  In one embodiment, selection of the first information level may result in the shared information including the name of the person 2704, and selection of the second information level may result in the provided information not including\nthe name of the person 2702.\nThe method 2820 further includes accessing stored data reflecting the determined information level for the at least one person at block 2826 and providing the information to the user 100 at block 2828 in accordance with the assigned information\nlevel(s).  For example, in one embodiment, a given information level may correspond to a potential romantic match, and the information provided to the user 100 may be information relevant to whether or not the at least one person is a romantic match.  In\nsuch an embodiment, the information provided to the user about the at least one person may include, for example, a name, age, gender, school(s) attended, mutual friends, common interests, dietary preferences, hair color, eye color, weight, height, etc.\nIn such an embodiment, another information level may correspond to someone who is not a potential romantic match, for example, because the gender of the at least one person does not match the gender preference of the user 100, the age gap between the\nuser 100 and the at least one person is too large, the at least one person is a family member, etc.\nIn some embodiments, the at least one person may include one or more persons.  For example, in some embodiments, separate affinity measurement(s) and/or information level(s) may be assigned to different persons identified in the captured images. More particularly, in one embodiment, a first person identified in the captured images may be assigned a first affinity measurement representing a degree of relationship between the user 100 and the first person.  A second person may also be identified\nin the captured images, and a second affinity measurement may be assigned to the second person.  The determination of the information level provided to the user may be based on both the first affinity measurement and the second affinity measurement.\nIdentifying a Verbal Contract\nIn some embodiments, wearable apparatus 110 may collect information related to an interaction between the user of the wearable apparatus 110 and at least one person detected in an environment of the user of the wearable apparatus 110.  For\nexample, in some embodiments, the wearable apparatus 110 may identify when the user and another person detected in the user's environment enter into a verbal contract.\nSome existing wearable device systems may encounter the technical problem of how to process the large amount of information collected by the wearable apparatus 110 and to use that information to provide useful feedback and/or services to the\nuser.  Some of the presently disclosed embodiments may address this problem by collecting visual and audio information, and using the collected information to determine which frames of the collected information to store for the user.  For example, in\nsome embodiments, the wearable apparatus 110 may store the collected information when the video and/or the audio information include features associated with a verbal contract.\nStill further, when a verbal contract has been detected, presently disclosed embodiments may address the problem of authenticating the identity of the user of the wearable apparatus 110 and/or the other party to the verbal contract.  For\nexample, when using automatically collected information regarding a verbal contract, it may be desirable to the user of the wearable apparatus 110 and/or the other party to the contract to register or log the existence of the contract.  However, given\nthe possibility of identity fraud, it may be desirable to log only contracts for which one or both of the parties are authenticated.  Presently disclosed embodiments may address this problem by using digital signatures or other electronic verifications\nof the identity of one or more of the parties to the contract.\nFIG. 29 is a diagram illustrating an example of memory 550 storing a plurality of modules, consistent with the disclosed embodiments.  The modules may be executable by at least one processing device to perform various methods and processes\ndisclosed herein.  Memory 550 may store more or fewer modules than those shown in FIG. 29.\nAs illustrated in FIG. 29, memory 550 may store software instructions to execute a data capture module 2901, a person identification module 2902, a contract identification module 2903, an action execution module 2904, a database access module\n2905, and may also include database(s) 2906.  Data capture module 2901 may include software instructions for receiving data from wearable apparatus 110.  Person identification module 2902 may include software instructions for analyzing data obtained by\nwearable apparatus 110 to identify subsets of the captured data including at least one person and information associated with the at least one person.  Contract identification module 2903 may include software instructions for analyzing images(s) and/or\naudio data capture by the wearable apparatus 110 to identify the presence of a verbal contract.  Action execution module 2904 may include software instructions to cause the occurrence of an action based on the information that a verbal contract has been\nidentified.  Database access module 2905 may include software instructions executable to interact with database(s) 2906, to store and/or retrieve information.\nData capture module 2901 may include software instructions for receiving data from a wearable apparatus, such as a wearable camera system and/or a wearable audio system.  Data received from a wearable camera system may include audio and image\ndata, captured, by, for example, an image sensor or microphone associated with the wearable camera system.  Image data may include raw images and may include image data that has been processed.  Raw images may be provided, for example, in the form of\nstill images and video data, either with or without embedded metadata.  In some embodiments, image data and audio data may be preprocessed prior to capture by data capture module 2901.  Preprocessing may include, for example, noise reduction, artifact\nremoval, compression, and other image pre-processing techniques.\nPerson identification module 2902 may be configured to analyze data captured by data capture module 2901 to detect or identify a subset of the captured data that includes at least one person.  In some embodiments, module 2902 may be configured\nto receive a plurality of images and to identify one or more of the plurality of images that include at least one person.  For example, module 2902 may receive a plurality of images of an environment surrounding a user wearing the wearable device 110 and\nidentify which of the plurality of images include at least one person, for example using person detection module 1701 described above, using person detection module 2001, using person detection module 2301, and so forth.\nIn some embodiments, such an analysis may be performed by employing a facial recognition algorithm designed to detect facial features (e.g., mouth, eyes, etc.), facial contours, body shape, or any other suitable identifying feature of a person. \nIn other embodiments, the at least one person may be identified using a thermal signature algorithm designed to detect the presence of at least one person based on the heat generated by the at least one person.  In such embodiments, the wearable device\n110 may capture thermal images, either alone or in combination with visual images, for processing by the thermal signature algorithm.  Thermal recognition of the at least one person may be desirable in implementations in which the wearable device 110 is\noperating in reduced lighting situations.  In some embodiments, the at least one person may be identified through the application of one or more image classification techniques.  For example, at least one image classification technique may be used to\nclassify at least one feature of an image.  In some embodiments, an image classification technique may include one or more of image enhancement, edge detection, image analysis, and data extraction.  Specific examples of the methods for identifying at\nleast one person are exemplary only, and a person of ordinary skill in the art will recognize other methods for identifying the at least one person that remain consistent with the present disclosure.\nPerson identification module 2902 may further be configured to determine or obtain information associated with the at least one person identified in the image(s).  The information associated with the at least one person may include a name,\nnickname, social security number, account number, or any other identifier of the at least one person.  The identification information of the at least one detected person may be sourced from any suitable location, such as prior-stored information in\ndatabase 2906.  In some embodiments, the identification information may be obtained by matching the captured image(s) of the person with one or more images accessible via the Internet associated with a given identity, and then assigning that identity to\nthe at least one person detected in the image(s).  In some embodiments, the identification information may be obtained using identification information module 2302, using person identification module 2602, and so forth.\nIn some embodiments, person identification module 2902 may further obtain one or more profiles of the at least one person in the image(s).  Based on the one or more profiles of the at least one person, the person identification module 2902 may\nauthenticate the identity of the at least one person.  For example, in one embodiment, person identification module 2902 may access the digital signature of the at least one person to authenticate the at least one person.  In other embodiments, person\nidentification module 2902 may authenticate the identity of the at least one person based on one or more motions of a body part (e.g., a hand) of the at least one person, which may be a signature movement of the at least one person.  In another example,\nperson identification module 2902 may authenticate the identity of the at least one person based on a face image of the person and using a face recognition algorithm.  In yet another example, person identification module 2902 may authenticate the\nidentity of the at least one person using biometric readings captured by wearable apparatus 110 and by comparing the biometric readings with a biometric signature associated with the person, for example as retrieved from database 2906 using database\naccess module 2905.\nContract identification module 2903 may be configured to analyze the plurality of images identified by the person identification module 2902 to determine if the images reflect an action associated with the formation of a verbal contract. \nFurther, contract identification module 2903 may be configured to analyze at least a portion of the collected audio data to identify one or more sounds (e.g., words) associated with the formation of a verbal contract.  In some embodiments, the identified\nsounds may be linked to the user of the wearable apparatus 110 and/or the detected person in the collected image(s).  Still further, in some embodiments, contract identification module 2903 may utilize a combination of actions identified in the analyzed\nimages and sounds identified in the audio data to determine that a verbal contract has occurred between the user of the wearable apparatus 110 and the detected person.\nFor example, in one embodiment, the collected images may show that a handshake occurred between the user of the wearable apparatus 110 and another detected person.  The analysis of the audio data may show that one or more words associated with a\nverbal contract were spoken.  For example, the audio data may detect words such as \"I agree,\" \"yes,\" \"you have a deal,\" \"it's a deal,\" \"we're all set,\" \"I agree to the contract,\" \"those terms are acceptable to me,\" \"we agree,\" or any other words\nindicative of the formation of a verbal contract.  The one or more words indicative of a verbal contract may be used alone or in combination with one or more identified actions associated with a verbal contract in the captured images by the contract\nidentification module 2903 to determine that a verbal contract was agreed upon.\nSimilarly, in some embodiments, the collected images and/or audio data may be analyzed to determine that a verbal contract did not occur.  In such embodiments, the analysis of the collected images may show that no actions indicative of a verbal\ncontract were taken (e.g., no handshake occurred).  Further, the analysis of the audio data may include one or more words indicating the lack of a verbal contract, such as \"I don't agree,\" \"no,\" \"you don't have a deal,\" \"it's not a deal,\" \"there's no\ncontract,\" \"we'll have to keep working on this,\" etc. The one or more words indicating the lack of a verbal contract may be used alone or in combination with the collected images to determine that a contract is likely not to have occurred.\nAction execution module 2904 may be configured to perform a specific action in response to the identification of one or more images including the at least one person and/or the identification of one or more sounds indicative of a verbal\ncontract.  For example, action execution module 2904 may be configured to authenticate the identity of the user of the wearable apparatus 110, the at least one person identified in the collected images, and/or a witness to a verbal contract between the\nuser and the at least one person when a verbal contract has been identified by contract identification module 2903.  For example, in one embodiment, action execution module 2904 may obtain at least one profile of the user of the wearable apparatus 110\nand authenticate the user's identity based on the at least one profile.  The at least one profile may be any representation of identity that is suitable for linking the user and/or detected person to a given identity.  For example, in one embodiment,\nauthenticating the identity of the user may include analyzing one or more of the captured images, for example, to identify features in the image of the user's face that are unique to the user.  In another embodiment, authentication of the user may be\nperformed by analyzing the captured images to identify a motion of a body part (e.g., a hand) of the user and/or ego motion associated with the user.  In some embodiments, authentication of the user may use biometric readings captured by wearable\napparatus 110 and by comparing the biometric readings with a biometric signature associated with the user.\nIn some examples, authenticating the identity of the user may comprise identifying motion of a body part of the user (e.g., a hand, a head, etc.) from images captured by data capture module 2901, analyze the motion of the body part in the images\nto determine a distribution of motions (e.g., based on positions, relative positions, directions, relative directions, velocity, directional velocity, acceleration and/or deceleration, direction acceleration and/or deceleration, etc.), and comparing the\ndistribution to a known past distributions of the user.  For example, the distribution of motions may include a histogram of positions, relative positions, directions, relative directions, velocity, directional velocity, acceleration and/or deceleration,\ndirection acceleration and/or deceleration, and so forth.  In another example, the distribution of motions may include a statistical characteristic (such as mean, variance, entropy, etc.) of positions, relative positions, directions, relative directions,\nvelocity, directional velocity, acceleration and/or deceleration, direction acceleration and/or deceleration, and so forth.  Similarly, authenticating the identity of the at least one person may comprise identifying motion of a body part of the at least\none person (e.g., a hand, a head, etc.) from images captured by data capture module 2901, analyze the motion of the body part in the images to determine a distribution of motions (e.g., based on positions, relative positions, directions, relative\ndirections, velocity, directional velocity, acceleration and/or deceleration, direction acceleration and/or deceleration, etc., as described in details above), and comparing the distribution to a known past distributions of the at least one person.\nIn some embodiments, action module 2904 may authenticate the identity of at least one person identified in the collected images (which may be a side to the verbal contract, a witness, etc.) may include communicating with a device associated to\nthe at least one person (for example, with a wearable device worn by the at least one person), and obtaining identifying information and/or identity proof of the at least one person from the device associated with the at least one person.\nAction execution module 2904 may further be configured to register the verbal contract and the identification information associated with the detected person based on the authentication of the user, the detected person, and/or a witness to the\nverbal contract.  For example, in one embodiment, the verbal contract may be registered upon authentication of the user.  However, in other embodiments, the verbal contract may be registered when the identity of both the user and the detected person are\nidentified.  Still further, in some embodiments, the verbal contract may be registered when the identity of the user, the detected person, and the witness are authenticated.\nRegistration of the verbal contract may take on any of a variety of suitable forms, depending on implementation-specific considerations.  For example, in one embodiment, registering the verbal contract and the identification information of the\ndetected person may include storing at least a portion of the audio data associated with the verbal contract and at least part of the identification information associated with the detected person, for example, in database(s) 2906.  The portion of the\naudio data that is stored may include the audio data reflecting the terms of the contract, the offer made by one of the parties, the acceptance made by another of the parties, and any consideration given for the contract.  The portion of the\nidentification information that is stored may be any portion suitable to identify the person, such as a social security number, full legal name, nickname, etc.\nIn some embodiments, registration of the verbal contract and the identification information of the detected person may include transmitting at least a portion of the audio data associated with the verbal contract and at least part of the\nidentification information associated with the detected person using at least one communication device.  For example, in some embodiments, wearable apparatus 110 may include a communication device, such as one or more wireless transceivers, as discussed\nabove in connection with FIGS. 5A-5C, which may transmit information across network 240 to, for example, computing device 120 and/or server 250 In some embodiments, the registered verbal contract and identification information may be transmitted from the\ncommunication device to a longer term storage location, such as a cloud-based storage facility (e.g., server 250) via network 240.  In some embodiments, wearable apparatus 110 may transmit the information to a paired device (e.g., computing device 120),\nwhich may then in turn transmit the information to another destination (e.g., server 250) via network 240.\nIn some embodiments, registration of the verbal contract and the identification information of the detected person may include posting on a public database and/or a blockchain information based on at least a portion of the audio data associated\nwith the verbal contract and/or at least part of the identification information associated with the detected person.  The posted information may include at least a portion of the audio data associated with the verbal contract, at least part of the\nidentification information associated with the detected person, any other information related to the verbal contract (such as time, place, witnesses, context of the agreement, financial transfer and/or commitment, etc.), an encrypted version of any of\nthe above, a digitally signed version of any of the above, a digital signature of any of the above with or without the signed data, and so forth.  The posted information may also include any other information related to the contract.\nIn some embodiments, registration of the verbal contract and/or identification information may include digitally signing at least a portion of the audio data associated with the verbal contract and/or at least part of the identification\ninformation associated with the detected person.  Digitally signing may refer to any technique used to validate the authenticity and/or integrity of the data being signed.  For example, digitally signing may include applying a verified or previously\nauthenticated digital signature of the user, detected person, or witness to the audio data or identification information.  The digital signature may include a mark assigned to the user, detected person, or witness, for example, by a company that verifies\nthe identity of its customers.  For example, the company may have previously verified its customers' identities based on a review of government-issued documents, such as drivers' licenses, passports, etc.\nIn addition to the verbal contract and identification information, other data may also be registered by action execution module 2904.  For example, at least one clock may be configured to provide time information associated with the audio data,\nfor example, by timestamping the audio data as it is generated and/or collected with a date and/or time.  The time information may be registered with the audio data when the verbal contract is registered.  In other embodiments, additional details\nregarding the time, location, conditions, etc. surround the formation of the verbal contract may be registered.  For example, at least one positioning device may be configured to generate position information associated with the verbal contract.  The\npositioning device may be, for example, an accelerometer in a device paired with the wearable apparatus (e.g., the user's smartphone) configured to track the position of the user relative to the user's environment (e.g., that the user was sitting or\nstanding when the contract was established), a global positioning device configured to obtain the position of the wearable apparatus and/or a device paired with the wearable apparatus, and so forth.\nIn some embodiments, action execution module 2904 may recognize that the verbal contract comprise an obligation and/or a desire of a first entity (such as the user, the at least one person, a third party, etc.) to transfer funds to a second\nentity (such as the user, the at least one person, a third party, etc.), and cause the funds to be transferred from an account of the first entity to an account of the second entity.  In some examples, action execution module 2904 may also inform the\nfirst entity and/or the second entity about the transfer of funds, for example in an audible output, in an email, in a visual manner, and so forth.  In some examples, before transferring the funds, action execution module 2904 may ask the first entity\nexplicit permission, for example using a user interface, an audible user interaction bot, a graphical user interface, an email, etc.\nIn some embodiments, action execution module 2904 may provide information related to the verbal contract to parties involved in the verbal contract (such as the user, the at least one person, a witness, a third party, etc.).  For example, the\nprovided information may include a request to acknowledge and/or ratify the verbal contract, a summary of the verbal contract details, identifying information of parties of the verbal contract and/or witnesses to the verbal contract, a time and/or place\nassociated with the verbal contract, a reminder (for example in the form of a calendar event, a pop-up message, an email, etc.) to perform an action related to the verbal contract (such as an action the party obligated to take in the verbal contract).\nDatabase 2906 may be configured to store any type of information of use to modules 2901-2905, depending on implementation-specific considerations.  For example, in embodiments in which action execution module 2904 is configured to obtain the\nidentification information about the identified at least one person to the user of the wearable apparatus 110, database 2906 may store prior-collected information about the user's identity.  Further, the database 2906 may store the metadata associated\nwith the captured images and/or audio data.  In some embodiments, database 2906 may store the one or more images of the plurality of captured images that include the at least one person.  Database 2906 may further store some or all of the captured audio\ndata indicating formation of the verbal contract.  In other embodiments, database 2906 may be configured to store the profile of the user, detected person, and/or witness to the contract for reference when authenticating the identities of the parties to\nthe verbal contract and/or witnesses to the verbal contract.  For example, database 2906 may store one or more digital signatures.  Indeed, database 2906 may be configured to store any information associated with the functions of modules 2901-2905.\nModules 2901-2905 may be implemented in software, hardware, firmware, a mix of any of those, or the like.  For example, if the modules are implemented in software, they may be stored in memory 550.  However, in some embodiments, any one or more\nof modules 2901-2905 and data associated with database 2906, may, for example, be stored in processor 540 and/or located on server 250, which may include one or more processing devices.  Processing devices of server 250 may be configured to execute the\ninstructions of modules 2901-2905.  In some embodiments, aspects of modules 2901-2905 may include software, hardware, or firmware instructions (or a combination thereof) executable by one or more processors, alone or in various combinations with each\nother.  For example, modules 2901-2905 may be configured to interact with each other and/or other modules of server 250 and/or a wearable camera system to perform functions consistent with disclosed embodiments.  In some embodiments, any of the disclosed\nmodules may each include dedicated sensors (e.g., IR, image sensors, audio sensors, etc.) and/or dedicated application processing devices to perform the functionality associated with each module.\nFIG. 30 shows an example environment including wearable apparatus 110 for capturing and processing images and audio data.  In the depicted embodiment, user 100 may wear wearable apparatus 110 on his or her neck.  However, in other embodiments,\nwearable apparatus 110 may be differently positioned in any suitable location to enable capture of images of the user's environment, such as the locations explained in detail above.  User 100 may be in any location and engaging in any interaction\nencountered during user's daily activities.  For example, user 100 may be at a convenience store, grocery store, sports event, social event, work-related event, office, move theater, concert, etc. Wearable apparatus 110 may capture a plurality of images\ndepicting the environment to which the user is exposed while user 100 is engaging in his/her chosen activity.  Wearable apparatus 110 may also capture audio data via at least one audio sensor (e.g., a microphone) reflecting the sounds occurring in the\nenvironment surrounding the user.  For example, in the illustrated embodiment, wearable apparatus 110 may capture images that include a first person 3002 and/or a second person 3004 interacting with user 100.  As such, the images may show that the user\n100 is exposed to persons 3002 and 3004.  The images depicting the exposure of user 100 to particular persons 3002 and 3004 may be included in a log or otherwise saved in database 2906.  Wearable apparatus 110 may further capture audio sounds 3006 spoken\nby user 100 and/or audio sounds 3008 spoken by first person 3002.\nWearable apparatus 110 may also capture audio data reflecting one or more words spoken by user 100, first person 3002, and/or second person 3004.  As such, at least a portion of the audio data may reflect that a verbal contract has occurred\nbetween user 100 and first person 3002 by detecting one or more words indicative of contract formation.  For example, at least a portion of the audio data at a first timestamp may reflect that first person 3002 made an offer.  Another portion of the\naudio data at a second, later timestamp may indicate that user 100 accepted the offer.  Finally, another portion of the audio data may indicate that consideration was exchanged to support the offer and acceptance.\nStill further, in some embodiments, wearable apparatus 110 may capture images showing that a handshake occurred between user 100 and first person 3002.  This may be used either alone or in combination with the audio data indicating that a verbal\ncontract occurred to determine whether to register the verbal contract.  Additionally, wearable apparatus 110 may capture one or more words spoken by second person 3004, a witness to the verbal contract between user 100 and first person 3002.  For\nexample, witness 3004 may say one or more words indicating that witness 3004 believes a contract was formed, such as \"congratulations,\" \"I'm glad you were able to come to an agreement,\" \"it's wonderful that you agree,\" \"I'm excited that you decided to\nwork together,\" or any other words that indicate the belief on the part of witness 3004 that a contract has occurred.\nFIG. 30 shows user 100 being exposed to persons 3002 and 3004 while standing.  However, as would be understood by one of ordinary skill in the art, wearable apparatus 110 may capture images throughout the user's day at a variety of locations\nwith the user in a variety of positions as the environment surrounding the user changes.  For example, images may be captured when the user visits a restaurant for dinner, commutes to and from work, attends social events, attends work meetings, etc. In\nthis way, wearable apparatus 110 may be configured to monitor the environment surrounding user 100 throughout the user's activities to identify exposure to one or more persons throughout the time user wears wearable apparatus 110 and to capture audio\ndata associated with the user's daily activities.\nIn some embodiments, wearable apparatus 110 may be programmed to selectively collect data from at least one image sensor and at least one audio sensor to reduce the quantity of data collected that is irrelevant to the monitoring of verbal\ncontract formation.  For example, in one embodiment, the at least one image sensor may be activated to collect data, and the collected images may be processed to determine if the images include at least one person.  When at least one person is identified\nin the images, the at least one audio sensor may be triggered to collect data to capture any conversations that occur between user 100 and the at least one person.  In this way, the image and audio sensor(s) may be selectively controlled to address the\ntechnical problems associated with logging and storing large quantities of data that may be acquired by wearable apparatus 110 throughout use by user 100.\nFIG. 31A illustrates a flowchart of an exemplary method 3100 for receiving and analyzing image and/or audio data, consistent with embodiments of the present disclosure.  The method 3100 may be carried out, for example, by a processing device\nintegrated with and/or associated with wearable apparatus 110.  In such an embodiment, wearable apparatus 110 may include a wearable image sensor, e.g., image sensor 220, configured to capture a plurality of images from the environment of the user. \nWearable apparatus 110 may also include a wearable audio sensor configured to capture a plurality of sounds (e.g., one or more words) from the environment of the user.  In some other examples, the entire method 3100 or parts of method 3100 may be\nperformed by a device external to wearable apparatus 110, such as a device paired with wearable apparatus 110 (such as a smartphone, a tablet, etc.), a server communicating with wearable apparatus 110 (such as server 250), and so forth.  For exemplary\npurposes only, method 3100 for image and/or audio data is described herein with respect to processing device 210 cooperating with memory 550 to execute modules 2901-2905.\nIn accordance with method 3100, processor 210 may receive image data captured by a wearable image sensor at block 3102.  Block 3102 may be facilitated by software instructions of data capture module 2901.  Data capture module 2901 may be\nconfigured to execute instructions to receive image data from a wearable image sensor, and may also be configured to execute instructions to control the wearable image sensor.  Controlling the wearable image sensor may include issuing commands to record\nimages and/or videos, and may also include issuing commands to control an orientation or direction of viewing of the image sensor.\nReceived image data may be processed via software steps executed by person identification module 2902.  For example, at block 3104, person identification module 2902 may identify one or more images including the at least one person from a\nplurality of captured images.  For example, the at least one person may be person 3002, and the module 2902 may analyze the plurality of images to identify a subset of the captured images that include features sized, shaped, or otherwise resembling a\nperson.  Further, at block 3106, the one or more identified images may be processed to identify the particular person(s) depicted in the images flagged as including person(s).  In another example, at block 3104 and/or 3106, person identification module\n2902 may identify unique instances of the at least one person appearing in the plurality of captured images.\nAt block 3108, contract identification module 2903 may further analyze the subset of the captured images including the at least one person to identify one or more actions associated with a verbal contract.  For example, contract identification\nmodule 2903 may identify actions such as a handshake, head nod upward and downward, smiling, or any other physical movement cue associated with an offer or acceptance of a contract term.\nAt block 3110, processor 210 may receive audio data captured by a wearable image sensor.  Block 3110 may be facilitated by software instructions of data capture module 2901.  Data capture module 2901 may be configured to execute instructions to\nreceive audio data from a wearable image sensor, and may also be configured to execute instructions to control the wearable image sensor.  Controlling the wearable image sensor may include issuing commands to record audio data, and may also include\nissuing commands to control a collection volume or frequency of the audio sensor.  Received audio data may be processed via software steps executed by contract identification module 2903.  For example, at block 3112, contract identification module 2903\nmay identify at least a portion of the audio data including the at least one sound associated with the formation of a verbal contract.\nFor example, contract identification module 2903 may analyze the captured audio data to identify one or more portions that include one or more words associated with formation of a verbal contract, as discussed in detail above.  Further, in other\nembodiments, the one or more sounds identified in the audio data may be sounds associated with certain actions that support the conclusion that a verbal contract was formed.  For example, certain sound profiles or frequency components may be associated a\nhandshake of user 100 and person 3002 and/or the upward and downward movement of the hand of user 100 during the handshake with person 3002.  To that end, in some embodiments, wearable apparatus 110 may be located in other non-illustrated locations, such\nas proximate the user's hand, to better capture such audio data.\nFIG. 31B illustrates a flowchart of an exemplary method 3120 for authenticating the identity of one or more of the parties to the identified verbal contract, consistent with embodiments of the present disclosure.  The method 3120 may be carried\nout, for example, by a processing device integrated with and/or associated with wearable apparatus 110.  In some other examples, the entire method 3120 or parts of method 3120 may be performed by a device external to wearable apparatus 110, such as a\ndevice paired with wearable apparatus 110 (such as a smartphone, a tablet, etc.), a server communicating with wearable apparatus 110 (such as server 250), and so forth.  At block 3122, action execution module 2904 may obtain at least one profile of the\nuser 100 of wearable apparatus 110.  For example, action execution module 2904 may access a digital signature of user 100 stored in database 2906, as described above.  At block 3124, action execution module 2904 may authenticate an identity of the user\nbased on the accessed at least one profile.  For example, in one embodiment, the identity of user 100 may be authenticated by analyzing the captured images to identify a motion of at least one hand of the user.  In other embodiments, user 100 may be\nauthenticated by identifying ego motion associated with user 100.\nIn the illustrated embodiment, method 3120 further includes obtaining at least one profile of the detected person at block 3126.  For example, a digital signature of the detected person may be located in database 2906.  Based on the at least one\nprofile of the detected person, action execution module 2904 may authenticate the identity of the detected person at block 3128.  The identity of the detected person may be authenticated in a similar way to the authentication of user 100 described above. However, although the embodiment of method 3120 is illustrated with authentication of both user 100 and the detected person, in other embodiments, only one of the parties to the contract may be authenticated prior to registering the verbal contract.  For\nexample, the identity of user 100 may be known such that only the detected person is authenticated prior to contract registration.  In other embodiments, only the identity of user 100 may be authenticated prior to registration of the contract, for\nexample, to reduce or eliminate the likelihood that the individual wearing wearable apparatus 110 is not user 100.\nAt block 3130, the verbal contract and/or identification information of the detected person are registered if one or more registration conditions are met, for example using action execution module 2904 as described in details above.  The\nregistration conditions may be any suitable prerequisite to registration.  For example, the registration conditions may include authentication of user 100, authentication of the other party to the contract, presence of a detected witness, authentication\nof a detected witness, and so forth.  Further, registration may occur in any suitable manner.  For example, registration may include storing the image(s) and audio data evidencing the contract to database 2906, sending a confirmation of the verbal\ncontract to one or more parties, etc.\nFIG. 31C illustrates a flowchart of an exemplary method 3140 for identifying and/or authenticating the identity of one or more witnesses to the identified verbal contract, consistent with embodiments of the present disclosure.  The method 3140\nmay be carried out, for example, by a processing device integrated with and/or associated with wearable apparatus 110.  In some other examples, the entire method 3140 or parts of method 3140 may be performed by a device external to wearable apparatus\n110, such as a device paired with wearable apparatus 110 (such as a smartphone, a tablet, etc.), a server communicating with wearable apparatus 110 (such as server 250), and so forth.  At block 3142, action execution module 2904 may receive image data\ncaptured by a wearable image sensor.  Block 3142 may be facilitated by software instructions of data capture module 2901.  For example, data capture module 2901 may be configured to execute instructions to receive image data from a wearable image sensor.\nAt block 3144, the received image data may be processed via software steps executed by person identification module 2902.  For example, at block 3144, person identification module 2902 may identify one or more images including the at least one\nwitness from a plurality of captured images.  For example, the at least one witness may be person 3004, and the module 2902 may analyze the plurality of images to identify a subset of the captured images that include features sized, shaped, or otherwise\nresembling a person other than person 3002 and user 100.  Further, at block 3146, the one or more identified images may be processed to identify the particular person(s) that are witnesses and depicted in the images flagged as including person(s).\nThe identification information about the at least one witness may be sourced from any desired location.  For example, the information may be sourced from prior-sourced information stored in database(s) 2906 via database access module 2905.  For\nfurther example, the information may be sourced from one or more social media accounts.  In such embodiments, wearable device 110 may source the information publicly available via a web browser and/or through a private account of user 100.  In other\nembodiments, the information may be sourced from records of email, text, voicemail, or telephone communications between user 100 and the at least one identified witness.  Indeed, the information may be sourced from any suitable location, not limited to\nthose described herein, depending on implementation-specific considerations.  Once located, the at least part of the identification information may be registered at block 3148.  For example, the identification of the witness may be registered with the\nregistration of the verbal contract to indicate that the contract is verified.\nTransmitting Information Based on a Physical Distance\nIn some embodiments, wearable apparatus 110 may collect information related to at least one person or object detected in an environment of the user of the wearable apparatus 110.  The wearable apparatus 110 may then transmit information related\nto the at least one person or object based on an estimated physical distance from the user of the wearable apparatus to the at least one person or object.  Some existing wearable device systems may encounter the technical problem of how to process the\ninformation collected by the wearable device and use that information to provide useful feedback to the user.  For example, certain existing systems may capture images that include people or objects in the user's environment, but given the amount of\ncollected data and the likelihood that the majority of the data is not of interest to the user, fail to provide information pertinent to the user (e.g., information that the user finds useful or of interest).  Some of the presently disclosed embodiments,\non the other hand, may address this problem by providing information to the user based on the user's estimated physical distance to a particular person or object.  Such embodiments may make use of the estimated physical distance to determine whether a\nperson or object is likely relevant and/or of interest to the user and then provide information to the user on that basis.\nAs discussed above, system 200 may comprise a wearable apparatus 110, worn by user 100, and an optional computing device 120 and/or a server 250 capable of communicating with wearable apparatus 110 via a network 240.  Consistent with this\ndisclosure, wearable apparatus 110 may analyze image data to detect and identify an object or a person, may determine a distance or estimated distance from the user to the identified object or person, and may transmit information to, for example, update\na social media account, as described in greater detail below.  Wearable apparatus 110 may also transmit information to computing device 120, which may be, for example, a smartphone or tablet having a dedicated application installed therein.  A graphical\nuser interface (GUI) including, for example, a plurality of user-adjustable feature social media settings may be included on display 260 of computing device 120 to visibly output information to an operating user.\nFIG. 32 illustrates an exemplary embodiment of a memory containing software modules consistent with the present disclosure.  In particular, as shown, memory 3200 may include a detection module 3202, a distance module 3204, a transmission module\n3206, a database access module 3208, and a database 3210.  Modules 3202, 3204, 3206, and 3208 may contain software instructions for execution by at least one processing device, e.g., processor 210, included with a wearable apparatus (e.g., wearable\napparatus 110).  Detection module 3202, distance module 3204, and transmission module 3206, database access module 3208, and database 3210 may cooperate to detect an object, determine a distance to the object, and transmit information related to the\ndetected object.  In some embodiments, memory 3200 may be included in, for example, memory 550, discussed above.  Further, in other embodiments, the components of memory 3200 may be distributed over more than one location (e.g. stored in a server 250 in\ncommunication with, for example, network 240).\nIn some embodiments, detection module 3202 may detect a person or an object in the environment of the user of the wearable apparatus.  Detection module 3202 may operate in a manner similar to data capture module 2601 and person identification\nmodule 2602, as illustrated in FIG. 26 and discussed above.  For example, detection module 3202 may include software instructions for receiving data from wearable apparatus 110, such as a wearable camera system, and may include software instructions for\nanalyzing data obtained by wearable apparatus 110 to identify a person or an object associated with at least one person.  Data received from a wearable camera system may include audio and image data, captured, by, for example, an image sensor or a\nmicrophone associated with the wearable camera system and/or related to an audio topic.  Audio data captured by the microphone may identify an audio topic associated with the person.  Image data may include raw images and may include image data that has\nbeen processed.  Raw images may be provided, for example, in the form of still images and video data, either with or without embedded metadata.  In some embodiments, image data and audio data may be preprocessed prior to it being received by detection\nmodule 3202.  Preprocessing may include, for example, noise reduction, artifact removal, compression, and other image pre-processing techniques.\nIn some embodiments, detection module 3202 may detect or identify a subset or portion of the captured data that includes at least one person or object.  In some embodiments, detection module 3202 may be configured to receive a plurality of\nimages that include at least one person or object.  For example, detection module 3202 may receive a plurality of images of an environment surrounding a user wearing the wearable device 110 and may identify which of the plurality of images include at\nleast one person or object.\nIn some embodiments, detection analysis may be performed by executing a facial recognition algorithm designed to detect facial features (e.g., mouth, eyes, etc.), facial contours, paralinguistic indicators such as facial gestures or expressions,\nbody shape, or any other suitable identifying feature of a person.  In other embodiments, the at least one person may be identified using a thermal signature algorithm design to detect the presence of at least one person based on the heat generated by\nthe at least one person.  In such embodiments, the wearable device 110 may capture thermal images, either alone or in combination with visual images, for processing by the thermal signature algorithm.  Thermal recognition of the at least one person may\nbe desirable in implementations in which wearable device 110 is operating in reduced lighting situations.  In some embodiments, the at least one person may be identified through application of one or more classification techniques.  For example, at least\none image classification technique may be used to classify at least one feature of an image.  In some embodiments, an image classification technique may include at least one or more of image enhancement, edge detection, image analysis, and data\nextraction.  Specific examples of the methods for identifying at least one person or at least one object are exemplary only, and a person of ordinary skill in the art will recognize other methods for identifying the at least one person or object that\nremain consistent with present disclosure.\nIn some embodiments, the at least one person may be detected using a face detection algorithm, using a neural network trained to detect faces and/or persons in images, and so forth.  In some examples, the at least one person may be identified\nusing a face recognition algorithm, using a neural network trained to identify people in images, and so forth.  In other examples, the at least one object may be detected using an object detection algorithm, using a neural network trained to detect\nobjects and/or associated characteristics in images, and so forth.  In some examples, the at least one object may be identified using an object recognition algorithm, using a neural network trained to identify objects in images, and so forth.\nDetection module 3202 may be further configured to determine or obtain information associated with the at least one person or object identified in the image(s).  Information associated with the at least one person may include a name, nickname,\nsocial security number, account number, or any other identifier of the at least one person.  Information associated with the at least one object may include a length, width, depth, GPS position of an object, brand of an object, a value or cost of an\nobject, an occupancy of an object, or any other identifier or characteristic indicator of the at least one object.\nIn some embodiments, distance module 3204 may determine a measurement of an estimated physical distance of a detected person or object from the user of wearable apparatus 110.  In some examples, the distance to a detected person and/or object\nmay be estimated using depth imaging, such as: stereo, active stereo, LIDAR, and so forth.  Stereo imaging may include use of spatially separated multiple cameras to form images from different directions.  Depth information may then be extracted from the\ndifferences in the images to determine a measurement of an estimated physical distance.  In some embodiments, active stereo imaging may include a range of pulse techniques to measure a particular distance to a point of a person and/or object, and may\ninclude, for example, laser pulse or laser line scans, radar, and ultrasound.\nIn other embodiments, LIDAR techniques may be employed in accordance with software instructions from distance module 3204 to determine a measurement of an estimated physical distance to a person or an object.  LIDAR relates generally to systems\nand processes for measuring distances to a target person or object by illuminating the target person or object with laser light and detecting the reflection of the light.  For example, a pulsed laser light device, which may be included in wearable\napparatus 110, may emit light incident upon a surface of a person or an object, and pulsed light reflected from the surface of the person or object may be detected at a receiver.  A timer may measure an elapsed time from light being emitted from the\nlaser light device to the reflection reaching the receiver.  Based on a measurement of the elapsed time and the speed of light, processor 210 may be able to calculate the distance to the target person or object.\nIn some embodiments, a receiver in a LIDAR system of distance module 3204 of wearable apparatus 110 may be equipped with sensors such as avalanche photodiodes (APD) to detect reflected light pulses at particular wavelengths.  LIDAR systems may\nalso include a scanning mechanism so that the incident laser may scan over multiple points on the target person or object, and may generate 3-D point clouds that include object distance or depth information.  Mechanical LIDAR systems are well known in\nthe art and include mechanical scanning mechanisms to acquire distance information at multiple points of coverage, and may be incorporated as part of wearable apparatus 110.\nIn other embodiments, wearable apparatus 110 may include a mechanical rotatable LIDAR system that may include an upper scanning mechanism and a fixed lower part to determine a distance in accordance with software instructions from distance\nmodule 3204.  The upper scanning mechanism may include a predetermined number of laser-detector pairs, and may rotate at a fixed frequency to determine an estimated distance to a person or object.  Software instructions from distance module 3204 may\nchange the operation of a number of laser-detector pairs and may change the frequency of rotation in order to capture additional data and provide additional distance measurements.\nIn other embodiments, the distance to a detected person and/or object may be estimated by distance module 3204 based on the size in pixels of the person and/or object in the captured images, the position in the captured images, and/or based on\nan estimation of the physical size of the person and/or object.  For example, if the size in pixels of the person and/or object in the captured images is determined to be large or covering a large pixel area, a short distance to the person and/or object\nmay be estimated.  Conversely, if the size in pixels of the person and/or object in the captured images is determined to be small or covering a small pixel area, a far distance to the person and/or object may be estimated.  Similarly, if a position of\nthe person and/or object in the captured images is determined to be in the foreground, a short distance to the person and/or object may be estimated.  Conversely, if a position of the person and/or object in the captured images is determined to be in the\nbackground, a far distance to the person and/or object may be estimated.  Indeed, the distance measurement may be estimated in any suitable manner relating to a person and/or object, not limited to the examples herein, depending on\nimplementation-specific considerations.\nIn some embodiments, transmission module 3206 may transmit, according to the determined distance measurement, information related to the detected person and/or object.  For example, information may be communicated or transmitted from wearable\napparatus 110 to a paired device, such as computing device 120, or an external server, such as server 250.  In some embodiments, wearable apparatus 110 may include a communication device, such as one or more wireless transceivers, as discussed above in\nconnection with FIGS. 5A-5C, which may transmit information across network 240 to, for example, computing device 120 and/or server 250\nIn some embodiments, transmission module 3206 may determine whether to transmit information based on at least the determined distance estimated by distance module 3204.  For example, transmission module 3206 may determine whether to transmit\ninformation in accordance with a predetermined distance threshold.  If, for example, it is determined that an estimated distance to a person or object exceeds a particular predetermined distance estimate (e.g., greater than 1 meter, greater than 5\nmeters, greater than 10 meters, etc.), information may not be transmitted to the user.  Alternatively, if, for example, it is determined that an estimated distance to a person or object is within a particular predetermined distance estimate (e.g., less\nthan 5 meters, less than 2 meters, less than 1 meter, etc.), information may be transmitted to the user.\nThe information transmitted by transmission module 3206 may include any meaningful data extracted from the image and may include, for example, a person's identifier, name, job title, gender, interests, hobbies, political affiliation (e.g.,\nwhether the user and the at least one person have worked together in the past), leisure related information (e.g., whether the user and the at least one person have played sports together in the past, whether the user and the at least one person are\npredicted to be a successful match, whether the at least one person is single, etc.), matchmaking information (e.g., whether the user and the at least one person have dated in the past), or any other information about the at least one person that is\navailable to the wearable device 110.  The information transmitted by transmission module 3206 may also include, for example, any meaningful data related to a detected object such as a description of the object, value of the object, brand name of the\nobject, and any other information about the at least one object that is available to the wearable device 110.  In some examples, the information transmitted by transmission module 3206 may include images depicting the at least one person and/or the\nobject.  For example, the portions of the image data identified by detection module 3202 as depicting the at least one person and/or the object may be transmitted by transmission module 3206.  In some examples, the information transmitted by transmission\nmodule 3206 may include properties related to the at least one person and/or object identified by analyzing the images.\nIn some embodiments, detection module 3202 may detect multiple persons and/or objects, distance module 3204 may determine measurements of estimated physical distances of the detected persons and/or objects from the user of wearable apparatus\n110, and transmission module 3206 may determine whether to transmit information based on at least the distances estimated by distance module 3204.  For example, transmission module 3206 may determine a threshold based on the person and/or object with the\nsmallest estimated physical distance (for example, twice the estimated physical distance, three times the estimated physical distance, etc.), transmit information related to persons and/or objects corresponding to estimated physical distances smaller\nthan the determined threshold, and withhold transmission of information related to persons and/or objects corresponding to estimated physical distances greater than the determined threshold.  In another example, transmission module 3206 may cluster the\ndistances estimated by distance module 3204, for example using a clustering algorithm, and perform different actions with information related to persons and/or objects corresponding to different clusters, for example the actions may include transmitting\ninformation related to persons and/or objects corresponding to one cluster, providing audible output to a wearer of wearable apparatus 110 about persons and/or objects corresponding to a second cluster, storing information related to persons and/or\nobjects corresponding to a third cluster, ignoring information related to persons and/or objects corresponding to a fourth cluster, and so forth.\nIn some embodiments, database access module 3208 may cooperate with database 3210 to retrieve a plurality of captured images or any type of information.  Database 3210 may be configured to store any type of information of use to modules\n3202-3208, depending on implementation-specific considerations.  For example, in embodiments in which action execution database access module 3208 is configured to provide the information about the identified at least one person or object to the user of\nthe wearable apparatus 110, database 3210 may store prior-collected information about the user's social, familial, or other contacts.  Further, database 3210 may store the metadata associated with the captured images.  In some embodiments, database 3210\nmay store the one or more images of the plurality of captured images that include the at least one person or object.  In some embodiments, database 3210 may store images of known persons, places, or objects, which may be compared with one or more images\ncaptured by wearable apparatus 110.  Indeed, database 3210 may be configured to store any information associated with the functions of modules 3202-3208.\nModules 3202-3208 may be implemented in software, hardware, firmware, a mix of any of those, or the like.  For example, if the modules are implemented in software, they may be stored in memory 550, as shown in FIG. 32.  However, in some\nembodiments, any one or more of modules 3202-3208 and data associated with database 3210, may, for example, be stored in processor 540 and/or located on server 250, which may include one or more processing devices.  Processing devices of server 250 may\nbe configured to execute the instructions of modules 3202-3208.  In some embodiments, aspects of modules 3202-3208 may include software, hardware, or firmware instructions (or a combination thereof) executable by one or more processors, alone or in\nvarious combinations with each other.  For example, modules 3202-3208 may be configured to interact with each other and/or other modules of server 250 and/or a wearable camera system to perform functions consistent with disclosed embodiments.  In some\nembodiments, any of the disclosed modules may each include dedicated sensors (e.g., IR, image sensors, etc.) and/or dedicated application processing devices to perform the functionality associated with each module.\nFIG. 33A is a schematic illustration 3300 of an example of a user wearing a wearable apparatus and capturing an image of a person 3222 according to a disclosed embodiment.  User 100 may wear a wearable apparatus 110 consistent with an embodiment\nof the present disclosure (as shown in FIG. 9).  Capturing unit 710 may be located on an exterior surface of the clothing 750 of user 100.  Capturing unit 710 may also be connected to power unit 720 (not seen in this illustration) via connector 730,\nwhich wraps around an edge of clothing 750.  Wearable apparatus 110 may be differently positioned in any suitable location to enable capture of images of the user's environment, such as the locations explained in detail above.  User 100 may be in any\nlocation and engaging in any interaction encountered during user's daily activities.  For example, user 100 may be at a convenience store, grocery store, sports event, social event, work-related event, movie theater, concert, etc. Wearable apparatus 110\nmay capture a plurality of images depicting the environment to which the user is exposed while user 100 is engaging in his/her chosen activity.  For example, in the illustrated embodiment, wearable apparatus 110 may capture images that include person\n3222.\nA captured image may be analyzed to estimate at least one of: an age of person 3222, a height of person 3222, a weight of person 3222, a gender of person 3222, and so forth.  For example, the analysis may determine the gender of person 3222 is\nmale.  In other embodiments, an image may be analyzed to identify at least one of: an action associated with the person, a product associated with the person, a facial expression of the person, an emotional state of the person, and/or other\nparalinguistic indicators.  For example, as shown in FIG. 33A, the analysis may determine person 3222 is smiling and in a happy emotional state.  A captured image may be analyzed according to any digital processing techniques, as discussed above with\nregard to detection module 3202, to identify person 3222 and to measure an estimated physical distance, using any mechanisms as discussed above with regard to distance module 3204, from the user to person 3222.\nIn some embodiments, capturing unit 710 may capture an image of person 3222 and may estimate a distance D to person 3222.  Distance D to detected person 3222 may be estimated in accordance with instructions from distance module 3204 to implement\nat least one of depth imaging, such as: stereo, active stereo, LIDAR, and so forth.  For example, in one embodiment, wearable apparatus 110 may include at least one LIDAR sensor.  As discussed above, wearable apparatus 110 may be equipped with LIDAR\nsensors such as avalanche photodiodes (APD) to detect reflected light pulses at particular wavelengths.  LIDAR systems may also include a scanning mechanism so that an incident laser may scan over multiple points on the target person, and may generate\n3-D point clouds that include object distance or depth information to provide an estimate of distance D to person 3222.\nIn some examples, distance D to detected person 3222 may be estimated based on the size in pixels of person 3222 in the captured images, the position of person 3222 in the captured images, and possibly on an estimation of the physical size\n(height and weight) of person 3222.  For example, if the size in pixels of the person in the captured images is determined to be large or covering a large pixel area, a short distance to the person may be estimated.  Conversely, if the size in pixels of\nthe person in the captured images is determined to be small or covering a small pixel area, a far distance to the person may be estimated.\nIn some embodiments, an affinity measurement to the person may be determined based, at least in part, on estimated distance D. For example, the degree to which user 100 likes or dislikes person 3222 may be estimated based on distance D. When\nuser 100 likes person 3222, distance D may be small, whereas when user 100 dislikes person 3222, distance D may be large.  A social graph (not shown) may also be updated based on the affinity measurement.  Additional details regarding affinity\nmeasurements are provided above in connection with FIGS. 26-28B.\nFIG. 33B is a schematic illustration of an example of an update of a social network profile according to a disclosed embodiment.  Social network profile 3310 may be displayed as part of a personalized social media page and linked to a user's\nsocial media account.  In some cases, social network profile 3310 may include the name 3320 of user 100, and may further include a captured image of person 3222 during an update as part of the social network profile 3110.  As shown case in FIG. 33B, user\n3320 \"Mike Smith\" is linked to his social network profile 3310, and social network profile 3310 may include features such as a \"News Feed,\" \"Message,\" \"Business,\" \"Shortcuts,\" \"Events,\" \"Groups,\" \"Saved,\" and \"Pages\" functions to customize and control\nsocial network profile 3310.  User 3320 may post textual, photo, and video content, as well as share any emoticons or other paralinguistic indicators expressing a current emotional state of user 3320.\nIn some embodiments, social network profile 3310 may include a notification or alert to other users that user 3320 is in the company of the person 3222.  For example, as shown in FIG. 33B, the notification may include a textual posting 3380 that\n\"Mike Smith is with Joe Johnson\" and may include a pictorial posting of person 3222 or an image of \"Joe Johnson.\" This update may be triggered when it is determined that user 100 is within a predetermined threshold distance D to person 3222.  The updated\nposting may remain until it is determined that user 100 is no longer within a predetermined threshold distance D to person 3222.  This determination may be made based on image analysis of a plurality of images of person 3222 over a selected period of\ntime.  The update may include only one or both of textual and pictorial changes to the social network profile 3310.  In addition, the size of person 3222 as displayed within social network profile 3310 may increase or decrease based on a decrease or\nincrease of distance D from user 100 to person 3222.  For example, the size of person 3222 as displayed may be proportional to the distance D, to log(D), to exp((D-A)/B) for some constants A and B, to f(D) for some monotonically increasing function f,\nand so forth.  The update may further include additional alerts or notifications sent out directly to friends included the social network of user 100, and the alerts or notifications may also be distributed as part of a shareable feed that friends in the\nsocial network of user 100 may be able to subscribe to and/or follow.  Mutual friends, and friends not directly part of the social network of user 100 may also be able to receive alerts or notifications based on adjustable social media settings set by\nuser 100.\nFIG. 33C is a schematic illustration 3330 of an example of a user wearing a wearable apparatus and capturing an image of an object in the environment of the user according to a disclosed embodiment.  In some embodiments, a plurality of images\ncaptured from an environment of a user of a wearable apparatus may be obtained.  The plurality of images may be analyzed to detect an object.  Tent 3332 may be included in a plurality of captured images.\nCapturing unit 710 may capture an image of an object or tent 3332 and may estimate a distance D to tent 3332.  Distance D to detected tent 3332 may be estimated using depth imaging, such as: stereo, active stereo, LIDAR, and so forth.  For\nexample, in one embodiment, wearable apparatus 110 may include at least one LIDAR sensor.  As discussed above, wearable apparatus 110 may be equipped with LIDAR sensors such as avalanche photodiodes (APD) to detect reflected light pulses at particular\nwavelengths.  LIDAR systems may also include a scanning mechanism so that an incident laser may scan over multiple points on the target object, and may generate 3-D point clouds that include object distance or depth information to provide an estimate of\ndistance D to object 3332.  In some examples, distance D to detected tent 3332 may be estimated based on the size in pixels of tent 3332 in the captured images, the position of tent 3332 in the captured images, and possibly on an estimation of the\nphysical size (length, width, and depth) of tent 3332.  A proximity measure to tent 3332 may be based, at least in part, on estimated distance D. For example, the degree to which user 100 is near or far from tent 3332 may be determined based on distance\nD. In addition, information associated with distance D measurement may be stored in memory and/or transmitted to an external device.\nIn some embodiments, captured audio data may be captured by a microphone and analyzed to identify audio topics associated with tent 3332.  For example, captured audio data may include sounds from nature such as rushing river rapids or other\nsounds determined near to tent 3332, and may indicate the location of tent 3332 and/or associated camping grounds.  Consistent with this disclosure, captured lighting data such as intensity or brightness may also be analyzed to determine a time of date,\nsunrise, sunset, and so forth.\nFIG. 33D is a schematic illustration of an example of an update of a social network profile according to a disclosed embodiment.  Social network profile 3340 may be displayed as part of a personalized social media page and linked to a user's\nsocial media account.  In some cases, social network profile 3340 may include the name 3344 of user 100, and may further include an image of tent 3332 during an update as part of social network profile 3340.  As shown in FIG. 33D, user 3344 \"Mike Smith\"\nis linked to his social network profile 3340, and social network profile 3340 may include features such as a \"News Feed,\" \"Message,\" \"Business,\" \"Shortcuts,\" \"Events,\" \"Groups,\" \"Saved,\" and \"Pages\" functions to customize and control social network\nprofile 3340.  User 3344 may post textual, photo, and video content, as well as share any emoticons or other paralinguistic indicators expressing a current emotional state of user 3344 in association with the detection of object or tent 3332.\nIn some embodiments, a social network profile 3340 update may include a notification or alert to other users that user 3344 is positioned in the location of tent 3332.  For example, as shown in FIG. 33D, the notification may include a textual\nposting 3390 that \"Mike Smith is at Local Campgrounds,\" and may include a pictorial posting of tent 3332.  This update may be triggered when it is determined that user 3344 is within a predetermined threshold distance D to tent 3332.  The updated posting\nmay remain until it is determined that user 3344 is no longer within a predetermined threshold distance D to tent 3332.  This determination may be made based on image analysis of a plurality of images of tent 3332 over a period of time.  The update may\ninclude only one or both of textual and pictorial changes to the social network profile 3340.  In addition, the size of tent 3332 as displayed within social network profile 3340 may increase or decrease based on a decrease or increase of distance D from\nuser 100 to tent 3332.  For example, the size of tent 3332 as displayed may be proportional to the distance D, to log(D), to exp((D-A)/B) for some constants A and B, to f(D) for some monotonically increasing function f, and so forth.  In other\nembodiments, images may be analyzed to estimate at least one of: a location of an object, such as tent 3332, a GPS position of an object, a brand of an object, a value or cost of an object, an occupancy status of an object, or other characteristic\nindicators.\nFIG. 34 is a flowchart of an example of a method 3400 for providing information to a user of a wearable apparatus.  Steps of method 3400 may be performed by one or more processors of server 250 and/or memory 550 and memory modules 3200.\nAt step 3402, detection module 3202 may detect a person or an object in the environment of the user of the wearable apparatus.  Detection module 3202 may operate in a manner similar to data capture module 2601 and person identification module\n2602, as illustrated in FIG. 26.  Detection module 3202 may include software instructions for receiving data from wearable apparatus 110, such as a wearable camera system, and may include software instructions for analyzing data obtained by wearable\napparatus 110 to identify a person or an object associated with at least one person.  An object may include, for example, person 3222 or tent 3332.  In some embodiments, detection module 3202 may detect or identify a subset or portion of the captured\ndata that includes person 3222 or object 3332.  In some embodiments, detection module 3202 may be configured to receive a plurality of images that include person 3222 or object 3332.  For example, detection module 3202 may receive a plurality of images\nof an environment surrounding a user wearing the wearable device 110 and may identify which of the plurality of images include person 3222 or object 3332.\nIn some embodiments, detection analysis may be performed by executing a facial recognition algorithm designed to detect facial features (e.g. mouth, eyes, etc.), facial contours, paralinguistic indicators such as facial gestures or expressions,\nbody shape, or any other suitable identifying feature of person 3222.  The plurality of images may be analyzed to estimate an age of person 3222, a height of person 3222, a weight of person 3222, a gender of person 3222, an action associated with person\n3222, a product associated with person 3222, and an action associated with tent 3332 and person 3222 associated with tent 3332.  The analysis may include extraction of meaningful data and may employ digital processing techniques to identify a person or\nan object captured in a digital image.\nAt step 3404, distance module 3204 may determine a measurement of an estimated physical distance from the user to the detected person and/or object.  For example, wearable apparatus 110 may include at least one LIDAR sensor.  As discussed above,\nwearable apparatus 110 may be equipped with LIDAR sensors such as avalanche photodiodes (APD) to detect reflected light pulses at particular wavelengths.  LIDAR systems may also include a scanning mechanism so that an incident laser may scan over\nmultiple points on the target person, and may generate 3-D point clouds that include object distance or depth information to provide an estimate of distance D to person 3222 or tent 3332.  In other examples, the distance to detected person and/or object\nmay be estimated based on the size in pixels of person 3222 or tent 3332 in the captured images, the position in the captured images, and possibly on an estimation of the physical size of person 3222 or tent 3332.  In some embodiments, the plurality of\nimages may depict a second object in the environment of the user of the wearable apparatus, and may determine a second distance measurement for basing a transmission.  The second measurement may include an estimated physical distance from the user to the\nsecond object.\nAt step 3406, transmission module 3206 may transmit, based on the determined physical distance measurement, information related to the detected person and/or object.  As discussed, in some embodiments, transmission module 3206 may determine\nwhether to transmit information in accordance with a predetermined distance threshold (e.g., when the determined distance measurement is within the predetermined distance threshold).  Information may be communicated or transmitted from wearable apparatus\n110 via a communication device (e.g., wireless transceiver 530) to a paired device, such as computing device 120, or an external server, such as server 250.  In some embodiments, transmission module 3206 may provide information to the user of the\nwearable apparatus 110 based on information associated with the at least one person or object identified in the captured images.\nIn some embodiments, information related to the detected object may be transmitted to update at least one of a social graph and a social network profile.  This update may be triggered when it is determined that user 100 is within a predetermined\nthreshold distance to to a person or object.  The update may remain in place until it is determined that user 100 is no longer within a predetermined threshold distance to the person or object, for a selected time period, until another notification\narrives, and so forth.  The update may include a notification of a textual and/or pictorial posting, and may be based on information related to the detected person or object.\nIn some embodiments, step 3406 of method 3400 may be replaced by other steps performing other actions with information related to detected persons and/or objects based on the determined physical distance measurement.  For example, method 3400\nmay provide audible information about detected persons and/or objects when the determined physical distance measurement corresponding to the detected person and/or object is smaller than a selected threshold.  In another example, the level of details\nprovided to a user (for example, as audible and/or visual output) may be determined on the determined physical distance measurement corresponding to the detected person and/or object, for example as described above in connection with FIGS. 26-28B.\nProviding a Social Media Recommendation\nIn some embodiments, wearable apparatus 110 may analyze one or more images captured by a wearable image sensor included in the wearable apparatus, obtain information based on analysis of the one or more captured images, and generate one or more\ncontact recommendations for at least one new social network contact based on the obtained information.  In some embodiments, the one or more contact recommendations may include new social network contact recommendations, and the at least one new social\nnetwork contact may be a member of one or more social networks.\nSome existing wearable device systems may encounter the technical challenge of how to process the information collected by the wearable device and use that information to provide useful feedback to the user.  Some of the presently disclosed\nembodiments may address this problem by providing social media recommendations to the user based on image data captured by a wearable device.  As such, the recommendations may be targeted as they may be based on information related to persons and/or\nobjects encountered by the user of the wearable device.\nFor example, in one embodiment, wearable apparatus (e.g. wearable apparatus 110) may analyze images for providing social media recommendations based on images captured in the environment of a user.  The analysis may include extraction of\nmeaningful data and may employ digital processing techniques to obtain information captured in a digital image that may be relevant to providing a social media recommendation.  In some embodiments, the obtained information may include identity\ninformation of at least one person present in the environment of the user.  At least one contact recommendation of the user may be made based on an interaction with identified people and a length of the interaction with identified people.  The at least\none contact recommendation of the user may also include a new contact in a social network.  In other embodiments, the obtained information may include information related to an activity or object present in the environment of the user of the wearable\napparatus.  For example, the activity may include reading a book and an object may include a book written by an author.  The obtained information may further include information related to the author of the book.\nAs discussed above, system 200 may comprise a wearable apparatus 110, worn by user 100, and an optional computing device 120 and/or a server 250 capable of communicating with wearable apparatus 110 via a network 240.  Consistent with this\ndisclosure, apparatus 110 may analyze image data to obtain information captured in a digital image and may provide a social media recommendation, as described in greater detail below.  Apparatus 110 may also transmit information to computing device 120,\nwhich may be, for example, a smartphone or tablet having a dedicated application installed therein.  A graphical user interface (GUI) including a plurality of user-adjustable feature social media settings may be included on display 260 of computing\ndevice 120 to visibly output social media recommendations to an operating user.  Additionally or alternatively, server 250 may receive information based on image data captured by wearable apparatus 110, server 250 may analyze the received information to\nprovide a social media recommendation, as described in greater detail below, and transmit information to computing device 120 associated with a user of wearable apparatus 110, which may be, for example, a smartphone or tablet having a dedicated\napplication installed therein.\nFIG. 35 illustrates an exemplary embodiment of a memory containing software modules consistent with the present disclosure.  In particular, as shown, memory 3500 may include an analysis module 3502, an information module 3504, a recommendation\nmodule 3506, a database access module 3508, and a database 3510.  Modules 3502, 3504, 3506, and 3508 may contain software instructions for execution by at least one processing device, e.g., processor 210, included with a wearable apparatus (e.g.,\nwearable apparatus 110).  Analysis module 3502, information module 3504, recommendation module 3506, database access module 3508, and database 3510 may cooperate to analyze a captured image, obtain information based on the analysis, and generate and\nprovide a contact recommendation.  The contact recommendation may be generated for a user of a wearable apparatus.  In some embodiments, the contact recommendation may be generated for the user of the wearable apparatus and an other person.  Further, the\ncontact recommendation may be for a new social network contact.  In some embodiments, memory 3500 may be included in, for example, memory 550, discussed above.  Further, in other embodiments, the components of memory 3500 may be distributed over more\nthan one location (e.g. stored in a server 250 in communication with, for example, network 240).\nIn some embodiments, analysis module 3502 may analyze at least one image captured by a wearable image sensor included in the wearable apparatus from an environment of a user of the wearable apparatus.  Analysis module 3502 may operate in a\nmanner similar to detection module 3202, as illustrated in FIG. 32 and discussed above.  Analysis module 3502 may include software instructions for receiving data from wearable apparatus 110, such as a wearable camera system, and may include software\ninstructions for analyzing data obtained by wearable apparatus 110 to identify a person, activity, or an object associated with at least one person.  Data received from a wearable camera system may include audio and image data, captured, by, for example,\nan image sensor or a microphone associated with the wearable camera system and/or related to an audio topic.  Audio data captured by the microphone may identify an audio topic associated with the person.  Image data may include raw images and may include\nimage data that has been processed.  Raw images may be provided, for example, in the form of still images and video data, either with or without embedded metadata.  In some embodiments, image data and audio data may be preprocessed prior to capture by\nanalysis module 3502.  Preprocessing may include, for example, noise reduction, artifact removal, compression, and other image pre-processing techniques.\nIn some embodiments, analysis module 3502 may detect or identify a subset or portion of the captured data that includes at least one person, activity, or object.  In some embodiments, analysis module 3502 may be configured to receive a plurality\nof images that include at least one person or object.  For example, analysis module 3502 may receive a plurality of images of an environment surrounding a user wearing the wearable device 110 and may identify which of the plurality of images include at\nleast one person or object.\nIn some embodiments, analysis may be performed by performing a facial recognition algorithm designed to detect facial features (e.g. mouth, eyes, etc.), facial contours, paralinguistic indicators such as facial gestures or expressions, body\nshape, or any other suitable identifying feature of a person.  In other embodiments, at least one person may be identified using a thermal signature algorithm design to detect the presence of at least one person based on the heat generated by the at\nleast one person.  In such embodiments, the wearable device 110 may capture thermal images, either alone or in combination with visual images, for processing by the thermal signature algorithm.  Thermal recognition of the at least one person may be\ndesirable in implementations in which wearable device 110 is operating in reduced lighting situations.  In some embodiments, at least one person may be identified through application of one or more classification techniques.  For example, at least one\nimage classification technique may be used to classify at least one feature of an image, such as a particular activity of the user, such as reading a book.  In some embodiments, an image classification technique may include at least one or more of image\nenhancement, edge detection, image analysis, and data extraction.\nIn some examples, the at least one person may be detected using a face detection algorithm, using a neural network trained to detect faces and/or persons in images, and so forth.  In some examples, the at least one person may be identified using\na face recognition algorithm, using a neural network trained to identify people in images, and so forth.  In other examples, the at least one object may be detected using an object detection algorithm, using a neural network trained to detect objects\nand/or associated characteristics in images, and so forth.  In some examples, the at least one object may be identified using an object recognition algorithm, using a neural network trained to identify objects in images, and so forth.  Specific examples\nof the methods for identifying at least one person or at least one object, such as a book, are exemplary only, and a person of ordinary skill in the art will recognize other methods for identifying the at least one person or object that remain consistent\nwith present disclosure.\nInformation module 3504 may then obtain information based on a result of the analysis of the at least one captured image.  For example, information module 3504 may be further configured to determine or obtain information associated with the at\nleast one person or object identified in the image(s).  Information module 3504 may, for example, access a local database (e.g., database 3510) and/or one or more remote databases (e.g., available via server 250) to search for information based on the\nanalysis.  Information that may be obtained for the at least one person may include a name, nickname, social security number, account number, or any other identifier of the at least one person.  Information obtained for the at least one object may\ninclude a length, width, depth, GPS position of an object, brand of an object, a value or cost of an object, an occupancy of an object, or any other identifier or characteristic indicator of the at least one object.  In some embodiments, information\nmodule 3504 may obtain or determine at least one activity of the person in relation to the object, such as a person reading a book, and search a local or remote database based on the activity or information related to the activity (e.g., an author of the\nbook being read).\nRecommendation module 3506 may then generate at least one contact recommendation for the user based on the obtained information.  In some embodiments, recommendation module 3506 may generate at least one contact recommendation for at least one\nnew social network contact based on the obtained information.  For example, the at least one contact recommendation may include a recommendation for a new social network contact for the user and/or for a person other than the user.  In some embodiments,\nthe new social network contact and the user and/or the person other than the user may be members of one or more common social networks.\nRecommendation module 3506 may facilitate transmission of obtained information and a corresponding recommendation.  For example, information may be communicated or transmitted from wearable apparatus 110 to a paired device, such as computing\ndevice 120, to a device associated with the user of wearable apparatus 110 and/or a person other than the user, to an external server (such as server 250), and so forth.\nIn some embodiments, recommendation module 3506 may provide a social media recommendation to the user of the wearable apparatus 110 based on information associated with the at least one person or object identified in the captured images.  The at\nleast one other person may also receive a social media recommendation.\nThe at least one contact recommendation may include, for example, any one or more of a person's name, job title, gender, interests, hobbies, political affiliation (e.g., whether the user and the at least one person have worked together in the\npast), leisure related information (e.g., whether the user and the at least one person have played sports together in the past, whether the user and the at least one person are predicted to be a successful match, whether the at least one person is\nsingle, etc.), matchmaking information (e.g., whether the user and the at least one person have dated in the past), etc. The at least one contact recommendation may also include, for example, any meaningful data related to a detected object or activity\nsuch as one or more of a description of the object, value of the object, brand name of the object, etc.\nIn some embodiments, recommendation module 3506 may generate at least one contact recommendation for the user based also on an affinity between the user and the at least one person in a social network, for example as calculated by step 2810\ndescribed above.\nIn some embodiments, database access module 3508 may cooperate with database 3510 to retrieve a plurality of captured images or any type of information.  Database 3510 may be configured to store any type of information of use to modules\n3502-3508, depending on implementation-specific considerations.  For example, in embodiments in which database access module 3508 is configured to provide the information about a detected person, database 3510 may store prior-collected information about\nthe detected person's social, familial, or other contacts.  Further, database 3510 may store the metadata associated with the captured images.  In some embodiments, database 3510 may store the one or more images of the plurality of captured images that\ninclude the at least one person or object.  In some embodiments, database 3510 may store a social graph, such as a social graph of a social network.  Indeed, database 3510 may be configured to store any information associated with the functions of\nmodules 3502-3510.\nModules 3502-3508 may be implemented in software, hardware, firmware, a mix of any of those, or the like.  For example, if the modules are implemented in software, they may be stored in memory 550, as shown in FIG. 35.  However, in some\nembodiments, any one or more of modules 3502-3508 and data associated with database 3510, may, for example, be stored in processor 540 and/or located on server 250, which may include one or more processing devices.  Processing devices of server 250 may\nbe configured to execute the instructions of modules 3502-3508.  In some embodiments, aspects of modules 3502-3508 may include software, hardware, or firmware instructions (or a combination thereof) executable by one or more processors, alone or in\nvarious combinations with each other.  For example, modules 3502-3508 may be configured to interact with each other and/or other modules of server 250 and/or a wearable camera system to perform functions consistent with disclosed embodiments.  In some\nembodiments, any of the disclosed modules may each include dedicated sensors (e.g., IR, image sensors, etc.) and/or dedicated application processing devices to perform the functionality associated with each module.\nFIG. 36A is a schematic illustration of an example 3600 of a user wearing a wearable apparatus and capturing an image of a person according to a disclosed embodiment.  User 100 may wear a wearable apparatus 110 consistent with an embodiment of\nthe present disclosure (as shown in FIG. 9).  Capturing unit 710 may be located on an exterior surface of the clothing 750 of user 100.  Capturing unit 710 may also be connected to power unit 720 (not seen in this illustration) via connector 730, which\nwraps around an edge of clothing 750.\nAs shown, capturing unit 710 may capture an image including person 3622 and a processor may determine an identity of a person 3622 for providing a social media recommendation.  Further, capturing unit 710 may also capture book 3626 and a\nprocessor may determine an author of book 3626.\nWearable apparatus 110 may be differently positioned in any suitable location to enable capture of images of the user's environment, such as the locations explained in detail above.  User 100 may be in any location and engaging in any\ninteraction encountered during user's daily activities.  For example, user 100 may be at a convenience store, grocery store, sports event, social event, work-related event, movie theater, concert, etc. Wearable apparatus 110 may capture a plurality of\nimages depicting the environment to which the user is exposed while user 100 is engaging in his/her chosen activity.  For example, in the illustrated embodiment, wearable apparatus 110 may capture images that include person 3622.\nA captured image may be analyzed to obtain information in accordance with software instructions from information module 3504.  For example, an image 3630 may be analyzed to obtain information including an age of person 3622, a height of person\n3622, a weight of person 3622, a gender of person 3622, facial features of person 3622 suitable for facial recognition analysis, and so forth.  For example, the analysis may determine the gender of person 3622, named \"Veronica\" is female.  In other\nembodiments, an image may be analyzed to identify at least one of: an action or activity associated with the person, a product associated with the person, a facial expression of the person, an emotional state of the person, and/or other paralinguistic\nindicators.  For example, the analysis may determine \"Veronica\" is reading a book.  Information module 3504 may obtain information from image 3630 that may include information related to an activity present in the environment of the user of the wearable\napparatus, information related to an object present in the environment of the of the wearable apparatus, and identity information of person 3622 present in the environment of the user of the wearable apparatus.  For example, information module 3504 may\nobtain information including information related to the author of the book \"Veronica\" is reading.\nFIG. 36B is a schematic illustration of an example of generating a contact recommendation 3610 according to a disclosed embodiment.  For example, in some embodiments, recommendations for new contacts for a user of a wearable apparatus may be\nbased on obtained information from image data captured using the wearable apparatus.  For example, people and objects appearing in captured image data may be identified, and social media recommendations may be made based on their identity.  In some\nembodiments, wearable apparatus 110 may transmit information including social media recommendations across network 240 to computing device 120 and/or server 250, and recommendation module 3506 may provide a social media recommendation to the user of the\nwearable apparatus 110 based on information associated with the at least one person, activity, or object identified in the captured images.\nAs shown in FIG. 36B, a GUI displaying a contact recommendation of a profile image 3630 of person 3622 identified in the captured image as \"Veronica\" may be provided to the user of the wearable image device 110.  Contact recommendation 3610 may\nbe made based on obtained information.  For example, the obtained information may indicate that the book \"Veronica\" is reading is by an author that the user has read.  The contact recommendation may source user author information from database 3510 in\naccordance with the obtained information prior to providing recommendation 3610.  As shown in FIG. 36B, the user may select button 3624 to add \"Veronica\" as a friend to his social network.  The user may also select \"Comments,\" \"Information,\" and\n\"Buddies\" as displayed in the GUI to see comments relating to \"Veronica,\" obtained information leading to contact recommendation 3610, and buddies or other contacts that \"Veronica\" has in her network before deciding to add \"Veronica\" to his social\nnetwork.  In some examples, the system may access a list of favorite authors of the user in database 3510, may determine if the author of the book \"Veronica\" is reading is in the list, and may decide whether to provide contact recommendation 3610 based\non said determination, for example providing contact recommendation 3610 if the author is in the list, and not to provide contact recommendation 3610 or to turn to other decision rules if the author is not in the list.  In some examples, the system may\nmeasure a distance to \"Veronica\", for example using distance module 3204 described above, and may decide whether to provide contact recommendation 3610 based on the measured distance, for example withholding contact recommendation 3610 when the measured\ndistance is larger than a selected distance.  In some examples, the system may determine whether the person is physically present, for example using physical presence identification module 2002 and/or step 2210 described above, and may decide whether to\nprovide contact recommendation 3610 based on whether the person is present.  In some examples, the system may determine whether the person is visible on a display, for example using physical presence identification module 2002 and/or step 2240 described\nabove, and may decide whether to provide contact recommendation 3610 based on whether the person is visible on a display.  In some examples, the system may determine whether the person is visible in a photo, for example using physical presence\nidentification module 2002 described above, and may decide whether to provide contact recommendation 3610 based on whether the person is visible in a photo.\nFIG. 36C is a schematic illustration of an example 3634 of a user wearing a wearable apparatus 110 and capturing an image of person 3520 in the environment of the user according to a disclosed embodiment.  User 100 may wear a wearable apparatus\n110 consistent with an embodiment of the present disclosure (as shown in FIG. 9) and discussed above in connection with FIG. 36A.\nA show in FIG. 36C, capturing unit 710 may capture an image including person 3520 and a processor may determine an identity of a person 3520 for providing a social media recommendation.  As shown in FIG. 36C, an image of \"Sally Cooper\" 3520 is\ncaptured and information from her image may be gathered in the form of a social media recommendation discussed in FIG. 36D.\nFIG. 36D is a schematic illustration of an example of generating a contact recommendation 3640 according to a disclosed embodiment.  In some embodiments, a contact recommendation may be made based on a level or length of interaction with at\nleast one other person.  For example, social media recommendations may be made and further based on the type of interaction with identified people, on the length of the interaction, and so forth.  As shown in FIG. 36D, the user's interaction with person\n3520 may yield a social media recommendations.  As shown in FIG. 36D, a recommendation to add a new contact \"Sally Cooper\" may be provided.\nAs shown, an invite friend window 3642 with an invite friend button 3644 based on a suggestion text 3646, e.g. \"Mike Smith suggests the following friends\" may be displayed in a GUI.  This recommendation may be made to a person other than the\nuser or the person in the captured image.  For example, rather than providing a social media recommendation directly to person 3520, a social media suggestion to add \"Sally Cooper\" is provided to another person (e.g., a contact of the user of the\nwearable apparatus, e.g., Mike Smith).\nIn some embodiments, second information based on a second plurality of images captured by one or more wearable apparatuses may be obtained.  For example, second information may include information relating to the stretching, exercising, or Yoga\nposture associated with person 3520.  Generating the at least one recommendation may also be based on the second information.  For example, since person 3520 likes to exercise, a social recommendation may be made to friends of the user of the wearable\napparatus who also like to exercise according to information stored in database 3510.  In some examples, a plurality of wearable apparatuses may be associated with a plurality of users to generate a plurality of social media content recommendations.  In\nother embodiments, generating the at least one recommendation may also be based on identity of people appearing in the plurality of images.  In some examples, a plurality of wearable apparatuses may be associated with a plurality of users, information\nbased on image data captured by two or more wearable apparatuses may be analyzed (for example by analyzing profile 2400 described above), and generating the at least one recommendation may also be based on the analysis results.\nFIG. 37 is a flowchart of an example of a method 3700 for providing recommendations based on captured images, consistent with disclosed embodiments.  Steps of method 3700 may be performed by one or more processors of server 250 and/or memory 550\nor memory modules 3500.\nAt step 3702, analysis module 3702 may analyze at least one image captured by a wearable image sensor.  For example, as discussed earlier, analysis module 3702 may analyze the at least one image to detect a person, an object, and/or an activity. Analysis of a person included in the at least one image may be performed by performing a facial recognition algorithm designed to detect facial features (e.g. mouth, eyes, etc.), facial contours, paralinguistic indicators such as facial gestures or\nexpressions, body shape, or any other suitable identifying feature of a person.\nAt step 3704, information module 3504 may obtain information based on the result of the analysis of the captured image.  For example, obtained information in accordance with software instructions from information module 3504 may include an age,\na height, a weight, a gender, etc., of a person detected in the at least one image.  In some embodiments, the obtained information may include information related to an activity or an object present in the environment of the user of the wearable\napparatus.  For example, the object may include a book written by an author and the obtained information may include information related to the author (e.g., stored in database 3510).  In some examples, the obtained information may also include identity\ninformation of at least one other person present in the environment of the user of the wearable apparatus 110.\nAt step 3706, recommendation module 3706 may generate at least one contact recommendation of the user of the wearable apparatus 110 and/or a person other than the user.  The other person may be a person who is known to or a new social network\ncontact of the user.  As discussed earlier, the at least one contact recommendation may include a recommendation for a new social network contact (e.g., a person who is a member of a social network of which the user and/or the at least one other person\nmay also be members).\nAt step 3708, recommendation module may provide the contact recommendation to the user and/or the other person.  For example, recommendation module 3506 may provide a social media recommendation to the user of the wearable apparatus 110 based on\ninformation associated with the at least one person or object identified in the captured images.  In some embodiments, the at least one contact recommendation of the user may be based on an interaction with identified people and a length of interaction. \nThe contact recommendation may include a recommendation for the user in a user window to add a new friend as shown in FIG. 36B, and/or based on a detected identify of a user in a captured image.  Alternatively, as shown in FIG. 36D, the contact\nrecommendation may be based on the identity of the user in the captured image and provided to another person or user.  This recommendation may include a mutual friend of the user and the identified person at a person deemed worthy of the friendship of\nthe identified person.  In some embodiments, the contact recommendation of the user may include a new contact in a social network.  The contact recommendation of the user may also include an existing contact in a social network.  The contact\nrecommendation may be provided to one or both of the user and at least one other person.\nProviding Visual Feedback of a Field of View\nIn some embodiments, a wearable apparatus (e.g., wearable apparatus 110) may include at least one image sensor and at least one projector configured to move together, and the projector may be configured to emit a light pattern that shows the\nborders of the field of view of the at least one image sensor.\nIn some examples, the at least one projector may be controlled by a processing unit, such as processing unit 210.  The processing unit may activate and/or deactivate the at least one projector.  In some examples, the at least one projector may\nbe activated and/or deactivated based, at least in part, on a visual trigger appearing in images captured using the at least one image sensor.  In some examples, the at least one projector may be activate and/or deactivated based, at least in part, on\nvisual context associated with images captured using the at least one image sensor.\nIn some embodiments, a wearable apparatus (e.g., wearable apparatus 110) may obtain a plurality of images captured from an environment of a user of the wearable apparatus.  The plurality of images may be analyzed to identify a visual trigger. \nAt least one projector may be configured to project a light pattern, for example based on the identification of the visual trigger.  In some examples, the light pattern may be indicative of a field of view of an image sensor.  In some examples, the light\npattern may comprise two parallel lines; for example, two parallel lines showing two borders of the field of view of the image sensor.  In some examples, the light pattern may comprise two perpendicular lines; for example, two perpendicular lines showing\ntwo borders of the field of view of the image sensor.  In some examples, the light pattern may comprise lines or dashes in a rectangular pattern; for example, a rectangular pattern showing the borders of the field of view of the image sensor.\nAs discussed earlier, the visual trigger that may activate and/or deactivate the at least one light projector may comprise a hand gesture.  In some examples, the visual trigger may comprise a textual document appearing in the plurality of\nimages.  In some examples, a first visual trigger may appear in at least a first one of a plurality of images to activate at least one projector.  The at least one projector may also be deactivated, for example, after being activated for a selected\nduration, after a second visual trigger is identified, and so forth.  The second trigger may appear in at least a second of the plurality of images.\nFIG. 38A provides a diagrammatic view of wearable apparatus 110, including glasses 130 fitted with image sensor system 220, which may include a light projector 3802, such as the light projector described above.  As discussed, light projector\n3802 may provide feedback to the user of wearable apparatus 110 of a field of view associated with one or more image sensors provided on the wearable apparatus.  In such a way, the user may be informed, through a visual guide, of where the image sensor\nor image sensors of the apparatus are aimed, what those sensors \"see,\" and/or what area of text or object that the processing device may effectively analyze through processing of images captured by the image sensor(s).\nFor example, in some embodiments, a wearable apparatus (e.g., wearable apparatus 110) equipped with light projector 3802 may assist a visually impaired user of the wearable apparatus by providing the user with an indication of a field of view of\none or more image sensors included in the wearable apparatus.  Such information may help the user provide input to the wearable apparatus through the use of gestures, which may act as triggers for causing the wearable apparatus to undertake certain\nactions.\nAlthough FIG. 38A depicts light projector 3802 as surrounding image sensor system 220, light projector 3802 may be arranged in an appropriate manner or location on wearable apparatus 110.  For example, light projector 3802 may be located above,\nbelow, or beside image sensor system 220.  In some embodiments, light projector 3802 may be attached to a side surface of wearable apparatus 110 (e.g., positioned on a surface other than the surface including image sensor system 220) or included in a\ncomponent attached to wearable apparatus 110.  Any appropriate location and positioning of light projector 3802 is contemplated.\nIn some embodiments, light projector 3802 and image sensor system 220 may be configured to move with the head of the user, for example by being connected and/or mounted to glasses 130 as depicted in FIG. 38A.\nAlthough the example shown on FIG. 38A shows apparatus 110, including glasses 130 fitted with image sensor system 220, in other embodiments, a light projector (such as light projector 3802) may be included in any of the variations of wearable\napparatus 110 shown in FIGS. 1-16, and in the example shown in FIG. 38B.\nFIG. 38B shows an example of a wearable apparatus that can be secured to an article of clothing, consistent with disclosed embodiments.  In particular, FIG. 38B provides a diagrammatic view of a wearable apparatus 3800, including a capturing\nunit 3804 fitted with image sensor system 220.  Capturing unit 3804 may be located on an exterior surface of the clothing 750 of user 100.  Capturing unit 3804 may be connected to power unit 720 (not seen in this illustration) via a connector, which may\nwrap around an edge of clothing 750.  Capturing unit 3804 may further include a light projector, such as light projector 3802 described above.  Consistent with this disclosure, light projector 3802 may generate light and project light onto a surface,\nobject, text, etc., in an environment of the user.  Although FIG. 38B depicts light projector 3802 as surrounding image sensor system 220, light projector 3802 may be arranged in an appropriate manner or location on wearable apparatus 3800, as discussed\nabove in connection with FIG. 38A.\nLight projector 3802 may be configured to include any component or components capable of generating light and projecting light onto a surface, object, text, etc., in an environment of the user.  In some embodiments, light projector 3802 may\ninclude a light emitting diode (LED).  In some embodiments, light projector 3802 may include an array of LEDs.  The array of LEDs may be positioned in any suitable arrangement, such as around an aperture of an image sensor associated with system 220, for\nexample.  In some cases, light projector 3802 may include one or more light path altering structures.  For example, light projector 3802 may include one or more lenses to direct light from a light source onto a surface or object, etc., in an environment\nof the user along a desired light path.  Light projector 3802 may also include one or more light hoods or shrouds to aid in selectively illuminating only a portion of a surface or object in an environment of the user (e.g., by blocking light or otherwise\nreducing or limiting a light field emitted from one or more light sources of light projector 3802).\nIn addition to LEDs, light projector 3802 may also include one or more solid state lasers.  Such lasers (and/or LEDs) may be used to illuminate a fixed area relative to a surface or object in an environment of the user.  In other embodiments,\nhowever, lasers and/or LEDs may be configured to scan at least a portion of a surface or object in an environment of the user.  For example, such scanning light sources may be scanned over a particular pattern such that portions of the pattern are\nilluminated at different times.  A scan may be associated with a lighting profile comparing light intensity over the scan pattern as a function of time.  At relatively low scan rates (e.g., 30 Hz or below), the scanning of the pattern may be perceptible\nto a user.  At higher scan rates (e.g., 30 Hz or above, 60 Hz or above, or even higher), scanning of the pattern may be more difficult to discern.  For certain light sources and scan rates, the scanned light pattern may appear to a user as a continuously\nilluminated pattern.\nFIG. 39 provides a diagrammatic illustration of one example of a type of visual feedback that light projector 3802 in FIG. 38A may provide to a user of wearable apparatus 110.  For example, light projector 3802 may generate a light projection\npattern 3902 that illuminates one or more surfaces or objects in an environment of the user.  As shown in FIG. 39, light projection pattern 3902 is illuminating a text book 3906 including multiple lines of text 3908 with a light incident pattern 3904. \nAs shown, light incident pattern 3904 includes a series of dashed lines roughly arranged in a rectangular outline pattern.  Light projector 3802 may provide visual feedback to the user to indicate that wearable apparatus 110, including one or more image\nsensors and/or one or more processing devices associated with wearable apparatus 110, is able to capture and/or process images from a field of view at least partially overlapping with an area bounded by light incident pattern 3904.  In some cases, image\ncapture area can be substantially the same as the area bounded by light incident pattern 3904.  In this way, a user may better understand what the apparatus \"sees\" and, therefore, whether the apparatus will be able to provide information relative to an\nobject, etc., within the environment of the user.  If the light incident pattern 3904 does not align with an area of interest to the user, the user can reposition his or her head and facing direction until light incident pattern 3904 surrounds or covers\nan area of interest (e.g., text, bank note, object portion, person, etc.) in an environment of the user.\nIn some embodiments, as discussed above, wearable apparatus 110 may be securable to clothing, such as shown in FIG. 39B.  In such embodiments, wearable apparatus 110 and light projector 3802 may operate in a substantially similar manner as shown\nin FIG. 39 to project a light projection patter that illuminates one or more surfaces or objects in an environment of a user.\nMoreover, light incident pattern 3904 is not limited to the dashed outline pattern shown in FIG. 39.  Rather, any suitable illumination pattern for providing visual feedback to the user may be employed.  FIGS. 40A-40H provide several examples of\nvarious patterns that can be generated by light projector 3802.  For example, light incident pattern 3904 can have a circular or elliptical outline pattern (FIG. 40A), a rectangular/square outline pattern (FIG. 40B), a dashed pattern (FIG. 40C), one or\nmore illuminated dots (FIG. 40D), a rectangular/square solid illumination pattern (FIG. 40E), a circular or elliptical solid illumination pattern (FIG. 40F), two parallel horizontal lines (FIG. 40G), two parallel vertical lines (FIG. 40H), or any other\nsuitable pattern.\nLight incident pattern 3904 may also comprise one or more colors.  In some embodiments, pattern 3904 may be created by white light, red light, green light, blue light, or any other color or combination of colors.\nThere may also be various ways to initiate illumination by light projector 3802.  In some cases, light projector 3802 may be illuminated in response to a user input.  For example, the user may manually activate a switch, button, etc. to change\nillumination states of the light projector 3802 (e.g., from OFF to ON or from ON to OFF).  In some cases, the user may control operation of light projector 3802 through voice commands.  In addition to changes in illumination state, other operational\ncharacteristics of the light projector may also be controlled by the user.  For example, the user may control a brightness level associated with light generated by the light projector, may change colors of one or more portions of light incident pattern\n3904, or may control any other operational characteristic of light projector 3802.  As discussed above, operation of light projector 3802 may occur automatically.  For example, light projector 3802 may be activated, deactivated, dimmed, etc. by at least\none processing device in response to a detected trigger (e.g., a gesture by the user, including pointing, hand wave, or any other gesture), in response to a detected type of object (e.g., text, bank note, etc.), in response to an object detected in the\nuser's hand, or any other type of trigger.  In another example, light projector 3802 may be activated, deactivated, dimmed, etc. by at least one processing device in response to a determination that a text and/or an object of interest is partially\ncaptured by an image sensor, for example as determined by analyzing images captured using the image sensor.  Light projector 3802 may also be controlled based on an amount of time that an object remains present in a field of view of the user.  For\nexample, if an object lingers in a field of view of the user for more than 1 second, 2 seconds (or any other suitable time threshold), then the at least one processing device may determine that the user would like information about the object and may\ntake one or more actions relative to the object, including illuminating at least a portion of the object with an incident light pattern 3904.  As discussed in FIG. 5A, apparatus 110 may also include microphone, which may initiate illumination by light\nprojector 3802.  For example, a processing device may change, based on a voice command captured at the microphone, an illumination state of light projector 3802, a color of the light pattern, or a shape of the light pattern.\nAlignment of light incident pattern 3904 with an active field of view of one or more image sensors associated with wearable apparatus 110 may be accomplished with an adjustment component and/or in various ways.  For example, in some embodiments,\nthe user may be prompted (e.g., by audible signals or voice instructions) generated by at least one processor in response to the processor's analysis of one or more images captured by an image capture device (e.g., camera) associated with apparatus 110. \nSuch an alignment procedure may occur through the processor causing light projector to turn ON and analyzing at least one image captured by a camera associated with apparatus 110 to detect the presence of light incident pattern 3904 on a surface or\nobject represented by the captured image.  If the light incident pattern is fully present in the captured image, then no alignment may be needed.  However, if only a portion of the incident pattern present in the captured image, the processor may\ninstruct the user (e.g., voice commands, audible signals, visible guides, etc.) to manipulate one or more alignment controls (set screws, knobs, etc.) until the incident pattern is sufficiently present in one or more subsequently captured images.  Such\nan alignment process can also be fully automatic.  That is, rather than instructing the user to manipulate one or more adjustments to align light incident pattern 3904 with an operational field of view of the image capture device(s), the processor may\nautomatically control one or more adjusters to align light incident pattern 3904 with the field of view of the image capture device(s).  Such adjustors may include, e.g., micromotors for adjusting set screws, splined or threaded rods (e.g., screw drive\nunits), piezoelectric steppers, or any other type of electromechanical adjustment device.\nFIG. 41 is a flowchart of an example of a method 4100 for providing visual feedback to a user of a wearable apparatus, such as wearable apparatus 110 or wearable apparatus 3800, consistent with disclosed embodiments.  Some of the steps of method\n4100 may be performed by at least one processor, which may execute software instructions stored in, for example, memory 550.\nAt step 4102, the wearable apparatus may capture, via image sensor system 220 included in the wearable apparatus, a plurality of images from an environment of the user of the wearable apparatus.  For example, glasses 130 may be fitted with image\nsensor system 220, as shown in FIG. 38A.  Alternatively, as shown in FIG. 38B, capturing unit 3804 may be fitted with image sensor system 220.  Capturing unit 3804 may be located, for example, on an exterior surface of the clothing 750 of user 100 and as\nsuch may be positioned so that image sensor system 220 may capture images from the environment of user 100.\nAt step 4104, the at least one processor may activate a projector included in the wearable apparatus based on a visual trigger appearing in a plurality of images.  In some embodiments, the activation may occur in response to a user input.  The\nuser input may include a voice command or an input associated with a depressible button or other input device of the wearable apparatus.  In some embodiments, the activation may be based at least one a visual trigger appearing in at least one of the\nplurality of images or based on at least an amount of time that an object remains present in the active field of view of the at least one image sensor.  In some embodiments, the activation may be based on a determination that images captured in step 3802\ninclude only partial view of a text and/or an object of interest.  For example, a determination that images captured in step 3802 include only partial view of a text may be made based on a detection of a partial view of letters at the edge of the images. In another example, a determination that images captured in step 3802 include only partial view of a text may be made based on a natural language processing of textual information obtained by analyzing the images using OCR algorithms.  In a third\nexample, a determination that images captured in step 3802 include only partial view of an object may be made by analyzing the images with a classifier and/or a neural network trained to identify and/or detect partial views of objects.\nAt step 4106, light projector 3802 may emit a light pattern to visually indicate to the user of the wearable apparatus an active field of view of the image sensor.  For example, the light pattern may include a circular or elliptical outline\npattern, a rectangular outline pattern, a square outline pattern, a dashed pattern, one or more illuminated dots, a rectangular solid illumination pattern, a square solid illumination pattern, a circular solid illumination pattern, an elliptical solid\nillumination pattern, two parallel horizontal lines, or two parallel vertical lines.  In some embodiments, the light pattern may coincide with one or more borders of the active field of view of the at least one image sensor.  In some embodiments, the\nlight pattern may also be included with the active field of view of the at least one image sensor or the light pattern may substantially overlap with the active field of view of the at least one image sensor.\nThe foregoing description has been presented for purposes of illustration.  It is not exhaustive and is not limited to the precise forms or embodiments disclosed.  Modifications and adaptations will be apparent to those skilled in the art from\nconsideration of the specification and practice of the disclosed embodiments.  Additionally, although aspects of the disclosed embodiments are described as being stored in memory, one skilled in the art will appreciate that these aspects can also be\nstored on other types of computer readable media, such as secondary storage devices, for example, hard disks or CD ROM, or other forms of RAM or ROM, USB media, DVD, Blu-ray, Ultra HD Blu-ray, or other optical drive media.\nComputer programs based on the written description and disclosed methods are within the skill of an experienced developer.  The various programs or program modules can be created using any of the techniques known to one skilled in the art or can\nbe designed in connection with existing software.  For example, program sections or program modules can be designed in or by means of .Net Framework, .Net Compact Framework (and related languages, such as Visual Basic, C, etc.), Java, C++, Objective-C,\nHTML, HTML/AJAX combinations, XML, or HTML with included Java applets.\nMoreover, while illustrative embodiments have been described herein, the scope of any and all embodiments having equivalent elements, modifications, omissions, combinations (e.g., of aspects across various embodiments), adaptations and/or\nalterations as would be appreciated by those skilled in the art based on the present disclosure.  The limitations in the claims are to be interpreted broadly based on the language employed in the claims and not limited to examples described in the\npresent specification or during the prosecution of the application.  The examples are to be construed as non-exclusive.  Furthermore, the steps of the disclosed methods may be modified in any manner, including by reordering steps and/or inserting or\ndeleting steps.  It is intended, therefore, that the specification and examples be considered as illustrative only, with a true scope and spirit being indicated by the following claims and their full scope of equivalents.", "application_number": "15793370", "abstract": " The present disclosure relates to systems and methods for selecting an\n     action based on a detected person. In one implementation, a wearable\n     apparatus may include a wearable image sensor configured to capture a\n     plurality of images from the environment of the user of the wearable\n     apparatus and at least one processing device. The at least one processing\n     device may be programmed to analyze at least one of the plurality of\n     images to detect the person; analyze at least one of the plurality of\n     images to identify an attribute of the detected person; select at least\n     one category for the detected person based on the identified attribute;\n     select at least one action based on the at least one category; and cause\n     the at least one selected action to be executed.\n", "citations": ["7395507", "20050259035", "20120290950", "20140347265", "20150201181", "20160055499", "20160246378"], "related": ["62413103", "62418296", "62418300", "62439899", "62546141"]}, {"id": "20180167404", "patent_code": "10375096", "patent_name": "Filtering onion routing traffic from malicious domain generation algorithm\n     (DGA)-based traffic classification", "year": "2019", "inventor_and_country_data": " Inventors: \nMachlica; Lukas (Prague, CZ), Vejman; Martin (Litomysl, CZ)  ", "description": "<BR><BR>TECHNICAL FIELD\nThe present disclosure relates generally to computer networks, and, more particularly, to filtering onion routing traffic from malicious domain generation algorithm (DGA)-based traffic classification.\n<BR><BR>BACKGROUND\nOne type of network attack that is of particular concern in the context of computer networks is a Denial of Service (DoS) attack.  In general, the goal of a DoS attack is to prevent legitimate use of the services available on the network.  For\nexample, a DoS jamming attack may artificially introduce interference into the network, thereby causing collisions with legitimate traffic and preventing message decoding.  In another example, a DoS attack may attempt to overwhelm the network's resources\nby flooding the network with requests, to prevent legitimate requests from being processed.  A DoS attack may also be distributed, to conceal the presence of the attack.  For example, a distributed DoS (DDoS) attack may involve multiple attackers sending\nmalicious requests, making it more difficult to distinguish when an attack is underway.  When viewed in isolation, a particular one of such a request may not appear to be malicious.  However, in the aggregate, the requests may overload a resource,\nthereby impacting legitimate requests sent to the resource.\nBotnets represent one way in which a DDoS attack may be launched against a network.  In a botnet, a subset of the network devices may be infected with malicious software, thereby allowing the devices in the botnet to be controlled by a single\nmaster.  Using this control, the master can then coordinate the attack against a given network resource. <BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nThe embodiments herein may be better understood by referring to the following description in conjunction with the accompanying drawings in which like reference numerals indicate identically or functionally similar elements, of which:\nFIGS. 1A-1B illustrate an example communication network;\nFIG. 2 illustrates an example network device/node; and\nFIG. 3 illustrates an example of a network device capturing traffic data;\nFIGS. 4A-4B illustrate an example of a network device capturing domain information;\nFIGS. 5A-5D illustrate example histograms of character distributions from domain names associated with non-onion routing traffic;\nFIGS. 6A-6D illustrate example histograms of character distributions from domain names associated with onion routing traffic;\nFIGS. 7A-7D illustrate example probability distribution functions (PDFs) for onion routing and non-onion routing traffic with a restricted number of assessed domain names;\nFIGS. 8A-8D illustrate example PDFs for onion routing and non-onion routing traffic with no restriction on the number of assessed domain names; and\nFIG. 9 illustrates an example simplified procedure for filtering onion routing traffic from domain generation algorithm (DGA)-based traffic classification.\n<BR><BR>DESCRIPTION OF EXAMPLE EMBODIMENTS\n<BR><BR>Overview\nAccording to one or more embodiments of the disclosure, a device in a network receives domain information from a plurality of traffic flows in the network.  The device identifies a particular address from the plurality of traffic flows as part\nof an onion routing system based on the received domain information.  The device distinguishes the particular address during analysis of the traffic flows by a traffic flow analyzer that includes a domain generation algorithm (DGA)-based traffic\nclassifier.  The device detects a malicious traffic flow from among the plurality of traffic flows using the traffic flow analyzer.  The device causes performance of a mitigation action based on the detected malicious traffic flow.\n<BR><BR>Description\nAccording to one or more embodiments of the disclosure, a device in a network receives traffic data regarding one or more traffic flows in the network.  The device applies a machine learning classifier to the traffic data.  The device determines\na priority for the traffic data based in part on an output of the machine learning classifier.  The output of the machine learning classifier comprises a probability of the traffic data belonging to a particular class.  The device stores the traffic data\nfor a period of time that is a function of the determined priority for the traffic data.\n<BR><BR>Description\nA computer network is a geographically distributed collection of nodes interconnected by communication links and segments for transporting data between end nodes, such as personal computers and workstations, or other devices, such as sensors,\netc. Many types of networks are available, with the types ranging from local area networks (LANs) to wide area networks (WANs).  LANs typically connect the nodes over dedicated private communications links located in the same general physical location,\nsuch as a building or campus.  WANs, on the other hand, typically connect geographically dispersed nodes over long-distance communications links, such as common carrier telephone lines, optical lightpaths, synchronous optical networks (SONET), or\nsynchronous digital hierarchy (SDH) links, or Powerline Communications (PLC) such as IEEE 61334, IEEE P1901.2, and others.  The Internet is an example of a WAN that connects disparate networks throughout the world, providing global communication between\nnodes on various networks.  The nodes typically communicate over the network by exchanging discrete frames or packets of data according to predefined protocols, such as the Transmission Control Protocol/Internet Protocol (TCP/IP).  In this context, a\nprotocol consists of a set of rules defining how the nodes interact with each other.  Computer networks may further be interconnected by an intermediate network node, such as a router, to extend the effective \"size\" of each network.\nSmart object networks, such as sensor networks, in particular, are a specific type of network having spatially distributed autonomous devices such as sensors, actuators, etc., that cooperatively monitor physical or environmental conditions at\ndifferent locations, such as, e.g., energy/power consumption, resource consumption (e.g., water/gas/etc. for advanced metering infrastructure or \"AMI\" applications) temperature, pressure, vibration, sound, radiation, motion, pollutants, etc. Other types\nof smart objects include actuators, e.g., responsible for turning on/off an engine or perform any other actions.  Sensor networks, a type of smart object network, are typically shared-media networks, such as wireless networks.  That is, in addition to\none or more sensors, each sensor device (node) in a sensor network may generally be equipped with a radio transceiver or other communication port, a microcontroller, and an energy source, such as a battery.  Often, smart object networks are considered\nfield area networks (FANs), neighborhood area networks (NANs), personal area networks (PANs), etc. Generally, size and cost constraints on smart object nodes (e.g., sensors) result in corresponding constraints on resources such as energy, memory,\ncomputational speed and bandwidth.\nFIG. 1A is a schematic block diagram of an example computer network 100 illustratively comprising nodes/devices, such as a plurality of routers/devices interconnected by links or networks, as shown.  For example, customer edge (CE) routers 110\nmay be interconnected with provider edge (PE) routers 120 (e.g., PE-1, PE-2, and PE-3) in order to communicate across a core network, such as an illustrative network backbone 130.  For example, routers 110, 120 may be interconnected by the public\nInternet, a multiprotocol label switching (MPLS) virtual private network (VPN), or the like.  Data packets 140 (e.g., traffic/messages) may be exchanged among the nodes/devices of the computer network 100 over links using predefined network communication\nprotocols such as the Transmission Control Protocol/Internet Protocol (TCP/IP), User Datagram Protocol (UDP), Asynchronous Transfer Mode (ATM) protocol, Frame Relay protocol, or any other suitable protocol.  Those skilled in the art will understand that\nany number of nodes, devices, links, etc. may be used in the computer network, and that the view shown herein is for simplicity.\nIn some implementations, a router or a set of routers may be connected to a private network (e.g., dedicated leased lines, an optical network, etc.) or a virtual private network (VPN), such as an MPLS VPN, thanks to a carrier network, via one or\nmore links exhibiting very different network and service level agreement characteristics.  For the sake of illustration, a given customer site may fall under any of the following categories:\n1.) Site Type A: a site connected to the network (e.g., via a private or VPN link) using a single CE router and a single link, with potentially a backup link (e.g., a 3G/4G/LTE backup connection).  For example, a particular CE router 110 shown\nin network 100 may support a given customer site, potentially also with a backup link, such as a wireless connection.\n2.) Site Type B: a site connected to the network using two MPLS VPN links (e.g., from different service providers), with potentially a backup link (e.g., a 3G/4G/LTE connection).  A site of type B may itself be of different types:\n2a.) Site Type B1: a site connected to the network using two MPLS VPN links (e.g., from different service providers), with potentially a backup link (e.g., a 3G/4G/LTE connection).\n2b.) Site Type B2: a site connected to the network using one MPLS VPN link and one link connected to the public Internet, with potentially a backup link (e.g., a 3G/4G/LTE connection).  For example, a particular customer site may be connected to\nnetwork 100 via PE-3 and via a separate Internet connection, potentially also with a wireless backup link.\n2c.) Site Type B3: a site connected to the network using two links connected to the public Internet, with potentially a backup link (e.g., a 3G/4G/LTE connection).\nNotably, MPLS VPN links are usually tied to a committed service level agreement, whereas Internet links may either have no service level agreement at all or a loose service level agreement (e.g., a \"Gold Package\" Internet service connection that\nguarantees a certain level of performance to a customer site).\n3.) Site Type C: a site of type B (e.g., types B1, B2 or B3) but with more than one CE router (e.g., a first CE router connected to one link while a second CE router is connected to the other link), and potentially a backup link (e.g., a\nwireless 3G/4G/LTE backup link).  For example, a particular customer site may include a first CE router 110 connected to PE-2 and a second CE router 110 connected to PE-3.\nFIG. 1B illustrates an example of network 100 in greater detail, according to various embodiments.  As shown, network backbone 130 may provide connectivity between devices located in different geographical areas and/or different types of local\nnetworks.  For example, network 100 may comprise local networks 160, 162 that include devices/nodes 10-16 and devices/nodes 18-20, respectively, as well as a data center/cloud environment 150 that includes servers 152-154.  Notably, local networks\n160-162 and data center/cloud environment 150 may be located in different geographic locations.\nServers 152-154 may include, in various embodiments, a network management server (NMS), a dynamic host configuration protocol (DHCP) server, a constrained application protocol (CoAP) server, an outage management system (OMS), an application\npolicy infrastructure controller (APIC), an application server, etc. As would be appreciated, network 100 may include any number of local networks, data centers, cloud environments, devices/nodes, servers, etc.\nThe techniques herein may also be applied to other network topologies and configurations.  For example, the techniques herein may be applied to peering points with high-speed links, data centers, etc. Further, in various embodiments, network 100\nmay include one or more mesh networks, such as an Internet of Things network.  Loosely, the term \"Internet of Things\" or \"IoT\" refers to uniquely identifiable objects/things and their virtual representations in a network-based architecture.  In\nparticular, the next frontier in the evolution of the Internet is the ability to connect more than just computers and communications devices, but rather the ability to connect \"objects\" in general, such as lights, appliances, vehicles, heating,\nventilating, and air-conditioning (HVAC), windows and window shades and blinds, doors, locks, etc. The \"Internet of Things\" thus generally refers to the interconnection of objects (e.g., smart objects), such as sensors and actuators, over a computer\nnetwork (e.g., via IP), which may be the public Internet or a private network.\nNotably, shared-media mesh networks, such as wireless networks, etc., are often on what is referred to as Low-Power and Lossy Networks (LLNs), which are a class of network in which both the routers and their interconnect are constrained.  In\nparticular, LLN routers typically operate with highly constrained resources, e.g., processing power, memory, and/or energy (battery), and their interconnections are characterized by, illustratively, high loss rates, low data rates, and/or instability. \nLLNs are comprised of anything from a few dozen to thousands or even millions of LLN routers, and support point-to-point traffic (e.g., between devices inside the LLN), point-to-multipoint traffic (e.g., from a central control point such at the root node\nto a subset of devices inside the LLN), and multipoint-to-point traffic (e.g., from devices inside the LLN towards a central control point).  Often, an IoT network is implemented with an LLN-like architecture.  For example, as shown, local network 160\nmay be an LLN in which CE-2 operates as a root node for nodes/devices 10-16 in the local mesh, in some embodiments.\nFIG. 2 is a schematic block diagram of an example node/device 200 that may be used with one or more embodiments described herein, e.g., as any of the computing devices shown in FIGS. 1A-1B, particularly the PE routers 120, CE routers 110,\nnodes/device 10-20, servers 152-154 (e.g., a network controller located in a data center, etc.), any other computing device that supports the operations of network 100 (e.g., switches, etc.), or any of the other devices referenced below.  The device 200\nmay also be any other suitable type of device depending upon the type of network architecture in place, such as IoT nodes, etc. Device 200 comprises one or more network interfaces 210, one or more processors 220, and a memory 240 interconnected by a\nsystem bus 250, and is powered by a power supply 260.\nThe network interfaces 210 include the mechanical, electrical, and signaling circuitry for communicating data over physical links coupled to the network 100.  The network interfaces may be configured to transmit and/or receive data using a\nvariety of different communication protocols.  Notably, a physical network interface 210 may also be used to implement one or more virtual network interfaces, such as for virtual private network (VPN) access, known to those skilled in the art.\nThe memory 240 comprises a plurality of storage locations that are addressable by the processor(s) 220 and the network interfaces 210 for storing software programs and data structures associated with the embodiments described herein.  The\nprocessor 220 may comprise necessary elements or logic adapted to execute the software programs and manipulate the data structures 245.  An operating system 242 (e.g., the Internetworking Operating System, or IOS.RTM., of Cisco Systems, Inc., another\noperating system, etc.), portions of which are typically resident in memory 240 and executed by the processor(s), functionally organizes the node by, inter alia, invoking network operations in support of software processors and/or services executing on\nthe device.  These software processors and/or services may comprise classifier process 244 and/or an onion routing identification process 248.\nIt will be apparent to those skilled in the art that other processor and memory types, including various computer-readable media, may be used to store and execute program instructions pertaining to the techniques described herein.  Also, while\nthe description illustrates various processes, it is expressly contemplated that various processes may be embodied as modules configured to operate in accordance with the techniques herein (e.g., according to the functionality of a similar process). \nFurther, while processes may be shown and/or described separately, those skilled in the art will appreciate that processes may be routines or modules within other processes.\nIn general, classifier process 244 may execute one or more machine learning-based classifiers to classify traffic data regarding traffic in the network for any number of purposes.  In one embodiment, classifier process 244 may assess captured\ntraffic data to determine whether a given traffic flow or set of flows are caused by malware in the network (e.g., whether the traffic flow is considered malicious).  Example forms of traffic that can be caused by malware may include, but are not limited\nto, traffic flows reporting exfiltrated data to a remote entity, spyware or ransomware-related flows, command and control (C2) traffic that oversees the operation of the deployed malware, traffic that is part of a network attack, such as a zero day\nattack or denial of service (DoS) attack, combinations thereof, or the like.  In further embodiments, classifier process 244 may classify the gathered traffic data to detect other anomalous behaviors (e.g., malfunctioning devices, misconfigured devices,\netc.), traffic pattern changes (e.g., a group of hosts begin sending significantly more or less traffic), or the like.\nClassifier process 244 may employ any number of machine learning techniques, to classify the gathered traffic data.  In general, machine learning is concerned with the design and the development of techniques that receive empirical data as input\n(e.g., traffic data regarding traffic in the network) and recognize complex patterns in the input data.  For example, some machine learning techniques use an underlying model M, whose parameters are optimized for minimizing the cost function associated\nto M, given the input data.  For instance, in the context of classification, the model M may be a straight line that separates the data into two classes (e.g., labels) such that M=a*x+b*y+c and the cost function is a function of the number of\nmisclassified points.  The learning process then operates by adjusting the parameters a,b,c such that the number of misclassified points is minimal.  After this optimization/learning phase, classifier process 244 can use the model M to classify new data\npoints, such as information regarding new traffic flows in the network.  Often, M is a statistical model, and the cost function is inversely proportional to the likelihood of M, given the input data.\nIn various embodiments, classifier process 244 may employ one or more supervised, unsupervised, or semi-supervised machine learning models.  Generally, supervised learning entails the use of a training set of data, as noted above, that is used\nto train the model to apply labels to the input data.  For example, the training data may include sample traffic data that is \"normal,\" or \"malware-generated.\" On the other end of the spectrum are unsupervised techniques that do not require a training\nset of labels.  Notably, while a supervised learning model may look for previously seen attack patterns that have been labeled as such, an unsupervised model may instead look to whether there are sudden changes in the behavior of the network traffic. \nSemi-supervised learning models take a middle ground approach that uses a greatly reduced set of labeled training data.\nExample machine learning techniques that classifier process 244 can employ may include, but are not limited to, nearest neighbor (NN) techniques (e.g., k-NN models, replicator NN models, etc.), statistical techniques (e.g., Bayesian networks,\netc.), clustering techniques (e.g., k-means, mean-shift, etc.), neural networks (e.g., reservoir networks, artificial neural networks, etc.), support vector machines (SVMs), logistic or other regression, Markov models or chains, principal component\nanalysis (PCA) (e.g., for linear models), multi-layer perceptron (MLP) ANNs (e.g., for non-linear models), replicating reservoir networks (e.g., for non-linear models, typically for time series), or the like.\nThe performance of a machine learning model can be evaluated in a number of ways based on the number of true positives, false positives, true negatives, and/or false negatives of the model.  For example, the false positives of the model may\nrefer to the number of traffic flows that are incorrectly classified as malware-generated, anomalous, etc. Conversely, the false negatives of the model may refer to the number of traffic flows that the model incorrectly classifies as normal, when\nactually malware-generated, anomalous, etc. True negatives and positives may refer to the number of traffic flows that the model correctly classifies as normal or malware-generated, etc., respectively.  Related to these measurements are the concepts of\nrecall and precision.  Generally, recall refers to the ratio of true positives to the sum of true positives and false negatives, which quantifies the sensitivity of the model.  Similarly, precision refers to the ratio of true positives the sum of true\nand false positives.\nIn some cases, classifier process 244 may assess the captured traffic data on a per-flow basis.  In other embodiments, classifier process 244 may assess traffic data for a plurality of traffic flows based on any number of different conditions. \nFor example, traffic flows may be grouped based on their sources, destinations, temporal characteristics (e.g., flows that occur around the same time, etc.), combinations thereof, or based on any other set of flow characteristics.\nOnion routing identification process 248, as described in greater detail below, may operate in conjunction with classifier process 244.  For example, onion routing identification process 248 may identify whether a particular network address and\nits corresponding traffic flows are associated with an onion routing system/network.  In other words, classifier process 244 and onion routing identification process 248 may function together as part of a traffic flow analyzer that assesses traffic flows\nin the network (e.g., to detect flows that are potentially malicious/related to malware).  In turn, device 200 may cause the performance of any number of mitigation actions with respect to a detected malicious flow, either directly or indirectly, such as\ndropping/blocking the flow, generating an alert, or the like.\nAs shown in FIG. 3, various mechanisms can be leveraged to capture information about traffic in a network.  For example, consider the case in which host node 10 initiates a traffic flow with remote server 154 that includes any number of packets\n302.  Any number of networking devices along the path of the flow may analyze and assess packet 302, to capture traffic data regarding the traffic flow.  For example, as shown, consider the case of edge router CE-2 through which the traffic between node\n10 and server 154 flows.\nIn some embodiments, a networking device may analyze packet headers, to capture information about the traffic flow.  For example, router CE-2 may capture the source address and/or port of host node 10, the destination address and/or port of\nserver 154, the protocol(s) used by packet 302, or other header information by analyzing the header of a packet 302.  In further embodiments, the device may also assess the payload of the packet to capture information about the traffic flow.  For\nexample, router CE-2 or another device may perform deep packet inspection (DPI) on one or more of packets 302, to assess the contents of the packet.  Doing so may, for example, yield additional information that can be used to determine the application\nassociated with the traffic flow (e.g., packets 302 were sent by a web browser of node 10, packets 302 were sent by a videoconferencing application, etc.).\nThe networking device that captures the traffic data may also compute any number of statistics or metrics regarding the traffic flow.  For example, CE-2 may determine the start time, end time, duration, packet size(s), the distribution of bytes\nwithin a flow, etc., associated with the traffic flow by observing packets 302.  In turn, the capturing device may itself perform analysis of the traffic flows (e.g., to detect malicious/malware-related flows) or provide the captured traffic data to\nanother device in the network that performs such an analysis.\nAs noted above, botnets represent a security concern for network administrators.  Once a client device has been infected with malware for the botnet, it may communicate with a command and control (C&C) server which sends control commands to the\ninfected device.  If the address of the C&C server is hardcoded into the malware itself, preventing operation of the botnet becomes a trivial task.  Notably, all an administrator would need to do is block the address of the C&C server, to defeat control\nover the infected client device.  However, many modern forms of malware do not use hardcoded addresses, but instead rely on domain generation algorithms (DGAs), to elude detection.  Similar mechanisms are also used by other forms of malware, such as\nthose that exfiltrate data from a network and the like.\nIn general, a DGA is a mechanism that generates a set of domain names based on some criteria, such as the time of day, month, year, etc. For example, a DGA may generate the domain names {a.com, b.com, a.b.com, .  . . } on one day and the domains\n{bc.com, b.info, .  . . } on the next day.  In turn, the infected client device may perform a lookup of some or all of the generated domain names, to obtain the IP address of the C&C server.\nFIGS. 4A-4B illustrate an example of a domain name system (DNS) lookup.  As shown, assume that client device A has been infected with malware that uses a DGA to communicate with a corresponding C&C server, endpoint device C shown.  During\noperation, both infected client device A and endpoint C may execute a DGA to generate a corresponding set of domain names.  In turn, endpoint C or the entity associated therewith may register one or more of the generated domain names with a DNS service\n(e.g., to associate the IP address of endpoint C with a generated domain name).  Infected client device A may then send one or more DNS requests 406 to a DNS service provided by DNS server(s) 402, to look up the IP address associated with one or more of\nthe generated domain names, as shown in FIG. 4A.  This allows the C&C server to constantly switch IP addresses to avoid blocking mechanism and still retain control over the infected client devices.  In turn, as shown in FIG. 4B, CE-2 may capture this\ndomain information by analyzing DNS response 406 and/or by assessing the ensuing traffic directly.\nAccording to various embodiments, a flow analyzer may employ the use of one or more DGA-based classifiers, to identify traffic flows that are potentially malicious/malware-related.  Such a classifier may, for example, assess the domain name\ninformation for a particular flow, to label the flow as either \"benign\" or \"malicious.\" The traffic flow analyzer may further employ other classifiers (e.g., based on other traffic characteristics) and/or rule-based approaches, to make a final\ndetermination about the flow.\nWhile DGA-based classification may be a powerful way to identify traffic flows that are associated with malware, not all DGA-like domain names are necessarily related to malware.  One example of this is in the case of onion routing systems, most\nnotably The Onion Router (TOR) network, although other onion routing systems are also in use.  In general, onion routing systems afford anonymous communications by implementing the following features: 1.) communications are wrapped in multiple layers of\nencryption, 2.) each node along the path is only able to strip off the topmost layer of encryption to identify the next destination, 3.) each node along the path is unable to determine whether the prior hop was the originator of the communication, and\n4.) each node along the path is further unable to determine whether the next hop is the final destination.\nTo further obfuscate communications in an onion routing system, it has been found that the domain names observed in TOR-related traffic are also generated automatically and never registered.  This is very similar to the DGAs used by\nmalware-related traffic flows and potentially acting as a source of false positives by a traffic flow analyzer that uses a DGA-based classifier.  For example, as shown in FIGS. 4A-4B, traffic flows associated with onion routing system 408 may exhibit\ndomain name information that resembles DGA-generated domain names that are associated with malware.\nFiltering Onion Routing Traffic from Malicious DGA-Based Traffic Classification\nThe techniques herein allow for the identification of addresses/servers that are dedicated to onion routing systems in a way that is data driven and does not rely on static lists of addresses.  In some aspects, this identification can be used to\nreduce false positives by a traffic analyzer that uses a DGA-based classifier to detect malware-related traffic flows.\nSpecifically, according to one or more embodiments of the disclosure as described in detail below, a device in a network receives domain information from a plurality of traffic flows in the network.  The device identifies a particular address\nfrom the plurality of traffic flows as part of an onion routing system based on the received domain information.  The device distinguishes the particular address during analysis of the traffic flows by a traffic flow analyzer that includes a domain\ngeneration algorithm (DGA)-based traffic classifier.  The device detects a malicious traffic flow from among the plurality of traffic flows using the traffic flow analyzer.  The device causes performance of a mitigation action based on the detected\nmalicious traffic flow.\nIllustratively, the techniques described herein may be performed by hardware, software, and/or firmware, such as in accordance with classifier process 244 and onion routing identification process 248, which may include computer executable\ninstructions executed by the processor 220 to perform functions relating to the techniques described herein.\nOperationally, the techniques herein are able to identify IP addresses used by an onion routing system based on the analysis of traffic associated with a given server IP address.  The techniques rely on the observation that one server IP,\ndedicated to use by the onion routing system, primarily operates for onion routing purposes.  In other words, no other domains, or only a very small percentage given all of the observed DGA-generated domains, are registered on such servers.  Conversely,\nmalware typically spreads the registration of its DGAs over many different servers, to avoid blacklisting of the server IPs.  Moreover, other malicious or legitimate non-DGA domains are often registered at these servers as well.\nA further key observation is that onion routing-related domains have uniform probability distribution of characters given one server IP, that can be computed if sufficient amount of observed domains is available.  This is not true for\ndistribution of characters in domain names related to non-onion routing servers.  Since each connection to an onion routing-related server uses a unique domain name, the number of different domains observed at a specific onion routing dedicated server is\nhigh.\nAccording to various embodiments, the traffic analyzer may identify an onion routing-related address/server by performing any or all of the following: 1.  Gather all domains related to IP addresses for which a DGA was detected.  2.  Given an\nobserved domain related to a server IP in question, update a histogram for this IP with the number of occurrences of different characters in the domain name.  All connections in which the domain is given by IP can also be ignored, since these are of no\nimportance.  3.  After a sufficient amount of domains to a specific IP has been collected, compute the distance from the uniform distribution (e.g. entropy or flatness of the distribution) based on the histogram of character occurrences (i.e., the\nestimated probability distribution of characters related to one server IP).  4.  If the distance is lower (or entropy is higher) than a pre-specified threshold then add the server IP to the set of onion routing-related IPs.\nInstead of focusing on each individual domain separately, all of the domains related to one server IP are investigated at once, in one embodiment.  Note that the histogram for one IP is updated each time a domain is observed, i.e., there is no\nneed to store all previously processed domains in memory.  Further embodiments provide for the storage and analysis of the individual domains.\nFIGS. 5A-5D and 6A-6D illustrate example histograms of character distributions from domain names associated with non-onion routing traffic and onion routing traffic, respectively.  Each histogram shown also depicts a different scenario.  More\nspecifically, for the two types of traffic, example character distributions are depicted for 5, 10, 100, and 300 domains for a single server IP address.\nMore specifically, FIG. 5A illustrates a histogram 500 of the character counts of five domain names for a non-onion routing related IP address.  FIG. 5B illustrates a histogram 510 of the character counts of ten domain names for the address. \nFIG. 5C illustrates a histogram 520 of the character counts of one hundred domains for the address.  Finally, FIG. 5D illustrates a histogram 530 of the character counts of three hundred domains for the non-onion routing-related address.\nConversely, FIG. 6A illustrates a histogram 600 of the character counts of five domain names for an onion routing-related IP address.  FIG. 6B illustrates a histogram 610 of the character counts of ten domain names for the address.  FIG. 6C\nillustrates a histogram 620 of the character counts of one hundred domains for the address.  Finally, FIG. 6D illustrates a histogram 630 of the character counts of three hundred domains for the onion routing-related address.\nFrom FIGS. 5A-5D, one observation is that for non-onion routing related addresses, peaks begin to form at the position of vowels as the number of domains increases.  This may be due to the server IP addresses hosting not only DGA-generated\ndomains, but also many other non-DGA-generated domains that are formed from ordinary words.  In addition, the number of occurrences of non-alphabetic characters is also very low (e.g., numbers, symbols, etc.).\nFrom FIGS. 6A-6D, another observation is that the distribution of character counts for domains associated with the onion routing related IP appears to converse to a uniform distribution, as the number of domains increases.  One can see that the\nalphabetic and numerical characters have a comparable number of occurrences even for low number of domains observed for the server IP address.  Still, this fact alone does not have to be indicative enough for the server IPs in question.\nThus, by calculating the distance between the actual character count distribution and a uniform distribution, the traffic analyzer may be able to identify those IP addresses that are associated with an onion routing system/network.  In turn,\nthis information can be used to reduce false positives by the DGA-based traffic classifier.  In some embodiments, for example, the traffic analyzer may simply bypass analysis of a traffic flow associated with an IP address that has been identified as\npart of an onion routing system.  In further embodiments, the analyzer may still analyze such a flow, but filter out any positive results for flows associated with the address.  In yet another embodiment, the above techniques can be used to train the\nDGA-based classifier to label a given traffic flow as benign, malicious, or onion routing-related.\nSeveral experiments were conducted using the techniques herein and onion routing-related servers/addresses were identified with almost 100% precision.  It was also observed that the precision is proportional to number of observed domains for\neach server IP.\nIn order to demonstrate the idea on real examples, a dataset of 612 server IPs and related domains were collected.  At all of these server addresses, at least one domain was detected as a DGA-generated domain and at least five domains were\nregistered and observed at each IP.  If necessary, however, all server IPs could be monitored, since it is possible that a DGA-generated domain could be missed and, consequently, also miss a possible candidate for an onion routing-related server/address. Based on each of the histograms of domain character counts for each of the IP addresses in question, the Shannon entropy was computed.\nFIGS. 7A-7D illustrate example probability distribution functions (PDFs) for the tested onion routing and non-onion routing traffic.  The tested dataset included 197 regular IP addresses and 415 onion routing-related addresses.  Plot 700 in FIG.\n7A illustrates the case where the count of domain names for each address is in the range of [5, 10).  Plot 710 in FIG. 7B illustrates the case where the count of domain names for each address is in the range of [10, 20).  Similarly, plot 720 in FIG. 7C\nillustrates the case where the count of domain names for each address is in the range of [20, 50).  Finally, plot 730 in FIG. 7D illustrates the case in which each address has at least fifty associated domains.\nThe results show, that the techniques herein are able to identify an onion routing-related server/address with high precision, even for low number of observed domains, but at the cost of recall.  This is the case for 5-10 observed domains, as\nshown in FIG. 7A.  With increasing number of observed domains, however, the recall increases.  In fact, the overlap between entropies computed for regular and onion routing-related addresses is very small already for 10 to 20 observed domains, as shown\nin FIG. 7B.  With more than 20 observed domains, as shown in FIGS. 7C-7D, 100% recall and 100% precision was achieved.\nA further experiment was also conducted to remove the upper limit restriction on the observed domains per address/server.  FIGS. 8A-8D illustrate the resulting PDFs.  More specifically, plot 800 in FIG. 8A illustrates the case in which each\naddress had at least five observed domains, with no upper limit on the number of domains.  Plot 810 in FIG. 8B illustrates the case in which each address had at least ten domains with no upper limit restriction.  Similarly, plot 820 in FIG. 8C\nillustrates the case in which each address had at least twenty domains with no upper limit restriction.  Finally, plot 830 in FIG. 8D illustrates the case in which each address had at least fifty observed domains with no upper limit.\nAgain, it can be seen that a very high precision can be achieved even for the case in which only five domains per address were observed (e.g., for an entropy threshold higher than 4.8).  Additionally, a high recall for the classifier is also\nachievable when at least ten domains are observed.\nFIG. 9 illustrates an example simplified procedure for filtering onion routing traffic from DGA-based traffic classification, in accordance with one or more embodiments described herein.  For example, a non-generic, specifically configured\ndevice (e.g., device 200) may perform procedure 900 by executing stored instructions (e.g., processes 244, 248).  The procedure 900 may start at step 905, and continues to step 910, where, as described in greater detail above, the device may receive\ndomain information (e.g., domain names, etc.) from a plurality of traffic flows in the network.  In some cases, the device may capture the information as the traffic flows through the device.  In other embodiments, the device may receive the information\nfrom one or more other devices in the network that capture this information.\nAt step 910, as detailed above, the device may identify an address/server as part of an onion routing system based on the received domain information.  For example, the device may determine whether a particular address is part of the TOR\nnetwork.  In some embodiments, the device may generate character counts for the domains associated with the address and assess the distribution of such counts, to determine whether the address is part of an onion routing system.  For example, the device\nmay compute a statistical distance between the distribution of character counts to that of a uniform distribution and compare the computed distance to a threshold.  As discovered, the character counts of domain names at a specific server IP address may\nbe more uniformly distributed for onion routing-related domains than for those that are not related to onion routing.\nAt step 915, the device may distinguish the address during analysis of the traffic flows by a traffic flow analyzer, as described in greater detail above.  In one embodiment, the analysis may include assessing the traffic using a DGA-based\ntraffic classifier.  The device may, for example, exclude traffic for the address from analysis by the classifier, may adjust or filter a result of the classifier if traffic associated with the address is classified, or may even use a modified classifier\nthat has a separate label for onion routing-related traffic (e.g., benign, malicious/malware, and onion routing-related).\nAt step 920, as detailed above, the device may determine that one of the traffic flows is malicious.  For example, the DGA-based classifier may assess the domain for the traffic and determine that the traffic flow is associated with malware.  By\ndistinguishing onion routing-related addresses (e.g., as in step 915) during the analysis of the traffic flows, false positives are reduced (e.g., to ensure that the malware-related flow is actually related to malware).\nAt step 925, the device may cause the performance of a mitigation action based on the malicious traffic flow, as described in greater detail above.  In some cases, the mitigation action may entail blocking the traffic flow or multiple traffic\nflows.  In other cases, the mitigation action may entail generating an alert regarding the flow, such as to an administrator or to a supervisory device in the network.  Procedure 900 then ends at step 930.\nIt should be noted that while certain steps within procedure 900 may be optional as described above, the steps shown in FIG. 9 are merely examples for illustration, and certain other steps may be included or excluded as desired.  Further, while\na particular order of the steps is shown, this ordering is merely illustrative, and any suitable arrangement of the steps may be utilized without departing from the scope of the embodiments herein.\nThe techniques described herein, therefore, allow for the dynamic detection of onion routing-related domains.  Such detection can be achieved without the use of static lists and does not require any additional information other than the server\nIP address and the domain name(s) associated with the address.  Further, the identified addresses that are onion routing-related can be used to reduce false positives in DGA-based malware detection systems.\nWhile there have been shown and described illustrative embodiments that provide for dynamically identifying onion routing-based addresses, it is to be understood that various other adaptations and modifications may be made within the spirit and\nscope of the embodiments herein.  For example, while certain embodiments are described herein with respect to using certain models for purposes of traffic analysis, the models are not limited as such and may be used for other functions, in other\nembodiments.  In addition, while certain protocols are shown, other suitable protocols may be used, accordingly.\nThe foregoing description has been directed to specific embodiments.  It will be apparent, however, that other variations and modifications may be made to the described embodiments, with the attainment of some or all of their advantages.  For\ninstance, it is expressly contemplated that the components and/or elements described herein can be implemented as software being stored on a tangible (non-transitory) computer-readable medium (e.g., disks/CDs/RAM/EEPROM/etc.) having program instructions\nexecuting on a computer, hardware, firmware, or a combination thereof.  Accordingly this description is to be taken only by way of example and not to otherwise limit the scope of the embodiments herein.  Therefore, it is the object of the appended claims\nto cover all such variations and modifications as come within the true spirit and scope of the embodiments herein.", "application_number": "15372580", "abstract": " In one embodiment, a device in a network receives domain information from\n     a plurality of traffic flows in the network. The device identifies a\n     particular address from the plurality of traffic flows as part of an\n     onion routing system based on the received domain information. The device\n     distinguishes the particular address during analysis of the traffic flows\n     by a traffic flow analyzer that includes a domain generation algorithm\n     (DGA)-based traffic classifier. The device detects a malicious traffic\n     flow from among the plurality of traffic flows using the traffic flow\n     analyzer. The device causes performance of a mitigation action based on\n     the detected malicious traffic flow.\n", "citations": ["8260914", "8516585", "20130191915", "20160381070"], "related": []}, {"id": "20180176240", "patent_code": "10375097", "patent_name": "Identifying self-signed certificates using HTTP access logs for malware\n     detection", "year": "2019", "inventor_and_country_data": " Inventors: \nKopp; Martin (Beroun, CZ), Grill; Martin (Prague, CZ), Kohout; Jan (Roudnice Nad Labem, CZ)  ", "description": "<BR><BR>TECHNICAL FIELD\nThe present disclosure relates generally to computer networks, and, more particularly, to identifying self-signed certificates using Hypertext Transfer Protocol (HTTP) access logs for malware detection.\n<BR><BR>BACKGROUND\nMalicious or otherwise undesirable network traffic can take many different forms.  For example, some traffic may seek to overwhelm a service by sending a large number of requests to the service.  Such attacks are also sometimes known as denial\nof service (DoS) attacks.  Other forms of malicious traffic may seek to exfiltrate sensitive information from a network, such as credit card numbers, trade secrets, and the like.  Typically, such traffic is generated by a client that has been infected\nwith malware.  Thus, a further type of malicious network traffic includes network traffic that propagates the malware itself.  Additionally, some network traffic may simply be deemed inappropriate for a particular network.\nIntrusion prevention systems (IPS), next-generation firewalls, and flow monitoring systems generally attempt to detect and block malicious traffic, whenever possible.  However, in recent years, the amount of encrypted and evasive network traffic\nhas increased considerably.  Notably, with the advent of transport layer security (TLS) and other security mechanisms, many websites are now encrypting even traditional webpage data.  This presents certain challenges to traditional security systems as\nthe contents of the traffic may not be available for analysis and pattern matching is generally inapplicable to encrypted traffic. <BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nThe embodiments herein may be better understood by referring to the following description in conjunction with the accompanying drawings in which like reference numerals indicate identically or functionally similar elements, of which:\nFIG. 1 illustrates an example communication network;\nFIG. 2 illustrates an example network device/node;\nFIGS. 3A-3D illustrate an example of the use of Hypertext Transfer Protocol (HTTP) access logs to identify domains that use self-signed certificates;\nFIG. 4 illustrates an example architecture to identify self-signed certificates for malware detection; and\nFIG. 5 illustrates an example simplified procedure for identifying self-signed certificates for malware detection.\n<BR><BR>DESCRIPTION OF EXAMPLE EMBODIMENTS\n<BR><BR>Overview\nAccording to one or more embodiments of the disclosure, a device in a network receives traffic information regarding one or more secure sessions in the network.  The device associates the one or more secure sessions with corresponding\ncertificate validation check traffic indicated by the received traffic information.  The device makes a self-signed certificate determination for an endpoint domain of a particular secure session based on whether the particular secure session is\nassociated with certificate validation check traffic.  The device causes the self-signed certificate determination for the endpoint domain to be used as input to a malware detector.\n<BR><BR>Description\nA computer network is a geographically distributed collection of nodes interconnected by communication links and segments for transporting data between end nodes, such as personal computers and workstations.  Many types of networks are\navailable, with the types ranging from local area networks (LANs) to wide area networks (WANs).  LANs typically connect the nodes over dedicated private communications links located in the same general physical location, such as a building or campus. \nWANs, on the other hand, typically connect geographically dispersed nodes over long-distance communications links, such as common carrier telephone lines, optical lightpaths, synchronous optical networks (SONET), or synchronous digital hierarchy (SDH)\nlinks.  The Internet is an example of a WAN that connects disparate networks throughout the world, providing global communication between nodes on various networks.  The nodes typically communicate over the network by exchanging discrete frames or\npackets of data according to predefined protocols, such as the Transmission Control Protocol/Internet Protocol (TCP/IP).  In this context, a protocol consists of a set of rules defining how the nodes interact with each other.  Computer networks may be\nfurther interconnected by an intermediate network node, such as a router, to extend the effective \"size\" of each network.\nFIG. 1 is a schematic block diagram of an example communication network 100 illustratively comprising nodes/devices 200, such as a plurality of routers/devices interconnected by links and/or networks, as shown.  For example, a customer edge (CE)\nrouter CE1 may interconnect nodes A and B on a local network 110 with a provider edge (PE) router PE1 of an access network 112.  In turn, access network 112 may provide local network 110 with connectivity to a core network 114, such as the Internet.\nThe various nodes/devices 200 may exchange data packets 106 (e.g., traffic/messages) via communication network 100 over links 102 using predefined network communication protocols such as the Transmission Control Protocol/Internet Protocol\n(TCP/IP), User Datagram Protocol (UDP), Asynchronous Transfer Mode (ATM) protocol, Frame Relay protocol, or any other suitable protocol.  Those skilled in the art will understand that any number of nodes, devices, links, etc. may be used in the computer\nnetwork, and that the view shown herein is for simplicity.  For example, node A in local network 110 may communicate with an endpoint node/device C (e.g., a remote server, etc.) via communication network 100.\nAs would be appreciated, links 102 may include any number of wired and/or wireless connections between devices.  For example, node A may communicate wirelessly using a WiFi.TM.  connection, CE1 and PE1 may communicate wirelessly using a cellular\nconnection or via a hardwired connection (e.g., DSL, etc.), etc. In addition, while certain devices are depicted in FIG. 1, the view shown is a simplified view of communication network 100.  In other words, communication network 100 may also include any\nnumber of intermediary networking devices such as, but not limited to, routers, switches, firewalls, etc., that are not shown.\nIn various embodiments, nodes/devices 200 may employ a secure communication mechanism, to encrypt and decrypt data packets 106.  For example, nodes/devices 200 shown may use a Transport Layer Security (TLS) mechanism, such as the HTTP Secure\n(HTTPS) protocol, to encrypt and decrypt data packets 106.\nFIG. 2 is a schematic block diagram of an example node/device 200 that may be used with one or more embodiments described herein, e.g., as any of the computing devices shown in FIG. 1, any other computing device that supports the operations of\nnetwork 100 (e.g., switches, etc.), or any of the other devices referenced below.  The device 200 may also be any other suitable type of device depending upon the type of network architecture in place.  As shown, device 200 comprises one or more network\ninterface(s) 210, one or more processor(s) 220, and a memory 240 interconnected by a system bus 250 and powered by a power supply 260.\nThe network interface(s) 210 include the mechanical, electrical, and signaling circuitry for communicating data over physical links coupled to the network 100.  The network interfaces may be configured to transmit and/or receive data using a\nvariety of different communication protocols.  Notably, a physical network interface 210 may also be used to implement one or more virtual network interfaces, such as for virtual private network (VPN) access, known to those skilled in the art.\nThe memory 240 comprises a plurality of storage locations that are addressable by the processor(s) 220 and the network interface(s) 210 for storing software programs and data structures associated with the embodiments described herein.  The\nprocessor 220 may comprise necessary elements or logic adapted to execute the software programs and manipulate the data structures 245.  An operating system 242 (e.g., the Internetworking Operating System, or IOS.RTM., of Cisco Systems, Inc., another\noperating system, etc.), portions of which are typically resident in memory 240 and executed by the processor(s), functionally organizes the node by, inter alia, invoking network operations in support of software processors and/or services executing on\nthe device.  These software processors and/or services may comprise a malware detection process 247 and/or a signature analysis process 248, as described herein.\nIt will be apparent to those skilled in the art that other processor and memory types, including various computer-readable media, may be used to store and execute program instructions pertaining to the techniques described herein.  Also, while\nthe description illustrates various processes, it is expressly contemplated that various processes may be embodied as modules configured to operate in accordance with the techniques herein (e.g., according to the functionality of a similar process). \nFurther, while processes may be shown and/or described separately, those skilled in the art will appreciate that processes may be routines or modules within other processes.\nIn general, malware detection process 247 may employ machine learning and/or detection rules, to detect the presence of malware in the network.  For example, malware detection process 247 may classify traffic in the network (and its\ncorresponding application or client node) as being either benign or malware-related.  In one embodiment, malware detection process 247 may assess captured traffic data to determine whether a given traffic flow or set of flows are caused by malware in the\nnetwork, such as a particular family of malware applications.  Example forms of traffic that can be caused by malware may include, but are not limited to, traffic flows reporting exfiltrated data to a remote entity, spyware or ransomware-related flows,\ncommand and control (C&C) traffic that oversees the operation of the deployed malware, traffic that is part of a network attack, such as a zero day attack or denial of service (DoS) attack, combinations thereof, or the like.  In further embodiments,\nmalware detection process 247 may classify the gathered traffic data to detect other anomalous behaviors (e.g., malfunctioning devices, misconfigured devices, etc.), traffic pattern changes (e.g., a group of hosts begin sending significantly more or less\ntraffic), or the like.\nMalware detection process 247 may employ any number of machine learning techniques to assess the gathered traffic data.  In general, machine learning is concerned with the design and the development of techniques that receive empirical data as\ninput (e.g., traffic data regarding traffic in the network) and recognize complex patterns in the input data.  For example, some machine learning techniques use an underlying model M, whose parameters are optimized for minimizing the cost function\nassociated to M, given the input data.  For instance, in the context of classification, the model M may be a straight line that separates the data into two classes (e.g., labels) such that M=a*x+b*y+c and the cost function is a function of the number of\nmisclassified points.  The learning process then operates by adjusting the parameters a,b,c such that the number of misclassified points is minimal.  After this optimization/learning phase, detection process 247 can use the model M to classify new data\npoints, such as information regarding new traffic flows in the network.  Often, M is a statistical model, and the cost function is inversely proportional to the likelihood of M, given the input data.\nIn various embodiments, malware detection process 247 may employ one or more supervised, unsupervised, or semi-supervised machine learning models.  Generally, supervised learning entails the use of a training set of data, as noted above, that is\nused to train the model to apply labels to the input data.  For example, the training data may include sample traffic data that is \"normal,\" or \"malware-generated.\" On the other end of the spectrum are unsupervised techniques that do not require a\ntraining set of labels.  Notably, while a supervised learning model may look for previously seen attack patterns that have been labeled as such, an unsupervised model may instead look to whether there are sudden changes in the behavior of the network\ntraffic.  Semi-supervised learning models take a middle ground approach that uses a greatly reduced set of labeled training data.\nExample machine learning techniques that malware detection process 247 can employ may include, but are not limited to, nearest neighbor (NN) techniques (e.g., k-NN models, replicator NN models, etc.), statistical techniques (e.g., Bayesian\nnetworks, etc.), clustering techniques (e.g., k-means, mean-shift, etc.), neural networks (e.g., reservoir networks, artificial neural networks, autoencoders, etc.), support vector machines (SVMs), logistic or other regression, Markov models or chains,\nprincipal component analysis (PCA) (e.g., for linear models), multi-layer perceptron (MLP) ANNs (e.g., for non-linear models), replicating reservoir networks (e.g., for non-linear models, typically for time series), random forest classification, or the\nlike.\nThe performance of a machine learning model can be evaluated in a number of ways based on the number of true positives, false positives, true negatives, and/or false negatives of the model.  For example, the false positives of the model may\nrefer to the number of traffic flows that are incorrectly classified as malware-generated, anomalous, etc. Conversely, the false negatives of the model may refer to the number of traffic flows that the model incorrectly classifies as normal, when\nactually malware-generated, anomalous, etc. True negatives and positives may refer to the number of traffic flows that the model correctly classifies as normal or malware-generated, etc., respectively.  Related to these measurements are the concepts of\nrecall and precision.  Generally, recall refers to the ratio of true positives to the sum of true positives and false negatives, which quantifies the sensitivity of the model.  Similarly, precision refers to the ratio of true positives the sum of true\nand false positives.\nIn some cases, malware detection process 247 may assess the captured traffic data on a per-flow basis.  In other embodiments, malware detection process 247 may assess traffic data for a plurality of traffic flows based on any number of different\nconditions.  For example, traffic flows may be grouped based on their sources, destinations, temporal characteristics (e.g., flows that occur around the same time, etc.), combinations thereof, or based on any other set of flow characteristics.\nAs noted above, botnets represent a particular security concern for network administrators.  Once a client device has been infected with malware for the botnet, it may communicate with a command and control (C&C) server which sends control\ncommands to the infected device.  For example, a C&C server may issue commands to many malware-infected devices, to instruct the infected devices to launch a coordinated DDoS attack on a particular target.\nEncryption is increasingly being used to conceal malware-related traffic.  For example, a large number of malware reports indicate that ongoing advanced persistent threat (APT) campaigns use Transport Layer Security (TLS) to encrypt network\ncommunications between an infected client device and the C&C servers.  From analysis of encrypted traffic that is malware-related, the inventors herein have observed that the security certificates used for malware-related traffic are also typically\nself-signed.  This is in contrast to security certificates that are signed by a trusted certification authority.\nTwo reasons may exist for malicious traffic using self-signed certificates.  First, self-signed certificates are free, whereas certification authority-signed certificates are not.  Second, to obtain a certificate signed by a certification\nauthority, the attacker/malicious entity would need to provide an identity to the certification authority for verification.  Naturally, an attacker/malicious entity would want to remain as anonymous as possible, to avoid criminal penalties.\nWith respect to self-signed certificates, most web browsers now display a security alert since self-signed certificates are not verified by a trusted CA.  Often, the alert advises the user to abort the webpage for security reasons (e.g., by\ndisplaying a message that the page is untrusted).  Consequently, Internet users tend to avoid browsing sites that use self-signed certificates.  In contrast, however, non-browser applications and/or malware may not be as regulating and continue to\nestablish connections with servers that use self-signed certificates.  Accordingly, the use of self-signed certificates by websites and domains may be an indicator of malicious activity.\nIdentifying Self-Signed Certificates using HTTP Access Logs for Malware Detection\nThe techniques herein allow for the identification of domains that use self-signed certificates based only on the HTTP access logs regarding the secure session.  In some aspects, the techniques herein may analyze certificate validation checks in\nthe access logs, to identify whether the domain of the secure session issued a self-signed certificate or not.  More specifically, if the receiving client of a certificate for a secure session does not perform a subsequent validation check (e.g., by\nissuing a validation request to a validation authority), this is a strong indication that the certificate was self-signed by the corresponding domain.  By modeling the HTTP logs surrounding the HTTP Secure (HTTPS) requests to each domain, the techniques\nare able to distinguish self-signed certificates with high precision.  In further aspects, the self-signature decision may be used as input to a malware detection process, e.g., as one of the factors assessed by the detector.\nSpecifically, according to one or more embodiments of the disclosure as described in detail below, a device in a network receives traffic information regarding one or more secure sessions in the network.  The device associates the one or more\nsecure sessions with corresponding certificate validation check traffic indicated by the received traffic information.  The device makes a self-signed certificate determination for an endpoint domain of a particular secure session based on whether the\nparticular secure session is associated with certificate validation check traffic.  The device causes the self-signed certificate determination for the endpoint domain to be used as input to a malware detector.\nIllustratively, the techniques described herein may be performed by hardware, software, and/or firmware, such as in accordance with the signature analysis process 248, which may include computer executable instructions executed by the processor\n220 (or independent processor of interfaces 210) to perform functions relating to the techniques described herein, e.g., in conjunction with malware detection process 247.\nOperationally, FIGS. 3A-3D illustrate an example of the use of HTTP access logs to identify domains that use self-signed certificates, according to various embodiments.  As shown in FIG. 3A, network 100 may include an endpoint client node/device\nA, an endpoint node/device C (e.g., a server), and an intermediary node/device I located therebetween.  For example, the intermediary device I may be a router, switch, firewall, intrusion detection system (IDS), intrusion prevention system (IPS), proxy\nserver, or other networking device through which traffic exchanged between nodes/device A and C may flow.\nA simplified example of the establishment of a secure session between devices A and C is also shown in FIG. 3A.  First, client A may send a request 302 to endpoint C to establish a secure session/connection.  As would be appreciated, client A\nmay perform a domain name system (DNS) lookup for the domain of endpoint C, prior to sending request 302, if the address of endpoint C is not already known.  In other words, client A may initiate handshaking with endpoint C during which client A and\nendpoint C may exchange the information needed to securely communicate with one another.  For example, request 302 may be a `hello` message that indicates the ciphersuites supported by client A, a random number, or other such handshaking information.\nIn response to receiving request 302, endpoint C may send a response 304 to client A that includes its security certificate.  As noted previously, two possibilities exist with respect to the signature on the certificate.  In some cases, the\nsecurity certificate may be signed by a certification authority.  Such may be the case if the entity associated with the domain of endpoint C first proved his or her identity to the certification authority and paid a fee.  The other possibility, as noted\npreviously, is that the security certificate sent by endpoint C to client A is self-signed.  In such a case, the certificate may instead be signed by the entity associated with endpoint C to certify its own identity (e.g., as opposed to certification\nfrom a third party certification authority).  Example security certificates may include, but are not limited to, Secure Socket Layer (SSL) certificates, X.509 certificates, public key infrastructure (PKI) certificates, or the like.\nIntermediary device I may capture traffic information regarding the exchange between client A and endpoint C. For example, intermediary device I may capture information regarding the address of client A, the address of endpoint C, the domain of\nendpoint C, the ciphersuite in use, timing information (e.g., when client A sent request 302, etc.), or any other information available from the data exchange between client A and endpoint C.\nAs shown in FIG. 3B, after receiving the certificate from endpoint C, client A may attempt to validate the certificate using a validation authority, if the certificate was signed by a certification authority.  Notably, if a domain uses such an\nSSL certificate, a browser on the client will then typically verify that the signature from the certification authority is still valid and not revoked.  For example, client A may send a certificate validation request 306 to validation authority device V\nvia intermediary device I. In some cases, the validation authority may be issuing certification authority itself.  In other cases, however, the validation authority may be kept separate so that previously issued certificates can still be validated, even\nif the certification authority is unavailable.\nValidation request 306 may use any of several different HTTP protocols to perform the certificate validation (e.g., PKI, etc.).  One such protocol is the Online Certificate Status Protocol (OCSP) which is used to check the revocation status of\nX.509 certificates.  In such a case, the validation authority VA may be viewed as an OCSP responder.  Another validation approach would be to use Certificate Revocation List (CRL) downloads.  In general, CRLs are lists of certificates that have been\nrevoked by their issuing certification authorities.  OCSP and CRL downloads work in a similar manner, but with OCSP responses including less overall information than that of a CRL download.  As would be appreciated, validation request 306 may take the\nform of any other validation protocol/mechanism, as desired.\nIn response to receiving request 306, the validation authority V may return a validation result 308 to client A. In turn, client A is now able to determine whether the certification authority-signed certificate sent by endpoint C is indeed\nvalid.  If so, client A may continue the handshaking process and establish a secure session with endpoint C (e.g., by exchanging cryptographic data with endpoint C, etc.).\nSimilar to the initial request/response between client A and endpoint C, intermediary device I may also capture HTTP information regarding the validation exchange between client A and validation authority V. For example, intermediary device I\nmay capture address information for client A, address information for validation authority V, timing information (e.g., when client A sent validation request 306, etc.), information regarding the certificate validation protocol/mechanism in use, or any\nother information regarding the validation exchange.  Such validation requests can be thus be recognized in the HTTP access log data (e.g., using their signatures--typical patterns in uniform resource locators (URLs) such as file name extensions or MIME\ntypes, etc.).  In this way, intermediary device I may capture HTTP traffic logs for both the initial client A-endpoint C exchange, as well as the subsequent validation exchange between client A and validation authority V.\nAs shown in FIG. 3C, intermediary device I may analyze its HTTP access log data, to determine whether the certificate used by the domain of endpoint C was self-signed.  Notably, if the certificate used by endpoint C was self-signed, it is not\nexpected for there to be associated certificate validation check traffic from client A. By analyzing the HTTP logs in this manner, intermediary device I can make a determination as to whether or not the certificate used by endpoint C was self-signed.  In\nother embodiments, intermediary device I may provide the captured traffic information to another device that performs this analysis.  For example, intermediary device I (or a plurality of such intermediaries) may provide HTTP access logs to another\nanalysis device for purposes of identifying self-signed certificates.\nAs shown in FIG. 3D, based on the self-signed certificate determination for the certificate from endpoint C, intermediary device I (or another device) may use this determination as a factor when detecting the presence of malware in the network. \nOther factors that the malware detector may consider could include, but are not limited to, metrics regarding the traffic itself (e.g., packet length and time information, associated applications, etc.), DNS information regarding the domain of endpoint C\n(e.g., whether the domain was generated by a domain generation algorithm (DGA), etc.), header information from the exchange between client A and endpoint C, and the like.\nFIG. 4 illustrates an example architecture 400 to identify self-signed certificates for malware detection, according to various embodiments.  As shown, signature analysis process 248 may include a number of sub-processes and access storage\nlocations, either on a local device (e.g., device 200) or across a plurality of devices that implement signature analysis process 248.\nIn some embodiments, signature analysis process 248 may include a log analyzer 404 that receives HTTP access logs 402.  Such logs may be captured by one or more intermediary devices between any number clients and endpoints/domains.  More\nspecifically, access logs 402 may comprise traffic information regarding one or more secure sessions in the network (e.g., handshake traffic, certificate validation traffic, etc.).\nLog analyzer 404 may be configured to parse out secure session data 406 and certificate validation check data 408 from HTTP access logs 402.  Secure session data 406 may include, for example, data indicative of the clients and domains involved\nin secure sessions in the network, timing information for the secure sessions (e.g., when a secure sessions was requested or established, etc.), or any other information that can be captured from the HTTP traffic associated with a secure session.\nLikewise, certificate validation check data 408 may include any information regarding the HTTP-based certificate validation check traffic.  For example, certificate validation check data 408 may include information indicative of the clients that\nissued the validation check requests, the validation authorities queried, timing information regarding the validation check traffic (e.g., when a client sent a validation request, etc.), or any other information available from HTTP access logs 402\nregarding certificate validation request.  Log analyzer 404 may, for example, identify traffic information from HTTP access logs 402 that conform to a known certificate validation protocol/mechanism (e.g., OCSP, CRL downloads, etc.), by comparing the\nvalidation authorities to a known list of common certification or validation authorities (e.g., certificate authority list 414), or the like.\nAccording to various embodiments, signature analysis process 248 may include a traffic correlator 410 configured to correlate secure session data 406 with certificate validation check data 408.  Based on the correlations, traffic correlator 410\nmay generate and provide self-signed certificate determinations 416 to malware detection process 247 which may be executed on the same local device or another device in the network.\nIn some embodiments, traffic correlator 410 may employ any number of heuristic rules 412, to determine whether a particular domain likely issued a self-signed certificate.  For example, rules 412 may be heuristics without 100% precision to\nincrease their recall because, as described below, traffic correlator 410 may base a determination 416 on a plurality of visits to the corresponding domain, to suppress possible errors.  Generally, traffic correlator 410 may determine whether or not a\ngiven client that requested a secure session subsequently performed a validation of the certificate (e.g., within a predefined timeframe of one another, etc.), based on data 406-408.\nIn some cases, a certificate check might not be always present in the logs even if the domain is using a properly signed certificate.  For example, if the information about the certificate has already been cached by the browser application of\nthe client, the browser is unlikely to generate certificate validation check traffic.  Thus, in some embodiments, traffic correlator 410 may combine and assess traffic information from visits to the domain under scrutiny by a plurality of clients and\nover a longer period of time, to determine whether the domain uses a self-signed certificate.  For example, if there is at least one client that generated certificate validation check traffic that is associated with a secure session request from that\nclient and involving the domain, traffic correlator 410 may determine that the domain issued a certificate signed by a certification authority.  Otherwise, traffic correlator 410 may determine that the domain issued a self-signed certificate (e.g., if\nnone of the clients involved in secure sessions with the domain sent certificate validation requests).\nMalware detection process 247 may use the self-signed certificate determinations 416 as input, to generate malware determinations 418.  In turn, malware determinations 418 can be used by the malware detector to initiate one or more mitigation\nactions in the network.  For example, if a particular domain is flagged as malware-related, the malware detector may block traffic to this domain.  Similarly, if a particular client is found to have contacted one or more malware-related domains, the\nmalware detector may send an alert (e.g., to a network administrator) to initiate corrective actions.\nBy way of non-limiting example of the operation of architecture 400, consider the case of a client device visiting twitter.com.  Such a visit is typically accompanied by a certificate check to verify the certificate at the issuer.  Thus, the\nHTTP access logs may capture the following URLs:\nhttps://twitter.com\nhttp://ocsp2.globalsign.com/gsalphasha2g2/MFMwUTBPMEOwSzAJBgUrDgMC GgUABBSE1Wv4CYvTB7dm2OHrrWWWqmtnYQQU9c3VPAhQ%2BWpPOreX2laD5mn SaPcCEhEhQy17zc7ooUokaeXd8t99CQ%3D%3D\nFrom the HTTP access logs 402, log analyzer 404 may parse out the secure session data 406 regarding the traffic to the first URL (e.g., the address of the client that requested the URL, timing information regarding the request, etc.) and the\ncertificate validation check data 408 regarding the traffic to the second URL.  By assessing this data, traffic correlator 410 may determine that the same client sent traffic to both URLs within a predefined timespan of one another, thereby indicating\nthat the client performed a validation check on the certificate issued by the first URL.  Based on this, traffic correlator may determine that the domain of the first URL used a certificate signed by a certification authority.\nFIG. 5 illustrates an example simplified procedure for identifying self-signed certificates for malware detection, in accordance with the teachings herein.  For example, a non-generic, specifically configured device (e.g., device 200) may\nperform procedure 400 by executing stored instructions (e.g., process 247 and/or process 248).  The procedure 500 may start at step 505, and continues to step 510, where, as described in greater detail above, the device may receive traffic information\nregarding one or more secure sessions in the network.  For example, the device may receive captured HTTP access log data regarding traffic flows before, during, and/or after a secure session is established between a client in the network and an endpoint\ndomain.\nAt step 515, as detailed above, the device may associate the one or more secure sessions with corresponding certificate validation check traffic indicated by the received traffic information.  For example, the device may match the client address\nthat requested a secure session with a client address from the certificate validation check traffic (e.g., the same client both requested a secure session and performed a certificate validation check).  In some embodiments, the device may make such\nassociations based on the timing of the secure session request/handshake and the certificate validation check traffic (e.g., if the same client sent both types of traffic within a predefined timespan).\nAt step 520, the device may make a self-signed certificate determination for an endpoint domain of a particular secure session based on whether the particular secure session is associated with certificate validation check traffic, as described\nin greater detail above.  Notably, if the client involved in the particular secure session also generated certificate validation check traffic associated with the secure session, the device may determine that the certificate used by the domain was signed\nby a certification authority.  Conversely, if the device finds no such association between the session and a validation check by the client, the device may determine that the endpoint domain of the secure session used a self-signed certificate.  In\nfurther embodiments, the device may base self-signed certificate determination for the domain on a plurality of secure sessions.  For example, if none of the secure sessions with the domain are associated with certificate validation check traffic (e.g.,\nacross multiple clients, etc.), the device may determine that the domain uses self-signed certificates.\nAt step 525, as detailed above, the device may cause the self-signed certificate determination to be used as input to a malware detector.  In some cases, the malware detector may be executed by the device itself.  In other cases, the device may\nprovide the self-signed certificate determination to another device in the network that acts as a malware detector.  Such a malware detector may use rules, machine learning, or any other malware analysis technique to determine whether the corresponding\ndomain is associated with malware.  Procedure 500 then ends at step 530.\nIt should be noted that while certain steps within procedure 500 may be optional as described above, the steps shown in FIG. 5 are merely examples for illustration, and certain other steps may be included or excluded as desired.  Further, while\na particular order of the steps is shown, this ordering is merely illustrative, and any suitable arrangement of the steps may be utilized without departing from the scope of the embodiments herein.\nThe techniques described herein, therefore, allow for the detection of self-signed certificates based solely on HTTP access log data.  In other words, the techniques herein do not require content inspection of the certificate itself and can be\nused in real-time applications.  Further, the techniques herein do not require the malware detection system to download the certificate itself, to determine whether the corresponding domain uses self-signed certificates.\nWhile there have been shown and described illustrative embodiments that provide for the identification of self-signed certificates, it is to be understood that various other adaptations and modifications may be made within the spirit and scope\nof the embodiments herein.  For example, while certain embodiments are described herein with respect to using certain models for purposes of malware detection, the models are not limited as such and may be used for other functions, in other embodiments. \nIn addition, while certain protocols are shown, other suitable protocols may be used, accordingly.\nThe foregoing description has been directed to specific embodiments.  It will be apparent, however, that other variations and modifications may be made to the described embodiments, with the attainment of some or all of their advantages.  For\ninstance, it is expressly contemplated that the components and/or elements described herein can be implemented as software being stored on a tangible (non-transitory) computer-readable medium (e.g., disks/CDs/RAM/EEPROM/etc.) having program instructions\nexecuting on a computer, hardware, firmware, or a combination thereof.  Accordingly this description is to be taken only by way of example and not to otherwise limit the scope of the embodiments herein.  Therefore, it is the object of the appended claims\nto cover all such variations and modifications as come within the true spirit and scope of the embodiments herein.", "application_number": "15386006", "abstract": " In one embodiment, a device in a network receives traffic information\n     regarding one or more secure sessions in the network. The device\n     associates the one or more secure sessions with corresponding certificate\n     validation check traffic indicated by the received traffic information.\n     The device makes a self-signed certificate determination for an endpoint\n     domain of a particular secure session based on whether the particular\n     secure session is associated with certificate validation check traffic.\n     The device causes the self-signed certificate determination for the\n     endpoint domain to be used as input to a malware detector.\n", "citations": ["7634811", "7739494", "8429734", "9288190", "9407644", "9419942", "20140298420", "20160080404", "20160285861", "20160359842"], "related": []}, {"id": "20180199291", "patent_code": "10375643", "patent_name": "Data bundling and fast dormancy based upon intelligent application\n     learning", "year": "2019", "inventor_and_country_data": " Inventors: \nBrisebois; Arthur Richard (Cumming, GA), Tong; Yonghui (Alpharetta, GA)  ", "description": "<BR><BR>TECHNICAL FIELD\nThe subject disclosure relates to wireless communications and, more particularly, to a mechanism that facilitates data bundling and fast dormancy based on intelligent learning and characterization of applications employed by a user equipment\n(UE), to reduce network signaling in the communication network.\n<BR><BR>BACKGROUND\nUniversal Mobile Telecommunications System (UMTS) networks have seen an explosive data growth in past few years and, in future, are expected to see continuing growth in the Packet Switched (PS) domain.  Beyond data traffic volume growth, an even\nmore aggressive growth in data signaling load has been detected.  Among all the signaling messages/procedures on UMTS networks, Radio Access Network (RAN) signaling procedures have caused the most growth and impact.  This is due to complicated radio\nresource sharing techniques required to conserve resources occupied by various users and services.\nThe majority of RAN signaling events are for connection setup and state transitions (e.g., during Channel Switching).  Typically, when a data payload is to be sent from/received by a user equipment (UE), a request is sent to a radio network\ncontroller (RNC) to establish a dedicated channel (DCH).  Once the data payload is sent or received, multiple inactivity timers are triggered by the RNC and upon expiration of the timers, the RNC transitions the UE from DCH to forward access channel\n(FACH) and then to IDLE state.  To achieve resource efficiency, such timers are often set to short values (cumulatively around 12-16 seconds).  Thus, the UE is quickly moved into the IDLE state after completion of a current data session (download and/or\nupload).  Since there is no active data connection between the UE and the core network during the IDLE state, power consumption is minimized.  Data applications on the UE initiate multiple data payloads for communication between the UE and RNC.  However,\nthe applications operate independently from the radio network perspective, which leads to requesting and establishing multiple independent connections for payloads from different applications.  As a result, even though battery life of the UE is\nconserved, a large number of signaling events are generated and RNC processing load is substantially increased.\nIn addition to network initiated inactivity-based state transition, UE manufacturers have introduced a fast dormancy (FD) feature that initiates direct transition from DCH to IDLE or FACH to IDLE, before the respective network inactivity timer\nexpires.  In this type of system control, the UE proactively releases the data connection, established by the RNC, directly from DCH to IDLE or FACH to IDLE as quickly as possible, to further conserve UE battery life.  However, once the UE is in the IDLE\nstate, the data connection must be reestablished to communicate another payload.  The reestablishment of the data connection is resource intensive, consumes a high amount of power in the RNC, and can significantly drive up the RNC load. <BR><BR>BRIEF\nDESCRIPTION OF THE DRAWINGS\nFIG. 1 illustrates an example system that facilitates signaling load optimization in a communication network.\nFIG. 2 illustrates an example system that can be employed for creating application profiles in a user equipment (UE).\nFIG. 3 illustrates are example profiles created by observing and/or receiving information associated with data applications on a UE.\nFIG. 4 illustrates an example system that facilitates data bundling and fast dormancy based upon intelligent application learning.\nFIG. 5 illustrates an example system that facilitates automating one or more features in accordance with the subject innovation.\nFIG. 6 illustrates an example system that facilitates data bundling and fast dormancy based on user interactivity.\nFIG. 7 illustrates an example timeline that depicts when an application profiler (AP) engine can perform data bundling and/or can disable fast dormancy.\nFIG. 8 illustrates an example methodology that can be utilized to facilitate data bundling and fast dormancy based on application monitoring.\nFIG. 9 illustrates an example methodology that facilitates data bundling of data flows from different applications to reduce network signaling.\nFIG. 10 illustrates an example methodology that overrides a fast dormancy mechanism based on application learning.\nFIG. 11 illustrates a block diagram of a UE suitable for data bundling and controlling fast dormancy based on application characterization in accordance with the innovation.\nFIG. 12 illustrates a Global System for Mobile Communications (GSM)/General Packet Radio Service (GPRS)/Internet protocol (IP) multimedia network architecture that can employ the disclosed architecture.\nFIG. 13 illustrates a block diagram of a computer operable to execute the disclosed communication architecture.\n<BR><BR>DETAILED DESCRIPTION\nOne or more embodiments are now described with reference to the drawings, wherein like reference numerals are used to refer to like elements throughout.  In the following description, for purposes of explanation, numerous specific details are\nset forth in order to provide a thorough understanding of the various embodiments.  It may be evident, however, that the various embodiments can be practiced without these specific details, e.g., without applying to any particular networked environment\nor standard.  In other instances, well-known structures and devices are shown in block diagram form in order to facilitate describing the embodiments in additional detail.\nAs used in this application, the terms \"component,\" \"module,\" \"system,\" \"interface,\" \"platform,\" \"service,\" \"engine,\" or the like are generally intended to refer to a computer-related entity, either hardware, a combination of hardware and\nsoftware, software, or software in execution or an entity related to an operational machine with one or more specific functionalities.  For example, a component may be, but is not limited to being, a process running on a processor, a processor, an\nobject, an executable, a thread of execution, a program, and/or a computer.  By way of illustration, both an application running on a controller and the controller can be a component.  One or more components may reside within a process and/or thread of\nexecution and a component may be localized on one computer and/or distributed between two or more computers.  As another example, an interface can include I/O components as well as associated processor, application, and/or API components.\nFurther, the various embodiments can be implemented as a method, apparatus, or article of manufacture using standard programming and/or engineering techniques to produce software, firmware, hardware, or any combination thereof to control a\ncomputer to implement the disclosed subject matter.  The term \"article of manufacture\" as used herein is intended to encompass a computer program accessible from any computer-readable device or computer-readable storage/communications media.  For\nexample, computer readable storage media can include but are not limited to magnetic storage devices (e.g., hard disk, floppy disk, magnetic strips .  . . ), optical disks (e.g., compact disk (CD), digital versatile disk (DVD) .  . . ), smart cards, and\nflash memory devices (e.g., card, stick, key drive .  . . ). Of course, those skilled in the art will recognize many modifications can be made to this configuration without departing from the scope or spirit of the various embodiments.\nIn addition, the word \"exemplary\" is used herein to mean serving as an example, instance, or illustration.  Any aspect or design described herein as \"exemplary\" is not necessarily to be construed as preferred or advantageous over other aspects\nor designs.  Rather, use of the word exemplary is intended to present concepts in a concrete fashion.  As used in this application, the term \"or\" is intended to mean an inclusive \"or\" rather than an exclusive \"or\".  That is, unless specified otherwise,\nor clear from context, \"X employs A or B\" is intended to mean any of the natural inclusive permutations.  That is, if X employs A; X employs B; or X employs both A and B, then \"X employs A or B\" is satisfied under any of the foregoing instances.  In\naddition, the articles \"a\" and \"an\" as used in this application and the appended claims should generally be construed to mean \"one or more\" unless specified otherwise or clear from context to be directed to a singular form.\nMoreover, terms like \"user equipment,\" \"mobile station,\" \"mobile,\" subscriber station,\" \"access terminal,\" \"terminal,\" \"handset,\" \"mobile device,\" and similar terminology, refer to a wireless device utilized by a subscriber or user of a wireless\ncommunication service to receive or convey data, control, voice, video, sound, gaming, or substantially any data-stream or signaling-stream.  The foregoing terms are utilized interchangeably in the subject specification and related drawings.  Likewise,\nthe terms \"access point,\" \"base station,\" \"Node B,\" \"evolved Node B,\" and the like, are utilized interchangeably in the subject application, and refer to a wireless network component or appliance that serves and receives data, control, voice, video,\nsound, gaming, or substantially any data-stream or signaling-stream from a set of subscriber stations.  Data and signaling streams can be packetized or frame-based flows.\nFurthermore, the terms \"user,\" \"subscriber,\" \"customer,\" and the like are employed interchangeably throughout the subject specification, unless context warrants particular distinction(s) among the terms.  It should be appreciated that such terms\ncan refer to human entities or automated components supported through artificial intelligence (e.g., a capacity to make inference based on complex mathematical formalisms), which can provide simulated vision, sound recognition and so forth.  In addition,\nthe terms \"data flow,\" \"data session,\" and the like are also employed interchangeably throughout the subject specification, unless context warrants particular distinction(s) among the terms.\nThe systems and methods disclosed herein, in one aspect thereof, can facilitate bundling data sessions and controlling fast dormancy based on learning and characterization of different applications on a user equipment (UE), through historic data\ntracking and analysis.  As an application starts to generate data flows, the system tracks the characteristics of the application and builds a histogram for various application characteristics, such as, but not limited to, inter-packet arrival time,\nfrequency of use, packet size, session duration, delay tolerance level etc. Moreover, the system includes an application profiler (AP) engine that can predict arrival time of data flows from multiple applications (downlink or uplink) based on history\nbuilding and statistical analysis of the tracked characteristics.  Based on the arrival time, the AP engine can determine if the current data flow can be delayed and bundled with one or more next data flows, as well as determine whether a fast dormancy\ntimer can be delayed on completion of the current data flow.\nAccording to an aspect, the system, via enabling data bundling and controlling fast dormancy, reduces the number of radio resource control (RRC) connection establishments (as part of data connection setup) and thus minimizes Radio network\ncontroller (RNC) processing load and call setup time (latency).  Moreover, the system can bundle closely spaced data payloads and stack data packets from one or more applications in one single connection.  This results in decreased number of signaling\nevents and reduces RNC processing load.  Further, by temporarily disabling fast dormancy the system can avoid unnecessary and pre-mature data connection releases (and corresponding new data connection setups).\nAnother aspect of the disclosed subject matter relates to a method that can be employed to facilitate data bundling and fast dormancy based on observing application behavior in a UE.  The method comprises monitoring data applications\ninstalled/downloaded on the UE, and generating an application profile for each application.  Further, the application profiles can be analyzed and the applications can be categorized, for example, as \"random,\" \"delay tolerant,\" and/or \"not delay\ntolerant,\" etc. Furthermore, the method includes identifying an arrival time of a next data flow based on statistical analysis.  Moreover, it can be determined whether the current data flow can be bundled with the next data flow based on the arrival time\nand/or the categorization.  In addition, the arrival time can also be employed to determine whether a fast dormancy timer can be delayed.\nThe systems and methods disclosed herein reduce network signaling load, and shorten user perceived latency with minimum loss on battery life of a user equipment (UE).  Moreover, the disclosed systems and methods perform data bundling and/or fast\ndormancy based on intelligent learning and characterization of applications on the UE.  Specifically, the UE monitors and profiles application activity and tunes data bundling and/or fast dormancy techniques accordingly, as explained in detail infra.\nAspects, features, or advantages of the subject innovation can be exploited in substantially any wireless communication technology; e.g., Universal Mobile Telecommunications System (UMTS), Wi-Fi, Worldwide Interoperability for Microwave Access\n(WiMAX), General Packet Radio Service (GPRS), Enhanced GPRS, Third Generation Partnership Project (3GPP) Long Term Evolution (LTE), Third Generation Partnership Project 2 (3GPP2) Ultra Mobile Broadband (UMB), High Speed Packet Access (HSPA), or Zigbee. \nAdditionally, substantially all aspects of the subject innovation can be exploited in legacy telecommunication technologies.\nReferring initially to FIG. 1, there illustrated is an example system 100 that facilitates signaling load optimization in a communication network, according to an aspect of the subject specification.  Moreover, system 100 performs data bundling\nand fast dormancy based on intelligent learning and characterization of applications by a user equipment (UE) 102, to reduce network signaling 104 with a Radio Network Controller (RNC) 106.  Typically, the core network 108 can include a UMTS network;\nhowever, it can be appreciated that the subject innovation is not so limited and most any communication network can be utilized.  The core network 108 can be connected to various backbone networks (not shown) for example, the Internet, Integrated\nServices Digital Network (ISDN), etc.\nTypically, UE 102 can include most any electronic communication device such as, but not limited to, most any consumer electronic device, for example, a digital media player, a digital photo frame, a digital camera, a cellular phone, a personal\ncomputer, a personal digital assistant (PDA), a smart phone, a laptop, a gaming system, etc. Further, UE 102 can also include LTE based devices, such as, but not limited to, most any home or commercial appliance that includes an LTE radio.  It can be\nappreciated that the UE 102 can be mobile, have limited mobility and/or be stationary.\nTypically, when data is communicated between the UE 102 and the core network 108, the UE 102 sends request to the RNC 106, requesting resources to establish a connection.  Once the data payload is sent or received, the RNC 106 activates multiple\ninactivity timers, which facilitate channel switching at the UE 102.  In one example, upon expiration of the timers, the UE 102 can transition from dedicated channel (DCH) to Forward Access Channel (FACH) to IDLE state.  Oftentimes, the inactivity timers\nare set to short values (e.g., cumulatively around 12-16 seconds) to achieve resource efficiency.  The short values enable the UE 102 to quickly move to IDLE state after completion of current data session (download or upload).\nIn addition to network initiated inactivity-based state transition, the UE 102 can also perform fast dormancy (FD) to initiate direct transition from DCH to IDLE or FACH to IDLE, before network inactivity timer expires.  The FD feature in the UE\n102 reduces power consumption in the UE 102, by transitioning the UE 102 to the IDLE mode and releasing the connection as quickly as possible.  In traditional systems, when more data is expected after the UE 102 releases the connection, the connection\nneeds to be reestablished, which can significantly increase the RNC load.  However, according to an embodiment of the subject system, UE 102 includes a signaling load optimization component 110 that identifies such cases (e.g., when data communication is\nexpected) and deactivates the FD mechanism and/or performs data bundling for closely spaced sequential data sessions.\nTypically, a user can download and utilize various applications on UE 102.  These applications operate independently from the radio network perspective and independently establish different connections with the RNC to communicate data payloads. \nIn one aspect, UE 102 can employ signaling load optimization component 110 to bundle closely spaced data payloads.  Moreover, the signaling load optimization component 110 can ensure that small data packets are stacked together and sent in a single\nconnection, rather than individually via multiple connections.  As a result, substantial amount of signaling can be reduced and the RNC processing load can be decreased significantly.\nThe UE 102 can further include an Application Profiler (AP) component 112, which can generate profiles for applications on the UE 102.  The AP component 112 can monitor activity and track characteristics associated with each application that\ngenerates an outgoing data flow request and/or receives an incoming data flow request from the network 108.  Moreover, the AP component 112 can predict if and/or when a new data flow request will be initiated by observing data flow related\ncharacteristics of the applications, forecasting trends and/or identifying probabilities for the new data flow request.  Based on the prediction, the signaling load optimization component 110 can determine whether to bundle data payloads from one or more\napplications and send the bundled data payloads as one data transmission.  Further, the signaling load optimization component 110 can identify whether fast dormancy can be disabled/delayed at the end of a given data flow transmission (e.g., due to a\nprediction that a new data flow request will be initiated shortly).\nReal network study has shown that the RNC processor load related to channel switching and data connection setup, e.g., radio resource control (RRC)/radio access bearer (RAB) setup, together can be as high as 70% of the total RNC processor load. \nFurther, study shows that RRC connection establishment (as part of data connection setup) involves multiple signaling handshake messages, not only increasing RNC processing load but also increasing prolong call setup time (latency).  Conventional systems\nemploy static state transition timers at RNC level, for all UEs served by the RNC, regardless of the UE type, applications running on these UEs and/or User interactions/awareness of these applications.  In contrast, system 100 provides UEs (e.g., UE 102)\nthat can intelligently bundle delay-tolerant data to reduce signaling load and disable/delay FD for situations with high likelihood of close spaced data sessions to reduce signaling load, as well as shorten call setup time.  In addition, system 100 can\nalso improve performance and extend battery life of UE 102.  Typically, when a new bearer/connection is set up by the UE 102, a ramp-up interval occurs initially, wherein conservative and inefficient attributes produce sub-optimal performance for some\ntime at the beginning of a data flow.  With bundling, the number of bearer activations are reduced and proportionally less time is spent in ramp-up; thus leading to improved performance and battery life of UE 102.\nReferring to FIG. 2, there illustrated is an example system 200 that can be employed for creating application profiles in a UE 102 in accordance with an aspect of the subject disclosure.  It can be appreciated that the UE 102 and the AP\ncomponent 112 can include functionality, as more fully described herein, for example, with regard to system 100.  As discussed supra, in one example, the UE 102 can be connected to the mobile core network through a wireless radio access network, such as,\nbut not limited to UMTS Terrestrial Radio Access Network (UTRAN).  Moreover, the UE 102 can include most any mobile and/or stationary, wireless and/or wired, electronic communication device (e.g., cell phone, PDA, tablet, PC, laptop, etc.).\nApplication(s) 202 can include most any computer program(s), function(s) and/or instruction(s) to perform an activity, action, and/or task associated with UE 102.  Typically, application(s) 202 can be pre-installed on UE 102 during manufacture,\ndownloaded by customers from app stores and/or other mobile software distribution platforms, and/or provided by a service provider on service activation or at most any other time.  Moreover, application(s) 202 can include data application(s) related to\nvarious fields, such as, but not limited to, business, entertainment, finance, games, health and fitness, maps and navigation, music or radio, news and weather, productivity, ringtones, wallpapers, skins and themes, social networking, sports and\nrecreation, travel, utilities, etc. For example, application(s) 202 can include an application that enables users to surf the Internet, blog, access social networking websites, listen to radio stations, and/or play simple to complex games.  In another\nexample, a location application can determine a location of the user, a friend or provide turn-by-turn instruction for navigation.  In yet another example, a medical application can be used to record/track a user's vital signs (e.g., heart rate) over a\nperiod of time and send them to a doctor for review.\nIn one aspect, monitoring component 204 can tack data flows associated with each application 202.  In particular, monitoring component 204 can identify characteristics of the application(s) 202 and perform statistical analysis, for example,\nbuild a histogram, for various features associated with an application, such as, but not limited to, inter-packet arrival time, frequency of use, packet size, session duration, delay tolerance level etc. In one example, when an application is installed\nand/or downloaded, information associated with the characteristics of the application can be received by the monitoring component 204 from the application itself, and/or queried by the monitoring component 204 from another entity, for example, a network\nserver or web server.  In another example, monitoring component 204 can monitor, track and/or record behavior (e.g., inter-packet arrival time, frequency of use, packet size, session duration, delay tolerance level etc.) associated with an application\nover a period of time.\nFurther, the AP component 112 can include a profile creation component 206 that can analyze data monitored by monitoring component 204 to generate a profile for each application(s) 202.  Moreover, a profile can include one or more profile keys\nthat describe the behavior of the application with respect to data flows.  Typically, more an application is utilized, more accurate the learning (e.g., by monitoring component 204) and profiling (e.g., by profile creation component 206) for such\napplication will be.  In one aspect, the by monitoring component 204 and profile creation component 206 can concurrently track, learn and build profiles for all applications that generate an outgoing data flow request and receive a incoming data flow\nrequest from network.  The profile creation component 206 can store the profiles in an application profile database 208.  Moreover, on learning or monitoring (e.g., by monitoring component 204) new information associated with an application, the profile\ncreation component 206 can update and/or modify the application's profile stored in the application profile database 208.  Further, profile creation component 206 can delete a profile associated with an application from the application profile database\n208, if the application is deleted or uninstalled.\nAdditionally, the profile creation component 206 can employ most any machine learning/artificial intelligence technique and develop a statistically satisfactory prediction module for a set of applications 202.  A set of applications can be\nmarked as random due to their randomness of data flows, and will not be included in final decision for optimization of signaling load.  In one example, UE 102 can be employed to operate as a mobile hotspot and can communicate with one or more tethered\ndevices.  In this example scenario, data flows to/from the tethered devices can also be marked as \"random\" and generally not included to identify whether data bundling is to be performed and/or FD is to be delayed.\nTypically, the application profile database 208 can include volatile memory or nonvolatile memory, or can include both volatile and nonvolatile memory.  By way of illustration, and not limitation, nonvolatile memory can include read only memory\n(ROM), programmable ROM (PROM), electrically programmable ROM (EPROM), electrically erasable PROM (EEPROM), or flash memory.  Volatile memory can include random access memory (RAM), which acts as external cache memory.  By way of illustration and not\nlimitation, RAM is available in many forms such as static RAM (SRAM), dynamic RAM (DRAM), synchronous DRAM (SDRAM), double data rate SDRAM (DDR SDRAM), enhanced SDRAM (ESDRAM), Synchlink DRAM (SLDRAM), and direct Rambus RAM (DRRAM).  The memory (e.g.,\ndata stores, databases, caches) of the subject systems and methods is intended to comprise, without being limited to, these and any other suitable types of memory.\nReferring now to FIG. 3, there illustrated are example profiles 300 created by observing and/or receiving data associated with applications on a UE, according to an aspect of the subject disclosure.  Typically, profiles 302.sub.1-N can be\ncreated/updated by profile creation component 206 and stored in application profile database 208, which are more fully described herein, for example, with regard to system 200.  It can be appreciated that most any number \"N\" of application profiles\ncorresponding to applications on the UE can be created and stored (where N is a natural number from one to infinity).  Further, it can be appreciated that although nine profile keys (304-320) are depicted in each profile of FIG. 3, the profiles\n302.sub.1-N can include greater or fewer number of profile keys.\nIn one aspect, a profile key (304-320) can be most any parameter or characteristic of an application with respect to data sessions/flows to and/or from the communication network.  As an example, Inter-arrival time (I) 304 can be measured by\nobserving the interval between one data flow request and the next data flow request, associated with an application (e.g., the data flow request can be initiated by the application or the network).  Typically, the last interval, the last X number of\nintervals (e.g., X is most any natural number), average of the intervals, etc. can be stored in the in the profile.  Further, Frequency Use (F) 306 can be indicative of the usage intensity of the application.  For example, how often is the application\ngenerating a data flow request, regardless of the request being initiated by network or by the UE itself.  Furthermore, Packet Size (P) 308 can provide information indicative of the size of a payload sent/received during a data flow.  For example, the\nnumber of bytes of payload that are sent/received for each data flow, an average number of bytes of payload are sent/received during data flows, and/or number of bytes sent/received based on a type of data flow, etc., can be stored in the profile. \nTypically, this data can be used to predict when the transmission will be completed, if session duration data is incomplete and/or missing.\nAdditionally or alternately, Session duration (S) 310 can indicate how long each data flow transmission will last, from a first byte of payload to a last byte of payload transmission.  Further, Delay tolerance (D) 312 can indicate whether or not\nthe application can tolerant delay and can also specify how many seconds of delay it will allow without compromising service quality or user satisfaction.  As an example, the delay tolerance 312 can be determined by measuring how often the user interacts\nwith the application (e.g., pressing a button related to the application, responding to a prompt of the application, having audio or video played--which can indicate that the user is listening or viewing, etc.).  For instance, web browsing with Web\nSearch Application (App) will likely be profiled as less delay tolerant app than Social Networking App (when the Social Networking App is not opened by user).\nFurthermore, Application Interactivity time (IT) 314 can also be stored within a profile 302.sub.1-N. Moreover, the Application Interactivity time indicates how soon after data reception, does the user utilize/interact with the data.  For\nexample, if a user typically puts the pertinent application in the foreground soon after data reception then the application can be considered less delay tolerant.  In another example, if the user never opens the application within 15 minutes of incoming\ndata receipt then the application can be considered as delay tolerant (e.g., with 15 second delay).  In one aspect, once applications 302.sub.1-N have been flagged as delay tolerant, their continued use may be used to identify an Average Interactivity\nTime for delay tolerant applications.  Average Interactivity Time for delay tolerant applications can be used to identify an optimal bundling interval.  For example, if a delay tolerant application is rarely opened within 15 minutes of data reception\nthen the risk of 15-minute bundling intervals is relatively low.\nIn yet another aspect, service provider, UE manufacturer and/or application creator's/developer's preferences 316 can also be identified (e.g., during provisioning) and stored in a profile 302.sub.1-N. Further, the profile 302.sub.1-N can store\nuser preferences 318, defined by a user, for example, a UE owner.  In one aspect, the user can define user preferences 318 during an initial setup phase, for example, when the application is installed.  However, it can be appreciated that the service\nprovider, UE manufacturer and/or application creator's preferences 316 and/or user preferences 306 can be updated, defined and/or deleted at most any time.  With the above profile keys 304-320, and the time of the day (T) and date or day of the week (W)\n320, the Profile (P) of a given application (e.g., i.sup.th application, wherein i is any natural number from one to infinity) can be defined as: Pi=f(Ii, Fi, Pi, Si, Di, T, W).  It can be appreciated that the profile can be a function of a subset of the\nabove listed profile keys and/or can be a function of most any additional profile keys, which indicate data employed to predict optimization of signaling load.\nFIG. 4 illustrates an example system 400 that facilitates data bundling and fast dormancy based upon intelligent application learning, according to an aspect of the subject innovation.  Specifically, system 400 performs intelligent learning and\ncharacterization of applications on the UE 102, to reduce network signaling in the communication network (e.g., UMTS).  Application activity is monitored and profiled over time and data bundling and/or fast dormancy techniques are tuned to suit. \nMoreover, UE 102, signaling load optimization component 110, AP component 112, and the application profile database 208 can include functionality, as more fully described herein, for example, with regard to systems 100 and 200.\nTypically, the signaling load optimization component 110 can include an AP engine 402 that is utilized to identify whether data flows should be bundled together, and/or whether fast dormancy should be disabled.  In one aspect, the AP engine can\nanalyze profile data (e.g., stored in application profile database 208) associated with an application to classify the application.  For example, applications can be classified as random, delay tolerant, low delay tolerant, etc. Typically, applications\nclassified as random, e.g., due to their randomness of data flows, are not included in final decision for data bundling and/or FD.  According to an aspect, the AP engine 402 can predict when a new data flow will be initiated, based on an analysis (e.g.,\nmathematical/statistical) of profile data and/or machine learning techniques.  Further, the AP engine 402 can employ the analysis and/or classification data to determine whether or not to hold (delay) transmission of a data flow request, based on\nprediction of when a new data flow request will be initiated (e.g., so they can be bundled together and be sent as one data transmission).  For example, if the AP engine 402 can identify a first data flow as delay tolerant and determine that a new data\nflow will be initiated shortly.  In this example scenario, a data bundling (DB) component 404 can be employed to delay the first data flow and bundle the first and second data flows.  Moreover, the DB component 404 can store the first data flow request\nin a bundle cache 408, until the second data flow request is initiated/received.  If the prediction is incorrect and the second data flow request does not arrive within the expected delay, the first data flow can be sent/received, for example, on\nexpiration of a bundling timer.  Further, the AP engine 402 can also identify whether the fast dormancy mechanism performed by a fast dormancy (FD) component 406 can be disabled at the end of a data flow transmission (e.g., due to forecast of new data\nflow request shortly).\nAccording to an embodiment, the AP engine 402 can predict and/or calculate, for an i.sup.th application (wherein i is any natural number from one to infinity), arrival time (Ai) of a next data flow based on the start time of last data flow\n(Ai-1) and profile Pi.  Thus, Ai=f(Pi, Ai-1).  Moreover, the AP engine 402 can simultaneously and/or concurrently calculate a value `A` for each application active in background or foreground, and generate a comprehensive view of a possibility (or\nlikelihood) of concurrence of data flow request from multiple applications, and/or a possibility (or likelihood) and duration of overlapping of data flow requests from multiple applications.  Based on the foregoing, the AP engine 402 can instruct the DB\ncomponent 404 to whether or not to delay transmissions (bundle), and/or the FD component 406 to hold/disable a FD trigger, which transitions the UE 102 into an IDLE state.\nIn general, the AP engine 402 does not apply bundling or FD hold for low delay tolerant applications.  For example, the AP engine 402 can simply generate an invalid input (e.g., such as negative infinite number) for Pi, so no valid output can be\nderived for the low delay tolerant application, leading to no decision (e.g., the DB component 404 and/or the FD component 406 can follow default handling: send data flow request out immediately).  It can be appreciated that user input can override the\nresults generated by the AP engine 402.  Moreover, the AP engine can receive a copy of what a User Interface (UI) function module (not shown) receives and communicate internally at most any time.  This can ensure user perceived latency is kept at the\nsame level as if no bundling is in use.\nFIG. 5 illustrates an example system 500 that employs an artificial intelligence (AI) component 502, which facilitates automating one or more features in accordance with the subject innovation.  It can be appreciated that the UE 102, AP\ncomponent 112, AP engine 402, DB component 404, and FD component 406 can include respective functionality, as more fully described herein, for example, with regard to systems 100, 200, and 400.\nThe subject innovation (e.g., in connection with predicting arrival time, creating profiles, classifying applications, bundling data flows or applying FD hold) can employ various AI-based schemes for carrying out various aspects thereof.  For\nexample, a process for determining whether to delay a data transmission for bundling and/or disable a FD mechanism can be facilitated via an automatic classifier system and process.  Moreover, the classifier can be employed to determine an arrival time\nof a next data flow, generate an application profile, classify applications, identify when bundling or FD hold is to be applied, etc.\nA classifier is a function that maps an input attribute vector, x=(x1, x2, x3, x4, xn), to a confidence that the input belongs to a class, that is, f(x)=confidence(class).  Such classification can employ a probabilistic and/or statistical-based\nanalysis (e.g., factoring into the analysis utilities and costs) to prognose or infer an action that a user desires to be automatically performed.  In the case of communication systems, for example, attributes can be information (e.g., profile keys)\nstored in application profile database 208 and the classes can be categories or areas of interest (e.g., levels of priorities, classification of applications, etc.).\nA support vector machine (SVM) is an example of a classifier that can be employed.  The SVM operates by finding a hypersurface in the space of possible inputs, which the hypersurface attempts to split the triggering criteria from the\nnon-triggering events.  Intuitively, this makes the classification correct for testing data that is near, but not identical to training data.  Other directed and undirected model classification approaches include, e.g., naive Bayes, Bayesian networks,\ndecision trees, neural networks, fuzzy logic models, and probabilistic classification models providing different patterns of independence can be employed.  Classification as used herein also is inclusive of statistical regression that is utilized to\ndevelop models of priority.\nAs will be readily appreciated from the subject specification, the subject innovation can employ classifiers that are explicitly trained (e.g., via a generic training data) as well as implicitly trained (e.g., via observing UE behavior, user\ninteraction, application behavior/activity, application characteristics, receiving extrinsic information, etc.).  For example, SVM's are configured via a learning or training phase within a classifier constructor and feature selection module.  Thus, the\nclassifier(s) can be used to automatically learn and perform a number of functions, including but not limited to determining according to a predetermined criteria when data flows from multiple application can be delayed and bundled together in one\ntransmission, when a FD mechanism can be disabled, how a profile for an application can be populated, how can application can be classified, etc. The criteria can include, but is not limited to, historical patterns, UE behavior, user preferences,\napplication creator preferences, service provider preferences and/or policies, UE device parameters, profile keys, location of the UE, etc.\nFIG. 6 illustrates an example system 600 that facilitates data bundling and FD based on user interactivity (UI), according to an aspect of the subject innovation.  Moreover, system 600 utilizes multiple user interaction indicators to identify\ndata sessions with low \"user awareness sensitivity\" (e.g., user is not actively interactive with device) and bundles the identified data sessions together, thus avoiding multiple single data connections.  Further, FD is avoided when high \"user awareness\nsensitivity\" is detected; thus avoiding unnecessary and/or pre-mature data connection releases and corresponding new data connection setups.  It can be appreciated that the UE 102, AP component 112, AP engine 402, DB component 404, and FD component 406\ncan include respective functionality, as more fully described herein, for example, with regard to systems 100, 200, 400, and 500.\nUE 102 can include a UI component 602 that can be employed to monitor user input and/or data flow requests.  Typically, the UI component 602 can detect time correlations and/or gaps and provide information associated with the UI to the AP engine\n402.  In addition to classification based on delay tolerance, the AP engine 402 can also categorize and treat each data flow request according to \"interactivity\", for example, by considering the user input that preceded it.  As an example, if a data flow\nrequest closely follows user input it can be considered \"interactive\".  In another example, if audio or video playback is in progress the associated data application can be considered \"interactive\".  Alternately, if a data flow request does not follow\nuser input closely or does not include audio or video playback it can be considered \"non-interactive\".\nAccording to an aspect, if a data flow request is initiated more than \"X\" (wherein X can be most any predefined or dynamically adjusted positive rational number) seconds after last user input, the large time distance (X) between user input and\nthe start of the data flow is an indication that the user is not interacting with the device, not waiting for the data and/or not likely to request anything else soon.  In this exemplary case, the AP engine 402 can instruct the DB component 404 to hold\nthe outgoing data request in a bundle cache (e.g., 408) until a bundling timer expires.  Typically, the bundling timer can provide a fixed and/or dynamic delay, which can be adjusted automatically according to load measurements.  After the bundling timer\nhas expired, the DB component 404 can initiate a data flow, process all bundled requests together, and terminate the data flow using fast dormancy immediately after the bundle cache is empty.  The bundle cache can also be emptied by various other events,\nsuch as, but not limited to, a voice call and/or user input, that trigger data flows to begin before the bundling timer has expired.  Some applications shall run in the background without requiring user inputs (e.g., internet radio application).  The AP\nengine 402 can classify these applications in certain conditions as \"high awareness applications\" (e.g., Interactive) which will be excluded from bundling.\nIn addition, based on the interactivity data from the UI component 602, the AP engine 402 can disable FD by the FD component 406.  For example, if a user input occurred less than \"X\" seconds (wherein X can be most any predefined or dynamically\nadjusted positive rational number) before the data flow request is made; the AP engine 402 can classify this data flow request as \"interactive\" and accordingly disable FD.\nIn one aspect, the DB component 404 can also utilize network radio load indications, such as, but not limited to, downlink Ec/Io (ratio of received pilot energy, Ec, to total received energy or the total power spectral density, Io) and/or a\ncurrent uplink interference level (SIB7), to dynamically determine and/or scale the bundling delay (T.sub.b).  For example, if Ec/Io and/or SIB 7 uplink noise are poor, the value of bundling timer (T.sub.b) can be increased, to reduce additional loading\nimpact.  However, if Ec/Io and/or SIB 7 uplink noise are optimal, the value of bundling timer (T.sub.b) can be decreased (e.g., bundling is not performed, or bundling is performed for a very short interval).\nReferring to FIG. 7, there illustrated is an example timeline 700 that depicts when the AP engine 402 can perform data bundling and/or can disable fast dormancy.  Each bar in FIG. 7 can represent a unique data session of the different\napplications (APP1-APP3).  Although only three applications are illustrated, it can be appreciated that the subject disclosure is not so limited and that most any number of applications can be employed for data bundling and/or detecting when fast\ndormancy can be disabled.\nIn one example, at the end of data flow transmission of the second data session of APP 2 (at t.sub.0), fast dormancy would be typically triggered by FD component 406.  In one aspect, if the AP engine 402 predicts a high likelihood of a second\ndata session of APP1 coming in within `Delay 1` seconds by employing profiling function P( ) and if the fast dormancy timer&lt;delay 1&lt;Sum (DCH_to_FACH timer, FACH_to_IDLE timer), then the AP engine 402 can notify the FD component 406 to hold off FD\nand allow the network timers to continue the state transition process.  Accordingly, UE can avoid re-establishing a new RRC connection for the second data session of APP1.\nIn another example, consider a scenario wherein APP1 and APP2 are classified by the AP engine 402 as \"delay tolerant\" applications.  When the fourth data flow request of APP1 is received (at t.sub.1), the AP engine 402 can predict that another\ndata flow request, for example, the fifth data flow request of APP2 will be initiated within a short delay.  Typically, the AP engine 402 can also forecast the amount of delay (e.g., `Delay 2`) and notify the DB component 404 to hold off sending the\nfourth data flow request of APP1 and wait for forecasted delay (e.g., `Delay 2`).  According to one aspect, if the fifth data flow request of APP2 is not initiated before `Delay 2` expires, the AP engine 402 can notify the DB component 404 to release the\nhold and allow the fourth data flow request of APP1 to be sent/received.  Alternately, when the fifth data flow request of APP2 is initiated within the `Delay 2`, the DB component 404 can bundle the fourth data flow request of APP1 and the fifth data\nflow request of APP2, and transmit the data flows together.  In another example scenario, wherein APP3 is profiled by the AP engine 402 as \"low delay tolerant\" and/or \"high awareness\" application (e.g., such as web browsing application, mapping\napplication, gaming application, etc.), a delay will not be applied (e.g., by the DB component 404) for any data flow request to/from APP3.\nFIGS. 8-10 illustrate methodologies and/or flow diagrams in accordance with the disclosed subject matter.  For simplicity of explanation, the methodologies are depicted and described as a series of acts.  It is to be understood and appreciated\nthat the subject innovation is not limited by the acts illustrated and/or by the order of acts, for example acts can occur in various orders and/or concurrently, and with other acts not presented and described herein.  Furthermore, not all illustrated\nacts may be required to implement the methodologies in accordance with the disclosed subject matter.  In addition, those skilled in the art will understand and appreciate that the methodologies could alternatively be represented as a series of\ninterrelated states via a state diagram or events.  Additionally, it should be further appreciated that the methodologies disclosed hereinafter and throughout this specification are capable of being stored on an article of manufacture to facilitate\ntransporting and transferring such methodologies to computers.  The term article of manufacture, as used herein, is intended to encompass a computer program accessible from any computer-readable device or computer-readable storage/communications media.\nReferring now to FIG. 8, illustrated is an example methodology 800 that can be utilized to facilitate data bundling and FD control, based on application learning, according to an aspect of the subject disclosure.  Typically, methodology 800 can\nbe performed by a UE, such as, but not limited to, a cellular phone, a laptop, a tablet, a PC, a PDA, a netbook, a gaming module, a media player, a media recorder, a media viewer, etc. Moreover, one or more applications can be utilized by the UE.  For\nexample, the applications can be installed on the UE during manufacture and/or downloaded/installed at any other time (e.g., by a user, service provider, etc.) from an online app store and/or other software distribution platform.\nAt 802, the applications, for example installed/downloaded on the UE, can be monitored.  Moreover, the characteristics of the applications can be observed and histograms and/or other statistical/historical data analysis tools can be generated\nfor various profile keys associated with the applications, such as, but not limited to, inter-packet arrival time, frequency of use, packet size, session duration, delay tolerance level etc. Typically, the more a given application is used, more the\napplication activity/behavior can be monitored and more accurate the \"learning\" and \"profiling\" for such application will be.  At 804, an application profile can be created and stored for each application.  According to an aspect, the application profile\nincludes the profile keys identified for the application.\nFurther, at 806, the application profiles can be analyzed.  Furthermore, at 808, the applications can be classified based on the analysis.  For example, the applications can be classified as \"random,\" \"delay tolerant,\" and/or \"not delay\ntolerant.\" In addition, at 810, the arrival time of a next data flow can be predicted based on the analysis.  At 812, it can be determined whether the current data flow can be bundled with the next data flow.  In an aspect, the classification of the\ncurrent data flow can be utilized for the determination.  For example, if the current data flow is \"random,\" and/or \"not delay tolerant,\" it can be determined that bundling cannot be performed.  However, if the current data flow is \"delay tolerant\" it\ncan be determined that bundling can be performed.  Accordingly, if determined that bundling can be performed, then at 814, data bundling can be performed, based on the prediction and/or classification.\nTypically, after the current data flow is completed, at 816, it can be determined whether the next data flow is expected to arrive within a short delay.  As an example, the short delay value can be predefined and/or dynamically adjusted and can\ntypically lie between the FD timer expiration value and the sum of the T.sub.DCH.sub._.sub.to.sub._.sub.FACH and T.sub.FACH.sub._.sub.to.sub._.sub.IDLE timer expiration values.  If the next data flow is expected to arrive within the short delay, then at\n818, FD can be disabled.  Alternately, if the next data flow is not expected to arrive within the short delay, then at 820, FD is enabled and can operate normally.\nFIG. 9 illustrates an example methodology 900 that facilitates data bundling of data flows from different applications to reduce network signaling in accordance with an aspect of the subject specification.  UEs enable users to download and\nutilize multiple data applications in the same device, but each application operates independently from the communication network perspective.  Methodology 900 provides automated intelligence to bundle closely spaced data payloads from different\napplications, such that, data packets from different applications can be stacked and sent in one single connection, instead of sending each data packet individually over different connections.  As a result, the amount of signaling events can be\nsubstantially reduced, leading to a decrease in RNC processing load.\nAt 902, a first data flow request associated with an application is received.  At 904, an arrival time of a second data flow request can be determined.  It can be appreciated that the second data flow request can be initiated by the same\napplication or a disparate application.  Moreover, the arrival time of the second data flow request can be predicted based on an analysis of profile data associated with the applications.  Typically, profile data can include historical information and/or\napplication/UE behavior characteristics relating to data flows.  In addition the profile data can also be employed to classify (e.g., random, delay tolerant, not delay tolerant, etc.) the application associated with the first data flow request.\nAccording to an embodiment, at 906, it can be determined whether the application associated with the first data flow request is delay tolerant, for example, based on the classification.  If determined that the application is not delay tolerant,\nthen at 908, the first data flow can be sent/received without data bundling.  In contrast, if determined that the application is delay tolerant, then at 910, the first data flow request can be sent to (and stored within) a bundle cache.  Moreover, at\n912, it can be identified whether the second data flow request has been received.  At 914, the first and second data flows can be sent/received together, if the second data flow request has been received.  If not, at 916, it can be determined whether a\nbundle timer T.sub.b has expired.  If T.sub.b has not expired, the methodology can wait for the second data flow request, as shown at 912, else, if T.sub.b has expired, the first data flow can be sent/received, as shown at 908.  It can be appreciated\nthat although methodology 900 illustrates an example scenario wherein two data flow requests can be bundled, it can be appreciated that the subject disclosure is not so limited, and that most any number of data flow requests can be bundled together.\nFIG. 10 illustrates an example methodology 1000 that overrides a FD mechanism based on application learning, according to an aspect of the subject innovation.  Moreover, methodology 1000 employs historical and statistical based prediction, to\ndetermine whether FD can be temporarily delayed to reduce signaling load.  Specifically, by temporarily delaying/disabling FD, unnecessary and pre-mature data connection releases (and corresponding new data connection setups) can be prevented, resulting\nin reduced signaling load on RNC and improved user perceived latency.\nAt 1002, completion of a first data flow can be detected.  Typically, once the data flow is completed, multiple inactivity timers (T.sub.DCH.sub._.sub.to.sub._.sub.FACH, T.sub.FACH.sub._.sub.to.sub._.sub.IDLE) are initiated by the RNC, upon\nexpiration of which the UE transitions from DCH_to_FACH and FACH to IDLE state.  In addition, the UE can initiate a FD timer (T.sub.FD) to initiate direct transition from DCH to IDLE or FACH to IDLE, before the network inactivity timers expire.  At 1004,\nan arrival time (T.sub.x) of a second data flow request can be predicted.  For example, application behavior can be monitored and/or tracked to generate profile keys, which can be analyzed to facilitate the prediction.  At 1006, it can be determine\nwhether the second data flow request is likely to arrive after the expiration of the FD timer (T.sub.FD), but before the expiration of the network inactivity timers(T.sub.DCH.sub._.sub.to.sub._.sub.FACH, or T.sub.FACH.sub._.sub.to.sub._.sub.IDLE).  In\none example, a high likelihood can be obtained by comparing a calculated probability value to a threshold.  If the probability is lower than the threshold, the likelihood can be determined as low likelihood; else, it can be determined as high likelihood. Accordingly, if T.sub.FD&lt;T.sub.X&lt;(T.sub.DCH.sub._.sub.to.sub._.sub.FACH+T.sub.FACH.- sub._.sub.to.sub._.sub.IDLE), then at 1008, FD can be disabled (or delayed).  However, if\nT.sub.FD&lt;T.sub.X&lt;(T.sub.DCH.sub._.sub.to.sub._.sub.FACH+T.sub.FACH.- sub._.sub.to.sub._.sub.IDLE) does not hold true, then at 1010, FD can be triggered to transition the UE to IDLE state.\nReferring now to FIG. 11, there is illustrated a block diagram of a UE 1100 suitable for reducing RNC load, based on application profiling, in accordance with the innovation.  The UE 1100 can include a processor 1102 for controlling all onboard\noperations and processes.  A memory 1104 can interface to the processor 1102 for storage of data and one or more applications 1106 (e.g., applications 202) being executed by the processor 1102.  A communications component 1108 can interface to the\nprocessor 1102 to facilitate wired/wireless communication with external systems (e.g., femtocell and macro cell).  The communications component 1108 interfaces to a location component 1109 (e.g., GPS transceiver) that can facilitate location detection of\nthe UE 1100.  Note that the location component 1109 can also be included as part of the communications component 1108.\nThe UE 1100 can include a display 1110 for displaying content downloaded and/or for displaying text information related to operating and using the device features.  A serial I/O interface 1112 is provided in communication with the processor 1102\nto facilitate serial communication (e.g., USB, and/or IEEE 1394) via a hardwire connection.  Audio capabilities are provided with an audio I/O component 1114, which can include a speaker for the output of audio signals related to, for example, recorded\ndata or telephony voice data, and a microphone for inputting voice signals for recording and/or telephone conversations.\nThe device 1100 can include a slot interface 1116 for accommodating a subscriber identity module (SIM) 1118.  Firmware 1120 is also provided to store and provide to the processor 1102 startup and operational data.  The UE 1100 can also include\nan image capture component 1122 such as a camera and/or a video decoder 1124 for decoding encoded multimedia content.  The UE 1100 can also include a power source 1126 in the form of batteries, which power source 1126 interfaces to an external power\nsystem or charging equipment via a power I/O component 1128.  In addition, the UE 1100 can be substantially similar to and include functionality associated with UE 102 described supra.  Moreover, UE 1100 can include a signaling load optimization\ncomponent 110, AP component 112, DB component 404, and FD component 406, which can include respective functionality, as more fully described herein, for example, with regard to systems 100, 200, and 400-600.\nNow turning to FIG. 12, such figure depicts an example GSM/GPRS/IP multimedia network architecture 1200 that can employ the disclosed communication architecture.  In particular, the GSM/GPRS/IP multimedia network architecture 1200 includes a GSM\ncore network 1201, a GPRS network 1230 and an IP multimedia network 1238.  The GSM core network 1201 includes a Mobile Station (MS) 1202, at least one Base Transceiver Station (BTS) 1204 and a Base Station Controller (BSC) 1206.  The MS 1202 is physical\nequipment or Mobile Equipment (ME), such as a mobile phone or a laptop computer that is used by mobile subscribers, with a Subscriber identity Module (SIM).  The SIM includes an International Mobile Subscriber Identity (IMSI), which is a unique\nidentifier of a subscriber.  The MS 1202 includes an embedded client 1202a that receives and processes messages received by the MS 1202.  The embedded client 1202a can be implemented in JAVA and is discuss more fully below.  It can be appreciated that MS\n1202 can be substantially similar to UE 102 and include functionality described with respect to UE 102 in systems 100, 200, and 400-600.\nThe embedded client 1202a communicates with an application 1202b (e.g., application(s) 202) that provides services and/or information to an end user.  Additionally or alternately, the MS 1202 and a device 1202c can be enabled to communicate via\na short-range wireless communication link, such as BLUETOOTH.RTM..  As one of ordinary skill in the art would recognize, there can be an endless number of devices 1202c that use the SIM within the MS 1202 to provide services, information, data, audio,\nvideo, etc. to end users.\nThe BTS 1204 is physical equipment, such as a radio tower, that enables a radio interface to communicate with the MS 1202.  Each BTS can serve more than one MS.  The BSC 1206 manages radio resources, including the BTS.  The BSC 1206 can be\nconnected to several BTSs.  The BSC and BTS components, in combination, are generally referred to as a base station (BSS) or radio access network (RAN) 1203.\nThe GSM core network 1201 also includes a Mobile Switching Center (MSC) 1208, a Gateway Mobile Switching Center (GMSC) 1210, a Home Location Register (HLR) 1212, Visitor Location Register (VLR) 1214, an Authentication Center (AuC) 1218, and an\nEquipment Identity Register (EIR) 1218.  The MSC 1208 performs a switching function for the network.  The MSC also performs other functions, such as registration, authentication, location updating, handovers, and call routing.  The GMSC 1210 provides a\ngateway between the GSM network and other networks, such as an Integrated Services Digital Network (ISDN) or Public Switched Telephone Networks (PSTNs) 1220.  In other words, the GMSC 1210 provides interworking functionality with external networks.\nThe HLR 1212 is a database or component(s) that comprises administrative information regarding each subscriber registered in a corresponding GSM network.  The HLR 1212 also includes the current location of each MS.  The VLR 1214 is a database or\ncomponent(s) that contains selected administrative information from the HLR 1212.  The VLR contains information necessary for call control and provision of subscribed services for each MS currently located in a geographical area controlled by the VLR. \nThe HLR 1212 and the VLR 1214, together with the MSC 1208, provide the call routing and roaming capabilities of GSM.  The AuC 1216 provides the parameters needed for authentication and encryption functions.  Such parameters allow verification of a\nsubscriber's identity.  The EIR 1218 stores security-sensitive information about the mobile equipment.\nA Short Message Service Center (SMSC) 1209 allows one-to-one Short Message Service (SMS) messages to be sent to/from the MS 1202.  A Push Proxy Gateway (PPG) 1211 is used to \"push\" (e.g., send without a synchronous request) content to the MS\n1202.  The PPG 1211 acts as a proxy between wired and wireless networks to facilitate pushing of data to the MS 1202.  A Short Message Peer to Peer (SMPP) protocol router 1213 is provided to convert SMS-based SMPP messages to cell broadcast messages. \nSMPP is a protocol for exchanging SMS messages between SMS peer entities such as short message service centers.  It is often used to allow third parties, e.g., content suppliers such as news organizations, to submit bulk messages.\nTo gain access to GSM services, such as speech, data, and short message service (SMS), the MS first registers with the network to indicate its current location by performing a location update and IMSI attach procedure.  The MS 1202 sends a\nlocation update including its current location information to the MSC/VLR, via the BTS 1204 and the BSC 1206.  The location information is then sent to the MS's HLR.  The HLR is updated with the location information received from the MSC/VLR.  The\nlocation update also is performed when the MS moves to a new location area.  Typically, the location update is periodically performed to update the database as location-updating events occur.\nThe GPRS network 1230 is logically implemented on the GSM core network architecture by introducing two packet-switching network nodes, a serving GPRS support node (SGSN) 1232, a cell broadcast and a Gateway GPRS support node (GGSN) 1234.  The\nSGSN 1232 is at the same hierarchical level as the MSC 1208 in the GSM network.  The SGSN controls the connection between the GPRS network and the MS 1202.  The SGSN also keeps track of individual MS's locations, security functions, and access controls.\nA Cell Broadcast Center (CBC) 1233 communicates cell broadcast messages that are typically delivered to multiple users in a specified area.  Cell Broadcast is one-to-many geographically focused service.  It enables messages to be communicated to\nmultiple mobile phone customers who are located within a given part of its network coverage area at the time the message is broadcast.\nThe GGSN 1234 provides a gateway between the GPRS network and a public packet network (PDN) or other IP networks 1236.  That is, the GGSN provides interworking functionality with external networks, and sets up a logical link to the MS through\nthe SGSN.  When packet-switched data leaves the GPRS network, it is transferred to an external TCP-IP network 1236, such as an X.25 network or the Internet.  In order to access GPRS services, the MS first attaches itself to the GPRS network by performing\nan attach procedure.  The MS then activates a packet data protocol (PDP) context, thus activating a packet communication session between the MS, the SGSN, and the GGSN.  In a GSM/GPRS network, GPRS services and GSM services can be used in parallel.  A\nGPRS network 1230 can be designed to operate in three network operation modes (NOM1, NOM2 and NOM3).  A network operation mode of a GPRS network is indicated by a parameter in system information messages transmitted within a cell.  The system information\nmessages dictates a MS where to listen for paging messages and how signal towards the network.  The network operation mode represents the capabilities of the GPRS network.\nThe IP multimedia network 1238 was introduced with 3GPP Release 5, and includes an IP multimedia subsystem (IMS) 1240 to provide rich multimedia services to end users.  A representative set of the network entities within the IMS 1240 are a\ncall/session control function (CSCF), a media gateway control function (MGCF) 1246, a media gateway (MGW) 1248, and a master subscriber database, called a home subscriber server (HSS) 1250.  The HSS 1250 can be common to the GSM network 1201, the GPRS\nnetwork 1230 as well as the IP multimedia network 1238.\nThe IP multimedia system 1240 is built around the call/session control function, of which there are three types: an interrogating CSCF (I-CSCF) 1243, a proxy CSCF (P-CSCF) 1242, and a serving CSCF (S-CSCF) 1244.  The P-CSCF 1242 is the MS's\nfirst point of contact with the IMS 1240.  The P-CSCF 1242 forwards session initiation protocol (SIP) messages received from the MS to an SIP server in a home network (and vice versa) of the MS.  The P-CSCF 1242 can also modify an outgoing request\naccording to a set of rules defined by the network operator (for example, address analysis and potential modification).\nThe I-CSCF 1243 forms an entrance to a home network and hides the inner topology of the home network from other networks and provides flexibility for selecting an S-CSCF.  The I-CSCF 1243 can contact a subscriber location function (SLF) 1245 to\ndetermine which HSS 1250 to use for the particular subscriber, if multiple HSS's 1250 are present.  The S-CSCF 1244 performs the session control services for the MS 1202.  This includes routing originating sessions to external networks and routing\nterminating sessions to visited networks.  The S-CSCF 1244 also decides whether an application server (AS) 1252 is required to receive information on an incoming SIP session request to ensure appropriate service handling.  This decision is based on\ninformation received from the HSS 1250 (or other sources, such as an application server 1252).  The AS 1252 also communicates to a location server 1256 (e.g., a Gateway Mobile Location Center (GMLC)) that provides a position (e.g., latitude/longitude\ncoordinates) of the MS 1202.  The MME 1258 provides authentication of a user by interacting with the HSS 1250 in LTE networks.\nThe HSS 1250 contains a subscriber profile and keeps track of which core network node is currently handling the subscriber.  It also supports subscriber authentication and authorization functions (AAA).  In networks with more than one HSS 1250,\na subscriber location function provides information on the HSS 1250 that contains the profile of a given subscriber.\nThe MGCF 1246 provides interworking functionality between SIP session control signaling from the IMS 1240 and ISUP/BICC call control signaling from the external GSTN networks (not shown).  It also controls the media gateway (MGW) 1248 that\nprovides user-plane interworking functionality (e.g., converting between AMR- and PCM-coded voice).  The MGW 1248 also communicates with a PSTN network 1254 for TDM trunks.  In addition, the MGCF 1246 communicates with the PSTN network 1254 for SS7\nlinks.\nReferring now to FIG. 13, there is illustrated a block diagram of a computer operable to execute the disclosed communication architecture.  In order to provide additional context for various aspects of the subject specification, FIG. 13 and the\nfollowing discussion are intended to provide a brief, general description of a suitable computing environment 1300 in which the various aspects of the specification can be implemented.  While the specification has been described above in the general\ncontext of computer-executable instructions that can run on one or more computers, those skilled in the art will recognize that the specification also can be implemented in combination with other program modules and/or as a combination of hardware and\nsoftware.\nGenerally, program modules include routines, programs, components, data structures, etc., that perform particular tasks or implement particular abstract data types.  Moreover, those skilled in the art will appreciate that the inventive methods\ncan be practiced with other computer system configurations, including single-processor or multiprocessor computer systems, minicomputers, mainframe computers, as well as personal computers, hand-held computing devices, microprocessor-based or\nprogrammable consumer electronics, and the like, each of which can be operatively coupled to one or more associated devices.\nThe illustrated aspects of the specification can also be practiced in distributed computing environments where certain tasks are performed by remote processing devices that are linked through a communications network.  In a distributed computing\nenvironment, program modules can be located in both local and remote memory storage devices.\nComputing devices typically include a variety of media, which can include computer-readable storage media and/or communications media, which two terms are used herein differently from one another as follows.  Computer-readable storage media can\nbe any available storage media that can be accessed by the computer and includes both volatile and nonvolatile media, removable and non-removable media.  By way of example, and not limitation, computer-readable storage media can be implemented in\nconnection with any method or technology for storage of information such as computer-readable instructions, program modules, structured data, or unstructured data.  Computer-readable storage media can include, but are not limited to, RAM, ROM, EEPROM,\nflash memory or other memory technology, CD-ROM, digital versatile disk (DVD) or other optical disk storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or other tangible and/or non-transitory media which\ncan be used to store desired information.  Computer-readable storage media can be accessed by one or more local or remote computing devices, e.g., via access requests, queries or other data retrieval protocols, for a variety of operations with respect to\nthe information stored by the medium.\nCommunications media typically embody computer-readable instructions, data structures, program modules or other structured or unstructured data in a data signal such as a modulated data signal, e.g., a carrier wave or other transport mechanism,\nand includes any information delivery or transport media.  The term \"modulated data signal\" or signals refers to a signal that has one or more of its characteristics set or changed in such a manner as to encode information in one or more signals.  By way\nof example, and not limitation, communication media include wired media, such as a wired network or direct-wired connection, and wireless media such as acoustic, RF, infrared and other wireless media.\nWith reference again to FIG. 13, the example environment 1300 for implementing various aspects of the specification includes a computer 1302, the computer 1302 including a processing unit 1304, a system memory 1306 and a system bus 1308.  The\nsystem bus 1308 couples system components including, but not limited to, the system memory 1306 to the processing unit 1304.  The processing unit 1304 can be any of various commercially available processors.  Dual microprocessors and other\nmulti-processor architectures can also be employed as the processing unit 1304.\nThe system bus 1308 can be any of several types of bus structure that can further interconnect to a memory bus (with or without a memory controller), a peripheral bus, and a local bus using any of a variety of commercially available bus\narchitectures.  The system memory 1306 includes read-only memory (ROM) 1310 and random access memory (RAM) 1312.  A basic input/output system (BIOS) is stored in a non-volatile memory 1310 such as ROM, EPROM, EEPROM, which BIOS contains the basic\nroutines that help to transfer information between elements within the computer 1302, such as during startup.  The RAM 1312 can also include a high-speed RAM such as static RAM for caching data.\nThe computer 1302 further includes an internal hard disk drive (HDD) 1314 (e.g., EIDE, SATA), which internal hard disk drive 1314 can also be configured for external use in a suitable chassis (not shown), a magnetic floppy disk drive (FDD) 1316,\n(e.g., to read from or write to a removable diskette 1318) and an optical disk drive 1320, (e.g., reading a CD-ROM disk 1322 or, to read from or write to other high capacity optical media such as the DVD).  The hard disk drive 1314, magnetic disk drive\n1316 and optical disk drive 1320 can be connected to the system bus 1308 by a hard disk drive interface 1324, a magnetic disk drive interface 1326 and an optical drive interface 1328, respectively.  The interface 1324 for external drive implementations\nincludes at least one or both of Universal Serial Bus (USB) and IEEE 1394 interface technologies.  Other external drive connection technologies are within contemplation of the subject specification.\nThe drives and their associated computer-readable storage media provide nonvolatile storage of data, data structures, computer-executable instructions, and so forth.  For the computer 1302, the drives and storage media accommodate the storage of\nany data in a suitable digital format.  Although the description of computer-readable storage media above refers to a HDD, a removable magnetic diskette, and a removable optical media such as a CD or DVD, it should be appreciated by those skilled in the\nart that other types of storage media which are readable by a computer, such as zip drives, magnetic cassettes, flash memory cards, cartridges, and the like, can also be used in the example operating environment, and further, that any such storage media\ncan contain computer-executable instructions for performing the methods of the specification.\nA number of program modules can be stored in the drives and RAM 1312, including an operating system 1330, one or more application programs 1332, other program modules 1334 and program data 1336.  All or portions of the operating system,\napplications, modules, and/or data can also be cached in the RAM 1312.  It is appreciated that the specification can be implemented with various commercially available operating systems or combinations of operating systems.\nA user can enter commands and information into the computer 1302 through one or more wired/wireless input devices, e.g., a keyboard 1338 and a pointing device, such as a mouse 1340.  Other input devices (not shown) can include a microphone, an\nIR remote control, a joystick, a game pad, a stylus pen, touch screen, or the like.  These and other input devices are often connected to the processing unit 1304 through an input device interface 1342 that is coupled to the system bus 1308, but can be\nconnected by other interfaces, such as a parallel port, an IEEE 1394 serial port, a game port, a USB port, an IR interface, etc.\nA monitor 1344 or other type of display device is also connected to the system bus 1308 via an interface, such as a video adapter 1346.  In addition to the monitor 1344, a computer typically includes other peripheral output devices (not shown),\nsuch as speakers, printers, etc.\nThe computer 1302 can operate in a networked environment using logical connections via wired and/or wireless communications to one or more remote computers, such as a remote computer(s) 1348.  The remote computer(s) 1348 can be a workstation, a\nserver computer, a router, a personal computer, portable computer, microprocessor-based entertainment appliance, a peer device or other common network node, and typically includes many or all of the elements described relative to the computer 1302,\nalthough, for purposes of brevity, only a memory/storage device 1350 is illustrated.  The logical connections depicted include wired/wireless connectivity to a local area network (LAN) 1352 and/or larger networks, e.g., a wide area network (WAN) 1354. \nSuch LAN and WAN networking environments are commonplace in offices and companies, and facilitate enterprise-wide computer networks, such as intranets, all of which can connect to a global communications network, e.g., the Internet.\nWhen used in a LAN networking environment, the computer 1302 is connected to the local network 1352 through a wired and/or wireless communication network interface or adapter 1356.  The adapter 1356 can facilitate wired or wireless communication\nto the LAN 1352, which can also include a wireless access point disposed thereon for communicating with the wireless adapter 1356.\nWhen used in a WAN networking environment, the computer 1302 can include a modem 1358, or is connected to a communications server on the WAN 1354, or has other means for establishing communications over the WAN 1354, such as by way of the\nInternet.  The modem 1358, which can be internal or external and a wired or wireless device, is connected to the system bus 1308 via the serial port interface 1342.  In a networked environment, program modules depicted relative to the computer 1302, or\nportions thereof, can be stored in the remote memory/storage device 1350.  It will be appreciated that the network connections shown are example and other means of establishing a communications link between the computers can be used.\nThe computer 1302 is operable to communicate with any wireless devices or entities operatively disposed in wireless communication, e.g., a printer, scanner, desktop and/or portable computer, portable data assistant, communications satellite, any\npiece of equipment or location associated with a wirelessly detectable tag (e.g., a kiosk, news stand, restroom), and telephone.  This includes at least Wi-Fi and Bluetooth.TM.  wireless technologies.  Thus, the communication can be a predefined\nstructure as with a conventional network or simply an ad hoc communication between at least two devices.\nWi-Fi, or Wireless Fidelity, allows connection to the Internet from a couch at home, a bed in a hotel room, or a conference room at work, without wires.  Wi-Fi is a wireless technology similar to that used in a cell phone that enables such\ndevices, e.g., computers, to send and receive data indoors and out; anywhere within the range of a base station.  Wi-Fi networks use radio technologies called IEEE 802.11 (a, b, g, etc.) to provide secure, reliable, fast wireless connectivity.  A Wi-Fi\nnetwork can be used to connect computers to each other, to the Internet, and to wired networks (which use IEEE 802.3 or Ethernet).  Wi-Fi networks operate in the unlicensed 2.4 and 5 GHz radio bands, at an 11 Mbps (802.11a) or 54 Mbps (802.11b) data\nrate, for example, or with products that contain both bands (dual band), so the networks can provide real-world performance similar to the basic 10BaseT wired Ethernet networks used in many offices.\nAs it employed in the subject specification, the term \"processor\" can refer to substantially any computing processing unit or device comprising, but not limited to comprising, single-core processors; single-processors with software multithread\nexecution capability; multi-core processors; multi-core processors with software multithread execution capability; multi-core processors with hardware multithread technology; parallel platforms; and parallel platforms with distributed shared memory. \nAdditionally, a processor can refer to an integrated circuit, an application specific integrated circuit (ASIC), a digital signal processor (DSP), a field programmable gate array (FPGA), a programmable logic controller (PLC), a complex programmable logic\ndevice (CPLD), a discrete gate or transistor logic, discrete hardware components, or any combination thereof designed to perform the functions described herein.  Processors can exploit nano-scale architectures such as, but not limited to, molecular and\nquantum-dot based transistors, switches and gates, in order to optimize space usage or enhance performance of user equipment.  A processor may also be implemented as a combination of computing processing units.\nIn the subject specification, terms such as \"data store,\" data storage,\" \"database,\" \"cache,\" and substantially any other information storage component relevant to operation and functionality of a component, refer to \"memory components,\" or\nentities embodied in a \"memory\" or components comprising the memory.  It will be appreciated that the memory components, or computer-readable storage media, described herein can be either volatile memory or nonvolatile memory, or can include both\nvolatile and nonvolatile memory.  By way of illustration, and not limitation, nonvolatile memory can include read only memory (ROM), programmable ROM (PROM), electrically programmable ROM (EPROM), electrically erasable ROM (EEPROM), or flash memory. \nVolatile memory can include random access memory (RAM), which acts as external cache memory.  By way of illustration and not limitation, RAM is available in many forms such as synchronous RAM (SRAM), dynamic RAM (DRAM), synchronous DRAM (SDRAM), double\ndata rate SDRAM (DDR SDRAM), enhanced SDRAM (ESDRAM), Synchlink DRAM (SLDRAM), and direct Rambus RAM (DRRAM).\nAdditionally, the disclosed memory components of systems or methods herein are intended to comprise, without being limited to comprising, these and any other suitable types of memory.\nWhat has been described above includes examples of the present specification.  It is, of course, not possible to describe every conceivable combination of components or methodologies for purposes of describing the present specification, but one\nof ordinary skill in the art may recognize that many further combinations and permutations of the present specification are possible.  Accordingly, the present specification is intended to embrace all such alterations, modifications and variations that\nfall within the spirit and scope of the appended claims.  Furthermore, to the extent that the term \"includes\" is used in either the detailed description or the claims, such term is intended to be inclusive in a manner similar to the term \"comprising\" as\n\"comprising\" is interpreted when employed as a transitional word in a claim.", "application_number": "15916424", "abstract": " Data bundling and fast dormancy controls are provided based on\n     application monitoring and classification. Moreover, a balance is enabled\n     between saving battery power of a user equipment (UE) and reducing\n     signaling and processing load in a radio resource controller (RRC). For\n     instance, a system can observe data flow related behavior of applications\n     on the UE. On receiving a first data flow request, an arrival time of a\n     next data flow request is predicted based on an analysis of the behavior,\n     and the system determines whether the two data flows can be bundled\n     together and transmitted over a single connection. Additionally, on\n     completion of the first data flow, the arrival time of the next data flow\n     request is predicted based on the analysis, and the system determines\n     whether a fast dormancy timer can be disabled to transmit the next data\n     flow over the current connection.\n", "citations": ["8060054", "8060154", "8537674", "20050265294", "20070049239", "20070058669", "20070168417", "20080162637", "20090268672", "20090279460", "20090323686", "20100036921", "20100064353", "20100118752", "20100135326", "20100281112", "20100281326", "20110038347", "20110059742", "20110116427", "20110142039", "20110237273", "20110264663", "20110292815", "20110307354", "20110317631", "20110319064", "20120005351", "20120192258", "20130039179"], "related": ["15484195", "14858405", "12947188"]}, {"id": "20180205819", "patent_code": "10375229", "patent_name": "Detecting driving with a wearable computing device", "year": "2019", "inventor_and_country_data": " Inventors: \nKadous; Mohammed Waleed (Santa Clara, CA)  ", "description": "<BR><BR>BACKGROUND\nSome mobile devices (e.g., wearable computing devices, mobile phones, tablet computing devices, vehicle entertainment or navigation systems, etc.) provide a variety of functions that a user may wish to access while traveling in a vehicle.  User\ninteraction with certain functions of a mobile device may be unsafe and/or unlawful when a user of the mobile device is simultaneously operating a vehicle.  To promote safe and/or lawful interaction with the mobile device, some mobile devices enable\nand/or disable certain functions responsive to the mobile device detecting that the mobile device is located in a moving vehicle.  As such, even if a user of the mobile device is merely a passenger in the moving vehicle (and thus is not actually\noperating or driving the vehicle), the mobile device may unnecessarily prevent the user from safely and lawfully accessing one or more functions of the mobile device.\n<BR><BR>SUMMARY\nIn one example, the disclosure is directed to a method that includes detecting that a wearable computing device is located within a moving vehicle, detecting, by the wearable computing device, an indication of movement associated with the\nwearable computing device, and determining, based at least in part on the indication of movement, that a user of the wearable computing device is currently driving the moving vehicle.  The method further includes performing, based on the determination\nthat the user of the wearable computing device is currently driving the moving vehicle, an operation.\nIn another example, the disclosure is directed to a wearable computing device that includes at least one processor and at least one module operable by the at least one processor to detect that the wearable computing device is located within a\nmoving vehicle, detect an indication of movement associated with the wearable computing device and determine, based at least in part on the indication of movement, that a user of the wearable computing device is currently driving the moving vehicle.  The\nat least one module is further operable by the at least one processor to perform, based on the determination that the user of the wearable computing device is currently driving the moving vehicle, an operation.\nIn another example, the disclosure is directed to a method that includes receiving, by a computing system, from a wearable computing device, information that includes one or more indications of movement associated with the wearable computing\ndevice and at least one indication that the wearable computing device is located within a moving vehicle, and determining, by the computing system, based at least in part on the one or more indications of movement and the at least one indication that the\nwearable computing device is located within the moving vehicle, a probability that a user of the wearable computing device is performing an act of driving.  The method further includes responsive to determining that the probability satisfies a\nprobability threshold, determining, by the computing system, that the user of the wearable computing device is currently driving the moving vehicle, and outputting, by the computing system, for transmission to at least one of the wearable computing\ndevice or at least one second computing device, information that configures the at least one of the wearable computing device or the at least one second device to perform an operation.\nThe details of one or more examples are set forth in the accompanying drawings and the description below.  Other features, objects, and advantages of the disclosure will be apparent from the description and drawings, and from the claims.\n<BR><BR>BRIEF DESCRIPTION OF DRAWINGS\nFIG. 1 is a conceptual diagram illustrating an example computing system configured to determine whether a user of a wearable computing device is driving a moving vehicle, in accordance with one or more aspects of the present disclosure.\nFIG. 2 is a block diagram illustrating an example wearable device configured to determine whether a user of the wearable computing device is driving a moving vehicle, in accordance with one or more aspects of the present disclosure.\nFIG. 3 is a block diagram illustrating an example computing device that outputs graphical content for display at a remote device, in accordance with one or more techniques of the present disclosure.\nFIG. 4 is a flowchart illustrating example operations of an example wearable computing device configured to determine whether a user of the wearable computing device is driving a moving vehicle, in accordance with one or more aspects of the\npresent disclosure.\nFIG. 5 is a flowchart illustrating example operations of an example computing system configured to determine whether a user of a wearable computing device is driving a moving vehicle, in accordance with one or more aspects of the present\ndisclosure.\n<BR><BR>DETAILED DESCRIPTION\nIn general, techniques of this disclosure may enable a wearable computing device (e.g., a computerized watch, computerized eyewear, etc.) to perform an operation based on a determination that a user of the wearable computing device (e.g., a\nperson wearing the wearable computing device) is driving a moving vehicle.  When the wearable computing device is located at, on, or within the transportation moving vehicle (e.g., at or near a location of the transportation vehicle, within range of a\nwireless communication signal of the transportation vehicle, etc.) an inference may be made that the user of the wearable computing device is riding in the transportation vehicle.  Based on an indication of movement detected by the wearable computing\ndevice, a determination can be made as to whether the person riding in the moving vehicle is driving the moving vehicle (e.g., by performing an act of driving such as turning a steering wheel, moving a gear shift, etc.).  The wearable computing device\nand/or other computing devices (e.g., a server device, a mobile phone, etc.) may accordingly perform one or more operations (e.g., enabling and/or disabling a function, feature, and/or component of the wearable computing device, outputting information\nfrom the wearable computing device, etc.) if the determination is made that the person is performing driving the moving vehicle (and not merely riding in the transportation vehicle).\nUnlike some mobile computing devices that may enable and/or disable certain features of a device whenever a user is riding in a transportation vehicle, a wearable computing device or other computing devices in accordance with techniques of this\ndisclosure may perform certain operations responsive to first determining whether a user of the wearable computing device is actually driving the transportation vehicle, and not merely a passenger riding in the transportation vehicle.  In this manner,\nthe wearable computing device can promote safe and lawful use of the device without unnecessarily enabling or disabling certain features when a person wearing the wearable computing device is riding in the transportation vehicle.  In other words, if a\nuser of the wearable computing device is merely a passenger of the moving vehicle, and is not actually operating or driving the vehicle, the wearable computing device may be configured to refrain from unnecessarily inhibiting the wearable computing\ndevice from performing certain operations.\nThroughout the disclosure, examples are described where a computing system (e.g., a server, etc.) and/or computing device (e.g., a wearable computing device, etc.) may analyze information (e.g., locations, speeds, accelerations, orientations,\netc.) associated with the computing system and/or computing device, only if the computing system and/or computing device receives permission from a user (e.g., a person wearing a wearable computing device) to analyze the information.  For example, in\nsituations discussed below in which the mobile computing device may collect or may make use of information associated with the user and the computing system and/or computing device, the user may be provided with an opportunity to provide input to control\nwhether programs or features of the computing system and/or computing device can collect and make use of user information (e.g., information about a user's e-mail, a user's social network, social actions or activities, profession, a user's preferences,\nor a user's past and current location), or to dictate whether and/or how to the computing system and/or computing device may receive content that may be relevant to the user.  In addition, certain data may be treated in one or more ways before it is\nstored or used by the computing system and/or computing device, so that personally-identifiable information is removed.  For example, a user's identity may be treated so that no personally identifiable information can be determined about the user, or a\nuser's geographic location may be generalized where location information is obtained (such as to a city, ZIP code, or state level), so that a particular location of a user cannot be determined.  Thus, the user may have control over how information is\ncollected about the user and used by the computing system and/or computing device.\nFIG. 1 is a conceptual diagram illustrating computing system 1 which is configured to determine whether a user of a wearable computing device is driving a moving vehicle, in accordance with one or more aspects of the present disclosure.  System\n1 includes wearable computing device 10, remote computing system 6, mobile computing device 8, and network 34.\nFIG. 1 shows wearable computing device 10 and mobile computing device 8 as each being located within or on a transportation vehicle 2 (e.g., an example moving vehicle).  In the example of FIG. 1, vehicle 2 represents an overhead view of an\nautomobile having four tires 3A-3D and five seats labeled seats 4A-4E.\nThe term \"transportation vehicle\" or \"moving vehicle\" as used herein refers to any machine or apparatus capable of transporting passengers between geographic locations.  Some examples of transportation vehicle 2 include, but are not limited to,\nan automobile, a railway car, a tram, a trolley, a bus, a taxicab, a shuttle, a monorail, an airplane, a ferry, a motorcycle, a snowmobile, a dirt bike, a boat, a ship, a vessel, a water taxi, and a hovercraft.  Transportation vehicles may be\ncommercially owned and operated, privately owned and operated, publicly owned and operated, government owned and operated, military owned and operated, or owned and operated by any other entity.\nThe term \"passenger\" as used herein refers to a person or a user who rides in, on, or otherwise travels with a moving, transportation vehicle and does not drive operate or otherwise control the moving, transportation vehicle.  The terms \"driver\"\nand \"operator\" as used herein refers to a person or user of a device that not only rides in, on, or otherwise travels with a moving, transportation vehicle, but also drives, operates, otherwise controls the moving, transportation vehicle.  A driver is\nnot a passenger.\nThe phrase \"act of driving\" as used herein refers to any type of action that a driver or operator of a transportation vehicle may perform when driving, operating, or otherwise controlling the transportation vehicle.  For example, an act of\ndriving may include, but is not limited to, turning a steering wheel, moving a gear shift, engaging a brake pedal, pressing an acceleration pedal, moving a throttle lever, etc.\nNetwork 34 represents any public or private communication network.  Wearable computing device 10, mobile computing device 8, and remote computing system 6 may send and receive data across network 34 using any suitable communication techniques. \nFor example, wearable computing device 10 may be operatively coupled to network 34 using network link 36A.  Remote computing system 6 may be operatively coupled to network 34 by network link 36B and mobile computing device 8 may be operatively coupled to\nnetwork 34 using network link 36C.\nNetwork 34 may include network hubs, network switches, network routers, etc., that are operatively inter-coupled thereby providing for the exchange of information between wearable computing device 10, mobile computing device 8, and remote\ncomputing system 6.  In some examples, network links 36A, 36B, and 36C may be Ethernet, ATM or other network connections.  Such connections may be wireless and/or wired connections.\nRemote computing system 6 of system 1 represents any suitable mobile or stationary remote computing system, such as one or more desktop computers, laptop computers, mainframes, servers, cloud computing systems, etc. capable of sending and\nreceiving information across network link 36B to network 34.  In some examples, remote computing system 6 represents a cloud computing system that provides one or more services through network 34.  One or more computing devices, such as wearable\ncomputing device 10 and mobile computing device 8, may access the one or more services provided by the cloud using remote computing system 6.  For example, wearable computing device 10 and/or mobile computing device 8 may store and/or access data in the\ncloud using remote computing system 6.  In some examples, some or all the functionality of remote computing system 6 exists in a mobile computing platform, such as a mobile phone, tablet computer, etc. that can travel with transportation vehicle 2.  For\ninstance, some or all the functionality of remote computing system 6 may in some examples reside and execute from within mobile computing device 8.\nRemote computing system 6 includes driving probability module 30 and driving patterns data store 32.  Driving probability module 30 may perform operations described using software, hardware, firmware, or a mixture of hardware, software, and\nfirmware residing in and/or executing at remote computing system 6.  Remote computing system 6 may execute driving probability module 30 with multiple processors or multiple devices.  Remote computing system 6 may execute driving probability module 30 as\na virtual machine executing on underlying hardware.  Driving probability module 30 may execute as a service of an operating system or computing platform.  Driving probability module 30 may execute as one or more executable programs at an application\nlayer of a computing platform.\nData store 32 represents any suitable storage medium for storing actual, modeled, predicted, or otherwise derived patterns of location and sensor data that a machine learning system of driving probability module 30 may accesses to infer whether\na user of a computing device is performing an act of driving.  For example, data store 32 may contain lookup tables, databases, charts, graphs, functions, equations, and the like that driving probability module 30 may access to generate one or more\nrules.  Driving probability module 30 may rely on the rules generated from the information contained at data store 32 to determine whether location and sensor data obtained from wearable computing device 10 and/or mobile computing device 8 indicates that\na person is performing an act of driving.  Remote computing system 6 may provide access to the data stored at data stored 32 as a cloud based service to devices connected to network 34, such as wearable computing device 10 and mobile computing device 8.\nDriving probability module 30 may respond to requests for information (e.g., from wearable computing device 10 and/or mobile computing device 8) indicating whether persons or users of computing devices 10 and 8 are driving or at least performing\nacts of driving.  For instance, driving probability module 30 may receive a request from wearable computing device 10 via network link 36B for a probability indicating whether a person wearing wearable computing device 10 is performing an act of driving\nand/or whether the person is driving transportation vehicle 2.  Driving probability module 30 may receive location and sensor data via link 36B and network 34 from mobile computing device 8 and/or wearable computing device 10 and compare the received\nlocation and sensor data to one or more patterns of location and sensor data stored at data store 32 to derive a probability that the person wearing wearable computing device 10 is driving transportation vehicle 2 or at least performing an act of\ndriving.  Driving probability module 30 may respond to requests for information from wearable computing device 10 and mobile computing device 8 by sending data via network link 36B and through network 34.\nIn some examples, wearable computing device 10 may output, for transmission to remote computing system 6, information comprising an indication of movement (e.g., data indicative of a direction, speed, location, orientation, position, elevation,\netc. of wearable computing device 10.  Responsive to outputting the information comprising the indication of movement, wearable computing device 10 may receive, from remote computing system 6, a probability that a person wearing wearable computing device\nis 10 performing the act of driving.  In other words, whether driving probability module 30 exists at a remote server or a mobile computing platform, wearable computing device 10 may communicate with remote computing system 6 to obtain a probability that\na person wearing wearable computing device 10 is driving transportation vehicle 2.\nIn the example of FIG. 1, mobile computing device 8 is a mobile phone and wearable computing device 10 is a watch.  However other examples of mobile computing device 8 and wearable computing device 10 exist.\nWearable computing device 10 may be any type of computing device, which can be worn, held, or otherwise physically attached to a person driving a transportation vehicle, and which includes one or more processors configured to detect movement of\nthe person while the person is driving the transportation vehicle.  Examples of wearable computing device 10 include, but are not limited to, a watch, computerized eyewear, a computerized headset, a computerized glove, computerized jewelry, or any other\ncombination of hardware, software, and/or firmware that can be used to detect movement of a person who is wearing, holding, or otherwise attached to wearable computing device 10.\nMobile computing device 8 may be any mobile computing device that includes one or more processors configured to perform operations while physically located in a passenger area or compartment of a transportation vehicle, such as transportation\nvehicle 2, while the transportation vehicle is in motion.  Numerous examples of mobile computing device 8 exist and include, but are not limited to, a mobile phone, a tablet computer, a personal digital assistant (PDA), a laptop computer, a portable\ngaming device, a portable media player, an e-book reader, a wearable computing device, or any other combination of hardware, software, and/or firmware that can function while contained within a passenger area of a moving transportation vehicle, such as\ntransportation vehicle 2.  In some examples, mobile computing device 8 represents an onboard computing platform that is built into a transportation vehicle, such as transportation vehicle 2.\nAlthough shown in FIG. 1 as a separate element apart from remote computing system 6, in some examples, mobile computing device 8 may be a remote computing system including functionality of driving probability module 30 for providing a\nprobability that a person is driving transportation vehicle 2.  In other words, although not shown, driving probability module 30 and driving patterns data store 32 may exist locally at mobile computing device 8 and/or may exist locally at wearable\ncomputing device 10, to receive information comprising an indication of movement from wearable computing device 10, determine a probability, based on the indication of movement, that the person wearing wearable computing device 10 is performing an act of\ndriving, and output, for transmission to wearable computing device 10, the probability.\nIn any event, as shown in FIG. 1, wearable computing device 10 includes a user interface device (UID) 12.  UID 12 of wearable computing device 10 may function as an input device for wearable computing device 10 and as an output device.  UID 12\nmay be implemented using various technologies.  For instance, UID 12 may function as an input device using a microphone and as an output device using a speaker to provide an audio based user interface.  UID 12 may function as an input device using a\npresence-sensitive input display, such as a resistive touchscreen, a surface acoustic wave touchscreen, a capacitive touchscreen, a projective capacitance touchscreen, a pressure sensitive screen, an acoustic pulse recognition touchscreen, or another\npresence-sensitive display technology.  UID 12 may function as an output (e.g., display) device using any one or more display devices, such as a liquid crystal display (LCD), dot matrix display, light emitting diode (LED) display, organic light-emitting\ndiode (OLED) display, e-ink, or similar monochrome or color display capable of outputting visible information to the user of wearable computing device 10.\nUID 12 of wearable computing device 10 may include a presence-sensitive display that may receive tactile input from a user of wearable computing device 10.  UID 12 may receive indications of the tactile input by detecting one or more gestures\nfrom a user of wearable computing device 10 (e.g., the user touching or pointing to one or more locations of UID 12 with a finger or a stylus pen).  UID 12 may present output to a user, for instance at a presence-sensitive display.  UID 12 may present\nthe output as a graphical user interface which may be associated with functionality provided by wearable computing device 10.  For example, UID 12 may present various user interfaces of applications executing at or accessible by wearable computing device\n10 (e.g., an electronic message application, a navigation application, an Internet browser application, etc.).  A user may interact with a respective user interface of an application to cause wearable computing device 10 to perform operations relating to\na function.\nFIG. 1 shows that wearable computing device 10 includes one or more sensor devices 14 for capturing location and sensor data associated with wearable computing device 10.  Many examples of sensor devices 14 exist including microphones, cameras,\naccelerometers, gyroscopes, thermometers, galvanic skin response sensors, pressure sensors, barometers, ambient light sensors, heart rate monitors, altimeters, and the like.  One or more sensors 14 may capture location and sensor data and output the\ncaptured location and sensor data to one or more components of wearable computing device 10, such as modules 20, 22, and 24.\nWearable computing device 10 may include user interface (\"UP\") module 20, location module 22, and driver module 24.  Modules 20, 22, and 24 may perform operations described using software, hardware, firmware, or a mixture of hardware, software,\nand firmware residing in and/or executing at wearable computing device 10.  Wearable computing device 10 may execute modules 20, 22, and 24 with multiple processors.  Wearable computing device 10 may execute modules 20, 22, and 24 as a virtual machine\nexecuting on underlying hardware.  Modules 20, 22, and 24 may execute as one or more services of an operating system, a computing platform.  Modules 20, 22, and 24 may execute as one or more remote computing services, such as one or more services\nprovided by a cloud and/or cluster based computing system.  Modules 20, 22, and 24 may execute as one or more executable programs at an application layer of a computing platform.\nUI module 20 may cause UID 12 to present audio (e.g., sounds), graphics, or other types of output (e.g., haptic feedback, etc.) associated with a user interface that a person may use to interact with features and/or functions of wearable\ncomputing device 10.  UI module 20 may receive information from driver module 24 that causes UI module 20 to alter or otherwise change, the presentation of a user interface at UID 12.  For instance, when wearable computing device 10 determines that a\nperson wearing computing device 10 is currently driving a transportation vehicle, driver module 24 may output information to UI module 20 that causes UI module 20 to disable UID 12 to prevent the person from being distracted by the audio, graphics, or\nother types of output that UI module 20 may otherwise cause UID 12 to output.  UI module 20 may receive information from driver module 24 that indicates that the person wearing wearable computing device 10 is not driving a transportation vehicle and may\nenable UID 12 to allow the person to interact with the audio, graphics or other types of output that UI module 20 causes UID 12 to present.\nLocation module 22 may determine whether wearable computing device 10 is within the presence of transportation vehicle 2.  Modules 20 and 24 may receive information (e.g., data) from location module 22 when location module 22 detects a presence\nof transportation vehicle 2.  When wearable computing device 10 is in the presence of transportation vehicle 2, wearable computing device 10 may be located in, on, or otherwise contained within a passenger area of transportation vehicle 2.  For example,\nlocation module 22 may determine whether a location of wearable computing device 10 is within a threshold distance of transportation vehicle 2 based on signal data received by wearable computing device 10 and/or location and sensor data received from\nsensor devices 14.\nFor instance, a communication unit (e.g., near-field-communication radio, Wi-Fi radio, CB radio, cellular radio, Bluetooth radio, etc.) of wearable computing device 10 may receive communication signals from and/or transmit communication signals\nto a communication unit (e.g., near-field-communication radio, Wi-Fi radio, CB radio, Bluetooth radio, etc.) of transportation vehicle 2.  Location module 22 may infer that when the communication unit of wearable computing device 10 is in range of the\ncommunication unit of transportation vehicle 2 that, wearable computing device 10 and transportation vehicle 2 are collocated (e.g., within a threshold distance of each other) or otherwise within the presence of each other.  Location module 22 may infer\nthat when the communication unit of wearable computing device 10 is not in range of the communication unit of transportation vehicle 2, that wearable computing device 10 and transportation unit 2 are not collocated or otherwise within the presence of\neach other.\nLocation module 22 may rely on location and sensor data obtained by sensor devices 14 to determine whether wearable computing device 10 is within a threshold distance of transportation vehicle 2 or otherwise within a presence of transportation\nvehicle 2.  For example, location module 22 may determine a speed associated with wearable computing device 10 based on location and sensor data obtained by sensor devices 14.  If the speed associated with wearable computing device 10 is approximately or\nequal to a threshold speed at which transportation vehicle 2 generally travels at (e.g., an average speed of a moving automobile) location module 22 may determine that wearable computing device 10 is on, in, or otherwise within a presence of\ntransportation vehicle 2.\nIn some examples, wearable computing device 10 may include a global positioning system (GPS) radio for receiving GPS signals (e.g., from a GPS satellite) having location and sensor data corresponding to the current location of wearable computing\ndevice 10.  Location module 22 may include, or otherwise access (e.g., by communicating over network 34 with remote computing system 6) maps and transportation information associated with transportation vehicle 2.  Location module 22 may look up the\ndetermined location of wearable computing device 10 from the maps and transportation information to determine whether wearable computing device is within a presence or threshold distance of a travel route (e.g., a road, a track, etc.) associated with\ntransportation vehicle 2.\nDriver module 24 may determine, infer, or otherwise obtain information indicating that the person wearing wearable computing device 10 is driving transportation vehicle 2.  Driver module 24 may analyze location and sensor data obtained by sensor\ndevices 14 to identify indications of movement that may or may not indicate when a person wearing wearable computing device 10 is driving transportation vehicle 2.  Driver module 24 may communicate with remote computing system 6 via network 34 to obtain\na probability or other information indicating whether location and sensor data obtained by sensor devices 14 of wearable computing device 10 indicates that a person wearing wearable computing device 10 is driving transportation vehicle 2.\nResponsive to determining that the person wearing wearable computing device 10 is driving transportation vehicle 2, driver module 24 may cause wearable computing device 10 to perform an operation.  For example responsive to determining that the\nperson wearing wearable computing device 10 is driving transportation vehicle 2, driver module 24 may output information to UI module 20 that causes UI module 20 to disable or otherwise turn-off UID 12.\nIn some examples, driver module 24 may include features and or capabilities of driving probability module 30 and/or driving patterns data store 32.  In other words, driver module 24 may store information that a machine learning system of driver\nmodule 24 may accesses to infer whether a user of wearable computing device 10 is performing an act of driving.  Driver module 24 may rely on rules generated by the machine learning system to determine whether location and sensor data obtained from\nsensors 14 and/or mobile computing device 8 indicates that a person is performing an act of driving.\nIn some examples, driver module 24 may factor a probability indicating whether a person wearing wearable computing device 10 is driving transportation vehicle 2 with other sources or types of information (e.g., co-presence with others in the\nvehicle, schedule information indicating that the user may typically drive to work at a current time, etc.) to determine that the person wearing wearable computing device 10 is driving transportation vehicle 2.  Said differently, there are other\ncomputation models or information that can be relied on by wearable computing device 10 to determine that a person is driving before performing an operation in response to determining that the person wearing wearable computing device 10 is driving.\nIn accordance with techniques of this disclosure, computing device 10 may detect a presence of transportation vehicle 2.  In other words, computing device 10 may detect that a user of computing device 10 is located within a moving vehicle. \nBefore or after detecting the presence of transportation vehicle 2, wearable computing device 10 may detect an indication of movement associated with wearable computing device 10.\nFor example, location module 22 may obtain signal data received by wearable computing device 10 that includes a Bluetooth communication radio identifier associated with transportation vehicle 2.  Location module 22 may determine that a maximum\nrange associated with Bluetooth signal data is less than a threshold distance for indicating whether wearable computing device 10 is collocated with transportation vehicle 2.  Location module 22 may determine that by receiving the Bluetooth communication\nradio data that the location of wearable computing device 10 is within the threshold distance of transportation vehicle 2 or otherwise indicate the detection of a presence of transportation vehicle 2.\nIn some examples, location module 22 may interpret information contained in signal data received from transportation vehicle 2 to determine that the signal data did in fact originate at transportation vehicle 2.  Some examples of wearable\ncomputing device 10 detecting a presence of transportation vehicle 2 may be examples when location module 22 detects signal data originating from transportation vehicle 2.  In some examples, mobile computing device 8 may detect the presence of\ntransportation vehicle 2 and send information to wearable computing device 10 and/or remote computing system 6 indicating the presence of transportation vehicle 2.\nIn some examples, location module 22 may determine that an acceleration, speed, or direction associated with computing device 10 indicates that a user of computing device 10 is within a moving vehicle.  For instance, if the speed of computing\ndevice 10 exceeds a speed threshold (e.g., 55 miles per hour), the location module 22 may infer that the user of computing device 10 is in a moving vehicle traveling at highway speeds.\nLocation module 22 may provide an alert to driver module 24 that indicates the location of wearable computing device 10 is collocated with transportation vehicle 2.  After receiving the alert, driver module 24 may detect an indication of\nmovement associated with wearable computing device 10.  For instance, driver module 24 may receive accelerometer data, gyroscope data, speedometer data, etc. from sensors 14.  Driver module 24 may detect a change in the location and sensor data from\nsensors 14 indicating that wearable computing device 10 has moved.\nIn response to the indication of movement, driver module 24 may determine whether a person wearing wearable computing device 10 is driving transportation vehicle 2.  For example, driver module 24 may determine whether the person is driving\ntransportation vehicle in order to determine whether to output information to UI module 20 to cause UI module 20 to alter the presentation of a user interface at UID 12 or to otherwise cause wearable computing device 10 to perform an operation if driver\nmodule 24.\nIn some examples, location module 22 may rely on image data captured by a camera of wearable computing device 10 and/or mobile computing device 8 to infer whether the person wearing wearable computing device is within the presence or otherwise\nlocated within transportation vehicle 2.  For instance, wearable computing device 10 may include a camera as one example of sensor devices 14.  Location module 22 may receive image data captured by the camera and compare the captured image data to one or\nmore stored images of vehicle parts (e.g., pedals, steering wheel, gauges, dials, buttons, seats, views from within, etc.) or logos (e.g., car manufacturer logos, etc.).  If location module 22 receives image data that corresponds to one or more of these\nknown or stored images associated with transportation vehicle 2 then location module 22 may alert driver module 24 that the person wearing wearable computing device 10 is located within or in the presence of transportation vehicle 2.\nIn some examples, to determine whether the person is driving, driver module 24 may output the location and sensor data gathered by sensors 14 and/or other information specifying the indication of movement detected by driver module 24 to driving\nprobability module 30 of remote computing system 6.  Based at least in part on the indication of movement received from driver module 24, driving probability module 30 may determine a probability that a person wearing wearable computing device 8 is\nperforming an act of driving.  For example, driving probability module 30 may compare the gyroscopic data, acceleration data, speed data, barometric pressure data, etc. to one or more patterns of location and sensor data stored at driving patterns data\nstore 32.\nA machine learning system of driving probability module 30 may receive the location and sensor data from wearable computing device 10 as input, and by using rules for predicting acts of driving based on location and sensor data, the machine\nlearning system may output a probability that the person wearing the computing device from which the location and sensor data was received, is performing an act of driving.  For example, the machine learning system of driving probability module 30 may\nanalyze barometric pressure data received from wearable computing device 10 to determine relative changes in elevation of wearable computing device 10.  The variation in barometric pressure can be used by the machine learning system to determine small\nchanges in elevation that may indicate whether a person wearing wearable computing device 10 is moving his or hand up and down in a way that is consistent with a pattern of movement associated with driving a vehicle.  For example, a person who wears\nwearable computing device 10 may cause the elevation of wearable computing device 10 to change as the person steers a steering wheel, moves a gear shift, etc.\nThe techniques described herein are not limited to a machine learning system.  For example, driving probability module 30 may rely on a machine learning system as described above, and/or manually engineered heuristics programmed into driving\nprobability module 30 to make a determination as to the probability that a person is performing an act of driving or otherwise driving or operating a moving vehicle.\nIn any event, the machine learning system of driving probability module 30 may produce one or more probabilities indicating whether a person associated with the location and sensor data is performing an act of driving.  Driving probability\nmodule 30 may compare the one or more probabilities to one or more respective probability thresholds for determining whether the person associated with the location and sensor data is driving a transportation vehicle.  In some examples, driving\nprobability module 30 may output the one or more probabilities to driver module 24 and driver module 24 may use the probabilities to determine whether a person wearing wearable computing device 10 is driving transportation vehicle 2.\nResponsive to determining that the probability satisfies a probability threshold, wearable computing device 10 may determine that the person wearing wearable computing device 10 is currently driving the transportation vehicle.  For example,\ndriver module 24 may receive one or more probabilities from remote computing system 6 and compare each of the one or more probabilities to respective probability thresholds to determine whether the values of the probabilities exceed the values (e.g.,\n50%, etc.) of the probability thresholds.  In some examples, if a sufficient quantity of probabilities are satisfied, driver module 24 and/or driving probability module 30 may determine that the person wearing wearable computing device 10 is performing\ndriving transportation vehicle 2.\nIf driver module 24 determines or otherwise receives information indicating that the person wearing wearable computing device 10 is driving, wearable computing device 10 may perform, based on the determination that the person wearing wearable\ncomputing device 10 is the driver, an operation.  For example driver module 24 may output information to UI module 20 indicating that the person wearing wearable computing device 10 is currently driving the transportation vehicle 2.\nIn response to receiving information that the person is that the person driving transportation vehicle 2, UI module 20 may cause UID 12 to refrain from outputting information for display, may enable or disable features provided by wearable\ncomputing device 10, or may cause wearable computing device 10 to perform some other operation.  For example, UI module 20 may enable an audio feedback and voice recognition system in response to receiving information that the person is that the person\ndriving transportation vehicle 2 to prevent the person from viewing or otherwise navigating through information displayed at UID 12.  UI module 20 may disable UID 12 or prevent access to certain features of wearable computing device 10 that may be\ndangerous or otherwise be a distraction to the person while driving transportation vehicle 2.\nIn some examples, wearable computing device 10 may output data indicating that the person is driving to remote computing system 6 for use in generating additional rules that driving probability module 30 or other wearable computing devices may\nuse to determine whether a person is driving.  In some examples, wearable computing device 10 may output information over network 34 to mobile computing device 8 that causes mobile computing device 8 to enable or disable one or more features provided to\nthe person wearing wearable computing device 10 while driving.  For example, mobile computing device 8 may disable text based messaging functions in response to receiving information from wearable computing device 10 that the person is driving.  In other\nexamples, mobile computing device 8 may enable speech-to-text based messaging functions in response to receiving such information.\nIn this way, techniques of this disclosure may enable a wearable computing device to automatically perform one or more operations based on a determination that a person wearing the wearable computing device is also driving a transportation\nvehicle.  By automatically performing operations based on an inference that the person is driving the transportation vehicle, and not just based on a determination that the person is a passenger, the person wearing the wearable computing device may\nperceive the wearable computing device as being more accurate and/or more useful than some other computing devices that may generally perform operations in all instances whenever the person is a driving and a non-driving passenger in a transportation\nvehicle.\nAlthough the example system 1 of FIG. 1 includes a mobile phone and a remote computing device, it should be understood that the techniques of this disclosure may be performed entirely by a wearable computing device such as wearable computing\ndevice 10.  In some examples, the techniques may be mostly performed by a mobile computing device, such as mobile computing device 8, that merely relies on sensor data obtained by wearable computing device 10 to make a determination about whether a\nperson who is wearing a wearable computing device is driving a transportation vehicle.  In some examples, the techniques may be mostly performed by a mobile computing device such as mobile computing device 8 and/or mostly performed by a remote computing\nsystem such as remote computing system 6 that merely relies on sensor data obtained by wearable computing device 10 to make a determination about whether a person who is wearing a wearable computing device is driving a transportation vehicle.\nThroughout the disclosure, examples are described where a computing system (e.g., a server, etc.) and/or computing device (e.g., a wearable computing device, etc.) may analyze information (e.g., locations, speeds, accelerations, orientations,\netc.) associated with the computing system and/or computing device, only if the computing system and/or computing device receives permission from a user (e.g., a person wearing a wearable computing device) to analyze the information.  For example, in\nsituations discussed below in which the mobile computing device may collect or may make use of information associated with the user and the computing system and/or computing device, the user may be provided with an opportunity to provide input to control\nwhether programs or features of the computing system and/or computing device can collect and make use of user information (e.g., information about a user's e-mail, a user's social network, social actions or activities, profession, a user's preferences,\nor a user's past and current location), or to dictate whether and/or how to the computing system and/or computing device may receive content that may be relevant to the user.  In addition, certain data may be treated in one or more ways before it is\nstored or used by the computing system and/or computing device, so that personally-identifiable information is removed.  For example, a user's identity may be treated so that no personally identifiable information can be determined about the user, or a\nuser's geographic location may be generalized where location information is obtained (such as to a city, ZIP code, or state level), so that a particular location of a user cannot be determined.  Thus, the user may have control over how information is\ncollected about the user and used by the computing system and/or computing device.\nFIG. 2 is a block diagram illustrating an example wearable device configured to determine whether a person wearing the wearable computing device is driving a transportation vehicle, in accordance with one or more aspects of the present\ndisclosure.  Wearable computing device 10 of FIG. 2 is described below within the context of system 1 of FIG. 1.  FIG. 2 illustrates only one particular example of wearable computing device 10 of system 1, and many other examples of wearable computing\ndevice 10 may be used in other instances and may include a subset of the components included in example wearable computing device 10 or may include additional components not shown in FIG. 2.\nAs shown in the example of FIG. 2, wearable computing device 10 includes user interface device 12 (\"UID 12\"), one or more sensor devices 14, one or more processors 40, one or more input devices 42, one or more communication units 44, one or more\noutput devices 46, and one or more storage devices 48.  Storage devices 48 of wearable computing device 10 also include UI module 20, location module 22, and driver module 24, and application modules 34A-34N (collectively referred to as, \"application\nmodules 34\").  Driver module 24 includes driving probability module 26 and driving patterns data store 28.  Communication channels 50 may interconnect each of the components 12, 14, 20, 22, 24, 26, 28, 34, 40, 42, 44, and 46 for inter-component\ncommunications (physically, communicatively, and/or operatively).  In some examples, communication channels 50 may include a system bus, a network connection, an inter-process communication data structure, or any other method for communicating data.\nOne or more input devices 42 of wearable computing device 10 may receive input.  Examples of input are tactile, audio, and video input.  Input devices 42 of wearable computing device 10, in one example, includes a presence-sensitive display,\ntouch-sensitive screen, mouse, keyboard, voice responsive system, video camera, microphone or any other type of device for detecting input from a human or machine.\nOne or more output devices 46 of wearable computing device 10 may generate output.  Examples of output are tactile, audio, and video output.  Output devices 46 of wearable computing device 10, in one example, includes a presence-sensitive\ndisplay, sound card, video graphics adapter card, speaker, cathode ray tube (CRT) monitor, liquid crystal display (LCD), or any other type of device for generating output to a human or machine.\nOne or more communication units 44 of wearable computing device 10 may communicate with external devices (e.g., computing device 8, transportation vehicle 2, remote computing system 6, and the like) via one or more networks by transmitting\nand/or receiving network signals on the one or more networks.  For example, wearable computing device 10 may use communication unit 44 to send and receive data to and from remote computing system 6 of FIG. 1.  Wearable computing device 10 may use\ncommunication unit 44 to transmit and/or receive radio signals on a radio network such as a cellular radio network.  Likewise, communication units 44 may transmit and/or receive satellite signals on a satellite network such as a global positioning system\n(GPS) network.  Examples of communication unit 44 include a network interface card (e.g. such as an Ethernet card), an optical transceiver, a radio frequency transceiver, a GPS receiver, or any other type of device that can send and/or receive\ninformation.  Other examples of communication units 44 may include short wave radios, cellular data radios, wireless Ethernet network radios, as well as universal serial bus (USB) controllers.\nIn some examples, UID 12 of wearable computing device 10 may include functionality of input devices 42 and/or output devices 46.  In the example of FIG. 2, UID 12 may be or may include a presence-sensitive input device.  In some examples, a\npresence sensitive input device may detect an object at and/or near a screen.  In one example, a presence-sensitive input device of UID 12 may detect an object, such as a finger or stylus that is within 2 inches or less of the screen.  The\npresence-sensitive input device may determine a location (e.g., an (x,y) coordinate) of a screen at which the object was detected.  In another example range, a presence-sensitive input device may detect an object six inches or less from the screen and\nother ranges are also possible.  The presence-sensitive input device may determine the location of the screen selected by a user's finger using capacitive, inductive, and/or optical recognition techniques.  In some examples, presence sensitive input\ndevice also provides output to a user using tactile, audio, or video stimuli as described with respect to output device 46, e.g., at a display.  UI module 20 may cause UID 12 to present a graphical user interface.  Said differently, UI module 20 may\ncause UID 12 to output a graphical user interface for display at a screen of a display device.\nWhile illustrated as an internal component of wearable computing device 10, UID 12 also represents and external component that shares a data path with wearable computing device 10 for transmitting and/or receiving input and output.  For\ninstance, in one example, UID 12 represents a built-in component of wearable computing device 10 located within and physically connected to the external packaging of wearable computing device 10 (e.g., a screen on a mobile phone).  In another example,\nUID 12 represents an external component of wearable computing device 10 located outside and physically separated from the packaging of wearable computing device 10 (e.g., a monitor, a projector, etc. that shares a wired and/or wireless data path with a\ntablet computer).\nOne or more storage devices 48 within wearable computing device 10 may store information for processing during operation of wearable computing device 10 (e.g., wearable computing device 10 may store data, for instance as driving patterns data\nstore 28, accessed by modules 20, 22, 24, 26, and 34 during execution at wearable computing device 10).  In some examples, storage device 48 is a temporary memory, meaning that a primary purpose of storage device 48 is not long-term storage.  Storage\ndevices 48 on wearable computing device 10 may configured for short-term storage of information as volatile memory and therefore not retain stored contents if powered off.  Examples of volatile memories include random access memories (RAM), dynamic\nrandom access memories (DRAM), static random access memories (SRAM), and other forms of volatile memories known in the art.\nStorage devices 48, in some examples, also include one or more computer-readable storage media.  Storage devices 48 may be configured to store larger amounts of information than volatile memory.  Storage devices 48 may further be configured for\nlong-term storage of information as non-volatile memory space and retain information after power on/off cycles.  Examples of non-volatile memories include magnetic hard discs, optical discs, floppy discs, flash memories, or forms of electrically\nprogrammable memories (EPROM) or electrically erasable and programmable (EEPROM) memories.  Storage devices 48 may store program instructions and/or data associated with modules 20, 22, 24, and 26 and data stores 28.\nOne or more processors 40 may implement functionality and/or execute instructions within wearable computing device 10.  For example, processors 40 on wearable computing device 10 may receive and execute instructions stored by storage devices 48\nthat execute the functionality of UI module 20, location module 22, driver module 24, driving probability module 26, and application modules 34.  These instructions executed by processors 40 may cause wearable computing device 10 to store information,\nwithin storage devices 48 during program execution.  Processors 40 may execute instructions of modules 20, 22, 24, 26, and 34 to cause wearable computing device 10 to execute an operation when a person wearing computing device 10 is driving a\ntransportation vehicle.  For instance, processors 40 may execute instructions of module 20, 22, 24, 26, and 34 to activate a voice-to-text feature of wearable computing device 10 and/or suppress touch-based input at computing device 8 when a person\nwearing wearable computing device 10 is driving.\nApplication modules 34 may include any type of application that computing device 2 may execute in response to determining that a person wearing wearable computing device 10 is driving a transportation vehicle.  For example, application modules\n14 may include a speech-to-text application, a hands-free application, a navigation application, a turn-by-turn driving directions application, a text-to-audio application, an emergency assistance application, a telephone application, or any other type\nof application that may be used by a person wearing a wearable computing device while driving a transportation vehicle.  Application modules 34 may be stand-alone applications or processes.  In some examples, application modules 34 represent only a\nportion or some functionality of one or more other applications or systems.  In some examples, applications modules 34 represent an operating system or computing platform of wearable computing device 34 for executing or controlling features and\noperations performed by other applications.\nIn accordance with techniques of this disclosure, location module 22 of wearable computing device 10 may detect a presence of transportation vehicle 2.  For example, communication units 44 may receive Bluetooth signal data, location data, and/or\nother signal data being transmitted from transportation vehicle 2.  Location module 22 may determine (e.g., based on values attributed to the signal data, an identifier associated with the signal data, etc.) that transportation vehicle 2 generates the\nsignal data.  Location module 22 may determine that the range of the signal data satisfies (e.g., is less than or equal to) a threshold distance (e.g., several feet, several, meters, etc.).  For example, one example range of signal data may be between\nzero and one hundred meters or approximately three hundred thirty feet for a Bluetooth signal.  Bluetooth low energy signals may have a range from between zero to fifty meters or one hundred sixty feet.  In response to receiving a Bluetooth signal,\nlocation module 22 may infer that the origin of the signal data is less than the range (e.g., less than one hundred meters in one example, less than fifty meters in another example, etc.).  Other types of signal data include near field communication\n(NFC).  The range of NFC signals may be on the order of twenty centimeters or less.  Location module 22 may rely on a distance threshold set to a maximum range of a particular type of signal data (e.g., low energy Bluetooth).  If wearable computing\ndevice 10 detects a signal having a maximum range (e.g., 50 meters) that is less than the distance threshold used by location module 22, location module 22 may infer that wearable computing device 10 is in range or in the presence of the origin of the\nsignal data (e.g., transportation vehicle 2).\nIn some examples, location module 22 may rely on movement data detected by a separate mobile computing device (e.g., mobile computing device 8) to determine that wearable computing device 10 is in, on, within or otherwise in the presence of a\ntransportation vehicle such as transportation vehicle 2.  In some examples, wearable computing device 10 may detect a vibration (e.g., from a bumpy road, an engine, etc.) and determine that wearable computing device 10 is within the presence of a\ntransportation vehicle.  For instance, location module 22 may receive accelerometer data or other vibration data from one of sensors 14 and compare the vibration data to a pattern of sensor data associated with a moving vehicle.  Location module 22 may\ndetermine that when the vibration data shares a strong correlation (e.g., greater than 0.5) to a pattern, that wearable computing device 10 may be located within a moving vehicle.\nIn some examples, location module 22 may receive information from a mobile computing device, such as mobile computing device 8, that includes data indicating that the mobile computing device has inferred that wearable computing device 10 is in\nthe presence of transportation vehicle 2.  Said differently, the techniques described above with respect to location module 22 may be performed onboard a mobile computing platform that is separate from wearable computing device 10 (e.g., a mobile phone,\na vehicle computer, etc.) and location module 22 may rely on the determination performed by the separate mobile computing device to determine that wearable computing device 10 is in the presence of transportation vehicle 2.\nLocation module 22 may determine that the receipt of such data, having a range being less than the threshold distance, indicates that wearable computing device 10 is in the presence of transportation vehicle 2.  Location module 22 may output an\nindication (e.g., data) to driver module 24 indicating to driver module 24 that wearable computing device 10 is in the presence of transportation vehicle 2.\nAfter location module 22 of wearable computing device 10 detects the presence of transportation vehicle 2, driver module 24 of wearable computing device 10 may detect an indication of movement associated with wearable computing device 10.  For\ninstance, sensor devices 14 (e.g., an accelerometer, a gyroscope, a barometer, etc.) may capture sensor data that indicates a position, speed, a location, a direction, or other degree of movement associated with wearable computing device 10.  Driver\nmodule 24 may receive one or more indications of movement (e.g., sensor data) from sensors 14 via communication channels 50.\nSaid differently, driver module 24 may receive sensor information from sensors 14 and determine, based at least in part on one or more sensors 14 of wearable computing device 10, at least a portion of the sensor information that indicates at\nleast one of an acceleration of wearable computing device 10, an orientation of wearable computing device 10, and a barometric pressure of wearable computing device 10.  Based on the sensor information, driver module 24 may define the indication of\nmovement for use in determining whether a person wearing computing device 10 is driving.  In other words, driver module 24 may convert the raw sensor data (e.g., gyroscopic data, accelerations, positions, etc.) into one or more indications of movement\n(e.g., data indicating a direction of movement, a speed of movement, an acceleration of movement, etc.) for later use in determining whether a person wearing wearable computing device 10 is driving.\nIn some examples, driver module 24 may detect an indication of movement associated with transportation vehicle 2 and subtract the indication of movement associated with transportation vehicle 2 from the indication of movement associated with\nwearable computing device 10.  In other words, to obtain a more precise indication of movement associated with wearable computing device 10 and/or to eliminate any noise or movement associated with transportation vehicle 2, driver module 24 may filter\nany movement attributable to transportation vehicle 2 from the indication of movement of wearable computing device 10 to determine the probability that the person wearing wearable computing device 10 is driving.  Said differently, driver module 24 may\nisolate the movement that is more likely attributed to a person moving wearable computing device 10 from any movement that is more likely attributed to the movement of transportation vehicle 2.\nDriver module 24 of wearable computing device 10 may invoke driving probability module 26 to determine, based at least in part on the indication of movement, a probability that a person wearing wearable computing device 10 is performing an act\nof driving.  Driver module 24 may invoke driving probability module 30 of remote computing system 6 to determine the probability.  In other words, wearable computing device 10 may in some examples determine a probability that the person wearing wearable\ncomputing device 10 is performing an act of driving, locally, or may rely on a determination of the probability by a remote server.\nDriver module 24 may determine a probability that the person wearing wearable computing device 10 is performing a variety of acts of driving.  For example, driver module 24 may determine a probability of the following acts of driving: turning a\nsteering wheel, shifting a gear shift, changing lanes, staying in a lane, changing acceleration while shifting the gear shift, raising and lowering a hand of the person wearing wearable computing device 10, etc.\nDriving probability module 26 may include similar logic and functionality as driving probability module 30 of remote computing system 6 of system 1 in FIG. 1 to determine the probability.  For instance, driving probability module 26 may use the\nindications of movement (e.g., movement data) determined from the sensor data captured by sensor devices 14 and compare the movement data to one or more stored driving patterns at driving patterns data store 28.\nDriving probability module 26 may compare gyroscopic data, acceleration data, speed data, barometric pressure data, etc. captured by sensor devices 14 to one or more patterns of location and sensor data stored at driving patterns data store 28. \nIn some examples, rather than rely on actual sensor data, driving probability module 26 may compare defined indications of movement indicating orientations, positions, speeds, directions, elevations, etc. defined by sensor data to one or more patterns of\nindications of movement stored at driving patterns data store 28.\nIn any event, a machine learning system of driving probability module 26 may receive the sensor data and from wearable computing device 10 as input, and by using rules for predicting acts of driving based on location and sensor data, the machine\nlearning system may output a probability that the person wearing the computing device from which the location and sensor data was received, is performing an act of driving.\nFor example, the machine learning system of driving probability module 26 may analyze gyroscopic and/or accelerometer data received from wearable computing device 10 to determine relative changes in speed and/or direction of wearable computing\ndevice 10.  The variation in speed and/or direction can be used by the machine learning system to determine small changes in speed and/or direction that may indicate whether a person wearing wearable computing device 10 is moving his or hand or other\nappendage in a way that is consistent with a pattern of movement associated with driving a vehicle.  For example, a person who wears wearable computing device 10 may cause the direction and/or speed of wearable computing device 10 to change as the person\nmoves a gear shift, steers a steering wheel, etc.\nThe machine learning system of driving probability module 26 may produce one or more probabilities indicating whether a person associated with the location and sensor data is performing an act of driving.  Driving probability module 26 may\ncompare the one or more probabilities to one or more respective probability thresholds for determining whether the person associated with the location and sensor data is driving transportation vehicle 2.  In some examples, driving probability module 26\nmay output the one or more probabilities to driver module 24 and driver module 24 may use the probabilities to determine whether a person wearing wearable computing device 10 is driving transportation vehicle 2.\nResponsive to determining that the probability of the person who wears wearable computing device 10 satisfies a probability threshold, driver module 24 may determine that the person wearing the wearable computing device is currently driving\ntransportation vehicle 2.  In other words, driver module 24 may compare the probability that the person is performing an act of driving to determine whether the probability is high enough and therefore indicates that the person is likely driving\ntransportation vehicle 2.\nIn some examples, driver module 24 may determine that the person is driving if and when a single probability that a person is performing any one act of driving satisfies a threshold.  In some examples, driver module 24 may compute a weighted\naverage of probabilities of multiple acts of driving to determine whether the overall weighted probability satisfies the threshold for indicating that the person is driving.  For example, driver module 24 may determine a weighted average of the\nprobability that the person is turning a steering wheel, shifting a gear shift, and operating a pedal of a transportation vehicle.  Driver module 24 may determine that the person is driving if the weighted average probability of the multiple acts of\ndriving satisfies the threshold.  In any event, driver module 24 may output information to other modules of wearable computing device 10, such as UI module 20 and/or application modules 34, to cause wearable computing device 10 to perform an operation.\nWearable computing device 10 may perform, based on the determination that the person wearing wearable computing device 10 is currently driving transportation vehicle 2, an operation.  For example, one or more applications 34 (e.g., as part of an\napplication, process, platform, operating system, etc.) executing at wearable computing device 10 may limit access to certain features of the applications 34 in response to receiving information from driver module 24 that the person wearing wearable\ncomputing device 10 is currently driving.  In some examples, one or more other applications or operating systems executing remotely to wearable computing device 10 (e.g., at mobile computing device 8) may receive the indication from driver module 24 that\nthe person is driving and in response to receiving the information from driver module 24, these one or more other applications or operating systems may limit access to certain features.\nSaid differently, wearable computing device 10 may restrict access to at least some functionality of an application or operating system (e.g., applications 34) being executed by at least one of the wearable computing device or a second computing\ndevice.  For example, mobile computing device 8 may prevent a person from accessing a virtual keyboard or a messaging application associated with mobile computing device 8 when the person is driving, wearable computing device 10 may prevent UID 12 from\nreceiving input when the person is driving, etc.\nIn some examples, in performing the operation in response to determining that the person is driving, wearable computing device 10 may output, for transmission to at least one second computing device, information usable by the at least one second\ncomputing device to learn driving habits of the person that is wearing the wearable computing device and/or to learn a driving route associated with the person.  Said differently, through communication units 44, driver module 24 may output indications\n(e.g., data) to remote computing system 6 containing information specifying when, where, and how driver module 24 determines that the person is driving, information specifying sensor data captured by sensor devices 14 used to discern that the person is\ndriving, information for providing driving patterns or driving habits of the person that is driving, and other types of information about the time and/or location of the person and wearable computing device 10 when wearable computing device 10 determined\nthat the person is driving.  Remote computing system 6 may use the information received from wearable computing device 10 when the person is driving to generate one or more rules of a machine learning system for predicting when the person and/or other\npersons of other wearable computing devices are driving, where the person and/or other persons may drive and the route or routes the person and/or other persons may take.\nIn some examples, in performing the operation in response to determining that the person is driving, wearable computing device 10 may deactivate a display device that is operatively coupled to at least one of wearable computing device 10 and a\nsecond computing device.  For instance, UI module 20 may receive information from driver module 24 indicating that the person is driving and in response, deactivate UID 12 and cause UID 12 to cease outputting information for display.  Mobile computing\ndevice 8 (e.g., the person's mobile phone) may receive information from driver module 24 indicating that the person is driving and in response, deactivate a screen, a display device, or other input/output device of mobile computing device 8.\nSome computing systems and/or devices may perform operations or offer features depending on whether a user of such a system is actually driving or whether the person is simply riding in a transportation vehicle.  For example, a system or device\nthat provides a contextual user interface (e.g., an interface that may change depending on the time of day, user's emotion, user's location, etc.) may rely on information that indicates whether the user of the system is driving.  One objective of such a\nsystem or device may be to promote safety and \"save lives\" by \"locking out\" a messaging service (e.g., SMS) provided by the system or device while the user is driving.  Such a system or device may be unable to determine whether the person is driving\nbased on accelerometer data detected by a mobile phone since both a driver and a passenger will see similar acceleration values regardless whether either is actually driving.  Hence, some systems or devices may require the ability to detect wither a\nperson is driving to offer such functionality with accuracy and without annoying non-driving passengers.  A system (e.g., a wearable computing device, a server, a mobile device, etc.) in accordance with techniques of this disclosure may offer this type\nof capability.\nBy using data obtained by a wearable computing device (e.g., a smart watch), a computing system and/or device can receive accurate information identifying whether a person is a driver or a passenger.  The techniques may also provide a way, for\nexample, for a system and/or device to learn and begin to understand historical driving patterns (e.g., how much of the time was the user a passenger and how much were they a driver).  The techniques may also be used to eliminate and improve the\nprediction of activities (e.g. if we detect that the user is driving, then they are probably not on a bus).  In some examples, low energy beacons (e.g., Bluetooth beacons) may be located at opposite sides of a transportation vehicle to internally\nlocalize the position of a person who wears a wearable computing device to determine whether that person is driving or a passenger.\nFIG. 3 is a block diagram illustrating an example computing device that outputs graphical content for display at a remote device, in accordance with one or more techniques of the present disclosure.  Graphical content, generally, may include any\nvisual information that may be output for display, such as text, images, a group of moving images, etc. The example shown in FIG. 3 includes computing device 100, presence-sensitive display 101, communication unit 110, projector 120, projector screen\n122, mobile device 126, and visual display device 130.  Although shown for purposes of example in FIGS. 1 and 2 as a stand-alone wearable computing device 10, a computing device such as computing devices 10 and 100 may, generally, be any component or\nsystem that includes a processor or other suitable computing environment for executing software instructions and, for example, need not include a presence-sensitive display.\nAs shown in the example of FIG. 3, computing device 100 may be a processor that includes functionality as described with respect to processor 40 in FIG. 2.  In such examples, computing device 100 may be operatively coupled to presence-sensitive\ndisplay 101 by a communication channel 102A, which may be a system bus or other suitable connection.  Computing device 100 may also be operatively coupled to communication unit 110, further described below, by a communication channel 102B, which may also\nbe a system bus or other suitable connection.  Although shown separately as an example in FIG. 3, computing device 100 may be operatively coupled to presence-sensitive display 101 and communication unit 110 by any number of one or more communication\nchannels.\nIn other examples, such as illustrated previously by wearable computing device 10 and computing device 8 in FIGS. 1-2, a computing device may refer to a portable or mobile device such as mobile phones (including smart phones), laptop computers,\ncomputing watches, computing eye glasses, wearable computing devices, etc. In some examples, a computing device may be a desktop computers, tablet computers, smart television platforms, cameras, personal digital assistants (PDAs), servers, mainframes,\netc.\nPresence-sensitive display 101 may include display device 103 and presence-sensitive input device 105.  Display device 103 may, for example, receive data from computing device 100 and display the graphical content.  In some examples,\npresence-sensitive input device 105 may determine one or more inputs (e.g., continuous gestures, multi-touch gestures, single-touch gestures, etc.) at presence-sensitive display 101 using capacitive, inductive, and/or optical recognition techniques and\nsend indications of such input to computing device 100 using communication channel 102A.  In some examples, presence-sensitive input device 105 may be physically positioned on top of display device 103 such that, when a user positions an input unit over\na graphical element displayed by display device 103, the location at which presence-sensitive input device 105 corresponds to the location of display device 103 at which the graphical element is displayed.  In other examples, presence-sensitive input\ndevice 105 may be positioned physically apart from display device 103, and locations of presence-sensitive input device 105 may correspond to locations of display device 103, such that input can be made at presence-sensitive input device 105 for\ninteracting with graphical elements displayed at corresponding locations of display device 103.\nAs shown in FIG. 3, computing device 100 may also include and/or be operatively coupled with communication unit 110.  Communication unit 110 may include functionality of communication unit 44 as described in FIG. 2.  Examples of communication\nunit 110 may include a network interface card, an Ethernet card, an optical transceiver, a radio frequency transceiver, or any other type of device that can send and receive information.  Other examples of such communication units may include Bluetooth,\n3G, and Wi-Fi radios, Universal Serial Bus (USB) interfaces, etc. Computing device 100 may also include and/or be operatively coupled with one or more other devices, e.g., input devices, output devices, memory, storage devices, etc. that are not shown in\nFIG. 3 for purposes of brevity and illustration.\nFIG. 3 also illustrates a projector 120 and projector screen 122.  Other such examples of projection devices may include electronic whiteboards, holographic display devices, and any other suitable devices for displaying graphical content. \nProjector 120 and projector screen 122 may include one or more communication units that enable the respective devices to communicate with computing device 100.  In some examples, the one or more communication units may enable communication between\nprojector 120 and projector screen 122.  Projector 120 may receive data from computing device 100 that includes graphical content.  Projector 120, in response to receiving the data, may project the graphical content onto projector screen 122.  In some\nexamples, projector 120 may determine one or more inputs (e.g., continuous gestures, multi-touch gestures, single-touch gestures, etc.) at projector screen 122 using optical recognition or other suitable techniques and send indications of such input\nusing one or more communication units to computing device 100.  In such examples, projector screen 122 may be unnecessary, and projector 120 may project graphical content on any suitable medium and detect one or more user inputs using optical recognition\nor other such suitable techniques.\nProjector screen 122, in some examples, may include a presence-sensitive display 124.  Presence-sensitive display 124 may include a subset of functionality or all of the functionality of UI device 4 as described in this disclosure.  In some\nexamples, presence-sensitive display 124 may include additional functionality.  Projector screen 122 (e.g., an electronic display of computing eye glasses), may receive data from computing device 100 and display the graphical content.  In some examples,\npresence-sensitive display 124 may determine one or more inputs (e.g., continuous gestures, multi-touch gestures, single-touch gestures, etc.) at projector screen 122 using capacitive, inductive, and/or optical recognition techniques and send indications\nof such input using one or more communication units to computing device 100.\nFIG. 3 also illustrates mobile device 126, visual display device 130, and wearable computing device 134.  Devices 126, 130, and 134 may each include computing and connectivity capabilities.  One example of mobile device 126 may be computing\ndevice 8 of FIG. 1.  Other examples of mobile device 126 may include e-reader devices, convertible notebook devices, and hybrid slate devices.  Examples of visual display devices 130 may include other semi-stationary devices such as televisions, computer\nmonitors, etc. One example of wearable computing device 134 may be wearable computing device 10 of FIG. 1.  Other examples of wearable computing device 134 include computerized watches, computerized eyeglasses, etc.\nAs shown in FIG. 3, mobile device 126 may include a presence-sensitive display 128.  Visual display device 130 may include a presence-sensitive display 132.  Wearable computing device 134 may include a presence-sensitive display 136. \nPresence-sensitive displays 128, 132, and 136 may include a subset of functionality or all of the functionality of UID 12 as described in this disclosure.  In some examples, presence-sensitive displays 128, 132, and 136 may include additional\nfunctionality.  In any case, presence-sensitive displays 128, 132, and 136, for example, may receive data from computing device 100 and display the graphical content.  In some examples, presence-sensitive displays 128, 132, and 136 may determine one or\nmore inputs (e.g., continuous gestures, multi-touch gestures, single-touch gestures, etc.) at projector screen using capacitive, inductive, and/or optical recognition techniques and send indications of such input using one or more communication units to\ncomputing device 100.\nAs described above, in some examples, computing device 100 may output graphical content for display at presence-sensitive display 101 that is coupled to computing device 100 by a system bus or other suitable communication channel.  Computing\ndevice 100 may also output graphical content for display at one or more remote devices, such as projector 120, projector screen 122, mobile device 126, visual display device 130, and wearable computing device 134.  For instance, computing device 100 may\nexecute one or more instructions to generate and/or modify graphical content in accordance with techniques of the present disclosure.  Computing device 100 may output the data that includes the graphical content to a communication unit of computing\ndevice 100, such as communication unit 110.  Communication unit 110 may send the data to one or more of the remote devices, such as projector 120, projector screen 122, mobile device 126, visual display device 130, and/or wearable computing device 134. \nIn this way, computing device 100 may output the graphical content for display at one or more of the remote devices.  In some examples, one or more of the remote devices may output the graphical content at a presence-sensitive display that is included in\nand/or operatively coupled to the respective remote devices.\nIn some examples, computing device 100 may not output graphical content at presence-sensitive display 101 that is operatively coupled to computing device 100.  In other examples, computing device 100 may output graphical content for display at\nboth a presence-sensitive display 101 that is coupled to computing device 100 by communication channel 102A, and at one or more remote devices.  In such examples, the graphical content may be displayed substantially contemporaneously at each respective\ndevice.  For instance, some delay may be introduced by the communication latency to send the data that includes the graphical content to the remote device.  In some examples, graphical content generated by computing device 100 and output for display at\npresence-sensitive display 101 may be different than graphical content display output for display at one or more remote devices.\nComputing device 100 may send and receive data using any suitable communication techniques.  For example, computing device 100 may be operatively coupled to external network 114 using network link 112A.  Each of the remote devices illustrated in\nFIG. 3 may be operatively coupled to network external network 114 by one of respective network links 112B, 112C, and 112D.  External network 114 may include network hubs, network switches, network routers, etc., that are operatively inter-coupled thereby\nproviding for the exchange of information between computing device 100 and the remote devices illustrated in FIG. 3.  In some examples, network links 112A-112D may be Ethernet, ATM or other network connections.  Such connections may be wireless and/or\nwired connections.\nIn some examples, computing device 100 may be operatively coupled to one or more of the remote devices included in FIG. 3 using direct device communication 118.  Direct device communication 118 may include communications through which computing\ndevice 100 sends and receives data directly with a remote device, using wired or wireless communication.  That is, in some examples of direct device communication 118, data sent by computing device 100 may not be forwarded by one or more additional\ndevices before being received at the remote device, and vice-versa.  Examples of direct device communication 118 may include Bluetooth, Near-Field Communication, Universal Serial Bus, Wi-Fi, infrared, etc. One or more of the remote devices illustrated in\nFIG. 3 may be operatively coupled with computing device 100 by communication links 116A-116D.  In some examples, communication links 112A-112D may be connections using Bluetooth, Near-Field Communication, Universal Serial Bus, infrared, etc. Such\nconnections may be wireless and/or wired connections.\nComputing device 100 may be operatively coupled to visual display device 130 using external network 114.  Computing device 100 may determine, based on one or more indications of movement and at least one indication that wearable computing device\n134 is in the presence of transportation vehicle 2, a probability that a person wearing wearable computing device 134 is performing an act of driving.  For example, a driving probability module of computing device 100 may obtain information from wearable\ncomputing device 134 that includes one or more indications (e.g., data) of movement associated with wearable computing device 134.  The movement data may indicate an acceleration, an orientation, and/or an elevation of wearable computing device 134.  The\ndriving probability module of computing device 100 may also receive information from wearable computing device 134 indicating that wearable computing device 134 is communicating via Bluetooth with transportation vehicle 2.  Computing device 100 may infer\nthat wearable computing device 134 is in the presence of transportation vehicle 2 when wearable computing device 134 is communicating via Bluetooth with transportation vehicle 2\nThe driving probability module of computing device 100 may compare the indications of movement associated with wearable computing device 134 to one or more patterns of movement associated with driving actions (e.g., turning a steering wheel,\ndepressing a pedal, moving a gear shift, and the like).  For example, computing device 100 may compare the movement data to indications of movement associated with turning a steering wheel of transportation vehicle 2, moving a gear shift of\ntransportation vehicle 2, etc. Computing device 100 may compute an overall probability that the person wearing wearable computing device 134 is performing an act of driving if the patterns associated with any one of the driving acts match (e.g., have a\ncorrelation value greater than 0.5 or 50%) the pattern of movement associated with the movement data.\nComputing device 100 may compare the probability that the person wearing wearable computing device 134 is performing an act of driving to a probability threshold for determining whether the person is driving.  Computing device 100 may determine\nthat the probability satisfies the probability threshold and, in response, output information to wearable computing device 134 to configure device 134 to perform an operation, an action, and/or otherwise provide a function related to driving.  For\nexample, computing device 100 may output graphical information for transmission to device 134 in addition to providing instructions for causing wearable computing device 134 device to present a graphical user interface at presence-sensitive screen 136\nbased on the graphical information.  Computing device 100 may send the graphical information to wearable computing device 134 via communication unit 110 and external network 114.  Presence-sensitive screen 128 may receive the information and present a\ngraphical indication that computing device 100 predicts the person wearing wearable computing device 134 is driving.\nFIG. 4 is a flowchart illustrating example operations of an example wearable computing device configured to determine whether a person wearing the wearable computing device is driving a transportation vehicle, in accordance with one or more\naspects of the present disclosure.  The process shown in FIG. 4 may be performed by one or more processors of a computing device, such as wearable computing devices 10 and 100 illustrated in FIG. 1, FIG. 2, and FIG. 3.  For purposes of illustration, FIG.\n4 is described below within the context of computing system 1 of FIG. 1.\nWearable computing device 10 may detect that the wearable computing device is located within a moving vehicle (200).  For example, location module 22 may correlate signal data having an identifier of transportation vehicle 2 indicates that\nwearable computing device 10 is in the presence of transportation vehicle 2.\nAfter wearable computing device 10 detects the presence of the transportation vehicle, wearable computing device 10 may detect an indication of movement associated with wearable computing device 10 (210).  For instance, driver module 24 of\nwearable computing device 10 may receive signal data obtained by an accelerometer, a gyroscope, a barometer, or other sensor of wearable computing device 10 and based on that signal data, driver module 24 may define one or more indications of movement\nassociated with wearable computing device 10.  The one or more indications of movement may indicate a direction, a speed, an orientation, etc. of wearable computing device 10.\nWearable computing device 10 may determine, based at least in part on the indication of movement, a probability that a user of the wearable computing device 10 is performing an act of driving (220).  For example, wearable computing device 10 may\noutput, data or other forms of information that define the one or more indications of movement associated with wearable computing device 10 to mobile computing device 8 and/or remote computing system 6.  Driving probability module 30 of remote computing\nsystem 6 or a similar driving probability module of mobile computing device 8, may analyze the information received from wearable computing device 10 to determine whether the information matches sensor data associated with one or more predefined and/or\nlearned driving patterns.  Remote computing system 6 and/or mobile computing device 8 may computer one or more probabilities that the person wearing wearable computing device 10 is performing an act of driving.  Wearable computing device 10 may then\nreceive back, from mobile computing device 8 and/or remote computing system 6, information containing the one or more probabilities that the person wearing wearable computing device 10 is performing an act of driving.\nSaid differently, wearable computing device 10 may output, for transmission to at least one second computing device, information comprising the one or more indications of movement.  Responsive to outputting the information comprising the one or\nmore indications of movement, wearable computing device 10 may receive, from the at least one second computing device, the probability that the person wearing wearable computing device 10 is performing the act of driving.\nIn some examples, wearable computing device 10 may determine a relative elevation between wearable computing device 10 and at least one second computing device associated with the person wearing the wearable computing device, and determine,\nbased at least in part on the relative elevation, the probability that the person wearing wearable computing device 10 is performing the act of driving.  For example, the person wearing wearable computing device 10 may also rely on mobile computing\ndevice 8, for instance, to make phone calls.  If the person is driving while wearing wearable computing device 10, for example, on his or her wrist, he or she may place his or her phone (e.g., mobile computing device 8) in a console of transportation\nvehicle 2 or in his or her pocket.  In any event, the elevation of mobile computing device 8 while either in the console or in the pocket may remain fixed, relative to the interior of the passenger compartment of transportation vehicle 2.  In contrast,\nthe elevation of wearable computing device 10 may change relative to the interior of the passenger compartment of transportation vehicle 2 (e.g., as the person moves the arm at which wearable computing device 10 is attached as the person performs various\nacts of driving).  Wearable computing device 10 may identify changes in the difference between the elevation of mobile computing device 8 and wearable computing device 10 to determine when the person is driving.\nFor example, driver module 24 of wearable computing device 10 may receive sensor information from mobile computing device 8 that contains barometric pressure information associated with mobile computing device 8.  In addition, driver module 24\nmay obtain barometric pressure information from one or more sensor devices 14 of wearable computing device 10.  Driver module 24 of wearable computing device 10 may determine, based on a barometric pressure associated with each of wearable computing\ndevice 10 and mobile computing device 8 (e.g., at least one second computing device), the relative elevation between wearable computing device 10 and mobile computing device 8 associated with the person wearing the wearable computing device.  Wearable\ncomputing device 10 may compare the relative elevation over time to one or more patterns stored at driving patterns data store 28 and/or rely on a machine learning system of driving probability module 26 to determine, based at least in part on the\nrelative elevation, the probability that the person wearing wearable computing device 10 is performing the act of driving.\nIn some examples, wearable computing device 10 may determine a relative acceleration, degree of tilt, or orientation between wearable computing device 10 and at least one second computing device associated with the person wearing the wearable\ncomputing device, and determine, based at least in part on the relative acceleration, degree of tilt, or orientation, the probability that the person wearing wearable computing device 10 is performing the act of driving.  For example, the person wearing\nwearable computing device 10 may also rely on mobile computing device 8, for instance, to make phone calls.  If the person is driving while wearing wearable computing device 10, for example, on his or her wrist, he or she may place his or her phone\n(e.g., mobile computing device 8) in a console of transportation vehicle 2 or in his or her pocket.  In any event, the acceleration of mobile computing device 8 while either in the console or in the pocket may remain fixed, relative to the interior of\nthe passenger compartment of transportation vehicle 2.  In contrast, the acceleration of wearable computing device 10 may change relative to the interior of the passenger compartment of transportation vehicle 2 (e.g., as the person moves the arm at which\nwearable computing device 10 is attached as the person performs various acts of driving).  Wearable computing device 10 may identify changes in the difference between the acceleration of mobile computing device 8 and wearable computing device 10 to\ndetermine when the person is driving.\nFor example, driver module 24 of wearable computing device 10 may receive sensor information from mobile computing device 8 that contains accelerometer data, a degree of tilt from a tilt sensor of mobile computing device 8, or a degree of\norientation from a gyroscope of mobile computing device 8.  In addition, driver module 24 may obtain an acceleration, a degree of tilt, or a degree of orientation from one or more sensor devices 14 of wearable computing device 10.  Driver module 24 of\nwearable computing device 10 may determine, based on an acceleration, a degree of tilt, or a degree of orientation associated with each of wearable computing device 10 and mobile computing device 8 (e.g., at least one second computing device), the\nrelative acceleration, degree of tilt, or orientation between wearable computing device 10 and mobile computing device 8 associated with the person wearing the wearable computing device.  Wearable computing device 10 may compare the relative\nacceleration, degree of tilt, or orientation over time to one or more patterns stored at driving patterns data store 28 and/or rely on a machine learning system of driving probability module 26 to determine, based at least in part on the relative\nacceleration, degree of tilt, or orientation, the probability that the person wearing wearable computing device 10 is performing the act of driving.\nResponsive to determining that the probability satisfies a probability threshold (230), wearable computing device 10 may determine that the user of the wearable computing device 10 is currently driving the moving vehicle (240).  For instance,\ndriver module 24 of wearable computing device 10 may compare the probability to a fifty percent threshold and if the probability is greater than fifty percent, determine that the person is likely driving transportation vehicle 2.\nWearable computing device 10 may perform, based on the determination that the person wearing wearable computing device 10 is currently driving the transportation vehicle, an operation (250).  For example, driver module 24 may output information\nto applications 34 to cause one or more of applications 34 to cease functioning and/or provide additional functionality to the person while he or she is driving.\nA system and/or device (e.g., a wearable computing device, a server, a mobile device, etc.) in accordance with the techniques of this disclosure may determine that a person who is wearing a wearable computing device is driving and not a\npassenger in one of various ways.  For instance, one way for a system and/or device to determine that a person wearing a device is driving may be for the system and/or device to rely on sensors (e.g., accelerometers, gyroscopes, etc.) for receiving data\nthat indicates a turning motion.  Turning motions may be used to indicate: adjustments to a steering wheel (e.g., large turning motions while parking, small turning motions to maintain an automobile's position in a lane, etc.) done by a user.  Vertical\npositions may be used to indicate where a person's hand is held relative to the controls of a transportation vehicle.  For instance, if the same vertical position of a user's hand is maintained, the person may not be driving.  However if the system\nand/or device detects a change in vertical position (e.g., raising and lowering the hand periodically), the system and/or device may infer that the person is taking his or her hand off the steering wheel to grab a gear shifter to change gears of the\nautomobile.  Patterns of movement (e.g., a repeated motion of shifting gears and then returning to the steering position) may be used by system and/or device to detect driving.\nOne other way for a system and/or device to determine that a person wearing a device is driving may be for the system and/or device to rely on comparative measurements of a person's handheld device (e.g., a mobile phone, a tablet, etc.) and the\nwearable computing device.  For example, a system and/or device may determine a comparative barometric pressure between a wearable computing device and a mobile phone to determine relative elevation and changes of elevation.  In some examples, the system\nand/or device may use accelerometer data detected by a wearable computing device and a mobile phone to \"subtract out\" and/or \"filter\" vibrations and other system noise attributed to the transportation vehicle to isolate the signals and motions detected\nby the wearable computing device.\nOne other way for a system and/or device to determine that a person wearing a device is driving may be for the system and/or device to rely on information from multiple devices that are all in the presence of a transportation vehicle.  For\ninstance, the system and/or device may sense when there are more than one person wearing computerized watches in a vehicle.  If two people wear watches in a vehicle, only one person may be a driver.  The system and/or device may compute a probability\nthat each is driving and determine that the person with the higher degree of likelihood of being the driver is driving and the other person is merely a passenger riding in the vehicle.\nFIG. 5 is a flowchart illustrating example operations of an example computing system configured to determine whether a person wearing a wearable computing device is driving a transportation vehicle, in accordance with one or more aspects of the\npresent disclosure.  The process shown in FIG. 5 may be performed by one or more processors of a computing system, such as remote computing system 6 illustrated in FIG. 1.  For purposes of illustration, FIG. 5 is described below within the context of\nsystem 1 of FIG. 1.\nRemote computing system 6 may receive, from wearable computing device 10, information that includes one or more indications of movement associated with wearable computing device 10 and at least one indication that wearable computing device 10 is\nlocated within a moving vehicle (300).  For example, driver probability module 30 may receive a request from driver module 24 of wearable computing device 10 for a probability indicating whether or not the person wearing wearable computing device 10 is\ndriving transportation vehicle 2.  The request from driver module 24 may include raw sensor data obtained by one or more sensor devices 14 and/or one or more defined indications of movement based on that sensor data.  In addition, the request may include\ninformation indicating that wearable computing device 10 is in the presence of transportation vehicle 2 and may further indicate the type, location, speed, and/or elevation of transportation vehicle 2.\nDriving probability module 30 may process the request from driver module 24 and analyze the information indicating movement and the presence obtained with the request.  Remote computing system 6 may determine, based at least in part on the one\nor more indications of movement and the at least one indication that wearable computing device 10 is within the presence of transportation vehicle 2, a probability that a user of the wearable computing device is performing an act of driving (310).  For\ninstance, driving probability module 30 may feed the indications of movement as input to one or more rules of a machine learning system for determining whether the indications of movement match or correlate movements contained in other information that\nthe machine learning system identifies when other persons wearing wearable computing devices are driving.\nIn some examples, driving probability module 30 may identify one or more changes to a measurement of acceleration or orientation of wearable computing device 10.  For instance, driving probability module 30 may identify a portion of the data\ncontaining the indications of movement having an increase and/or a decrease to the acceleration of wearable computing device 10 and/or a change to the orientation of wearable computing device 10.\nDriving probability module 30 may determine, based on the one or more changes to the measurement of acceleration or orientation, a probability that the person wearing the wearable computing device is turning a steering wheel or shifting a gear\nshift of the transportation vehicle.  For instance, the machine learning system of driving probability module 30 may compute a respective probability associated with each one of multiple acts of driving (e.g., steering, shifting, etc.).  Driving\nprobability module 30 may determine, based at least in part on each respective probability associated with each one of the multiple acts of driving, the probability that the person wearing wearable computing device 10 is performing the act of driving. \nFor instance, driving probability module 30 may compute a weighted average of the probabilities associated with the various acts of driving to compute a single probability indicative of whether the person is driving or not.\nIn some examples, driving probability module 30 may rely on the probability of one act of driving to determine the probability of a different act of driving.  For instance, driving probability module 30 may compute the probability that the\nperson is changing lanes with transportation vehicle 2 based on a probability that the person is steering or turning the steering wheel of transportation vehicle 2, in addition to other information (e.g., speed, acceleration, direction, etc.) of wearable\ncomputing device 10.\nIn some examples, driving probability module 30 of remote computing system 6 may identify, based on the one or more indications of movement received from wearable computing device 10, a pattern of movement associated with the act of driving. \nResponsive to identifying the pattern of movement, driving probability module 30 may determine the probability that the person wearing wearable computing device 10 is performing the act of driving.  For example, as described above, driving probability\nmodule 30 may compare the indications of movement obtained from wearable computing device 10 to data stored at driving patterns data store 32 to determine whether the received indications of movement have a strong enough correlation (e.g., greater than\n0) for indicating that the person is driving.  In some examples, the patterns of movement identified by driving probability module 30 may include one or more sequences of movements such as shifting a gear shift of transportation vehicle 2, raising a hand\nof the person, turning a steering wheel of transportation vehicle 2, and lowering the hand of the person, and a combination of raising and lowering the hand of the person, turning the steering wheel, and shifting the gear shift of the transportation\nvehicle.\nRemote computing system 6 may determine whether the probability satisfies a probability threshold (320).  For example, driving probability module 30 may compare the probability computed above to one or more threshold probabilities used for\ndetermining whether the person is driving.  A probability threshold may be fifty percent, ninety percent, or other percentage.\nResponsive to determining that the probability satisfies a probability threshold, remote computing system 6 may determine that the user of the wearable computing device 10 is currently driving the moving vehicle (330).  For instance if driving\nprobability module 30 determines that the probability that the person is turning a steering wheel of transportation vehicle 2 is greater than fifty percent and/or that the probability that the person is moving a gear shift of transportation vehicle 2 is\ngreater than thirty percent, that the person is then likely to be driving transportation vehicle 2.\nRemote computing system 6 may output, for transmission to wearable computing device 10, information that configures wearable computing device 10 to perform an operation (340).  For example, driving probability module 30 may respond to the\nrequest from driver module 24 by providing information, data, and/or other indications that the person is determined to be driving to cause wearable computing device 10 to perform an operation.\nIn some examples, remote computing system 6 may rely on information about other passengers of transportation vehicle 2 to determine the probability that the person wearing wearable computing device 10 is driving and causing wearable computing\ndevice 10 to perform certain operations when the person is driving.  For example, driving probability module 30 may receive information from wearable computing device 10 and/or mobile computing device 8 and determine a probability that at least one\nperson not wearing wearable computing device 10 and in transportation vehicle 2 is performing the act of driving.  Said differently, a person other than the person wearing wearable computing device 10 may be holding, operating, or otherwise be associated\nwith mobile computing device 8 or may even be wearing a different wearable computing device.  Driving probability module 30 may receive sensor information obtained from the mobile and/or wearable device associated with the other person to determine\nwhether that other person is driving transportation vehicle 2.\nDriving probability module 30 may compare the probability that each of the other persons in transportation vehicle 2 is driving to the probability that the person wearing wearable computing device 10 is driving.  Responsive to determining that\nthe probability that the person wearing wearable computing device 10 is driving exceeds the probability that any of the other persons not wearing wearable computing device 10 is driving, driving probability module 30 may determine that the person wearing\nwearable computing device 10 and not the at least one person not wearing wearable computing device 10 is currently driving transportation vehicle 2.  After determining that the person wearing wearable computing device 10 and not the other persons is\ndriving, driving probability module 26 of remote computing system 6 may output, for transmission to wearable computing device 10, the information that configures wearable computing device 10 to perform the operation.\nClause 1.  A method comprising: detecting that a wearable computing device is located within a moving vehicle; detecting, by the wearable computing device, an indication of movement associated with the wearable computing device; determining,\nbased at least in part on the indication of movement, that a user of the wearable computing device is currently driving the moving vehicle; and performing, based on the determination that the user of the wearable computing device is currently driving the\nmoving vehicle, an operation.\nClause 2.  The method of clause 1, wherein determining that the user of the wearable computing device is currently driving the moving vehicle comprises: determining a probability that the user of the wearable computing device is performing an\nact of driving; and responsive to determining that the probability satisfies a probability threshold, determining that the user is currently driving the moving vehicle.\nClause 3.  The method of clause 2, wherein determining the probability that the user of the wearable computing device is performing the act of driving comprises: outputting, by the wearable computing device, for transmission to at least one\nsecond computing device, information comprising the indication of movement; and responsive to outputting the information comprising the indication of movement, receiving, by the wearable computing device, from the at least one second computing device,\nthe probability that the user of the wearable computing device is performing the act of driving.\nClause 4.  The method of any of clauses 2-3, wherein the act of driving comprises at least one of turning a steering wheel, depressing the steering wheel, shifting a gear shift, pressing a pedal, depressing the pedal, or raising and lowering a\nhand.\nClause 5.  The method of any of clauses 1-4, wherein performing the operation comprises restricting access to at least some functionality of an application or operating system being executed by at least one of the wearable computing device or a\nsecond computing device.\nClause 6.  The method of any of clauses 1-5, wherein performing the operation comprises outputting, by the wearable computing device, for transmission to at least one second computing device, information usable by the at least one second\ncomputing device to learn driving habits of the user of the wearable computing device.\nClause 7.  The method of any of clauses 1-6, wherein performing the operation comprises outputting, by the wearable computing device, for transmission to at least one second computing device, information usable by the at least one second\ncomputing device to determine a driving route associated with the user of the wearable computing device.\nClause 8.  The method of any of clauses 1-7, wherein performing the operation comprises deactivating, by the wearable computing device, a display device that is operatively coupled to at least one of the wearable computing device or a second\ncomputing device.\nClause 9.  The method of any of clauses 1-8, wherein detecting the indication of movement associated with the wearable computing device comprises: determining, by the wearable computing device and based at least in part on a sensor of the\nwearable computing device, sensor information indicating at least one of an acceleration of the wearable computing device, an orientation of the wearable computing device, or a barometric pressure of the wearable computing device; and defining, based on\nthe sensor information, the indication of movement.\nClause 10.  The method of any of clauses 1-9, wherein detecting the indication of movement associated with the wearable computing device comprises: detecting an indication of movement associated with the moving vehicle; and subtracting the\nindication of movement associated with the moving vehicle from the indication of movement associated with the wearable computing device.\nClause 11.  A wearable computing device comprising: at least one processor; at least one module operable by the at least one processor to: detect that the wearable computing device is located within a moving vehicle; detect an indication of\nmovement associated with the wearable computing device; determine, based at least in part on the indication of movement, that a user of the wearable computing device is currently driving the moving vehicle; and perform, based on the determination that\nthe user of the wearable computing device is currently driving the moving vehicle, an operation.\nClause 12.  The wearable computing device of clause 11, wherein the at least one module is further operable by the at least one processor to determine that the user is currently driving the moving vehicle by at least: determining a probability\nthat the user of the wearable computing device is performing an act of driving; and responsive to determining that the probability satisfies a probability threshold, determining that the user of the wearable computing device is currently driving the\nmoving vehicle.\nClause 13.  The wearable computing device of clause 12, wherein the at least one module is further operable by the at least one processor to determine the user of the wearable computing device is performing the act of driving by at least:\noutputting, by the wearable computing device, for transmission to at least one second computing device, information comprising the one or more indications of movement; and responsive to outputting the information comprising the one or more indications of\nmovement, receiving, by the computing system, from the at least one second computing device, the probability that the user of the wearable computing device is performing the act of driving.\nClause 14.  The wearable computing device of any of clauses 11-13, wherein the at least one module is further operable by the at least one processor to: determine at least one of a relative elevation, acceleration, degree of tilt, or orientation\nbetween the wearable computing device and at least one second computing device associated with the user of the wearable computing device; and determine, based at least in part on the at least one of the relative elevation, acceleration, degree of tilt,\nor orientation, that the user of the wearable computing device is currently driving the moving vehicle.\nClause 15.  The wearable computing device of clause 14, wherein the at least one module is further operable by the at least one processor to determine the at least one of the relative elevation, acceleration, degree of tilt, or orientation\nbetween the wearable computing device and the at least one second computing device associated with the user of the wearable computing device by at least determining, based on at least one of a barometric pressure, an acceleration, a degree of tilt, or a\ndegree of orientation associated with each of the wearable computing device and the at least one second computing device, the at least one of the relative elevation, acceleration, degree of tilt, or orientation between the wearable computing device and\nthe at least one second computing device associated with the user of the wearable computing device.\nClause 16.  A method comprising: receiving, by a computing system, from a wearable computing device, information that includes one or more indications of movement associated with the wearable computing device and at least one indication that the\nwearable computing device is located within a moving vehicle; determining, by the computing system, based at least in part on the one or more indications of movement and the at least one indication that the wearable computing device is located within the\nmoving vehicle, a probability that a user of the wearable computing device is performing an act of driving; responsive to determining that the probability satisfies a probability threshold, determining, by the computing system, that the user of the\nwearable computing device is currently driving the moving vehicle; and outputting, by the computing system, for transmission to at least one of the wearable computing device or at least one second computing device, information that configures the at\nleast one of the wearable computing device or the at least one second device to perform an operation.\nClause 17.  The method of clause 16, wherein the probability is a first probability, the method further comprising: determining, by the computing system, a second probability that at least one person not wearing the wearable computing device and\nlocated in the moving vehicle is performing the act of driving; responsive to determining that the first probability exceeds the second probability, determining, by the computing system, that the user of the wearable computing device and not the at least\none person not wearing the wearable computing device is currently driving the moving vehicle; and outputting, by the computing system, for transmission to the at least one of the wearable computing device or the at least one second computing device, the\ninformation that configures the at least one of the wearable computing device or the at least one second computing device to perform the operation.\nClause 18.  The method of any of clauses 16-17, wherein the probability is a first probability, the method further comprising: identifying, by the computing system, one or more changes to a measurement of acceleration or orientation of the\nwearable computing device; determining, by the computing system, based on the one or more changes to the measurement of acceleration or orientation, a second probability that the user of the wearable computing device is turning a steering wheel or\nshifting a gear shift of the moving vehicle; and determining, by the computing system, based at least in part on the second probability, the first probability that the user of the wearable computing device is performing the act of driving.\nClause 19.  The method of any of clauses 16-18, wherein determining the probability that the user of the wearable computing device is performing the act of driving comprises: identifying, by the computing system, based on the one or more\nindications of movement, a pattern of movement associated with the act of driving; and responsive to identifying the pattern of movement, determining, by the computing system, the probability that the user of the wearable computing device is performing\nthe act of driving.\nClause 20.  The method of clause 19, wherein the pattern of movement comprises a sequence of movements comprising at least one of shifting a gear shift, raising a hand, turning a steering wheel, lowering the hand, or a combination of raising and\nlowering the hand, turning the steering wheel, and shifting the gear shift.\nClause 21.  A computer readable storage medium comprising instructions, that when executed, configure one or more processors of a computing device to perform any of the methods of clauses 1-10.\nClause 22.  A computer readable storage medium comprising instructions, that when executed, configure one or more processors of a computing system to perform any of the methods of clauses 16-20.\nClause 23.  A computing device comprising means for performing any of the methods of clauses 1-10.\nClause 24.  A computing system comprising means for performing any of the methods of clauses 16-20.\nClause 24.  A computing system comprising means for performing any of the methods of clauses 1-10 and 16-20.\nIn each of the various examples described above, computing devices, mobile computing devices, wearable computing devices, computing systems, and other computing devices may analyze information (e.g., locations, speeds, etc.) associated with the\nwearable computing devices, computing systems, and other computing devices, only if the wearable computing devices, computing systems, and other computing devices, receive permission from a user of such wearable computing devices, computing systems, and\nother computing devices, to analyze the information.  For example, in situations discussed below in which a wearable computing device or computing system may collect or may make use of information associated with a user and the wearable computing device\nand computing system, the user may be provided with an opportunity to control whether programs or features of the wearable computing device and computing system can collect and make use of user information (e.g., information about a user's location,\nspeed, mode of transportation, e-mail, a user's social network, social actions or activities, profession, a user's preferences, or a user's past and current location), or to control whether and/or how to the wearable computing device and computing system\nreceive content that may be relevant to the user.  In addition, certain data may be treated in one or more ways before it is stored or used by the wearable computing device and computing system, so that personally identifiable information is removed. \nFor example, a user's identity may be treated so that no personally identifiable information can be determined about the user, or a user's geographic location may be generalized where location information is obtained (such as to a city, ZIP code, or\nstate level), so that a particular location of a user cannot be determined.  Thus, the user may have control over how information is collected about the user and used by the wearable computing device and computing system.\nIn one or more examples, the functions described may be implemented in hardware, software, firmware, or any combination thereof.  If implemented in software, the functions may be stored on or transmitted over, as one or more instructions or\ncode, a computer-readable medium and executed by a hardware-based processing unit.  Computer-readable media may include computer-readable storage media, which corresponds to a tangible medium such as data storage media, or communication media including\nany medium that facilitates transfer of a computer program from one place to another, e.g., according to a communication protocol.  In this manner, computer-readable media generally may correspond to (1) tangible computer-readable storage media, which is\nnon-transitory or (2) a communication medium such as a signal or carrier wave.  Data storage media may be any available media that can be accessed by one or more computers or one or more processors to retrieve instructions, code and/or data structures\nfor implementation of the techniques described in this disclosure.  A computer program product may include a computer-readable medium.\nBy way of example, and not limitation, such computer-readable storage media can comprise RAM, ROM, EEPROM, CD-ROM or other optical disk storage, magnetic disk storage, or other magnetic storage devices, flash memory, or any other medium that can\nbe used to store desired program code in the form of instructions or data structures and that can be accessed by a computer.  Also, any connection is properly termed a computer-readable medium.  For example, if instructions are transmitted from a\nwebsite, server, or other remote source using a coaxial cable, fiber optic cable, twisted pair, digital subscriber line (DSL), or wireless technologies such as infrared, radio, and microwave, then the coaxial cable, fiber optic cable, twisted pair, DSL,\nor wireless technologies such as infrared, radio, and microwave are included in the definition of medium.  It should be understood, however, that computer-readable storage media and data storage media do not include connections, carrier waves, signals,\nor other transient media, but are instead directed to non-transient, tangible storage media.  Disk and disc, as used herein, includes compact disc (CD), laser disc, optical disc, digital versatile disc (DVD), floppy disk and Blu-ray disc, where disks\nusually reproduce data magnetically, while discs reproduce data optically with lasers.  Combinations of the above should also be included within the scope of computer-readable media.\nInstructions may be executed by one or more processors, such as one or more digital signal processors (DSPs), general purpose microprocessors, application specific integrated circuits (ASICs), field programmable logic arrays (FPGAs), or other\nequivalent integrated or discrete logic circuitry.  Accordingly, the term \"processor,\" as used herein may refer to any of the foregoing structure or any other structure suitable for implementation of the techniques described herein.  In addition, in some\naspects, the functionality described herein may be provided within dedicated hardware and/or software modules.  Also, the techniques could be fully implemented in one or more circuits or logic elements.\nThe techniques of this disclosure may be implemented in a wide variety of devices or apparatuses, including a wireless handset, an integrated circuit (IC) or a set of ICs (e.g., a chip set).  Various components, modules, or units are described\nin this disclosure to emphasize functional aspects of devices configured to perform the disclosed techniques, but do not necessarily require realization by different hardware units.  Rather, as described above, various units may be combined in a hardware\nunit or provided by a collection of interoperative hardware units, including one or more processors as described above, in conjunction with suitable software and/or firmware.\nVarious examples have been described.  These and other examples are within the scope of the following claims.", "application_number": "15921239", "abstract": " A wearable computing device is described that detects an indication of\n     movement associated with the wearable computing device when a user of the\n     wearable computing device detected being located within a moving vehicle.\n     Based at least in part on the indication of movement, a determination is\n     made that the user of the wearable computing device is currently driving\n     the moving vehicle. An operation is performed based on the determination\n     that the user of the wearable computing device is currently driving the\n     moving vehicle.\n", "citations": ["6353778", "7292152", "7876205", "8000689", "8213962", "8401578", "8478306", "8577703", "8952869", "9037125", "9037199", "9042872", "9571629", "9832306", "20010006886", "20010040590", "20030109246", "20050119002", "20050255874", "20060148490", "20060273888", "20080299954", "20090002147", "20090048944", "20090111422", "20090132197", "20090234614", "20100148920", "20100216509", "20100255778", "20110059731", "20110212737", "20110219080", "20110275321", "20120250517", "20120265977", "20120329444", "20130035117", "20130041521", "20130150004", "20130245986", "20130249785", "20130281079", "20130303143", "20130332160", "20140002357", "20140113619", "20140187219", "20140192781", "20140195125", "20140256303", "20140342718", "20140375477", "20150024786", "20150126170", "20150181414", "20150185827", "20150213555", "20150312404", "20160021238", "20160249191", "20180063319"], "related": ["15794626", "15401782", "14627122", "14246966"]}, {"id": "20180206280", "patent_code": "10375748", "patent_name": "Method for terminal-to-terminal communication in a cellular mobile radio\n     network, and a system therefor", "year": "2019", "inventor_and_country_data": " Inventors: \nKasparick; Martin (Berlin, DE), Stanczak; Slawomir (Berlin, DE), Garrido Cavalcante; Renato Luis (Berlin, DE), Gozalvez Serrano; David (Munich, DE), Botsov; Mladen (Munich, DE), Fertl; Peter (Munich, DE)  ", "description": "<BR><BR>BACKGROUND AND SUMMARY OF THE INVENTION\nThe present invention relates to a method for terminal-to-terminal communication in a cellular mobile radio network, and to a system therefor.\nCooperative intelligent transport systems (C-ITS) are based on the exchange of messages between vehicles, so-called vehicle-to-vehicle communication (V2V), and/or on the exchange of messages between vehicles and infrastructure units, so-called\nvehicle-to-infrastructure communication (V2I).  Their goal is to increase traffic safety and traffic efficiency.  Said exchange of messages or said communication is generally realized by means of wireless communication, for example by means of\ncommunication on the basis of a wireless local area network (WLAN) communication standard, such as, for example, on the basis of the IEEE 802.11p standard issued by the Institute of Electrical and Electronics Engineers (IEEE).\nMost theoretical considerations concerning communication by means of cellular networks in this regard focus on cellular networks for a terminal-to-terminal communication, so-called \"D2D-enabled cellular networks\", and are principally concerned\nwith establishing efficient protocols for direct communication.  In this context, base stations have to exchange a multiplicity of control data with the D2D peers.  In this case, it is always assumed that a connection between a base station and a mobile\nradio communication terminal is maintained in order to exchange said control data.  This may be regarded as network-assisted terminal-to-terminal communication, thus as \"network-assisted device-to-device communication\" (aD2D).  This approach promises a\nhigher performance and reliability than the conventional ad-hoc communication, such as by means of IEEE 802.11p, for example.\nHitherto, however, a complete standard for aD2D communication has not yet been published or established.  By way of example, resource allocation has been one of the greatest unsolved problems hitherto at the conceptual level.  Most concepts for\nfinding solutions here are based on the concept of \"accumulating channel state information\" for all connections in the system, at the base station.  In order to ensure an efficient utilization of the radio resources here, the base stations require\nprecise information about the status of the corresponding transmission channel in the network.  In order furthermore to guarantee a sufficient reliability in critical application cases, D2D transmitters likewise require a specific knowledge regarding the\ntransmission channel.  In this context, the requirements made of C-ITS applications by means of aD2D communication in mobile communication systems and the ascertainment of precise knowledge regarding the transmission channel at the base stations are in\ncompetition with one another.\nIn this case, direct measurements of the channel may prove to be difficult, on account of the then large number of signal information items (such as, for example, the transmission of reference signals for the channel measurement and measurement\nprotocols) that have to be exchanged between the base stations and the mobile radio communication terminals.  Furthermore, a suitable design is demanded in order to avoid frequency collisions between D2D and cellular reference signals from neighboring\ncells.\nGenerally, it may become extremely difficult to effect sufficiently good implementation of channel measurements and corresponding measurement protocols for all D2D pairs and cellular mobile radio communication terminals in the cell if the number\nof mobile radio communication terminals in a far cell increases in the future.  In the case of the mobile radio communication terminals, it may be even more challenging to obtain reliable channel estimations at the base station for the D2D communication\npairs, as a result of a channel to be propagated highly dynamically.  It can be deduced from this that a regulation for C-ITS applications by means of network-controlled D2D communication, from the technical side, has not yet been sufficiently clarified.\nTherefore, it would be desirable to provide a possibility which reliably enables a reliable D2D communication, on the basis of cellular mobile radio networks.\nIt is an object of the invention to propose a possibility which avoids or at least reduces at least some of the disadvantages known in the prior art.\nThe invention relates to a method for terminal-to-terminal communication in a cellular mobile radio network, in particular for a cooperative intelligent transport systems communication, the method comprising the following steps: providing a\nradio signal interference map, wherein the radio signal interference map is indicative of a location-dependent radio signal interference disturbance within a coverage area of a base station, wherein the location-dependent radio signal interference\ndisturbance is indicative of a radio signal interference disturbance of a terminal-to-terminal communication, and wherein providing the radio signal interference map is carried out on the basis of a learning algorithm for a geographic radio signal\ninterference map, providing configuration information regarding a configuration setting of a radio transmission for a terminal-to-terminal communication between a first mobile radio communication terminal and a second mobile radio communication terminal,\nwherein providing the configuration information is carried out on the basis of the location-dependent radio signal interference disturbance of the radio signal interference map, and wherein the configuration setting enables a statement regarding a\nconfiguration to be performed in respect of the radio transmission, and setting up a terminal-to-terminal communication between the first mobile radio communication terminal and the second mobile radio communication terminal, taking account of the\nprovided configuration information regarding the configuration setting.\nIn this case, the method steps can be performed in an automated manner.\nA location-dependent radio signal interference disturbance within the meaning of the invention here can be an interference disturbance of an electromagnetic radio wave, wherein the interference disturbance is location-dependent and therefore\ngenerally not constant in the coverage area of a base station.\nA radio signal interference disturbance of a terminal-to-terminal communication within the meaning of the invention here means that the interference disturbance of the radio signal can concern a terminal-to-terminal communication, in particular.\nA learning algorithm for a geographic radio signal interference map within the meaning of the invention here can mean an algorithm in which firstly the interference is always learned or determined, for example as an estimated value.  It is only\nafterward, on the basis of this information, that a channel condition is estimated, such as, for example, a channel quality of the communication channel utilized for the terminal-to-terminal communication, that is to say the D2D communication.\nThis learning approach can be divided into two components, the learning of the location-dependent interference disturbance and, on the basis thereof, the channel estimation for the D2D communication.  In this case, the learning of the\ninterference disturbance is carried out at the base station, whereas the measurements for ascertaining the interference disturbances are performed by the D2D subscribers and communicated to the base station.\nThe learning approach can be summarized at the conceptual level as follows: instead of taking account of the interference and hence interference disturbances experienced by a pair of communication subscribers, a geographic interference map is\ncreated.  In this case, each coordinate in the map is assigned an interference value or interference disturbance value.  This value may be interpreted here as the interference or interference disturbance which a D2D communication subscriber typically\nexperiences, that is to say experiences on average, when moving along the geographic locations on the map.\nA second step is the utilization of the interference map which has been learned by the base station.  This step comprises ascertaining the estimation of the channel condition of the channel utilized by the D2D communication subscribers.  For\nthis estimation it may be assumed that for this purpose it is possible to have recourse to the measurement values of the D2D communication subscribers in the system.\nConfiguration information regarding a configuration setting of a radio transmission within the meaning of the invention here can mean information suitable for informing a communication terminal of how a desired radio transmission for a\nterminal-to-terminal communication should be established.  This can be carried out for example by means of a prediction which exhibits probabilities and enables a statement about the quality of the radio channel to be utilized for the radio transmission. Consequently, the configuration information can be prediction information and the configuration setting for the radio transmission can correspondingly be a channel quality condition for the radio channel to be utilized for the radio transmission.  The\nchannel quality condition can then be a statement about a channel damping regarding the radio channel.\nIn the case of network-supported terminal-to-terminal communication, the base station can also communicate information as to how the direct transmission including transmitter power and/or modulation should be configured.  As a result, the base\nstation can communicate this configuration instead of the prediction of the channel quality, or else in addition to the prediction of the channel quality, to the relevant communication terminals.\nA terminal-to-terminal communication within the meaning of the invention here can mean a communication in a cellular mobile radio telecommunication network in which at least two mobile radio communication devices communicate directly with one\nanother.  In this case, however, the communication is not exclusively limited to two subscribers, but rather can also comprise a third or even further subscribers.  By way of example, a terminal-to-terminal communication can also comprise a broadcast\ncommunication in which a terminal directly communicates broadcasting information to further terminals.\nA terminal-to-terminal communication can mean that in principle there may be the possibility of carrying out broadcast transmissions by the transmission being configured in accordance with the device with the poorest quality conditions.\nThe mobile radio communication terminal within the meaning of the invention here can be a mobile radio communication device, but--in contrast to an arbitrary mobile radio communication device connected to a base station--one which is designed\nfor terminal-to-terminal communication.\nThe teaching according to the invention affords the advantage that it is possible to provide a terminal-to-terminal communication in a cellular mobile radio communication network for at least two mobile radio communication terminals, such as is\nrequired for a D2D-based communication for C-ITS applications.\nA further advantage is that as a result there is no longer a need for channel measurements and corresponding measurement protocols for all D2D pairs and cellular mobile radio communication terminals in the cell.\nIn the case of a cooperative intelligent transport systems communication, the terminal-to-terminal communication can be designed to enable a direct vehicle-to-vehicle communication and/or a direct vehicle-to-infrastructure communication.  Direct\nhere means that, in the cellular mobile radio communication network, the base station is only accorded the importance of making statements about a communication channel and/or the communication subscribers in order to enable the terminal-to-terminal\ncommunication.  In this case, such statements may be channel quality information, an indication of how transmission, reception and/or channel selection should be performed, or else subscriber information, that is to say in the case of broadcasting, for\nexample, which subscribers would like to participate in the corresponding broadcasting.  However, the base station is no longer needed for carrying out the actual communication between the subscribers; consequently, the base station no longer carries out\nrouting of the communication data.\nThe subject matter of an alternative independent claim relates to a system for terminal-to-terminal communication in a cellular mobile radio network, the system comprising a base station of a mobile radio system, a first mobile radio\ncommunication terminal, and a second mobile radio communication terminal.  In this case, the base station is designed to provide terminal-to-terminal mobile radio communication devices connected to the base station with corresponding information,\nregarding configuration information, regarding a configuration setting of a radio transmission for a terminal-to-terminal communication.  Furthermore, the system is configured to implement a method according to the invention.\nIn this case, configuring and/or setting up the D2D communication connection can be performed by the base station of the cell, whereas the communication data can be transmitted directly, that is to say without a detour via the base station, from\none D2D communication subscriber to the other further communication subscriber or communication subscribers participating in the D2D communication connection.\nThe teaching according to the invention affords the advantage that it is possible to provide a terminal-to-terminal communication in a cellular mobile radio communication network for at least two mobile radio communication terminals, such as is\nrequired for a D2D-based communication for C-ITS applications.\nA further advantage is that there is no need for cost-intensive upgrading or new installation of the base station present and required in an existing cellular mobile radio communication network.\nYet another advantage is that a better performance can be achieved, through a centralized coordination and configuration of transmissions, since the central coordination and configuration unit has more information.\nThe subject matter of a further alternative independent claim relates to a computer program product for a system, wherein the system is operable according to a method according to the invention.\nThe teaching according to the invention affords the advantage that the method can be implemented particularly efficiently in an automated manner.\nThe subject matter of a further alternative independent claim relates to a data carrier comprising a computer program product according to the invention.\nThe teaching according to the invention affords the advantage that the method can be distributed and/or kept available particularly efficiently among the devices and/or systems implementing the method.\nBefore embodiments of the invention are described more thoroughly below, firstly it should be emphasized that the invention is not restricted to the components described or the method steps described.  Furthermore, the terminology used does not\nconstitute any restriction either, but rather has only exemplary character.  Insofar as the singular is used in the description and the claims, in each case the plural is concomitantly encompassed here, unless this is explicitly precluded by the context. Any possible method steps can be performed in an automated manner, unless this is explicitly precluded by the context.\nFurther exemplary embodiments of the method according to the invention are explained below.\nIn accordance with a first exemplary embodiment, the method furthermore comprises the fact that providing the radio signal interference map is carried out on the basis of received signal strength indication information.\nReceived signal strength indication information within the meaning of the invention here can mean information which represents an indicator of the received field strength of wireless communication applications and is also called RSSI.  Since the\nRSSI does not have a defined unit, the value has to be interpreted depending on the respective application.  There are various definitions even within the \"Institute of Electrical and Electronics Engineers\", also called IEEE, 802.11 standard, wherein a\nhigher value corresponds to better reception.  In measurement receivers, the RSSI value can also be output as an analog voltage at a dedicated terminal for further processing.  With a device-dependent scaling factor, the RSSI value can be expressed as a\npower level in the dimensionless unit dBm.  This indicator is required by mobile radio telephones, radio-frequency modules and other systems that rely on radio communication, in order to find a channel that is usable for the communication.  If the signal\nstrength required for a successful communication is undershot on the channel currently being utilized, it is possible to change to a better channel, if appropriate, with the aid of the RSSI value.\nIn this case, the RSSI information is important for the summation of all received energy distributions, also called \"received power contributions\", and may be interpreted here as an estimation of the total interference.  In this case,\ninterference estimations can be generated by means of a learning algorithm used.  This can be carried out thereby even for geographic locations at which no measurement values are available.  If there are sufficient interference measurement values around\nsuch a location and the learning algorithm has been trained as it were well enough, these interference estimation values can indeed likewise be regarded as reliable.\nIn order to enable this learning, that is to say in order to be able to generate a geographic interference map from the interference measurement values of the D2D communication subscribers, it is possible to utilize machine learning programs,\nalso called \"machine learning tools\", which are already publically available.  Two such exemplary methods are adaptive projected subgradient method based algorithms, also called APSM algorithms, and multi-kernel approaches.\nThis embodiment has the advantage that measurement data such as are provided on the basis of corresponding protocols in the cellular mobile radio communication system can be utilized for the learning of the interference map.  Consequently, no\nadditional measurements are required, as a result of which resources can be saved.\nIn accordance with a further exemplary embodiment, the method furthermore comprises the fact that providing the radio signal interference map is carried out on the basis of measurement information from mobile radio communication devices\nconnected to the base station, such as are provided at least in a long term evolution standard, in particular in a 4G standard and/or 3.9G standard.\nA long term evolution standard within the meaning of the invention here may be a long term evolution standard of a cellular mobile radio communication network, abbreviated to LTE or else 3.9G.  This is a designation for a 3.9G standard in the\ncontext of 3GPP, which does not completely fulfill the 4G definitions of the \"Telecommunication Standardization Sector\", also called ITU-T. This standard is nevertheless promoted as 4G for marketing reasons.  The planned successor of LTE is the\nIMT-advanced 4G mobile radio standard called LTE-advanced, currently being standardized, and is backward-compatible with LTE.\nIn this case, the measurement information is based on measurement information and/or protocols as provided at least in a long term evolution standard or 3.9G standard.  In this case, a mobile radio standard that is defined more highly than 3.9G\nor 4G may also be provided for a cellular mobile radio communication network.\nConsequently, the base station can acquire measurements which can be restricted for example to measurements such as are defined in the LTE standard or more highly defined standards.  This information can then be utilized to average for example\nan interference map from the geographic location measurements.  The corresponding base station can thus perform for example an estimation of an average channel condition, for a corresponding terminal-to-terminal communication.\nThese measurement values or reports created therefrom may be for example \"Signal to Interference plus Noise Ratio\" reports, also called SINR reports, RSSI reports or other reports defined in the long term evolution standard or thereabove.\nThis embodiment has the advantage that measurement data such as are provided on the basis of corresponding LTE protocols and standards based thereon in the cellular mobile radio communication system can be utilized for the learning of the\ninterference map.  Consequently, no additional measurements are required, as a result of which resources can be saved.\nIn accordance with a further exemplary embodiment, the method furthermore comprises the fact that the learning algorithm for a geographic radio signal interference map is implemented on the basis of an adaptive projected subgradient method based\nalgorithm.\nSuch an algorithm may be based for example on \"Interference Identification in Cellular Networks via Adaptive Projected Subgradient methods\", and also on \"MMSE Interference Estimation in LTE Networks\".  These two exemplary methods can be used\nhere to form a learning algorithm in this regard.  In this case, the first method describes an adaptive projected subgradient approach, for learning methods in diffusion networks, also known as \"Adaptive Projected Subgradient Approach to Learning in\nDiffusion Networks\".  The second exemplary method can be utilized for the learning algorithm by virtue of the fact that, by utilizing this method, an optimum linear \"Minimum Mean Square Error\" estimator, also called \"MMSE estimator\", is utilized in order\nto generate the geographic radio signal interference map.\nThis embodiment has the advantage that efficient learning algorithms tailored to cellular mobile radio networks can be utilized in order to be able to generate the interference map.\nIn accordance with a further exemplary embodiment, the method furthermore comprises the fact that the configuration information regarding the configuration setting is suitable, regarding a terminal-to-terminal communication, for a\nterminal-to-terminal mobile radio communication terminal and/or a user of the mobile radio communication terminal, for configuring a transmission channel, for increasing a service quality, for compensating for a radio signal interference in this way, for\nperforming a proactive resource allocation and/or for performing a link adaptation.\nThis embodiment has the advantage that a better communication quality may be achievable.\nIn accordance with a further exemplary embodiment, the method furthermore comprises the fact that a determination of the configuration setting comprises: ascertaining a total received power at a receiving mobile radio communication terminal,\ndetermining a transmitted power for a transmitting mobile radio communication terminal, and ascertaining a channel damping, on the basis of a division of a subtraction of the location-dependent radio signal interference disturbance from the total\nreceived power at the receiving mobile radio communication terminal by the transmitted power for the transmitting mobile radio communication terminal.  In this case, the transmitting mobile radio communication terminal corresponds to the first mobile\nradio communication terminal and the receiving mobile radio communication terminal corresponds to the second mobile radio communication terminal.\nThis embodiment has the advantage that an even better communication quality may be achievable.\nIn accordance with a further exemplary embodiment, the method furthermore comprises the fact that setting up a terminal-to-terminal communication between the first mobile radio communication terminal and the second mobile radio communication\nterminal, taking account of the provided configuration information regarding the configuration setting, comprises an adaptability of a modulation, a channel encoding, a signal transmission power and/or an antenna transmission mode to the mobile radio\ncommunication terminals participating in the terminal-to-terminal communication.\nThis embodiment has the advantage that an even better communication quality may be achievable.\nIn accordance with a further exemplary embodiment, the method furthermore comprises the fact that the terminal-to-terminal communication is designed to be carried out exclusively between the first mobile radio communication terminal and the\nsecond mobile radio communication terminal.\nExclusively carrying out a terminal-to-terminal communication within the meaning of the invention can mean here that the terminal-to-terminal communication is additionally carried out exclusively between two subscribers, that is to say takes\nplace without the participation of a base station.  The base station here is merely accorded the task, for example, of making a statement about a channel quality condition during the terminal-to-terminal communication, but--in contrast to conventional\ncommunication in a cellular mobile radio communication network--not of performing the routing between the subscribers and thus functioning as it were as a man-in-the-middle, that is to say itself forwarding the call and data packets (to the subscribers).\nMoreover, the terminal-to-terminal communication can be carried out exclusively between two subscribers or else exclusively between a plurality of subscribers; in other words thus in each case without a direct participation of a base station in\nthe respective communication itself.\nThis embodiment has the advantage that the actual communication between the terminals manages without a base station.\nThe invention thus allows the provision of a terminal-to-terminal communication in a cellular mobile radio communication network for at least two mobile radio communication terminals, wherein no base station is necessary for the actual\ncommunication between the subscribers.  The base station here serves merely to provide a radio signal interference map in order to enable a statement about the channel quality of the radio signal for the terminal-to-terminal communication.  As a result,\nresources can be saved and the D2D communication for C-ITS applications is made possible.  The teaching according to the invention obviates complex channel measurements, implementable possibly only with difficulty or not at all, and corresponding\nmeasurement protocols for all D2D pairs and cellular mobile radio communication terminals in the cell.\nOther objects, advantages and novel features of the present invention will become apparent from the following detailed description of one or more preferred embodiments when considered in conjunction with the accompanying drawings. <BR><BR>BRIEF\nDESCRIPTION OF THE DRAWINGS\nFIG. 1 shows a schematic illustration of a proposed method in accordance with one exemplary embodiment of the invention.\nFIG. 2 shows a schematic illustration of a proposed method in accordance with a further exemplary embodiment of the invention.\nFIG. 3 shows a schematic illustration of a proposed method in accordance with a further exemplary embodiment of the invention.\nFIG. 4 shows a schematic illustration of a proposed system in accordance with a further exemplary embodiment of the invention.\n<BR><BR>DETAILED DESCRIPTION OF THE DRAWINGS\nFIG. 1 shows a schematic illustration of a proposed method in accordance with one exemplary embodiment of the invention.\nIn this case, FIG. 1 shows a schematic illustration of a method for terminal-to-terminal communication in a cellular mobile radio network, wherein the method comprises: providing 110 a radio signal interference map 110.  In this case, the radio\nsignal interference map 110 is indicative of a location-dependent radio signal interference disturbance within a coverage area of a base station 100.  In this case, the location-dependent radio signal interference disturbance is indicative of a radio\nsignal interference disturbance of a terminal-to-terminal communication 130.  The method moreover involves providing the radio signal interference map 110 on the basis of a learning algorithm for a geographic radio signal interference map.  Providing 20\nconfiguration information 120 regarding a configuration setting of a radio transmission for a terminal-to-terminal communication 130 between a first mobile radio communication terminal 210 and a second mobile radio communication terminal 220.  In this\ncase, providing the configuration information 120 is carried out on the basis of the location-dependent radio signal interference disturbance of the radio signal interference map 110.  In this case moreover the configuration setting enables a statement\nregarding a configuration to be performed in respect of the radio transmission.  In addition setting up 30 a terminal-to-terminal communication 130 between the first mobile radio communication terminal 210 and the second mobile radio communication\nterminal 220, taking account of the provided configuration information 120 regarding the configuration setting.\nMathematically, that can be considered as follows:\nThe estimation of the geographic radio signal interference map is based on measurements x.sub.n, y.sub.n at the point in time n, wherein x.sub.n.di-elect cons..sup.2 is the geographic spatial coordinate of the reporting subscriber or user.  In\nthis case, y.sub.n.di-elect cons.  then represents the total measured interference at the coordinate x.sub.n.  The relationship between y.sub.n and x.sub.n reads y.sub.n:=f(x.sub.n)+.di-elect cons., wherein f:.sup.2.fwdarw.  represents an unknown\nfunction and .di-elect cons.  indicates the measurement error.\nAs considered from a more practically oriented standpoint, what is principally of interest is learning the interference of a set of discrete geographic locations.  The latter can be abstracted as pixels of the interference map.  Therefore, we\ncan define V.di-elect cons..sup.X.sup.1.sup..times.X.sup.2 as an interference matrix which maps the geographic coordinates onto a total interference value.  Therefore, each element of the matrix can be regarded as an aggregated interference at the\ncorresponding geographic location.  On the basis of the current estimation of f, it is possible to find an approximation for {tilde over (V)}.  At the point in time t, communication subscribers or communication users in the cell report their\nmeasurements, such as RSSI measurements, for example.  In this case, not only the D2D communication users but also other mobile radio subscribers can provide reports.  These measurements v(x,t).di-elect cons.  regarding their current positions X.di-elect\ncons..sup.2 are communicated or reported to the corresponding base station of the cell.  As soon as these data reach the base station, they can be utilized by means of a then estimated {tilde over (V)} for an updating of the true interference matrix V.\nAssuming that a D2D subscriber or D2D communication receiver r is able to measure the total received power and to generate a corresponding report, such a report of a D2D communication receiver at the point in time t can be interpreted as:\n.gamma.(t,x.sub.s,x.sub.r)=p.sub.sh.sub.sr(t)+V(t,x.sub.r)\nIn this case, x.sub.s is the geographic location of the D2D communication transmitter, x.sub.r is the geographic location of the D2D communication receiver, p.sub.s is the transmission power of the D2D communication transmitter, and h.sub.sr(t)\nis the channel between the two D2D communication subscribers.\nOne solution approach here may involve utilizing the knowledge about {tilde over (V)}(t,x.sub.r) and p.sub.s at the base station to obtain an estimated value of the channel h.sub.sr(t) as h.sub.sr(t), where:\n.function..gamma..function..function.  ##EQU00001##\nFIG. 2 shows a schematic illustration of a proposed method in accordance with a further exemplary embodiment of the invention.\nIn this case, FIG. 2 shows a schematic illustration of a method that has been developed further with respect to FIG. 1.  The statements made previously in respect of FIG. 1 therefore also hold true for FIG. 2.\nFIG. 2 shows the method from FIG. 1 in which, furthermore, a determination of the channel quality condition regarding the statement about the channel damping comprises: ascertaining 21 a total received power at a receiving mobile radio\ncommunication terminal, determining 22 a transmitted power for a transmitting mobile radio communication terminal, and ascertaining 23 the channel damping, on the basis of a division of a subtraction of the location-dependent radio signal interference\ndisturbance from the total received power at the receiving mobile radio communication terminal by the transmitted power for the transmitting mobile radio communication terminal, and in this case the transmitting mobile radio communication terminal\ncorresponds to the first mobile radio communication terminal 210 and the receiving mobile radio communication terminal corresponds to the second mobile radio communication terminal 220.\nFIG. 3 shows a schematic illustration of a proposed method in accordance with a further exemplary embodiment of the invention.\nIn this case, FIG. 3 shows a schematic illustration of a method that has been developed further with respect to FIG. 1.  The statements made previously in respect of FIG. 1 therefore also hold true for FIG. 3.\nFIG. 3 shows the method from FIG. 1 in which, furthermore, providing 10 the radio signal interference map 110 is carried out on the basis of measurement information from mobile radio communication devices 200, 210, 220 connected to the base\nstation 100.  In this case moreover the measurement information is based on measurement information and/or protocols as provided at least in a long term evolution standard or 3.9G standard.  In this case, a mobile radio standard defined more highly than\n3.9G or 4G for a cellular mobile radio communication network may also be provided.\nIn order better to distinguish between a mobile radio communication device 200 which is generally connected to a base station and a mobile radio communication device 210, 200 which is utilized for terminal-to-terminal communication 130 in the\ncellular mobile radio network, the latter mobile radio communication device is called a mobile radio communication terminal 210, 200.\nFIG. 4 shows a schematic illustration of a proposed system for terminal-to-terminal communication 130 in a cellular mobile radio network, in accordance with a further exemplary embodiment of the invention.\nAs can be gathered from FIG. 4, the system comprises: a base station 100 of a mobile radio system, a first mobile radio communication terminal 210 and a second mobile radio communication terminal 220.  In this case, the base station 100 is\ndesigned to provide terminal-to-terminal mobile radio communication devices 200, 210, 220 connected to the base station 100 with corresponding information, regarding configuration information 120, regarding a configuration setting of a radio transmission\nfor a terminal-to-terminal communication 130.  In this case moreover the system is designed to implement a method according to the invention.\nIn FIG. 4, by way of example, three further mobile radio communication devices 200 are also illustrated over and above the two mobile radio communication terminals 210, 220.  Said mobile radio communication devices do not themselves participate\nin the terminal-to-terminal communication 130, but transmit their measurement results to the base station 100 in order that the latter can keep the radio signal interference map 110 up to date, and from said map can make the information required for the\nterminal-to-terminal communication 130 available to the two communication subscribers 210, 220 illustrated in FIG. 4.\nOne application scenario for such a C-ITS application by means of D2D communication might be for example a truck journey with cars traveling behind the truck on a country road.  In this case, the truck may be equipped with a forward-looking\nvideo camera, for example for a lane and distance keeping assistance system.  In this scenario, the video feed of said camera can then be communicated for example to the car or cars traveling behind the truck via D2D communication, such that the driver\nor drivers of the car or cars situated behind the truck can see, on their respective internal display by means of the received video feed of the truck camera, what the traffic situation is in front of the truck.  As a result, a driver of a car is better\nable to decide whether he/she ought to initiate a maneuver to overtake the truck or ought better to wait.  In this way, accidents can be avoided since important information that has not been available hitherto is available for the overtaking decision.\nThe concept of the invention can be summarized as follows.  A method and a system therefor are provided, whereby it may become possible to provide a D2D communication, also called terminal-to-terminal communication, for a cellular mobile radio\ncommunication network.  In this case, the base station that is responsible in the cell provides an interference map for its cell.  In this case, the measurement data for generating said interference map are provided by mobile radio communication\nsubscribers in the corresponding cell.  By means of the interference map generated, a channel interference estimation for the utilized channel of a D2D communication can be made available for the D2D communication subscribers utilizing said channel, such\nthat the D2D communication subscribers can correspondingly adapt their communication.  In this case, the D2D communication can be carried out exclusively between the subscribers, without the corresponding base station participating therein.\n<BR><BR>LIST OF REFERENCE SIGNS\n10 Providing a radio signal interference map 20 Providing configuration information regarding a configuration setting of a radio transmission for a terminal-to-terminal communication 21 Ascertaining a total received power at a receiving mobile\nradio communication terminal 22 Determining a transmitted power for a transmitting mobile radio communication terminal 23 Ascertaining a channel damping 30 Setting up a terminal-to-terminal communication 100 Base station 110 Radio signal interference map\n120 Configuration information regarding a configuration setting 130 Terminal-to-terminal communication 200 Mobile radio communication device 210 First mobile radio communication terminal 220 Second mobile radio communication terminal\nThe foregoing disclosure has been set forth merely to illustrate the invention and is not intended to be limiting.  Since modifications of the disclosed embodiments incorporating the spirit and substance of the invention may occur to persons\nskilled in the art, the invention should be construed to include everything within the scope of the appended claims and equivalents thereof.", "application_number": "15921445", "abstract": " A method for terminal-to-terminal communication in a cellular mobile\n     radio network includes providing a radio signal interference map that is\n     indicative of a location-dependent radio signal interference disturbance\n     within a coverage area of a base station. The location-dependent radio\n     signal interference disturbance is indicative of a radio signal\n     interference disturbance of a terminal-to-terminal communication, and the\n     radio signal interference map is based on a learning algorithm for a\n     geographic radio signal interference map. Configuration information\n     regarding a configuration setting of a radio transmission for a\n     terminal-to-terminal communication between a first mobile radio\n     communication terminal and a second mobile radio communication terminal\n     is provided based on the location-dependent radio signal interference\n     disturbance of the radio signal interference map. The configuration\n     setting enables a statement regarding a configuration to be performed in\n     respect of the radio transmission. The method also includes setting up a\n     terminal-to-terminal communication between the first mobile radio\n     communication terminal and the second mobile radio communication\n     terminal, taking into account of the provided configuration information\n     regarding the configuration setting.\n", "citations": ["9125071", "10200937", "20130150106", "20130294296", "20140141789", "20150223257"], "related": ["2016069852"]}, {"id": "20180278629", "patent_code": "10375090", "patent_name": "Machine learning-based traffic classification using compressed network\n     telemetry data", "year": "2019", "inventor_and_country_data": " Inventors: \nMcGrew; David (Poolesville, MD), Anderson; Blake Harrell (San Jose, CA)  ", "description": "<BR><BR>TECHNICAL FIELD\nThe present disclosure relates generally to computer networks, and, more particularly, to machine learning-based traffic classification using compressed network telemetry data.\n<BR><BR>BACKGROUND\nEnterprise networks are carrying a very fast growing volume of both business and non-business critical traffic.  Often, business applications such as video collaboration, cloud applications, etc., use the same hypertext transfer protocol (HTTP)\nand/or HTTP secure (HTTPS) techniques that are used by non-business critical web traffic.  This complicates the task of optimizing network performance for specific applications, as many applications use the same protocols, thus making it difficult to\ndistinguish and select traffic flows for optimization.\nBeyond the various types of legitimate application traffic in a network, some network traffic may also be malicious.  For example, some traffic may seek to overwhelm a service by sending a large number of requests to the service.  Such attacks\nare also sometimes known as denial of service (DoS) attacks.  Other forms of malicious traffic may seek to exfiltrate sensitive information from a network, such as credit card numbers, trade secrets, and the like.  Typically, such traffic is generated by\na client that has been infected with malware.  Thus, further types of malicious network traffic include network traffic that propagate the malware itself and network traffic that passes control commands to already infected devices. <BR><BR>BRIEF\nDESCRIPTION OF THE DRAWINGS\nThe embodiments herein may be better understood by referring to the following description in conjunction with the accompanying drawings in which like reference numerals indicate identically or functionally similar elements, of which:\nFIGS. 1A-1B illustrate an example communication network;\nFIG. 2 illustrates an example network device/node;\nFIG. 3 illustrates an example of a device capturing traffic information;\nFIG. 4 illustrates an example architecture for classifying a traffic flow using compressed telemetry data;\nFIGS. 5A-5B illustrate examples of a device analyzing telemetry data; and\nFIG. 6 illustrates an example simplified procedure for using compressed network telemetry data to classify traffic in a network.\n<BR><BR>DESCRIPTION OF EXAMPLE EMBODIMENTS\n<BR><BR>Overview\nAccording to one or more embodiments of the disclosure, a device in a network receives telemetry data regarding a traffic flow in the network.  One or more features in the telemetry data are individually compressed.  The device extracts the one\nor more individually compressed features from the received telemetry data.  The device performs a lookup of one or more classifier inputs from an index of classifier inputs using the one or more individually compressed features from the received\ntelemetry data.  The device classifies the traffic flow by inputting the one or more classifier inputs to a machine learning-based classifier.\n<BR><BR>Description\nA computer network is a geographically distributed collection of nodes interconnected by communication links and segments for transporting data between end nodes, such as personal computers and workstations, or other devices, such as sensors,\netc. Many types of networks are available, with the types ranging from local area networks (LANs) to wide area networks (WANs).  LANs typically connect the nodes over dedicated private communications links located in the same general physical location,\nsuch as a building or campus.  WANs, on the other hand, typically connect geographically dispersed nodes over long-distance communications links, such as common carrier telephone lines, optical lightpaths, synchronous optical networks (SONET), or\nsynchronous digital hierarchy (SDH) links, or Powerline Communications (PLC) such as IEEE 61334, IEEE P1901.2, and others.  The Internet is an example of a WAN that connects disparate networks throughout the world, providing global communication between\nnodes on various networks.  The nodes typically communicate over the network by exchanging discrete frames or packets of data according to predefined protocols, such as the Transmission Control Protocol/Internet Protocol (TCP/IP).  In this context, a\nprotocol consists of a set of rules defining how the nodes interact with each other.  Computer networks may further be interconnected by an intermediate net81work node, such as a router, to extend the effective \"size\" of each network.\nSmart object networks, such as sensor networks, in particular, are a specific type of network having spatially distributed autonomous devices such as sensors, actuators, etc., that cooperatively monitor physical or environmental conditions at\ndifferent locations, such as, e.g., energy/power consumption, resource consumption (e.g., water/gas/etc. for advanced metering infrastructure or \"AMI\" applications) temperature, pressure, vibration, sound, radiation, motion, pollutants, etc. Other types\nof smart objects include actuators, e.g., responsible for turning on/off an engine or perform any other actions.  Sensor networks, a type of smart object network, are typically shared-media networks, such as wireless networks.  That is, in addition to\none or more sensors, each sensor device (node) in a sensor network may generally be equipped with a radio transceiver or other communication port, a microcontroller, and an energy source, such as a battery.  Often, smart object networks are considered\nfield area networks (FANs), neighborhood area networks (NANs), personal area networks (PANs), etc. Generally, size and cost constraints on smart object nodes (e.g., sensors) result in corresponding constraints on resources such as energy, memory,\ncomputational speed and bandwidth.\nFIG. 1A is a schematic block diagram of an example computer network 100 illustratively comprising nodes/devices, such as a plurality of routers/devices interconnected by links or networks, as shown.  For example, customer edge (CE) routers 110\nmay be interconnected with provider edge (PE) routers 120 (e.g., PE-1, PE-2, and PE-3) in order to communicate across a core network, such as an illustrative network backbone 130.  For example, routers 110, 120 may be interconnected by the public\nInternet, a multiprotocol label switching (MPLS) virtual private network (VPN), or the like.  Data packets 140 (e.g., traffic/messages) may be exchanged among the nodes/devices of the computer network 100 over links using predefined network communication\nprotocols such as the Transmission Control Protocol/Internet Protocol (TCP/IP), User Datagram Protocol (UDP), Asynchronous Transfer Mode (ATM) protocol, Frame Relay protocol, or any other suitable protocol.  Those skilled in the art will understand that\nany number of nodes, devices, links, etc. may be used in the computer network, and that the view shown herein is for simplicity.\nIn some implementations, a router or a set of routers may be connected to a private network (e.g., dedicated leased lines, an optical network, etc.) or a virtual private network (VPN), such as an MPLS VPN, thanks to a carrier network, via one or\nmore links exhibiting very different network and service level agreement characteristics.  For the sake of illustration, a given customer site may fall under any of the following categories:\n1.) Site Type A: a site connected to the network (e.g., via a private or VPN link) using a single CE router and a single link, with potentially a backup link (e.g., a 3G/4G/LTE backup connection).  For example, a particular CE router 110 shown\nin network 100 may support a given customer site, potentially also with a backup link, such as a wireless connection.\n2.) Site Type B: a site connected to the network using two MPLS VPN links (e.g., from different service providers), with potentially a backup link (e.g., a 3G/4G/LTE connection).  A site of type B may itself be of different types:\n2a.) Site Type B1: a site connected to the network using two MPLS VPN links (e.g., from different service providers), with potentially a backup link (e.g., a 3G/4G/LTE connection).\n2b.) Site Type B2: a site connected to the network using one MPLS VPN link and one link connected to the public Internet, with potentially a backup link (e.g., a 3G/4G/LTE connection).  For example, a particular customer site may be connected to\nnetwork 100 via PE-3 and via a separate Internet connection, potentially also with a wireless backup link.\n2c.) Site Type B3: a site connected to the network using two links connected to the public Internet, with potentially a backup link (e.g., a 3G/4G/LTE connection).\nNotably, MPLS VPN links are usually tied to a committed service level agreement, whereas Internet links may either have no service level agreement at all or a loose service level agreement (e.g., a \"Gold Package\" Internet service connection that\nguarantees a certain level of performance to a customer site).\n3.) Site Type C: a site of type B (e.g., types B1, B2 or B3) but with more than one CE router (e.g., a first CE router connected to one link while a second CE router is connected to the other link), and potentially a backup link (e.g., a\nwireless 3G/4G/LTE backup link).  For example, a particular customer site may include a first CE router 110 connected to PE-2 and a second CE router 110 connected to PE-3.\nFIG. 1B illustrates an example of network 100 in greater detail, according to various embodiments.  As shown, network backbone 130 may provide connectivity between devices located in different geographical areas and/or different types of local\nnetworks.  For example, network 100 may comprise local networks 160, 162 that include devices/nodes 10-16 and devices/nodes 18-20, respectively, as well as a data center/cloud environment 150 that includes servers 152-154.  Notably, local networks\n160-162 and data center/cloud environment 150 may be located in different geographic locations.\nServers 152-154 may include, in various embodiments, a network management server (NMS), a dynamic host configuration protocol (DHCP) server, a constrained application protocol (CoAP) server, an outage management system (OMS), an application\npolicy infrastructure controller (APIC), an application server, etc. As would be appreciated, network 100 may include any number of local networks, data centers, cloud environments, devices/nodes, servers, etc.\nThe techniques herein may also be applied to other network topologies and configurations.  For example, the techniques herein may be applied to peering points with high-speed links, data centers, etc. Further, in various embodiments, network 100\nmay include one or more mesh networks, such as an Internet of Things network.  Loosely, the term \"Internet of Things\" or \"IoT\" refers to uniquely identifiable objects/things and their virtual representations in a network-based architecture.  In\nparticular, the next frontier in the evolution of the Internet is the ability to connect more than just computers and communications devices, but rather the ability to connect \"objects\" in general, such as lights, appliances, vehicles, heating,\nventilating, and air-conditioning (HVAC), windows and window shades and blinds, doors, locks, etc. The \"Internet of Things\" thus generally refers to the interconnection of objects (e.g., smart objects), such as sensors and actuators, over a computer\nnetwork (e.g., via IP), which may be the public Internet or a private network.\nNotably, shared-media mesh networks, such as wireless networks, etc., are often on what is referred to as Low-Power and Lossy Networks (LLNs), which are a class of network in which both the routers and their interconnect are constrained.  In\nparticular, LLN routers typically operate with highly constrained resources, e.g., processing power, memory, and/or energy (battery), and their interconnections are characterized by, illustratively, high loss rates, low data rates, and/or instability. \nLLNs are comprised of anything from a few dozen to thousands or even millions of LLN routers, and support point-to-point traffic (e.g., between devices inside the LLN), point-to-multipoint traffic (e.g., from a central control point such at the root node\nto a subset of devices inside the LLN), and multipoint-to-point traffic (e.g., from devices inside the LLN towards a central control point).  Often, an IoT network is implemented with an LLN-like architecture.  For example, as shown, local network 160\nmay be an LLN in which CE-2 operates as a root node for nodes/devices 10-16 in the local mesh, in some embodiments.\nFIG. 2 is a schematic block diagram of an example node/device 200 that may be used with one or more embodiments described herein, e.g., as any of the computing devices shown in FIGS. 1A-1B, particularly the PE routers 120, CE routers 110,\nnodes/device 10-20, servers 152-154 (e.g., a network controller located in a data center, etc.), any other computing device that supports the operations of network 100 (e.g., switches, etc.), or any of the other devices referenced below.  The device 200\nmay also be any other suitable type of device depending upon the type of network architecture in place, such as IoT nodes, etc. Device 200 comprises one or more network interfaces 210, one or more processors 220, and a memory 240 interconnected by a\nsystem bus 250, and is powered by a power supply 260.\nThe network interfaces 210 include the mechanical, electrical, and signaling circuitry for communicating data over physical links coupled to the network 100.  The network interfaces may be configured to transmit and/or receive data using a\nvariety of different communication protocols.  Notably, a physical network interface 210 may also be used to implement one or more virtual network interfaces, such as for virtual private network (VPN) access, known to those skilled in the art.\nThe memory 240 comprises a plurality of storage locations that are addressable by the processor(s) 220 and the network interfaces 210 for storing software programs and data structures associated with the embodiments described herein.  The\nprocessor 220 may comprise necessary elements or logic adapted to execute the software programs and manipulate the data structures 245.  An operating system 242 (e.g., the Internetworking Operating System, or IOS.RTM., of Cisco Systems, Inc., another\noperating system, etc.), portions of which are typically resident in memory 240 and executed by the processor(s), functionally organizes the node by, inter alia, invoking network operations in support of software processors and/or services executing on\nthe device.  These software processors and/or services may comprise a traffic analysis process 248.\nIt will be apparent to those skilled in the art that other processor and memory types, including various computer-readable media, may be used to store and execute program instructions pertaining to the techniques described herein.  Also, while\nthe description illustrates various processes, it is expressly contemplated that various processes may be embodied as modules configured to operate in accordance with the techniques herein (e.g., according to the functionality of a similar process). \nFurther, while processes may be shown and/or described separately, those skilled in the art will appreciate that processes may be routines or modules within other processes.\nIn general, traffic analysis process 248 may execute one or more machine learning-based classifiers to classify encrypted traffic in the network (and its originating application) for any number of purposes.  In one embodiment, traffic analysis\nprocess 248 may assess captured telemetry data regarding one or more traffic flows, to determine whether a given traffic flow or set of flows are caused by malware in the network, such as a particular family of malware applications.  Example forms of\ntraffic that can be caused by malware may include, but are not limited to, traffic flows reporting exfiltrated data to a remote entity, spyware or ransomware-related flows, command and control (C2) traffic that oversees the operation of the deployed\nmalware, traffic that is part of a network attack, such as a zero day attack or denial of service (DoS) attack, combinations thereof, or the like.  In further embodiments, traffic analysis process 248 may classify the gathered telemetry data to detect\nother anomalous behaviors (e.g., malfunctioning devices, misconfigured devices, etc.), traffic pattern changes (e.g., a group of hosts begin sending significantly more or less traffic), or the like.\nTraffic analysis process 248 may employ any number of machine learning techniques, to classify the gathered telemetry data.  In general, machine learning is concerned with the design and the development of techniques that receive empirical data\nas input (e.g., telemetry data regarding traffic in the network) and recognize complex patterns in the input data.  For example, some machine learning techniques use an underlying model M, whose parameters are optimized for minimizing the cost function\nassociated to M, given the input data.  For instance, in the context of classification, the model M may be a straight line that separates the data into two classes (e.g., labels) such that M=a*x+b*y+c and the cost function is a function of the number of\nmisclassified points.  The learning process then operates by adjusting the parameters a,b,c such that the number of misclassified points is minimal.  After this optimization/learning phase, traffic analysis 248 can use the model M to classify new data\npoints, such as information regarding new traffic flows in the network.  Often, M is a statistical model, and the cost function is inversely proportional to the likelihood of M, given the input data.\nIn various embodiments, traffic analysis process 248 may employ one or more supervised, unsupervised, or semi-supervised machine learning models.  Generally, supervised learning entails the use of a training set of data, as noted above, that is\nused to train the model to apply labels to the input data.  For example, the training data may include sample telemetry data that is \"normal,\" or \"malware-generated.\" On the other end of the spectrum are unsupervised techniques that do not require a\ntraining set of labels.  Notably, while a supervised learning model may look for previously seen attack patterns that have been labeled as such, an unsupervised model may instead look to whether there are sudden changes in the behavior of the network\ntraffic.  Semi-supervised learning models take a middle ground approach that uses a greatly reduced set of labeled training data.\nExample machine learning techniques that traffic analysis process 248 can employ may include, but are not limited to, nearest neighbor (NN) techniques (e.g., k-NN models, replicator NN models, etc.), statistical techniques (e.g., Bayesian\nnetworks, etc.), clustering techniques (e.g., k-means, mean-shift, etc.), neural networks (e.g., reservoir networks, artificial neural networks, etc.), support vector machines (SVMs), logistic or other regression, Markov models or chains, principal\ncomponent analysis (PCA) (e.g., for linear models), multi-layer perceptron (MLP) ANNs (e.g., for non-linear models), replicating reservoir networks (e.g., for non-linear models, typically for time series), random forest classification, or the like.\nThe performance of a machine learning model can be evaluated in a number of ways based on the number of true positives, false positives, true negatives, and/or false negatives of the model.  For example, the false positives of the model may\nrefer to the number of traffic flows that are incorrectly classified as malware-generated, anomalous, etc. Conversely, the false negatives of the model may refer to the number of traffic flows that the model incorrectly classifies as normal, when\nactually malware-generated, anomalous, etc. True negatives and positives may refer to the number of traffic flows that the model correctly classifies as normal or malware-generated, etc., respectively.  Related to these measurements are the concepts of\nrecall and precision.  Generally, recall refers to the ratio of true positives to the sum of true positives and false negatives, which quantifies the sensitivity of the model.  Similarly, precision refers to the ratio of true positives the sum of true\nand false positives.\nIn some cases, traffic analysis process 248 may assess the captured telemetry data on a per-flow basis.  In other embodiments, traffic analysis 248 may assess telemetry data for a plurality of traffic flows based on any number of different\nconditions.  For example, traffic flows may be grouped based on their sources, destinations, temporal characteristics (e.g., flows that occur around the same time, etc.), combinations thereof, or based on any other set of flow characteristics.\nAs shown in FIG. 3, various mechanisms can be leveraged to capture information about traffic in a network, such as telemetry data regarding a traffic flow.  For example, consider the case in which client node 10 initiates a traffic flow with\nremote server 154 that includes any number of packets 302.  Any number of networking devices along the path of the flow may analyze and assess packet 302, to capture telemetry data regarding the traffic flow.  For example, as shown, consider the case of\nedge router CE-2 through which the traffic between node 10 and server 154 flows.\nIn some embodiments, a networking device may analyze packet headers, to capture feature information about the traffic flow.  For example, router CE-2 may capture the source address and/or port of host node 10, the destination address and/or port\nof server 154, the protocol(s) used by packet 302, or other header information by analyzing the header of a packet 302.  Example captured features may include, but are not limited to, Transport Layer Security (TLS) information (e.g., from a TLS\nhandshake), such as the ciphersuite offered, user agent, TLS extensions, etc., HTTP information (e.g., URI, etc.), Domain Name System (DNS) information, or any other data features that can be extracted from the observed traffic flow(s).\nIn further embodiments, the device may also assess the payload of the packet to capture information about the traffic flow.  For example, router CE-2 or another device may perform deep packet inspection (DPI) on one or more of packets 302, to\nassess the contents of the packet.  Doing so may, for example, yield additional information that can be used to determine the application associated with the traffic flow (e.g., packets 302 were sent by a web browser of node 10, packets 302 were sent by\na videoconferencing application, etc.).\nThe networking device that captures the flow telemetry data may also compute any number of statistics or metrics regarding the traffic flow.  For example, CE-2 may determine the start time, end time, duration, packet size(s), the distribution of\nbytes within a flow, etc., associated with the traffic flow by observing packets 302.  In further examples, the capturing device may capture sequence of packet lengths and time (SPLT) data regarding the traffic flow, sequence of application lengths and\ntime (SALT) data regarding the traffic flow, or byte distribution (BD) data regarding the traffic flow.\nAs noted above, a traffic classifier may leverage captured flow telemetry data, to classify a given traffic flow or set of traffic flows (e.g., to identify malware-related flows, to associate a given flow with an application, to perform network\nforensics, etc.).  However, such telemetry data may include features that are not typically captured or reported.  Notably, the amount of telemetry data needed for the classifier may be much greater than typically seen in existing telemetry mechanisms,\nsuch as NetFlow from Cisco Systems, Inc.  and IP Flow Information Export (IPFIX).  For example, a typical NetFlow record may approximately 40 bytes in size per flow, whereas some of the telemetry features mentioned above can exceed over a kilobyte per\nflow when combined.  This increase in data size may also lead to a corresponding increase in resource requirements, such as bandwidth, storage, etc.\nMachine Learning-Based Traffic Classification Using Compressed Network Telemetry Data\nThe techniques herein allow for the use of compressed network telemetry data to classify traffic flows in the network.  In some aspects, features in the telemetry data may be individually compressed and the traffic analyzer may be configured to\nprocess the compressed data.  In further aspects, various approaches to the feature compression are introduced herein that significantly compact the network telemetry data during transmission and/or storage, without losing any of the information that is\nimportant for applying machine learning to the data.  For example, the techniques herein may employ dictionary compression and classification in a way that reduces the overall computational cost, with no change in the accuracy of the classification.\nSpecifically, according to one or more embodiments of the disclosure as described in detail below, a device in a network receives telemetry data regarding a traffic flow in the network.  One or more features in the telemetry data are\nindividually compressed.  The device extracts the one or more individually compressed features from the received telemetry data.  The device performs a lookup of one or more classifier inputs from an index of classifier inputs using the one or more\nindividually compressed features from the received telemetry data.  The device classifies the traffic flow by inputting the one or more classifier inputs to a machine learning-based classifier.\nIllustratively, the techniques described herein may be performed by hardware, software, and/or firmware, such as in accordance with the traffic analysis process 248, which may include computer executable instructions executed by the processor\n220 (or independent processor of interfaces 210) to perform functions relating to the techniques described herein.\nOperationally, FIG. 4 illustrates an example architecture 400 for classifying a traffic flow using compressed telemetry data.  As shown, traffic analyzer process 248 may include any number of sub-processes and/or may access any number of memory\nlocations.  As would be appreciated, these sub-processes and/or memory locations may be located on the same device or implemented in a distributed manner across multiple devices, the combination of which may be viewed as a single system/device that\nexecutes traffic analyzer process 248.  Further, while certain functionalities are described with respect to the sub-processes and memory locations, these functions can be added, removed, or combined as desire, in further implementations.\nDuring operation, traffic analysis process 248 may receive compressed telemetry data 402 from one or more sources.  For example, the device executing traffic analysis process 248 may capture and compress compressed telemetry data 402 locally\nand/or receive compressed telemetry data 402 from one or more exporters via the network.  In various embodiments, compressed telemetry data 402 may be Netflow or IPFIX records, or the like.  Note that an IPFIX information element is equivalent to a data\nfeature, as described herein, with the former term being used typically in the field of network telemetry and the latter term being used typically in the field of data science.  In further embodiments, one or more of the features in compressed telemetry\ndata 402 may be individually compressed.  For example, SALT or BD information in compressed telemetry data 402 may be compressed on its own, HTTP URIs can be compressed on their own, etc. Such compression may employ different compression techniques, as\nwell, depending on the specific feature being compressed.\nIn some embodiments, a telemetry preprocessor 404 may preprocess the features included in compressed telemetry data 402 and operate in conjunction with an indexer 406, to populate a compressed feature index 408.  In general, compressed feature\nindex 408 may comprise a table or other data structure that stores the processed form of a feature from preprocessor 404 and using the compressed form of that feature as an index.  For example, a compressed URI may be associated with one or more strings\nextracted from the uncompressed form of the URI (e.g., the top-level domain, keywords, etc.).  In another example, the system can be applied to data features that are derived from the raw network data features, such as the histograms of lengths and\ntimes, to convert SPLT information into a form suitable to input to a machine learning-based classifier 412.  In particular, each cell in the histogram counts the number of packet lengths or times that fall within a particular range.  Thus, this approach\nresults in an approximate match for the data feature.\nAs would be appreciated, any form of processing may be performed on the uncompressed features and associated with a compressed index, prior to use as input to machine learning-based classifier 412.  Note that this is most useful when the actions\nof the classifier on each type of input data feature is substantially independent of that on other features, such as in the case of regression-based classifiers (e.g., linear, logistic, etc.), random forests, and certain other types of classifiers.\nIn various embodiments, classifier 412 may be trained using training data 414 which may include labeled feature data.  For example, in the case of application classification, training data 414 may include features found in captured telemetry\ndata indicative of a certain type of application (e.g., a videoconferencing application, a web browser, etc.).  In further cases, training data 414 may include labeled features from traffic associated with malware.  For example, certain telemetry\nfeatures of a traffic flow may be indicative of the flow being generally associated with malware and/or a certain class or type of malware.  Such training data 414 may be obtained by labeling observed traffic in a live network, traffic from a sandbox\nenvironment (e.g., by executing a certain application type or malware type, etc.), or even in a synthetic manner by extending features of observed traffic to unobserved situations.\nTo assess a certain set of compressed telemetry data 402, traffic analyzer process 248 may execute a lookup engine 410.  Generally, lookup engine 410 may compare the individually compressed features in telemetry data 402 to the indices in\ncompressed feature index 408, to obtain the inputs for classifier 412.  For example, lookup engine 410 may construct a feature vector of classifier inputs, based on the individually compressed features in compressed telemetry data 402.  In turn,\nclassifier 412 may output one or more traffic classifications 416.\nIn various cases, traffic analysis process 248 may use a traffic classification 416 to cause an action to be performed in the network.  For example, in the case of the traffic flow being classified as malware-related, traffic analysis process\n248 may cause a notification to be sent (e.g., to the user of the infected device, a network administrator, etc.), may cause the network to begin blocking or redirecting traffic, or cause any other form of mitigation action to be taken.\nBy way of example, assume that machine learning-based classifier 412 classifies TLS ciphersuites using a vector of binary features whereby a zero means that the ciphersuite was not present in the clientHello message and vice-versa.  Ciphersuites\nare normally represented with 16 bits and a set of 10-16 ciphersuites are commonly used, with another approximately twenty that are not uncommon, and another one hundred or so that appear infrequently.  The compression would be able to represent each\nciphersuite in compressed telemetry data 402 with an amortized cost of approximately 4-5 bits.  Now, traffic analyzer process 248 can decompress the ciphersuites according to the compression dictionary, assign a zero or one to the appropriate element in\nthe feature vector, and finally send the feature vector to classifier 412.  For categorical features, like the ciphersuites, this decompression is redundant.  In other words, the value itself is unimportant and all that matters are the presence and the\nmapping to the appropriate place in the feature vector, which can be handled without any decompression.  For non-categorical inputs (e.g., integers such as the number of bytes in a packet, etc.), however, the actual value may be important.  In such\ncases, traffic analyzer process 248 may build a decompression context for this set.  Once traffic analyzer process 248 needs to classify a specific sample with a non-categorical feature, it may decompress the specific element and that element will be\nused as input to classifier 412.\nFIGS. 5A-5B illustrate examples of a device analyzing telemetry data, according to various embodiments.  As shown in FIG. 5A, any number of telemetry exporters 502a-502n (e.g., a first through nth exporter) may observe traffic flows in the\nnetwork, capture telemetry data regarding the observed flows, and export compressed telemetry data 506 to a collector/analyzer device 504.\nIn various embodiments, telemetry exporters 502 may compress the reported telemetry features on an individual and independent basis in exported telemetry data 506.  For example, telemetry exporter 502a may compress SPLT data features, DNS names,\nHTTP URI information, etc., individually and independently of one another for inclusion in the exported telemetry data 506.  That is, a separate data compression context may be maintained for each information element in a Netflow or IPFIX template set.\nIn some cases, a reliable transport protocol such as TCP, Stream Control Transmission Protocol (SCTP), or the like, may be used between telemetry exporters 502 and collector 504.  Doing so ensures that the order in which an exporter 502 sends\ninformation elements in telemetry data 506 is identical to the order in which collector 504 receives them and there are no elements are lost during transmission.  By using a reliable transport protocol, a dictionary-type compression can be used to\ncompress the sequence of features of a certain type.  For example, if a particular instance of a data feature/element is identical to the 7th data element of that type, then the compressor of the exporter 502 can encode that fact and send it, instead of\nsending the whole data element.  One class of dictionary-based compression techniques that can be used is Lempel-Ziv compression, such as LZ78, although other forms of compression can be used, in other embodiments.\nThe details of the compression mechanism used by an exporter 502 can be tailored to the specific data feature.  In many cases, if an instance of a data feature is an each match for an earlier one, then a reference to the earlier instance is used\nin exported telemetry data 506.  Notably, in some instances, it may be acceptable for a reference to an earlier instance to be sent via exported telemetry data 506, whenever a data feature is an approximate match for an earlier data feature.  For\nexample, this could be done for SPLT data by considering lengths and times to match if they are within a certain threshold.  For DNS names, a standard LZ78 approach can be taken.  In this case, it may be beneficial to apply the compression from right to\nleft, so that names with a common root domain will match.  Preliminary testing indicates that applying compression in this manner can result in a compression factor of approximately ten.\nIn another example, LZ78 compression can be applied to HTTP URIs by replacing a target string with the longest, previously seen string that matches the prefix of the target string, appending the next element of the target string, and adding that\nstring to the dictionary.  Variations of the LZ78 method that use an operation other than the append operation may be useful in some implementations, as well.  For instance, the target string can be represented by adding a small delta vector to the\nclosest vector in a dictionary.\nAs shown in FIG. 5B, the collector/analyzer device 504 may use the exported, compressed telemetry data 506 to classify the corresponding traffic flow(s).  In some embodiments, device 504 may employ a traffic analyzer that can operate directly on\ncompressed telemetry data, such as by employing architecture 400 shown in FIG. 4.  In further embodiments, device 504 may decompress telemetry data 506 and classify the decompressed features.  In one embodiment, a custom IPFIX \"options template\" may be\nused, which is sent by an exporter 502 to indicate that a new compression context is being used and enabling a decompression context of device 504 to stay in sync.\nIn cases in which collector/analyzer device 504 must first decompress the exported telemetry data 506 before classifying a traffic flow, device 504 may use a corresponding dictionary-style decompressor to decompress the individually compressed\nfeatures in telemetry data 506.  In turn, device 504 can input the decompressed features directly into its classifier or into a rules engine.\nIn another embodiment, device 504 may store the compressed form of telemetry data 504 for later decompression and use.  However, in this case, device 504 will also need to store information about the decompression context (e.g., information\nabout what features/elements have been compressed with a particular compression context), thus increasing storage overhead.\nAs would be appreciated, an exporter 502 first compressing the features of exported telemetry data 506 will save bandwidth, but does not necessarily reduce the data storage requirements of device 504.  Alternatively, in further embodiments,\nexporters 502 may simply export the features/information elements of telemetry data 506 in an uncompressed form.  In turn, collector/analyzer device 504 may compress telemetry data 506 prior to storage, to save on data storage costs.\nFIG. 6 illustrates an example simplified procedure for using compressed network telemetry data to classify traffic in a network, in accordance with one or more embodiments described herein.  For example, a non-generic, specifically configured\ndevice (e.g., device 200) may perform procedure 600 by executing stored instructions (e.g., process 248).  The procedure 600 may start at step 605, and continues to step 610, where, as described in greater detail above, the device may receive telemetry\ndata regarding a traffic flow in the network.  In various embodiments, one or more features in the telemetry data may be individually/independently compressed.  For example, a HTTP URI data feature in the telemetry data may be compressed using LZ78, etc.\nAt step 615, as detailed above, the device may extract the one or more individually compressed features from the received telemetry data.  For example, the device may identify a compressed HTTP URI data feature from the received telemetry data.\nAt step 620, the device may perform a lookup of one or more classifier inputs from an index of classifier inputs, as described in greater detail above.  For example, the device may use the one or more individually compressed features from the\nreceived telemetry data to find a matching index value in the index and identify a corresponding classifier input.  Such a classifier input may, for example, be in a processed form for consumption by the classifier.\nAt step 625, as detailed above, the device may classify the traffic flow by inputting the one or more classifier inputs to a machine learning-based classifier.  In some embodiments, the classifier may determine whether the flow is associated\nwith malware.  In further embodiments, the classifier may determine an application associated with the traffic flow.  Based on the classification, the device may also cause the performance of any number of actions in the network, such as mitigation\nactions when a malware-related flow is identified, configuration changes based on an identified application, etc. Procedure 600 then ends at step 630.\nIt should be noted that while certain steps within procedure 600 may be optional as described above, the steps shown in FIG. 6 are merely examples for illustration, and certain other steps may be included or excluded as desired.  Further, while\na particular order of the steps is shown, this ordering is merely illustrative, and any suitable arrangement of the steps may be utilized without departing from the scope of the embodiments herein.\nThe techniques described herein, therefore, allow for the use of compressed telemetry data by a machine learning-based traffic classifier.  In some aspects, the techniques allow for the storage, transmission, and/or use of compressed information\nelements/features, while preserving the ability to use those features in flow classification and forensics systems.  The compression can be essentially as good as LZ78 compression, while it preserves the ability to operate on independent elements, and\nintroduces the ability for a classifier to work on compressed data inputs.  In addition, by compressing each feature/element individually and independently, the techniques herein can achieve better compression than by applying the compression across the\nentire stream/set of features as a whole, since each feature can benefit for a compression technique tailored to it and the compression scheme can work with stored features/information elements.\nWhile there have been shown and described illustrative embodiments that provide for classifying network traffic using compressed telemetry data, it is to be understood that various other adaptations and modifications may be made within the\nspirit and scope of the embodiments herein.  For example, while certain embodiments are described herein with respect to using certain models for purposes of traffic classification, the models are not limited as such and may be used for other functions,\nin other embodiments.  In addition, while certain protocols are shown, other suitable protocols may be used, accordingly.\nThe foregoing description has been directed to specific embodiments.  It will be apparent, however, that other variations and modifications may be made to the described embodiments, with the attainment of some or all of their advantages.  For\ninstance, it is expressly contemplated that the components and/or elements described herein can be implemented as software being stored on a tangible (non-transitory) computer-readable medium (e.g., disks/CDs/RAM/EEPROM/etc.) having program instructions\nexecuting on a computer, hardware, firmware, or a combination thereof.  Accordingly this description is to be taken only by way of example and not to otherwise limit the scope of the embodiments herein.  Therefore, it is the object of the appended claims\nto cover all such variations and modifications as come within the true spirit and scope of the embodiments herein.", "application_number": "15469716", "abstract": " In one embodiment, a device in a network receives telemetry data\n     regarding a traffic flow in the network. One or more features in the\n     telemetry data are individually compressed. The device extracts the one\n     or more individually compressed features from the received telemetry\n     data. The device performs a lookup of one or more classifier inputs from\n     an index of classifier inputs using the one or more individually\n     compressed features from the received telemetry data. The device\n     classifies the traffic flow by inputting the one or more classifier\n     inputs to a machine learning-based classifier.\n", "citations": ["7788371", "7911975", "9065767", "9432269", "20130033994", "20160352761", "20170359362"], "related": []}, {"id": "20180288355", "patent_code": "10375374", "patent_name": "Dimension extractable object comprising spatial metadata for a captured\n     image or video", "year": "2019", "inventor_and_country_data": " Inventors: \nMinami; Eric (Palo Alto, CA), Chu; Charles (Cupertino, CA)  ", "description": "<BR><BR>TECHNICAL FIELD\nThis invention relates to capturing a still or moving image as well as spatial metadata for the image, storing the image and the spatial metadata as a dimension extractable object, and utilizing the dimension extractable object.\n<BR><BR>BACKGROUND OF THE INVENTION\nCameras are well-known in the prior art.  FIG. 1 depicts a simplified diagram of prior art camera 110 comprising photodiode 111.  Photodiode 111 receives light through an aperture in camera 110 and converts light into electric current or\nvoltage.  As is well-known, a still image or moving image (video) can be captured of an object of interest, such as physical object 120 in this example.\nAlso in the prior art, certain digital image capture formats allow for the insertion of metadata, that is, data that is related to the image but not necessary for reconstruction of the image.  Example of such image formats are the JPEG format\nfor still images and the MPEG-4 (H.261) format for moving images (video).  These formats allow for the embedding of arbitrary metadata that describes information related to the capture of the image.  Examples of metadata that is frequently embedded in\ndigital images include date of capture, GPS coordinates of capture location, camera settings (shutter speed, aperture), copyright data, image size, and software used to process the image.\nThere are numerous metadata standards that define the data model for the metadata.  One example is Extensible Metadata Platform (XMP).  XMP is a standard format that does not restrict the attribute field of the metadata and has a flexible\ndefinition of the type of data value.  Other standard formats, such as Dublin Core (DCI), Information Interchange Model (IIM), and Exchangeable Image File Format (EXIF), have specific elements (attributes) and value types.  For example, XMP standard\ndefines special namespaces for DCI elements.  In general, image formats do not require any metadata.  Furthermore, image formats do not require existing metadata to follow a standard.\nSome encoding formats for digital still image are JPEG, GIF, PNG, JPEG2000, BMP, FITS, TIFF, RAW, and FITS.  All these formats allow insertion of metadata into the file.  Common video encoding formats typically are all container formats because\nthe formats need to support multiple types of data streams--e.g. video, audio, and subtitles.  The formats typically support metadata for the container itself; a few support a metadata stream that can be synchronized with the video image.  Examples of\nvideo container formats include MP4, AVI, MPEG, MKV, Ogg, MXF, DPX, and Quicktime, of which only MKV (Matroska) and Ogg are open source.  The most common video stream encoding format is MPEG-2 (H.261), which is streamed video supported in most container\nformats.\nThe prior art also includes numerous three-dimensional (3D) modeling formats, some proprietary and tied to a specific software tool such as the products sold with trademarks AutoCad and Lightwave, while others are more general.  Simple 3D\nmodeling formats like STL and OBJ do not have definitions for metadata, but most proprietary and newer formats supports embedded metadata.  Examples of 3D modeling formats include AMF, STL, OBJ, Blender, DWG (used by the product with trademark Autocad),\nX3D, SKP (used by the product with trademark Google Sketchup), and LWO (used by the product with trademark Lightwave).\nAlso known in the prior art are laser distance measuring devices for measuring the distance between the device and an object.  FIG. 2 depicts prior art distance measuring device 210, which comprises laser diode 211, photodiode 212, lens 213, and\nlens 214.  In one prior art technique, laser diode 211 emits modulated laser light.  The light is focused through lens 213, hits physical object 120, and the light reflects off of physical object 120.  A portion of the light will return to distance\nmeasuring device 210 through lens 214 and hit photodiode 212.  Distance measuring device 210 can capture the distance between photodiode 212 and each portion of physical object 120 using numerous different techniques.  In one technique, distance\nmeasuring device 210 measures the time that elapses between the emission of the laser light from laser diode 211 and the moment when reflected light is received by photodiode 212, and it then calculates distance from that time measurement.  An example of\na novel laser distance measuring device and calibration technique is described in U.S.  patent application Ser.  No. 15/458,969, filed on Mar.  14, 2017, and titled \"Using Integrated Silicon LED to Calibrate Phase Offset in Optical Receiver in Laser\nRange Finder,\" which is incorporated by reference herein.\nTo date, the prior art has not integrated a laser distance measuring device with a camera to capture spatial information for an object with sufficient accuracy to enable the types of applications described herein.  The prior art also lacks a\ndata structure for sending and receiving spatial metadata related to an image.  The prior art also lacks the ability to capture, transmit, and modify spatial metadata and transactional metadata for a product that is captured in an image, which limits the\ndetail that can be exchanged as part of an e-commerce transaction.\nWhat is needed is the ability to capture spatial metadata with the captured image, to store spatial metadata with the image, and to later utilize the spatial metadata.  What is further needed are applications that utilize such spatial metadata\nand transactional metadata that can be associated with the image.\n<BR><BR>SUMMARY OF THE INVENTION\nThe invention enables capturing an image as well as spatial metadata for the image, storing the image and the spatial metadata as a dimension extractable object, and utilizing the dimension extractable object.  As used herein, \"dimension\nextractable object\" refers to an object that comprises 2D or 3D still or video image data and spatial metadata, such as some or all of the metadata described in Table 1, below.  The dimensional extractable object optionally comprises transactional\nmetadata, such as some or all of the metadata described in Table 2, below. <BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nFIG. 1 depicts a prior art camera.\nFIG. 2 depicts a prior art distance measuring device.\nFIG. 3 depicts the creation of a dimension extractable object by an image and distance capture device.\nFIG. 4 depicts an alternative configuration for the image and distance capture device.\nFIG. 5 depicts characteristics of the dimension extractable object.\nFIG. 6 depicts an example of distance data within a dimension extractable object.\nFIG. 7 depicts an embodiment of the dimension extractable object within a JPEG file.\nFIG. 8 depicts an embodiment of the dimension extractable object within a video file.\nFIG. 9 depicts an embodiment of the dimension extractable object within a tar or zip archive file.\nFIG. 10 depicts an embodiment of the dimension extractable object within a container.\nFIG. 11 depicts an embodiment of the dimension extractable object used in conjunction with a lens model transform.\nFIG. 12 depicts an embodiment of a method of generating a stitched-image dimension extractable object from a plurality of image dimension extractable objects.\nFIG. 13 depicts an embodiment of a method of generating a stitched-image dimension extractable object from a video dimension extractable object.\nFIG. 14 depicts an embodiment of a method of generating a 3D model dimension extractable object from a video dimension extractable object.\nFIG. 15 depicts another embodiment of a method of generating a 3D model dimension extractable object from a video dimension extractable object.\nFIG. 16 depicts an embodiment of a method of calculating the velocity of a moving physical object using a video dimension extractable object captured using a stationary image and distance capture device.\nFIG. 17 depicts an embodiment of a method of calculating the velocity of a moving physical object using a video dimension extractable object captured using a moving or rotating image and distance capture device.\nFIG. 18 depicts an embodiment of a system and method for performing e-commerce using dimension extractable objects.\nFIG. 19 depicts an example of a dimension extractable object and a modified dimension extractable object.\nFIG. 20 depicts an example of a transaction initiated by a buyer application.\nFIG. 21 depicts an example of a transaction initiated by a seller application.\nFIG. 22 depicts an example of a transaction involving a manager application.\nFIG. 23 depicts another example of a transaction involving a manager application.\nFIG. 24 depicts an example of a transaction involving a broker application.\nFIG. 25 depicts a server providing results to a computing device based on criteria from the computing device.\nFIG. 26 depicts a server providing results to a computing device based on user data.\n<BR><BR>DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS\nI. Image Format\nReferring to FIG. 3, system 300 comprises image and distance capture device 310 and computing device 330.  In this embodiment, image and distance capture device 310 comprises camera 110, distance measuring device 210, and processing unit 350. \nImage and distance capture device 310 captures an image of physical object 140 and spatial metadata for physical object 140, and processing unit 350 generates dimension extractable object 320.\nImage and distance capture device 310 can transmit dimension extractable object 320 over a link or network to computing device 330, which can store, process, modify, transmit, or otherwise utilize dimension extractable object 320.  For example,\ncomputing device 330 can provide e-commerce services that utilize the information stored in dimension extractable object.\nComputing device 330 can comprise a server, laptop, desktop, mobile device, cloud system, or other known device.  Computing device 330 comprises one or more processing units and one or more storage units and is capable of executing software\ncode.\nFIG. 4 depicts an alternative configuration.  Here, processing unit 350 is external to image and distance capture device 310.  In one embodiment, image and distance capture device 310 is a peripheral that plugs into processing unit 350, which\nmight be a smartphone or other computing device.  In another embodiment, image and distance capture device 310 and processing unit 350 communicate over a network or link, and processing unit 350 might be a server or cloud system.\nIt is to be understood that camera 110 and distance measuring device 210 can be part of a single physical structure, or they can be part of separate physical structures.\nFIG. 5 depicts additional aspects of dimension extractable object 320.  Dimension extractable object 320 comprises image data 510, metadata 520, spatial metadata 530, and optionally, transactional metadata 540.  Image data 510 comprises image\ndata known in the prior art, such as the image data stored in JPEG, MP4, and 3D model files.  Metadata 520 comprises metadata known in the prior art, such as date and time of capture, GPS location, etc. Spatial metadata 530 and transactional metadata 540\ncomprise metadata as described with reference to embodiments contained below.\nIn the preferred embodiments, spatial metadata 530 comprises data encoding the absolute distance of physical object 140 from a camera lens of image and distance capture device 310, as well as other metadata that enables a wide variety of\nphotogrammetric and e-commerce applications.  Photogrammetry is a science of making measurements from photographs and can apply to both still and moving (video) digital images.\nTable 1 describes an exemplary embodiment of spatial metadata 530 within dimension extractable object 320 that can enable the applications described herein.  The term \"spatial metadata\" encompasses any or all of the data described in Table 1, as\nwell as any other data that represents or can be used to determine distance, dimensions, shapes, or contours of the captured physical object or that can used to increase the accuracy or precision of such determinations.\nTABLE-US-00001 TABLE 1 EXEMPLARY SPATIAL METADATA 530 OF DIMENSION EXTRACTABLE OBJECT 320 Element Description Identifier/ A unique alphanumeric identification string Classification Part of the identification is used for classification Extensible\n- (a) possible to have multiple hierarchical categories for classification; (b) possible to have reference (link) to related dimension extractable object Distance May be in any standard or arbitrary measurement unit Millimeter + phase shift + calibration\ndata Frequency + number of periods There is no requirement to interpret the data as a standard distance.  The data can be specified in terms of phase shift, frequency, and calibration if so desired.  One or more points in the image, must be paired with\nthe image position data.  Image position In any coordinate system or at pre-defined points in the image.  Coordinate system: cartesian, polar, complex plane.  Pre-defined: (center, mid-left, mid-right) where each has a specific definition relative to the\ncenter/edges of the image One or more points in the image, must be paired with the distance data.  May include image position data as before and/or after lens model correction.  Lens model Lens model includes distortion and zoom.  The lens model can be\nequation based, e.g. empirical polynomial fitting, or physics based model.  The lens model can be look-up table based.  Image sequence Reference images for stitched dimension extractable object model- May be a pointer to multiple images that interact\nwith the current image.  May include information of the entire sequence of images necessary to reproduce the whole view.  May include any data relevant to reproducing the stitching algorithm used in the global image model Global map For stitched\ndimension extractable object model- May include thumbnail of the whole view (global map).  The global map may include metadata on the all points that exist in the 2D image/3D object model The (point, distance) data for the global image model may be\nincluded in each image.  Camera angular Raw data from gyroscope velocity May include pointer to reference image in a sequence of images used to create the global image model (see Global image sequence).  Camera linear Raw data from accelerometer velocity\nMay include pointer to reference image in a sequence of images used to create the global image model (see Global image sequence).  Camera tilt 3-axis tilt angle calculated from accelerometer/gyro data.  position May include pointer to reference image in\na sequence of images used to create the global image model (see Global image sequence).  Camera velocity Velocity of camera calculated from accelerometer/GPS data May include pointer to reference image in a sequence of images used to create the global\nimage model (see Global image sequence).  Embedded EXIF Already included in many digital images.  data EXIF format typically includes all relevant camera setting data Auxiliary sensor Include any other sensor data that may be important in downstream data\napplication.  Temperature, humidity, anemometer readings GPS lock information Cell phone tower location Accuracy/ Include data related to the accuracy of distance measurement.  Precision Number of samples and statistics (sigma, variance) Sampling time\nA simple example of spatial metadata 530 is shown in FIG. 6.  Processing unit 350 generates dimension extractable object 320, which comprises image 510 of physical object 140 captured by image and distance capture device 310.  Image and distance\ncapture device 310 and processing unit 350 also generate metadata 520, such as date and file size information, and spatial metadata 530, which here comprises image position data 531, and distance data 532.  Image position data 531 is the location of\ndistance measurement within image 510, here expressed as row and column numbers.  Distance data 532 is the distance, at the time image 510 was captured, between a specific portion of distance capture device 310, such as lens 213, and the portion of\nphysical object 140 corresponding to that particular pixel in image 510.\nDimension extractable object 320 optionally can utilize known file and data structure formats as long as such file and data structure formats can accommodate spatial metadata 530.\nFor example, FIG. 7 depicts the structure of an exemplary image file 700.  Image file 700 in this example is based on the prior art JPEG file format.  Image file 700 comprises numerous fields, one of which is field 710, which in this example is\nthe APPn field within the JPEG file format.  Field 710 comprises unspecified space that can be used by the implementer.  In this embodiment, field 710 is used to store spatial metadata 530, such as some or all of the metadata set forth in Table 1.  In\nthis manner, image file 700 is an embodiment of dimension extractable object 320.\nFIG. 8 depicts an embodiment for video data.  Here, image and distance capture device 310 captures video data 810.  Image and distance capture device 310 generates video dimension extractable object 320.  Dimension extractable object 320\ncomprises a video stream (image data 510) and an audio stream and subtitles (metadata 520).  In this embodiment, spatial metadata 530 is split into two portions, with each portion stored separately.  The portion of dimension metadata 530 that applies to\nthe entire video stream is stored in a metadata section within dimension extractable object 320.\nThe portion of dimension metadata 530 that is specific to a frame within the video stream is stored in a section of dimension extractable object that allows such metadata to be associated with the particular frame.  For video container formats\nthat support metadata streams or extensible streams, this portion of spatial metadata 530 is stored as encoded per-frame metadata in one or more metadata streams.  For video container formats that do not support a metadata stream but support other types\nof overlay data streams, this portion of spatial metadata 530 is stored as encoded per-frame metadata in a supported overlay data stream.\nFIG. 9 depicts another embodiment of dimension extractable object 320.  Here, dimension extractable object 320 comprises tar or zip file 910.  An image is captured by image and distance capture device 310 of physical object 140 and is stored as\nimage data 510 in tar or zip file 910.  Spatial metadata 530 also is captured and stored in sidecar files 920.  Sidecar files 920 also comprise pointers to image data 510 and vice versa.\nFIG. 10 depicts another embodiment.  Here, container 1000 is generated.  Container 1000 comprises a plurality of dimension extractable objects 320.  Each dimension extractable object 320 optionally is associated with other metadata 1010 that is\nstored in container 1000.  Each dimension extractable object 320 comprises image data 510 (still, video, or 3D) and may or may not contain spatial metadata 530.  In this embodiment, spatial metadata 530 can be stored outside of the dimension extractable\nobject 320 itself and instead can be stored in one or more metadata sections 1010 within container 1000.\nContainer 1000 is an optimal format for storage of multiple images that are related to one another.  Examples include: Multiple images from a stitched-together scene; A 3D model with the source images that were used to create the 3D model; A\nvideo image with some key frames extracted as still images; A 3D image of a main object and individual 3D images of components required to assemble the main object; and A main image and post-processed or marked-up derivative images, for example, with key\nfeatures extracted.\nIn this embodiment, container 1000 may include other containers 1000 of dimension extractable objects 320.  That is, the structure of this embodiment can be nested to include multiple hierarchical levels.\nEach container 1000 comprises a unique identification sequence 1020.  A dimension extractable object 320 can reference any number of additional dimension extractable objects 320 using the unique identification sequences 1020.  Unique\nidentification sequence 1020 comprises a segment that encodes a classification for container 1000.  The classification code may have multiple hierarchical levels.\nFIG. 11 depicts an application that integrates a lens model transform function with a dimension extractable object 320.  Here, image and distance capture device 310 captures image 1110 and spatial metadata 530 and generates dimension extractable\nobject 320.  Lens model transform 1120 (which optionally is an algorithm performed by a processing unit in image and distance capture device 310 or in computing device 330) performs a transformation on image 1110 to generate corrected image 1130, which\nis combined with spatial metadata 530 to generate a new dimension extractable object 320'.\nUsing this embodiment, a downstream application in computing device 330 can use lens model transform 1120 and spatial metadata 530, including distance information to an image point, to calculate the size of the physical object that was captured\nin the image.  Features of interest in the image can be extracted by using image recognition and object detection algorithms.\nBy storing the lens model, the downstream application in computing device 330 can correct for a large physical object that is captured using a wide-angle lens (as was the case with image 1110).  Using a wide-angle lens on image and distance\ncapture device 310 allows distance measurements to be made at closer ranges, which will increase the accuracy and sampling rate of image and distance capture device 310.\nFIG. 12 depicts a method that provides additional functionality based on the embodiment of FIG. 11.  Method 1200 comprises multiple steps that together allows a sequence of still images to be stitched together.\nIn step 1210, image and distance capture device 310 captures a sequence of images at close range and generates a sequence of dimension extractable objects 320.\nIn step 1220, the lens model transform 1120 of FIG. 11 is performed on each image in the sequence of images.\nIn step 1230, image and distance capture device 310 or computing device 330 stitches together the sequence of transformed images.  Even without any additional sensor data, the additional distance measurements contained in spatial metadata 530\nwill allow accurate stitching of the transformed images.  Additional data collected from an accelerometer and gyroscope in image and distance capture device 310 can help make corrections to the position of the camera and can further improve the accuracy\nof the stitching process.  Additional sensor data to calculate the camera tilt angle and position (accelerometer and gyro) assist in the creation of a 3D dimension extractable object 320 by (a) correcting for potential errors from stitching algorithm,\nand (b) reducing the number of images required to create the full 3D images.  The multiplicity of distance measurements in overlapping images can be used generate accurate surface profile of the target object.\nIn step 1240, a new dimension extractable object 320 is generated to embody the stitched-together image and spatial metadata 530.\nFIG. 13 depicts a method that utilizes an improved stitching capability for images extracted from video using spatial metadata 530.\nIn step 1310, video image dimension extractable object 320 is generated.  For example, image and distance capture device 310 can record video as the user walks around the physical extent of physical object 1350.\nIn step 1320, a sequence of still images is extracted from the captured video stream.\nIn step 1330, the still images are stitched together using the same technique discussed previously with reference to FIG. 12.\nIn step 1340, a new dimension extractable object 320 is generated to embody the stitched-together image and spatial metadata 530.  Thus, using the spatial metadata 530, a dimension extractable object comprising a single, stitched-together still\nimage can be generated for a large physical object 1350 using a captured video stream of the object that enables extraction of object surface profile.  The accuracy of this process can be improved through slower movement of image and distance capture\ndevice 310.\nFIG. 14 depicts a method for generating a 3D model of physical object 1440.  In this method, a user hold image and distance capture device 310 and captures video of physical object 1440.  For example, the user might walk around physical object\n1440.  If physical object 1440 is a room, then the user might stand in the center of the room and record video while facing the perimeter of the room and rotating.\nIn step 1410, image and distance capture device 310 captures video image dimension extractable object 1410.\nIn step 1420, a sequence of image frames is extracted from the video.\nIn step 1430, a 3D model dimension extractable object is generated from the sequence of image frames.  Thus, a single video recording, stored as a video dimension extractable object 320, contains all data necessary to construct a 3D model.\nFIG. 15 depicts a method 1500 for generating a 3D model of physical object 1540.  The first three steps are the same as in method 1400 described previously with reference to FIG. 14.  In step 1510, image and distance capture device 310 captures\nvideo image dimension extractable object 320.  In step 1520, a sequence of image frames is extracted from the video.  In step 1530, a 3D model dimension extractable object is generated from the sequence of image frames.  Thus, a single video recording,\nstored as a video dimension extractable object 320, contains all data necessary to construct a 3D model.\nIn step 1540, for objects with complex surface details, additional dimension extractable objects can be used to progressively include more surface details in the 3D model.\nWith reference to FIGS. 16 and 17, a video dimension extractable object 320 can be used to calculate the velocity of a moving physical object 1610.\nIn FIG. 16, image capturing device 110 is stationary, and moving physical object 1610 crosses the field of view and rangefinder range.  Multiple distance measurement points and time metadata, which is stored as spatial metadata 530 in dimension\nextractable object 320, can be used to calculate the velocity of moving physical object 1610.\nIn FIG. 17, image capturing device 110 is moving or rotating.  Multiple distance measurement points, time metadata, and accelerometer and gyroscope metadata, which is stored as spatial metadata 530 in dimension extractable object 320, can be\nused to calculate the velocity of moving physical object 1610.\nIn each of the embodiments described above, spatial metadata 530 in dimension extractable object 320 will allow a user operating computing device 130 to be provided with accurate dimension information for the physical object that is the subject\nof the captured images.  For example, if the physical object is a room, spatial metadata 530 will allow the user to be provided with the exact dimensions of the room, including all size measurements and angles.  If the physical object is a car, spatial\nmetadata 530 will allow the user to be provided with the exact dimensions of each visible surface and aspect of the car.  If the physical object is a landscape, spatial metadata 530 will allow the user to be provided with the exact distances between\nobjects in the landscape (e.g., distance between two trees in a landscape plan).\nII.  E-Commerce Applications of Dimension Extractable Objects\nThere are numerous benefits in using dimension extractable objects for e-commerce.  Optionally, metadata that is particularly useful for e-commerce can be added to a dimension extractable object.  With reference again to FIG. 5, dimension\nextractable object 320 optionally comprises transactional metadata 540.  Table 2 describes an exemplary embodiment of transactional metadata 540 within dimension extractable object 320 that can enable the applications described herein.  The term\n\"transactional metadata\" encompasses any or all of the data described in Table 2, as well as any other data that is useful to a commercial transaction involving the product that is the subject of dimension extractable object 320.\nTABLE-US-00002 TABLE 2 EXEMPLARY TRANSACTIONAL METADATA 540 OF DIMENSION EXTRACTABLE OBJECT 320 Element Description Project related Min, max total cost Min, max cost of material Min number required.  Type of material Min, max cost for shipping\nMaterial - color, type (e.g. bamboo, maple, Brazilian teak, 300 thread cotton) Accuracy specifications Validity date Bid related Limit geographical location for bids Job due/required by date Payment methods Contractual Return/restocking requirements\nBonus/penalty provisions for early/late completion Intellectual property registrations - trademark, copyright, patent information Supplier Ratings Relevant rating system, reviews Recommendations, references Buyer Ratings Relevant rating system, reviews\nShipping related Fragile/non-Fragile Ship method, conditions Others freight constraints Transaction related Maximum number of bids accepted.  Optimization criteria - examples include cost, quality, precision, yield, reputation.  Informational links Links\nto public/private webs page Links to shared file on a serve\nEach metadata field within transactional metadata 540 (such as the metadata listed in Table 2, above) may encode additional data that categorizes the level confidentiality of the data.  For example, in a completely open transaction, all fields\nare public to the parties involved in the transaction.  However, adding more specific confidentiality levels can improve the flow of the transaction process being handled by software applications.  For example, a buyer may make the minimum cost public,\nbut may wish to hide the maximum cost.  In general, completely open delineation of the requirements is often not desired by one or both of the parties.\nFIG. 18 depicts an embodiment of e-commerce system and method 1800.  An end user, which can be the buyer or the supplier, operates an end user application 1810 (buyer) or 1830 (supplier) in conjunction with image and distance capture device 310\nto capture image data and dimension data from physical object 140.  The end user application 1810/1830 generates dimension extractable object 320, which includes the image data and spatial metadata 530 as well as transactional metadata 540 such as some\nor all of the metadata described in Table 2, above.  The end user applications 1810/1830 transmits and receives dimension extractable object 320 to and from transaction application 1820.  The end user application 1810/1830 and transaction application\n1820 may modify dimension extractable object 320 by modifying spatial metadata 530 and/or transactional metadata 540 field or adding/removing dimension extractable object(s) 320 within the original dimension extractable object container.  The modified\ndimension extractable object 320' is treated as any other dimension extractable object.\nAn example of dimension extractable object 320 is depicted in FIG. 19.  Dimension extractable object 320 comprises image data 1901, spatial metadata 1902, and transactional metadata 1903.  In this example, the buyer wishes to purchase the\nproduct shown in image data 1901.  The product is further defined by spatial metadata 1902 (input by the buyer), which in this example includes an identifier of \"7H6% R3\"; distance data paired with image position (expressed in matrix of binary numbers),\nsuch as a distance measurement for each pixel in the image, where the distance is the measurement from image and distance capture device 310 to that portion of physical object 140; and a lens model, and transactional metadata 1903 (input by the buyer),\nwhich in this example includes a maximum cost per unit of $3.50; the minimum number of the product required of 500; a material of aluminum, and shipping of 2-day by courier.\nAn example of dimension extractable object 320' is shown in FIG. 19.  Dimension extractable object 320' comprises image data 1901 (which has not changed compared to dimension extractable object 320), spatial metadata 1902', and transactional\nmetadata 1903'.  In this example, spatial metadata 1902' is the same as spatial metadata 1902.  In other instances, a buyer or seller might change spatial metadata 1902 to create spatial metadata 1902', for example, if a seller wanted to offer a buyer a\nproduct that was similar to but not identical the product represented by spatial metadata 1902.  Transactional metadata 1903' is identical to transactional metadata 1903 except that the supplier has changed the maximum cost per unit to $3.75 and has\nchanged the material to iron.\nTransaction application 1820 can establish communication between buyer and supplier, or buyer application 1810 and supplier application 1830, through the internet and can complete the contractual agreement for the job if both parties agree to\nthe terms indicated in dimension extractable object 320.  One of ordinary skill in the art will appreciate that numerous rounds of dimension extractable objects can be exchanged between the buyer and the seller, similar to the manner in which drafts of\ncontracts can be exchanged in a negotiation.\nTransaction application 1820 optionally operates an event-driven engine that responds to transactional events using machine-learning or other computational algorithms.  For example, a dimension extractable object registration or retrieval by a\nbuyer can trigger an advertising widget for a related product.\nOne of ordinary skill in the art will understand that any number of individuals might make modifications to the same dimension extractable object, or that multiple modified versions of the same dimension extractable object may be created.  The\nexamples of the Figures included herein are merely illustrative.\nFIG. 20 depicts an embodiment of e-commerce system and method 2000 implementing an e-commerce transaction initiated by the buyer.  The buyer application 1810 registers a dimension extractable object 320 through transaction application 1820 and\nrequests a search for possible supplier matches through search engine 2030, which may be a part of transaction application 1820.  In other cases, buyer application 1810 may request direct access to the pool of dimension extractable objects 1840 through\nan application interface 2090 to implement a customized search algorithm.  The search responses 2020 are sent back to the buyer, who selects suppliers and contacts the potential supplier through the transaction application 1820.  Once the supplier\napplication 1830 receives the request for attention from the buyer application 1310, a communication path 2080 is established through transaction application 1820 for further negotiation on the terms, as necessary.  Applications 1810, 1820, and 1830 may\nalso conduct the initial phases of the negotiation algorithmically through an artificial intelligence (AI)-directed negotiation engine 2070 based on private/public e-commerce metadata.\nFIG. 21 depicts an embodiment of e-commerce system and method 1800 implementing an e-commerce transaction initiated by the supplier.  The supplier application 1830 registers a dimension extractable object 320 through the transaction application\n1820 and requests a search for possible buyer matches through the search engine 2030, which may be a part of transaction application 1820.  In other cases, supplier application 1830 may request direct access to the pool of dimension extractable objects\n1840 through an application interface 2190 to implement a customized search algorithm.  The search responses 2120 are sent back to the supplier, who selects and markets the product through the transaction application 1820.  Once the buyer application\n1810 receives the request for attention from the seller application 1830, a communication path 2180 is established through the transaction application for further negotiation on the terms, as necessary.  Applications 1810, 1820, and 1830 may also conduct\nthe initial phases of the negotiation algorithmically through a AI-directed negotiation engine 2070 based on private/public e-commerce metadata.\nFIG. 22 shows an embodiment of e-commerce system and method 2200 where buyer application 1810 and supplier application 1830 communicate directly through application interfaces 2190 and 2090, respectively, to manager application 2210, which\nmanages pool of dimension extractable objects 1840.  Manager application 2210 provides services commonly performed by storage servers, such as implementing access controls and monitoring access.  The application interface functions performed over\napplication interfaces 2190 and 2090 may include some functions ascribed to transaction application 1820 in previous examples, such as adding, removing, and returning qualified applications as directed by supplier and buyer applications.  The\ncommunications between the end-users (buyer and seller) are conducted independently of manager application 2210.\nFIG. 23 shows an embodiment of e-commerce system and method 2300, which is a variation of the embodiment 2200 of FIG. 22.  As in FIG. 22, buyer application 1810 and the supplier application 1830 communicate directly through an application\ninterface to manager application 2210, which manages pool of dimension extractable objects 1840.  The communications between the end-users (buyer and seller) are conducted independently of manager application 2210.  Manager application 2210 has the\nability to provide access for buyer application 1810 and supplier application 1830 to a plurality of manager applications (such as exemplary manager applications 2310, 2311, and 2312), each of which manages a pool of dimension extractable objects (such\nas exemplary pool of dimension extractable objects 1841, 1842, and 1843, respectively) Thus, pools of dimension extractable objects may be distributed across many servers.  Any number of additional manager applications and pools of dimension extractable\nobjects may exist.  Any given pool of dimension extractable objects may have private or public access.  The manager application controlling each pool will have a standard interface for communicating with the other manager applications and pools.  Thus,\ninstead of buyer application 1810 and supplier application 1830 accessing pool of dimension extractable objects 1841 or manager application 2310 directly, access is provided through manager application 2210.  Manager application 2210 will then provide\naccess to one or more pools of dimension extractable objects depending on its access rights.\nThus, in one implementation of e-commerce system and method 2300, the operator of manager application 2210 may negotiate or pay for access to various other manager applications and/or pools and market and sell such access to supplier application\n1830 and/or buyer application 1810.  For instance, a large retail provider might have access to a greater number of manager applications and pools than a smaller retail provider.\nIn another implementation of e-commerce system and method 2300, the cost of maintaining, collecting, and marketing a pool of dimension extractable objects can be funded by membership fees levied on buyers, suppliers, brokers, or others, or on a\n\"per transaction\" fee, or using any other pricing model.  Under this model, the fact that manager application 2210 acts as a gateway to manager applications 2310, 2311, and 2312 will make is easier for such a fee system to be imposed on buyers,\nsuppliers, or other users who wish to access pools 1841, 1842, and 1843.\nFIG. 24 shows an embodiment of e-commerce system and method 2400.  A broker operates broker application 2410 that communicates with buyer application 1810 through communication path 2420 and with supplier application 1830 through communication\npath 2430.  To access pool of dimension extractable objects 1840, broker application 2410 communicates with transaction application 1820 through communication path 2430 or with manager application 1810 through application interface 2440.\nOne benefit of system and method 2400 is that broker application 2410 can provide an additional layer of security for transaction application 1820 and pool of dimension extractable objects 1840.  This architecture also might be useful if pool of\ndimension extractable objects 1840 are proprietary in nature and if its owner does not wish them to be publicly accessible\nSystem and method 2400 also may be particularly useful in a situation where the operator of broker application has specialized expertise that itself provides value to potential transactions, as might be the case if the operator of broker\napplication 2410 is an interior designer, architect, systems designer, assembly line designer, or other professional with specialized knowledge and experience in design or manufacturing.  For example, broker application 2410 might have access rights to\nexemplary manager application 2210 and pool 1840 through application interface 2450, where manager application 2210 specializes in managing objects within a particular niche area (e.g., customized home furniture).  Broker application 2410 can have access\nrights to any number of other manager applications and pools.\nThis architecture also would allow a broker to modify dimension extractable objects to suit the needs of the buyer, seller, or other user.  For example, if a buyer uses buyer application 1810 to create dimension extractable object 1840, broker\ncan review the contents of dimension extractable object 1840, and modify it into dimension extractable objet 1841 using the broker's expertise in a given field.  Broker application 2410 can then find relevant objects managed by manager application 1810\nand can then provide them or provide a modified version of them to buyer application 1810.\nAn example of such a scenario would be if the broker is an interior designer.  The buyer can create a dimension extractable object 320 using buyer application, and might specify a space within his home that he wishes to decorate.  The broker can\nthen review an image of the space and the associated dimensions and can then choose dimension extractable objects from manager application 2210 (and perhaps other manager applications), which in this example might be suppliers of furniture, artwork, etc.\nThe broker might then create a new container that includes the original dimension extractable object 320 and the dimension extractable objects obtained from manager application 1810.  The broker might create multiple containers, each container including\ndifferent options and dimension extractable objects for the buyer to consider.  In this example, the broker will have access to the pools that are useful for the job either directly (e.g., through access to manager applications and pools) or indirectly\n(e.g. choosing light fixtures through a specialty lighting supplier).  Indirect access might be available only through another broker.  In another variation of this embodiment, the broker may just give the buyer application 1810 access to a subset of\ndimension extractable object pools that he has access to or that he has filtered based upon the buyer's taste, and the broker then can ask the buyer to choose dimension extractable objects for the design job or to place objects into the original\ndimension extractable object.\nIf broker does not find sufficient dimension extractable objects from manager application 1810 or other manager applications, the broker might communicate with the supplier pools managed by supplier application 1830 and other supplier\napplications and request customization by exchanging dimension extractable objects with the suppliers, as discussed previously.\nIn these situations, the broker optionally may ask for a service fee from the buyer, or the broker might ask for a commission from the supplier pool.\nIn a variation of this embodiment, a human broker need not be used at all.  Broker application 2410 can perform the tasks described above on its own without human intervention.\nAdditional detail is shown in FIG. 25 regarding performing searches with a pool of dimension extractable objects.  Server 2510 maintains pool of dimension extractable objects 1840.  Here, server 2510 is a high-level representation of transaction\napplication 1820, supplier application 1830, manager applications 2210, 2310, 2311, or 2312, and/or broker application 2410 shown in FIGS. 21-24.  Computing device 2520 is a high-level representation of buyer application 1810, supplier application 1830,\ntransaction application 1820, manager applications 2210, 2310, 2311, or 2312, and/or broker application 2410 shown in FIGS. 21-24 Computing device 2520 communicates with server 2510 through application interface 2550 to implement a customized search\nalgorithm.  Computing device 2520 provides server 2510 with a set of criteria for products that are of interest to the user of computing device 2520, such as type of device, size of device, cost of device, etc. Server 2510 identifies a set of dimension\nextractable objects within pool 1840 that satisfy the set of criteria.  Server 2510 then provides the set of dimension extractable objects to computing device 2520.\nAnother embodiment is shown in FIG. 26.  Server 2610 maintains pool of dimension extractable objects 1840.  Computing device 2620 is operated by User A and communicates with server 2610.  Server 2610 operates recommendation engine 2640 and\nmaintains user data 2630.  Recommendation engine 2640 uses prior art method of recommending products that might be of interest to a particular user based on data collected for that user and other users.  Recommendation engine 2640 optionally uses an AI\nengine.  User Data 2630 optionally comprises purchasing history, credit history, web activity information, demographic information, and other information for User A and other users.  Recommendation engine 2640 identified dimension extractable objects\nwithin pool 1840 that might be of interest to User A and sends those objects to computing device 2620 for User A's consideration.\nIn the embodiments described above, buyer application 1810, transaction application 1820, supplier application 1830, manager applications 2210 and 2310, and broker application 2410 each comprise lines of software code that operate on local\ndevices (such as image and distance capture device 310, computing device 130, or other computing devices) or may reside in cloud-based servers.\nExamples of transactions that can be performed using e-commerce system and method 2000 include the following: A buyer generates a dimension extractable object for a target model (e.g., replacement mechanical parts, custom fittings).  A supplier\nmatches the target model from stock or generates bids to reproduce the target model.  A buyer generates a dimension extractable object for a target model.  A supplier matches the derivative part from stock or generates bids to produce the derivative\npart.  For example, a furniture cover (derivative part) for a sofa model (target model) or a helmet (derivative part) for a human model (target model) A buyer generates a dimension extractable object for a target space, e.g. unusually-shaped alcove\nwithin a home and requests custom furniture to fill the space.  A supplier generates a bid to produce the object for the target space.  Or, a buyer uses the supplier's dimensional extractable object to visualize the product within the target space.  A\nsupplier creates dimension extractable objects for his products and registers them with the transaction application.  The transaction application may be enabled for automatic searches of potential buyers.  The buyer application may search for and bid on\nthe [suppliers'] registered dimension extractable objects.  A buyer generates a dimension extractable object for his body with fashion preferences.  A supplier or broker searches for clothing items to suggest to the buyer.\nThere may be more than two parties involved in a transaction.  The parties may be multiple buyers or multiple suppliers.  For example, a city may create a dimension extractable object container with multiple dimension extractable objects with\ndetails of sections of a community center and take bids for completing different sections of the project--e.g. landscaping, furniture, art, lighting.  This will be the case of one buyer, multiple suppliers.  Or, a city may subcontract each section of the\nproject, each with responsibility for completing a section of the community center.  Because each subcontractor may use different set of suppliers, the transaction may involve multiple buyer (subcontractors) and multiple suppliers.\nThe invention will have the following benefits for B2B (business-to-business), C2B (consumer-to-business), C2C (consumer-to-consumer), and other e-commerce transactions: Low-overhead search for matching supplier.  Because a dimension extractable\nobject encapsulates all dimensions necessary to spec-out a part or a job, there is no need to take additional measurements.  Transaction application 1320 sends supplier application 1330 a list of dimension extractable objects based on the supplier's\nprofile.  Machine-learning algorithms may be used to select dimension extractable objects based on the history of the supplier's transactions.  Supplier application 1330 can perform second-level filtering of the dimension extractable objects by matching\nagainst supplier stock or capability.  Expansion of supplier/buyer market.  When a buyer creates a request for a job in an internet-based open market, all necessary information is encapsulated in the dimension extractable object.  Supplier application\n1330 automatically generates the bid, for example, based on supplier capacity and stock, required date, and material cost.  There is no need for on-site estimate or additional discussion with the buyer to take measurements.  The buyer can choose from a\nlarge pool of global suppliers and choose a supplier that best fits his or her needs.  The supplier has a much bigger pool of customers with very low marketing cost.  A remote supplier (e.g., a custom machine-shop in Wyoming) can bid on projects\nnationwide or even worldwide.  An artisan in Africa can market his jewelry worldwide.  Rapid qualification and execution of transaction.  E-commerce metadata quickly qualifies the transactional parameters.  For example, buyer application 1310 or supplier\napplication 1330 can insert metadata related to maximum cost, required date, payment terms/method into a dimension extractable object.  A qualified buyer/supplier can be identified by a private or open rating system (e.g., consumer credit scores,\nrecommendations, qualified reviews, bank collaterals).  Full contractual text can be appended to the dimension extractable object and the transaction completed with digital signatures.  Flexible, open system for project management.  A dimension\nextractable object can be a request for an identical part, or a dimension extractable object job request can be more flexible.  The buyer can specify whether he or she is willing to accept a modified proposal for a job.  If there is a reasonable\nmechanical/aesthetic substitute for the original dimension extractable object request, the supplier can propose the modification through another dimension extractable object or more spec-based information (e.g. bamboo floor instead of hardwood).  The\nflexible model will work especially well when the buyer requires expert feedback for a project.  For example, the initial dimension extractable object can contain 3D images of an apartment for a re-model.  A supplier or a broker (possibly an interior\ndecorator or architect) can add elements into the dimension extractable object and bid for the job.  The supplier dimension extractable object can spawn off new dimension extractable objects to furnish the interior space (custom furniture, light\nfixtures).  Object-based visualization/negotiation Additional image formats/dimension extractable objects can be inserted into dimension extractable object containers.  This allows suppliers to add information on the project.  If the images are dimension\nextractable objects, then buyer can use another application (e.g. 3D viewing software or virtual reality software) to visualize the project.  Visualization can be used before contract and during the project to assist in the interaction between buyer and\nsupplier.  This process can reduce miscommunication/errors during the project.  A single dimension extractable object can contain information that describes many different parts of a complex architecture, landscaping, or manufacturing project with links\nto related dimension extractable objects.  Anonymity/Privacy Personal information is not required for dimension extractable objects but each dimension extractable object must have identifiers so a transaction can later establish communication between\nbuyers and suppliers.  By keeping all e-commerce related metadata with the object model, buyer application 1310, transaction application 1320, and supplier application 1330 can automate and optimize the work required to create bids.  Using digital\nidentifiers and signatures, the initial negotiations can be anonymous.\nReferences to the present invention herein are not intended to limit the scope of any claim or claim term, but instead merely make reference to one or more features that may be covered by one or more of the claims.  Structures, processes and\nnumerical examples described above are exemplary only, and should not be deemed to limit the claims.  It should be noted that, as used herein, the terms \"over\" and \"on\" both inclusively include \"directly on\" (no intermediate materials, elements or space\ndisposed there between) and \"indirectly on\" (intermediate materials, elements or space disposed there between).", "application_number": "15473098", "abstract": " The invention relates to capturing a still or moving image as well as\n     object position and displacement data for the image, storing the image\n     and the data as a dimension extractable object, and utilizing the\n     dimension extractable object.\n", "citations": ["9369727", "20050086582", "20130142431", "20150325038", "20170046594", "20180276841"], "related": []}, {"id": "20180295554", "patent_code": "10375617", "patent_name": "Mobile application testing engine", "year": "2019", "inventor_and_country_data": " Inventors: \nRoutt; Thomas (Edmonds, WA), Maria; Arturo (Bellevue, WA)  ", "description": "<BR><BR>TECHNICAL FIELD\nThe disclosed subject matter relates to a mobile application testing engine, e.g., for analyzing data associated with a mobile application traversing a carrier core-network.\n<BR><BR>BACKGROUND\nBy way of brief background, conventional mobile application testing and mobile application data capture is generally performed at a device executing the mobile application or at a device executing an application service in support of the mobile\napplication, e.g., at, or near, the endpoints of communication between the mobile application and the mobile application service.  Even where mobile application testing and data capture is performed at devices other than a mobile executing a mobile\napplication or at devices other than a device executing an application service, these conventional systems generally are engineered for a specific operating system, a specific mobile application, or a specific application service.  Conventional systems\nare typically not well positioned for implementation in a carrier core-network environment and generally cannot capture data related to execution of a mobile application from a high percentage of mobile applications or their corresponding application\nservices as they traverse a carrier core-network because they are not designed for, and thus cannot be implemented on, a carrier core-network. <BR><BR>BRIEF DESCRIPTION OF DRAWINGS\nFIG. 1 is an illustration of an example system that facilitates mobile application testing and data capture in accordance with aspects of the subject disclosure.\nFIG. 2 is a depiction of an example system that facilitates mobile application testing and data capture at a carrier core-network component in accordance with aspects of the subject disclosure.\nFIG. 3 illustrates an example system that facilitates mobile application testing and data capture to support adaptation of data transport in accordance with aspects of the subject disclosure.\nFIG. 4 illustrates an example system that facilitates mobile application testing and data capture via components at various network locations in accordance with aspects of the subject disclosure.\nFIG. 5 illustrates an example system depicting mobile application testing and simulation supporting adaptation of data transport in accordance with aspects of the subject disclosure.\nFIG. 6 illustrates an example method facilitating mobile application data capture and analysis in accordance with aspects of the subject disclosure.\nFIG. 7 depicts an example method facilitating analysis of historical data related to a mobile application in accordance with aspects of the subject disclosure.\nFIG. 8 illustrates an example method facilitating access to captured mobile application data for devices outside of a carrier core-network in accordance with aspects of the subject disclosure.\nFIG. 9 depicts an example schematic block diagram of a computing environment with which the disclosed subject matter can interact.\nFIG. 10 illustrates an example block diagram of a computing system operable to execute the disclosed systems and methods in accordance with an embodiment.\n<BR><BR>DETAILED DESCRIPTION\nThe subject disclosure is now described with reference to the drawings, wherein like reference numerals are used to refer to like elements throughout.  In the following description, for purposes of explanation, numerous specific details are set\nforth in order to provide a thorough understanding of the subject disclosure.  It may be evident, however, that the subject disclosure may be practiced without these specific details.  In other instances, well-known structures and devices are shown in\nblock diagram form in order to facilitate describing the subject disclosure.\nConventional application testing generally relies on testing services located at endpoints in a testing system, e.g., at a mobile device, at a testing server, an application service device, etc., and employs a network simply for transporting\ndata between those endpoints.  In contrast, the presently disclosed subject matter relates to capturing data related to mobile applications, e.g., mobile application data (MAD) which, as used herein, is inclusive of data sent by a mobile application and\ndata sent to a mobile application unless otherwise explicitly or implicitly stated as being otherwise, at a midpoint for performing testing or simulation on the data as it passes through a carrier core-network.  The capture of this data, or the results\nof the tests and simulations, can be employed for adapting the mobile application, the data transport topology, the application service, etc., to adjust the performance of a mobile application environment, often in real time.  Moreover, while\nconventional techniques can often be siloed and proprietary for each mobile application, mobile device, mobile operating system (OS), application service, etc., the disclosed subject matter allows for the capture of nearly any data related to the mobile\napplication or application service transitioning the carrier core-network and can be agnostic with regard to mobile device type, OS, application service, mobile application, etc.\nIn addressing these and/or other issues, one or more embodiments of the subject application relate to a mobile application testing engine.  For instance, one or more embodiments relate to data associated with a mobile application traversing a\ncarrier core-network, e.g., capturing mobile application data sent to a mobile application service via a carrier core-network device, capturing application service data sent to a mobile application via a carrier core-network device, analysis of captured\ndata, e.g., to determine trends, behaviors, characteristics, etc., to enable adapting a mobile application or parameter related thereto, adapting transport, deriving correlated information, e.g., big data analysis, etc., mobile application testing or\nsimulation of supporting systems, etc., wherein a carrier is uniquely positioned to harvest the data associated with a mobile application traversing the carrier core-network from nearly any mobile application or application service executing in nearly\nany mobile operating system or application service operating system.\nThe instant subject matter enables capture of huge repositories of data related to a mobile application, which can be stored for data mining, testing, analysis, etc., to reveal characteristics, trends, behaviors, etc., related to a mobile\napplication or support thereof.  This can allow for implementation of services, provisioning of equipment, adaptation of topologies, etc., to alter the performance of a mobile application environment.  As an example, monitoring and analyzing a repository\nof mobile application data can enable can reveal that during peak use periods latency increases are degrading the performance of a video chat application for mobile users in urban regions.  In response to this information, the carrier core-network can be\nadapted to route video chat data through other low latency nodes during peak use periods for urban regions.  As another example, analysis of data related to a mobile voice over internet protocol (VoIP) or voice over long term evolution (VoLTE)\napplication can rank radio access networks (RANs) for different geographic areas by performance to enable updates to the RANs to be targeted to the those that would show the greatest improvement to users, e.g., older RAN equipment and software in a rural\narea may be ranked low for upgrading while an urban area with newer RAN equipment/software may be ranked at a middle priority and a suburban area with the similar RAN equipment/software can be ranked as high priority for an upgrade due to a\ndisproportionate dropped call rate, that perhaps could be correlated to a surge in population of the suburban region, geography of the suburban region, etc. A carrier core-network centric mobile application testing engine (MATE) can provide unique\nadvantages over conventional technologies because it can monitor nearly all traffic through the carrier level network in contrast to other technologies that may incidentally use a carrier core-network for transport but are typically restricted to\nmonitoring data only at the endpoints because they generally do not have access to carrier level networks, e.g., carrier core-networks.\nIn an aspect, a carrier core-network, a central part of a telecommunications network for the carrier, e.g., a carrier entity, can provide various services to customers who are connected by an access network.  As an example, a radio access\nnetwork (RAN) can connect to a carrier core-network to support mobile devices connected to the RAN.  A carrier core-network can comprise a component(s) related to establishing a connection with a device via an access network(s), a component(s) for\ntransporting data or signals across the carrier core-network, a component(s) for allowing access to the transported data or signals to another component(s), etc. A carrier core-network can be connected to a component(s) outside the carrier core-network,\nfor example, an internet node, an application service, a server, a data store, etc. Typically, the term `carrier core-network` refers to high capacity communication facilities that connect primary nodes and provides paths for the exchange of information\nbetween different sub-networks.  In enterprise level private networks, e.g., a proprietary corporate network, the term backbone is more commonly employed, while for service providers, e.g., wireless carriers, etc., the term core-network is more common. \nCore-networks can be linked by interexchange networks, e.g., within a nation-sized region, however, a core-network can be extended to encompass an entire region, e.g., extending to national or geographic boundaries.  Most core-networks have a mesh\ntopology that can provide m-to-m connections among devices on the network.  Common devices and facilities of a core-network includes gateways, switches, routers, a home location register, a visitor location register, a mobile switching center, an\nequipment identity register, an authentication center, etc. An example a carrier core-network can comprise IS-41 systems, also known as ANSI-41 systems, an SS7 protocol, such as Mobile Application Part, etc., or other systems and technologies, to support\nmobile/cellular telecommunications and mobility management.\nFurthermore, capture of mobile application data at the carrier level, e.g., as it is transported across a carrier core-network, can allow for data queries and data analysis by a wide array of entities, e.g., application developers, application\nservice providers, government entities, advertising or marketing entities, network equipment/software manufacturers/developers, carriers, etc., subject to privacy policies and systems of a carrier implementing MATE technology.  In some embodiments,\nentities can request an analysis to be performed on a stored MAD set, can request subsets of a stored MAD set, can request real-time feeds of MAD, can request best practices information based on carrier analysis of MAD, etc. As an example, a mobile\napplication developer can be provided, in response to a request, information indicating that, for mobile applications located in Mexico, there is a lower latency associated with data moved through a U.S.  network node than a local Mexican network node,\nwhich can be due to the example U.S.  node having newer equipment/software than the Mexican node, allowing the developer to adapt the mobile application development to prefer U.S.  network nodes to Mexican network nodes.  As another example, in response\nto a query, an application service provider can be provided information that mobile application calls to a specific port are failing at a higher rate that calls to other ports of the application service, which can allow the provider to take corrective\naction with regard to the suspect port.  As a further example, a carrier can publish best practices that can comprise information indicating relative latency, throughput, and availability, for NodeB nodes as compared to Wi-Fi nodes, which can allow the\nmobile industry to incorporate this data into development of products or updates to existing products, by standards setting organizations in rolling out or updating policies, etc. As will be appreciated by those of skill in the art, MAD data sets can be\nexpected to be significantly larger and more comprehensive than testing data from conventional technologies, in part due to the carrier-centric nature of MATE technologies as disclosed herein.  It is further noted that large scale data analysis, e.g.,\ndata mining, can provide a vast array of information, all of which is to be considered within the scope of the instant disclosure even where not explicitly recited for the sake of clarity and brevity.\nTo the accomplishment of the foregoing and related ends, the disclosed subject matter, then, comprises one or more of the features hereinafter more fully described.  The following description and the annexed drawings set forth in detail certain\nillustrative aspects of the subject matter.  However, these aspects are indicative of but a few of the various ways in which the principles of the subject matter can be employed.  Other aspects, advantages and novel features of the disclosed subject\nmatter will become apparent from the following detailed description when considered in conjunction with the provided drawings.\nFIG. 1 is an illustration of a system 100, which facilitates mobile application testing and data capture in accordance with aspects of the subject disclosure.  System 100 can comprise mobile application testing engine (MATE) component 120 that\ncan receive and pass mobile application data (MAD) 102 and application service data 190.  MATE component 120 can comprise data store 130 to enable storage of data related to monitoring MAD 102 and application service data 190 as it transits through a\ncarrier core-network component, not illustrated.  MATE component 120 can duplicate MAD 102, application service data 190, RAN information, core-network information, etc., and can store some, none, or all, of the duplicated data, e.g., at data store 130. \nGiven that MATE technology can enable massive stores of data related to a mobile application, generally all, or nearly all, duplicated data can be stored.  However, in some circumstances, duplication of data, or storage thereof, can be undesirable or\nprohibited, in which situations, less than all of the MAD 102 or application service data 190 can be duplicated and/or stored.  As an example, privacy constraints can be applied to avoid duplication of, or removing, personal identifiers, content, etc. As\nanother example, information subject to governmental interests, international treaties, etc., can be ignored.  As a further example, some cyberattacks, such as a denial of service attack, can flood a network with atypical data that can be, in some\ninstances, ignored by MATE technology.\nMATE component 120 can capture behavior and behavior patterns of mobile applications and store these captured mobile application behaviors or patterns in a knowledge database, e.g., based on data stored at data store 130, etc. A carrier has a\nunique advantage in that it can, for example, inspect headers, etc., as data flows through a carrier's core-network and determine the behavior of all, or in some instances some or none, of the mobile application behaviors before they leave or return\nthrough the carrier network to/from the Internet or application servers, etc. In an embodiment, MATE component 120, e.g., via data store 130, can provide mobile application usage and validated data to big data analytics engines.  In some embodiments,\nMATE component 120 can perform tests or analysis on captured data, test or simulate a condition with a mobile application in execution, or test/simulate an interaction with an applications service in execution, etc., for example, testing geographic\nfunctionality among a plurality of captured geo-tagged location-based services and mapping mobile application data, providing functional testing of captured/stored mobile application data, utilizing test data to validate mobile application functionality,\nutilizing captured/tested data to validate new releases of mobile applications based on previous mobile application functionality, test the security of mobile applications, etc.\nIn some embodiments, MATE component 120 can be implemented as a front-end, e.g., to LTE Gateways, to 3G GGSN Gateway, as a front end processor, as a software-defined network element, etc. In other embodiments, MATE component 120 can be\nimplemented as a cloud network component that can reside in the cloud and access mobile applications through an interface with the carrier core-network.  Information generated by MATE component 120, where a carrier has a large geographic footprint, e.g.,\na worldwide carrier, can be unique from data captured from conventional technologies that are not able to capture, process, analyze, curate, store, search, and re-target mobile application functional, geo-tagged, release-fit, security data, etc., where a\nconventional technology is not designed for use on a large carrier level scale.\nIt is envisioned that the MATE component 120 can provide a central point of integration among a plurality of mobile applications through a single carrier core-network, e.g., an LTE Gateway, etc., or through a plurality of carrier core-networks,\ne.g., multiple LTE Gateways, etc. Further, MATE component 120, can provide a central point of integration among a plurality of mobile applications through a software-defined element as a functional superset of a carrier core-network systems, e.g., a\nCommercial Connectivity Service (CCS) LTE Gateway, a plurality of carrier core-network CCS LTE Gateways, etc. Similarly, MATE component 120 can provide a central point of integration among a plurality of mobile applications through a carrier core-network\n3G GGSN Gateway, through a plurality of carrier core-network 3G GGSN Gateways, etc. As such, MATE component 120 can provide a central point of integration among a plurality of mobile applications and mobile devices, including IPdata originating from SS7\npackets, short message service (SMS) data, multimedia messaging service (MMS) data, location-based services and GPS coordinate-based data, for optimization of Quality of Service (QoS) on demand, etc.\nMoreover, analysis of data captured/stored by MATE component 120 can provide a central point of dynamic, software-defined management, control and provisioning functions for and among a plurality of mobile applications and mobile devices.  In an\naspect, data analysis that captures mobile application behaviors, etc., can allow adaptation of the carrier core-network or components thereof, a RAN or components thereof, a mobile device executing the mobile application, an application service or\ncomponents executing the application service, etc. As such, MATE component 120 can provide a central point of dynamic, software-defined management, control and provisioning functionality for, and among, a plurality of carrier-based core-network\ncomponent; a central point of dynamic, software-based surveillance and on-demand QoS awareness and optimization for, and among, a plurality of mobile applications and mobile devices; a central point of dynamic, software-based, optimized data-centric,\nhierarchical, and location-based medium access control and routing functions among a plurality of mobile applications and mobile devices, a central point of continuous, event-driven, query-driven, and hybrid data acquisition and delivery models on behalf\nof and among a plurality of mobile applications and mobile devices; a central point of data correlation on behalf of and among a plurality of mobile applications and mobile devices; etc.\nMATE component 120, in some embodiments, can correlate mobile application and mobile device management data to redirect correlated data to specific customer enterprise networks.  For example, data from a plurality of mobile applications and\nmobile devices can be correlated into a data record, or a plurality of integrated records, and then routed via an LTE Gateway or 3G GGSN carrier core-network gateway to one or a plurality of enterprise servers, data repositories can be accessed by Big\nData Analytics Engines, such as carrier-resident Big Data Analytics Engines, or customer premise-resident Big Data applications, targeted to MATE data repositories, etc. In an aspect, a MATE appliance, for example, mate appliance 422, can reside in the\ncustomer enterprise network and can interface closely with a carrier MATE component, e.g., MATE component 120, 420, etc. In an aspect, a MATE appliance can receive integrated and correlated records, and communicate with the carrier core-network MATE\ncomponent, e.g., MATE component 120, 420, etc., in a secure manner in order to request further information, to receive data analytics, etc., where the MATE component can collect and correlate mobile app and mobile device data, store these data in\nextremely large data repositories that reside either within the carrier network or in the cloud, etc.\nIn some embodiments, MATE component 120 can provide a central point of dynamically allocated security protocols among a plurality of mobile apps and mobile devices through use of data-centric routing of metadata exchanged among a plurality of\nconstituent MATE aggregation nodes, e.g., MATE appliance(s) such as 221, 421, 422, etc. This can enable data aggregation and data fusion among a plurality of mobile applications and mobile devices by dynamically optimizing among a plurality of signal\nprocessing and beamforming combinatorial techniques.\nFurther, in certain embodiments, MATE component 120 can provide low-level application programming interfaces (LLAPIs) between internal MATE components and sub-components.  Similarly, some embodiments can provide high-level application\nprogramming interfaces (HLAPIs) to external, non-MATE software-defined components.\nOf note, mobile application data 102 can be related to application service data 190.  In some embodiments, MAD 102 and application service data 190 can be the same.  As an example, where MATE component 120 only copies data sent from and to a\nmobile application, MAD 102 and application service data 190 can be the same because they represent the data from/to the mobile application, e.g., data passed through the carrier core-network component(s) that is merely copied to data store 130.  In\nother embodiments, MAD 102 and application service data 190 can be different.  As an example, where MATE component 120 copies the data and also alters the data flow between the mobile application and an application service, such as by injecting or\nremoving data as part of a test, interrupting the flow of data and substituting other data as part of a simulation, etc. This can be illustrated by MATE component 120 substituting simulated application service data in place of application service data\n190 for an interaction with a mobile application sending/receiving MAD 102, for example to examine mobile application behaviors related to the substituted simulation application service data.  This can allow an application service provider to test\nalternative embodiments of an application service.  Similarly, as an example, MATE component 120 can interrupt at least a portion of MAD 102, substitute simulated MAD, etc., for interactions with an application service component associated with\napplication service data 190 to test or simulate situation related to the behavior of the application service.  As an example, MAD 102 can be interrupted and routed through another set of network nodes, such as employing nodes in foreign countries,\nemploying deployed test nodes, employing simulated test nodes, etc., to examine the application service response via application service data 190, in which case, MAD 102 can be different from application service data 190 because of the MATE component 120\nalteration(s), such as altered headers, altered latency, altered throughput, etc. Thus, wherein MAD 102 can comprise data sent from a mobile application and data sent to a mobile application, and similarly, application service data 190 can comprises data\nsent from a mobile application and data sent to a mobile application, these data can be the same where MATE component 120 does not alter them, e.g., only a copy is made, they are passed without any interaction, etc., or they can be different where MATE\ncomponent 120 alters the data or conditions for traversing between the application service component and the mobile device.\nFIG. 2 is a depiction of a system 200 that can facilitate mobile application testing and data capture at a carrier core-network component in accordance with aspects of the subject disclosure.  System 200 can comprise carrier core-network\ncomponent 208 that can comprise MATE component 220A and data store 230A.  In an embodiment, carrier core-network component 208 can be a gateway device, such as an LTE gateway, 3GPP gateway, etc. In other embodiments, carrier core-network device can be\nanother component of the core-network of the carrier, meaning that it can be embodied in a device that is not part of mobile device 204, RAN 206, or application service component 294, e.g., a device of the central part of a telecommunications network\nassociated with a carrier, for example, gateways, switches, routers, a home location register, a visitor location register, a mobile switching center, an equipment identity register, an authentication center, etc. Carrier core-network component 208 can,\nin some embodiments, be a component involved in transport of MAD 202 to/from mobile device 204 via RAN 206.  As previously noted, in some instances MAD 202 can be the same as application service data 290, while in other instances, MAD 202 can be\ndifferent from application service data 290, e.g., in instances where MAD 202 is altered as part of testing/simulation, transport, filtering, etc.\nMATE component 220A can duplicate MAD 202, application service data 290, RAN information from RAN 206, core-network information, etc., and can store some, none, or all, of the duplicated data, e.g., at data store 230A.  MATE technology can\nenable massive stores of data related to a mobile application, generally all, or nearly all, duplicated data can be stored.  However, in some circumstances, duplication of data, or storage thereof, can be undesirable or prohibited, in which situations,\nless than all of the MAD 202 or application service data 290 can be duplicated and/or stored.\nMATE component 220A can capture behavior and behavior patterns of mobile applications and store these captured mobile application behaviors or patterns in a knowledge database, e.g., based on data stored at data store 230A, etc. A carrier has a\nunique advantage in that it can, for example, inspect headers, etc., as data flows through a carrier's core-network and determine the behavior of all, or in some instances some or none, of the mobile application behaviors before they leave or return\nthrough the carrier network to/from communication framework 292 or application service component 294, etc. In an embodiment, MATE component 220A, e.g., via data store 230A, can provide mobile application usage and validated data to big data analytics\nengines.  In some embodiments, MATE component 220A can perform tests or analysis on captured data, test or simulate a condition with a mobile application in execution, or test/simulate an interaction with an applications service in execution, etc.\nSystem 200 further illustrates alternate and/or supplemental locations of MATE component 220A.  In an embodiment, MATE component 220a can be supplemented or replaced by a MATE component in another location that is communicatively coupled to\ncarrier core-network component 208, e.g., MATE component 220B-D. MATE Component 220B can be communicatively coupled to, but located external to, carrier core-network component 208 and can receive/send MAD 202, and/or application serviced data 290, via a\nconnection with the data flows between RAN 206 and carrier core-network component 208.  In some embodiments, MATE component 220C can be similar to MATE component 220B but can receive MAD 202 and/or application service data 290 via carrier core-network\ncomponent 208 rather than from a connection between RAN 206 and carrier core-network component 208.  In another embodiment, MATE component 220D can be similar to 220B and/or 220C, but can be have a communicative coupling with carrier core-network\ncomponent 208 via communication framework 292.  In some embodiments, MATE component 220D can be embodied as a cloud-based component.  Similarly, a data store for storing MAD 202 and/or application service data 290, can be located within carrier\ncore-network component 208, e.g., data store 230A, can be located locally but external to carrier core-network component 208, e.g., data store 230B, can be located remote from carrier core-network component 208, e.g., data store 230C, etc. Data store,\ne.g., 230A-C can send/receive data to/from MATE component, e.g., 220A-D, via one or more communicative links.\nIn an aspect, MAD 202, and/or application service data 290, can be received by and/or sent from a MATE component, e.g., 220A-D, in communication with carrier core-network component 208.  MAD 202 and/or application service data 290 can be stored,\nin whole or in part, at a data store, e.g., data store 230A-C, in communication with carrier core-network component 208.  The stored data can be analyzed to facilitate determining a behavior, a trend, a characteristic, etc., relating to a mobile\napplication.\nIn an embodiment, system 200 can comprise other carrier core-network component 209 that can be part of a core-network associated with another carrier entity, with another core-network of the same carrier entity, etc. As an example, a wireless\ncarrier can have more than one core-network, such as in nation-sized geographic regions, etc., and carrier core-network component 208 can be communicatively coupled with other carrier core-network component 209 in a manner that enables sharing of data\nbetween MATE component 220A and MATE appliance 221.  In an embodiment, MATE appliance 221 can mirror a MATE component, e.g., 220A-D. In another embodiment, MATE appliance 221 can share some, none, or all information with a MATE component, e.g., 220A-D,\ne.g., via a query-response protocol, via a push protocol, via a pull protocol, etc. As such, MATE appliance 221 can perform analysis on data captured by a MATE component, e.g., 220A-D, manage and respond to queries regarding MATE information, etc. In\nsome embodiments, MATE appliance 221 can be located at a carrier core-network associated with a different carrier.  In still other embodiments, MATE appliance 221 can be located an enterprise level core network for a private network or corporate network,\nthough this is not illustrated for the sake of simplicity.\nSystem 200 illustrates the flexible placement of a MATE component, e.g., 220A-D, and the related data store, e.g., 230A-C, in relation to carrier core-network component 208.  It will be noted that a MATE component, e.g., 220A-D, can be hardware,\nsoftware, virtualized, or cloud-based, as will be readily appreciated by one of ordinary skill in the relevant art, however, unlike conventional technologies, MATE component 220A-D has an inherent distinction in that it can capture, by being associated\ndirectly with a carrier, data associated with a mobile application as it transitions the carrier core-network.  This advantage will typically allow more data for more mobile applications to be captured than other conventional technologies.  Moreover,\nanalysis of the data can enable adaptation of data transport readily because of the close integration between MATE component 220A-D and carrier core-network component 208, which is an aspect not typically available in conventional mobile application\ntesting systems.\nFIG. 3 illustrates a system 300 that facilitates mobile application testing and data capture to support adaptation of data transport in accordance with aspects of the subject disclosure.  System 300 can comprise carrier core-network component\n308.  Carrier core-network component 308 can comprise MATE component 320, which can receive MAD 302.  MATE component 320 can comprise MAD capture component 350.  MAD capture component 350 can capture MAD 302 for storage on data store 330.\nMAD analysis component 340 can access MAD information stored on data store 330.  In an aspect, MAD analysis component 340 can perform analysis on MAD 302 or related information, e.g., MAD information stored on data store 330, carrier network\ntopology information, carrier network performance information, etc. MAD 302 can comprise user information, data size/volume information, application version information, latency information, QOS information, duration information, key performance\nindicator (KPI) information, geographic or location information, called application information, routing information, temporal information, device type information, OS information, or nearly any other type of information related to execution of a mobile\napplication, transport of data related to the mobile application, or devices associated with the mobile application.  In some embodiments, supplementary information can be collected from other components, e.g., data related to transport of mobile\napplication data 302 and/or application service data via carrier network devices, data related to services or components accessed outside of the carrier network, etc. MAD analysis component 340 can determine characteristics, behaviors, trends, or\npredictions related to performance of a mobile application, transport of MAD 302 and/or application service data, etc.\nIn some embodiments, MAD analysis component 340 can test mobile applications, such as by altering MAD 302 to change routing, alter latency, alter accessibility to services, alter throughput, altering data in an attempt to cause errors/failures,\ne.g., fuzzing of MAD 302 and/or application service data, etc. In other embodiments, MAD analysis component 340 can disrupt the interaction between a mobile application in execution and an application service as part of simulating test conditions.  This\ncan allow MAD analysis component 340 to act as a simulated application service to test a mobile application in execution and/or can enable MAD analysis component 340 to act as a simulated mobile application in execution to test an application service.\nMATE component 320 can further comprise MAD adaptation component 360 that can access an analysis/test/simulation performed by MAD analysis component 340.  MAD adaptation component 360 can interact with a carrier network to alter conditions\nrelated to the transport and management of MAD 302 and/or application service data.  As illustrated in system 300, MAD adaptation component 360 can alter the flow of MAD 302 causing it to flow via different carrier network paths, e.g., carrier\ncore-network data path 362, 363, 364, etc. This adaptation of the carrier network can cause MAD 302 to flow to and from different instances of an application service, e.g., as application service data 390A-C, etc. In an aspect, application service data\n390A can be located in a different location, on a different component, in a different software environment, in a different application provider entity device, etc., than application service data 390B, which in turn can be different from application\nservice data 390C.  As an example, application service data 390A can be located on a first generation server located in the U.S., application service data 390B can be located on a first generation server located in Canada, and application service data\n390C can be located on a third generation server executing a beta version of a software environment and be located in Canada.  In this example, MAD adaptation component 360 can cause MAD 302 to be routed to one or more of the application services\nassociated with application service data 390A-C to test the effects of the different geographical locations, the different hardware, the different software, etc. Of note, other parameters, characteristics, or behaviors can be interrogated in a similar\nmanner, all of which are considered within the scope of the present disclosure even where not explicitly recited for the sake of clarity and brevity.\nFIG. 4 illustrates a system 400 that facilitates mobile application testing and data capture via components at various network locations in accordance with aspects of the subject disclosure.  System 400 can comprise a carrier core-network\ncomponent 408 that can comprise MATE component 420 and data store 430.  MATE component 420 can receive MAD 402 via carrier core-network component 408 and can store a facsimile of MAD 302 or related information at data store 430.  MAD 402 can be passed\nvia communication framework 492 to application service component 494 and data, e.g., application service data 490, can flow back to the mobile application from application service component 494.\nSystem 400 can further comprise MATE appliance 421 that can be part of another carrier core-network component 409.  MATE appliance 421 can interact with MATE component 420 and data store 430 via communication framework 492 and carrier\ncore-network component 408 to enable access to MATE information, e.g., MAD 402, results of analysis of MAD 402, testing or simulation results related to MAD 402, etc. In some aspects, other carrier core-network component 409 can be associated with\nanother carrier, an enterprise network, another branch of the same carrier, etc.\nSystem 400 can further comprise MATE appliance 422 that can be part of MATE data subscriber access component 470.  MATE data subscriber access component 470 can further comprise query component 471 that can cause queries of MATE information,\ne.g., MAD 402, results of analysis of MAD 402, testing or simulation results related to MAD 402, etc., via application service component 494, communication framework 492, and carrier core-network component 408.  MATE appliance 422 can interact with MATE\ncomponent 420 and data store 430 to enable access to MATE information, e.g., MAD 402, results of analysis of MAD 402, testing or simulation results related to MAD 402, etc., in relation to queries caused by query component 471.  In an aspect, MATE data\nsubscriber access component 470 can be associated with an application service provider, e.g., via application service component 494, which can be a MATE data subscriber, e.g., the application service provider can subscribe to MATE data and be allowed to\nquery MATE information.  Subscription to MATE information can be governed by a subscriber agreement, e.g., detailing what MATE information can be accessed/queried, levels of granularity, such as single mobile application in execution data, agglomerated\nmobile application in execution information, market trends, competing product behavior/statistics, etc.\nIn some embodiments, system 400 can comprise MATE appliance 423 that can be part of MATE data publication component 472.  MATE data publication component 472 can further comprise query component 473 that can cause queries of MATE information,\ne.g., MAD 402, results of analysis of MAD 402, testing or simulation results related to MAD 402, etc., via communication framework 492 and carrier core-network component 408.  MATE appliance 423 can interact with MATE component 420 and data store 430 to\nenable access to MATE information, e.g., MAD 402, results of analysis of MAD 402, testing or simulation results related to MAD 402, etc., in relation to queries caused by query component 473.  In an aspect, MATE data publication component 472 can enable\naccess to MATE information on an ad hoc basis.  This access to MATE information can be governed by a use agreement, e.g., detailing what MATE information can be accessed/queried, levels of granularity, such as single mobile application in execution data,\nagglomerated mobile application in execution information, market trends, product behavior/statistics for various application services and/or mobile applications found in the market, etc. In an embodiment, MATE data publication component 472 can act as a\nportal to allow external users access to MATE information, subject to rules related to privacy, protection of proprietary information, etc. As an example, a university researcher can query MATE information to discover information/behaviors related to a\ncomparison of carrier core-network latency, hop count, etc., for MAD 402 originating in Oklahoma in comparison to MAD 402 originating in New York.  Similar examples are readily appreciated for comparing U.S.  core-networks with Asian core-networks or\nAfrican core-networks, etc. Further examples can comprise access to MATE information for the same region but for different carriers.  Still other examples can comprise access to MATE information related to different hardware/software environments,\ndifferent transport topologies, different mobile devices, different mobile device operating systems, different application server types, etc., all of which are considered within the scope of the instant disclosure though not further discussed for the\nsake of clarity and brevity.  In an aspect, MATE data publication component 472 can be linked to data store 431 to enable MATE information to be copied from data store 430 to data store 431 in relation to a query of MATE information, to allow saving of\nan external analysis to data store 431, to allow storage of query arguments for later use, etc.\nFIG. 5 illustrates a system 500 that facilitates mobile application testing and simulation supporting adaptation of data in accordance with aspects of the subject disclosure.  System 500 can comprise MATE component 520 that can access and/or\nreturn MAD 502.  MATE component 520 can comprise MAD capture component 550 that can copy MAD 302 flowing from or to a mobile application, e.g., to or form an application service.  Captured MAD 502 and supplementary information relating to transport of\nMAD 502, carrier network topology or other KPIs or conditions, application service component information, etc., collectively MATE information, can be made accessible to MAD analysis component 540.  MAD analysis component 540 can analyze MATE information. MAD adaptation component 560 can adapt MAD 502, an environment related to MAD 502, transport of MAD 502, etc., in response to the analysis performed by MAD analysis component 540.  This adaptation can be made to carrier core-network data path 562 to\ncause changes to MAD 502 as it is passed to an application service and/or can cause changes to application service data 590 as it is returned via carrier core-network data path 562.  As an example, MAD analysis component 540 can determine that streaming\nvideo data comprised in MAD 502 can experience lower latency by employing a particular path between the mobile device and a streaming video service provider component, and can instruct carrier core-network data path 562 to be adapted to provide the\nparticular path to reduce said latency.  In an embodiment, MATE component 520 can employ continuously variable machine logic, to facilitate adaptation of MAD 502 and/or application service data 590, e.g., via an analysis performed by MAD analysis\ncomponent 540, etc. In an aspect, continuously variable machine logic can be a form of machine learning that can allow programming to be self-updated in real time based on logical rules.  Continuously variable machine logic can enable a system or\ncomponent to self-update programming based on changes experienced by the system or component.\nMATE component 520 can further comprise application service simulation component 580 that can interrupt flow of MAD 502 from a mobile application in execution to an application service component and substitute data simulating the mobile\napplication in execution to allow for testing of the application service and/or application service component.  In some embodiments, the simulation can be based on MAD 502.  In other embodiments, the simulation can be based on a simulation profile, etc.\nAs an example, a VoLTE application service can be tested by simulating a VoLTE mobile application in execution at application service simulation component 580.  The simulation can be an engineered simulation profile or can be based on MAD 502 from a real\nVoLTE mobile application in execution that has been altered to interrogate aspects of the VoLTE application service.  The simulation can fuzz VoLTE data comprised in MAD 502, can present simulation data via different transport paths, can introduce\nartificial latency, availability, or throughput constraints, etc.\nIn some embodiments, MATE component 520 can comprise MAD analysis testing component 582 that can interrupt flow of application service data 590 from an application service and substitute data simulating the application service interacting with a\nmobile application in execution associated with MAD 502 to allow for testing of the mobile application and/or mobile device environment.  In some embodiments, the MAD analysis testing can be based on application service data 590.  In other embodiments,\nthe MAD analysis testing can be based on an engineered testing profile, etc. As an example, a mobile mapping application can be tested by simulating a mapping application service.  The simulation can be an engineered testing profile or can be based on\napplication service data 590 from a real mapping application service that has been altered to test aspects of the mobile mapping application in execution.  The testing can fuzz mapping service data comprised in application service data 590, can present\ntesting data via different transport paths, can introduce artificial latency, availability, or throughput constraints, etc.\nIn view of the example system(s) described above, example method(s) that can be implemented in accordance with the disclosed subject matter can be better appreciated with reference to flowcharts in FIG. 6-FIG. 8.  For purposes of simplicity of\nexplanation, example methods disclosed herein are presented and described as a series of acts; however, it is to be understood and appreciated that the claimed subject matter is not limited by the order of acts, as some acts may occur in different orders\nand/or concurrently with other acts from that shown and described herein.  For example, one or more example methods disclosed herein could alternatively be represented as a series of interrelated states or events, such as in a state diagram.  Moreover,\ninteraction diagram(s) may represent methods in accordance with the disclosed subject matter when disparate entities enact disparate portions of the methods.  Furthermore, not all illustrated acts may be required to implement a described example method\nin accordance with the subject specification.  Further yet, two or more of the disclosed example methods can be implemented in combination with each other, to accomplish one or more aspects herein described.  It should be further appreciated that the\nexample methods disclosed throughout the subject specification are capable of being stored on an article of manufacture (e.g., a computer-readable medium) to allow transporting and transferring such methods to computers for execution, and thus\nimplementation, by a processor or for storage in a memory.\nFIG. 6 illustrates a method 600 facilitating mobile application data capture and analysis in accordance with aspects of the subject disclosure.  At 610, method 600 can comprise receiving mobile application data (MAD).  MAD can comprise data sent\nby a mobile application and data sent to a mobile application.  In some aspects, MAD can further comprise information related to a device executing a mobile application, an OS, network transport information, e.g., hops, devices/software, latency,\nthroughput, availability, alternate routes, etc., application service information, etc.\nAt 620, method 600 can comprise, storing at least a portion of the MAD received at 610.  In an aspect, storage of MAD, or related/supplementary information, can be in addition to passing MAD through the carrier core-network to an application\nservice or returning application service data back to a mobile application via the carrier core-network.  In some embodiments, some, none, or all of the MAD can be stored.  In some embodiments, some, none, or all of the related/supplementary information\ncan be stored.  MAD and/or related information can be stored on a local data storage device, a remotely located data storage device, on a cloud-network implemented storage component, etc.\nAt 630, method 600 can enable access to the MAD, facsimiles of the MAD, or an adapted version of the MAD, via a carrier core-network component.  In an embodiment, access to the MAD can be from a data store via the carrier core-network component. In other embodiments, access to the MAD can be directed through the carrier core-network component while permitting storage of a copy of the MAD.  In further embodiments, access to adapted MAD, e.g., as in a testing or simulation process, etc., can be\ndirected through the carrier core-network component directly or from a data store.\nMethod 600, at 640, can comprise enabling analysis of the stored MAD, supplementary information, etc., to facilitate determining a characteristic of the MAD.  At this point, method 600 can end.  Analysis can reveal trends, patterns,\ncharacteristics, behaviors, etc., associated with a mobile application, such as discovering communication bottlenecks for MAD traversing a carrier core-network, a RAN, other networks, etc., determining alternate paths that can be associated with\ndifferent latency, throughput, or service availability, etc., identifying issues associated with the mobile application and devices, hardware, software, operating systems, etc., or nearly any other aspect related to the mobile application.  Analysis, in\nsome embodiments, can be based on header information for data packets that comprise MAD both to and from an application service, e.g., via an application service component.\nIn certain embodiments, a system performing method 600 can comprise a component in communication with a carrier core-network device, or can be comprised in a carrier core-network device, e.g., a gateway, router, switch, etc. Furthermore, a\nsystem performing method 600 can be in communication with devices or components outside of the carrier core-network, such as an application service device, a mobile device, a RAN device, other networking devices, servers, etc., that can facilitate\ncapture of information that is supplementary to MAD, for example, RAN hardware or software configuration information, location information, application service component OS information, network path location/device/software information, etc., can be\ncaptured in addition to, or as part of, MAD.\nFIG. 7 illustrates a method 700 that facilitates analysis of historical data related to a mobile application in accordance with aspects of the subject disclosure.  At 710, method 700 can comprise receiving historical MAD from a data store\ndevice.  In an embodiment, the data store device can be located local to, or as part of, a carrier core-network component.  In another embodiment, the data store device can be located remotely and be coupled to a carrier core-network component via a\nwired and/or wireless connection, e.g., the Internet, etc. In an embodiment, historical MAD can comprise data generated by a mobile application in execution, data sent to a mobile application, or supplementary data related to execution of the mobile\napplication or transport of data to, or from, the mobile application.\nAt 720, an analysis of the historical MAD can be performed by method 700.  Analysis can reveal trends, patterns, characteristics, behaviors, etc., associated with a mobile application, such as discovering communication characteristics for MAD\ntraversing a carrier core-network, a RAN, other networks, etc., determining data communication paths that can be associated with different latency, throughput, or service availability, etc., identifying issues associated with the mobile application and\ndevices, hardware, software, operating systems, etc., or nearly any other aspect related to the mobile application.  Analysis, in some embodiments, can be based on header information for data packets that comprise MAD, both to and from an application\nservice.\nAt 730, transport of MAD can be adapted in response to determining a characteristic of the historical MAD at 720.  At this point method 700 can end.  Adapting the transport can comprise, in some embodiments, altering latency, throughput, or\nservice availability, for the MAD data as it traverses a carrier core-network.  In some embodiments, adapting can comprise selecting another transport path between a mobile device executing a mobile application and an application service component. \nEmbodiments can also comprise testing and/or simulation via adapting the transport, e.g., where a carrier core-network component interrupts communication between a mobile application in execution and an application service by altering the MAD or\nsubstituting MAD to test either, or both, the mobile application or application service under a different transport permutations.\nFIG. 8 illustrates a method 800 that facilitates access to captured mobile application data for devices outside of a carrier core-network in accordance with aspects of the subject disclosure.  At 810, method 800 can comprise receiving MAD at a\ncarrier-core network component.  MAD can comprise data sent by a mobile application and data sent to a mobile application.  In some aspects, MAD can further comprise information related to a device executing a mobile application, an OS, network transport\ninformation, e.g., hops, devices/software, latency, throughput, availability, alternate routes, etc., application service information, etc.\nAt 820, method 800 can comprise, storing at least a portion of the MAD at a data storage device.  The data store device can be located local to, or as part of, the carrier core-network component.  In another embodiment, the data store device can\nbe located remotely and be coupled to the carrier core-network component via a wired and/or wireless connection.  In an embodiment, MAD can comprise data generated by a mobile application in execution, data sent to a mobile application, or supplementary\ndata related to execution of the mobile application or transport of data to, or from, the mobile application.\nAt 830, a request for access to the stored MAD can be received from a MATE appliance, e.g., 221, 421-423, etc. A MATE appliance can receive integrated and correlated records related to MAD, and communicate with a carrier core-network component,\ne.g., MATE component 120, 420, etc., in a secure manner in order to request further information, to receive data analytics, etc., and the MATE component can collect and correlate mobile application and mobile device data, store these data in extremely\nlarge data repositories that reside either within the carrier network or in the cloud, etc. In some embodiments, the request for access to the stored MAD can be received from a MATE appliance of the carrier network, of another carrier network, of an\nenterprise private network, of an application service provider, etc.\nAt 840, access to the stored MAD can be allowed to enable analysis of the stored MAD data, by a device connected with the MATE appliance.  At this point method 800 can end.  In an aspect, the device connected to the MATE appliance can be\nexternal to the carrier core-network.  As an example, an application service provider can analyze stored MAD data received via a MATE appliance, to better understand or test performance of a mobile application that transports data to/from the mobile\napplication via the carrier core-network.  In an aspect, access to data can be controlled, e.g., by the carrier via access rules, subscription agreements, related laws and policies, etc., to protect private data, limit access to sensitive data, quash\nunfair competition, etc., as will be appreciated by those familiar with the relevant arts.\nFIG. 9 is a schematic block diagram of a computing environment 900 with which the disclosed subject matter can interact.  The system 900 comprises one or more remote component(s) 910.  The remote component(s) 910 can be hardware and/or software\n(e.g., threads, processes, computing devices).  In some embodiments, remote component(s) 910 can comprise servers, personal servers, wireless telecommunication network devices, etc. As an example, remote component(s) 910 can be mobile device 204, RAN\n206, application service component 294, 494, etc., other carrier core-network component 209, 409, etc., MATE data subscriber access component 470, MATE data publication component 472, etc.\nThe system 900 also comprises one or more local component(s) 920.  The local component(s) 920 can be hardware and/or software (e.g., threads, processes, computing devices).  In some embodiments, local component(s) 920 can comprise, for example,\ncarrier core-network component 208, 308, 408, etc., MATE component 120, 220A, 220B, 220C, 320, 420, 520, etc., carrier core-network data path 562, etc.\nOne possible communication between a remote component(s) 910 and a local component(s) 920 can be in the form of a data packet adapted to be transmitted between two or more computer processes.  Another possible communication between a remote\ncomponent(s) 910 and a local component(s) 920 can be in the form of circuit-switched data adapted to be transmitted between two or more computer processes in radio time slots.  The system 900 comprises a communication framework 940 that can be employed\nto facilitate communications between the remote component(s) 910 and the local component(s) 920, and can comprise an air interface, e.g., Uu interface of a UMTS network.  Remote component(s) 910 can be operably connected to one or more remote data\nstore(s) 950, such as a hard drive, SIM card, device memory, etc., that can be employed to store information on the remote component(s) 910 side of communication framework 940.  Similarly, local component(s) 920 can be operably connected to one or more\nlocal data store(s) 930, that can be employed to store information on the local component(s) 920 side of communication framework 940.\nIn order to provide a context for the various aspects of the disclosed subject matter, FIG. 10, and the following discussion, are intended to provide a brief, general description of a suitable environment in which the various aspects of the\ndisclosed subject matter can be implemented.  While the subject matter has been described above in the general context of computer-executable instructions of a computer program that runs on a computer and/or computers, those skilled in the art will\nrecognize that the disclosed subject matter also can be implemented in combination with other program modules.  Generally, program modules comprise routines, programs, components, data structures, etc. that performs particular tasks and/or implement\nparticular abstract data types.\nIn the subject specification, terms such as \"store,\" \"storage,\" \"data store,\" data storage,\" \"database,\" and substantially any other information storage component relevant to operation and functionality of a component, refer to \"memory\ncomponents,\" or entities embodied in a \"memory\" or components comprising the memory.  It is noted that the memory components described herein can be either volatile memory or nonvolatile memory, or can comprise both volatile and nonvolatile memory, by\nway of illustration, and not limitation, volatile memory 1020 (see below), non-volatile memory 1022 (see below), disk storage 1024 (see below), and memory storage 1046 (see below).  Further, nonvolatile memory can be included in read only memory,\nprogrammable read only memory, electrically programmable read only memory, electrically erasable read only memory, or flash memory.  Volatile memory can comprise random access memory, which acts as external cache memory.  By way of illustration and not\nlimitation, random access memory is available in many forms such as synchronous random access memory, dynamic random access memory, synchronous dynamic random access memory, double data rate synchronous dynamic random access memory, enhanced synchronous\ndynamic random access memory, Synchlink dynamic random access memory, and direct Rambus random access memory.  Additionally, the disclosed memory components of systems or methods herein are intended to comprise, without being limited to comprising, these\nand any other suitable types of memory.\nMoreover, it is noted that the disclosed subject matter can be practiced with other computer system configurations, comprising single-processor or multiprocessor computer systems, mini-computing devices, mainframe computers, as well as personal\ncomputers, hand-held computing devices (e.g., personal digital assistant, phone, watch, tablet computers, netbook computers, .  . . ), microprocessor-based or programmable consumer or industrial electronics, and the like.  The illustrated aspects can\nalso be practiced in distributed computing environments where tasks are performed by remote processing devices that are linked through a communications network; however, some if not all aspects of the subject disclosure can be practiced on stand-alone\ncomputers.  In a distributed computing environment, program modules can be located in both local and remote memory storage devices.\nFIG. 10 illustrates a block diagram of a computing system 1000 operable to execute the disclosed systems and methods in accordance with an embodiment.  Computer 1012, which can be, for example, carrier core-network component 208, 308, 408, etc.,\nMATE component 120, 220A, 220B, 220C, 320, 420, 520, etc., carrier core-network data path 562, etc., mobile device 204, RAN 206, application service component 294, 494, etc., other carrier core-network component 209, 409, etc., MATE data subscriber\naccess component 470, MATE data publication component 472, etc., comprises a processing unit 1014, a system memory 1016, and a system bus 1018.  System bus 1018 couples system components comprising, but not limited to, system memory 1016 to processing\nunit 1014.  Processing unit 1014 can be any of various available processors.  Dual microprocessors and other multiprocessor architectures also can be employed as processing unit 1014.\nSystem bus 1018 can be any of several types of bus structure(s) comprising a memory bus or a memory controller, a peripheral bus or an external bus, and/or a local bus using any variety of available bus architectures comprising, but not limited\nto, industrial standard architecture, micro-channel architecture, extended industrial standard architecture, intelligent drive electronics, video electronics standards association local bus, peripheral component interconnect, card bus, universal serial\nbus, advanced graphics port, personal computer memory card international association bus, Firewire (Institute of Electrical and Electronics Engineers 1194), and small computer systems interface.\nSystem memory 1016 can comprise volatile memory 1020 and nonvolatile memory 1022.  A basic input/output system, containing routines to transfer information between elements within computer 1012, such as during start-up, can be stored in\nnonvolatile memory 1022.  By way of illustration, and not limitation, nonvolatile memory 1022 can comprise read only memory, programmable read only memory, electrically programmable read only memory, electrically erasable read only memory, or flash\nmemory.  Volatile memory 1020 comprises read only memory, which acts as external cache memory.  By way of illustration and not limitation, read only memory is available in many forms such as synchronous random access memory, dynamic read only memory,\nsynchronous dynamic read only memory, double data rate synchronous dynamic read only memory, enhanced synchronous dynamic read only memory, Synchlink dynamic read only memory, Rambus direct read only memory, direct Rambus dynamic read only memory, and\nRambus dynamic read only memory.\nComputer 1012 can also comprise removable/non-removable, volatile/non-volatile computer storage media.  FIG. 10 illustrates, for example, disk storage 1024.  Disk storage 1024 comprises, but is not limited to, devices like a magnetic disk drive,\nfloppy disk drive, tape drive, flash memory card, or memory stick.  In addition, disk storage 1024 can comprise storage media separately or in combination with other storage media comprising, but not limited to, an optical disk drive such as a compact\ndisk read only memory device, compact disk recordable drive, compact disk rewritable drive or a digital versatile disk read only memory.  To facilitate connection of the disk storage devices 1024 to system bus 1018, a removable or non-removable interface\nis typically used, such as interface 1026.\nComputing devices typically comprise a variety of media, which can comprise computer-readable storage media or communications media, which two terms are used herein differently from one another as follows.\nComputer-readable storage media can be any available storage media that can be accessed by the computer and comprises both volatile and nonvolatile media, removable and non-removable media.  By way of example, and not limitation,\ncomputer-readable storage media can be implemented in connection with any method or technology for storage of information such as computer-readable instructions, program modules, structured data, or unstructured data.  Computer-readable storage media can\ncomprise, but are not limited to, read only memory, programmable read only memory, electrically programmable read only memory, electrically erasable read only memory, flash memory or other memory technology, compact disk read only memory, digital\nversatile disk or other optical disk storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or other tangible media which can be used to store desired information.  In this regard, the term \"tangible\" herein\nas may be applied to storage, memory or computer-readable media, is to be understood to exclude only propagating intangible signals per se as a modifier and does not relinquish coverage of all standard storage, memory or computer-readable media that are\nnot only propagating intangible signals per se.  In an aspect, tangible media can comprise non-transitory media wherein the term \"non-transitory\" herein as may be applied to storage, memory or computer-readable media, is to be understood to exclude only\npropagating transitory signals per se as a modifier and does not relinquish coverage of all standard storage, memory or computer-readable media that are not only propagating transitory signals per se.  Computer-readable storage media can be accessed by\none or more local or remote computing devices, e.g., via access requests, queries or other data retrieval protocols, for a variety of operations with respect to the information stored by the medium.  As such, for example, a computer-readable medium can\ncomprise executable instructions stored thereon that, in response to execution, cause a system comprising a processor to perform operations, comprising: receiving trigger information a remote device, e.g., a UE, and in response, generating communication\naugmentation information that can be accessed via an air interface or other wireless interface by one or more service interface components or other UEs to enable context sensitive communication augmentation.\nCommunications media typically embody computer-readable instructions, data structures, program modules or other structured or unstructured data in a data signal such as a modulated data signal, e.g., a carrier wave or other transport mechanism,\nand comprises any information delivery or transport media.  The term \"modulated data signal\" or signals refers to a signal that has one or more of its characteristics set or changed in such a manner as to encode information in one or more signals.  By\nway of example, and not limitation, communication media comprise wired media, such as a wired network or direct-wired connection, and wireless media such as acoustic, RF, infrared and other wireless media.\nIt can be noted that FIG. 10 describes software that acts as an intermediary between users and computer resources described in suitable operating environment 1000.  Such software comprises an operating system 1028.  Operating system 1028, which\ncan be stored on disk storage 1024, acts to control and allocate resources of computer system 1012.  System applications 1030 take advantage of the management of resources by operating system 1028 through program modules 1032 and program data 1034 stored\neither in system memory 1016 or on disk storage 1024.  It is to be noted that the disclosed subject matter can be implemented with various operating systems or combinations of operating systems.\nA user can enter commands or information into computer 1012 through input device(s) 1036.  In some embodiments, a user interface can allow entry of user preference information, etc., and can be embodied in a touch sensitive display panel, a\nmouse input GUI, a command line controlled interface, etc., allowing a user to interact with computer 1012.  As an example, mobile device 204, application service component 294, 494, etc., can receive touch, motion, audio, visual, or other types of\ninput.  Input devices 1036 comprise, but are not limited to, a pointing device such as a mouse, trackball, stylus, touch pad, keyboard, microphone, joystick, game pad, satellite dish, scanner, TV tuner card, digital camera, digital video camera, web\ncamera, cell phone, smartphone, tablet computer, etc. These and other input devices connect to processing unit 1014 through system bus 1018 by way of interface port(s) 1038.  Interface port(s) 1038 comprise, for example, a serial port, a parallel port, a\ngame port, a universal serial bus, an infrared port, a Bluetooth port, an IP port, or a logical port associated with a wireless service, etc. Output device(s) 1040 use some of the same type of ports as input device(s) 1036.\nThus, for example, a universal serial busport can be used to provide input to computer 1012 and to output information from computer 1012 to an output device 1040.  Output adapter 1042 is provided to illustrate that there are some output devices\n1040 like monitors, speakers, and printers, among other output devices 1040, which use special adapters.  Output adapters 1042 comprise, by way of illustration and not limitation, video and sound cards that provide means of connection between output\ndevice 1040 and system bus 1018.  It should be noted that other devices and/or systems of devices provide both input and output capabilities such as remote computer(s) 1044.\nComputer 1012 can operate in a networked environment using logical connections to one or more remote computers, such as remote computer(s) 1044.  Remote computer(s) 1044 can be a personal computer, a server, a router, a network PC, cloud\nstorage, a cloud service, code executing in a cloud-computing environment, a workstation, a microprocessor based appliance, a peer device, or other common network node and the like, and typically comprises many or all of the elements described relative\nto computer 1012.\nFor purposes of brevity, only a memory storage device 1046 is illustrated with remote computer(s) 1044.  Remote computer(s) 1044 is logically connected to computer 1012 through a network interface 1048 and then physically connected by way of\ncommunication connection 1050.  Network interface 1048 encompasses wire and/or wireless communication networks such as local area networks and wide area networks.  Local area network technologies comprise fiber distributed data interface, copper\ndistributed data interface, Ethernet, Token Ring and the like.  Wide area network technologies comprise, but are not limited to, point-to-point links, circuit-switching networks like integrated services digital networks and variations thereon, packet\nswitching networks, and digital subscriber lines.  As noted below, wireless technologies may be used in addition to or in place of the foregoing.\nCommunication connection(s) 1050 refer(s) to hardware/software employed to connect network interface 1048 to bus 1018.  While communication connection 1050 is shown for illustrative clarity inside computer 1012, it can also be external to\ncomputer 1012.  The hardware/software for connection to network interface 1048 can comprise, for example, internal and external technologies such as modems, comprising regular telephone grade modems, cable modems and digital subscriber line modems,\nintegrated services digital network adapters, and Ethernet cards.\nThe above description of illustrated embodiments of the subject disclosure, comprising what is described in the Abstract, is not intended to be exhaustive or to limit the disclosed embodiments to the precise forms disclosed.  While specific\nembodiments and examples are described herein for illustrative purposes, various modifications are possible that are considered within the scope of such embodiments and examples, as those skilled in the relevant art can recognize.\nIn this regard, while the disclosed subject matter has been described in connection with various embodiments and corresponding Figures, where applicable, it is to be understood that other similar embodiments can be used or modifications and\nadditions can be made to the described embodiments for performing the same, similar, alternative, or substitute function of the disclosed subject matter without deviating therefrom.  Therefore, the disclosed subject matter should not be limited to any\nsingle embodiment described herein, but rather should be construed in breadth and scope in accordance with the appended claims below.\nAs it employed in the subject specification, the term \"processor\" can refer to substantially any computing processing unit or device comprising, but not limited to comprising, single-core processors; single-processors with software multithread\nexecution capability; multi-core processors; multi-core processors with software multithread execution capability; multi-core processors with hardware multithread technology; parallel platforms; and parallel platforms with distributed shared memory. \nAdditionally, a processor can refer to an integrated circuit, an application specific integrated circuit, a digital signal processor, a field programmable gate array, a programmable logic controller, a complex programmable logic device, a discrete gate\nor transistor logic, discrete hardware components, or any combination thereof designed to perform the functions described herein.  Processors can exploit nano-scale architectures such as, but not limited to, molecular and quantum-dot based transistors,\nswitches and gates, in order to optimize space usage or enhance performance of user equipment.  A processor may also be implemented as a combination of computing processing units.\nAs used in this application, the terms \"component,\" \"system,\" \"platform,\" \"layer,\" \"selector,\" \"interface,\" and the like are intended to refer to a computer-related entity or an entity related to an operational apparatus with one or more\nspecific functionalities, wherein the entity can be either hardware, a combination of hardware and software, software, or software in execution.  As an example, a component may be, but is not limited to being, a process running on a processor, a\nprocessor, an object, an executable, a thread of execution, a program, and/or a computer.  By way of illustration and not limitation, both an application running on a server and the server can be a component.  One or more components may reside within a\nprocess and/or thread of execution and a component may be localized on one computer and/or distributed between two or more computers.  In addition, these components can execute from various computer readable media having various data structures stored\nthereon.  The components may communicate via local and/or remote processes such as in accordance with a signal having one or more data packets (e.g., data from one component interacting with another component in a local system, distributed system, and/or\nacross a network such as the Internet with other systems via the signal).  As another example, a component can be an apparatus with specific functionality provided by mechanical parts operated by electric or electronic circuitry, which is operated by a\nsoftware or firmware application executed by a processor, wherein the processor can be internal or external to the apparatus and executes at least a part of the software or firmware application.  As yet another example, a component can be an apparatus\nthat provides specific functionality through electronic components without mechanical parts, the electronic components can comprise a processor therein to execute software or firmware that confers at least in part the functionality of the electronic\ncomponents.\nIn addition, the term \"or\" is intended to mean an inclusive \"or\" rather than an exclusive \"or.\" That is, unless specified otherwise, or clear from context, \"X employs A or B\" is intended to mean any of the natural inclusive permutations.  That\nis, if X employs A; X employs B; or X employs both A and B, then \"X employs A or B\" is satisfied under any of the foregoing instances.  Moreover, articles \"a\" and \"an\" as used in the subject specification and annexed drawings should generally be\nconstrued to mean \"one or more\" unless specified otherwise or clear from context to be directed to a singular form.\nFurther, the term \"include\" is intended to be employed as an open or inclusive term, rather than a closed or exclusive term.  The term \"include\" can be substituted with the term \"comprising\" and is to be treated with similar scope, unless\notherwise explicitly used otherwise.  As an example, \"a basket of fruit including an apple\" is to be treated with the same breadth of scope as, \"a basket of fruit comprising an apple.\"\nMoreover, terms like \"user equipment (UE),\" \"mobile station,\" \"mobile,\" subscriber station,\" \"subscriber equipment,\" \"access terminal,\" \"terminal,\" \"handset,\" and similar terminology, refer to a wireless device utilized by a subscriber or user\nof a wireless communication service to receive or convey data, control, voice, video, sound, gaming, or substantially any data-stream or signaling-stream.  The foregoing terms are utilized interchangeably in the subject specification and related\ndrawings.  Likewise, the terms \"access point,\" \"base station,\" \"Node B,\" \"evolved Node B,\" \"eNodeB,\" \"home Node B,\" \"home access point,\" and the like, are utilized interchangeably in the subject application, and refer to a wireless network component or\nappliance that serves and receives data, control, voice, video, sound, gaming, or substantially any data-stream or signaling-stream to and from a set of subscriber stations or provider enabled devices.  Data and signaling streams can comprise packetized\nor frame-based flows.\nAdditionally, the terms \"core-network\", \"core\", \"core carrier network\", \"carrier-side\", or similar terms can refer to components of a telecommunications network that typically provides some or all of aggregation, authentication, call control and\nswitching, charging, service invocation, or gateways.  Aggregation can refer to the highest level of aggregation in a service provider network wherein the next level in the hierarchy under the core nodes is the distribution networks and then the edge\nnetworks.  UEs do not normally connect directly to the core networks of a large service provider but can be routed to the core by way of a switch or radio access network.  Authentication can refer to determinations regarding whether the user requesting a\nservice from the telecom network is authorized to do so within this network or not.  Call control and switching can refer determinations related to the future course of a call stream across carrier equipment based on the call signal processing.  Charging\ncan be related to the collation and processing of charging data generated by various network nodes.  Two common types of charging mechanisms found in present day networks can be prepaid charging and postpaid charging.  Service invocation can occur based\non some explicit action (e.g. call transfer) or implicitly (e.g., call waiting).  It is to be noted that service \"execution\" may or may not be a core network functionality as third party network/nodes may take part in actual service execution.  A gateway\ncan be present in the core network to access other networks.  Gateway functionality can be dependent on the type of the interface with another network.\nFurthermore, the terms \"user,\" \"subscriber,\" \"customer,\" \"consumer,\" \"prosumer,\" \"agent,\" and the like are employed interchangeably throughout the subject specification, unless context warrants particular distinction(s) among the terms.  It\nshould be appreciated that such terms can refer to human entities or automated components (e.g., supported through artificial intelligence, as through a capacity to make inferences based on complex mathematical formalisms), that can provide simulated\nvision, sound recognition and so forth.\nAspects, features, or advantages of the subject matter can be exploited in substantially any, or any, wired, broadcast, wireless telecommunication, radio technology or network, or combinations thereof.  Non-limiting examples of such technologies\nor networks comprise broadcast technologies (e.g., sub-Hertz, extremely low frequency, very low frequency, low frequency, medium frequency, high frequency, very high frequency, ultra-high frequency, super-high frequency, terahertz broadcasts, etc.);\nEthernet; X.25; powerline-type networking, e.g., Powerline audio video Ethernet, etc.; femtocell technology; Wi-Fi; worldwide interoperability for microwave access; enhanced general packet radio service; third generation partnership project, long term\nevolution; third generation partnership project universal mobile telecommunications system; third generation partnership project 2, ultra mobile broadband; high speed packet access; high speed downlink packet access; high speed uplink packet access;\nenhanced data rates for global system for mobile communication evolution radio access network; universal mobile telecommunications system terrestrial radio access network; or long term evolution advanced.\nWhat has been described above includes examples of systems and methods illustrative of the disclosed subject matter.  It is, of course, not possible to describe every combination of components or methods herein.  One of ordinary skill in the art\nmay recognize that many further combinations and permutations of the claimed subject matter are possible.  Furthermore, to the extent that the terms \"includes,\" \"has,\" \"possesses,\" and the like are used in the detailed description, claims, appendices and\ndrawings such terms are intended to be inclusive in a manner similar to the term \"comprising\" as \"comprising\" is interpreted when employed as a transitional word in a claim.", "application_number": "16000865", "abstract": " Providing collection and storage of mobile application data associated\n     with execution of a mobile application is disclosed for a carrier\n     core-network connected component. In an aspect, the carrier core-network\n     connected component can enable storage of some, none, or all mobile\n     application data related to mobile application information traversing a\n     carrier core-network comprising the carrier core-network connected\n     component. Analysis of the mobile application data can provide insight\n     into a characteristic, behavior, etc., associated with the mobile\n     application. The analysis can enable adaptation of mobile application\n     information transport topography, environment, etc., modification of the\n     mobile application data, testing of mobile application performance\n     related to mobile application information traversing the carrier\n     core-network, simulations to test application services and/or mobile\n     applications, etc. A carrier core-network is generally able to capture\n     more mobile application data from more mobile devices, under more\n     operating systems, and provide better access to collected data than\n     conventional technologies.\n", "citations": ["7234132", "7339891", "8744431", "8881111", "8997081", "9053242", "9053435", "9147201", "20060159033", "20080181117", "20090177567", "20110310744", "20130286844", "20130326074", "20140298297", "20140302837", "20150099493", "20150189552", "20150358391"], "related": ["14885279"]}, {"id": "20180310159", "patent_code": "10375558", "patent_name": "Modular emergency communication flow management system", "year": "2019", "inventor_and_country_data": " Inventors: \nKatz; Henry (Brookline, MA), Ferentz; Zvika (Rye Brook, NY), Sternberg; Jeffrey (New York, NY), Horelik; Nicholas Edward (Long Island City, NY), Martin; Michael John (Long Island City, NY)  ", "description": "<BR><BR>BACKGROUND OF THE INVENTION\nA person in an emergency may request help using a mobile communication device such as a cell phone to dial a designated emergency number like 9-1-1 or a direct access phone number for the local emergency service provider (e.g. an emergency\ndispatch center).  This call is assigned to one or more first responders by the emergency service provider.  However, these communications are typically limited to audio calls with narrow functionality since most emergency service providers that receive\nemergency calls currently lack the capacity for more sophisticated communications.\n<BR><BR>SUMMARY OF THE INVENTION\nOne advantage provided by the systems, servers, devices, methods, and media of the instant application is the ability to easily and efficiently make customized emergency flow scripts for handling emergency alerts.  In some embodiments, emergency\nflows scripts are assembled into custom emergency flows using various emergency flow building blocks.  These blocks provide a modular structure that allows authorized users to select specific building blocks as desired to create unique emergency flows\nthat are tailored to the needs of the users.  In some embodiments, the blocks are depicted on a graphic user interface that allows users to drag and drop building blocks to generate emergency flows with ease.\nAnother advantage provided by the systems, servers, devices, methods, and media of the instant application are emergency response management processes that execute customized emergency flow scripts based on the emergency alert.  In some\nembodiments, an emergency alert comprises an emergency flow identifier that is used to determine the emergency flow script to execute in response to receiving the alert.  Thus, a single call management system may provide multiple customized emergency\nflow scripts based on various parameters (e.g. type of device sending the emergency alert, device functionality, type of emergency, etc.).\nAnother advantage provided by the systems, servers, devices, methods, and media of the instant application is the facilitation of emergency response communications.  Many companies and institutions provide devices and services for monitoring\npotential emergencies, such as wearable devices that monitor heart rates or intelligent vehicular systems that can detect when a vehicle has been in an accident.  However, due to the regulatory requirements for emergency communications (e.g. calling\n9-1-1 in the United States), few of the aforementioned services and devices are capable of connecting end users directly to emergency responders.  The management systems provided herein provide these services and devices with the infrastructure necessary\nto connect their end users with emergency responders through a simple and customizable integration.\nOne advantage provided by the systems, servers, devices, methods, and media of the instant application is the ability to easily and efficiently make customized emergency flow scripts for handling emergency alerts.  In some embodiments, emergency\nflows scripts are assembled into custom emergency flows using various emergency flow building blocks.  These blocks provide a modular structure that allows authorized users to select specific building blocks as desired to create unique emergency flows\nthat are tailored to the needs of the users.  In some embodiments, the blocks are depicted on a graphic user interface that allows users to drag and drop building blocks to generate emergency flows with ease.\nAnother advantage provided by the systems, servers, devices, methods, and media of the instant application are emergency response management processes that execute customized emergency flow scripts based on the emergency alert.  In some\nembodiments, an emergency alert comprises an emergency flow identifier that is used to determine the emergency flow script to execute in response to receiving the alert.  Thus, a single call management system may provide multiple customized emergency\nflow scripts based on various parameters (e.g. type of device sending the emergency alert, device functionality, type of emergency, etc.).\nAnother advantage provided by the systems, servers, devices, methods, and media of the instant application is the facilitation of emergency response communications.  Many companies and institutions provide devices and services for monitoring\npotential emergencies, such as wearable devices that monitor heart rates or intelligent vehicular systems that can detect when a vehicle has been in an accident.  However, due to the regulatory requirements for emergency communications (e.g. calling\n9-1-1 in the United States), few of the aforementioned services and devices are capable of connecting end users directly to emergency responders.  The management systems provided herein provide these services and devices with the infrastructure necessary\nto connect their end users with emergency responders through a simple and customizable integration.\nAnother advantage provided by the systems, servers, devices, methods, and media of the instant application is that an emergency alert can be sent with by voice command.  In many situations, a user may find it easier to communicate verbally (as\nopposed to unlocking a smart phone and dialing an emergency number).  For example, the user may be driving and unable to take their hands of the wheel.  The user may be hijacked and not be free to dial for emergency help, but able to shout out a voice\ncommand.\nAnother advantage provided by the systems, servers, devices, methods, and media of the instant application is that sensors can be activated by sound triggers (e.g. gun shots, breaking glass, etc.) and an emergency alert can be sent without user\ninput.  A highly customizable emergency flow may be initiated to deal with the emergency (e.g. contacting the user, contacting emergency contacts, etc.) and make an emergency call, as needed.\nIn one aspect, provided herein are emergency flow management systems comprising: (a) an emergency flow editor and b) an emergency response server.  In some embodiments, the emergency flow editor is configured to perform steps comprising: (i)\nproviding a plurality of emergency flow building blocks available for selection, each of the plurality of emergency flow building blocks comprising instructions to perform an emergency response function; and (ii) providing an interactive environment\nwithin which emergency flow building blocks are visually assembled into a customized emergency flow script that is associated with at least one of an emergency flow identifier and an organizational identifier.  In some embodiments, the emergency response\nserver is configured to perform steps comprising: (i) receiving an emergency alert from an electronic device, the emergency alert comprising at least one of the emergency flow identifier and the organizational identifier; (ii) identifying the emergency\nflow script associated with at least one of the emergency flow identifier and the organizational identifier; and (ii) executing the emergency flow script to establish and manage a communication session associated with the emergency alert.  In some\nembodiments, the emergency flow script comprises an emergency flow building block configured with instructions prompting the emergency response server to call an emergency dispatch center (EDC), add the EDC to the communication session, and generate an\noutput based on a response by the EDC.  In some embodiments, the emergency flow script comprises at least one emergency flow building block having instructions prompting the emergency response server to perform steps comprising: (a) transmitting an\ninteractive message to an account associated with the user; (b) receiving confirmation of receipt of the interactive message; and (c) establishing a communication link between an emergency responder and the account associated with the user in response to\nreceiving the confirmation of receipt.  In some embodiments, the account comprises a phone number, email address, messaging ID, device ID, or social media profile of the user or an emergency contact of the user.  In some embodiments, the emergency flow\nscript comprises at least one emergency flow building block having instructions prompting the emergency response server to establish the communication session as an audio call with at least three participants selected from the group consisting of a user\nof the electronic device, an emergency contact, an emergency service provider personnel, a customer service representative, an organizational representative, and a first responder.  In some embodiments, the emergency flow script comprises at least one\nemergency flow building block having instructions prompting the emergency response server to perform steps comprising: (a) accessing a prioritized list of accounts associated with the user; (b) attempting to connect with an account on the prioritized\nlist in order of priority; and (c) attempting to connect to a next account on the prioritized list in order of priority when a connection with a preceding associated device is unsuccessful.  In some embodiments, adjacent emergency flow building blocks in\nthe emergency flow script are connected such that an output of a preceding building block forms an input for a succeeding building block.  In some embodiments, the emergency flow script comprises a building block configured with instructions for calling\nat least one emergency contact of a user of the electronic device in response to an input that is call timeout or call not answered.  In some embodiments, the emergency alert is triggered by detection of at least one trigger sound or voice command.  In\nsome embodiments, the electronic device is a computer, a tablet, a mobile phone, a panic button, a smartphone, a laptop, a vehicle emergency system, a server, a wearable device, a sensor, an IoT device, a smart device, or a home security device.\nIn one aspect, provided herein are computer-implemented methods for managing emergency flows comprising: (a) providing, by an emergency flow management system (EFMS), a plurality of emergency flow building blocks available for selection, each of\nthe plurality of emergency flow building blocks comprising instructions to perform an emergency response function; (b) providing, by the EFMS, an interactive environment within which emergency flow building blocks are visually assembled into a customized\nemergency flow script that is associated with at least one of an emergency flow identifier and an organizational identifier; (c) receiving, by the EFMS, an emergency alert from an electronic device, the emergency alert comprising at least one of the\nemergency flow identifier and the organizational identifier; (d) identifying, by the EFMS, the emergency flow script associated with at least one of the emergency flow identifier and the organizational identifier; and (e) executing, by the EFMS, the\nemergency flow script to establish and manage a communication session associated with the emergency alert.  In some embodiments, the emergency flow script comprises an emergency flow building block configured with instructions to call an emergency\ndispatch center (EDC), add the EDC to the communication session, and generate an output based on a response by the EDC.  In some embodiments, the emergency flow script comprises at least one emergency flow building block having instructions to perform\nsteps comprising: (a) transmitting an interactive message to an account associated with the user; (b) receiving confirmation of receipt of the interactive message; and (c) establishing a communication link between an emergency responder and the account\nassociated with the user in response to receiving the confirmation of receipt.  In some embodiments, the account comprises a phone number, email address, messaging ID, device ID, or social media profile of the user or an emergency contact of the user. \nIn some embodiments, the emergency flow script comprises at least one emergency flow building block having instructions to establish the communication session with at least three participants selected from the group consisting of a user of the electronic\ndevice, an associated contact of the user, an emergency contact, an emergency service provider personnel, a customer service representative, an organizational representative, and a first responder.  In some embodiments, the emergency flow script\ncomprises at least one emergency flow building block having instructions to perform steps comprising: (a) accessing a prioritized list of accounts associated with the user; (b) attempting to connect with an account on the prioritized list in order of\npriority; and (c) attempting to connect to a next account on the prioritized list in order of priority when a connection with a preceding associated device is unsuccessful.  In some embodiments, adjacent emergency flow building blocks in the emergency\nflow script are connected such that an output of a preceding building block forms an input for a succeeding building block.  In some embodiments, the emergency flow script comprises a building block configured with instructions for calling at least one\nemergency contact of a user of the electronic device in response to an input that is call timeout or call not answered.  In some embodiments, the emergency alert is triggered by detection of at least one trigger sound or voice command.  In some\nembodiments, the electronic device is a computer, a tablet, a mobile phone, a panic button, a smartphone, a laptop, a vehicle emergency system, a server, a wearable device, a sensor, an IoT device, a smart device, or a home security device.\nIn one aspect, provided herein are non-transitory computer readable storage media encoded with at least one computer program including instructions executable by at least one processor to perform steps comprising: (a) providing a plurality of\nemergency flow building blocks available for selection, each of the plurality of emergency flow building blocks comprising instructions to perform an emergency response function; (b) providing an interactive environment within which emergency flow\nbuilding blocks are visually assembled into a customized emergency flow script that is associated with at least one of an emergency flow identifier and an organizational identifier; (c) receiving an emergency alert from an electronic device, the\nemergency alert comprising at least one of the emergency flow identifier and the organizational identifier; (d) identifying the emergency flow script associated with at least one of the emergency flow identifier and the organizational identifier; and (e)\nexecuting the emergency flow script to establish and manage a communication session associated with the emergency alert.  In some embodiments, the emergency flow script comprises an emergency flow building block configured with instructions to call an\nemergency dispatch center (EDC), add the EDC to the communication session, and generate an output based on a response by the EDC.  In some embodiments, the emergency flow script comprises at least one emergency flow building block having instructions to\nperform steps comprising: (a) transmitting an interactive message to an account associated with the user; (b) receiving confirmation of receipt of the interactive message; and (c) establishing a communication link between an emergency responder and the\naccount associated with the user in response to receiving the confirmation of receipt.  In some embodiments, the account comprises a phone number, email address, messaging ID, device ID, or social media profile of the user or an emergency contact of the\nuser.  In some embodiments, the emergency flow script comprises at least one emergency flow building block having instructions to establish a communication session with at least three participants selected from the group consisting of a user of the\nelectronic device, an associated contact of the user, an emergency contact, an emergency service provider personnel, a customer service representative, an organizational representative, and a first responder.  In some embodiments, the emergency flow\nscript comprises at least one emergency flow building block having instructions to perform steps comprising: (a) accessing a prioritized list of accounts associated with the user; (b) attempting to connect with an account on the prioritized list in order\nof priority; and (c) attempting to connect to a next account on the prioritized list in order of priority when a connection with a preceding associated device is unsuccessful.  In some embodiments, adjacent emergency flow building blocks in the emergency\nflow script are connected such that an output of a preceding building block forms an input for a succeeding building block.  In some embodiments, emergency flow script comprises a building block configured with instructions for calling at least one\nemergency contact of a user of the electronic device in response to an input that is call timeout or call not answered.  In some embodiments, the emergency alert is triggered by detection of at least one trigger sound or voice command.  In some\nembodiments, the electronic device is a computer, a tablet, a mobile phone, a panic button, a smartphone, a laptop, a vehicle emergency system, a server, a wearable device, a sensor, an IoT device, a smart device, or a home security device.\nIn one aspect, provided herein are systems for emergency communications comprising: a) a triggering device of the user transmitting an emergency alert when an emergency is triggered; and b) at least one server providing an emergency management\nsystem server application, wherein the application: i) maintains at least one database comprising a list of at least one associated device of the triggering device; ii) receives the emergency alert from the triggering device; iii) establishes a\nconnection with the triggering device upon determining that the triggering device is able to host a connection with the server; and iv) connects to at least one associated device in the list; wherein the at least one associated device in the list is\nauthorized by the user to be connected to the at least one dispatch center for requesting emergency assistance.  In some embodiments, the server application connects to at least two associated devices concurrently.  In some embodiments, the server\napplication connects with the triggering device before connecting with the at least one associated device.  In some embodiments, the list is a prioritized list having a plurality of associated devices.  In some embodiments, the server application\nconnects to a next associated device in the prioritized list in order of priority when a connection with a preceding associated device is unsuccessful.  In some embodiments, the server application looks up an associated device by querying the at least\none database using a telephone number associated with the triggering device.  In some embodiments, the server application looks up an associated device by querying the at least one database using one of an email address, physical address, x-y coordinate,\nBSSID, SSID, and MAC address of the associated device.  In some embodiments, the user is connected to a dispatch center by an audio call.  In some embodiments, the server application establishes an audio call with the triggering device upon determining\nthe triggering device is able to host an audio call.  In some embodiments, the user is presented with an option to connect to or disconnect from a dispatch center.  In some embodiments, the user receives a notification when the emergency alert has been\ntriggered and the server application was unable to connect to a user.  In some embodiments, the emergency alert is triggered autonomously based on sensor data.  In some embodiments, the emergency alert is triggered when sensor values exceed a threshold. \nIn some embodiments, the emergency alert is triggered by detection of at least one trigger sound.  In some embodiments, the at least one trigger sound comprises at least one of a gunshot, explosion, breaking glass, person falling, baby crying, police\nsiren, ambulance siren, fire truck siren, outdoor warning siren, and a crash.  In some embodiments, the emergency alert is triggered by detection of at least one trigger sound that exceeds a decibel threshold.  In some embodiments, the emergency is\ntriggered by user interaction with the triggering device.  In some embodiments, the user interaction comprises at least one of pressing a soft or hard button, a gesture, or a voice command.  In some embodiments, the emergency is triggered by the user\ngiving a voice command, wherein the triggering device recognizes the voice command to detect one or more trigger phrases.  In some embodiments, the triggering device confirms user identity using voice recognition.\nIn another aspect, provided herein are emergency management systems comprising at least one processor, a memory, a network element, and a computer program including instructions executable by the at least one processor to create a server\nsoftware application for connecting a user to at least one dispatch center for facilitating emergency assistance, the application comprising: a) a software module maintaining at least one database comprising a list of at least one associated device of\nthe triggering device; b) a software module receiving an emergency alert from the triggering device; c) a software module establishing a connection with the triggering device upon determining that the triggering device is able to host a connection with\nthe emergency management system; and d) a software module connecting to at least one associated device in the list; wherein the at least one associated device in the list is authorized by the user to be connected to the at least one dispatch center for\nrequesting emergency assistance.\nIn another aspect, provided herein are non-transitory computer readable storage media encoded with a computer program including instructions executable by at least one processor to create a server application for connecting a user to at least\none dispatch center for facilitating emergency assistance, the application comprising: a) a software module receiving an emergency alert from the triggering device; b) a software module maintaining at least one database comprising a list of at least one\nassociated device of the triggering device; c) a software module establishing a connection with the triggering device upon determining that the triggering device is able to host a connection with the EMS; and d) a software module connecting to at least\none associated device in the list; wherein the at least one associated device in the list is authorized by the user to be connected to the at least one dispatch center for requesting emergency assistance.  In some embodiments, the server application\nconnects to at least two associated devices concurrently.\nIn another aspect, provided herein are methods for an emergency management system to connect to a user for providing emergency assistance comprising: a) maintaining, by the emergency management system, at least one database for lookup of\ntriggering devices and a list of at least one associated device for each triggering device; b) receiving, by the emergency management system, an emergency alert from a triggering device; c) determining, by the emergency management system, whether the\ntriggering device is able to host a connection with the user; and d) connecting, by the emergency management system, to the triggering device if the triggering device is able to host the connection; e) connecting, by the emergency management system, to\nat least one associated device; and f) communicating, by the emergency management system, with the user via the at least one associated device for requesting emergency assistance from at least one dispatch center.\nIn another aspect, provided herein are methods for providing the location of an emergency, by an emergency management system, comprising: a) receiving, by the emergency management system, an emergency alert from a triggering device, wherein the\nemergency alert does not include location data; b) searching, by the emergency management system, one or more databases for the location data for the triggering device; c) requesting, by the emergency management system, current location data from the\ntriggering device or at least one associated device; and d) making, by the emergency management system, the location data available to one or more dispatch centers for providing emergency assistance.  In some embodiments, the location data is a physical\naddress or x-y coordinates.  In some embodiments, the method further comprises calculating an accuracy for the location data.  In some embodiments, the accuracy of the location data is made available to the one or more dispatch centers.  In some\nembodiments, the method further comprises converting the location data into converted location data, wherein the converted location data is in compatible format for the one or more dispatch centers.  In some embodiments, an accuracy of the converted\nlocation data is evaluated.  In some embodiments, the emergency management system queries one or more databases for location data for the triggering device.  In some embodiments, the emergency management system queries one or more databases for location\ndata for one or more users associated with the triggering device.  In some embodiments, the triggering device is installed within a building and the location data associated with the triggering device is saved in one or more databases in the emergency\nmanagement system.  In some embodiments, the triggering device is an IoT device.  In some embodiments, the triggering device is a mobile device or hosted within a mobile vehicle and the emergency management system obtains the current location data from\nthe triggering device.  In some embodiments, the triggering device is a vehicle console or a vehicle computer.  In some embodiments, the triggering device is assigned a token, wherein the emergency alert from the triggering device is tagged with the\ntoken and the location of the triggering device is provisioned with the user's phone number.\nIn another aspect, provided herein are emergency management systems comprising at least one processor, a memory, a network element, and a computer program including instructions executable by the at least one processor to create a server\nsoftware application for connecting a user to at least one dispatch center for facilitating emergency assistance, the application comprising: a) a software module receiving an emergency alert from a triggering device, wherein the emergency alert does not\ninclude location data; and b) a software module: i) searching one or more databases for the location data for the triggering device ii) requesting current location data from the triggering device or at least one associated device; and iii) making the\nlocation data available to one or more dispatch centers for providing emergency assistance.  In some embodiments, the location data is a physical address or x-y coordinates.  In some embodiments, the emergency management system calculates an accuracy for\nthe location data.\nIn another aspect, provided herein are non-transitory computer readable storage media encoded with a computer program including instructions executable by at least one processor to create a server application for connecting a user to at least\none dispatch center for facilitating emergency assistance, the application comprising: a) a software module receiving an emergency alert from a triggering device, wherein the emergency alert does not include location data; and b) a software module: i)\nsearching one or more databases for the location data for the triggering device ii) requesting current location data from the triggering device or at least one associated device; and iii) making the location data available to one or more dispatch centers\nfor providing emergency assistance.\nIn another aspect, provided herein are triggering devices comprising at least one processor, a memory, a network element, and a computer program including instructions executable by the at least one processor to create an emergency trigger\napplication comprising: a) a software module detecting an emergency; b) a software module triggering an emergency alert upon detecting an emergency; c) a software module transmitting the emergency alert to an emergency management system when the\nemergency alert is triggered, wherein the emergency alert comprises information for at least one associated device capable of communicating with the emergency management system; and d) a software module providing data from at least one of the triggering\ndevice and the at least one associated device to the emergency management system, wherein the data comprises to host an audio call.  In some embodiments, a user of the triggering device is presented with an option to connect to or disconnect from a\ndispatch center.\nIn another aspect, systems for connecting a user to one or more dispatch centers for the purpose of providing emergency assistance are disclosed comprising: (a) a triggering device, wherein the triggering device transmits an emergency alert when\nan emergency is triggered; (b) an emergency management system for receiving the emergency alert from the triggering device, wherein the emergency management system: (i) maintains one or more databases for lookup of the triggering device for a list of\nassociated devices, (ii) evaluates whether the triggering device has a capability to host a connection with a user and attempts to connect to the triggering device if the triggering device has the capability, and (iii) the emergency managements system\nattempts to connect to one or more associated devices in the list; and (c) one or more associated devices where a user may agree to be connected to the dispatch centers for requesting emergency assistance.  In some embodiments, the emergency management\nsystem connects to each associated device simultaneously and connects the user who agrees to be connected first.  In some embodiments, the list is a prioritized list with more than one associated devices and the emergency management system connects to\nthe next associated device only when the connection with the preceding associated device has been unsuccessful.  In some embodiments, associated devices can be looked up in one or more databases using telephone numbers.  In some embodiments, associated\ndevices can be looked up in one or more databases using email addresses, physical address, x-y coordinates, BSSID, SSID or MAC address.  In some embodiments, the user is connected to a dispatch center by an audio call.  In some embodiments, the emergency\nmanagement system evaluates whether the triggering device has a capability to host an audio call and calls the triggering device if the triggering device has the capability.  In some embodiments, the user may choose to connect to the dispatch center or\ndisconnect.  In some embodiments, the user may get notifications when the emergency alert has been triggered and the emergency management system was unable to connect to a user.  In some embodiments, the emergency is triggered autonomously based on\nsensor data.  In some embodiments, the emergency is triggered by detection of trigger sounds.  In some embodiments, the trigger sounds comprises sounds of gunshots, explosions, breaking glass, person falling, baby crying, etc. In some embodiments, the\nemergency is triggered by user interaction with the triggering device.\nIn another aspect, disclosed herein are methods for an emergency management system to connect to a user for providing emergency assistance comprising: (a) receiving the emergency alert from a triggering device; wherein the emergency management\nsystem maintains one or more databases for lookup of the triggering device for a list of associated devices; (b) evaluating whether the triggering device has a capability to host a connection with a user and attempts to connect to the triggering device\nif the triggering device has the capability; (c) attempting to connect to one or more associated devices in the list; and (d) communicating with the user on one or more associated devices for requesting emergency assistance from one or more dispatch\ncenters.\nIn another aspect, disclosed herein are systems, methods, devices and media for providing the location of an emergency, by an emergency management system, comprising: (a) receiving an emergency alert from a triggering device, wherein the\nemergency alert does not include location data; (b) searching one or more databases for the location data for the triggering device; (c) request current location data from the triggering device or associated devices; and (d) making the location data\navailable to one or more dispatch centers for providing emergency assistance.  In some embodiments, the location data is a physical address or x-y coordinates.  In some embodiments, the method comprising the step of calculating an accuracy for the\nlocation data.  In some embodiments, the accuracy of the location data is made available to one or more dispatch centers.\n<BR><BR>INCORPORATION BY REFERENCE\nAll publications, patents, and patent applications mentioned in this specification are herein incorporated by reference to the same extent as if each individual publication, patent, or patent application was specifically and individually\nindicated to be incorporated by reference. <BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nThe novel features of the invention are set forth with particularity in the appended claims.  A better understanding of the features and advantages of the present invention will be obtained by reference to the following detailed description that\nsets forth illustrative embodiments, in which the principles of the invention are utilized, and the accompanying drawings of which:\nFIG. 1A depicts an exemplary embodiment of an emergency flow management system (EFMS).\nFIG. 1B, depicts exemplary embodiments of (i) an electronic device (e.g. a communication device) and (ii) an emergency management system.\nFIG. 1C depicts exemplary embodiments of (iii) a triggering device and (iv) an emergency flow program.\nFIG. 1D depicts exemplary embodiments of (v) a public safety answering point (PSAP) system and (vi) PSAP software.\nFIG. 2 depicts a first exemplary embodiment of a system for managing emergency flows;\nFIG. 3 depicts a second exemplary embodiment of a system for managing emergency flows;\nFIG. 4 depicts a first exemplary embodiment of a system for developing and deploying emergency flows;\nFIG. 5 depicts a second exemplary embodiment of a system for developing and deploying emergency flows;\nFIG. 6 shows a screenshot of an exemplary embodiment of an application or application interface for developing and deploying emergency flows;\nFIG. 7 illustrates a first exemplary embodiment of an emergency flow;\nFIG. 8 illustrates a second exemplary embodiment of an emergency flow;\nFIG. 9 illustrates a third exemplary embodiment of an emergency flow;\nFIG. 10 illustrates a fourth exemplary embodiment of an emergency flow;\nFIG. 11 illustrates a fifth exemplary embodiment of an emergency flow;\nFIG. 12 depicts an exemplary embodiment of (i) the triggering device and (ii) the emergency management system;\nFIG. 13 depicts an exemplary embodiment of a system for connecting with other devices after an emergency alert has been triggered;\nFIG. 14 illustrates an exemplary method for an emergency management system to communicate with dispatchers for assistance after an emergency alert has been triggered;\nFIG. 15 illustrates an exemplary method for an emergency management system to call dispatchers for assistance after an emergency has been triggered;\nFIG. 16 depicts embodiments of a triggering device within a smart home, emergency management system, and computer programs;\nFIG. 17 illustrates an exemplary method for providing a location for the emergency;\nFIG. 18 depicts exemplary embodiments of a connected vehicle, emergency management system, and computer programs;\nFIG. 19 depicts an exemplary embodiment of a voice-activated emergency alert from a user in a smart home;\nFIG. 20 depicts a system for requesting emergency assistance by voice activation;\nFIG. 21A depicts an exemplary implementation of an emergency call initiated by voice activation;\nFIG. 21B depicts another exemplary implementation of an emergency call initiated by voice activation;\nFIG. 22 illustrates a detailed exemplary embodiment of an emergency display;\nFIG. 23A depicts an exemplary embodiment of a dashboard for testing emergency flows; and\nFIG. 23B depicts another exemplary embodiment of a dashboard for testing emergency flows.\n<BR><BR>DETAILED DESCRIPTION OF THE INVENTION\nIn various embodiments, disclosed herein are systems and methods for emergency flow management.  In one aspect, disclosed herein is an emergency flow management system comprising: a) an emergency flow configuration editor application comprising:\ni) a software module providing a plurality of emergency flow building blocks available for selection, each of the plurality of emergency flow building blocks comprising instructions to perform an emergency response function; ii) a software module\nproviding an interactive space within which emergency flow building blocks are visually assembled into a customized emergency flow script that is associated with an emergency flow identifier; b) an emergency response server application comprising: i) a\nsoftware module receiving an emergency alert from an electronic device, the emergency alert comprising the emergency flow identifier; ii) a software module identifying the emergency flow script associated with the emergency flow identifier; and iii) a\nsoftware module executing the emergency flow script to establish and manage a communication session associated with the emergency alert.\nIn another aspect, disclosed herein is a system for emergency flow development and deployment, the system comprising: a) an electronic device associated with a user; b) a server communicatively coupled to the electronic device and configured to\nprovide an emergency flow management system; and c) an emergency dispatch center; wherein the emergency flow management system is configured to: a) receive, from the electronic device, an emergency alert, the emergency alert including an emergency\nindication, sensor data from at least one sensor coupled to the electronic device, location information, and an emergency flow identification number; and b) in response to the emergency alert, execute an emergency flow script associated with the\nemergency flow identification number; wherein the emergency flow script instructs the emergency flow management system to: a) transmit an emergency response message to the electronic device; b) receive confirmation of receipt of the emergency response\nmessage; and c) in response to receiving confirmation of receipt of the emergency response message: d) establish a communicative link between the emergency dispatch center and the electronic device; and e) transmit the sensor data to the emergency\ndispatch center.\nIn another aspect, disclosed herein is a system for emergency call flow development and deployment, the system comprising: a) an electronic device associated with a user; b) a server communicatively coupled to the electronic device and\nconfigured to provide an emergency flow management system; and c) an emergency responder; wherein the emergency flow management system is configured to: a) receive, from the electronic device, an emergency alert, the emergency alert including an\nemergency indication and an emergency flow identification number; and b) in response to receiving the emergency alert, execute an emergency flow script associated with the emergency flow identification number; wherein the emergency flow script instructs\nthe emergency flow management system to: a) transmit an emergency response message to the electronic device; b) receive confirmation of the emergency response message; and c) in response to receiving confirmation of receipt of the emergency response\nmessage, establish a communicative link between the emergency responder and the electronic device.\nIn another aspect, disclosed herein is a method for emergency flow development and deployment, the method comprising: a) receiving, from an electronic device associated with a user, an emergency alert, the emergency alert including an emergency\nindication, location information, and an emergency flow identification number; b) in response to receiving the emergency alert, executing an emergency flow script associated with the emergency flow identification number; wherein the emergency flow script\nincludes instructions for: a) transmitting an emergency response message to the electronic device; b) receiving confirmation of receipt of the emergency response message; and c) in response to receiving confirmation of receipt of the emergency response\nmessage, establishing a communicative link between the emergency responder and the electronic device.\nIn certain embodiments, disclosed herein are systems for emergency communications comprising: a) a triggering device of the user transmitting an emergency alert when an emergency is triggered; and b) at least one server providing an emergency\nmanagement system server application, wherein the application: i) maintains at least one database comprising a list of at least one associated device of the triggering device; ii) receives the emergency alert from the triggering device; iii) establishes\na connection with the triggering device upon determining that the triggering device is able to host a connection with the server; and iv) connects to at least one associated device in the list; wherein the at least one associated device in the list is\nauthorized by the user to be connected to the at least one dispatch center for requesting emergency assistance.  In certain embodiments, disclosed herein are triggering devices comprising at least one processor, a memory, a network element, and a\ncomputer program including instructions executable by the at least one processor to create an emergency assistance program.\nIn certain embodiments, disclosed herein are emergency management systems comprising at least one processor, a memory, a network element, and a computer program including instructions executable by the at least one processor to create a server\nsoftware application for connecting a user to at least one dispatch center for facilitating emergency assistance, the application comprising: a) a software module maintaining at least one database comprising a list of at least one associated device of\nthe triggering device; b) a software module receiving an emergency alert from the triggering device; c) a software module establishing a connection with the triggering device upon determining that the triggering device is able to host a connection with\nthe emergency management system; and d) a software module connecting to at least one associated device in the list; wherein the at least one associated device in the list is authorized by the user to be connected to the at least one dispatch center for\nrequesting emergency assistance.\nIn certain embodiments, disclosed herein are non-transitory computer readable storage media encoded with a computer program including instructions executable by at least one processor to create a server application for connecting a user to at\nleast one dispatch center for facilitating emergency assistance, the application comprising: a) a software module receiving an emergency alert from the triggering device; b) a software module maintaining at least one database comprising a list of at\nleast one associated device of the triggering device; c) a software module establishing a connection with the triggering device upon determining that the triggering device is able to host a connection with the EMS; and d) a software module connecting to\nat least one associated device in the list; wherein the at least one associated device in the list is authorized by the user to be connected to the at least one dispatch center for requesting emergency assistance.\nIn certain embodiments, disclosed herein are methods for an emergency management system to connect to a user for providing emergency assistance comprising: a) maintaining, by the emergency management system, at least one database for lookup of\ntriggering devices and a list of at least one associated device for each triggering device; b) receiving, by the emergency management system, an emergency alert from a triggering device; c) determining, by the emergency management system, whether the\ntriggering device is able to host a connection with the user; and d) connecting, by the emergency management system, to the triggering device if the triggering device is able to host the connection; e) connecting, by the emergency management system, to\nat least one associated device; and f) communicating, by the emergency management system, with the user via the at least one associated device for requesting emergency assistance from at least one dispatch center.\nIn certain embodiments, disclosed herein are methods for providing the location of an emergency, by an emergency management system, comprising: a) receiving, by the emergency management system, an emergency alert from a triggering device,\nwherein the emergency alert does not include location data; b) searching, by the emergency management system, one or more databases for the location data for the triggering device; c) requesting, by the emergency management system, current location data\nfrom the triggering device or at least one associated device; and d) making, by the emergency management system, the location data available to one or more dispatch centers for providing emergency assistance.\nIn certain embodiments, disclosed herein are emergency management systems comprising at least one processor, a memory, a network element, and a computer program including instructions executable by the at least one processor to create a server\nsoftware application for connecting a user to at least one dispatch center for facilitating emergency assistance, the application comprising: a) a software module receiving an emergency alert from a triggering device, wherein the emergency alert does not\ninclude location data; and b) a software module: i) searching one or more databases for the location data for the triggering device ii) requesting current location data from the triggering device or at least one associated device; and iii) making the\nlocation data available to one or more dispatch centers for providing emergency assistance.\nIn certain embodiments, disclosed herein are non-transitory computer readable storage media encoded with a computer program including instructions executable by at least one processor to create a server application for connecting a user to at\nleast one dispatch center for facilitating emergency assistance, the application comprising: a) a software module receiving an emergency alert from a triggering device, wherein the emergency alert does not include location data; and b) a software module:\ni) searching one or more databases for the location data for the triggering device ii) requesting current location data from the triggering device or at least one associated device; and iii) making the location data available to one or more dispatch\ncenters for providing emergency assistance.\nIn certain embodiments, disclosed herein are methods of transmitting an emergency alert and providing data to an emergency management system, comprising: a) detecting, by a triggering device, an emergency; b) triggering, by the triggering\ndevice, an emergency alert upon detecting the emergency; c) transmitting, by the triggering device, the emergency alert to an emergency management system when the emergency alert is triggered, wherein the emergency alert comprises information for at\nleast one associated device capable of communicating with the emergency management system; and d) providing, by the triggering device, data from at least one of the triggering device and the at least one associated device to the emergency management\nsystem, wherein the data comprises location information.\nIn certain embodiments, disclosed herein are triggering devices comprising at least one processor, a memory, a network element, and a computer program including instructions executable by the at least one processor to create an emergency trigger\napplication comprising: a) a software module detecting an emergency; b) a software module triggering an emergency alert upon detecting an emergency; c) a software module transmitting the emergency alert to an emergency management system when the\nemergency alert is triggered, wherein the emergency alert comprises information for at least one associated device capable of communicating with the emergency management system; and d) a software module providing data from at least one of the triggering\ndevice and the at least one associated device to the emergency management system, wherein the data comprises location information.\nIn certain embodiments, disclosed herein are non-transitory computer readable storage media encoded with a computer program including instructions executable by at least one processor to create a software application for a triggering device to\ntransmit an emergency alert and providing data to an emergency management system, the application comprising: a) a software module detecting an emergency; b) a software module triggering an emergency alert upon detecting an emergency; c) a software\nmodule transmitting the emergency alert to an emergency management system when the emergency alert is triggered, wherein the emergency alert comprises information for at least one associated device capable of communicating with the emergency management\nsystem; and d) a software module providing data from at least one of the triggering device and the at least one associated device to the emergency management system, wherein the data comprises location information.\nIn certain embodiments, disclosed herein are method for an emergency management system to connect to a user for providing emergency assistance comprising: (a) receiving the emergency alert from a triggering device; wherein the emergency\nmanagement system maintains one or more databases for lookup of the triggering device for a list of associated devices; (b) evaluating whether the triggering device has a capability to host a connection with a user and attempts to connect to the\ntriggering device if the triggering device has the capability; (c) attempting to connect to one or more associated devices in the list; and (d) communicating with the user on one or more associated devices for requesting emergency assistance from one or\nmore dispatch centers.\nIn certain embodiments, disclosed herein are systems for connecting a user to one or more dispatch centers for the purpose of providing emergency assistance comprising: (a) a triggering device, wherein the triggering device transmits an\nemergency alert when an emergency is triggered; (b) an emergency management system for receiving the emergency alert from the triggering device, wherein the emergency management system: (i) maintains one or more databases for lookup of the triggering\ndevice for a list of associated devices, (ii) evaluates whether the triggering device has a capability to host a connection with a user and attempts to connect to the triggering device if the triggering device has the capability, and (iii) the emergency\nmanagements system attempts to connect to one or more associated devices in the list; and (c) one or more associated devices where a user may agree to be connected to the dispatch centers for requesting emergency assistance.\nIn certain embodiments, disclosed herein are non-transitory computer-readable storage media encoded with a computer program including instructions executable by a processor to create an emergency application comprising: (a) receiving the\nemergency alert from a triggering device; wherein the emergency management system maintains one or more databases for lookup of the triggering device for a list of associated devices; (b) evaluating whether the triggering device has a capability to host\na connection with a user and attempts to connect to the triggering device if the triggering device has the capability; (c) attempting to connect to one or more associated devices in the list; and (d) communicating with the user on one or more associated\ndevices for requesting emergency assistance from one or more dispatch centers.\nIn certain embodiments, disclosed herein are non-transitory computer-readable storage media encoded with a computer program including instructions executable by a processor to create a device client application for triggering and sending an\nemergency alert.  In certain embodiments, voice or speech and/or sounds may be detected to trigger the emergency alert.  In certain embodiments, disclosed herein are emergency management systems comprising at least one processor, a memory, a network\nelement, and a computer program including instructions executable by the at least one processor to create server software application for managing emergency communications.\nIn certain embodiments, disclosed herein are methods for providing the location of an emergency, by an emergency management system, comprising: (a) receiving an emergency alert from a triggering device, wherein the emergency alert does not\ninclude location data; (b) searching one or more databases for the location data for the triggering device; (c) request current location data from the triggering device or associated devices; and (d) making the location data available to one or more\ndispatch centers for providing emergency assistance.\n<BR><BR>Certain Terminologies\nUnless otherwise defined, all technical terms used herein have the same meaning as commonly understood by one of ordinary skill in the art to which this invention belongs.  As used in this specification and the appended claims, the singular\nforms \"a,\" \"an,\" and \"the\" include plural references unless the context clearly dictates otherwise.  Any reference to \"or\" herein is intended to encompass \"and/or\" unless otherwise stated.\nAs used herein, a \"device\" is a digital processing device designed with one or more functionality.  A \"triggering device\" refers to a communication device configured to send an emergency alert or communication (e.g. a 911 call) in response to a\ntrigger (e.g. a voice command to call 911 or sensor data indicative of an emergency).  In some embodiments, the triggering device comprises a communication component, which allow the sending and receiving of information over a wireless channel, a wired\nchannel, or any combination thereof (e.g. sending/receiving information over the Internet).  Exemplary triggering devices include a mobile phone (e.g. a smartphone), a laptop, a desktop, a tablet, a radio (e.g. a two-way radio), and a vehicular\ncommunication system.  In some embodiments, a triggering device is part of a car security system (e.g. OnStar.RTM.), a home security system, or a home control system (e.g. a networked control system for providing network controlled and/or smart\ntemperature control such as a Wi-Fi smart thermostat, lighting, entertainment, and/or door control, such as Nest.RTM.).  In some embodiments, the triggering device is a smart speaker.  In some embodiments, the triggering device responds to voice\ncommand(s) such as a spoken command to call emergency services (e.g. call 911).  In some embodiments, the triggering device comprises a sensor for sensing environmental or health/physiological indicators.  In some embodiments, the sensor comprises a\nsensing component and a communication component.  In some embodiments, the triggering device is a sensor in a sensor network or a device that controls a sensor network.\nIn some embodiments, a triggering device is a wearable device (e.g. a communication device worn by a user).  In some embodiments, a triggering device (e.g. a wearable device) comprises one or more sensors.  As used herein, a \"mobile wireless\ndevice\" refers to a device that is portable and communicates wirelessly.  In some embodiments, a user wears or carries the mobile wireless device on the user's person or in the user's vehicle.  Exemplary mobile wireless devices include mobile or cellular\nphones, wearable devices (e.g. Smart watch, fitness tracker, wearable sensor, smart glasses, etc.).\nAs used herein, Internet of Things (IoT) refers to a network of physical devices, buildings, vehicles, and other objects that feature an IP address for internet network connectivity for exchanging data.  In many cases, IoT devices are embedded\nwith electronics, software, one or more sensors, and network connectivity.  As used herein, a IoT device can be a device that includes sensing and/or control functionality as well as a WiFi.TM.  transceiver radio or interface, a Bluetooth.TM. \ntransceiver radio or interface, a Zigbee.TM.  transceiver radio or interface, an Ultra-Wideband (UWB) transceiver radio or interface, a WiFi-Direct transceiver radio or interface, a Bluetooth.TM.  Low Energy (BLE) transceiver radio or interface, and/or\nany other wireless network transceiver radio or interface that allows the IoT device to communicate with a wide area network and with one or more other devices.  In some embodiments, IoT devices feature an IP address for internet connectivity.  In\naddition to an IP address, the IoT device can be associated with a MAC address or an SSID.  In some embodiments, IoT devices connect with other devices through Wi-Fi, Blue-Tooth.RTM., a Zigbee.TM.  transceiver radio or interface, an Ultra-Wideband (UWB)\ntransceiver radio or interface, a WiFi-Direct transceiver radio or interface, a Bluetooth.TM.  Low Energy (BLE) transceiver radio or interface, or other technologies which allow for transfer of data.  In some embodiments, IoT devices form a mesh network\nallowing information to be efficiently routed between the devices.  Examples of IoT devices include a home thermostat, intelligent home monitors, baby monitors, smoke or carbon monoxide detectors, home security camera systems, and other network-enabled\ndevices.  In some embodiments, a triggering device is an IoT device (e.g. a Nest.RTM.  device).\nAs an example, an exemplary vehicular on-board console is triggered upon detection of an emergency.  In some embodiments, the console contains a built-in microphone and uses the car speakers.  In some embodiments, the console is connected to a\nDevice Communication & Interface Module (DCIM), which uses an antenna to connect to the cellular network.  When the car is in a crash, sensors can transmit information to the Sensing Diagnostic Module (SDM).  In some embodiments, the SDM comprises an\naccelerometer, which measures the severity of the event based on gravitational force.  The SDM sends this information to the DCIM, which uses the cellular antenna to send the emergency alert.\nAs used herein, an \"account\" refers to contact information for a user, including emergency contacts of the user.  In some embodiments, the account is registered by the user to include a list of contact information for the user such as, for\nexample, a list of associated devices.  Examples of contact information on an account include phone number, email address, home address, work address, emergency contacts (e.g. name, phone number, email, etc), and associated devices (e.g. other\ncommunication devices of the user aside from the device or triggering device sending an emergency alert).  In some embodiments, the account includes contact information for organizational representatives.  For example, in some cases, a social media\napplication installed on the user's electronic device is used to send an emergency alert.  The communication session can be established with the user/user device, emergency service provider personnel, and an organizational representative for the social\nmedia entity.  This scenario can occur when analysis of the user's social media activity indicates a possible emergency situation such as, for example, a suicide attempt by the user.  In response, the social media application on the user device sends an\nemergency alert to an emergency management system.  Since the user did not choose to request help, a representative of the social media entity is connected to the 3-way communication session to help explain the emergency situation to the emergency\nservice provider.\nAs used herein, an \"associated device\" refers to a communication device that is associated with the triggering device.  As an example, a user uses several communication devices such as a mobile phone, a wearable, a home security system, a car\ncomputer.  In this example, the user has registered these devices with his or her account and linked these devices with a user name, user number(s), email address(es), home or other physical address(es).  In some embodiments, associated devices include\ncommunication device(s) of a second user who is associated with user (e.g. a husband and wife, a father and son, a patient and doctor, friends, work colleagues, or other individuals).  In some cases, the user has added the second user as an emergency\ncontact or a member of a group.  In some cases, user agreed to share location and other data with the second user.  In some embodiments, the second user is someone who is frequently contacted by the user and the communication device identifies the second\nuser from the \"Recently called\" or \"Frequently called\" list.  In some embodiments, the associated devices are devices that are proximal or near-by to the triggering device such as obtained through a Wi-Fi scan.  In some embodiments, an associated device\nis proximal to the triggering device when the location of the associated device is within 1, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 60, 70, 80, 90, 100, 200, 300, 400, or 500 meters of the location of the triggering device.\nAs used herein, the \"list of associated devices\" refers to a list of communication devices that are associated with the user or the triggering device (e.g. a second resident in a smart home).  In some embodiments, the list of associated devices\nis listed by user name, phone number, email address, physical address, coordinates etc. In some embodiments, the device entry in the list includes phone number, email address, physical address, coordinates, BSSID, SSID or MAC address, or any combination\nthereof.  In some embodiments, the list is user defined or generated by the device or the EMS.\nAs used herein, a \"request for assistance\" refers to a request or message sent to a recipient asking for help.  In some embodiments, a request for assistance is an emergency request for assistance (e.g. the request is associated with an\nemergency situation) such as, for example, an emergency alert.  In some embodiments, an emergency alert comprises a request for assistance.  In some embodiments, a request for assistance is associated with an emergency situation.  In some embodiments, a\nrequest for assistance comprises an emergency indication.  In some embodiments, an emergency indication is selected from one or more of the group consisting of traffic accident, police emergency, medical emergency, and fire emergency.  In some\nembodiments, a request for assistance is associated with a non-emergency situation (e.g. request for a tow truck after car breaks down).  In some embodiments, a request for assistance is associated with a device sending the request.  In other\nembodiments, a request for assistance is associated with a device not sending the request (e.g. a proxy request on behalf of a second device and/or a member device in a group of devices).  As used herein, a request is \"associated\" with a device or user\nwhen the request relates to an emergency or non-emergency situation involving the device or user.  In some embodiments, a request comprises data associated with a device (or user thereof).  In some embodiments, a request comprises a data set associated\nwith a device.  For example, in some embodiments, a request comprises a data set associated with a device, wherein the data set comprises current location data.  In other embodiments, a request for assistance is sent and/or received separately from data\nassociated with a device.  For example, in some embodiments, a request is sent first, and the recipient subsequently queries the device that sent the request for data or a data set associated with the emergency and/or device or user involved in the\nemergency.  Alternatively, in some embodiments, a request is sent first, and the recipient subsequently queries the device associated with the emergency for data or a data set associated with the emergency and/or device or user involved in the emergency.\nAs used herein, a \"emergency responder\" refers to any person or persons responsible for addressing an emergency situation.  In some embodiments, an emergency responder refers to government personnel responsible for addressing an emergency\nsituation.  In some embodiments, a emergency responder is responsible for a particular jurisdiction (e.g. a municipality, a township, a county, etc.).  In some embodiments, an emergency responder is assigned to an emergency by an emergency dispatch\ncenter.  In some embodiments, an emergency responder responds to a request for emergency assistance placed by a user via a user communication device.  In some embodiments, an emergency responder includes one or more fire fighters, police officers,\nemergency medical personnel, community volunteers, private security, security personnel at a university, or other persons employed to protect and serve the public and/or certain subsets of the population.\nAs used herein, a \"recipient\" refers to one or more persons, services, or systems that receive a request for assistance (e.g. an emergency alert).  The recipient varies depending on the type of request.  In some embodiments, a recipient is an\nemergency service.  In some embodiments, a recipient is an emergency service when he request for assistance pertains to an emergency (e.g. a tier 2 emergency).  In some embodiments, a recipient is an emergency management system.  In some embodiments, a\nrecipient is an emergency dispatch center.  In some embodiments, a recipient is an emergency dispatch center, wherein the request is first routed through an emergency management system (e.g. request is sent to the EMS, but ultimately is sent to an EDC). \nIn some embodiments, a recipient is a emergency responder (e.g. a communication device of a emergency responder).  In some embodiments, a recipient is a non-emergency service or personnel, for example, a relative or friend.  In such situations, a user of\na communication device (or member device or second device) does not require emergency assistance, but does need help.  As an example, a user of a member device in a group of devices is a child who is lost in a theme park.  The parent of the child has a\ncommunication device in the same group of devices as the child's member device.  The parent uses the communication device to send a request for assistance on behalf of the child's member device to theme park security guards who are closer to the child\nthan the parent.  Security is then able to pick up the child quickly using the data set associated with the member device, which they are given authorization to access by the parent's communication device.\nAs used herein, \"communication link\" refers to a communication pathway from a device (e.g. communication device) to another device or to an intermediate device (e.g. a router) on a network.  In some embodiments, the communication device\nestablishes a communication link with another device or an intermediate device to transfer information (e.g. a location of the device) or to obtain information from a recipient such as, for example, location of a emergency responder assigned to a request\nfor assistance associated with the communication device (e.g. device of emergency responder).  A communication link refers to the point-to-point communication channels, point-to-point and end-to-end data sessions, and the physical hardware facilitating\nthe communication channel(s) (e.g. antennas used to communicate/transmit information).  In some embodiments, a data session comprises session parameters and the network route taken from one device to another device.\nAs used herein, a \"data channel\" refers to a communication session between two devices wherein data packets are exchanged between the devices.  In some embodiments, a data session is setup using exchange of certain data packets, also called as\n\"handshake signals,\" which are able to define the capabilities of the data session.  For example, in some embodiments, the data session \"handshake\" provides for the ability to transfer multi-media data, voice data, and other data via the data session. \nIn some embodiments, the data session is setup without the use of handshake signals, wherein the two devices involved share data packets according to a predefined protocol (e.g. a previously agreed upon protocol).  In some embodiments, the data session\nis routed through an EMS, which stores the multi-media, voice, and/or other data from any of the devices that are part of the data session.  In some embodiments, the EMS shares the data from the data session with the other device (e.g. device of a\nemergency responder).  In some embodiments, the EMS manages the data session.\nAs used herein, a \"Received Signal Strength Indicator (RSSI)\" refers to a measurement of the power present in a received radio signal.  In some embodiments, the RSSI refers to a number assigned to the signal levels (e.g. power level) of packets\nas detected by a device receiving the packets from a device sending the packets.  For example, an RSSI value may be a number within an arbitrary range such as from 0 to 100.  In some embodiments, the RSSI refers to the decibel level of the power of the\nreceived data packets.  In other embodiments, the RSSI refers to the actual power, for example measured in mW, as detected by the receiver.  In some embodiments, RSSI is replaced with received channel power indicator (RCPI), which is a measure of the\nreceived radio signal power in a selected channel over the preamble and the entire received frame.\nAs used herein, \"voice or speech recognition software\" refers to computer programs that can recognize a person's speech to identify trigger phrases (e.g. iListen, Voice Navigator, Google Now, LilySpeech, etc.).  In some embodiments, the software\nmay be able to recognize the identity of the speaker.  As used herein, \"voice command\" refers to words or phrases that a user may use to give command to the triggering device.\nAs used herein, \"sound detection software\" refers to computer programs for detecting trigger sounds in and around the triggering device.  In some embodiments, the trigger sounds are user-defined or obtained from a library of phrases on the\ntrigger device or at a remote server.  Examples of trigger sounds include sounds (alarms, breakage, gunshots, explosion, fire, car crash, etc.).  In some embodiments, a trigger is an absence of sound (e.g. no heartbeat, etc).  As an example, a glass\nbreak detector software uses the microphone in the trigger device to monitor any noise or vibrations to detect burglaries in a smart home.  In some embodiments, vibrations exceeding a baseline threshold are analyzed by the software.  In some embodiments,\nthe software analyzes frequencies typical of glass shattering and trigger an emergency alert if the sound is above a trigger threshold.  In some cases, the software compares detected sounds with glass-break profiles analysis and triggers an alert if the\namplitude threshold and/or statistically expressed similarity threshold are breached.  In some embodiments, an emergency is detected or triggered when a trigger sound exceeds a threshold.  In some embodiments, a trigger sound threshold is about 50, 55,\n60, 65, 70, 75, 80, 85, 90, 95, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, or 200 decibels, including increments therein.  In some embodiments, a trigger sound threshold is at least about 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 110, 120, 130,\n140, 150, 160, 170, 180, 190, or 200 decibels, including increments therein.  In some embodiments, a trigger sound threshold is no more than about 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, or 200 decibels,\nincluding increments therein.\nIn some embodiments, voice recognition and speech recognition use recordings of the human voice, but differ in approach.  Speech recognition strips out the personal differences between speakers to detect the words.  Voice recognition typically\ndisregards the language and meaning to detect the physical person behind the speech.  Speech recognition is language dependent, while voice recognition is independent of language.  In essence, voice biometrics provides speaker recognition rather than\nspeech recognition.  In some embodiments, speech recognition is the inter-disciplinary sub-field of computational linguistics that develops methodologies and technologies that enables the recognition and translation of spoken language into text by\ncomputers.  It is also known as \"automatic speech recognition\" (ASR), \"computer speech recognition\", or \"speech to text\" (STT).  In some embodiments, some speech recognition systems require \"training\" (also called \"enrollment\") where an individual\nspeaker reads a text or isolated vocabulary into the system.  The system analyzes the person's specific voice and uses it to fine-tune the recognition of that person's speech, resulting in increased accuracy.  Systems that do not use training are called\n\"speaker independent\" systems.  Systems that use training are called \"speaker dependent\".  In some embodiments, speech recognition applications include voice user interfaces such as voice dialing (e.g. \"Call home\"), call routing (e.g. \"I would like to\nmake a collect call\"), domotic appliance control, search (e.g. find a podcast where particular words were spoken), simple data entry (e.g. entering a credit card number), preparation of structured documents (e.g. a radiology report), speech-to-text\nprocessing (e.g. word processors or emails), and aircraft (usually termed direct voice input).\nAs used herein, the term \"voice recognition\" or \"speaker identification\" refers to identifying the speaker, rather than what they are saying.  Recognizing the speaker can simplify the task of translating speech in systems that have been trained\non a specific person's voice or it can be used to authenticate or verify the identity of a speaker as part of a security process.  In some embodiments, to convert speech to on-screen text or a computer command, a computer or device has to go through\nseveral complex steps.\nAn exemplary speech detection program may work in the following way.  The analog-to-digital converter (ADC) translates the analog wave (vibrations from speech or sound) into digital data that the computer can understand.  To do this, the program\nsamples, or digitizes, the sound by taking precise measurements of the wave at frequent intervals.  The program filters the digitized sound to remove unwanted noise, and sometimes to separate it into different bands of frequency.  The program also\nnormalizes the sound, or adjusts it to a constant volume level.  In some embodiments, the program is temporally aligned.  Users speak at different speeds, so the sound may be adjusted to match the speed of the template sound samples already stored in the\nsystem's memory.  Next, the signal is divided into small segments as short as a few hundredths of a second, or even thousandths in the case of plosive consonant sounds--consonant stops produced by obstructing airflow in the vocal tract--like \"p\" or \"t.\"\nThe program then matches these segments to known phonemes in the appropriate language.  A phoneme is the smallest element of a language--a representation of the sounds we make and put together to form meaningful expressions.  There are roughly 40\nphonemes in the English language, while other languages have more or fewer phonemes.  The program examines phonemes in the context of the other phonemes around them.  The contextual phoneme plot is run through a complex statistical model and compared\nwith a large library of known words, phrases and sentences.  The program then determines what the user was probably saying and either outputs it as text or issues a computer command.\nIn some embodiments, speech recognition systems use powerful and complicated statistical modeling systems.  These systems use probability and mathematical functions (e.g. Hidden Markov Model and neural networks) to determine the most likely\noutcome.\nAs used herein, an \"audio detector\" refers to a device that can detect audio inputs (e.g. voice commands, trigger sounds).  In some cases, the audio detector is a separate device (e.g. a smart speaker).  In some cases, the audio detector is a\ncomponent of another device (e.g. microphone inside of a smart phone).  In some embodiments, sound may be detected \"sound sensors\", \"sound detectors\" or microphones.  For example, a sound is a board that combines a microphone and some processing\ncircuitry.  The sound detector provides an audio output, but also a binary indication of the presence of sound, and an analog representation of its amplitude.  In some embodiments, the microphone turns sound energy into electrical energy.  When a user\nspeaks, sound waves created by a user's voice carry energy toward the microphone.  Inside the microphone, the diaphragm and coil move back and forth upon contact with the sound waves.  A permanent magnet produces a magnetic field that cuts through the\ncoil and an electric current is produced.  The electric current flows out from the microphone to an amplifier on the sound recording device.\nIn some embodiments, the microphone in a phone or smart device turns the user's voice into a stream of instantaneous waveform samples (e.g. at a rate of 16000 per second).  A spectrum analysis stage converts the waveform sample stream to a\nsequence of frames, each describing the sound spectrum of approximately 0.01 sec. About twenty of these frames at a time (0.2 sec of audio) are fed to the acoustic model, a Deep Neural Network (DNN) which converts each of these acoustic patterns into a\nprobability distribution over a set of speech sound classes (e.g. 20 sound classes).  Each \"hidden\" layer is an intermediate representation discovered by the DNN during its training to convert the filter bank inputs to sound classes.  The system chooses\nthe number of units in each hidden layer of the DNN to fit the computational resources available.  On smart phones or smart devices, two neural networks may be used--one for initial detection and another as a secondary checker.  The output of the\nacoustic model provides a distribution of scores over phonetic classes for every frame.\nIn some embodiments, sound detection or voice activity detection (VAD) is accomplished by identifying a unique sound algorithm or pattern.  In some embodiments, the sound waves are sliced using a technology called analog to digital conversion\n(ADC).  Once this sound is in digital form, powerful and economical application specific integrated circuits (ASIC) along with digital signal processor (DSP) programming allows the sound detection software to identify trigger sounds and voice commands.\nIn some embodiments, voice activity detection (VAD), also known as \"speech activity detection\" or \"speech detection,\" is a technique used in speech processing in which the presence or absence of human speech is detected.  In some embodiments,\nVAD facilitates speech processing, and is also used to deactivate some processes during non-speech section of an audio session by avoiding unnecessary coding and transmission of silence packets in Voice over Internet Protocol applications, saving on\ncomputation and on network bandwidth.  In some embodiments, VAD is the processes of discrimination of speech from silence or other background noise.  The VAD algorithms are based on any combination of general speech properties such as temporal energy\nvariations, periodicity, and spectrum.\nIn some embodiments, to minimize power consumption of a voice-controlled devices, speech recognition is handled after \"keyword spotting\" is completed (e.g. by keeping the voice ADC and the DSP on).  In some embodiments, a VAD detector can\ncontinuously monitor surrounding sounds to identify voice activity so as to awaken the voice processing blocks (audio ADC and DSP) through a trigger signal as soon as, but not sooner than, when a voice is detected.\nEmergency Communications\nModern communication devices, for example, smart phones, tablet computers, wearable communication devices, smart sensor devices, and/or systems are often equipped with a variety of features for determining location information of the\ncommunication device using, for example, GPS, or triangulation with cellular phone towers or device-based hybrid location from location services.  Modern communication devices also often include functionality to store data regarding a user of the\ncommunication device, for example, health information about the user.\nIn some embodiments, the communication device (or communication module of the device) communicates with a recipient through a communication link comprising one or more data channels.  In some embodiments, the recipient is an emergency management\nsystem (EMS).  In some embodiments, the EMS routes communications to an EDC.  In some embodiments, the EMS establishes a first data communication link with the communication device and a second data communication link between the EMS and the EDC, wherein\nthe EMS bridges the first and second data channels to enable the communication device and the EDC to communicate.  In some embodiments, the EMS converts data (e.g. data set) from the communication device into a format suitable for the EDC (e.g. analog or\ndigital, audio, SMS, data, etc.) before sending or routing the formatted data to the EDC.  In some embodiments, the EMS routes communications to a device associated with a first responder or responder device.  In some embodiments, the communication\ndevice relays additional communications, updated or supplemental information, and/or data sent or shared between member devices in the group of devices to the EMS or EDC after a request for assistance has been sent.  In some embodiments, the additional\ninformation is relayed to the EMS or EDC after the request for assistance has been sent in order to provide current information that is relevant to the request.  For example, in some instances, communications between member devices contain information\nrelevant to the emergency (e.g. information that the user of member device who is experiencing a medical emergency suffers from diabetes).  Accordingly, in some embodiments, the information is sent autonomously, at request of a user of the communication\ndevice, or at request of the recipient (e.g. EMS, EDC, first responder, etc.).\nFIG. 1A shows a schematic diagram of one embodiment of an emergency flow management system 140 as described herein.  In some embodiments, the emergency flow management system 140 is a standalone system that can interact with devices, the\nemergency management system, and/or dispatch centers.  In other embodiments, the emergency flow management system is distributed in various devices and systems.\nIn some embodiments, the emergency flow management system (EFMS) 140 comprises one or more of an operating system 101, a central processing unit 103, a memory unit 136, a communication element 107, a display or user interface 149 (for an EFMS\nadministrator to maintain and update the system) and an emergency flow program 151 (e.g. server application) including at least one software module 153.\nIn some embodiments, the emergency flow management system 140 comprises one or more databases 135.  In some embodiments, the emergency flow management system 140 comprises an emergency flow database 157 from which emergency flows may be called\nwhen there is an emergency trigger.  In some embodiments, the emergency flow management system 140 comprises an emergency history database, which enters and maintains a log of emergency incidents, emergencies triggered, and/or emergency flows that have\nbeen implemented.  In some embodiments, emergency flow management system 140 includes a message bus 146, one or more servers 148 implementing instructions form the emergency flow program 151 (e.g. emergency flow server 128 for generating emergency flows\nand emergency response server 109 for executing emergency flows) and output services 160.\nFIG. 1B shows a schematic diagram of one embodiment of a device described herein.  In some embodiments, the device 167 is an electronic device such as a triggering device which may be a communication device (e.g. wearable, mobile or cellular\nphone, computer, laptop, vehicular console, smart car, personal digital assistant, IoT devices, smart home, smart TV, etc.).  In some embodiments, the triggering device is a wearable device.  In some embodiments, the triggering device is a wireless\nmobile device or a smartphone.  In some embodiments, the triggering device is a combination of devices (e.g. a wearable device communicatively connected to a mobile phone).  In some embodiments, the triggering device is a walkie-talkie or a two-way\nradio.  In some embodiments, a user 102 (not shown) is selected from one or more persons who are the primary users of the device 167.  When the user 102 is in an emergency situation, the device 167 can be triggered.\nIn some embodiments, the device 167 comprises at least one processor 161, a memory 171 (e.g. an EPROM memory, a RAM, a solid-state memory), a display 165, a user interface 163, a network component 164 (e.g. an antenna and associated components,\nWi-Fi adapters, Bluetooth.RTM.  adapters, etc.), sensors 181, and an emergency trigger program 169 (e.g. mobile application, server application, computer program, application).  In some embodiments, the device is equipped with a location component 173,\nfor example, a global positioning system (GPS), location services, etc. In some embodiments, the device comprises data storage 175.  In some embodiments, the device comprises a location data cache 177 and a user data cache 179.\nIn some embodiments, the device 167 has several components including a display 165 and user interface 173, which allow the user 102 (not shown) to interact with the device 167.  In some embodiments, the display 165 is a part of the user\ninterface 163 (e.g. a touchscreen is both a display and provides an interface to accept user interactions).  In some embodiments, the user interface 163 includes physical buttons such as the on/off and volume buttons.  In some embodiments, the display\n165 and/or the user interface 163 comprises a touch screen (e.g. a capacitive touch screen), which is capable of displaying information and receiving user input.  In some embodiments, the device 167 comprises hardware components not including a display\n165 and a user interface 163, wherein the device functions autonomously without requiring active user guidance or interaction.\nIn some embodiments, a device 167 includes various accessories that allow additional functionality.  In some embodiments, the accessories (not shown) include one or more of the following: microphone (e.g. for user voice interaction), a camera\n(e.g. for input of gestures commands or pictures from the user), speakers, one or more sensors 181, such as a fingerprint reader or scanner/health or environmental sensors, USB/micro-USB port, headphone jack, a card reader, SIM card slot, and any\ncombination thereof.\nIn some embodiments, a device 167 autonomously detects emergencies based on data from various sources such as the sensors 181 (sensor data) or sound detection picked up from the microphone or another accessory (e.g. smoke detector).  For\nexample, in some embodiments, a device autonomously detects an emergency based on sensor data when sensor readings or values exceed a threshold (e.g. a predefined threshold set by the device software by default, by a user, or by an EMS).  In some\nembodiments, the device 167 obtains relevant data from associated device to trigger the emergency.\nIn some embodiments, the emergency management system 130 houses components of the emergency flow management system (EFMS).  In some embodiments, the emergency management system 130 comprises an application for assembling and/or configuring\nemergency flow scripts.  In some embodiments, the emergency management system 130 comprises an application for managing an emergency response to an emergency alert including execution of an emergency flow script.  In some embodiments, the emergency\nmanagement system 130 comprises one or more of an operating system 132, at least one central processing unit or processor 134, a memory unit 136, a communication element 138, and a server application 151 (e.g. an emergency flow program).  In some\nembodiments, the emergency management system 130 comprises one or more databases 135 (e.g. location database 137, an additional information database 139, emergency flow database 157, and/or emergency history database 197).  In some embodiments, the\nemergency management system 130 may include one or more servers 148.  In some embodiments, an emergency response server 109 executes an emergency flow when an emergency or emergency-like situation is triggered via a user pathway 111).  In some\nembodiments, an emergency flow server 128 allows an administrator to generate and modify emergency flows via an admin pathway 113.  In some embodiments, the emergency management system 130 comprises at least one of a location database 137 and an\nadditional information database 139.  In some embodiments, the emergency flow program 151 comprises at least one software module 153 for carrying out one or more instructions.\nFIG. 1C shows a schematic diagram of one embodiment of an emergency trigger program 169 installed on a device, such as a triggering device 167.  In some embodiments, the emergency trigger program 169 comprises one or more device software modules\nselected from an emergency trigger module 172 (for saving conditions for triggering an emergency alert), an alert module 125 (for sending the emergency alert), a user data module 174 (for entering and storing user contacts and other preferences and user\ndata), a location determination module 129 (for determining device-based location through GPS, location services, or other methods), a data sharing module 176 (for sharing data from the device with EMS, EFMS, EDC or responders, etc.), a proxy\ncommunication module 127 (for making a proxy call on behalf of another), a notification module 178 (for displaying communications from the EFMS to the user), or any combination thereof.\nFIG. 1C also shows a schematic diagram of one embodiment of an emergency flow program 151 installed on a server (e.g. a server in an EMS).  In some embodiments, the emergency flow program 151 comprises one or more server software modules\nselected from an administrator's (admin) pathway 113 comprising an emergency flow editor module 170, a flow provisioning module 147, a data module 143, or any combination thereof.  In some embodiments, the emergency flow editor module 170 comprises a\nbuilding blocks module 194, an interactive space module 196, or any combination thereof.\nIn some embodiments, the emergency flow program 151 comprises one or more server software modules selected from a user pathway 111 comprising a response server API module 141, a core module 142, a service actions module 144, a telephony module\n145, or any combination thereof.  In some embodiments, the response server API module 141 comprises an emergency alert module 191, a flow identification module 192, a flow execution module 193, or any combination thereof.\nFIG. 1D shows a schematic diagram of one embodiment of a Public Safety Answering Point (PSAP) system 152 as described herein.  In some embodiments, the PSAP system 152 comprises one or more of a display 182, a user interface 186, at least one\ncentral processing unit or processor 183, a memory unit 184, a network component 185, an audio system 154 (e.g. microphone, speaker and/or a call-taking headset) and a computer program such as a PSAP Emergency Display Application 158.  In some\nembodiments, the PSAP application 158 comprises one or more software modules 159.  In some embodiments, the PSAP system 152 comprises a database of responders 156, such as medical assets, police assets, fire response assets, rescue assets, safety assets,\netc.\nFIG. 1D also shows a schematic diagram of one embodiment of a PSAP application 158 installed on a PSAP system 152 (e.g. a server in the PSAP system).  In some embodiments, the PSAP application 158 comprises one or more software modules.  In some\nembodiments, a software module is a call taking module 195, a PSAP display module 198, a supplemental or updated information module 199 or a combination thereof.  In some embodiments, the PSAP application 158 displays the information as depicted in FIG.\n22.  In some embodiments, the PSAP application 158 displays the information on a map.\nIn some embodiments, location and supplemental information may be displayed for emergency service providers (e.g. police, fire, medical, etc.) and/or responders on their devices.  It is contemplated that responder devices may be installed a\nresponder device program (not shown) similar to PSAP display module 198 as depicted in FIG. 22.  In some embodiments, the responder device program displays the emergency location on a map.\nFIG. 2 also depicts an embodiment of a system for connecting with other devices after an emergency alert has been triggered.  As shown, a triggering device 267 sends the emergency alert, which is a request for assistance to respond to an\nemergency, to the EFMS 240, which is housed in the EMS 230.\nIn some embodiments, the triggering device 267 includes a network component (e.g. a Wi-Fi antenna) which sends the emergency alert.  In some embodiments, the emergency alert is sent via the internet, cellular network, or landline network.  For\nexample, the emergency alert may be sent via an API call (such as through the alert module in FIG. 1B) over the internet to the EMS, or to a third-party system (e.g. a home security device could contact the security company's system), which is able to\ncontact the EMS via an API call.  In some embodiments, the alert is an SMS notification to the EMS or to the third-party system, which will contact the EMS.  In some embodiments, when the data signal is strong, data API calls are used because of good\ndata transfer rates for sending the alert quickly.  In some embodiments, when the data signal is not good (e.g. low signal-to-noise ratio, high latency), SMS and/or other methods are used as a fallback option for sending the emergency alert.\nIn some embodiments, the emergency alert is transmitted via a land mobile radio system (LMRS) using custom signals to a third-party system, which optionally connects to the EMS.  As an example, the triggering device is a two-way radio in a\nvehicle, which connects to another two-way radio in its vicinity to send the emergency alert.  In certain embodiments, whenever a device or user is able to connect to the EMS (e.g. via an API call), the alert is optionally sent to the EMS.\nIn some cases, the connectivity (e.g. Wi-Fi signals) is weak or the pathways are not secure and the emergency alert is transmitted using another communication device (e.g. a mobile phone) or a routing device (e.g. Wi-Fi router).  In some\nembodiments, the triggering device 267 is not Internet-enabled and sends the alert through designated pathways or to designated devices (e.g. via a mobile phone connected by Bluetooth to device 267).  In some embodiments, an IoT device sends the alert to\na controlling or master device (e.g. a home console that connects to smoke alarms).  In another example of this embodiment, the triggering device 267 is a physical \"panic\" button (e.g. a button under the desk of a bank teller) that is discreetly pressed\nin the event of a possible emergency situation and sends an emergency alert to a controlling or master device (e.g. a bank's central communication system).  In some embodiments, the controlling or master device forwards the alert to an emergency service\nsuch as an EMS or EDC such as according to an emergency flow script.  In some embodiments, an IoT device includes sensors that detect environmental conditions, and another device (e.g. a digital assistant, wearable, phone, etc.) evaluates if the sensed\nvalue indicates an emergency situation or likely emergency situation and sends an emergency alert.  In some embodiments, the IoT devices or digital assistants on the ground detect a voice command or sense data, and the processing is done at a remote\nlocation, such as on the cloud.\nIn some embodiments, an emergency is detected when sensor values exceed a certain threshold or fall outside an acceptable range.  In some embodiments, an emergency is detected when sensor values exceed a certain threshold (that is optionally\nuser configurable or preset) such as for a temperature sensor, a smoke detector, a carbon dioxide detector, or a carbon monoxide detector.\nIn some embodiments, the triggering device 267 includes a computer program 269 for triggering the emergency flow and sending the alert.  The computer program 269 pre-installed on the device or has been loaded and installed by the user (e.g. user\n202, not shown).  In some embodiments, the user 202 went through a setup or registration process for the device 267, where he or she has provided user data such as emergency contacts (e.g. phone numbers, email addresses, messaging IDs), user information,\nlocation (e.g. a physical address of the location of the device 267), etc. In some embodiments, user data, location data, emergency data, etc., is saved in data cache or storage in the device 267.  In other embodiments, the data is saved in one or more\ndatabases 235 in the EMS, in third-party servers or in cloud-based systems.  In some embodiments, the data is protected by password protection, authentication protocols for transmission, encryption, use of secured pathways, and other methods for limiting\nthe risk of security breaches.\nIn some embodiments, the emergency is triggered by user input such as the user interacting with the user interface 263 of the triggering device 267.  In some embodiments, the user presses one or more hard or soft buttons on the user interface\n263.  However, other types of user interactions such as touch, tap, pattern of touches, gesture, and voice-activation are also contemplated.  For example, a user in a taxicab who suspects that the cab driver does not know where he or she is going, or is\nintoxicated, or is attempting to abduct the user, the user may select a hard or soft button (e.g. a panic button) or a pattern of presses on the user's cell phone to discreetly trigger an emergency alert.\nIn some embodiments, the triggering device 267 autonomously detects emergency situations or likely emergency situations.  In some embodiments, the triggering device 267 sends an alert based on autonomously detected emergencies using one or more\nsensors (not shown) such as from a smoke alarm in a building.  In some embodiments, the sensors sense one or more environmental or health parameters.  In some embodiments, the environmental parameter is selected from the group consisting of: light,\nmotion, temperature, pressure, humidity, vibration, magnetic field, sound, smoke, carbon monoxide, radiation, hazardous chemicals, acid, base, reactive compounds, volatile organic compounds, and smog.  In some embodiments, health parameters include heart\nrate, pulse, electric signals from the heart, blood oxygen levels, blood pressure, blood sugar level, and other health parameters.  In some embodiments, the triggering device 267 is an Internet of Things (IoT) device such as a home thermostat, vehicle\nconsole, a pacemaker implant, etc. As used herein, IoT refers to the ever-growing network of physical devices, buildings, vehicles, and other objects that feature an IP address for internet network connectivity for exchanging data.  In many cases, IoT\ndevices are embedded with electronics, software, sensors, network connectivity, or a combination thereof.  In some embodiments, IoT devices feature an IP address for internet connectivity.  In addition to an IP address, an IoT device is optionally\nassociated with a MAC address or an SSID.  It is understood that, IoT devices are connected with one or more other devices through Bluetooth.RTM., Wi-Fi, or other wired and/or wireless technologies which allow for transfer of data.\nIn some embodiments, the IoT device is in a network of sensors.  As an example, IoT networks, wireless sensor networks (WSN) or wireless sensor and actuator networks (WSAN) monitor environmental parameters such as temperature, pressure, sound,\netc., using a network of sensors or devices.  When one sensor or device detects a sensed value outside of the identified range indicating a likely emergency, it will pass the data to other devices in the network.  In some embodiments, the sensor network\nis a Wi-Fi, WiMAX, or LTE MESH network.  In some embodiments, the sensor or IoT devices form nodes in the sensor network.  In some embodiments, the sensor network includes a central node for controlling the network.  In some embodiments, the sensor\nnetwork has a distributed architecture to reduce the impact of a failed node.\nIn some embodiments, an IoT device comprises at least one of the following components including a sensing component (e.g. thermocouple), a networking component (a radio transceiver with an antenna or connection for an external antenna), a\nmicrocontroller, an electronic circuit connected to the sensing component, and an energy source.  In some embodiments, the sensor network is controlled by a center console (e.g. a smart home console).\nIn some embodiments, the triggering device 267 is a communication device such as a mobile phone, a computer, a wearable device (e.g. a smart watch), a digital assistant, etc. In some embodiments, when the triggering device 267 is a mobile phone,\nthe emergency alert is sent via a cellular connection, if it is available.  In some embodiments, after the alert is sent to the EFMS 240 via communication link 222, the EFMS 230 initiates an emergency flow script.  As a part of the emergency flow, an\nemergency call along two-way communication link 226 may be established (e.g. via path 702, 704A, 708 shown in FIG. 7).  In some cases, communication link 222 is a data link, which optionally supports a data call such as VoIP.  In some embodiments, link\n222 is used for sending data such as user data, location data, emergency data, text, images, and video from the triggering device 267.  In some embodiments, communication link 222 is established via landline, cellular network or the Internet.  In some\nembodiments, the communication link 222 is through VoIP with both voice and data transmitted in the same path (e.g. in the SIP signaling, as headers or in multi-part messages).  In some embodiments, the communication link 222 is an analog voice call over\nlandline or cellular network and a data link for transferring data via Wi-Fi, cellular data, etc. Generally, data links are preferred for transmission of both voice and data whenever they are available, and the signal strength is good.  In certain cases,\nthe communication link is sent through NG911 systems, where the data is optionally sent through SIP signaling.  In some embodiments, updated data (e.g. current location data) may be transmitted via link 222 and provided to the EDC 250 via link 226.\nIn some embodiments, the EMS 230 includes one or more databases 235 housed in one or more servers in the same or in a remote location.  In some embodiments, location database 237 houses location data regarding the location of the emergency.  In\nsome embodiments, user database 239 houses user data and/or emergency data (such as an emergency contact list 265).  In other embodiments, the location, user, and/or emergency data (such as an emergency contact list 265) are saved on a data cache 215\n(not shown) in the triggering device 267 or in data storage in other devices such as mobile phone 206, computer 246, or mobile phone 216, etc. In other embodiments, the data is saved online in one or more remote servers or cloud-based systems.  The\nlocation data and additional data (user data and emergency data) may be provisioned in the databases 235, 239, or a third-party server, etc. and sent to the EDC 250 after a query has been received.  In some embodiments with a standalone EFMS 240 (as\ndepicted in FIG. 1A), databases 135 may be housed within the EFMS 240.\nIn some embodiments, the emergency contact list 265 is entered by the user 202 at the time of registration or installation of the computer program 269 or at another time.  In some embodiments, the emergency contact list 265 is a list of phone\nnumbers, email addresses, IP addresses, MAC addresses, or any combination thereof.  In some embodiments, computing devices which are not associated with a phone number (such as computer 246, not shown) are identified by an IP address, MAC address, URLs\nor SSIDs.\nIn some embodiments, the EFMS 240 attempts to connect to a user using one or more communication links (e.g. 234) when the user has not answered the call (NA response).  In some emergency flows, the EFMS 240 sends the alert indirectly to an EDC\n250 (e.g. a PSAP).  In some emergency flows, the EFMS 240 sends the alert to a third-party (not associated with user 202 (e.g. a corporate representative of an organization, etc.).  In some emergency flows, the EFMS 240 attempts to connect to user 202\nvia devices associated with user 202's account such as communication device 206 (not shown) or computer 246 (not shown).  In some emergency flows, the EFMS 240 optionally connects to user 205 (and other users designated as emergency contacts or\nfrequently called contacts, etc.) via communication device 216.  In some embodiments, a direct connection 238 between device 216 and dispatcher 250 is established or the user 205 is connected via the EFMS 240 via links 222 and 226.  However, the EFMS 240\nmay send the alert to any number of recipients, separately or simultaneously, in any order (e.g. according to any emergency flow).  Exemplary flows are depicted in FIGS. 14 and 15.\nWearable Device Communicatively Connected to a Communication Device\nIn another exemplary configuration of an emergency flow, a mobile device (e.g. a wearable 267) is communicatively connected to a communication device (e.g. a mobile phone 206, not shown) via Bluetooth.RTM., Wi-Fi, or other wireless communication\nmethods.  In some embodiments, the communication device 206 provides location and/or communication capabilities (e.g. whether the device is SMS, email, and/or data enabled).  In some embodiments, the mobile device is Internet-enabled & Location-enabled,\nallowing the user to send the emergency alert triggering an emergency flow in the EFMS 240.  Subsequently, the wearable device 267 optionally participates in an emergency session with EDC 250 via the communication device 206 through an indirect\nconnection.\nIn some emergency flows, the communication device 206 comprises an application 208, which may include some modules from emergency trigger program 269 on wearable 267.  In some embodiments, the emergency trigger program 269 on wearable 267 is\ndistributed between the two devices (wearable 267 & communication device 206).  This provides an advantage in allowing the wearable device that is light-weight with limited functions to be used in combination with the information and capabilities of the\ncommunication device 206 for responding to an emergency.  Various hardware and software capabilities of the wearable devices are contemplated.\nIn some scenarios, a user 202 configures emergency contacts (name, phone number), validates phone numbers, and enables the emergency flow from the communication device 206 while only sensor readings are obtained from the wearable 267 to trigger\nthe emergency.  In other embodiments, the application 208 receives an emergency alert from a device over Bluetooth.  In some embodiments, the application 208 sends an emergency alert with a list of two contacts and emergency location (as reported by the\nphone) to the EFMS 240.  The EFMS 240 may then execute an emergency flow comprising: first call contact and prompt contact to confirm emergency by pressing 1; if the call is not answered or confirmed, repeat with second contact; if someone answered and\nconfirmed emergency, connect call to appropriate PSAP based on the provided location; and wherein each contact hears a descriptive TTS message upon answering the call.\nFIG. 3 depicts an exemplary embodiment of a system 300 including triggering devices 367, an Emergency Flow Management System (EFMS) 340, & output services 360.  As previously mentioned, one advantage provided by the systems, servers, devices,\nand methods described herein is the ability to facilitate emergency response communications across devices, services, and systems managed by different parties.  As will be discussed in further detail, in some embodiments, the EFMS 340 is capable of being\nreconfigured and customized by any individual or organization (e.g. a smartphone application developer or a wearable device manufacturer) such that the EFMS 340 is seamlessly integrated into the devices, services, and/or systems provided by the\nindividual or organization (hereinafter, \"administrator\") according to the individual or organization's specific needs.  Accordingly, as shown in this exemplary embodiment, the system 300 is implemented with a variety of triggering devices 367 and a\nvariety of output services 360.\nIn some embodiments, a triggering device 367 is activated and delivers an emergency alert to the EFMS 340.  In some embodiments, the triggering device 367 is an electronic device associated with a user, such as a smartphone 367a (e.g. an\niPhone), a wearable device 367b (e.g. an Apple Watch or FitBit tracker), or a smart home IoT device 367e (e.g. an Amazon Echo or a Nest smoke and carbon monoxide alarm).  In some embodiments, the triggering device is a vehicle 367c such as a car or\ntruck.  In one example of this embodiment, the vehicle 367c includes an intelligent vehicle system capable of detecting when a component of the vehicle 367c has failed or when the vehicle 367c has been in an accident or otherwise compromised.  In another\nexample of this embodiment, the vehicle 367c includes or is otherwise communicatively coupled to a vehicle safety service that can connect the passengers of a vehicle that has been compromised with a first responder or a customer service agent (e.g.\nOnStar or AAA).  In this example, when the vehicle 367c becomes compromised, the intelligent vehicle system can deliver an emergency alert to the vehicle safety service, which may in turn attempt to contact passengers of the vehicle 367c or send a\ncorresponding emergency alert to another recipient (e.g. to the EFMS 340).  In some embodiments, the triggering device comprises a software or hardware panic button 367d.  As an example, the triggering device 367 is a physical button installed under the\nsteering wheel of a taxi cab, so that a taxi driver who feels threatened by a passenger (e.g. a passenger with a weapon or a passenger who is being verbally abusive) may discreetly call for help.  Similarly, in another example, the triggering device 367\nis a digital button found in a graphical user interface of a ride sharing smartphone application (e.g. the Uber app) that a passenger may select to discreetly call for help if the passenger feels threatened by a driver of a ride sharing vehicle.\nIn some embodiments, the triggering device 367 is triggered via user input or automatic detection.  For example, in embodiments in which the triggering device is a wearable device 367b (e.g. an Apple Watch), the wearable device 367b comprises at\nleast one sensor such as a gyroscope, an accelerometer, and/or a heart rate monitor.  In this example, if the heart rate monitor detects that the heartrate of the user is abnormal (e.g. higher or lower than average for the user, or arrhythmic), and the\ngyroscope and/or accelerometer detect a sudden, downward motion of the wearable device 367b (e.g. acceleration exceeds a threshold), the wearable device 367b determines that the user has potentially fallen due to a cardiac emergency and may need\nassistance.  In response to such a determination, the wearable device 367b automatically delivers an emergency alert to the EFMS 340 without input from the user.  Alternatively, in some embodiments, if a user of a wearable device 367b feels that they are\nexperiencing or soon to experience a medical emergency, the user optionally selects a button on the wearable device 367b to manually deliver an emergency alert to the EFMS 340.  Similarly, in some embodiments, a user of a smartphone 367a or wearable\ndevice 367b who is under assault or feels they will soon be under assault is able to select a button on the smartphone 367a or wearable device 367b to manually deliver an emergency alert to the EFMS 340.  In some embodiments, the emergency alert is\ndelivered to the EFMS by an electronic device communicatively coupled to the triggering device.  For example, in some embodiments, a wearable device coupled to a cell phone via Bluetooth generates an emergency alert that is then delivered to the EFMS by\nthe cell phone via Wi-Fi or cellular data.\nIn another example, in an embodiment in which the triggering device 367 is a smart home device 367e, the smart home device optionally includes at least one sensor such as a smoke detector or carbon monoxide detector.  In this example, when the\nsmart home device 367e detects a concentration of carbon monoxide that exceeds a threshold concentration, the smart home device 367e determines that the user and or house of the user is in a state of emergency, and automatically deliver an emergency\nalert to the EFMS 340.  In another example, when a user is experiencing an emergency, the user optionally manually prompts the smart home device 367e to deliver an emergency alert to the EFMS 340 by pressing a button on the smart home device 367e or by\ninteracting with the smart home device 367e non-physically, such as by verbally communicating with the smart home device 367e (e.g. by saying aloud, \"[name of smart home device 367e], call 9-1-1\").  In another example of this embodiment, the smart home\ndevice 367b includes a video camera or optical sensor.  When the video camera (and accompanying software and/or processor) or optical sensor determines the presence of an unauthorized person inside or otherwise proximal to the house of the user, in some\nembodiments, the smart home device 367e automatically delivers an emergency alert to the EFMS 340.  Alternatively, the triggering device 367 is a non-optical device or application and is activated manually or automatically in any fashion.\nIn some embodiments, the EFMS 340 is configured to receive an emergency alert from a triggering device 367 and execute an emergency flow, as will be discussed in further detail.  In some embodiments, as depicted in FIG. 3, the EFMS 340 includes\nan API module 341, a core module 342, a data module 343, a service actions module 344, and a telephony module 345.  In some embodiments, these modules interact to execute customized emergency flows according to various configurations of emergency flow\nbuilding blocks, wherein the emergency flow building blocks each represent an emergency flow script that performs at least one function.  In some embodiments, the various configurations of emergency flow building blocks are labeled and identified with\nunique emergency flow identification numbers (hereinafter, \"emergency flow ID\").  In some embodiments, an emergency alert delivered to the EFMS 340 from a triggering device 367 is accompanied by an emergency flow ID, which is recognized by the API module\n341 to point to an associated emergency flow for execution by the EFMS 340.\nIn some embodiments, the EFMS 340 is configured to receive an emergency alert delivered from a triggering device 367 at the API module 341.  In some embodiments, the emergency alert delivered from the triggering device 367 includes an emergency\nflow ID.  In some embodiments, the emergency alert delivered from the triggering device 367 includes additional data.  For example, in some embodiments, the emergency alert delivered from the triggering device 367 includes location data, such as a\nlongitude and latitude coordinate pair, or a street address.  In some embodiments, the location data includes information obtained from one or more sources such as, for example, a location component (such as a GPS, not shown), Wi-Fi access points\ninformation using a Wi-Fi antenna (not shown), Bluetooth beacon data using a Bluetooth antenna (not shown), cellular trilateration using a cellular transmitter capable of supporting various technologies such as CDMA, LTE, or WiMAX, and barometric\npressure using a pressure sensor to estimate altitude.  In some embodiments, the emergency alert delivered from the triggering device 367 includes user data associated with a user of the triggering device 367.  For example, the emergency alert delivered\nfrom the triggering device 367 is optionally accompanied by medical history data associated with a user that the user has stored on a smartphone 367a.  In another example, the emergency alert delivered from the triggering device 367 is accompanied by\nheart rate data obtained by a wearable device 367b before or during the time that the emergency alert was delivered.  In another example, the emergency alert delivered by the triggering device 367 is accompanied by driving data determined by an\nintelligent vehicle system integrated into a vehicle 367c, such as the speed at which the vehicle was moving before or during the time that the emergency alert was delivered.\nIn some embodiments, the API module 341 processes the emergency alert and accompanying data (e.g. emergency flow ID, location data, and user data) and activates the core module 342.  In some embodiments, the API module 341 determines which\nemergency flow for the EFMS 340 to execute based on an emergency flow ID included with the emergency alert delivered from the triggering device 367.  In some embodiments, the API module 341 determines which emergency flow for the EFMS 340 to execute\nbased on an emergency flow ID (also referred to as an emergency call flow identifier) and additional data included with the emergency alert.  For example, in some embodiments, an emergency flow ID corresponds to multiple emergency flows (e.g. emergency\nflow A, emergency flow B, emergency flow C, etc.) which are optionally executed preferentially based on the assessed situation of a user.  In one example of this embodiment, an emergency alert is delivered to the EFMS 340 from a wearable device 367b.  In\nthis example, the emergency alert includes emergency flow ID#123 and additional data gathered by a heart rate monitor, a gyroscope, and an accelerometer.  In this example, emergency flow ID#123 corresponds to two emergency flows, emergency flow A, which\nincludes contacting a nurse and calling 9-1-1, and emergency flow B, which includes contacting a nurse but does not include calling 9-1-1.  When the additional data included in the emergency alert indicates that a user of the wearable device has merely\nfallen, the API module 341 optionally executes emergency flow B. However, if the additional data included in the emergency alert indicates that the user has fallen due to a cardiac emergency, the API module 341 optionally executes emergency flow A\ninstead.  In some embodiments, emergency flow A and emergency flow B are considered and/or referred to as complete definitions of an emergency flow (e.g. emergency flow ID#123 represents a template of an emergency flow that requires one or more\nadditional inputs to complete the definition of the emergency flow; emergency flow A and emergency flow B represent complete definitions of the emergency flow corresponding to emergency flow ID#123).  In some embodiments, a particular emergency flow ID\nonly corresponds to one particular emergency flow.  In some embodiments, the triggering device 367 selects between multiple emergency flow IDs based on data collected by the triggering device 367 or provided by a user.  In some other embodiments, in\nwhich an emergency alert does not include an emergency flow ID, the API module 341 selects an emergency flow to execute based on alternative factors, such as the type or brand of triggering device 367, a location of the triggering device, a weather\nforecast at the location of the triggering device 367, or other parameters.  In some embodiments, the flow identifier is a flow identification number included in the emergency alert.  In some embodiments, the flow identifier is included in the header,\nfooter, message, metadata, or a combination thereof in the emergency alert.  In some embodiments, the flow identifier is not a flow identification number and takes another form (e.g. device type, device name, application name, application publisher,\netc.).  In some embodiments, an emergency alert includes an emergency flow ID and/or an identifier of the organization (hereinafter \"organization ID\" or \"organization identifier\") that created the associated emergency flow.  For example, in some\nembodiments, the emergency alert is an HTTP POST that includes an emergency flow ID in the payload of the HTTP POST and an administer ID associated with the organization that created the associated emergency flow in the header of the HTTP POST, as shown\nbelow.  In some embodiments, after receiving an emergency alert, the API module 341 first identifies an organization using an organization ID included in the emergency alert and then references the emergency flow database (e.g. data module 343) to\ndetermine one or more emergency flows created by the organization.  In some embodiments, the API module 341 then uses an emergency flow ID included in the emergency alert to select a corresponding emergency flow from the one or more emergency flows\ncreated by the organization to execute.  In some embodiments, the emergency flow ID is a name of the corresponding emergency flow selected by the organization that created the emergency flow.  In some embodiments, the API module 341 selects an emergency\nflow in response to an emergency alert from a triggering device 367 through any appropriate means regardless of the form of the flow identifier.  In some embodiments, the emergency alert does not include a flow identifier.  In some embodiments, the API\nmodule 341 selects a default emergency flow in response to an emergency alert that includes no additional data (e.g. no flow identifier, device location, sensor data, etc).\nAn exemplary template of an emergency alert is shown below in the form of an HTTP POST:\nTABLE-US-00001 url = \"https://api-sandbox.rapidsos.com/v1/rem/trigger\" payload = [ \"callflow\": \"company_contacts\", \"variables\": [ \"location\": [ \"latitude\": \"\", \"longitude\": \"\", \"uncertainty\": \"\" ], \"user\": [ \"full_name\": \"\", \"phone_number\": \"\"\n], \"contacts\": [ [ \"full_name\": \"\", \"phone_number\": \"\" ] ], \"company\": \"\" ]\nIn the foregoing exemplary template of an emergency alert, \"company_contacts\" is both the emergency flow ID and the name of the associated emergency flow as selected or inputted by the administrator that created the emergency flow.  In this\nexample, \"location\"; \"user\"; \"contacts\"; and \"company\" are variables required by the \"company_contacts\" emergency call flow.  \"Latitude\"; \"longitude\"; and \"uncertainty\" are components of the \"location\" variable; \"full_name\"; and \"phone_number\" are\ncomponents of the \"user\" variable; and \"full_name\" and \"phone_number\" are components of the \"contacts\" variable.  In some embodiments, a value is provided in the emergency alert for each of the variables or components of a variable.  In some embodiments,\nas described above, all variables, and components therein, defined or required by an emergency call flow are necessary for the emergency call flow to be executed by the API module 341.\nIn some embodiments, emergency flows are stored within a data module 343 located within or otherwise communicatively coupled to the EFMS 340.  In some embodiments, the API module 341 consults the data module to determine an emergency flow to\nexecute in response to the emergency alert.  For example, in some embodiments, the emergency alert includes an emergency flow ID that corresponds to one or more emergency flows stored within the data module 343.  The API module 341 then optionally\nreferences the data module 343 for an emergency flow corresponding to the emergency flow ID.  In some embodiments, after receiving an emergency alert including an emergency flow ID and any accompanying additional data, the API module 341 references the\ndata module 343 to find an emergency flow corresponding to the emergency flow ID.  In some embodiments, the API module 341 then processes the emergency flow, determines any necessary inputs for the emergency flow, and verifies that the additional\ninformation included in the emergency alert includes the necessary inputs for the emergency flow.  For example, a particular emergency flow may additionally require a measurement of a user's heart rate as a necessary input for the emergency flow.  In\nthis example, if the emergency alert does not include a user's heart rate (e.g. the emergency alert includes an emergency flow ID corresponding to the particular emergency flow and a location, but is missing a user's heart rate), the EFMS 340 may not be\nable to execute the particular emergency flow.  In response, the EFMS 340 optionally declines the emergency alert or delivers a notification to the triggering device 367 informing the user that the emergency alert was incomplete.  In this embodiment,\nwhen the API module 341 determines that the emergency alert does include the necessary inputs for the emergency flow, the API module 341 compiles the necessary inputs received from the emergency alert with the emergency flow to create a complete\ndefinition of the emergency flow (as discussed above), and delivers the complete definition of the emergency flow to the core module 342.\nIn some embodiments, the data module 343 additionally includes an emergency flow history database that records individual instances of particular emergency flow sessions.  For example, in some embodiments, after the API module 341 receives an\nemergency alert including an emergency flow ID and activates the core module 342, the core module 342 records an entry in the emergency flow history database for the particular emergency flow session of the particular emergency flow being executed.  In\nsome embodiments, the core module 342 records an entry in the emergency flow history database for every emergency flow session.  In some embodiments, the core module 342 records an entry in the emergency flow history database for each emergency alert\nreceived by the API module 341.  In some embodiments, the core module 342 records an entry in the emergency flow history database for each emergency alert received by the API module 341 that includes an emergency flow ID.  In some embodiments, the core\nmodule 342 records an entry in the emergency flow history database for a particular emergency flow session of a particular emergency flow after the particular emergency flow has been fully executed.  In some embodiments, the core module 342 updates an\nentry in the emergency flow history database for a particular emergency flow session of a particular emergency flow after each step (e.g. after each individual emergency flow building block) of the execution of the particular emergency flow, or after\nsome steps of the execution of the particular emergency flow.\nIn some embodiments, after an emergency flow is executed by the EFMS 340, the core module 342 updates an entry in the emergency flow history database for a particular emergency flow session of a particular emergency flow to include additional\ndata about the particular emergency flow session.  For example, in some embodiments, the core module 342 records in the emergency flow history database data including, but not limited to: which emergency contacts were contacted and/or which emergency\ncontacts responded, if an EDC was contacted, if contacting an EDC was successful or unsuccessful, if a party opted to contact an EDC, or which party opted to contact an EDC.  In some embodiments, after the execution of a particular emergency flow, the\ncore module 342 updates an entry in the emergency flow history database for the particular emergency flow session of the particular emergency flow to reflect that the particular emergency flow session was successful or unsuccessful.  In some embodiments,\nthe criteria for success of a particular emergency flow are predetermined by the administrator that created the particular emergency flow.  In some embodiments, the criteria for success of a particular emergency flow are predetermined by the EFMS 340.\nThe EFMS 340 is capable of executing many different permutations of emergency flows as disclosed herein.  In some embodiments, emergency flows are defined by various emergency flow building blocks, each emergency flow building block defined by a\nscript, written in a programming language, which contains instructions for executing various functions relating to an emergency flow.  In some embodiments, the various functions are executed by the telephony module 345 and the service actions module 344,\nas depicted in FIG. 3.\nIn some embodiments, the EFMS 340 employs the service actions module 344 to execute various emergency flow building blocks that require transmitting data and communications to and from various users and output services 360 using various mediums\nand communication modes.  For example, in some embodiments, an emergency flow includes an emergency flow building block with instructions for delivering a text message through short message service (SMS) or multimedia messaging service (MMS) or text\nmessage 360c to an account associated with a user, which is optionally executed by the service actions module 344.  In another example, in some embodiments, an emergency call block requires the EFMS 340 to deliver a message to an account associated with\na user through an internet enabled communication service 360e (e.g. WhatsApp, Slack, or Facebook Messenger) via an API call or HTTP post, which is optionally executed by the service actions module 344.  In some embodiments, associated contacts are also\ncontacted by a voice call (PSTN or data or VoIP call).  In some embodiments, associated contacts are called and a TTS message is played.  In yet another example, in some embodiments, an emergency flow includes an emergency flow building block with\ninstructions for delivering an audio adaptation of a text message (e.g. text-to-speech message) to an account associated with a user, which is optionally executed by the service action module 344.  In yet another example, an emergency flow may include an\nemergency flow building block with instructions for querying a triggering device 367 or an electronic device associated with a user for a location associated with a user, which is optionally executed by the service actions module 344.\nIn some embodiments, the service actions module 344 includes a location service (e.g. a location API) that can be employed by the API module 341 to send or retrieve locations to and from a location database.  In some embodiments, the location\ndatabase is external to the EFMS.  For example, in some embodiments, an emergency alert includes a location (e.g. a location generated by the triggering device or an electronic device associated with the triggering device).  After receiving the emergency\nalert, the API module 341 can employ the location service to transmit the location included in the emergency alert to an external location database.  In some embodiments, the service actions module 344 includes a voice command service that the API module\n341 can employ during emergency flows to receive oral input from users.  For example, in some embodiments, an emergency flow building block, such as an interactive call block, as described below, may accept voice inputs using the voice command service.\nIn some embodiments, the telephony module 345 is constructed using hardware components such as voice over internet protocol (VoIP) gateways and open source communication software.  In some embodiments, the EFMS 340 employs the telephony module\n345 to execute various emergency flow building blocks requiring communication links.  For example, in some embodiments, an emergency flow includes a building block with instructions for delivering an interactive phone call to a user (e.g. an automated\nphone call that accepts inputs from the recipient of the call).  In some embodiments, while executing the emergency flow, the core module 342 employs the telephony module 345 to execute the interactive call.  In another example, in some embodiments, an\nemergency flow includes a building block with instructions for delivering a call to an output service 360 (e.g. an emergency dispatch center 360a, specifically a 911 call center or PSAP 360b, or a customer service representative 360d), which is\noptionally executed by the telephony module 345.\nFIG. 4 depicts an exemplary embodiment of a system 400 including an emergency flow editor user interface 470 (hereinafter, \"emergency flow editor\"), triggering devices 467, an emergency alert 423, the EFMS 440, the EMS 430, and an output service\n450.  In some embodiments of the system 400, an administrator accesses the emergency flow editor 470 and uses the emergency flow editor user interface to configure an emergency flow.  The emergency flow editor 470 then optionally stores the emergency\nflow in an emergency flow database (e.g. the data module 343 depicted in FIG. 3) and assigns the emergency flow an emergency flow ID.  In some embodiments, the administrator then installs a program, application, or script (e.g. an API) into a triggering\ndevice 467 configured to deliver data pertaining to the emergency via pipeline 423.  In some embodiments, the data is transmitted within the emergency alert including the emergency flow ID.  In some embodiments, the data is transmitted before or after\nthe emergency alert is sent via the pipeline 423 to the EFMS 440, which functions in conjunction with the EMS 430 to execute the emergency flow.  In some embodiments, the emergency alert 425 (not shown) includes additional data, such as a location\nassociated with a user, health data associated with a user, or a list of accounts associated with a user.  In some embodiments, the execution of the emergency flow includes initiating communications with an output service 450, such as an EDC or PSAP.\nIn some embodiments, the data pertaining to the emergency is transmitted to the EDC 450 via a pipeline 424 (not marked).  The data is transmitted as a part of an emergency alert or afterwards.  In some embodiments, the data is provisioned in the\nEFMS 440, EMS 430 or a third-party server and sent to the EDC 450 in response to a query from the EDC 450.\nThe data transmitted through pipelines 423, 424 is optionally encrypted and sent through secure pathways to authorized recipients.  Pipelines 423, 424 are contemplated to deliver location, voice, and additional data (e.g. user data, images,\nvideo feed) from device 467 to the EDC 450 in a secure and compatible format.\nIn one exemplary implementation of this embodiment, the administrator is a company that produces a smartwatch.  The company optionally uses the emergency flow editor 470 to create an emergency flow that activates when a wearer of the smartwatch\n(e.g. the triggering device 467) presses a button on the smartwatch that indicates (e.g. delivers an emergency alert 425 to the EFMS 440) that the wearer is in a state of distress (e.g. the wearer of the smartwatch has fallen and is incapable of picking\nthemselves up).  When activated, the emergency flow is configured by the company to instruct the EFMS 440 to deliver an interactive call to the smartwatch (if the smartwatch is capable of receiving calls) or to a phone associated with the wearer in which\nthe interactive call asks the wearer if they are in need of emergency assistance.  The interactive call then optionally waits for a predetermined duration of time (e.g. 20 seconds) for an input from the wearer of the smartwatch (e.g. the interactive call\nmay present the wearer with two options: press 1 for yes or * for no).  If the wearer selects 1, or the predetermined duration of time expires before the wearer submits an input, the EFMS 440 then initiates a call with an EDC, and connects the wearer\nwith the EDC 450 once the EDC has accepted the call.  If the wearer selects *, the EFMS 440 terminates the emergency response flow.\nIn another exemplary implementation of this embodiment, the administrator is a company that provides a vehicle safety service (e.g. OnStar or AAA).  In this example, the company uses the emergency flow editor 470 to create an emergency flow that\nis automatically activated when an intelligent vehicular system (integrated with the vehicle safety service) within a vehicle detects that the vehicle has been compromised (e.g. when the vehicle has been in an accident).  In this example, when the\nintelligent vehicular system detects that the vehicle has been compromised, the vehicle (e.g. the triggering device 467) delivers an emergency alert 425 to the EFMS 440, which executes the emergency flow.  In this example, when executed, the emergency\nflow is configured by the company to instruct the EFMS 440 to call a customer service provider 450 (e.g. a AAA representative), call the vehicle, and bridge the calls between the vehicle and the customer service provider 450.  The emergency flow also\noptionally provides the customer service provider 450 with an option to initiate a call with an EDC 450 (e.g. call a PSAP).\nFIG. 5 depicts an exemplary embodiment of a system 500 for the creation and implementation of an emergency flow.  As depicted in FIG. 5, in some embodiments, the system 500 contains two pathways: an administrator pathway 513 (admin path) and a\nuser pathway 511 (user path).  The admin path 513 is initiated by an administrator.  In the admin path, the administrator accesses an emergency flow editor 570 to configure an emergency flow to fit the needs of the administrator's product or service,\nsuch as the smartwatch or vehicle described in the examples provided above with respect to FIG. 4.  In some embodiments, in the admin path, an emergency flow provisioning API service 547 compiles the emergency flow, assigns an emergency flow ID to the\nemergency flow, and stores the emergency flow within a data module 543.  The user path 511 is initiated by a user 500, or a device associated with a user, of the product or service provided by the administrator, such as the vehicle or the wearer of the\nsmartwatch described in the examples provided above with respect to FIG. 4.  In some embodiments, in the user path, the API module 541 receives an emergency alert including an emergency flow ID from a triggering device.  In some embodiments, the API\nmodule 541 then references the emergency flow ID with the data module 543 to find the emergency flow corresponding to the emergency flow ID and delivers the emergency flow to the core module 542 for execution.  In some embodiments, the core module 542\nemploys the service actions module 544 and the telephony module 545 to execute various blocks of the emergency flow.  In some embodiments, the API module 541, the core module 542, the service actions module 544, and the telephony module 545 are\nseparately and simultaneously in communication with the message bus 546, which facilitates and coordinates synchronous and asynchronous communications (e.g. a communication bridge, text messages, etc.) between the modules and various users and accounts\n(e.g. a user, emergency contacts, emergency responders, etc.).\nThe Emergency Console\nFIG. 6 depicts an exemplary view of an emergency flow configuration editor 670 (also referred to as the Emergency Console).  In some embodiments, the emergency flow editor 670 is used to configure customized emergency flows.  In some\nembodiments, the emergency flow editor 670 includes a programming language input field 671 in which users manually program an emergency flow by inputting written programming commands to create a script 672 (not shown) that defines the emergency flow.  In\nsome embodiments, the emergency flow editor additionally or alternatively includes a graphical user interface 673 (also referred to as an \"interactive space\") which users use to configure emergency flows by dragging and dropping (or otherwise\nmanipulating) graphical representations of emergency flow building blocks 674 into various arrangements.  In some embodiments, an emergency flow building block is defined by a short script (e.g. a compilation or block of written programming commands),\nwritten in a programming language, that contains instructions for executing various functions (also referred to as emergency response functions) relating to an emergency flow.  A single emergency flow building block generally contains instructions\nrelating to one emergency flow function, as will be described in greater detail, and does not represent an entire emergency flow.  In some embodiments, an arrangement of emergency flow building blocks in the graphical user interface 673 automatically\nresults in the creation of a script 672, which is optionally displayed in the programming language input field 671.  In some embodiments, an emergency flow building block receives at least one input, performs at least one emergency response function\nbased upon the at least one input, and generates at least one output.  In some embodiments, at least one input for an emergency flow building block comprises an output received from another emergency flow building block.  In some embodiments, adjacent\nemergency flow building blocks in an emergency flow script are connected such that an output of a preceding emergency flow building block forms an input for at least one succeeding emergency flow building block.  In some embodiments, the emergency flow\neditor 670 includes either the programming language input field 671 or the graphical user interface 673, but not both.  In some embodiments, the emergency flow editor includes both the programming language input field 671 and the graphical user interface\n673.  In some embodiments, the emergency flow editor 673 includes options or buttons to save, test, and publish an emergency flow.\nIn some embodiments, the Emergency Console allows a variety of customizable emergency flows between users, emergency contacts, emergency services, and related third parties by establishing a multitude of voice and data connections to Public\nSafety Answering Points (PSAPs) through a variety of trigger mechanisms.  In some embodiments, the trigger mechanisms enable implementation in a variety of scenarios including software panic buttons (e.g. within mobile applications), remote activation by\nassociated emergency contacts, and others.  The Emergency Console allows for the customization and generation of emergency flows while ensuring that the generated emergency flows comply with regulatory constraints (Federal, State or local laws,\nregulations, policies, best practices, etc.) applicable to the location and type of emergency.  In some embodiments, the Emergency Console is a part of the EMS.  In some embodiments, the Emergency Console is part of an EDC such as a PSAP.  In some\nembodiments, the Emergency Console is operated on an emergency response server.  In some embodiments, the EMS comprises an emergency response server.  In some embodiments, the Emergency Console is a web interface that provides tools for generating and\ntesting emergency flows.  In some embodiments, the Emergency Console allows for emergency flows to be initiated via simple API triggers from any device.\nEmergency Flow Building Blocks\nIn some embodiments, the emergency flow editor 670 (e.g. the Emergency Console) contains a set of predefined emergency flow building blocks.  Below is a non-exhaustive list of exemplary emergency flow building blocks that are optionally included\nin the set of predefined emergency flow building blocks and that may be incorporated into a customized emergency flow.\n(a) Create Emergency Bridge Block:\nIn some embodiments, the create emergency bridge block instructs the EFMS to create a communication bridge in which one or more calls are dynamically added or removed.  The communication bridge serves as a hub for various calls that are made\nduring the execution of an emergency flow.  In some embodiments, the create emergency bridge block takes no inputs and produces no outputs.  In some embodiments, the create emergency bridge block is a core component included in every emergency flow.  In\nsome embodiments, the create emergency bridge block is an implied emergency flow building block (e.g. the script defining the create emergency bridge block is included in every emergency flow but the create emergency bridge block is not depicted in the\ngraphical user interface 673).\n(b) Call User Block:\nIn some embodiments, the call user block instructs the EFMS to initiate a phone call to a phone number associated with the user associated with the triggering device and connect the phone call with a communication bridge.  The input for the call\nuser block is the phone number associated with the user.  The outputs of the call user block are: (i) the user answered the phone call or (ii) the user did not answer the phone call.\n(c) Play Pre-Recorded Message Block:\nIn some embodiments, the play pre-recorded message block instructs the EFMS to play a pre-recorded audio file to one or more parties currently connected to a communication bridge.  The input for the play pre-recorded message block is the name or\nfile location of the pre-recorded audio file.  The play pre-recorded message block has no output.\n(d) Play TTS Message Block:\nIn some embodiments, the play TTS (text-to-speech) message block instructs the EFMS to play an audio file adaptation of a text file to one or more parties currently connected to a communication bridge.  The input for the play TTS message block\nis the text of the message to be converted to audio.  The play TTS message block has no output.\n(e) Send SMS Message Block:\nIn some embodiments, the send SMS message block instructs the EFMS to deliver a SMS message to a user or a group of users.  In some embodiments, the SMS message includes information pertaining to status of the emergency flow session.  The inputs\nfor the send SMS message block are the contents of the text message to be sent and the phone number(s) of the intended recipients of the text message.  The send SMS message block has no output.\n(f) Timeout Block:\nThe timeout block instructs the EFMS to add a timeout instruction for a desired event.  For example, in some embodiments, an administrator can add a timeout instruction to another emergency flow building block, such as the call user block, and\nspecify an amount of time that the emergency flow should wait at the call user block before autonomously determining a negative outcome (e.g. in the case of the call user block, the user did not answer).  The input for the timeout block is the amount of\ntime (e.g. 1-30 seconds).  The output of the timeout is a confirmed negative outcome.\n(g) Location Block:\nIn some embodiments, the location block instructs the EFMS to query or detect a location of a user.  In some embodiments, the location block instructs the EFMS to parse a location database for a location.  In some embodiments, the location block\ninstructs the EFMS to communicate with a triggering device to determine the location of the triggering device.  The input for the location block is an account associated with a user (e.g. a phone number of the user).  The output of the location block is\na location of the user.\n(h) API/HTTP Request Block:\nIn some embodiments, the API/HTTP request block instructs the EFMS to execute an API or HTTP post to an internet-based service to provide status, alerts, and notifications regarding the current emergency.  The API or HTTP post may be provided by\nthe user or included in the Emergency Console.  In some embodiments, the inputs for the API/HTTP request block are a URL and any necessary parameters (named parameters included in HTTP post).  In some embodiments, the outputs of the API/HTTP request\nblock are (i) success or (ii) failure.\n(i) Find Next Available Contact Block:\nIn some embodiments, the find next available contact block instructs the EFMS to loop through a list of contacts (e.g. accounts associated with a user or emergency contacts), call each one-by-one in sequence, play an audio message to them and\nwait for confirmation to determine whether to call the next contact.  In some embodiments, a contact can confirm readiness to speak to an EDC or emergency dispatch center by responding to the audio message (e.g. by pressing 1).  In some embodiments, the\ncall of the find next available contact block is an interactive call (as discussed below).  In some embodiments, the input for the find next available contact block is a list of contacts, the list of contacts including phone numbers and names.  In some\nembodiments, the outputs of the find next available contact block are (i) contact answers the call, (ii) contact does not answer the call, and/or (iii) there are no available contacts (also referred to as an NAC response).\n(j) Interactive Call/IVR Block:\nIn some embodiments, the interactive call/IVR (interactive voice response) block instructs the EFMS to call a phone number (e.g. an account associated with a user) and play an audio message to the recipient of the call and wait for a dial tone\nresponse (e.g. an interactive call) to determine whether the recipient of the call confirms readiness to speak to an EDC or emergency dispatch center.  In some embodiments, the interactive call presents the recipient with a plurality of options to choose\nfrom (e.g. press 1 to dial 9-1-1, press 2 to call an emergency contact, press * to hang up).  In some embodiments, the inputs for the interactive call/IVR block are a name and associated phone number of the intended recipient of the call and an audio\nmessage to play to the recipient.  In some embodiments, the inputs for the interactive call include a plurality of options for the recipient to choose from.  In some embodiments, the outputs of the interactive call/IVR block are (i) a dial tone response\nfrom the recipient (ii) the call was answered or (iii) the call was unanswered.\n(k) Connect to Customer Call/Operations Center Block:\nIn some embodiments, the connect to customer/operations center block instructs the EFMS to initiate a call with an operations center associated with the administrator.  The input for the connect to customer call/operations center is a phone\nnumber of the customer call/operations center.  In some embodiments, the outputs of the connect to customer call/operations center are (i) successful connection to customer call/operations center or (ii) unsuccessful connection to customer\ncall/operations center.  In some embodiments, the call of the connect to customer call/operations center block is an interactive call (as described above).\n(l) Connect to 9-1-1 Block:\nIn some embodiments, the connect to 9-1-1 block instructs the EFMS to call 9-1-1 (or another emergency response/dispatch center number), add the call to a communication bridge, and provide the EDC with a location and name of a user.  The inputs\nfor the connect to 9-1-1 block are the location of the user and the name and phone number of the user.  The outputs of the connect to 9-1-1 block are (i) successful connection to 9-1-1 or (ii) unsuccessful connection to 9-1-1.\n(m) Add 3.sup.rd Party Block:\nIn some embodiments, the add 3.sup.rd party block instructs the EFMS to initiate a call with an additional party (e.g. an emergency contact, customer service, a suicide hotline, etc.) and add the call with the additional party to a communication\nbridge.  The inputs for the add 3.sup.rd party block are a name and number of a 3.sup.rd party.  The outputs of the add 3.sup.rd party block are (i) successful connection to 3.sup.rd party or (ii) unsuccessful connection to 3.sup.rd party.\n(n) Failsafe Block:\nIn some embodiments, the failsafe block instructs the EFMS to detect a failure within an emergency flow and deliver a message to a user notifying the user that the emergency flow has failed.  In some embodiments, the failsafe block further\ninstructs the API to prompt the user to call 9-1-1.  In some embodiments, the failsafe block is an implied emergency flow building block (e.g. the script defining the failsafe block is included in every emergency flow but the \"create emergency bridge\"\nblock is not depicted in the graphical user interface 673).  In some embodiments, the failsafe block is an implied additional or associated component of every emergency flow building block configured within an emergency flow.  In general, the failsafe\nblock functions to ensure that an emergency flow is at least as reliable as a traditional emergency call (e.g. calling 9-1-1 in the United States).  In some embodiments, the input for the failsafe block is a failed outcome of a previous or associated\nemergency flow building block (e.g. the previous or associated emergency flow building block failed to execute its intended function).  The failsafe block has no output.\nIn some embodiments, in addition to the emergency flow building blocks, the Emergency Console contains one or more utility building blocks.  For example, in some embodiments, utility building blocks may perform computational or logistical\nfunctions, as opposed to emergency functions.  For example, the utility building blocks may include a calculator building block configured to perform a mathematical equation on two inputs, a datetime building block configured to return the current day\nand time, an evaluate building configured to evaluate an expression (e.g. an algebraic expression), a compare building block configured to execute an if/then statement.  In some embodiments, the utility building blocks may include increase and decrease\nbuilding blocks configured to increase or decrease the value of a numerical variable, respectively.\nThe Emergency Console optionally contains any number of emergency flow building blocks defining any number of emergency response functions.  In some embodiments, additional emergency response functions include, but are not limited to, at least\none of the following: delivering a location of a user to an emergency dispatch center or database accessible by the emergency dispatch center, identifying an emergency dispatch center suitable for responding to an emergency alert based on location data\nassociated with or received from an electronic device associated with a user, calling an emergency contact of a user for whom an emergency alert has been received, calling an associated device of the user, and obtaining sensor data from a network of\nsensors associated with the user or user's electronic device.  In some embodiments, the Emergency Console allows administrators to edit the short script of an emergency flow building block to reprogram the building block to be more applicable to the\nneeds of the administrator.  For example, in some embodiments, the Emergency Console may contain a predefined call user block that takes a single phone number as an input.  In this example, the Emergency Console optionally allows an administrator to edit\nthe short script of the predefined call user block such that the edited call user block now takes a list of phone numbers as its input and dials each number in the list of phone numbers one-by-one in sequence until one of the numbers is successfully\nreached.  In some embodiments, the Emergency Console allows administrators to configure any parameter of an emergency flow building block, including, but not limited to: the input, output, and emergency response function.  In some embodiments, the\nEmergency Console allows administrators to design their own original emergency flow building blocks, such as by writing their own short script in the programming language input field 671.  In some embodiments, the Emergency Console includes a shared\n(e.g. accessible to all administrators) library of administrator generated emergency flow building blocks.\nExample Emergency Flows\nIn some embodiments, emergency flows are designed based on desired solutions.  As an example, a simple flow is configured to initiate a direct connection between the caller and 9-1-1.  In some embodiments, complex logical flows include multiple\ncallers or call centers with tunable timing, with text-to-speech and interactive voice response (IVR) components, with SMS messaging, with rich data interchange, etc. The EFMS service is designed to be modular so that the various building blocks, such as\nthe ones described above, can be assembled to construct an emergency flow that meets any particular emergency connectivity solution design.  In some embodiments, an emergency flow may be considered as a logic tree (e.g. a series of if/then statements\nthat progressively lead to various actions or decisions based on the outcome of the previous action or decision).  The following emergency flows examples depict exemplary emergency flows constructed by an administrator within the Emergency Console.\nFIG. 7 depicts an exemplary configuration of an emergency flow 700.  The emergency flow 700 may be designed for a user-triggered panic button (e.g. a soft button on a mobile application, for example).  FIG. 7 depicts the emergency flow 700, as\nit may appear in the graphical user interface 673 in some embodiments of the Emergency Console, as a configuration of emergency flow building blocks 701 and various accompanying connectors 703.  In some embodiments, connectors 703 are used to define the\npath or sequence of execution of the emergency flow building blocks 701.  In some embodiments, emergency flow 700 begins with emergency flow activation block 702.  The emergency flow activation block 702 can be triggered in various ways, such as by the\nEFMS 340 receiving an emergency alert including an emergency flow ID corresponding to emergency flow 700 from a triggering device 367 associated with a user at the API module 341; the API module 341 referencing the emergency flow ID with the data module\n343, identifying emergency flow 700 as the emergency flow corresponding to the emergency flow ID, and delivering the emergency flow 700 to the core module 342 for execution; and the core module 342 executing the emergency flow 700.  In some embodiments,\nthe emergency flow activation 702 is an emergency flow building block.  In some embodiments, the emergency flow activation 702 is an implied emergency flow building block.  In some embodiments, the emergency flow activation 702 is functionally equivalent\nto a \"create emergency bridge\" block, as described above.\nIn some embodiments, the emergency flow 700 continues with call user interactive block 704a, an interactive call/IVR block that instructs the EFMS to deliver an interactive call (as described above) to a phone number associated with a user.  The\ninteractive call optionally plays an audio message to the user and prompts the user to select one of a plurality of options.  In some embodiments, the plurality of options includes connect to an EDC (e.g. \"press 1 for 9-1-1) or disconnect (e.g. press *\nto end this call).  In some embodiments, the plurality of options additionally or alternatively includes an option to deliver a message to an emergency contact associated with the user (e.g. press 2 to alert an emergency contact).  In some embodiments,\nthe option to deliver a message to an emergency contact prompts the EFMS to deliver a text message (e.g. an SMS or MMS message) to an emergency contact.  In some embodiments, the interactive call/IVR block additionally includes at least one default input\nparameter such as a timeout criterion (e.g. user does not provide a selection before the expiration of 30 seconds), an NA response (e.g. the phone call is not answered), or a hang up response (e.g. the user answers the call but hangs up).\nAfter the call user interactive block 704a, in some embodiments, as depicted in FIG. 7, the emergency flow 700 proceeds in one or more directions, dependent upon the output of the call user interactive block 704a (e.g. the emergency flow\nbuilding blocks shown in emergency flow 700 optionally form parallel emergency flow pathways for both calling the emergency responder 708 and messaging a contact 706).  In some embodiments, when the output of the call user interactive block 704a is a\nuser command or selection to connect to an EDC, the emergency flow 700 continues with call EDC block 708.  In some embodiments, when the output of the call user interactive block 704a is a user command or selection to deliver a message to an emergency\ncontact, or the interactive call timed out (e.g. the timeout criteria was satisfied), or there was an NA response, the emergency flow 700 continues with call contact block 706.  In some embodiments, when the output of the call user interactive block 704a\nis the user selected to disconnect, the emergency flow 700 terminates.\nIn some embodiments, the emergency flow 700 continues with call contact block 706, a find next available contact block (as described above) that instructs the EFMS to loop through a list of contacts associated with the user and call each\none-by-one in sequence until one of the contacts answers.  In some embodiments, the calls prompted by the call contact block 706 are interactive calls that play an audio message that prompts the recipients of the calls to select one of a plurality of\noptions.  In some embodiments, the plurality of options includes an option to connect to an EDC or to disconnect.  In some embodiments, the call contact block 706 additionally includes a default input such as an NAC response (e.g. there are no available\ncontacts, or all of the listed contacts have been called and none answered) or a hang up response.\nAfter the call contact block 706, in some embodiments, as depicted in FIG. 7, the emergency flow 700 proceeds in one or more directions, dependent upon the output of the call contact block 706.  In some embodiments, when the output of the call\ncontact block 706 is a contact selected to connect to an EDC, the emergency flow 700 continues with call EDC block 708.  In some embodiments, when the output of the call contact block 706 is the contact selected to disconnect or there was an NAC\nresponse, the emergency flow 700 continues with call user interactive block 704b.  In some embodiments, when the output of the call contact block 706 is the contact selected to disconnect, a hang up response, or there was an NAC response, the emergency\nflow 700 terminates.\nIn some embodiments, the emergency flow 700 continues with call user interactive block 704b, an interactive call/IVR block.  In some embodiments, call user interactive block 704b functions the same as call user interactive block 704a, described\nabove.  In some embodiments, the interactive call executed by call user interactive block 704b takes different inputs and produces different outputs than those of call user interactive block 704a.  After the call user interactive block 704b, the\nemergency flow 700 proceed in one or more directions, depending upon the output of the call user interactive block 704b.  In some embodiments, when the output of the call user interactive block 704b is a user command or selection to disconnect, or a hang\nup response, or an NA response, or the interactive call timed out, the emergency flow 700 terminates.  In some embodiments, when the output of the call user interactive block 704b is the interactive call timed out or an NA response, the emergency flow\n700 repeats the call contact block 706.  In some embodiments, when the output of the call user interactive block 704b is a user command or selection to connect an EDC, the emergency flow 700 continues with call EDC block 708.\nIn some embodiments, the emergency flow 700 continues with call EDC block 708, a connect to 9-1-1 block (as described above) that instructs the EFMS to call an emergency response or emergency dispatch center.  In some embodiments, once the EDC\nhas answered the call and been added to the communication bridge, the emergency flow 700 ends.  In some embodiments, after answering the call, an EDC selects an option to query the location of the user.\nA section of an example of an emergency flow script corresponding to emergency flow 700 is shown below.  In the example shown below, an interactive call block ([0165]-[0183]) is shown first, followed first by a \"connect to 9-1-1\" block\n([0184]-[0191]) and then by a \"find next available contact\" block ([0192]-[0210]).  The following example should not be considered a complete emergency flow.\nExample Emergency Flow Script\nstates:\nTABLE-US-00002 - name: call_user action: INTERACTIVE_CALL inputs: phone_number: `${user.phone_number}` text: &gt;- \"This is an automated call from ${company}.  If this is a real emergency, dial one to talk to a nine one operator and receive\nhelp.  If this is a false alarm, please dial star to cancel your request for help.  You have activated the emergency feature in ${company}.  Please dial one to talk to a nine one one operator and receive help.  If this is a false alarm, please dial star\nto cancel your request for help.  If you do not press one for help or star to cancel, we will call your emergency contacts and prompt them to speak to nine one one in your place.  This call will now disconnect.  \" on_output: `#1@.*`: user_call_911_play\n`#\\*@.*`: user_cancelled_911 HANGUP@.*: user_hangup_sms `false`: find_available_contact name: user_call_911_play action: PLAY inputs: text: `Now calling nine one one...` phone_number: `${user.phone_number}` on_output: `true`: user_call_911_sms goto:\nfail_safe name: find_available_contact action: CALL_LOOP inputs: callee_list: contacts text: &gt;- \"This is an emergency call from ${company}.  ${user.full_name} has designated you as an emergency contact in the app. The user has activated the emergency\nprocedure, dial one to speak to a nine one one operator closest to ${user.full_name}'s location and send help to his exact location, or hang up to disconnect this call.  If you do not dial one to connect to a nine one one operator, we will call\n${user.full_name}'s other emergency contacts if he has provided any.  This call will now disconnect.  \" time: 120 store: found_contact on_output: `#1@.*`: contact_call_911_play `false`: user_call_final_911_call goto: fail_safe\nIn the example shown above, after the emergency flow is activated, the emergency flow begins by executing an interactive call block (e.g. call user interactive block 704a).  A call is placed to {user.phone_number} (e.g. a phone number included\nin or associated with the emergency alert that activated the emergency call flow) and the message shown in paragraphs [0170]-[0178] is dictated to the recipient of the call as a text-to-speech (TTS) message.  As shown above, if the recipient selects \"1\",\nthe emergency flow continues by executing a connect to 9-1-1 block (e.g. call emergency responder block 708).  If the recipient selects \"*\", the emergency flow ends.  If the recipient does not submit a response, the emergency flow continues by executing\na find next available contact block (e.g. call contact block 706).\nFIG. 8 depicts an exemplary configuration of an emergency flow 800.  FIG. 8 depicts the emergency flow 800, as it may appear in the graphical user interface 673 in some embodiments of the Emergency Console, as a configuration of emergency flow\nbuilding blocks and various accompanying connectors.  In some embodiments, emergency flow 800 begins with emergency flow activation block 802.  Similar to emergency flow activation block 702, emergency flow activation block 802 may be triggered in\nvarious ways and initiates the emergency flow 800.  Similar to emergency flow activation block 702, in some embodiments, the emergency flow activation block 802 is an implied emergency flow building block.  In some embodiments, emergency flow activation\nblock 802 is functionally equivalent to a create emergency bridge block (as described above).\nThe emergency flow 800 continues with call operations center block 812, a connect to customer call/operations center block that instructs the EFMS to call an operations center associated with the administrator of the emergency flow (as described\nabove).  In some embodiments, the call is an interactive call that plays an audio message to the recipient of the call (e.g. the operations center representative).  In some embodiments, the interactive call gives the recipient the option to call an EDC\n(e.g. press 1 to dial 9-1-1).  In some embodiments, if the recipient hangs up or does not answer the call, the emergency flow 800 ends.  In some embodiments, if the recipient selects to call an EDC, the emergency flow 800 continues with call EDC block\n808, a connect to 9-1-1 block (as described above) that instructs the EFMS to call an EDC or emergency dispatch center.  In some embodiments, once the EDC has answered the call and been added to the communication bridge, the emergency flow 800 ends.\nIn one example of one embodiment of the emergency flow 800, the administrator is a social media network (e.g. Facebook).  In this example, the social media network monitors activity on the network to determine potential emergencies.  If a user\nof the social media network posts (e.g. a user post) a status message that reads \"Help, someone is trying to break into my apartment!\" a representative of the social media network may receive a notification of the status message and determine whether or\nnot the user is experiencing a potential emergency.  As another example, when a user of the social media network streams a live video of themselves talking about suicide or attempting to commit suicide, a representative of the social media network\nreceives a notification of the live video and determines whether or not the user is in potential danger.  In some embodiments, detection of potential emergencies is based on text-based analysis of written user posts or audio/visual-based analysis of user\nposted videos.  In this example embodiment, the social media network has accessed the Emergency Console, created the emergency flow 800, and integrated an API call into their system that delivers an emergency alert to the EFMS when activated.  While\nmonitoring the social media network for potential emergencies, a representative of the social media network receives a notification about a live streaming video session in which a user is talking about ending his or her own life.  The representative\nquickly determines that the user is in potential danger, and clicks a button to send an emergency alert to the EFMS, which accordingly executes the emergency flow 800.  First, the EFMS creates an emergency bridge at emergency flow activation block 802\nand delivers a call to the representative of the social media network at call operations center block 812.  The representative presses 1 to dial 9-1-1, prompting the EFMS to deliver a call to an EDC and add the call to the emergency bridge at call EDC\nblock 808.  The representative gives the location of the user (determined by the social media network) to the EDC who then dispatches an emergency vehicle to the location of the user.\nFIG. 9 depicts an exemplary configuration of an emergency flow 900, in some embodiments.  FIG. 9 depicts the emergency flow 900, as it may appear in the graphical user interface 673 in some embodiments of the Emergency Console, as a\nconfiguration of emergency flow building blocks and various accompanying connectors.  In some embodiments, the emergency flow 900 begins with an emergency flow activation block 902 and continues with call user interactive block 904, which function\nsimilarly to emergency flow activation block 702 and call user interactive block 704a, respectively, as described above with respect to FIG. 7.  In some embodiments, the emergency flow 900 continues with \"connect operator\" block 912, a connect to\ncustomer call/operations center block (as described above) that functions similarly to call operations center block 812, as described above with respect to FIG. 8.  In some embodiments, \"connect operator\" block 912 delivers an interactive call that plays\nan audio message to a recipient (e.g. a customer service representative) that prompts the recipient to select one of a plurality of options.  In some embodiments, the plurality of options includes options to query the location of a user, call a contact\nassociated with a user, and call an EDC.  In some embodiments, if the recipient of the interactive call hangs up (e.g. the customer service representative determines that the emergency alert was falsely triggered or not meriting further response), the\nemergency flow 900 ends.  If the recipient of the interactive call setup by the \"connect operator\" block 912 selects to query the location of a user, the emergency flow 900 continues with identify location block 914, a location block that instructs the\nEFMS to query or detect the location of a user, as described above.  If the recipient of the interactive call setup by the \"connect operator\" block 912 selects to call a contact associated with a user, the emergency flow 900 continues with call contact\nblock 906, which functions similarly to call contact block 706, as described above with respect to FIG. 7.  If the recipient of the interactive call setup by the \"connect operator\" block 912 selects to call an EDC, the emergency flow 900 continues with\ncall EDC block 908, which functions similarly to call EDC block 708, as described above with respect to FIG. 7.  In some embodiments, after the recipient of the interactive call setup by the connect operator 912 selects an option from the plurality of\noptions, the emergency flow 900 ends.  In some embodiments, the recipient may select two or more of the options from the plurality of options before the emergency flow 900 ends.  In some embodiments, if the recipient of the interactive call setup by the\n\"connect operator\" block 912 selects to call an EDC, the user called in call user interactive block 904 is removed from the communication bridge by the EFMS before the EDC is called.  In some embodiments, if the recipient of the interactive call setup by\nthe \"connect operator\" block 912 selects to call an EDC, the recipient of the interactive call setup by the \"connect operator\" block 912 is removed from the communication bridge by the EFMS before the EDC is called.\nIn one example of one embodiment of the emergency flow 900, the administrator of the emergency flow is again the administrator of the taxi cab coordinating service (e.g. CurbCall) of the example described above with respect to FIG. 7.  Again, in\nthis example, the administrator accesses the Emergency Console, creates the emergency flow 900, and integrates an API call that delivers an emergency alert to the EFMS into a panic button within a mobile application developed by the taxi cab coordinating\nservice.  A rider with the mobile application installed on his or her smart phone gets into a taxi cab under the management of the taxi cab coordinating service.  In this example, the taxi cab driver accidentally runs a red light and the taxi is hit on\nthe left by a truck.  The driver is severely injured and unconscious.  The rider is conscious but injured and cannot escape the vehicle.  To get help, the rider presses the panic button on the mobile application, which sends an emergency alert to the\nEFMS, which accordingly executes the emergency flow 900.  First, the EFMS creates a communication bridge at emergency flow activation block 902 and calls the rider at call user interactive block 904.  The rider answers and receives an audio message that\nrecites \"press 1 for help.\" The rider presses 1, prompting the EFMS to call the taxi cab coordinating service's customer service center and bridges a call between the rider and a customer service representative of the taxi cab coordinating service.  The\nrider tells the customer service representative that the rider and the driver have been in an accident and need emergency help.  The customer service representative then queries the location of the rider at identify location block 914, calls 9-1-1 at\ncall EDC block 908, and calls one of the rider's emergency contacts at call contact block 906.\nFIG. 10 depicts an exemplary configuration of an emergency flow 1000.  FIG. 10 depicts the emergency flow 1000, as it may appear in the graphical user interface 673 in some embodiments of the Emergency Console, as a configuration of emergency\nflow building blocks.  In some embodiments, the emergency flow 1000 begins with emergency flow activation block 1002, which functions similarly to emergency flow activation block 702, as described above with respect to FIG. 7.  In some embodiments, the\nemergency flow 1000 continues with \"connect operator\" block 1012, a connect to customer call/operations center block (as described above) that functions similarly to call operations center block 812, as described above with respect to FIG. 8.  In some\nembodiments, \"connect operator\" block 1012 delivers an interactive call that plays an audio message to a recipient (e.g. a customer service representative) that prompts the recipient to select one of a plurality of options.  In some embodiments, the\nplurality of options includes options to query the location of a user, call a contact associated with a user, call a user, and call an EDC.  If the recipient of the interactive call setup by the \"connect operator\" block 1012 selects to query the location\nof a user, the emergency flow 1000 continues with identify location block 1014, a location block that instructs the EFMS to query or detect the location of a user, as described above.  If the recipient of the interactive call setup by the \"connect\noperator\" block 1012 selects to call a contact associated with a user, the emergency flow 1000 continues with call contact block 1006, which functions similarly to call contact block 706, as described above with respect to FIG. 7.  If the recipient of\nthe interactive call setup by the \"connect operator\" block 1012 selects to call an EDC, the emergency flow 1000 continues with call EDC block 1008, which functions similarly to call EDC block 708, as described above with respect to FIG. 7.  If the\nrecipient of the interactive call setup by the \"connect operator\" block 1012 selects to call a user, the emergency flow 1000 continues with call user block 1016, a call user block (as described above) that instructs the EFMS to initiate a phone call to a\nnumber associated with the user and connect the phone call to the communication bridge.  In some embodiments, the emergency flow 1000 continues with send SMS block 1018, a send SMS message block (as described above) that instructs the EFMS to deliver a\nSMS message to a user.  In some embodiments, the emergency flow 1000 continues with send SMS block 1018 after a contact associated with a user is successfully contacted at call contact block 1006.  In some embodiments, the SMS message includes a web link\nto a video stream (e.g. a live video stream of a user or an emergency event).  In some embodiments, at send SMS block 1018, the EFMS additionally or alternatively delivers a MMS message or a message through an internet-based messaging platform (e.g.\nemail, social media apps, messaging apps).\nIn one example of one embodiment of the emergency flow 1000, the administrator of the emergency flow is again the administrator of the social media network (e.g. Facebook) of the example described above with respect to FIG. 8.  In this example,\nthe administrator of the social media network has accessed the Emergency Console, created the emergency flow 1000, and integrated into their system an API call that delivers an emergency alert to the EFMS when a user of the social media network is\ndetected to be in an emergency situation.  In this example, the social media network has detected (e.g. using audio or visual analysis tools or software) that a user of the social media network has initiated a live streaming video session in which the\nuser is contemplating committing suicide.  In response, the social media network activates the emergency flow 1000 by sending an emergency alert to the EFMS, which executes the emergency flow 1000 accordingly.  First, the EFMS calls a suicide hotline at\n\"connect operator\" block 1012 to connect to a suicide hotline operator.  The EFMS plays an audio message to the operator that gives the operator the option to call the user of the social media network at call user block 1016.  The operator is optionally\ngiven the option to query the location of the user of the social media network at identify location block 1014, call an emergency contact associated with the user of the social media network at call contact block 1006, or call an EDC at call EDC block\n1008.  The operator chooses the option to call one of the social media network user's emergency contacts at call contact block 1006 and then chooses the option to deliver a text message to the emergency contact that includes a link to the live streaming\nvideo session.\nFIG. 11 depicts an exemplary configuration of an emergency flow 1100 as it may appear in the graphical user interface 673 in some embodiments of the Emergency Console.  As shown in this exemplary emergency flow, the TTS (text-to-speech) scripts\nfor users and emergency contacts are used until a live caller gives an indication to initiate an emergency call.\nIn some embodiments, as depicted in FIG. 11, the potential outputs of emergency flow building blocks are graphically represented in the graphical user interface 673, including the directions of the emergency flow determined by different outputs. The exemplary emergency flow of FIG. 11 comprises the following steps (labeled by number):\nTABLE-US-00003 .cndot.  Call user play TTS [Interactive Call Block 1102a] .cndot.  If user answers and accepts by pressing \"1\" .smallcircle.  Call 911 [Connect to 9-1-1 Block 1108] .smallcircle.  Send notification SMS to user [3, described\nbelow]; END of Flow .cndot.  Else .smallcircle.  If user answers and cancels by pressing \"*\" .box-solid.  Send notification SMS to user [Send SMS Message Block 1118b]; END of Flow .smallcircle.  If user answers and hangs up .box-solid.  Send notification\nSMS to user [Send SMS Message Block 1118a] .smallcircle.  If user does not answer or answers and hangs up .box-solid.  Find next available emergency contact (loop through) [Find Next Available Contact Block 1106] play TTS .box-solid.  If contacts answers\nand accepts by pressing \"1\" .cndot.  Send notification SMS to user [Send SMS Message Block 1118c] .cndot.  Send notification SMS to contact [Send SMS Message Block 1118d] .cndot.  Call 911 [Connect to 9-1-1 Block 1108] .cndot.  Send notification SMS [3];\nEND of Flow .box-solid.  Else .cndot.  Call user again [Interactive Call Block 1102b] .cndot.  If user answers and accepts by pressing \"1\" .smallcircle.  Call 911 [Connect to 9-1-1 Block 1108] .smallcircle.  Send notification SMS [3]; END of Flow .cndot. Else .smallcircle.  Send notification SMS [Send SMS Message Block 1118e]; END of Flow\nIn some embodiments, the exemplary emergency flow of FIG. 11 comprises the following SMS messages (labeled by number):\n[3] To User: \"Revolar: John Smith has been connected to the nearest 9-1-1 center.\"\n[1118a] To User: \"Revolar: A call from 9-1-1 was disconnected.  If you still need to speak to the nearest 9-1-1 operator, press the big red panic button on our app.\"\n[1118b] To User: \"Revolar: John Smith has canceled his request for help from 9-1-1.  No further action is needed.\"\n[1118c] To User: \"Revolar: Your contact Jane Smith has spoken to a 9-1-1 operator in response to your emergency.  Help is on its way.\"\n[1118d] To Contact: \"Revolar: Help is on its way to John Smith from 9-1-1.\"\n[1118e] To User: \"Revolar: We have attempted to call you at 6:00 am.  If you need to speak to the nearest 9-1-1 operator, press the panic button.\nIn some embodiments, the exemplar flow of FIG. 11 comprises the following TTS scripts (labeled by number):\n[1102a] \"This is an automated call from Revolar.  If this is a real emergency dial \"1\" to talk to a 9-1-1 operator and receive help.  If this is a false alarm, please dial `*` to cancel your request for help.  You have activated the emergency\nfeature in Revolar.  Please dial \"1\" to talk to a 9-1-1 operator and receive help.  If this is a false alarm, please dial `*` to cancel your request for help.  If you do not press `1` for help or `*` to cancel, we will call your emergency contacts and\nprompt them to speak to 9-1-1 in your place.  This call will now disconnect.\"\n\"This is an emergency call from Revolar.  John Smith has designated you as an emergency contact in the app. The user has activated the emergency procedure, dial `1` to speak to a 9-1-1 operator closest to John Smith's location and send help to\nhis exact location, or hang up to disconnect this call.  If you do not dial `1` to connect to a 9-1-1 operator, we will call John Smith's other emergency contacts if he has provided any.  This call will now disconnect.\"\n[1102b] same as [1102a]\nIn certain embodiments, disclosed herein are communication devices, systems and methods for connecting users with dispatch centers for emergency response.  FIG. 12 depicts embodiments of the (i) communication device (e.g. a triggering device)\nand (ii) emergency management system.  In some embodiments, the device 1267 is a digital processing device such as a communication device (e.g. mobile or cellular phone, computer, laptop, etc.).  In some embodiments, a communication device is a wearable\ndevice.  In some embodiments, a communication device is a wireless mobile device or a smart phone.  In some embodiments, a communication device is a walkie-talkie or a two-way radio.  In some embodiments, a user 1200 (not shown) is the primary user of\nthe device 1267.\nIn some embodiments, the device 1267 comprises at least one of a processor 1204, a memory 1206 (e.g. an EPROM memory, a RAM, a solid-state memory), a display 1202 (not shown), a user interface 1263, a network component 1264 (e.g. an antenna and\nassociated components, Wi-Fi adapters, Bluetooth.RTM.  adapters, etc.), a microphone 1218, speakers 1221 and a computer program 1209 (e.g. mobile application, server application, computer program, application) with an exemplary software module 1269.  In\nsome embodiments, the device 1267 is equipped with a location component 1216 (not shown), for example, a global positioning system (GPS).  In some embodiments, the device comprises data storage 1215.  In some embodiments, the device comprises at least\none of a location data cache 1217 and a user data cache 1219.\nIn some embodiments, the device 1267 has several components including a display 1202 (not shown) and user interface 1263, which allow the user 1200 to interact with the device 1267.  In some embodiments, the display 1202 is a part of the user\ninterface 1263 (e.g. a touchscreen is both a display and provides an interface to accept user interactions).  In some embodiments, the display 1202 and/or the user interface 1263 comprises a touch screen (e.g. a capacitive touch screen), which is capable\nof displaying information and receiving user input.  In some embodiments, the device 1267 comprises hardware components not including a display 1202 and a user interface 1263, wherein the device functions autonomously without requiring active user\nguidance or interaction.  In some embodiments, the computer program 1209 includes a voice or speech recognition module (not shown) for taking voice commands.  In some embodiments, the computer program 1209 comprises at least one software module 1269 for\ncarrying out one or more instructions.\nIn some embodiments, a device 1267 includes various accessories 1222 that allow additional functionality.  In some embodiments, the accessories 1222 include one or more of the following: a camera 1287 (not shown) (e.g. for input of gestures\ncommands or pictures from the user 1200), one or more sensors 1277 (not shown) such as a smoke detector or a heart monitor, USB/micro-USB port, headphone jack, a card reader, SIM card slot, and any combination thereof.\nIn some embodiments, a device 1267 autonomously detects emergencies based on data from various sources such as the sensors 1277 (sensor data) or sound detection picked up from the microphone 1218 or another accessory (e.g. smoke detector).  For\nexample, in some embodiments, a device autonomously detects an emergency based on sensor data when sensor readings or values exceed a threshold (e.g. a predefined threshold set by the device software by default, by a user, or by an EMS) or fall outside a\nthreshold range.  In some embodiments, the device 1267 may obtain relevant data from an associated device for the device 1267 to trigger the emergency.\nFIG. 12 also shows a schematic diagram of one embodiment of an emergency management system 1230 as described herein.  In some embodiments, the emergency management system 1230 comprises one or more of an operating system 1232, at least one\ncentral processing unit or processor 1234, a memory unit 1236, a communication element 1238, and a software application 1248 (e.g. server application).  In some embodiments, the emergency management system 1230 comprises one or more databases 1235, which\nmay also be a component of the \"emergency clearinghouse.\" In some embodiments, the emergency management system 1230 comprises at least one of a location database 1237 and a user information database 1239.  In some embodiments, the software application\n1248 comprises at least one software module 1249 for carrying out one or more instructions.\nIn some embodiments, the EMS 1230 includes an associated database 1245 where associated devices, emergency contacts, or associated accounts are stored for the device 1267 and/or the user 1200.  After an emergency alert has been received, the\ndatabase 1245 may be searched for information associated with the device 1267 using an account identifier (e.g. phone number, user name, device ID, etc.).  In some embodiments, the user 1200 had established an account when the triggering device 1267 has\nbeen deployed to detect emergencies.  For example, the user 1200 may have added emergency contacts, a secondary user or owner of the device 1267 (such as a spouse or a corporate representative), associated devices (e.g. other devices owned by the user or\ndevices owned by a secondary user or owner) a location for the device.  In some embodiments, an organization or a corporate entity provides device information for a corporate representative who can be contacted in case of an emergency.  In some\nembodiments, the database 1245 includes a prioritized list of associated devices or emergency contacts for the device 1267.  As shown, the associated database 1245 may be incorporated in the databases 1235 or it may be a standalone database or maintained\nby a third-party or corporate client.\nFIG. 13 depicts an embodiment of a system for connecting with other devices after an emergency alert has been triggered (similar to FIG. 2).  As shown, a triggering device 1367 sends the emergency alert, which is a request for assistance to\nrespond to an emergency, to an Emergency Management System or EMS 1330.  Ultimately, the EMS 1330 will forward the alert as a request for assistance and connect the triggering device 1367 with one or more dispatchers 1350.  The dispatcher 1350 may be an\nEmergency Dispatch Center (EDC) or a private dispatcher such as corporate security, university police, towing service, or a private security service.\nIn some embodiments, the triggering device 1367 includes a network component 1364 (e.g. a Wi-Fi antenna) which sends the emergency alert.  The emergency alert may be sent in various ways via the internet, cellular or landline network.  In some\nembodiments, the triggering device 1367 includes location data within its storage 1315 or in a database in the EMS 1330 (e.g. databases 1335).\nIn some embodiments, the triggering device 1367 includes a computer program 1369 for sending the emergency alert.  The computer program 1369 may be pre-installed on the device or may have been loaded and installed by the user (e.g. user 1300). \nUser 1300 may have followed through a setup or registration process for the device 1367, where he or she has provided user data such as emergency contacts (e.g. phone numbers, email addresses, messaging IDs), user information, location (e.g. a physical\naddress of the location of the device 1367), etc. In some embodiments, user data, location data, emergency data, etc., may be saved in data cache or storage 1315 in the device 1367.  In other embodiments, the data may be saved in one or more databases\n1335 in the EMS, in third-party servers or in cloud-based systems.  In some embodiments, data regarding associated devices, associated accounts and/or emergency contacts may be saved in databases 1335 (location database 1337, user database 1339 or\nassociated database 1345 (not shown)).  The data may be protected by password protection, authentication protocols for transmission, encryption, use of secured pathways, and other methods for limiting the risk of security breaches.\nIn some embodiments, the emergency is triggered by user input, which may involve the user (e.g. user 1300) interacting with the user interface 1363 of the triggering device 1367.  In some cases, the user 1300 may press one or more hard or soft\nbuttons on the user interface 1363.  However, other types of user interactions such as touch, tap, pattern of touches, gesture, voice-activation, etc. are contemplated.\nIn some embodiments, the triggering device 1367 may autonomously detect emergency situations or likely emergency situations.  The triggering device 1367 may send an alert based on autonomously detected emergencies using one or more sensors (not\nshown) such as from a smoke alarm in a building.\nIn some embodiments, the triggering device 1367 may be a communication device such as a mobile phone, a computer, a wearable device (e.g. a smart watch), a digital assistant, smart speakers, etc. If the triggering device 1367 is a mobile phone\nor another device with a cellular connection, the emergency alert may be sent via a cellular network.  After the alert is sent to the EMS 1330 via communication link 1322, the EMS 1330 may initiate a voice call along two-way communication link 1324.  In\nsome cases, communication link 1324 is a data link, which may support a data call such as VoIP.  Link 1324 may be used for sending data such as user data, location data, emergency data, text, images, and video from the triggering device 1367.  In some\nembodiments, communication link 1324 is established via landline, cellular network or the Internet.  In some embodiments, the communication link 1324 is through VoIP, both voice and data can be transmitted in the same path (e.g. in the SIP signaling, as\nheaders or in multi-part messages).  In some embodiments, the communication link 1324 may be an analog voice call over landline or cellular network and a data link for transferring data via Wi-Fi, cellular data, etc. Generally, data links are preferred\nfor transmission of both voice and data whenever they are available, and the signal strength is good.  In certain cases, the communication link may be arranged through NG911 systems or where the data is sent through SIP signaling.\nIn some embodiment, the EMS 1330 may include one or more databases 1335 housed in one or more servers in the same or in a remote location.  In some embodiments, a location database 1337 may house location data regarding the location of the\nemergency.  In some embodiments, a user database 1339 may house user data and/or emergency data (such as an emergency contact list 1365 of associated devices, emergency contacts or associated contacts).  In other embodiments, the location, user and/or\nemergency data (such as list 1365) may be saved on data cache 1315 in the triggering device 1367 or in data storage in other devices such as mobile phone 1306, computer 1346, or mobile phone 1316 (mobile phone of a secondary user user), etc. In other\nembodiments, the data may be saved online in one or more remote servers or cloud-based systems.\nIn some embodiments, the emergency contact list 1365 may be entered by user 1300 at the time of registration or installation of the computer program 1369 or at another time.  The emergency contact list 1365 may be a list of phone numbers, email\naddresses, IP addresses, MAC addresses, etc. In particular, computing devices which are not associated with a phone number (such as computer 1346) may be identified by an IP address, MAC address, URLs or SSIDs.\nIn some embodiments, the emergency contact list 1365 is a list with at least one contact (e.g. phone number, email address, messaging ID, device ID, etc.).  In some embodiments, the list 1365 is a non-prioritized list with more than one contact\nwherein attempts to connect to the contacts can be made in any order or even concurrently.  For example, attempts can be made to connect with two contacts simultaneously and continue with whoever picks up or whoever agrees to make the emergency call,\netc.\nIn some embodiments, the EMS 1330 may try to connect to a user using one or more communication links (e.g. 1324, 1332, 1334, 1336) before sending the emergency alert to one or more dispatchers 1350.  In some embodiments, the EMS will send the\nalert indirectly to a dispatcher (e.g. a PSAP).  The EMS may send the alert to an intermediary (e.g. other public emergency response agencies, private security or emergency response service providers, emergency contacts, etc.), which in turn will contact\nthe PSAP.  The EMS 1330 may attempt to connect to user 1300 using communication device 1306 or tablet 1346.  If unsuccessful or simultaneously, the EMS 1330 may connect to user 1305 via communication device 316 (e.g. an emergency contact, a secondary\nuser of device 1367, a corporate representative).  A direct connection 1338 between device 1316 and dispatcher 1350 may be established or the user 1305 may be connected via the EMS 1330 via links 1336 and 1326.  Further details may be found in FIGS. 14\nand 15.\nIt is contemplated that a wide range of devices may be the triggering device 1367.  Thus, the device 1367 be a simple device with limited capability (e.g. a smoke alarm, IoT device), possess a wide range of components or functionality (e.g. a\nsmart phone) or device with some basic functionality (e.g. a wearable device, a smart car console, a personal digital assistant).  Thus, the methods and systems herein are designed to handle emergency alerts from a variety of triggering devices through\nits emergency flows.\nFIG. 14 illustrates an exemplary method for an emergency management system to communicate with dispatchers for assistance after an emergency alert has been triggered.  Once the alert has been triggered (act 1412), the EMS attempts to connect to\na user through the triggering device.  If the triggering device has capabilities to connect to a user, the EMS may be able to connect to a user (act 1414).  For example, the triggering device may include an audio system with a microphone and speaker to\nhost a call with a user.  In some embodiments, the triggering device includes a user interface and display for hosting an SMS exchange or another type of messaging with a user.  In some embodiments, the EMS may evaluate whether the triggering device\nincludes compatible software for hosting a connection with EMS.  In some cases, the EMS decides not to connect to a triggering device if a responsive user is not likely available at that location (e.g. a burglar alarm in a smart home, car alarm in a\nparked vehicle, etc.).  In some embodiments, the connection to the triggering device is not successful.  For example, if an emergency is triggered by the control console in a smart home with an audio system, the EMS may attempt to reach a user there.  In\nsome cases, a user is unresponsive or not in the home and the attempt to connect will fail.  The evaluation of the capabilities of the triggering device may be conducted at the time of registration or setup of the account.  In some embodiments, the\nevaluation is done in real time after the emergency has been triggered.\nIt is contemplated that there could be variations in the decision to be made in step 1414.  For example, the decision could be whether the device that has triggered the emergency should be called or not based on user or administrator\npreferences, which may be established during registration.  In some embodiments, an organization or company policy may dictate whether a particular device should be called first or not.  Thus, information about whether to attempt to call the triggering\ndevice may be saved on the device or in one or more databases in the EMS.  For example, the EMS looks up the device (using a device or user identifier) in the database to check if it should call the device or skip to the next step.  In other embodiments,\nthe decision may depend on the type of device, type of emergency, likelihood that someone will be available, etc.\nIn some cases, an audio call is not preferred because of a variety of reasons and other connections (such as SMS or instant messaging, HTTP messages, push notifications, etc.) may be attempted.  For example, the user may be unable to talk\nbecause of presence of an intruder, disability, language difficulties, injury, age, etc. In some cases, voice communication may not be efficient to transfer all the information relevant for the emergency.  When signal strength is weak, or bandwidth is\nnot sufficient to maintain a good quality voice connection, non-voice communication (such as SMS) may be preferred.  In contrast, audio calls may be required in some jurisdictions where existing and legacy PSAP systems can only accept voice calls and\nhave limited capacity to accept other formats.  The emergency alert may be sent with location information as described in FIG. 17.\nFor example, the triggering device may be a smart speaker installed with a digital assistant and able to host a connection with the EMS (directly via an internet connection or indirectly through another device such a smart phone or router). \nOnce the EMS is connected to a user (act 1416 & 1418), the EMS may request user input to agree to be connected to a dispatcher (act 1422) or give other instructions (act 1424).  For example, the screen may indicate that user's car has been involved in a\ncrash and the user is allowed to press a button to connect to emergency dispatchers.  Based on the user input, the user may be connected to a dispatcher (act 1442).  An appropriate dispatcher may be chosen based on various factors including type of\nemergency, location of emergency, severity and likelihood of emergency, etc. Specifically, the EMS may respond differently when the severity of the emergency is high (e.g. when a person's life may be at risk in after a car crash has been detected) versus\na low severity incident (e.g. a car alarm has been triggered for an unattended parked car).  For example, if the severity of the emergency is low, the EMS may send notifications to emergency contacts when an emergency is triggered.\nIn some cases when the user is unresponsive or did not request to connect to dispatcher, an appropriate dispatcher may be contacted (e.g. by dialing an emergency number and playing a Text-to-Speech (TTS) message) (act 1426 and 1428).  The\nmessage may inform the dispatcher that the type and location of the emergency.  In some embodiments, when the user is not responsive, non-human users may be prohibited from calling emergency dispatchers in some jurisdictions or services.  In those\njurisdictions, notification messages may be sent to the associated devices, devices of emergency contacts or friends and family groups, a third-party monitoring service (e.g. OnStar), nearby devices and/or \"recently contacted\" devices, etc. (act 1432). \nThe EMS may consider various factors such as the existing laws (e.g. non-human users may be prohibited from calling emergency dispatchers in some jurisdictions), user preferences, severity and likelihood of the emergency, etc. Notification messages may\nbe sent at various points including when the emergency is triggered (act 1412) or when the emergency call is made (act 1428).  The messages may be sent simultaneously to several devices through text messages or SMS, push notifications, email\nnotifications, and/or automated calls.\nFor example, the EMS may consider likelihood of emergency based on trigger values, situational awareness, user inputted information, etc. In addition, the EMS may consider emergency response policies or rules, based on a variety of inputs such\nas user preferences, federal, state, or local regulations, ordinances, or other statutory requirements, technology limitations of various response options, private emergency response services, etc. to make routing and communication decisions.  For\nexample, if an alert comes from a connected car in a local jurisdiction where the PSAP does not allow non-human callers, the EMS may elect to attempt to establish a voice connection with various emergency contacts or third-party emergency response\nproviders instead of the PSAP or before contacting the PSAP.  In some cases, the triggering device may not allow a connection to a user, the EMS will check the account preferences of the user of the triggering device (act 1434) to identify associated\ndevices to connect to (act 1436 & 1438).  In some embodiments, the associated devices will be contacted simultaneously and when a user responds, the user will be connected to the dispatcher.  In other embodiments, the associated devices will be contacted\nin a sequence so that the next user on a list will be contacted if there is no response on a given device.\nIn some embodiments, the emergency alert is triggered in numerous ways such as, for example, sensor readings outside specified range, device safety warnings, environmental warnings, and other triggers.  In addition, the emergency alert may be\ntriggered through user input, such as pressing an interactive button on the device or by voice command.\nIn some embodiments, the triggering device is a stationary device with communication capabilities such as desktops, home consoles, smart speakers with digital assistants, smart home consoles, thermostats and other controllers for sensors, etc.\nIn some embodiments, the triggering device is a mobile device with communication capabilities such as mobile phones, radios or walkie talkies, wearable devices, vehicle consoles, etc.\nWaterfall Calling\nFIG. 15 illustrates method for an emergency management system to call dispatchers for assistance after an emergency has been triggered.  Here, the EMS will establish an emergency call connecting a user to one or more dispatchers.  The emergency\nmay be triggered in various ways (e.g. a smoke alarm, an IoT sensor, a car crash detector, a user pressing a panic button, a voice command, an API call for emergency assistance).\nIf the triggering device can support audio, then the EMS will try to connect to a user through the triggering device (act 1514 and 1548).  For example, the triggering device may have audio capabilities such as a microphone, speakers and may\ninclude calling software (e.g. VoIP call).  In some embodiments, the EMS determines the audio capabilities based on the identifier for the triggering device or user.  In some embodiments, the EMS finds the association of the device to a particular\naccount or an organization.  In some embodiments, the evaluation of the audio capabilities is done at the time of registration or setup of the device.  In other embodiments, the evaluation is done in real time after the emergency has been triggered.  In\nsome embodiments, for connecting to the triggering device, a user (e.g. an emergency contact, an authorized secondary user, a corporate representative) who is available to participate in an audio call is also required.\nFor example, after a car crash, an emergency alert may be sent to the EMS by the car console.  If the car console has a VoIP client for answering calls, which is operational and a user in the car is able to answer the call, the EMS will connect\na call between the user and a dispatcher.  If the driver is unresponsive, the EMS will try to connect to other devices as described below.\nIn some cases, a voice call is preferred.  As an example, after a car crash, a voice call is preferred for communicating emergency information to enable the user to quickly and efficiently provide the information to the dispatcher.  In addition,\na voice call allows a two-way exchange and the dispatcher can advise the user who is in the emergency situation quickly and efficiently.\nIn some embodiments, before triggering an emergency alert, the EMS collects information about the location of the emergency and basic information about the type of emergency.  In some embodiments, the EMS also provides information about the user\nassociated with the triggering device to one or more dispatchers.\nIn some embodiments, when the triggering device does not allow an audio connection to a user (act 1514), the EMS searches for the account for the user of the triggering device to identify associated user devices to call (act 1516).  In some\ncases, there is a mobile phone, or another device associated with the user and the EMS attempts to connect with the user via that device (act 1552).\nIn some cases, the triggering device does not support an audio call, and the EMS will identify associated user devices to call.  In some cases, there are not any associated devices and the EMS will proceed to determining whether an emergency\ncall should be made based on whether a TTS (text-to-speech) message is allowed by dispatch centers in the area (act 1544).  Some prohibit non-humans to make emergency calls.  In some embodiments, for some jurisdictions and for private dispatchers, the\nEMS connect to a dispatcher without a user on the line and play a TTS message providing details regarding the emergency (act 1546).\nIn some cases, the EMS identifies a contact number for the user of the triggering device.  In some embodiments, the user has provided his or her contact information during the registration and setup of the triggering device.  In some\nembodiments, when an emergency is triggered, the EMS locates the user data in the data storage on the triggering device, in one or more databases (e.g. 1335 in FIG. 13) in the EMS or some other data storage location.  If unable to call the user at the\ntriggering device, in certain cases, the EMS will try to call the user at other devices using a contact number (act 1516 and 1522).\nIn some embodiments, the EMS identifies a call list for the user of the triggering device.  The user may have provided a list of emergency contacts (emergency data) during the registration and setup of the triggering device.  In some\nembodiments, the emergency contacts includes name of an emergency contact and their contact number.\nWhen an emergency is triggered, the EMS may locate the emergency data in the data storage on the triggering device, in a database (e.g. 1337 in FIG. 13) in the EMS or some other data storage location.  If unable to call the user at the\ntriggering device, the EMS will try to call the user at other devices or other users who are on the contact list using the emergency contact numbers (act 1516, 1518 and 1522).  As shown, the EMS will call the next number on the list if a user was not\nresponsive in the previous contact number or did not choose to connect to a dispatcher (act 1518).\nThe advantage of sequentially going through a contact list (e.g. 1365 in FIG. 13) that is a prioritized list is that the user can place his or her devices and devices of other closely associated users on the top of the list (e.g. spouses can be\ngiven higher priority).  In the same way, the user may give lower priority to emergency contacts who would only be contacted in the event that higher priority contacts are unavailable.  For example, people with lower priority may be further away, but\nstill able to make an emergency call on behalf of the user (e.g. a proxy call).\nIn some embodiments, a prioritized list comprises at least one associated device.  In some embodiments, a prioritized list comprises a plurality of associated devices having an order of priority.  In some embodiments, the order of priority\ndetermines the sequence in which associated devices in the prioritized list are contacted by, for example, an emergency management system or a dispatch center.  As an example, a prioritized list comprises an emergency contact's cell phone and tablet\n(with a VoIP application) with the cell phone having a higher priority than the tablet (e.g. as determined by the user), wherein the cell phone and tablet are associated devices of a triggering device (e.g. a wearable heart rate monitor).  The user has a\nheart condition and has adjusted the settings for the heart rate monitor to detect an emergency when it detects an irregular heartbeat.  In this case, the heart rate monitor detects an irregular heart beat and transmits an emergency alert.  An emergency\nmanagement system receives the emergency alert and looks up a list of associated devices of the triggering device.  In this case, the associated devices are the cell phone and tablet.  The emergency management system then connects to the cell phone since\nit has the highest priority in the list and obtains location information and other user information (e.g. user identity, medical information).\nOnce a user picks up the call, the EMS determines whether that user is the one who is in the emergency situation (act 1524).  To determine this, the user may be asked a simple question such as \"Are you in an emergency?\" or \"Do you need help?\"\nFor a quick response, the EMS will prompt the user to respond by pressing a button on a keypad or by yes or no voice answer.  If the connected user is in the emergency, an abridged message may be played such as \"Do you want to be connected to 911?\" or\n\"Press 1 to connect to get connected to emergency dispatch center in your area\" (act 1526).  If the user is not the one in the emergency situation, the EMS will play a detailed message to explain the emergency situation and prompting a user response (act\n1532) An example detailed message may say: \"we have detected that John Doe's car may be in an emergency on I-90.  Do you want to be connected to 911 or an emergency dispatch center on their behalf?\"\nIn emergencies, it is preferred that the communications are clear and concise, and the user is prompted to respond in a specific and simple manner.  In some embodiments, if the user presses 1, they are connected to 911 and if the user presses *,\nthe session is cancelled.  In another embodiment, the device interface may ask the user to confirm the call based on a displayed authorization instructions (using a password, passcode, fingerprint sensor, voice recognition analysis, etc.) and user\ninteraction on the interface of the device.\nIn addition, a timer is started to allow a limited time for the user to respond and if the user is unresponsive to connect to the next user on the list.  The timer (also referred to as the timeout value may be between 0.01 seconds to 300\nseconds, preferably between 10 to 30 seconds.  In some embodiments, the timer is about 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 25, 30, 35, 40, 45, 50, 55, 60, 70, 80, 90, 100, 110, or about 120 seconds.  In some\nembodiments, the timer is at least about 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 25, 30, 35, 40, 45, 50, 55, 60, 70, 80, 90, 100, 110, or about 120 seconds.  In some embodiments, the timer is no more than about 1, 2, 3, 4,\n5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 25, 30, 35, 40, 45, 50, 55, 60, 70, 80, 90, 100, 110, or about 120 seconds.  If there is user input, the timeout will be reached before the timer has expired.  If there is no user input and the\ntimeout value has been reached (act 1528, 1534 & 1536), the EMS searches for other associated user devices to call (act 1518).  If there are no other associated user devices (act 1518), the EMS will send a TTS message if it is allowed in the jurisdiction\n(act 1544) and/or sends notifications to the user and emergency contacts (act 438).\nFIG. 16 depicts embodiments of a triggering device within a smart home, emergency management system and computer programs.  As shown, a triggering device 1667 (not shown) with computer program 1669 within the smart home 1660 sends the emergency\nalert, which is a request for assistance to respond to an emergency, to an Emergency Management System or EMS 1630.  The EMS 1630 will transmit the alert and connect the triggering device 1667 with one or more dispatchers 1650.  The dispatcher 1650 may\nbe an Emergency Dispatch Center (EDC) or a private dispatcher.\nIn some embodiments, the triggering device 1667 includes a network component 1664 (e.g. a Wi-Fi antenna) which sends the emergency alert.  In some cases, the connectivity (e.g. Wi-Fi signals) is weak or the pathways are not secure, and the\nemergency alert is transmitted using another communication device (e.g. a mobile phone) or a routing device (e.g. Wi-Fi router).\nIn some embodiments, the triggering device 1667 includes a computer program 1669 for sending the emergency alert.  In some embodiments, user 1600 followed through a setup or registration process for the device 1667, where he or she has provided\nuser data such as emergency contacts (e.g. phone numbers, email addresses, messaging IDs), user information, location (e.g. a physical address of the location of the device 1667), etc. In some embodiments, user data, location data, emergency data, etc.,\nmay be saved in data cache 1615 in the device 1667.  In other embodiments, the data is saved in one or more databases 1635 in the EMS, in third-party servers or in cloud-based systems.\nIn some embodiment, the EMS 1630 may include one or more databases 1635 housed in one or more servers in the same or in a remote location.  In some embodiments, a location database 1637 houses location data regarding the location of the\nemergency.  In some embodiments, a user database 1639 houses user data and/or emergency data (such as an emergency contact list 1665).  In other embodiments, the location, user and/or emergency data (such as an emergency contact list 1665) is saved on\ndata cache 1615 in the triggering device 1667 or in data storage in other devices such as mobile phone 1606, etc.\nIn some embodiments, the triggering device 1667 may be an IoT (IoT) device such as a home thermostat, intelligent home monitors, baby monitors, smoke or carbon monoxide detectors, home security camera systems, etc. In some embodiments, the\ntriggering device 1667 is a communication device such as a mobile phone, a computer, a wearable device (e.g. a smart watch, fitness tracker, smart jewelry).  If the triggering device 1667 is a mobile phone, the emergency alert may be sent via a cellular\nconnection, if it is available.  After the alert is sent to the EMS 1630 via communication link 1622, the EMS 530 may initiate a voice call (e.g. via a VoIP call) along link 1624.\nIn some embodiments, the triggering device 1667 autonomously detects emergency situations or likely emergency situations.  In some cases, the triggering device 1667 sends an alert based on autonomously detected emergencies using one or more\nsensors (not shown) such as from a smoke detector or a burglar alarm in a building.\nIn some embodiments, the triggering device 1667 sends the alert with location information of the emergency.  For example, user 1600 may have entered the physical address of the home 1660 into the triggering device 1667, which may be stored in\ndata storage 1615.  In some cases, the triggering device 1667 includes the address in the alert.  In some embodiments, the triggering device includes a location component (e.g. GPS, location API, etc.) to determine its location, which is optionally\nprovided in the alert.\nIn other embodiments, the emergency alert is sent without location information.  When the user 1600 set up an account for the triggering device 1667 with the EMS 1630, he or she may have provided an address of the home 1660.  When the EMS 1630\nreceives an alert without location information, it will search its databases 1635 (including location database 1637) for the location of the triggering device 1667.  In addition to x, y coordinates and/or physical address, the location information may\ninclude additional information such as altitude, floor number, apartment number, suite number to accurately identify the location.  In some embodiments, IoT devices and IoT networks (including a network of sensors and/or devices) may be stored in a\ndevice location database stored in a database in the EMS or a third-party server.  Further discussion about location information is provided in FIG. 17.\nIn some embodiments (not shown), the triggering device 1667 sends the alert using another device with communication capabilities such as mobile phone 1606 through an indirect connection (e.g. Wi-Fi, Bluetooth, etc.).  An indirect connection may\nbe advantageous when the triggering device 1667 is unable to send the alert to the EMS 1630 due to bad connection, busy networks, etc. In addition, the user 1600 will be instantly alerted about the emergency situation even if he or she is in a different\nroom as the triggering device 1667.\nIn some embodiments, the triggering device includes sensors that detect environmental conditions and triggers an alarm when the sensed value is outside a specified range.  Another device (e.g. home console, a digital assistant, wearable, phone,\netc.) evaluates if the sensed value indicates an emergency situation or likely emergency situation and sends an emergency alert.  In such cases, the triggering device 1667 does not directly send the alert, but the triggering device is part of network of\ndevices that sends the alert.\nIn some embodiments, the EMS 1630 attempts to connect to a user (e.g. 1600, 505, 510) using one or more communication links (e.g. 1624, 1632, 1634, 1636) before sending the emergency alert to one or more dispatchers 1650.  The EMS 1630 may\nattempt to connect to user 1600 using communication device 1606.  Another user 1605, who may also be an owner of the smart home 1660, may be contacted on their work laptop 1628.  If the connection to user 1605 is unsuccessful, the EMS 1630 may attempt to\nconnect to user 1610 (a corporate representative) via a communication device 1616 (e.g. a corporate phone).\nIn some embodiments, the emergency contact list 1665 is a list with no prioritization and emergency contacts can be contacted in any order or concurrently.  On the other hand, if the emergency contact list 1665 is a prioritized list, it will\nallow the user 1600 to give higher priority to informing user 1605 who may be living in the home 1660 over another user 1610, who may be living further away.\nOnce connected to user 1610, a direct connection 1638 between device 1616 (e.g. by calling the corporate operations center using an available number to get additional information such as the type of burglar alarm system, etc.) and dispatcher\n1650 may be established or the user 1610 may be connected via the EMS 1630 via links 1636 and 1626.  Thus, the user 1610 can respond if he or she wishes to call the dispatcher on behalf of the primary user 1600 as described in FIG. 15 to request\nemergency assistance.\nProcessing Location Information from the Emergency Alert\nFIG. 17 illustrates an exemplary method for providing a location for an emergency.  An emergency may be triggered in various ways, including user interaction indicating an emergency and autonomous detection of an emergency using one or more\nsensors.  When the emergency alert is received (act 1712), the EMS determines if the location of the emergency is included in the alert (act 1714).  In some cases, the triggering device may have a location component (such as GPS or device-based hybrid\nlocation application) or it may not have a saved address.  In such cases, the device may send the alert with the location information.  In some cases, the triggering device may be a simple device with limited capability (e.g. an IoT sensor) and it may\nnot know where it is.  In such cases, the device will not be able to send the alert with location data.  In some cases, where the triggering device is sending an alert using an indirect connection with a communication device (e.g. a mobile phone with\nlocation services), the location data from the communication device may be appended or included in the alert or sent in some other fashion.\nIn some embodiments, if the location of the emergency is provided in the alert, the EMS calculates an estimated location accuracy (also referred to as the reported accuracy) for the location information (act 1716), which is optionally sent to\nthe dispatcher.  In some embodiments, the location is calculated to within 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 25, 30, 35, 40, 45, 50, 60, 70, 80, 90, or 100 meters of the location of the triggering device or an\nassociated device of the triggering device, including increments therein.  In some embodiments, the location is calculated with at least 80%, 85%, 90%, 95%, or 99% estimated location accuracy, including increments therein.\nIn other embodiments, alternate location accuracy (e.g. accuracy radius, confidence) may be used.  As used herein, \"location accuracy metric\" refers to a measure for quantifying the accuracy of a location as compared to a ground-truth or actual\nlocation.  In some embodiments, the location accuracy metric is a distance (e.g. a distance from ground-truth).  In some embodiments, the location accuracy metric is a likelihood or probability.  In some embodiments, the location accuracy metric is a\nconfidence.\nDetermining the location of the emergency is a critical step in providing quick and efficient response to the location.  The location and type of the emergency (which may be also included in the alert) can be used to determine the dispatch\ncenter (e.g. PSAP) to send the emergency alert (act 1718).  In some embodiments, the type of emergency may include: fire, medical, police, car crash, etc. Once the dispatch center is determined, the EMS may look up characteristics of the dispatch center\nincluding the type and format of the location information that can be sent to the dispatch center.  For example, if the recipient PSAP requires a dispatchable address (with a physical address), the location information may need to be converted from\nlat-long format (x, y coordinates) to a physical address using reverse geocoding.  In some cases, the user may have saved an address for a home and the EMS may convert it into lat-long format and provide either or both formats to the dispatch center.\nIn some embodiments, the EMS evaluates the accuracy of the converted location such as according to any of the following methods.  If the location information has to be converted (act 1722 and 1742), the EMS may compare the converted location to\nthe received location.  In addition, the EMS may determine whether the accuracy of the converted location falls within the reported accuracy calculated in act 716 (act 1744).  If the accuracy is not sufficient, the EMS may send a request for the current\nlocation of the device (e.g. through a HTTP message, an automated call or push notification) (act 1734).  For example, the triggering device may be the vehicle console in a connected car as shown in FIG. 18 and the EMS may request the console to send an\nupdated location.  If the vehicle console is not responsive, the EMS may send a request to a user device such as a mobile phone or wearable device associated with the user to obtain the user's location information.  In some embodiments, the EMS may take\nthe battery life of the devices into account.  In some embodiments, the EMS may decide not to frequently request location information from devices with limited battery life to preserve battery life other essential functions.  In some embodiments, the\nemergency alert will contain location information within the body of the alert or in the metadata.\nReferring to FIG. 17, if location information is not sent within the alert or in the metadata, the EMS may search for location information within its databases (act 1726).  For example, there may be a user provided address for the device (such\nas a home address) or a last saved location for the device.  If a saved location for the triggering device has been found (act 1726), the EMS may try to get an updated location, if needed (act 1732).  For example, the triggering device may be a mobile\ndevice such as a wearable or the last saved location may have been saved a long time ago and an updated location may be needed.  In such cases, the EMS will send a request to the device or associated device for a current location, which may be used for\nsending the alert to one or more dispatch centers.  In some cases where the location information from the triggering device is not accurate (act 1746), changing rapidly (e.g. a connected vehicle, not shown), incomplete (not shown) or outdated (not\nshown), the EMS may obtain current location from the device in the same way.  For example, a current location may be obtained in the following cases: i) the radius of uncertainty reported for the location obtained by the device is too large; ii) the\ngeocoded location of the provided address is too far from the reported lat-long, iii) the timestamp on the location is too old (e.g. 10 minutes, the time cutoff may vary based on device type and trigger types).  The advantage of getting location directly\nfrom the triggering device even when there is a user-provided location is that the current location may be more accurate.  Often the location sent in the emergency alert or 911 calls are obtained from cellular carriers and are not accurate.  In contrast,\nthe location information obtained directly from the device (such as from the GPS receiver or location services) may be more accurate.  In addition, for mobile devices, the EMS may be able to follow the trajectory of the device and updated the dispatch\ncenter and emergency responders to facilitate the emergency response.  The frequency of updates from the device may be moderated to preserve battery life.\nIn some embodiments, the EMS may obtain location information from other sensors or IoT devices that are associated with the user account by sending push notifications or SMS or other methods of obtaining location.  For example, when a crash is\ndetected by a vehicle console, but the location information is not available or accurate, the EMS may send push notifications to the user's mobile phone to obtain location information.  In another the embodiment, if the emergency is triggered by a\nwearable device which does not have a GPS or location services and thus, sends the emergency alert to the EMS without location information, the EMS can request location from another device associated device such as the user's mobile phone.  In some\nembodiments, the trigger devices are programmed to autonomously send location data after the emergency has been triggered.\nIn some embodiments, mobile devices (e.g. smart phone 1806) have the capability for generating and transmitting device-based hybrid location (DBHL) using location services.  DBHL incorporates multiple technologies embedded in smart devices, such\nas GPS, Wi-Fi, and Bluetooth, to obtain accurate location even in indoor settings.  DBHL are calculated on an electronic or communication device (using location services or geolocation APIs), as opposed to locations calculated using a network (e.g. a\ncarrier network).  Device-based hybrid locations can be generated using inputs from multiple sensors such as GPS, network-based technologies, Wi-Fi access points, Bluetooth beacons, barometric pressure sensors, dead reckoning using accelerometers,\ngyrometers, etc.\nSpecifically, in some embodiments, the location is determined on the device using GPS, received location data from cellular base station signal triangulation, Wi-Fi location data, Bluetooth beacon data, or any other form of location information\nstored at the communication device.  In some embodiments, the altitude or elevation (z-direction coordinate) may be determined using GPS, barometric sensor or an accelerometer.  In some embodiments, the location comprises lat-lon (x, y coordinates) and\naltitude (z-direction).  In some embodiments, the lat-lon coordinates includes 5-13 decimal places.  In some embodiments, the lat-lon coordinates includes 6 decimal places.  In some embodiments, the reported location includes reported accuracy radius and\nconfidence percentage.  In some embodiments, the location data is validated before it is provided to dispatch centers by using results from ground truth-testing.  For example, test calls may be made from pre-surveyed buildings or outdoor location\ndetermined using a GPS chipset.\nThe EMS may use location information from various sources to validate the location of the emergency.  If a sensor's location determination doesn't match known locations or frequently visited locations, the EMS may send push notifications to\nadditional user devices to confirm location (e.g. a smart phone known to be affiliated with the user of a wearable or a connected car).  Location information obtained from the wearable or the connected car may be compared and used for confirmation of the\nlocation.  For example, if a connected car GPS sensor shows that a user is traveling at 50 mph in South Africa, the EMS may try to confirm the location of the emergency by obtaining location from the user's mobile phone.  If the user's phone also\nindicates a velocity of 50 mph but in the user's home state, the EMS may decide to use the smart phone's location rather than the connected car's transmitted location.  Thus, the EMS leverages every available piece of information to validate location.\nIn some embodiments, machine learning algorithms are used to understand frequent user behaviors and further validate accuracy of location.  For example, if a user is at work every day at 3 PM and a wearable indicates they are having a health\nemergency in another part of the country, the EMS may decide to query additional devices or sensors affiliated with the user to validate location.\nFIG. 18 depicts embodiments of a connected vehicle, emergency management system and computer programs.  It is understood that connected or smart vehicle may be connected to the internet through various ways including its own internet connection\nand connecting through another device such as a mobile phone, etc. As shown, a triggering device 1867 (e.g. a vehicle console, electronic control unit, engine control unit, or engine control module) with computer program 1869 within the connected vehicle\n1862.  The computer program 1869 may be a part of the operating system of the vehicle console or may be a separate program that can be opened by the user or triggered when a crash is detected.  Various sensors (e.g. electromagnetic sensors, ultrasonic\nsensors, pressure sensors, gyroscopes) and software (e.g. crash detection software, obstacle detection software) may be employed to automatically detect when there has been a car crash.\nAn emergency may be triggered by user 1800 interaction with the user interface 1863 or autonomously detected (e.g. detection of a car crash).  After an emergency is triggered, the triggering device sends the emergency alert, which is a request\nfor assistance to respond to an emergency, to an Emergency Management System or EMS 1830 network component 1864 (e.g. Wi-Fi, Bluetooth.RTM., or cellular radio).  In some embodiments, the vehicle 1862 may connect to the internet via another device such as\na mobile phone usingBluetooth.RTM.  or a cellular connection.  In other embodiments, the vehicle 1862 will be connected to the internet directly via a Wi-Fi connection or cellular connection.  The EMS 1830 transmits the alert and connects the triggering\ndevice 1867 with one or more dispatchers 1850, which may be PSAP other public-sector emergency response agency, private security service, private emergency response service, or a private fleet management service.\nIn some embodiments, the triggering device 1867 (e.g. vehicle console) in the connected vehicle 1862 includes a location component for determination of the current location such as GPS receiver.  In some embodiments, the vehicle 1862 includes\none or more accelerometers and gyroscopes, which may be used to estimate the location via methods such as dead reckoning.  In other embodiments, sensor data from odometers and tire rotation sensors may be used for estimating the location of the vehicle\n1862.  In some embodiments, the location of the vehicle 1862 may be determined by using another device such as mobile phone with a GPS or another locationing method.  In some embodiments, the vehicle 1862 is located using devices fixed to a particular\nlocation (e.g. traffic cameras, toll booths, satellites, and other external sensors that could sense or track the location of the vehicle and convey it to the EMS).  In some embodiments, the vehicle 1862 is equipped with Wi-Fi and barometers, which is\noptionally used for estimating the location of the vehicle.  In addition, the vehicle's location may be estimated from nearby Wi-Fi hotspots, other connected vehicles, smart buildings, etc. Based on the location information, the triggering device 1867\nmay generate location data by using, for example, a location API on GPS coordinates).  The location data may be in form or lat-long or x, y coordinates or a physical address.  The location data may be sent with the emergency alert using communication\nlink 1822.\nHowever, in other embodiments, the location data 1875 is not sent in the alert and have to be transmitted through other pathways such as communication links 1874 and 1878.  The location data 1875 may be sent after receiving a request from the\nEMS 1830 or dispatcher 1850.  In some embodiments, the location data 1875 is sent after an emergency is detected, periodically during the emergency or for some time after the emergency session has ended.  The location data may be provisioned in one or\nmore databases 1835 in the EMS 1830 and provided to the dispatch center 1850 through secure pathways.  In this way, the EMS 1830 and dispatchers 1850 may be able to follow the trajectory of the vehicle 1862 in real-time.  In some embodiments, the\nfrequency of location determination may be moderated to preserve battery life of the device 1865 (especially for portable mobile devices like smart phones).\nIf the location data from the triggering device 1867 is not available or insufficient (no connectivity, low accuracy, etc.), the EMS 1830 may look up associated user devices (e.g. 1806) and request current location data using communication link\n1832.\nIn some embodiments, user data, location data, emergency data, etc., are saved in data cache or storage 1815 in the device 1867.  In other embodiments, the data are saved in one or more databases in the EMS (e.g. 1837, 1839), in third-party\nservers or in cloud-based systems.  In some embodiments, the EMS 1830 attempts to connect to a user (e.g. 1800, 1805, 1810) using one or more communication links (e.g. 1824, 1832, 1834, 1836) before sending the emergency alert to one or more dispatchers\n1850.  The EMS 1830 may attempt to connect to user 1800 using communication device 1806, user 1805 through tablet 1846.  If the connection to user 1805 is unsuccessful, the EMS 1830 may attempt to connect to user 1810 via communication device 182916\n(e.g. smart speakers with a digital assistant).  In some embodiments, an emergency contact list 1865 (not shown) is accessed to identify devices to contact during the emergency.  It is understood that the emergency contact list may be user-defined\nemergency contacts, other associated user devices, devices of friends and family groups, a third-party monitoring service (e.g. OnStar.RTM.), etc. In some embodiments, the contact list 1865 is generated in real time from nearby devices (e.g. identified\nby a Wi-Fi scan of the area), by searching for a name such as \"Mom\" from the contact list on the device and/or \"recently contacted\" devices or numbers from the call log, etc.\nThe user 1810 may be connected via the EMS 1830 via links 1836 and to the dispatch center 1850 via link 1826.  The user 1810 can respond by voice command (Yes or No) he or she wishes to call the dispatch center 1850 on behalf of the primary user\n1800 as described in FIG. 15 for requesting emergency assistance.\nActivating Emergency Response by Voice Command\nFIG. 19 depicts embodiments of a voice-activated emergency alert from a user in a smart home.  The triggering device may be a device with audio capabilities (e.g. a microphone and/or speakers) and a network component (e.g. Wi-Fi, Ethernet, cable\nconnection) in the smart home 1960.  Other triggering devices may be a desktop, a laptop, a tablet, VoIP phone, a smartphone, a digital or virtual assistant, IoT devices (e.g. a smart smoke alarm) or other computing devices capable of receiving voice\ncommands, etc. In some embodiments, the triggering device may be a smart home or smart vehicle or a component thereof.  For example, a digital assistant service in a smart home or smart vehicle may be programmed to take voice commands.\nThe triggering device may be a component of a smart hub.  The connectivity to the internet may be managed through a hub (e.g. a smart home hub using a home network using Zigbee, Bluetooth or Wi-fi) and links several devices or sensors.  A sensor\nin the network may make a reading and the smart hub could determine if there is an emergency (include confirmation) and send an emergency alert (or directly initiate an emergency session or call).  In one embodiment, the hub may be a command panel with\ncomputing capabilities linked to sensors throughout a building via hardwires or other connections (e.g. a commercial fire detection system).\nIn some embodiments, a smart hub system triggers an emergency alert based on input from various IoT devices in its network.  A smart hub can provide easy control through machine-learning and artificial intelligence.  In some embodiments, an IoT\ndevice does not include a cellular network transceiver radio or interface, and thus may not be configured to directly communicate with a cellular network and may send an emergency alert via indirect communication via another device (e.g. smart phone\n1906).  In some embodiments, an IoT device may include a cellular transceiver radio, and may be configured to communicate with a cellular network using the cellular network transceiver radio.  Exemplary smart hubs include Samsung SmartThings, Amazon\nEcho, Google Home, Apple Home.\nExemplary IoT devices that can trigger an emergency alert include home automation network devices that allow a user to access, control, and/or configure various home appliances located within the user's home (e.g. a television, radio, light,\nfan, humidifier, sensor, microwave, iron), or outside of the user's home (e.g. exterior motion sensors, exterior lighting, garage door openers, sprinkler systems).\nIn some embodiments, network devices is used in other types of buildings, such as a business, a school, a university campus, a corporate park, or any place that can support a local area network to enable communication with network devices.  For\nexample, a network device can allow a user to access, control, and/or configure devices, such as office-related devices (e.g. copy machine, printer, fax machine), audio and/or video related devices (e.g. a receiver, a speaker, a projector, a DVD player,\na television, or the like), media-playback devices (e.g. a compact disc player, a CD player, or the like), computing devices (e.g. a home computer, a laptop computer, a tablet, a personal digital assistant, a computing device, a wearable device),\nlighting devices (e.g. a lamp, recessed lighting), security devices associated with a security, devices associated with an alarm system, devices that can be operated in vehicles or mobile devices (e.g. radio devices, navigation devices), etc.\nReferring to FIG. 19, for taking voice commands, the TV 1967 is installed with voice or speech recognition software.  Voice commands can be quick and efficient for the user to interact with a device as compared to typing on a keyboard,\nparticularly during emergency situations.  In some embodiments, voice recognition software may be able to match a user's voice and provide authorization for the call.  In some embodiments, participants in the emergency call or emergeny connection session\nmay have to obtain appropriate authentication credentials and be granted appropriate permissions during initial registration or setup.  For example, voice recognition may allow the adult resident of the home to make the call, but not children.  In some\nembodiments, an authorized user may be required to make an emergency call, which may be done based on recognition of the speaker of the voice command.  In some embodiments, the user may authorize the call by entering or voicing a phrase or a code.\nThe TV 1967 may be installed with sound recognition software for autonomous detection of emergencies.  Intelligent noise interpretation may detect emergencies based on sound (alarms, breakage, gunshots, explosion, fire, car crash, etc.) or\nabsence of sound (e.g. no heartbeat, etc.).  In some embodiments, the TV 1967 is installed with an application for identifying user-defined sounds such as a clap, a whistle, etc. The device may include a microphone and ability to intelligently interpret\nnoise or sounds.  In some embodiments, the sound may be detected using a sound transducer or piezoelectric sensor, LM393 Sound Detection Sensor, or microphones like dynamic microphone, condenser microphone, ribbon microphone, carbon microphone etc.\nThe software for sound detection or recognition software is generally hosted on the device, in the EMS or on a remote server.  In some embodiments, the software has access to a library of sounds or noises either on the device, the EMS or at a\nremote server.  In addition, the software may identify background ambient sounds that do not indicate emergencies.\nIt may be appreciated that users in a smart home are exposed to different types of ambient noises throughout the day.  As a result, relying on a trigger capable to continuously adapting itself to the environment may result in a good detection\nrate.  In some embodiment, the present invention proposes powerful algorithm wakes-up the DSP handling Keyword Spotting regardless of the environment and the distance between the user and the audio-capturing device.\nOnce a trigger sound or voice command is detected, the software may send an emergency alert and the EMS may try to contact a user who is available to connect to the dispatcher as described in FIGS. 14 and 15.  In some cases, the EMS may connect\nto a dispatcher without a human user on the line.\nIn some embodiment, a trigger sound or voice command is detected by user uttering one or two words, for example, a \"Hey Sam\" feature allows users to activate a digital assistant.  In some embodiments, a small speech recognizer runs all the time\nand listens for just those two words.  When it detects \"Hey Sam\", the digital assistant parses the following speech as a command or query.  The \"Hey Sam\" detector may use a Deep Neural Network (DNN) to convert the acoustic pattern of your voice at each\ninstant into a probability distribution over speech sounds.  Using a temporal integration, it may then to compute a confidence score that the phrase you uttered was \"Hey Sam.\" If the score is high enough, Sam wakes up.\nIn some embodiments, the user wakes up the device by saying a key phrase.  Hardware, software, and Internet services work together to provide a seamless experience.  In some embodiments, the audio detector has several components, many of which\nare hosted \"in the Cloud\", including the main automatic speech recognition, the natural language interpretation and the various information services.  There are also servers that can provide updates to the acoustic models used by the detector.\nIn some cases, once a trigger sound or phrase is detected, the software requests or asks for confirmation from a user before sending the alert.  For example, the software may alert the user that it is primed (or on standby) for sending the alert\nand request confirmation (e.g. TV is muted, would you like to connect to emergency services?\").  In some cases, voice recognition authenticates the identity of the user.  In other cases, the user has provided instructions to skip \"ask or request\nconfirmation\" (e.g. gun shots have triggered the system) and the emergency alert will be sent immediately after the trigger sound or phrase is detected.\nIn some embodiments, an emergency call is authorized by a two-tiers of confirmation: (tier i) emergency triggered by a user (e.g. by voice command) or autonomously by the triggering device (e.g. a trigger sound detection); followed by (tier ii)\nemergency that is authorized by a user (e.g. user may authorize using a pin, a passcode, speech recognition, face recognition, a fingerprint, etc.).  In some embodiments, authorization need not be provided in real time.  For example, a user may have\nprovided pre-authorization for sending an emergency alert for some types of emergencies.  In some cases, the EMS may decide to send the alert to one or more recipients (including emergency contacts) after evaluating the severity and likelihood of\nemergency.\nAs shown in FIG. 19, the triggering device is a connected TV or smart TV 1967 with a computer program 1969 for sending an emergency alert.  The TV 1967 includes a network component 1964 which may be an Ethernet connection, Wi-Fi connection\ncellular connection, etc. for communicating with the EMS 1930.  The emergency alert is sent to the EMS 1930 via communication link 1922.  The EMS 1930 may call back and get data (e.g. user data, location data, emergency data) from the device 1967 via the\ntwo-way communication link 1924.\nIn some embodiments, the TV 1967 sends the emergency alert using communication device 1906, e.g. user 1900's mobile phone.  In some embodiments, the communication device 1906 has a better connection to the EMS 1930 and has computer program 1908\nfor sending the emergency alert.  The emergency alert may contain location information and other information about the emergency, for example, user 1900 may have entered the address of smart home 1960 previously or the device-based hybrid location of the\ncommunication device 1906 may be included.\nThe emergency response provider (e.g. a PSAP, an emergency responder, a private dispatcher) is configured to receive emergency calls within its geographical jurisdiction.  Based on the location and (optionally, the type of emergency, or other\nfactors) the EMS 1930 may determine the appropriate dispatcher 1950 (e.g. an emergency dispatch center such as the local PSAP, other public emergency response agency, private emergency response service, etc.) for establishing an emergency session or\ncall.\nIn some embodiments, the system further comprises a sensor or IoT device database storing data collected by at least one sensor.  For example, the EMS optionally searches for other devices in the smart home 1960 by searching for the user name or\nlocation within one or more databases (e.g. user information database 1939, location database 1937 or an associated device database 1235 (not shown) or other databases).  When a device such as video camera 1987 is identified, the EMS 1930 may request\naccess to the video feed.  In some embodiments, the video feed is from the camera 1987 and is accessed from a cache 1989 on the device or associated device.  The video camera 1987 may be connected to the internet via network component 1984 and the video\nfeed may be stored in the cloud.  Transfer of the video feed requires connections with sufficient bandwidth (e.g. Wi-fi, 3G or 4G cellular data, etc.).  Thus, real-time video feed from the emergency location may be available to the EMS 1930.  In some\nembodiments, historical video feed may also be accessed to gain situational awareness regarding the emergency.\nIn some cases, the video feed from the emergency location is available to the EDC 1950 and to emergency responders 1980 via a responder device 1982.  The device 1982 may be installed with software 1983 for receiving information about the\nemergency from the EMS 1930 or from the EDC 1950.  The communication link 1936 may provide a secure pathway between the EMS 1930 and the responder device 1982 (without sending the data through a dispatch center).  In some embodiments, the IoT device\nsends the sensor data directly to an emergency responder, a public safety network, emergency responder device, or other emergency response systems (not shown).\nIn some embodiments, the EMS 1930 sends the sensor data (e.g. video feed) to one or more emergency responder devices (e.g. vehicle console, walkie talkies, mobile phones, tablets, etc.).  The EMS 1930 may check the credentials of the recipient\ndevices and send the data through secure pathways.  In some embodiments, the data may be made available to supervisors, managers, dispatchers and responders to access.  The EMS may evaluate and determine whether to make data available and to which\nemergency responders based on credentials.  For example, access to medical data may be limited to EMTs during a medical emergency and may not be available to police or fire personnel.  The EMS may also provide access to the data to emergency responders\nbased on their proximity to the location of the emergency.  Thus, the emergency responders and other private parties with permission (e.g. home security companies, emergency contacts) may be given have access to real-time video feed from the emergency\nlocation to determine the number of people in the area and other situational awareness for effective emergency response.\nConfirmation or Verification of the Emergency\nIn some embodiments, the trigger (e.g. voice command or sensor data) is followed by a confirmation or verification of the emergency.  For audio (voice command or sound) triggers, it may be necessary to get a confirmation of the emergency to\nprevent frivolous emergency calls to public safety.  Generally, there are many ambient sounds and noises that may trigger the emergency alert, but the situation is not an emergency.  The confirmation or verification step may be designed to prevent false\nalarms.  In some embodiments, the confirmation may be user-defined or chosen from a list of confirmation options.  For example, a sensor detects an emergency automatically or by voice command.  It may send an alert to another device (e.g. a smart home\nhub, a computer, smart phone, etc.), to a system (e.g. EMS) or a corporate entity (e.g. campus security) or specific individuals (e.g. emergency contacts, fire marshals, etc.).\nConfirmation of the emergency can be done in various ways including voice confirmation, security code, fingerprinting, face recognition, or through the use of sensors.  For example, the user may be asked to verbally confirm the emergency or by\npressing a button.\nIn some embodiments, the alert is sent to a third-party such as an emergency contact, a loved one, a corporate representative, an alarm monitoring company, a private response system, etc. The third party may be asked to verbally or press a\nbutton to confirm the emergency.\nIn some embodiments, sensor data is used to confirm the emergency.  For example, if a user commands \"Call 911\" and an alert is sent to the EMS.  For example, the emergency may be confirmed by identifying that nearby smoke detectors are going\noff.  In another example, the emergency may be confirmed by audio or video feed from the emergency location.  For example, audio or video feed may confirm movements inside the building, location of specific objects and people in the home (e.g. a\nheadcount), etc. In one example, when a trigger device detects a falling individual, a central device or EMS may determine whether the individual was a person or an animal and potentially identify the individual.\nIn some embodiments, the initial alert will activate other sensors in the area to capture situational awareness regarding the emergency.  For example, the secondary sensors that could be activated to detect environmental parameters (e.g.\ntemperature, moisture, smoke, toxins, etc.) or health parameters (e.g. heart rate, pulse, peripheral capillary oxygen saturation, blood pressure, blood sugar).\nIn some embodiments, the verification step may be conducted after connecting with the triggering device or another device.  For example, the triggering device has \"voice-calling\" ability (e.g. VoIP) such as a smart phone, smart TV or smart\nspeaker), the emergency call may be connected directly to the user, who can confirm the emergency.  In some embodiments, the triggering device does not have voice-calling capability (e.g. IoT sensor, crash detection sensor, smart home hub, etc.) and the\nverification may be done by calling a known phone number for the user or owner.  In some cases, the alert may also be sent using a nearby communication device (e.g. a smart phone).\nIn some embodiments, the emergency session or call is made to public safety (e.g. appropriate PSAP).  After receiving the emergency alert, the EMS may determine the appropriate PSAP based on the location of the emergency and the jurisdictional\nboundaries of PSAPs.  In some cases, other factors may be considered when choosing an appropriate EDC such as type of emergency, severity or priority of the emergency (e.g. need for ambulance), etc. In some embodiments, the situation may not rise to the\nlevel of an emergency and notifying third parties may be appropriate.  In some cases, the emergency call is made to a third-party security service (e.g. corporate or campus security) or a private response system and the verification step may not be\nneeded.\nThe present methods and systems disclose partially or fully integrated solutions emergency management systems, dispatch centers and triggering devices.  Previous systems were not integrated and could not provide secured pathways, user privacy\nand software compatibilities.  In some embodiments, partial integration with a dispatch center or emergency responder may involve adding the emergency management system as a \"trusted link\" on their systems or devices.  In some embodiments, end-to-end\nsoftware solutions are beneficial, e.g. for video feed from sensor to be available to emergency responders.\nFIG. 20 depicts a system for requesting emergency assistance by voice activation by a user 2000.  As shown, a user 2000 initiates an emergency using an audio detector 2004 (e.g. smart speakers) by giving a voice command for calling emergency as\ndescribed in FIGS. 21A & 21B.\nFor analyzing the voice command, speech recognition software may be installed on the device 2014, or on the triggering devices 2006, or in a cloud-based system (not shown).  The speech recognition software in the devices 2006 may use keyword\nspotting to analyze the voice command.  Upon detecting a voice command, the audio detector 2004 can trigger the audio file(s) to one or more nearby triggering devices 2006.  In an example, the triggering devices 2006 (e.g. smart phone, laptops, desktop). The triggering devices 2006 may perform an analysis of the audio(s) received from the audio detector 2004 to confirm the emergency.  In some cases, an additional confirmation from the user 2000 may be prompted such as a voice confirmation, fingerprint,\netc. In some cases, the match of the voice command to the user's voice by a voice recognition software may confirm the emergency without need for confirmation.  In other cases, the analysis may consider the user's habits, other sensor data, user\npreferences to make a decision about whether to send the emergency alert.  In some embodiments, the audio detector 2004 is a component of the triggering device 2006.  After the emergency is confirmed, the triggering device 2006 using its communication\ncapabilities transmits the emergency alert to one or more emergency service provides (ESPs) 2012 using one or more gateways 2008 having one or more access devices like routers, switches, gateway devices, etc. through the network 2010.  In some\nembodiments, the network 2012 is partially or fully in the cloud.  The emergency alert may comprise location data, user data, sensor data and/or additional data.  Upon receipt of the emergency event the one or more emergency service provides (ESPs) 2012\ncan send emergency help to the user 2002.\nFIGS. 21A and 21B depicts an exemplary implementation of an emergency call initiated by voice activation.  In some embodiments, a triggering device 2120 is activated when an emergency situation arises 2111 when a user 2110 gives a voice command\n2113 \"Ok Sam, call 9-1-1.\" Although not shown, an audio detector may be a part of the triggering device 2110 or a separate device.  For example, the voice command 2113 may be detected by a smart speaker with a digital assistant when the user 2110 utters\nthe trigger phrase \"Ok Sam, call 9-1-1.\" In some cases, \"OK Sam\" may be a wakeup command after which the digital assistant may be primed for a voice command \"call 9-1-1.\" Alternatively, the triggering device 2120 may be activated by trigger sounds (e.g.\ngunshots) to initiate an emergency alert.\nIn some embodiments, the triggering device 2120 transmits the emergency alert to an EMS 2130 with data regarding the emergency (e.g. user data, location data, sensor data, additional data, etc.).  In some cases, the triggering device 2120 may\nprompt the user to confirm or verify the emergency before sending the alert.  In some case, the EMS 2130 may require the user to confirm or verify the emergency.\nFor example, after activation, the triggering device 2120 may activate Session Initiation Protocol (SIP) User Agent 2118 through a \"setup SIP user agent\" 2114 mechanism, which in return sends an API request 2119 to the EMS 2130.  In some\nembodiments, the API request 2119 has inputs \"location\" and \"name\" and the output is the SIP endpoint, SIP password, and SIP domain/STUN.  In an exemplary embodiment where the triggering device does not have a phone number, upon receipt of API request\n2119, the EMS 2130 executes several internal mechanisms such as assign 10-digit phone number for emergency, provision call-back number (CBN) 2122, associate location with CBN 2124, etc. In some other embodiments, the emergency alert may be assigned a\nphone number associated with the account, which will be used to provision data including location data.\nVarious mechanisms for providing data to the ESP 2140 are contemplated.  In some embodiments, the ALI record (including location for the CBN is updated 2125, which will be provided to the ESP 2140.  In other embodiments, the data may be pushed\nout the ESP 2140 directly.\nIn some embodiments, the EMS 2130 sends an API response 2119 to the triggering device 2120 as an acknowledgement.  In some embodiments, the EMS 2139 may make an emergency determination and may decide not to forward the emergency alert to an ESP\n2140 if it appears to be a false alarm or prompt the user 2110 to confirm the emergency.\nIn some embodiments, the SIP user agent registers the session with ESP 2140 using a SIP register 2121 and a SIP invite 2123 commands, which is forwarded by the EMS 2130 (e.g. as a SIP Invite) and Telephony Trunking or NG911 SIP 2135.  A two-way\naudio session 2116 is established where the user 2110 can connect to an ESP personnel to get help.  The ESP 2140 can also access the data regarding the emergency including the location data.\nA similar SIP session can be initiated from ESP 2140 using the CBN.  As shown, the user 2115 has disconnected the call with a voice command, \"Hey Sam, hang up\" 2115 and the emergency call has ended.  Here, \"Hey Sam\" may be a wake-up phrase. \nRegardless of the reason for disconnections, a second two-way audio session may be initiated by the ESP 2140 using a call-back initiation step 2129.  When the EMS 2130 receives the telephone trunking from the ESP 2140, it looks up the previous\nassociation 2146 by getting an endpoint for the CBN and returning an SIP endpoint.  An SIP invite 2143 could be sent to the triggering device 2120 to initiate another two-way audio session 2126.\nReferring to FIG. 21B, an implementation of voice activation is shown where the two-way audio session is interrupted due to network trouble.  In some embodiments, a triggering device 2120 is activated when an emergency situation arises 2111 when\na user 2110 gives a voice command 2153 \"Ok Sam, call 9-1-1.\" Alternatively, the triggering device 2120 may be activated by trigger sounds (e.g. gunshots) to initiate an emergency alert.  In some embodiments, the triggering device 2120 transmits the\nemergency alert to an EMS 2130 with data regarding the emergency (e.g. user data, location data, sensor data, additional data, etc.)\nSimilar to scheme in FIG. 21A, the triggering device 2120 may activate Session Initiation Protocol (SIP) User Agent 2150 through a \"setup SIP user agent\" 2154 mechanism, which in return sends an API request 2167 to the EMS 2130.  In an exemplary\nembodiment where the triggering device does not have a phone number, upon receipt of API request 2167, the EMS 2130 executes several internal mechanisms such as assign 10-digit phone number for emergency, provision call-back number (CBN) 2158, associate\nlocation with CBN 2162, etc. In some other embodiments, the emergency alert may be assigned a phone number associated with the account, which will be used to provision data including location data.  In some embodiments, the ALI record (including location\nfor the CBN is updated 2175, which will be provided to the ESP 2140.\nIn some embodiments, the EMS 2130 sends an API response 2169 to the triggering device 2120 as an acknowledgement.  In some embodiments, the SIP user agent registers the session with ESP 2140 using a SIP register 2171 and a SIP invite 2173\ncommands, which is forwarded by the EMS 2130 (e.g. as a SIP Invite) and Telephony Trunking or NG911 SIP 2185.  A two-way audio session 2156 is established where the user 2110 can connect to an ESP personnel to get help.\nWhen network trouble begins 2177, the two-way audio session 2156 may get disconnected.  Then, the EMS 2130 may maintain the link with the ESP 2140 in a one-way audio session 2156 in order to continue the current emergency session.  In many\ncases, if a new session is started, the user 2110 may be connected with another ESP personnel, who will have to be brought up to speed regarding the emergency losing precious minutes.  However, in this way, the specific ESP personnel may be informed to\nstay on the line using a text-to-speech message 2161, while the EMS 2130 attempts to reconnect with the user 2110.\nIn some embodiments, the EMS 2130 may send a SIP invite 2183 to the triggering device 2110.  In some embodiments, the SIP invite may be sent periodically for a period of time.  When the network is restored 2179, the triggering device 2110\nreceives the invite and the two-way audio session 2166 is re-started.  In some embodiments, the first audio session 2156 is resumed after the short delay.\nFIG. 22 depicts an exemplary view of information displayed at an emergency dispatch center (e.g. PSAP).  As shown, the name and contact information for a corporate representative making the call on behalf of a user (a proxy call) is displayed. \nIn addition, the name, address, phone number for the operations center for the corporation is also listed.  In some embodiments, the location (x, y coordinates, accuracy) of the user in the emergency situation is displayed at the PSAP.  Specifically, the\nALI view is depicted in FIG. 22, but other displays are contemplated including displays with tabs and windows and a map view.\nFIGS. 23A and 23B depict exemplary screenshots for an interface for testing emergency flows.  FIG. 23A shows the \"code view\" for the flow and FIG. 23B shows the \"ALI display\" that is shown on a PSAP display (e.g. EDC 450 in FIG. 4).  In some\nembodiments, the Emergency Console is a web application that users (e.g. administrators) can access through an internet browser (e.g. Internet Explorer, Google Chrome).  In some embodiments, the Emergency Console is a desktop application that can be\naccessed offline.  In addition to the graphical user interface 673 (see FIG. 6) in which users can configure emergency flows, the Emergency Console may include a dashboard where users can view, test, and manage their emergency flows.  In some\nembodiments, users can select emergency flows from a drop-down list, as depicted in FIG. 23.  In the example depicted in FIG. 23, the selected emergency flow is called \"company_contacts.\" In some embodiments, once an emergency flow is selected, the\ndashboard automatically populates with information regarding the selected emergency flow.  In some embodiments, the information regarding the selected emergency flow includes user names, contact names, and associated accounts (e.g. phone numbers, email\naccounts).  In some embodiments, after an emergency flow is selected, the Emergency Console displays data pertaining to the selected emergency flow.  For example, in some embodiments, the Emergency Console displays an emergency flow history (such as from\nthe emergency flow history database described above with respect to FIG. 3) including an entry for every individual execution of the selected emergency flow.  An entry optionally includes data including, but not limited to: the day and time that the\nselected emergency flow was executed, what type of triggering device sent the emergency alert that triggered the execution of the selected emergency flow, an identity of the user associated with the triggering device, which contacts were contacted, if a\nparty opted to contact an emergency responder, if an emergency responder was contacted, if contacting an emergency responder was successful or unsuccessful, which party opted to contacted an emergency responder, how long the execution of the selected\nemergency flow lasted, or combinations thereof.  In some embodiments, the emergency flow history includes additional data pertaining to the selected emergency flow including, but not limited to: the average length of the execution of the selected\nemergency flow, the total number of times the selected emergency flow has been executed, the absolute number of false positives (e.g. how many times the selected emergency flow was triggered and then promptly canceled), the percentage of false positives\n(e.g. the absolute number of false positives divided by the total number of times the selected emergency flow has been executed), the absolute number of times and percentage that an EDC was contacted, the number of times the selected emergency flow\nfailed, or combinations thereof.\nIn some embodiments, as depicted in FIGS. 23A & B, the dashboard includes a code example tab, an API request tab, and an ALI display tab.  FIG. 23A depicts an exemplary view of a dashboard within the Emergency Console when the code example tab\nis selected.  In some embodiments, the code example tab displays the script that defines the selected emergency flow.  FIG. 23B depicts an exemplary view of a dashboard within the Emergency Console when the ALI display tab is selected.  In some\nembodiments, the ALI display tab depicts a simulation of information displayed at an emergency dispatch center (e.g. PSAP) generated by the selected emergency flow.\nAuthentication\nIn some embodiments, the EFMS authenticates requests using a short-lived OAuth 2.0 access token granted using application client credentials.  The token is valid for authenticating requests to the EFMS for a temporary duration of time (e.g. one\nhour), after which a new token for future requests will need to be fetched.  Details for retrieving and using one of these tokens are described below.\n(i) Getting an Authentication Token\nPOST https://api-sandbox.rapidsos.com/oauth/token\nRequest\nHeaders\nContent-Type: x-www-form-urlencoded\nParameters\nclient_id: Client ID of an application authorized to use the service\nclient_secret: Client secret of an application authorized to use the service\ngrant_type: client credentials\nResponse\nFields (access_token: Short-lived OAuth 2.0 access token)\n(ii) Using tin Authentication Token\nTo authenticate requests using an access token, requests typically must include an Authorization header set to the value Bearer &lt;ACCESS-TOKEN&gt; where &lt;ACCESS-TOKEN&gt; is substituted for the retrieved access token.\n(iii) Triggering/Initiating Emergency Flow\nThe logical entity triggering the emergency flow (e.g. a mobile application, service backend, or any connected device) will trigger the emergency flow with a single API call as described below.\nPOST https://api-sandbox.rapidsos.com/v1/rem/trigger\nRequest\nHeaders (Authorization: Bearer &lt;ACCESS-TOKEN&gt;; Content-Type: application/json); Parameters (emergency flow: The name of the emergency flow to execute; variables: JSON Object defining any variables that you have defined and use in your\nemergency flow).\nResponse: Fields (code: Status code of the emergency flow initiation; id: Unique identifier of the instance (e.g. the particular emergency flow session) of the emergency flow execution; message: Human-readable descriptor of the emergency flow\nstatus)\nExemplary Markup Language for Emergency Flows--REML (Proprietary Emergency Markup Language)\nAs discussed above, in some embodiments, emergency flows are defined by scripts written in a programming language.  In some embodiments, the programming language used to write the scripts is the YAML (YAML Ain't Markup Language) language or a\nderivative thereof.  In some embodiments, the programming language used to write the scripts is the RapidSOS Emergency Markup Language (REML).  The REML language may be used to define a REML state machine.  In some embodiments, the REML state machine may\nbe specified by a \"states\" tag.  Each REML state includes the following main items including name of state, description of state, action to be executed, action inputs (also referred to as arguments), variable name to store the results of the action,\ntransitions based on the result of the action (e.g. which state to move to).  An exemplary schema describes the syntax of a state:\nTABLE-US-00004 name: &lt;string&gt; description: &lt;string&gt; action: &lt;string&gt; inputs: name1: value1 name2: value2 ...  store: &lt;string&gt; on_output: result1: &lt;string&gt; result2: &lt;string&gt; ...  goto: &lt;string&gt; log:\n&lt;string&gt;\nTable 1 describes optional REML tags below.\nTABLE-US-00005 TABLE 1 REML Tags Tag Description Name Name of a state Description Nonfunctional, descriptive comment of what a state accomplishes Action Name of the action to run i this state Inputs Map of variables to provide the action as\ninput parameters Store Name of a variable to store the result of the action on_output Map of name/values to specify the state transition Goto Specify transition (if there's no match with \"on_output\") Log Mostly Troubleshooting add a customer log for this\nstate\nIn addition to the current states, there are two built-in states that can be used to stop execution of state-machine: (i)_STATE_END_: specify successful execution of state-machine; (ii)_STATE_ERROR_: specify unsuccessful execution of\nstate-machine.\nThe built-in states are optionally used in various scenarios: one of the built-in states can be specified in the script; in case of an error--the system will force transition to _STATE_ERROR_; when your state-machine ends--the state-machine will\nautomatically move to _STATE_END_.\nIn some embodiments, a \"transition\" is used for determining the \"next state\".  In order to demonstrate a simple transition, consider the next example of a state:\nTABLE-US-00006 name: example-state action: PLUS inputs: { a: 1, b: 2 } on_output: 3: _STATE_END.sub.-- goto: _STATE_ERROR.sub.--\nIn this example, the PLUS action is run with inputs a=1'' and b=2.  If the result is 3 (as expected), the transition moves to the _STATE_END_state, otherwise the transition moves to the _STATE_ERROR_state.  This example shows that the\n\"on_output\" acts as an \"if statement\" for the action's result.\nIn some embodiments, transition from one state to another is determined by the following factors: the result of an action: REML will try to match the result of the action with one of the \"on_output\" entries.  In case of a match--the next state;\nExternal events (e.g. while executing an action, the REML system can send events to your state-machine.  During a phone-call between two users, each user can hang up the call regardless of the state.  To handle such hang-up events and decide what is the\nnext state based on the user who hung up the call, transitions are used.  The same \"on_output\" tag may be used to handle events--just like the action=results); the \"goto\" tag--if there is no \"on_output\" match, the user can specify the \"default next\nstate\" using the \"goto\" tag; if the user hasn't specified goto--the system will move to the next state in the list.\nREML Actions\nIn some embodiments, there is a single action that can be executed (e.g. telephony operations, control actions, data actions) for each machine state.  Non-limiting examples of telephony operations includes CALL, EMERGENCY CALL, HANGUP, PLAY,\nSTOP_PLAYBACK.  In some embodiments, EMERGENCY_CALL is an enhanced version of the \"CALL\" action.  As an example, in some embodiments, for EMERGENCY_CALL, the Admin or designated representative must provide the location (latitude/longitude), which is\noptionally passed to the 911 via ALI database and SIP headers; the Admin or designated representative must provide the \"caller name\", which may be passed to the ALI database; specifying a destination \"phone number\" is optional as the emergency-number is\nknown (\"test number\" for non-production system, 911 for production).  If a phone-number is provided, the system will optionally use it as an emergency number.\nNon-limiting examples of inputs include: caller_name: name of caller that will be presented to 911; caller_number: phone-number of caller (will be presented to 911); longitude, latitude: location of emergency (will be presented to 911);\nemergency_number: optional string, the emergency number to call.  In some embodiments, the default emergency-number is an answering machine.\nIn some embodiments, the results include: \"true\": success--user has answered; \"false\": failure--user hasn't answered.  we do not distinguish between different errors/failures/disconnections.\nTABLE-US-00007 Example: name: call_911 action: EMERGENCY_CALL inputs: { latitude: 10.10., longitude: -20.20, caller_name=\"Kai\", caller_number=\"1234567789\" } on_output: \"true\": play-message \"false\": die\nAnother exemplary REML action is FAILSAFE.  A FAILSAFE action for handling different types of failures can be formulated.  In some embodiments, a failure is either triggered by the system (e.g. 9-1-1 not answering) or by the emergency flow (e.g.\nexplicitly triggered by the emergency flow author).\nIn some embodiments, the FAILSAFE action instructs the system to post an HTTP request to a backend system provided by the emergency flow author and then immediately terminates the session, and therefore the on output section is not applicable\nand not needed.\nIn order to initiate a fail-safe procedure, some embodiments of the emergency flow include failsafe metadata.  Two embodiments the FAILSAFE action--the minimal one and the advanced one--are described below.  A minimal failsafe requires a URL and\na failsafe-reason as shown below.  In this minimal case, the action will trigger a POST request to the URL provided by the author and the emergency flow will be terminated.  As a request-body, the system will provide the `reason` in JSON dictionary.\nIn some embodiments, composite actions are formulated.  Composite actions decompose into a series of smaller actions before they are executed by the EFMS.  In some embodiments, individual emergency flow building blocks are defined by a single\nREML action.  In some embodiments, individual emergency flow building blocks are defined by composite actions.  INTERACTIVE_CALL and CALL_LOOP are exemplary composite actions.\nIn some embodiments, the INTERACTIVE CALL action decomposes into CALL, PLAY, AND WAIT actions.  In cases where a user should be called, and an audio message played to them while a DTMF tone is waited on to further the emergency flow to the next\nstate, a composite action can be used.\nIn some embodiments, inputs may include: phone_number: string, the phone-number to call; playback: media-resource to play; text: Text (string) to speech; time: Time to wait in seconds after the playback has stopped.\nAn exemplar INTERACTIVE_CALL action is shown below.\nTABLE-US-00008 action: INTERACTIVE_CALL inputs: {phone_number: \"${phone_number}\", text: \"${text}\"} on_output: \"false\": fail_state \"HANGUP@${phone_number}\": fail_state \"#1@${phone_number}\": next_state\nIn some embodiments, the CALL_LOOP action decomposes into a list of actions that perform a loop through a list of dictionaries that each contain a phone number of a user to call.  When the users are called, an audio is played to them, and they\ncan hit a dtmf to break out of the loop.  The final callee dict that is called during the loop is optionally saved in the variable given in the store parameter after the CALL_LOOP runs.\nIn some embodiments, REML includes some specific events to handle cases where two users are contacted simultaneously.  For example, HANGUP@&lt;phone&gt;: a hang-up event from user; #&lt;num&gt;@&lt;phone&gt;: DTMF event from user.\nIn some embodiments, three players are connected in an emergency call.  First, the user is called.  When the user picks up the call, a message is played (e.g. \"press 1 to connect .  . . \"). Once the user press `1`, a corporate call center or\nrepresentative may be called and during their call--if anyone hits `9`, an emergency call is made and connected to the appropriate PSAP (based on location), so that there can be three or more participants in the conference emergency call or session.\nThere are cases where there may be several stakeholders or several individuals with interest and/or information regarding an emergency situation.  Thus, it will be advantageous to connect the stakeholders in one emergency conference call. \nHowever, setting up a conference call with two or more participants has its challenges.  For example, one participant may hang up before another participant has joined.  To deal with such challenges, a communication bridge can be used as described below.\nIn some embodiments, two or more participants may be added to an emergency conference call or emergency session on the communication bridge (as shown in FIGS. 7 & 11).  In some embodiments, two or more participants may be added to an emergency\nconference call on the conference bridge (as shown in FIGS. 9 & 10).  As shown in FIG. 10, a customer service representative at an organization's operations center is contacted first and added to the communication bridge, if they agree to be connected to\nan EDC.  Then, the \"emergency contact\" or \"associated contact\" is called and if they choose to connect to an EDC, an ESP personnel is called and connected to the communication bridge.  In some cases, the user may also be called and added to the\ncommunication bridge.  In some embodiments, specialized events for setting up the communication bridge are contemplated including \"hangup\" and \"Dual-tone multi-frequency signaling (DTMF)\" for playing an audio message while a user is allowed to make an\ninput (where the input can be stored as a variable for later use) and maintaining the call even when one user hangs up.\nIn some embodiments, the participants in an emergency call are selected from a user, an associated contact, an emergency contact, an organizational representative, a customer service representative, an ESP personnel.  In some embodiments, the\nemergency conference session is a three-way conference call including a user, a corporate representative and a PSAP operator.\n<BR><BR>Audio Activation Embodiments\nIn one aspect, provided herein is a system for emergency communications comprising: (a) a triggering device detecting an audio emergency trigger and sending an emergency alert to the emergency management system; and (b) at least one server\nproviding a server application at the emergency management system, said server application.  In some embodiment, the server application is configured to perform the steps of: (i) receiving the emergency alert from the triggering device; (ii) determining\na callback number for the triggering device, wherein the callback number is selected and assigned to the triggering device when the triggering device does not have a phone number; (iii) determining a location of the triggering device, pairing the\nlocation with the callback number, and updating a location database that can be made accessible to emergency service providers with the paired location and callback number; (iv) identifying an appropriate emergency service provider for responding to the\nemergency alert based on at least one of the location of the triggering device, a source of the emergency alert on the triggering device, and an emergency indication; and (v) facilitating an emergency communication session between the triggering device\nand the appropriate emergency service provider, wherein the callback number enables the session with the triggering device to be re-established after the session is disconnected.  In some embodiments, the audio emergency trigger is a voice command.  In\nsome embodiments, the emergency alert comprises an API request to the emergency management system to initiate the emergency communication session.  In some embodiments, the triggering device is a phone and the callback number is a phone number of the\ntriggering device.  In some embodiments, the triggering device is an Internet of Things (IoT) device.  In some embodiments, the triggering device is a smart speaker.  In some embodiments, the triggering device is connected as part of a mesh network.  In\nsome embodiments, the triggering device is part of a local network of smart devices.  In some embodiments, the local network is a home, office, or business network.  In some embodiments, the emergency alert from the triggering device is sent to the\nemergency management system through a mesh network or an intermediate communication device.  In some embodiments, the intermediate communication device is a cell phone or Internet router.  In some embodiments, the triggering device gathers emergency\ninformation in response to detecting the audio emergency trigger and provides the emergency information to the emergency management system.  In further embodiments, the emergency information comprises location information, sensor data, audio, video, user\ninformation, emergency contact information, type of emergency, or any combination thereof.  In further embodiments, the triggering device gathers the emergency information from one or more devices in a local network.  In some embodiments, the location\ndatabase is an E911 ALI database.  In some embodiments, the emergency communication session is a two-way audio session established using SIP protocol.  In some embodiments, the triggering device utilizes a voice assistance service to transmit the\nemergency alert to the emergency management system as an API request.  In some embodiments, the server application further comprises a software module re-establishing the communication session upon receiving a request to re-establish communications with\nthe callback number from the emergency service provider.  In some embodiments, the triggering device receives sensor data from a network of sensor devices indicative of an emergency situation and attempts to report the emergency situation to a user.  In\nfurther embodiments, the triggering device attempts to report the emergency situation to the user by broadcasting an audio message.  In some embodiments, participants in the emergency communication session are the emergency service provider and an\nassociated device of the triggering device.  In further embodiments, the associated device is a communication device associated with the triggering device or a user of the triggering device and is selected from a prioritized list of associated devices to\nparticipate in the emergency communication session.  In some embodiments, the source of the emergency alert on the triggering device is a mobile application.  In further embodiments, the mobile application is a social media application, an emergency\napplication, a messaging application, or an image or video sharing application.\nIn another aspect, provided herein is a computer-implemented method for facilitating emergency communications comprising: (a) receiving an emergency alert from a triggering device that is sent in response to detection of an emergency audio\ntrigger; (b) determining a callback number for the triggering device, wherein the callback number is selected and assigned to the triggering device when the triggering device does not have a phone number; (c) determining a location of the triggering\ndevice, pairing the location with the callback number, and updating a location database that can be made accessible to emergency service providers with the paired location and callback number; (d) identifying an appropriate emergency service provider for\nresponding to the emergency alert based on at least one of the location of the triggering device, a source of the emergency alert on the triggering device, and an emergency indication; and (e) facilitating an emergency communication session between the\ntriggering device and the appropriate emergency service provider, wherein the callback number enables the session with the triggering device to be re-established after the session is disconnected.  In some embodiments, the audio emergency trigger is a\nvoice command.  In some embodiments, the emergency alert comprises an API request to the emergency management system to initiate the emergency communication session.  In some embodiments, the triggering device is a phone and the callback number is a\nphone number of the triggering device.  In some embodiments, the triggering device is an Internet of Things (IoT) device.  In some embodiments, the triggering device is a smart speaker.  In some embodiments, the triggering device is connected as part of\na mesh network.  In some embodiments, the triggering device is part of a local network of smart devices.  In further embodiments, the local network is a home, office, or business network.  In some embodiments, the emergency alert from the triggering\ndevice is sent to the emergency management system through a mesh network or an intermediate communication device.  In further embodiments, the intermediate communication device is a cell phone or Internet router.  In some embodiments, the triggering\ndevice gathers emergency information in response to detecting the audio emergency trigger and provides the emergency information to the emergency management system.  In further embodiments, the emergency information comprises location information, sensor\ndata, audio, video, user information, emergency contact information, type of emergency, or any combination thereof.  In further embodiments, the triggering device gathers the emergency information from one or more devices in a local network.  In some\nembodiments, the location database is an E911 ALI database.  In some embodiments, the emergency communication session is a two-way audio session established using SIP protocol.  In some embodiments, the triggering device utilizes a voice assistance\nservice to transmit the emergency alert to the emergency management system as an API request.  In some embodiments, the server application further comprises a software module re-establishing the communication session upon receiving a request to\nre-establish communications with the callback number from the emergency service provider.  In some embodiments, the triggering device receives sensor data from a network of sensor devices indicative of an emergency situation and attempts to report the\nemergency situation to a user.  In further embodiments, the triggering device attempts to report the emergency situation to the user by broadcasting an audio message.  In some embodiments, participants in the emergency communication session are the\nemergency service provider and an associated device of the triggering device.  In further embodiments, the associated device is a communication device associated with the triggering device or a user of the triggering device and is selected from a\nprioritized list of associated devices to participate in the emergency communication session.  In some embodiments, the source of the emergency alert on the triggering device is a mobile application.  In further embodiments, the mobile application is a\nsocial media application, an emergency application, a messaging application, or an image or video sharing application.\nIn another aspect, provided herein is a non-transitory computer readable storage media encoded with at least one computer program including instructions executable by at least one processor to perform steps comprising: (a) receiving an emergency\nalert from a triggering device that is sent in response to detection of an emergency audio trigger; (b) determining a callback number for the triggering device, wherein the callback number is selected and assigned to the triggering device when the\ntriggering device does not have a phone number; (c) determining a location of the triggering device, pairing the location with the callback number, and updating a location database that can be made accessible to emergency service providers with the\npaired location and callback number; (d) identifying an appropriate emergency service provider for responding to the emergency alert based on at least one of the location of the triggering device, a source of the emergency alert on the triggering device,\nand an emergency indication; and (e) facilitating an emergency communication session between the triggering device and the appropriate emergency service provider, wherein the callback number enables the session with the triggering device to be\nre-established after the session is disconnected.  In some embodiments, the audio emergency trigger is a voice command.  In some embodiments, the emergency alert comprises an API request to the emergency management system to initiate the emergency\ncommunication session.  In some embodiments, the triggering device is a phone and the callback number is a phone number of the triggering device.  In some embodiments, the triggering device is an Internet of Things (IoT) device.  In some embodiments, the\ntriggering device is a smart speaker.  In some embodiments, the triggering device is connected as part of a mesh network.  In some embodiments, the triggering device is part of a local network of smart devices.  In further embodiments, the local network\nis a home, office, or business network.  In some embodiments, the emergency alert from the triggering device is sent to the emergency management system through a mesh network or an intermediate communication device.  In further embodiments, the\nintermediate communication device is a cell phone or Internet router.  In some embodiments, the triggering device gathers emergency information in response to detecting the audio emergency trigger and provides the emergency information to the emergency\nmanagement system.  In further embodiments, the emergency information comprises location information, sensor data, audio, video, user information, emergency contact information, type of emergency, or any combination thereof.  In further embodiments, the\ntriggering device gathers the emergency information from one or more devices in a local network.  In some embodiments, the location database is an E911 ALI database.  In some embodiments, the emergency communication session is a two-way audio session\nestablished using SIP protocol.  In some embodiments, the triggering device utilizes a voice assistance service to transmit the emergency alert to the emergency management system as an API request.  In some embodiments, the server application further\ncomprises a software module re-establishing the communication session upon receiving a request to re-establish communications with the callback number from the emergency service provider.  In some embodiments, the triggering device receives sensor data\nfrom a network of sensor devices indicative of an emergency situation and attempts to report the emergency situation to a user.  In further embodiments, the triggering device attempts to report the emergency situation to the user by broadcasting an audio\nmessage.  In some embodiments, participants in the emergency communication session are the emergency service provider and an associated device of the triggering device.  In further embodiments, the associated device is a communication device associated\nwith the triggering device or a user of the triggering device and is selected from a prioritized list of associated devices to participate in the emergency communication session.  In some embodiments, the source of the emergency alert on the triggering\ndevice is a mobile application.  In further embodiments, the mobile application is a social media application, an emergency application, a messaging application, or an image or video sharing application\n<BR><BR>Waterfall Calling Embodiments\nIn one aspect, provided herein is a system for emergency communications comprising: (a) triggering device of a user transmitting an emergency alert when an emergency is triggered; and (b) at least one server providing an emergency management\nsystem server application.  In some embodiments emergency management system server application: (i) maintains at least one database comprising a at least one associated device for the triggering device; (ii) receives the emergency alert from the\ntriggering device; (iii) establishes a connection with the triggering device upon determining that the triggering device is able to host a connection with the server; and (iv) connects to the at least one associated device in the list, wherein the at\nleast the one associated device is authorized by the user to be connected to the at least one dispatch center for requesting emergency assistance.  In some embodiments, the server application connects to at least two associated devices concurrently.  In\nsome embodiments, the server application attempts to connect with the triggering device before connecting with the at least one associated device.  In some embodiments, the database comprises a prioritized list having a plurality of associated devices. \nIn further embodiments, the server application connects to a next associated device in the prioritized list in order of priority when a connection with a preceding associated device is unsuccessful.  In some embodiments, the server application looks up\nan associated device by querying the at least one database using an identifier for the emergency alert, wherein the identifier is a telephone number, an email address, a physical address, x-y coordinates, BSSID, SSID, and MAC address of the associated\ndevice Temporary Mobile Station Identifier (TMSI), or Internet Protocol (IP) address of the triggering device.  In some embodiments, the server application assigns a telephone number to the triggering device.  In further embodiments, the phone number is\na telephone number of the associated device or a 10-digit number from a pool of inward dial telephone numbers.  In some embodiments, the server application establishes an audio call with the triggering device upon determining the triggering device is\nable to host an audio call.  In some embodiments, the user is presented with an option to connect to a dispatch center or disconnect.  In some embodiments, the user receives a notification when the emergency alert has been triggered and the server\napplication was unable to connect to the user or emergency contacts.  In some embodiments, the emergency alert is triggered autonomously based on sensor data when sensor values exceed a threshold.  In some embodiments, the emergency alert is triggered by\ndetection of at least one trigger sound or voice command.  In further embodiments, the at least one trigger sound comprises at least one of a gunshot, explosion, breaking glass, person falling, baby crying, police siren, ambulance siren, fire truck\nsiren, outdoor warning siren, and a crash.  In further embodiments, the emergency alert is triggered by detection of at least one trigger sound that exceeds a decibel threshold.  In some embodiments, the emergency is triggered by user interaction with\nthe triggering device, wherein the user interaction comprises at least one of pressing a soft or hard button, a gesture, or a voice command.  In some embodiments, the emergency is triggered by the user giving a voice command, wherein the triggering\ndevice recognizes the voice command to detect one or more trigger phrases.  In further embodiments, the triggering device confirms user identity using voice recognition.  In some embodiments, the triggering device transmits the emergency alert using\nanother communication device through an indirect channel.\nIn another aspect, provided herein is a method for an emergency management system to connect to a user for providing emergency assistance comprising: (a) maintaining, by the emergency management system, at least one database for lookup of\ntriggering devices and at least one associated device for each triggering device; (b) receiving, by the emergency management system, an emergency alert from a triggering device; (c) determining, by the emergency management system, whether the triggering\ndevice is able to host a connection with the user; (d) connecting, by the emergency management system, to the triggering device if the triggering device is able to host the connection; (e) connecting, by the emergency management system, to at least one\nassociated device; and (f) communicating, by the emergency management system, with the user via the at least one associated device for requesting emergency assistance from at least one dispatch center.  In some embodiments, the emergency management\nsystem maintains a pool of inward dial telephone numbers for assigning to the triggering device for a communication session.  In some embodiments, the dispatch center is able to call back to the triggering device or associated device using an assigned\ntelephone number.  In some embodiments, the emergency management system attempts to connect with the triggering device before connecting with the at least one associated device.  In some embodiments, the database comprises is a prioritized list having a\nplurality of associated devices.  In further embodiments, the emergency management system connects to a next associated device in the prioritized list in order of priority when a connection with a preceding associated device is unsuccessful.  In some\nembodiments, the emergency management system looks up an associated device by querying the at least one database using a telephone number associated with the triggering device.  In some embodiments, the emergency management system looks up an associated\ndevice by querying the at least one database using one of an email address, physical address, x-y coordinate, BSSID, SSID, and MAC address.  In some embodiments, the emergency management system determines an appropriate dispatch center based on the type\nof emergency, location of emergency and other factors.  In some embodiments, the system establishes an audio call with the triggering device upon determining the triggering device is able to host an audio call.  In some embodiments, the user is presented\nwith an option to connect to a dispatch center or disconnect.  In some embodiments, the emergency alert is sent to the emergency management center indirectly from the triggering device via a communication device.  In some embodiments, the emergency alert\nis triggered autonomously based on sensor data.  In further embodiments, the emergency alert is triggered by at least one trigger sound, wherein the trigger sound comprises at least one of a gunshot, explosion, breaking glass, person falling, baby\ncrying, police siren, ambulance siren, fire truck siren, outdoor warning siren, and a crash.  In further embodiments, the emergency alert is triggered by detection of at least one trigger sound that exceeds a decibel threshold.  In some embodiments, the\nemergency is triggered by user interaction with the triggering device.  In further embodiments, the user interaction comprises at least one of pressing a soft or hard button, a gesture, and a voice command.  In some embodiments, the emergency is\ntriggered by the user giving a voice command, wherein the triggering device recognizes the voice command to detect one or more trigger phrases.  In further embodiments, the triggering device confirms user identity using voice recognition.\nIn another aspect, provided herein is a non-transitory computer readable storage media encoded with a computer program including instructions executable by at least one processor to create a server application for connecting a user to at least\none dispatch center for facilitating emergency assistance, the application comprising: (a) a software module receiving an emergency alert from the triggering device; (b) a software module maintaining at least one database comprising a list of at least\none associated device of the triggering device; (c) a software module establishing a connection with the triggering device upon determining that the triggering device is able to host a connection with the EMS; and (d) a software module connecting to at\nleast one associated device in the list, wherein the at least one associated device in the list is authorized by the user to be connected to the at least one dispatch center for requesting emergency assistance.  In some embodiments, the server\napplication connects to at least two associated devices concurrently.  In some embodiments, the server application connects with the triggering device before connecting with the at least one associated device.  In some embodiments, the list is a\nprioritized list having a plurality of associated devices.\nIn another aspect, provided herein is a method for providing the location of an emergency, by an emergency management system, comprising: (a) receiving, by the emergency management system, an emergency alert from a triggering device, wherein the\nemergency alert does not include location data; (b) searching, by the emergency management system, one or more databases for the location data for the triggering device, wherein the location data is a physical address or a set of x-y coordinates; (c)\nrequesting, by the emergency management system, current location data from the triggering device or at least one associated device; and (d) making, by the emergency management system, the location data available to one or more dispatch centers for\nproviding emergency assistance.  In some embodiments, the method further comprises determining an accuracy and confidence for the location data.  In further embodiments, the accuracy of the location data is made available to the one or more dispatch\ncenters.  In some embodiments, the method further comprises converting the location data into converted location data, wherein the converted location data is in compatible format for the one or more dispatch centers.  In some embodiments, an accuracy of\nthe converted location data is evaluated.  In some embodiments, the emergency management system queries one or more databases for location data for the triggering device.  In some embodiments, the triggering device is installed within a building and the\nlocation data associated with the triggering device is saved in one or more databases in the emergency management system.  In some embodiments, the triggering device is a mobile device or hosted within a mobile vehicle and the emergency management system\nobtains the current location data from the triggering device.  In some embodiments, the triggering device is a vehicle console or a vehicle computer.  In some embodiments, the triggering device is assigned a token, wherein the emergency alert from the\ntriggering device is tagged with the token and the location of the triggering device is provisioned with the user's phone number.  In some embodiments, the emergency alert is triggered by detection of at least one trigger sound.  In further embodiments,\nthe at least one trigger sound comprises at least one of a gunshot, explosion, breaking glass, person falling, baby crying, police siren, ambulance siren, fire truck siren, outdoor warning siren, and a crash.  In further embodiments, the emergency alert\nis triggered by detection of at least one trigger sound that exceeds a decibel threshold.  In some embodiments, the triggering device transmits the emergency alert indirectly via a communication device.  In some embodiments, the emergency is triggered by\nuser interaction, wherein the user interaction comprises at least one of pressing a soft or hard button, a gesture, and a voice command.  In some embodiments, the emergency is triggered by a user giving a voice command, wherein the triggering device\nrecognizes the voice command to detect one or more trigger phrases.  In some embodiments, the triggering device confirms user identity using voice recognition.\nIn another aspect, provided herein is an emergency management system comprising at least one processor, a memory, a network element, and a computer program including instructions executable by the at least one processor to create a server\nsoftware application for connecting a user to at least one dispatch center for facilitating emergency assistance, the application comprising: (a) a software module receiving an emergency alert from a triggering device, wherein the emergency alert does\nnot include location data; and (b) a software module configured to perform steps comprising: (i) searching one or more databases for the location data for the triggering device; (ii) requesting current location data from the triggering device or at least\none associated device; and (iii) making the location data available to one or more dispatch centers for providing emergency assistance.  In further embodiments, the application further comprises a software module determining an accuracy and confidence\nfor the location data.\nIn another aspect, provided herein is a non-transitory computer readable storage media encoded with a computer program including instructions executable by at least one processor to create a server application for connecting a user to at least\none dispatch center for facilitating emergency assistance, the application comprising: (a) a software module receiving an emergency alert from a triggering device, wherein the emergency alert does not include location data; an (b) a software module\nconfigured to perform steps comprising: (i) searching one or more databases for the location data for the triggering device; (ii) requesting current location data from the triggering device or at least one associated device; and (iii) making the location\ndata available to one or more dispatch centers for providing emergency assistance.  In some embodiments, the location data is a physical address or a set of x-y coordinates.  In some embodiments, the server application queries one or more databases for\nlocation data for the triggering device.  In some embodiments, the server application queries one or more databases for location data for one or more users associated with the triggering device.  In some embodiments, wherein the triggering device is\ninstalled within a building and the location data associated with the triggering device is saved in one or more databases in the emergency management system.\nIn another aspect, provided herein is a method of transmitting an emergency alert and providing data to an emergency management system, comprising: (a) detecting, by a triggering device, an emergency; (b) initiating, by the triggering device, an\nemergency alert upon detecting the emergency; (c) transmitting, by the triggering device, the emergency alert to an emergency management system when the emergency alert is triggered, wherein the emergency alert comprises information for at least one\nassociated device capable of communicating with the emergency management system; and (d) providing, by the triggering device, data from at least one of the triggering device and the at least one associated device to the emergency management system,\nwherein the data comprises location information.  In some embodiments, a connection is established between the triggering device and at least one dispatch center.  In some embodiments, an audio call is established between the triggering device and at\nleast one dispatch center upon determining the triggering device is able to host an audio call.  In some embodiments, the emergency alert is triggered autonomously based on sensor data when sensor values exceed a threshold.\nIn another aspect, a triggering device comprising at least one processor, a memory, a network element, and a computer program including instructions executable by the at least one processor to create an emergency trigger application comprising:\n(a) a software module detecting an emergency; (b) a software module creating an emergency alert upon detecting an emergency; (c) a software module transmitting the emergency alert to an emergency management system when the emergency alert is triggered,\nwherein the emergency alert comprises information for at least one associated device capable of communicating with the emergency management system; and (d) a software module providing data from at least one of the triggering device and the at least one\nassociated device to the emergency management system, wherein the data comprises location information.  In some embodiments, an audio connection is established between the triggering device and at least one dispatch center.  In some embodiments, a user\nof the triggering device receives a notification when the emergency alert has been triggered and the emergency management system was unable to connect to the user.  In some embodiments, the emergency alert is triggered when sensor values exceed a\nthreshold.  In some embodiments, the emergency alert is triggered by detection of at least one trigger sound.  In further embodiments, the at least one trigger sound comprises at least one of a gunshot, explosion, breaking glass, person falling, baby\ncrying, police siren, ambulance siren, fire truck siren, outdoor warning siren, and a crash.  In some embodiments, the emergency alert is triggered by detection of at least one trigger sound that exceeds a decibel threshold.  In some embodiments, the\nemergency is triggered by user interaction with the triggering device.  In some embodiments, the user interaction comprises at least one of pressing a soft or hard button, a gesture, and a voice command.  In some embodiments, the emergency is triggered\nby a user giving a voice command, wherein the triggering device recognizes the voice command to detect one or more trigger phrases.  In some embodiments, the triggering device confirms user identity using voice recognition.\nElectronic Device\nIn some embodiments, the platforms, media, methods and applications described herein include an electronic device, a processor, or use of the same (also referred to as a digital processing device).  In further embodiments, the electronic device\nincludes one or more hardware central processing units (CPU) that carry out the device's functions.  In still further embodiments, the electronic device further comprises an operating system configured to perform executable instructions.  In some\nembodiments, the electronic device is optionally connected a computer network.  In further embodiments, the electronic device is optionally connected to the Internet such that it accesses the World Wide Web.  In still further embodiments, the electronic\ndevice is optionally connected to a cloud computing infrastructure.  In other embodiments, the electronic device is optionally connected to an intranet.  In other embodiments, the electronic device is optionally connected to a data storage device.  In\naccordance with the description herein, suitable electronic devices include, by way of non-limiting examples, server computers, desktop computers, laptop computers, notebook computers, sub-notebook computers, netbook computers, net pad computers, set-top\ncomputers, handheld computers, Internet appliances, mobile smartphones, tablet computers, personal digital assistants, video game consoles, and vehicles.  Those of skill in the art will recognize that many smartphones are suitable for use in the system\ndescribed herein.  Those of skill in the art will also recognize that select televisions, video players, and digital music players with optional computer network connectivity are suitable for use in the system described herein.  Suitable tablet computers\ninclude those with booklet, slate, and convertible configurations, known to those of skill in the art.\nIn some embodiments, the electronic device includes an operating system configured to perform executable instructions.  The operating system is, for example, software, including programs and data, which manages the device's hardware and provides\nservices for execution of applications.  Those of skill in the art will recognize that suitable server operating systems include, by way of non-limiting examples, FreeBSD, OpenBSD, NetBSD, Linux, Apple.RTM.  Mac OS X Server.RTM., Oracle.RTM. \nSolaris.RTM., Windows Server.RTM., and Novell.RTM.  NetWare.RTM..  Those of skill in the art will recognize that suitable personal computer operating systems include, by way of non-limiting examples, Microsoft.RTM.  Windows.RTM., Apple.RTM.  Mac OS\nX.RTM., UNIX.RTM., and UNIX-like operating systems such as GNU/Linux.RTM..  In some embodiments, the operating system is provided by cloud computing.  Those of skill in the art will also recognize that suitable mobile smart phone operating systems\ninclude, by way of non-limiting examples, Nokia.RTM.  Symbian.RTM.  OS, Apple.RTM.  iOS.RTM., Research In Motion.RTM.  BlackBerry OS.RTM., Google.RTM.  Android.RTM., Microsoft.RTM.  Windows Phone.RTM.  OS, Microsoft.RTM.  Windows Mobile.RTM.  OS,\nLinux.RTM., and Palm.RTM.  WebOS.RTM..\nIn some embodiments, the device includes a storage and/or memory device.  The storage and/or memory device is one or more physical apparatuses used to store data or programs on a temporary or permanent basis.  In some embodiments, the device is\nvolatile memory and requires power to maintain stored information.  In some embodiments, the device is non-volatile memory and retains stored information when the electronic device is not powered.  In further embodiments, the non-volatile memory\ncomprises flash memory.  In some embodiments, the non-volatile memory comprises dynamic random-access memory (DRAM).  In some embodiments, the non-volatile memory comprises ferroelectric random-access memory (FRAM).  In some embodiments, the non-volatile\nmemory comprises phase-change random access memory (PRAM).  In some embodiments, the non-volatile memory comprises magnetoresistive random-access memory (MRAM).  In other embodiments, the device is a storage device including, by way of non-limiting\nexamples, CD-ROMs, DVDs, flash memory devices, magnetic disk drives, magnetic tapes drives, optical disk drives, and cloud computing-based storage.  In further embodiments, the storage and/or memory device is a combination of devices such as those\ndisclosed herein.\nIn some embodiments, the electronic device includes a display to send visual information to a subject.  In some embodiments, the display is a cathode ray tube (CRT).  In some embodiments, the display is a liquid crystal display (LCD).  In\nfurther embodiments, the display is a thin film transistor liquid crystal display (TFT-LCD).  In some embodiments, the display is an organic light emitting diode (OLED) display.  In various further embodiments, on OLED display is a passive-matrix OLED\n(PMOLED) or active-matrix OLED (AMOLED) display.  In some embodiments, the display is a plasma display.  In some embodiments, the display is E-paper or E ink.  In other embodiments, the display is a video projector.  In still further embodiments, the\ndisplay is a combination of devices such as those disclosed herein.\nIn some embodiments, the electronic device includes an input device to receive information from a subject.  In some embodiments, the input device is a keyboard.  In some embodiments, the input device is a pointing device including, by way of\nnon-limiting examples, a mouse, trackball, trackpad, joystick, game controller, or stylus.  In some embodiments, the input device is a touch screen or a multi-touch screen.  In other embodiments, the input device is a microphone to capture voice or other\nsound input.  In other embodiments, the input device is a video camera or other sensor to capture motion or visual input.  In further embodiments, the input device is a Kinect, Leap Motion, or the like.  In still further embodiments, the input device is\na combination of devices such as those disclosed herein.\nNon-Transitory Computer Readable Storage Medium\nIn some embodiments, the platforms, media, methods and applications described herein include one or more non-transitory computer readable storage media encoded with a program including instructions executable by the operating system of an\noptionally networked digital processing device.  In further embodiments, a computer readable storage medium is a tangible component of an electronic device.  In still further embodiments, a computer readable storage medium is optionally removable from an\nelectronic device.  In some embodiments, a computer readable storage medium includes, by way of non-limiting examples, CD-ROMs, DVDs, flash memory devices, solid state memory, magnetic disk drives, magnetic tape drives, optical disk drives, cloud\ncomputing systems and services, and the like.  In some cases, the program and instructions are permanently, substantially permanently, semi-permanently, or non-transitorily encoded on the media.\nComputer Program\nIn some embodiments, the platforms, media, methods and applications described herein include at least one computer program, or use of the same.  A computer program includes a sequence of instructions, executable in the electronic device's CPU,\nwritten to perform a specified task.  Computer readable instructions may be implemented as program modules, such as functions, objects, Application Programming Interfaces (APIs), data structures, and the like, that perform particular tasks or implement\nparticular abstract data types.  In light of the disclosure provided herein, those of skill in the art will recognize that a computer program may be written in various versions of various languages.\nThe functionality of the computer readable instructions may be combined or distributed as desired in various environments.  In some embodiments, a computer program comprises one sequence of instructions.  In some embodiments, a computer program\ncomprises a plurality of sequences of instructions.  In some embodiments, a computer program is provided from one location.  In other embodiments, a computer program is provided from a plurality of locations.  In various embodiments, a computer program\nincludes one or more software modules.  In various embodiments, a computer program includes, in part or in whole, one or more web applications, one or more mobile applications, one or more standalone applications, one or more web browser plug-ins,\nextensions, add-ins, or add-ons, or combinations thereof.\nWeb Application\nIn some embodiments, a computer program includes a web application.  In light of the disclosure provided herein, those of skill in the art will recognize that a web application, in various embodiments, utilizes one or more software frameworks\nand one or more database systems.  In some embodiments, a web application is created upon a software framework such as Microsoft .NET or Ruby on Rails (RoR).  In some embodiments, a web application utilizes one or more database systems including, by way\nof non-limiting examples, relational, non-relational, object oriented, associative, and XML database systems.  In further embodiments, suitable relational database systems include, by way of non-limiting examples, Microsoft.RTM.  SQL Server, mySQL.TM.,\nand Oracle.RTM..  Those of skill in the art will also recognize that a web application, in various embodiments, is written in one or more versions of one or more languages.  A web application may be written in one or more markup languages, presentation\ndefinition languages, client-side scripting languages, server-side coding languages, database query languages, or combinations thereof.  In some embodiments, a web application is written to some extent in a markup language such as Hypertext Markup\nLanguage (HTML), Extensible Hypertext Markup Language (XHTML), or eXtensible Markup Language (XML).  In some embodiments, a web application is written to some extent in a presentation definition language such as Cascading Style Sheets (CSS).  In some\nembodiments, a web application is written to some extent in a client-side scripting language such as Asynchronous Javascript and XML (AJAX), Flash.RTM.  Actionscript, Javascript, or Silverlight.RTM..  In some embodiments, a web application is written to\nsome extent in a server-side coding language such as Active Server Pages (ASP), ColdFusion.RTM., Perl, Java.TM., JavaServer Pages (JSP), Hypertext Preprocessor (PHP), Python.TM., Ruby, Tcl, Smalltalk, WebDNA.RTM., or Groovy.  In some embodiments, a web\napplication is written to some extent in a database query language such as Structured Query Language (SQL).  In some embodiments, a web application integrates enterprise server products such as IBM.RTM.  Lotus Domino.RTM..  In some embodiments, a web\napplication includes a media player element.  In various further embodiments, a media player element utilizes one or more of many suitable multimedia technologies including, by way of non-limiting examples, Adobe.RTM.  Flash.RTM., HTML 5, Apple.RTM. \nQuickTime.RTM., Microsoft Silverlight.RTM., Java.TM., and Unity.RTM..\nMobile Application\nIn some embodiments, a computer program includes a mobile application provided to a mobile electronic device.  In some embodiments, the mobile application is provided to a mobile electronic device at the time it is manufactured.  In other\nembodiments, the mobile application is provided to a mobile electronic device via the computer network described herein.\nIn view of the disclosure provided herein, a mobile application is created by techniques known to those of skill in the art using hardware, languages, and development environments known to the art.  Those of skill in the art will recognize that\nmobile applications are written in several languages.  Suitable programming languages include, by way of non-limiting examples, C, C++, C#, Objective-C, Java.TM., Javascript, Pascal, Object Pascal, Python.TM., Ruby, VB.NET, WML, and XHTML/HTML with or\nwithout CSS, or combinations thereof.\nSuitable mobile application development environments are available from several sources.  Commercially available development environments include, by way of non-limiting examples, AirplaySDK, alcheMo, Appcelerator.RTM., Celsius, Bedrock, Flash\nLite, .NET Compact Framework, Rhomobile, and WorkLight Mobile Platform.  Other development environments are available without cost including, by way of non-limiting examples, Lazarus, MobiFlex, MoSync, and Phonegap.  Also, mobile device manufacturers\ndistribute software developer kits including, by way of non-limiting examples, iPhone and iPad (iOS) SDK, Android.TM.  SDK, BlackBerry.RTM.  SDK, BREW SDK, Palm.RTM.  OS SDK, Symbian SDK, webOS SDK, and Windows.RTM.  Mobile SDK.\nThose of skill in the art will recognize that several commercial forums are available for distribution of mobile applications including, by way of non-limiting examples, Apple.RTM.  App Store, Android.TM.  Market, BlackBerry.RTM.  App World, App\nStore for Palm devices, App Catalog for webOS, Windows.RTM.  Marketplace for Mobile, Ovi Store for Nokia.RTM.  devices, Samsung.RTM.  Apps, and Nintendo.RTM.  DSi Shop.\nStandalone Application\nIn some embodiments, a computer program includes a standalone application, which is a program that is run as an independent computer process, not an add-on to an existing process, e.g. not a plug-in. Those of skill in the art will recognize that\nstandalone applications are often compiled.  A compiler is a computer program(s) that transforms source code written in a programming language into binary object code such as assembly language or machine code.  Suitable compiled programming languages\ninclude, by way of non-limiting examples, C, C++, Objective-C, COBOL, Delphi, Eiffel, Java.TM., Lisp, Python.TM., Visual Basic, and VB .NET, or combinations thereof.  Compilation is often performed, at least in part, to create an executable program.  In\nsome embodiments, a computer program includes one or more executable compiled applications.\nSoftware Modules\nIn some embodiments, the platforms, media, methods and applications described herein include software, server, and/or database modules, or use of the same.  In view of the disclosure provided herein, software modules are created by techniques\nknown to those of skill in the art using machines, software, and languages known to the art.  The software modules disclosed herein are implemented in a multitude of ways.  In various embodiments, a software module comprises a file, a section of code, a\nprogramming object, a programming structure, or combinations thereof.  In further various embodiments, a software module comprises a plurality of files, a plurality of sections of code, a plurality of programming objects, a plurality of programming\nstructures, or combinations thereof.  In various embodiments, the one or more software modules comprise, by way of non-limiting examples, a web application, a mobile application, and a standalone application.  In some embodiments, software modules are in\none computer program or application.  In other embodiments, software modules are in more than one computer program or application.  In some embodiments, software modules are hosted on one machine.  In other embodiments, software modules are hosted on\nmore than one machine.  In further embodiments, software modules are hosted on cloud computing platforms.  In some embodiments, software modules are hosted on one or more machines in one location.  In other embodiments, software modules are hosted on one\nor more machines in more than one location.\nDatabases\nIn some embodiments, the platforms, systems, media, and methods disclosed herein include one or more databases, or use of the same.  In view of the disclosure provided herein, those of skill in the art will recognize that many databases are\nsuitable for storage and retrieval of barcode, route, parcel, subject, or network information.  In various embodiments, suitable databases include, by way of non-limiting examples, relational databases, non-relational databases, object-oriented\ndatabases, object databases, entity-relationship model databases, associative databases, and XML databases.  In some embodiments, a database is internet-based.  In further embodiments, a database is web-based.  In still further embodiments, a database is\ncloud computing-based.  In other embodiments, a database is based on one or more local computer storage devices.\nWeb Browser Plug-in\nIn some embodiments, the computer program includes a web browser plug-in. In computing, a plug-in is one or more software components that add specific functionality to a larger software application.  Makers of software applications support\nplug-ins to enable third-party developers to create abilities which extend an application, to support easily adding new features, and to reduce the size of an application.  When supported, plug-ins enable customizing the functionality of a software\napplication.  For example, plug-ins are commonly used in web browsers to play video, generate interactivity, scan for viruses, and display particular file types.  Those of skill in the art will be familiar with several web browser plug-ins including,\nAdobe.RTM.  Flash.RTM.  Player, Microsoft.RTM.  Silverlight.RTM., and Apple.RTM.  QuickTime.RTM..  In some embodiments, the toolbar comprises one or more web browser extensions, add-ins, or add-ons.  In some embodiments, the toolbar comprises one or more\nexplorer bars, tool bands, or desk bands.\nIn view of the disclosure provided herein, those of skill in the art will recognize that several plug-in frameworks are available that enable development of plug-ins in various programming languages, including, by way of non-limiting examples,\nC++, Delphi, Java.TM., PHP, Python.TM., and VB .NET, or combinations thereof.\nWeb browsers (also called Internet browsers) are software applications, designed for use with network-connected electronic devices, for retrieving, presenting, and traversing information resources on the World Wide Web.  Suitable web browsers\ninclude, by way of non-limiting examples, Microsoft.RTM.  Internet Explorer.RTM., Mozilla.RTM.  Firefox.RTM., Google.RTM.  Chrome, Apple.RTM.  Safari.RTM., Opera Software.RTM.  Opera.RTM., and KDE Konqueror.  In some embodiments, the web browser is a\nmobile web browser.  Mobile web browsers (also called microbrowsers, mini-browsers, and wireless browsers) are designed for use on mobile electronic devices including, by way of non-limiting examples, handheld computers, tablet computers, netbook\ncomputers, subnotebook computers, smartphones, music players, personal digital assistants (PDAs), and handheld video game systems.  Suitable mobile web browsers include, by way of non-limiting examples, Google.RTM.  Android.RTM.  browser, RIM\nBlackBerry.RTM.  Browser, Apple.RTM.  Safari.RTM., Palm.RTM.  Blazer, Palm.RTM.  WebOS.RTM.  Browser, Mozilla.RTM.  Firefox.RTM.  for mobile, Microsoft.RTM.  Internet Explorer.RTM.  Mobile, Amazon.RTM.  Kindle.RTM.  Basic Web, Nokia.RTM.  Browser, Opera\nSoftware.RTM.  Opera.RTM.  Mobile, and Sony.RTM.  PSP.TM.  browser.\n<BR><BR>EXAMPLES\nThe following illustrative example is representative of embodiments of the inventions described herein and is not meant to be limiting in any way.\n<BR><BR>Example 1--Wearables+Parallel Flows\nJohn, an elderly man, lives in a nursing home.  He suffers from a medical condition that makes him susceptible to falls and is given a smartwatch by the nursing home to provide health monitoring and to allow him to request help in case of an\naccident.  Other residents of the nursing home are also assigned various similar devices (panic buttons, smartphones, etc.).  The nursing home administrator has used an emergency flow editor software to create an emergency flow script for handling\nemergency alerts sent by the various smartwatches and other devices used by the nursing home.  The editor software provides a graphic user interface with an interactive space that allows the administrator (an authorized user) to select graphical\nrepresentations of emergency flow building blocks and drag them over to a desired location within the interactive space.  The administrator is easily able to generate the customized emergency flow script by dragging and dropping the building blocks and\nconnecting them.  Although the editor software provides some standard pre-made building block templates, the administrator is able to customize and configure selected building block templates.  In this case, the administrator adds a building block that\nsorts emergency alerts based on the type of device sending the alert.  The building block is connected to multiple downstream emergency flows, which are executed based on the device type that is identified.  In this case, a downstream emergency flow\nassociated with smartwatches has building blocks that open a text and/or chat-based communication channel with an operator at the nursing home.  The downstream emergency flow includes a first parallel emergency flow that obtains data from the smartwatch,\nincluding heart rate sensor data and current location data (either received through the emergency alert or by querying the smartwatch).  The downstream emergency flow also includes a second parallel emergency flow that has a building block that\ndetermines whether the emergency is a personal emergency (e.g. health emergency such as a heart attack) or an environmental emergency (e.g. fire, earthquake), and responsive to detecting an environmental emergency, obtains sensor data from any sensors\n(e.g. fire alarms and/or temperature sensors) that are in the vicinity of the smartwatch (in this case, within 10 meters).  In addition, the administrator sets the second parallel emergency flow to attempt detection of an environmental emergency by\nobtaining sensor data in vicinity of the smartwatch even in the case of a personal emergency.  Once the emergency flow script has been finished, the administrator assigns the script to an emergency flow identifier for the nursing home.  This identifier\nis programmed into the smartwatches and other nursing home devices.  Finally, the administrator activates the server software that runs the emergency response server application.\nA week later, John is getting out of bed when he falls and breaks his hip.  John immediately presses a panic button on his smartwatch, which sends an emergency alert (which contains the emergency flow identifier) over the Wi-Fi network to the\nemergency response server application.  The server application detects the emergency alert, extracts the emergency flow identifier, and queries the identifier against a database storing emergency flow scripts and corresponding emergency flow identifiers\nto identify the emergency flow script to execute in response to this emergency alert.  The server application then executes the emergency flow script setup by the administrator a week earlier.  The script executes instructions of the initial emergency\nflow building block to determine the source of the emergency alert.  Upon identifying the smartwatch as the source of the emergency alert, the initial building block then branches off to a smartwatch downstream emergency flow.  This downstream emergency\nflow has a series of building blocks forming a parallel flow for obtaining location and sensor data from the smartwatch (this custom flow takes advantage of the enhanced sensor functionality of smartwatches) by transmitting data requests to the\nsmartwatch over the Wi-Fi network.  A second parallel data flow is activated as part of this script that determines whether the emergency alert pertains to a personal or environmental emergency.  In this case, the emergency alert is sent when John\npressed a panic button for a personal medical emergency.  Accordingly, an initial building block of the second parallel data flow determines based on the emergency alert that this is a personal emergency.  However, this second parallel data flow includes\na building block for assessing sensor data throughout the nursing home to determine the possibility of an environmental emergency.  In this case, the building block queries thermometers and smoke/CO.sub.2/CO detectors located throughout the nursing home\nthat communicate with the server over the Wi-Fi network.  The sensor data from these thermometers and detectors indicate no abnormal readings.  This result causes the second parallel data flow to terminate since there is no need to provide environmental\nsensor data associated with the emergency request when there is no indication of an environmental emergency.  In parallel, the main emergency flow calls a smartphone belonging to John and bridges the call with an operator for the nursing home.  Although\nthe smartphone is out of John's reach, it is paired with John's smartwatch via Bluetooth.  John is able to answer the call by pressing a button on his smartwatch.  The operator asks John the nature of his medical emergency, and John indicates that he has\nfallen and cannot get up.  The emergency flow script provides an interactive element that bridges the communication session with a local public safety answering point when the operator interacts with the interactive element.  In this case, the operator\nrealizes that John needs urgent medical attention and presses the interactive element.  This activates a building block in the emergency flow script that calls the PSAP and bridges the call with the communication session between the nursing home operator\nand John (e.g. creating a conference call including the PSAP dispatcher, nursing home operator, and John).  John is in great pain and is unable to coherently respond to the PSAP dispatcher's questions.  Thankfully, the nursing home operator has medical\ninformation and sensor data for John and is able to relay this information to the PSAP dispatcher.  In addition, the emergency flow script executes a parallel emergency flow that transmits the location data, medical information, and sensor data to the\nPSAP (since the administrator customized the emergency flow script to take advantage of the fact that this local PSAP is equipped with the technology to receive enhanced data/meta-data).  The PSAP dispatcher is able to assign first responders (EMTs from\nthe local hospital) to John's location based on the location data received.  In this scenario, the PSAP dispatcher does not have authorization to view John's medical information but has the ability to forward medical and sensor data to the first\nresponders to keep them apprised of John's current medical condition.  In the meantime, orderlies working at the nursing home have reached John and kept him stable and still while waiting for the EMTs to arrive.\nFinally, the EMTs arrive and transport John to the local hospital for treatment.  Upon their arrival, John uses his smartwatch to hang up on the phone call.  The nursing home operator and the PSAP dispatcher also terminate their connections to\nthe emergency communication session, thus terminating the session.  Meanwhile, the emergency flow script has finished executing and terminates.  Fortunately, John makes a full recovery thanks to the prompt emergency response.\n<BR><BR>Example 2--Social Media Suicide Prevention\nJane, a freshman as a top tier university, is finishing up her first semester.  Jane has been struggling to fit in socially on-campus and is experiencing problems with her roommate.  Moreover, Jane is double majoring in analytical chemistry and\ncomputer science and is feeling overwhelmed by the course load.  For the past month, Jane has fallen into a deep depression as finals week approached.  She is not alone, and administrators have been cognizant of the rise of mental health problems in the\nstudent body.  In response, the university has launched a digital campaign to detect and head off mental health crises.  The university partnered with a popular social media platform to monitor publicly available user postings for early detection of\nmental health crises such as suicide attempts.  In this case, mental health crises are detected by dynamic web analysis of user postings for all university students using keywords indicative of suicidal thoughts or tendencies.  In this case, Jane posts\non the social media platform that she is saying farewell to her friends and is ready to end it all tonight.  This post is analyzed and leads to detection of a mental health crisis.  Next, an emergency alert is sent to an emergency response server along\nwith an emergency flow identifier associated with the emergency flow script for the university's mental health emergency management campaign.  The emergency flow identifier is used to determine the customized emergency flow script comprising various\nbuilding blocks for managing emergency communications between all entities in this particular emergency flow.  In this case, the emergency flow script is executed by the emergency response server.  A first building block attempts to locate contact\ninformation for Jane associated with the mental health crisis by querying the university server database storing student information.  The first building block retrieves a cell phone number, which is passed onto a second building block that calls the\nnumber.  Jane does not pick up.  The second building block passes on the \"no pickup\" output to a third building block that attempts to locate Jane's roommate Mary who does pick up her phone.  Upon successfully reaching Mary, a fourth building block\ninitiates a call to a university dispatcher and bridges the calls with Mary and the dispatcher to enable two-way communications.  A fifth building block running parallel to the fourth building block provides the emergency alert to the dispatcher via a\ndata connection to the dispatcher's computer console.  The dispatcher than speaks with Mary to explain the situation and asks for Jane's whereabouts.  Mary has not seen Jane all night.  Fortunately, another parallel building block is attempting to locate\nJane's whereabouts by searching university Wi-Fi routers to determine if any Jane's cell phone is connected to any of those routers.  This building block successfully identifies two routers in the basement of the library as having detected Jane's cell\nphone Wi-Fi signal in the past 20 minutes and provides this output to the dispatcher.  The university dispatcher then connects to campus security and requests a guard to be sent to investigate.  Upon connection to the campus security, another building\nblock is activated that transfers all collected information pertaining to this emergency to a cell phone of the security guard (which is running a mobile application in communication with the emergency response server).  The guard is able to enter the\nrouter location information into a campus map application on his phone and soon finds Jane sleeping on the couch in the library basement.  The guard wakes up Jane and assesses her condition.  She insists she is okay but agrees to meet with the school\ncounselor for a mental health checkup.\n<BR><BR>Example 3--Voice-Activated Emergency Call Using Smart TV for Medical Emergency\nAn elderly man, Harry, lives in an assisted living apartment.  He suffers from a medical condition that makes him susceptible to dizzy spells and would like to get emergency assistance through his TV.  The TV is particularly useful because Harry\nspends most of his time in the living room and the TV is on most of the time.  Harry has installed an emergency calling app on his mobile phone and registered the TV by inputting user data such as name, address, emergency contacts, medical conditions,\netc. The TV includes a Wi-Fi connection (or another connection that supports sufficient bandwidth), physical buttons, a microphone, speakers and a remote control.  In addition, the TV has voice recognition software for accepting voice commands such as\n\"change to channel 5\" and \"call 911.\" In addition, Harry has installed sound processing software with a library of trigger sounds for triggering an emergency call including the sound of breaking glass.\nOne day, Harry has a dizzy spell and falls in the living room, dropping a glass of water that breaks upon hitting the floor.  Finding it difficult to get up and make an emergency call using his phone, Harry says the voice command \"Hey Alex, call\n9-1-1\" to the TV.  The voice-recognition software on the TV recognizes the voice command and activates the App for sending the emergency alert.  The voice-recognition software on the TV is able to match the voice to samples that Harry had saved, which\nprovides verification that the registered user Harry is the one making the call.  In addition, the voice recognition software is able to distinguish the Harry's voice from the dialogue on the TV and ambient sounds (like people talking on the street). \nThe sound recognition software registers the sound of breaking glass.  Harry has not authorized making an emergency call based on such sounds.  However, the sound recognition software has alerted the App to be primed to take the voice command after\ndetecting the sound of breaking glass.  In addition, the detected sound is part of data regarding the emergency that can be made available to emergence service providers.  In addition, the user data regarding Harry's conditions and prescriptions may also\nbe made accessible to emergency service providers with proper authorization.\nWhen Harry gave the voice command, the emergency call is initiated by sending the alert in the form of an API request.  An emergency alert is sent to an Emergency Management System (EMS) via the Wi-Fi connection.  After receiving the alert, an\nemergency flow (e.g. FIG. 4), is triggered.  First, Harry's son, Barry (an emergency contact) is sent an SMS message that an emergency alert has been triggered associated with his father.  Since the TV does not have calling capabilities (e.g. VoIP), the\nEMS had assigned a token to the TV, which will be used to provision the call and associated with Harry's phone number.\nAn emergency call is activated, awaiting connection to an appropriate dispatch center and/or emergency responder.  The emergency alert from the TV is sent without information about the location of the emergency.  After successfully searching its\ndatabases for the location for the TV using the MAC address or the assigned token in the meta-data of the alert, the EMS locates the saved address of the home within \"NG911 Clearinghouse\" database (including floor and apartment number).  The EMS\nprocesses the address into a current location for the emergency that is compatible with the PSAP for that jurisdiction (based on the location of the emergency).  The EMS places the address in a database provisioned with Harry's number so that the PSAP\ncan query the database for the current location.  In addition, the PSAP is also provided a link to additional information about Harry to provide additional situational awareness about the emergency.\nWhile the emergency call is on-going, the EMS searches for IoT devices at the emergency location.  The EMS identifies a video camera feed from the lobby area of Harry's assisted living center and notes that there is a receptionist at the front\ndesk.  After finding the phone number for the center through publicly available sources, the EMS calls the front desk and informs the receptionist regarding the emergency in the user's apartment.  The lobby informs the on-call nurse who is able to reduce\nthe blood loss while the first responders (including EMTs) arrive on the scene to take the user to a nearby hospital.  Harry makes a quick recovery after a few days in the hospital.\n<BR><BR>Example 4--Voice-Activated Emergency Call Using Smart Speakers to Report Domestic Violence\nA domestic violence survivor, Mary, lives in an apartment with her 5-year-old daughter, Cary.  Mary is aware that her ex-husband, Barry, might find and abuse her again.  As a precaution, she has taught Cary to make an emergency call using a\nsmart speaker (e.g. Amazon Echo.RTM.).  The smart speaker is able to access the Internet via a Wi-fi connection and is also connected to an intelligent digital assistant service (e.g. Alexa) with voice analysis software.  Mary programmed the smart\nspeaker to take Cary's voice command, \"Assistant, call 911\" to make an emergency call (although the digital assistant does not usually implement a child's voice command, it can be programmed to do so by saving the child's voice command as a trigger\nphrase).  To prevent false triggers, Mary programmed that Alexa would call her friend, Sarah before connecting to 911.  Sarah can confirm the emergency by pressing 1 or * to cancel the emergency call.\nOne evening, Barry is waiting at the front door when Mary arrives home after work.  She tells Barry to leave, but he is intent on getting inside.  An altercation ensues in the living room.  Cary assesses the situation and realizes that she\nshould call 911 for emergency help as her mother has taught her.  Cary goes near the smart speakers and says \"Alexa, call 911.\" The smart speaker connects to the EMS and sends an emergency alert (including the registered address of the smart speaker\ndevice, user information).  Since Mary had updated the address on the smart speaker with their new address, that address gets provisioned with an assigned 10-digit phone number (the CBN) in the NG911 Clearinghouse (as she is the registered user of the\nsmart speaker).  In this case, the EMS will assign a 10-digit phone number to the emergency alert and maintain it for the duration of the emergency.\nBased on Mary's programmed emergency flow, the EMS establishes a call to her emergency contact, Sarah, to confirm the emergency.  Once connected, Cary explains to her grandmother, Sarah, that her mom is in danger.  The EMS prompts Sarah to\nconfirm the emergency or cancel the call.  Sarah presses 1 to confirm that a three-way conference call to an appropriate PSAP should be established.\nOnce connected to the local PSAP, Sarah explains the emergency, but is not able to confirm the exact location of the apartment (as they had recently moved into a new place).  Cary also cannot remember her street address.  The dispatcher is able\nto query the NG911 Clearinghouse for the emergency location using the assigned CBN and divert a police patrol to that location.  The police arrive and arrest Barry.  Fortunately, Mary is taken to the local hospital and receives treatment for minor\ninjuries.\n<BR><BR>Example 5--Emergency Calling Using On-Board Console in a Smart Vehicle\nLisa, while driving home during a snowstorm, gets hit from behind by a truck.  Lisa is badly injured and is not able to move her hands to make an call for emergency help.  Lisa is in a smart car with an intelligent on-board console capable of\nreceiving voice command and making cellular calls.  Lisa does not remember the trigger words, but saysm \"Please call emergency .  . . Hmmm .  . . Please help .  . . \". Because Lisa's voice instructions were not clear and her voice was low, the console\nprompts Lisa to confirm that she would like to make an emergency call, but she has become unresponsive.\nFortunately, the event data recorder in the smart vehicle has been activated due to the crash.  The event data recorder system in the on-board console includes four components: various sensors (camera, RADARs and LIDARs), the SDM (which includes\nthe event data recorder), the DCIM and the cellular antenna.  When Lisa's car got hit, the air bags deploy and the sensors are activated.  The sensors transmit the data to the SDM, which measures the severity of the accident using an accelerometer.  The\nSDM's assessment is sent to the DCIM, which uses the cellular antenna to send an alert (attaching Lisa's voice command and the assessment of the severity of the accident) to a private call center.\nWhen a customer service representative at the call center receives the call, she uses the built-in GPS to find the vehicle's location (or get a location through the cellular network) and calls the car to check-in with Lisa.  Since Lisa is\nunresponsive and reviewing the severity of the accident, the representative calls 9-1-1 makes a proxy call on behalf of Lisa using the Lisa's location.  Fortunately, paramedics are sent to the scene immediately and Lisa is taken to the hospital for\ntreating her injuries.\nWhile preferred embodiments of the present invention have been shown and described herein, it will be obvious to those skilled in the art that such embodiments are provided by way of example only.  Numerous variations, changes, and substitutions\nwill now occur to those skilled in the art without departing from the invention.  It should be understood that various alternatives to the embodiments of the invention described herein may be employed in practicing the invention.  It is intended that the\nfollowing claims define the scope of the invention and that methods and structures within the scope of these claims and their equivalents be covered thereby.", "application_number": "15960384", "abstract": " Disclosed are systems, methods, and media capable of generating and\n     implementing emergency flows. The emergency flow is highly customizable\n     and can connect with various stakeholders (user, emergency contacts,\n     corporate representatives, emergency service provider personnel, etc.).\n     The systems, methods, and media can be triggered in various ways\n     including user input (e.g. voice command) or by sensors (e.g. by using\n     sound detection capabilities).\n", "citations": ["5379337", "5479482", "5563931", "5596625", "5710803", "5742666", "6014555", "6249674", "6252943", "6477362", "6502030", "6510315", "6556816", "6571092", "6594666", "6600812", "6628933", "7058385", "7224773", "7324801", "7349706", "7409044", "7436938", "7437143", "7469138", "7483519", "7519351", "7519372", "7548158", "7565131", "7646854", "7676215", "7684782", "7848733", "7949326", "8009810", "8041335", "8041341", "8045954", "8068881", "8102972", "8126424", "8150367", "8165560", "8165562", "8185087", "8195121", "8219135", "8244205", "8249546", "8249547", "8289953", "8306501", "8326260", "8369488", "8401565", "8417212", "8472973", "8484352", "8489062", "8509729", "8516122", "8538370", "8538468", "8594015", "8606218", "8625578", "8626112", "8630609", "8644301", "8682279", "8682281", "8682286", "8712366", "8747336", "8751265", "8760290", "8811935", "8825687", "8848877", "8866606", "8868028", "8880021", "8890685", "8948732", "8971839", "8984143", "9008078", "9014657", "9019870", "9071643", "9077676", "9078092", "9094816", "9167379", "9244922", "9258680", "9277389", "9351142", "9369847", "9402159", "9408051", "9503876", "9544750", "9591467", "9635534", "9659484", "9693213", "9736670", "9756169", "9805430", "9838858", "9924043", "9942739", "20020001367", "20020057678", "20020120698", "20030069035", "20040203572", "20040266390", "20050085215", "20050104745", "20050151642", "20060293024", "20070030144", "20070033095", "20070049287", "20070053308", "20070058528", "20070060097", "20070161383", "20070164872", "20070218895", "20080019268", "20080063153", "20080081646", "20080166990", "20080194238", "20080294058", "20090075703", "20090257345", "20090322513", "20100002846", "20100156626", "20100159976", "20100166153", "20100202368", "20100238018", "20110009086", "20110086607", "20110103266", "20110134897", "20110153368", "20110201357", "20110263219", "20120002792", "20120029970", "20120092161", "20120144019", "20120202428", "20120210325", "20120218102", "20120257729", "20120289243", "20120295575", "20120309341", "20130005295", "20130030825", "20130084824", "20130122932", "20130138791", "20130183924", "20130203373", "20130203376", "20130226369", "20130237175", "20130237181", "20130331055", "20140051379", "20140057590", "20140086108", "20140087680", "20140113606", "20140126356", "20140148120", "20140155018", "20140155020", "20140199959", "20140248848", "20140324351", "20150055453", "20150081209", "20150094016", "20150109125", "20150111524", "20150137972", "20150140946", "20150172897", "20150181401", "20150289121", "20150304827", "20150317392", "20150350262", "20150358794", "20150365319", "20160004224", "20160026768", "20160219084", "20160219397", "20160269535", "20160307436", "20160337831", "20160345171", "20160363931", "20170004427", "20170140637", "20170142568", "20170150335", "20170161614", "20170164175", "20170171735", "20170180486", "20170180963", "20170180966", "20170213251", "20170238129", "20170238136", "20170245113", "20170245130", "20170251347", "20170325056", "20170330447", "20180020091", "20180039737", "20180053401"], "related": ["62489373", "62575935", "62614027"]}, {"id": "20180332082", "patent_code": "10375120", "patent_name": "Positionally-encoded string representations, including their use in\n     machine learning and in security applications", "year": "2019", "inventor_and_country_data": " Inventors: \nBaumgart; Marvin (Mountain View, CA), Povalyayev; Viktor (Santa Clara, CA), Hu; David C. (Palo Alto, CA)  ", "description": "<BR><BR>FIELD\nThe present disclosure generally relates to creating and manipulating positionally-encoded representations of character strings.  Particular implementations relate to using binary matrix string representations in machine learning applications\nand in security applications.\n<BR><BR>BACKGROUND\nA significant portion of data used by computer systems is in the form of character arrays or strings.  While comparing two numerical values using a processor (e.g., a microprocessor of a computer device) is typically very fast, comparing two\nstrings can be more time consuming, as potentially all characters in the smallest string would need to be evaluated before a mismatch could be determined.  Thus, in the worst case scenario, string comparison has a linear time complexity based on the\nnumber of characters in the shortest string.  In some cases, it may be desirable to determine the degree of overlap between two strings, in which case the time complexity will always be linear with respect to the number of characters in the shortest\nstring.  For very long strings, or when large numbers of strings are to be compared, significant processing resources, and time, can be required.\nString comparison can be even more complex, and time and processor intensive, when one or more characters of a string are to be analyzed to determine whether they fall within a set or range of values.  In some cases, both a source string a\ntarget string may have one or more characters that can fall within a set, or range, of values.  In these cases, it may be necessary to sequentially test a character of the target string against possible values specified in the source string.  Thus, in\nthe worst case, for a particular character in the target string, multiple possible character values may need to be tested against every possible value of the source string.\nMachine learning can be an efficient way to compare quantities, to categorize or classify input values, and to discover and utilize patterns and relationships in data.  In many cases, the results provided by machine learning can be more\naccurate, and can be obtained in a shorter amount of time and with less processing power, than using more traditional methods.  In some cases, traditional methods may be simply be unable to replicate machine learning.  However, machine learning typically\nrequires numerical input, and thus may be unsuitable for use with strings, given how strings are typically represented in computing languages (e.g., as sequences of characters).\nThus, there remains room for improvement in string comparison technology, and its application in fields such as security and authorization.\n<BR><BR>SUMMARY\nThis Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description.  This Summary is not intended to identify key features or essential features of the claimed subject\nmatter, nor is it intended to be used to limit the scope of the claimed subject matter.\nTechniques and solutions are described for facilitating evaluation of strings, including comparing a plurality of strings, comparing a string with a rule, and evaluating a string using a machine learning algorithm.  In a particular aspect, an\nauthorization value is received that includes at least one character string.  At least a portion of the character string is converted to a positionally-encoded representation.  The converting includes, for each of a plurality of characters of the\ncharacter string, comparing the character to an encoding schema.  The encoding schema includes an array of a plurality of unique characters, such as the unique characters from which a string may be formed.  The array has a length, and each unique\ncharacter in the array has a unique position in the array.\nA number of elements in a data structure are provided, the number being equal to the length of the array.  The plurality of elements has an initial position, such as an initial position within an array.  For a character being converted, a unique\ncharacter in the array of unique characters is determined that is the same as the character of the character string.  An index position of the unique character is determined.  A position in the data structure is determined that is equal to the sum of the\ninitial position and the index value.  A value at the determined position of the data structure is set to indicate that the character of the character string is the determined unique character.  Elements of the data structure that are not the unique\ncharacter have a value that indicates that the unique character at the corresponding position in the array of unique characters is not the character of the character string.  For example, characters present in the character string can have a value of\none, or a first Boolean value, and characters not present in the character string can have a value of zero, or a second Boolean value.  Or, the designations can be reversed (e.g., zero representing that a value is present in the character string).\nThe positionally-encoded string representation is compared with an authorization rule to determine whether the positionally-encoded string representation meets authorization criteria.  Authorization is provided or denied based on results of the\ncomparing.\nAccording to another aspect, a positionally-encoded representation of at least a portion of a string is created, such as a positionally-encoded representation that includes binary or Boolean values indicating a character of the string.  The\nstring includes a plurality of characters.  The positionally-encoded representation is created by encoding a data structure based on the positions of a plurality of characters of the at least a portion of the string in an encoding schema.  The\npositionally-encoded string representation is provided as an input to a machine learning algorithm.  The machine learning algorithm provides an output value, which is analyzed to determine whether the string is a member of a set.\nIn a further aspect, a method is provided that includes creating a positionally-encoded representation of an authorization token, where the positionally-encoded string representation can include digits or Boolean values.  The authorization token\nincludes at least one string.  The positionally-encoded representation is created by encoding a data structure based on the positions of a plurality of characters of at least a portion of the string in an encoding schema.  The positionally-encoded\nrepresentation of the authorization token is compared with a positionally-encoded representation of an authorization rule.  The authorization rule defines a set of authorized strings.  It is determined if the positionally-encoded representation of the\nauthorization token meets authorization criteria of the authorization rule.  If the results of the comparing meet authorization criteria, an action, such as a restricted action, is permitted to be executed.\nThe present disclosure also includes computing systems and tangible, non-transitory computer readable storage media configured to carry out, or including instructions for carrying out, an above-described method.  As described herein, a variety\nof other features and advantages can be incorporated into the technologies as desired. <BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nFIG. 1 is a diagram schematically depicting how an object, and optionally a rule or criteria, having at least one string can be positionally-encoded, such as by a binary matrix, using an encoding schema.\nFIG. 2 is a diagram schematically depicting binary matrix cubes in which a plurality of strings, or string portions, can be positionally-encoded by applying an encoding schema.\nFIG. 3 is a diagram schematically depicting how an object and an authorization rule, each having a plurality of strings or string portions, can be represented as a matrix cube, or three-dimensional array, by applying an encoding schema.\nFIG. 4 is a diagram schematically depicting a process for converting a string to a binary matrix representation by applying an encoding schema.\nFIG. 5 is diagram schematically depicting a process for comparing two positionally-encoded string representations using a comparison engine.\nFIG. 6 is a diagram schematically depicting a process for analyzing a positionally-encoded string representation with a machine learning algorithm.\nFIG. 7 is an example system architecture for a computing system in which at least certain disclosed innovations can be implemented.\nFIG. 8 is a diagram schematically depicting how, in a rules framework, a plurality of data objects and a plurality of data functions can be processed according to a plurality of rules of a rule set.\nFIG. 9 is example C++ source code describing an encoding schema and methods for determining a character type of encoded data.\nFIG. 10 is example C++ source code describing a method for converting a string to a positionally-encoded representation using an encoding schema.\nFIG. 11 is example C++ source code describing a method for converting a positionally-encoded string representation to a one-dimensional binary representation of the string or a character representation of the string.\nFIG. 12 is example C++ source code describing a method for comparing two positionally-encoded string representations.\nFIG. 13 is example C++ source code describing a method for analyzing wildcard characters in a positionally-encoded string representation.\nFIG. 14 is example C++ source code describing a method of comparing a discrete value of one positionally-encoded string representation to a range of values.\nFIG. 15 is a table illustrating example comparison results from comparing values of an authorization object or token with an authorization rule.\nFIG. 16 is a flowchart of an example method for analyzing a string to determine whether the string complies with an authorization rule.\nFIG. 17 is a flowchart of an example method for determining whether a string is a member of a set.\nFIG. 18 is a flowchart of an example method for determining if an authorization token meets authorization criteria.\nFIG. 19 is a diagram of an example computing system in which some described embodiments can be implemented.\nFIG. 20 is an example cloud computing environment that can be used in conjunction with the technologies described herein.\n<BR><BR>DETAILED DESCRIPTION\n<BR><BR>Example 1--Overview\nA significant portion of data used by computer systems is in the form of character arrays or strings.  While comparing two numerical values using a processor (e.g., a microprocessor of a computer device) is typically very fast, comparing two\nstrings can be more time consuming, as potentially all characters in the smallest string would need to be evaluated before a mismatch could be determined.  Thus, in the worst case scenario, string comparison has a linear time complexity based on the\nnumber of characters in the shortest string.  In some cases, it may be desirable to determine the degree of overlap between two strings, in which case the time complexity will typically be linear with respect to the number of characters in the shortest\nstring.  For very long strings, or when large numbers of strings are to be compared, significant processing resources, and time, can be required.\nString comparison can be even more complex, and time and processor intensive, when one or more characters of a string are to be analyzed to determine whether they fall within a set or range of values.  In some cases, both a source string a\ntarget string may have one or more characters that can fall within a set, or range, of values.  In these cases, it may be necessary to sequentially test a character of the target string against possible values specified in the source string.  Thus, in\nthe worst case, for a particular character in the target string, multiple possible character values may need to be tested against every possible value of the source string.\nAs an example, strings can be used for security purposes, including evaluating access rights and action permissions.  A particular employee may be associated with an access object, which can include many fields, each field having a range of\npossible string values.  Access to a resource, or permission for an action, may be provided for a range of possible access objects, such as authorization tokens.  When a user wants to access a resource or perform an action, one or more fields of their\naccess object may be compared with a rule specifying possible acceptable values that will allow the access or action.  It may also be of interest whether the permissions provided to a user (e.g., in an authorization object) may result in an undesired\nsituation, such as the potential for fraud or a compliance issue.  In this case, multiple permissions of the user may be analyzed using multiple rules, where each rule is associated with many possible string values.  Carrying out these types of analyses\nusing traditional string comparison techniques can be very time consuming.  In many cases, because of analysis complexity and long processing times, the analysis is only performed after a problem has been discovered.\nMachine learning can be an efficient way to compare quantities, to categorize or classify input values, and to discover and utilize patterns and relationships in data.  In many cases, the results provided by machine learning can be more\naccurate, and can be obtained in a shorter amount of time and with less processing power, than using more traditional methods.  In some cases, traditional methods may be simply be unable to replicate machine learning.  However, machine learning typically\nrequires numerical input, and thus may be unsuitable for use with strings, given how strings are typically represented in computing languages (e.g., as sequences of characters).\nThus, there remains room for improvement in string comparison technology, and its application in fields such as security and authorization.\nThe present disclosure provides an innovative mechanism for converting strings into a positionally-encoded representation.  Specifically, an encoding schema can be used to convert the string into a matrix (including represented as an array),\nincluding as a matrix cube, or three-dimensional array, for a positionally-encoded representation of multiple strings or string portions.  Various encoding schema can be used, such as depending on the character set used for the string.  In some cases, an\nobject to be evaluated can have multiple components that are to be evaluated.  For instance, an authorization object can be used to describe access and action permissions for a user.  The authorization object may be formed from many fields.  A plurality\nof the fields may be analyzed in determining whether access to a particular resource or performance of an action is authorized.  In such cases, at least a portion of each field, or statement, can be represented as a matrix.  When a positionally-encoded\nrepresentation of authorization object includes multiple matrices, the matrices can be stacked to form a cube.\nOnce a positionally-encoded representation of a string, or multiple strings, has been created, the positionally-encoded string representation can be subject to various types of analysis, including comparing the representation to other\npositionally-encoded string representations or a positionally-encoded representation of a rule.  Generally, a positionally-encoded string representation can be analyzed to determine whether it is equivalent to, or a subset of, another\npositionally-encoded string representation, or meets rule criteria.  For instance, an authorization object can be represented as a matrix or cube, and a rules object, specifying values that will result in an action or access being authorized, can be\nrepresented as a matrix or cube.  The positionally-encoded representations of the authorization object and the rules object can be compared to determine whether access is authorized.\nBeing expressed as positionally-encoded representations, the matrix, cube, or collapsed (e.g., one-dimensional) representation of a string, or multiple string statements, can be used as input to a machine learning component.  For instance, one\nor both of a positionally-encoded representation of an authorization object and a positionally-encoded representation of rules object can be provided to a machine learning component.  In some cases, machine learning can be applied to compare the\nauthorization object and the rules object.  For instance, a machine learning component can determine if one, or multiple, string statements of each object match, or can determine a degree of matching.\nConverting strings to positionally-encoded representations so that machine learning algorithms can be used for comparison purposes can greatly speed the comparison process, particularly when one or both of the objects include characters that can\ntake multiple values.  In addition to the more rapid comparison provided by using a positionally-encoded representation of a string, machine learning can uncover patterns such that fewer characters of the strings are be compared before a result can be\ndetermined.  Thus, using machine learning, positionally-encoded representations of strings can allow characters to be compared more quickly, and can require fewer comparisons before a decision is reached.\nAs discussed above, the disclosed innovations can be used in checking authorization, as well as checking to see whether a particular authorization may give rise to risks (e.g., fraud, compliance violations).  The faster processing provided by\nthe disclosed innovations can enable real time access comparisons, including real-time risk assessment.  For instance, while typical string comparison techniques for risk assessment can require hours or days to complete, the disclosed innovations can\nprovide comparison results in seconds, or less, allowing for real-time authorization checking.  Real-time authorization checking can prevent problems from occurring, rather than merely being used to discover the source of a problem that has already\noccurred.\nThe disclosed innovations can be applied in other contexts, such as comparing other types of security tokens that are formed from, or include, strings.\n<BR><BR>Example 2--Example Process for Creating Positionally-Encoded String Representation\nFIG. 1 illustrates how an instance 110 of an object or datatype, having one or more strings 120 (e.g., a sequence of characters, such as letters, numbers, symbols, and combinations thereof that is not a numerical datatype) can be converted into\na positionally-encoded representation 140 using an interface or encoding schema 150.  Stings 120 are not limited to any particular datatype for a collection of characters.  Strings 120 can be, for example, string literals, instances of a string class,\nnull terminated character arrays (C-style strings), and the like.  Rather than multiple discrete strings 120, the strings can represent a portion of a string, including multiple portions of a longer string.\nInstance 110 is shown as including two strings 120, but can optionally include more strings (indicated by dashed lines 160) or fewer strings.  The strings 120 can be of the same or different string types, and can be of the same or different\nlengths.  In at least some cases, an instance 110 can include non-string elements.  That is, for example, the instance 110 can be an abstract data type that includes at least one string 120 data member and one or more non-string data members.  The data\nmembers can be maintained separately, or can be maintained as a continuous sequence of characters, with, for example, various sequence ranges being associated with particular data members and data types (e.g., characters 1-50 may be a string and\ncharacters 51-60 represent an integer value).\nA positionally-encoded representation 140 of a string 120 can be a representation where each character of the string (or a portion of the string to be included in the representation) is represented by the position of the character in the\nencoding schema 150.  The positionally-encoded representation can be a binary representation (e.g., using zeros and ones, or Boolean values), where the value at a position in the positionally-encoded representation 140 indicates that a corresponding\ncharacter is, or can be, present in the string 120.  The positionally-encoded representation can thus be a collection of n-tuples, where n is equal to the number of potential characters (or, the number of characters in the encoding schema 150, which, in\nsome cases, may be less than all possible characters, such as if some characters are removed from the string 120 in forming the positionally-encoded representation).  The number of elements in the collection can be equal to the number of characters in\nthe string 120, or at least the number of characters that are to be included in the positionally-encoded representation 140.  The possible values of the tuple elements are typically binary values, including Boolean values, which serve as status flags to\nindicate whether the particular character at the position in the tuple is, or can be, present in the string 120.\nThe positionally-encoded representation 140 can an intermediate string representation.  The positionally-encoded representation 140 can be stored internally in a computer system in hardware, such as in memory or persistent storage.\nThe schema 150 can act as a blueprint of a conversion tool for converting a string 120 to the positionally-encoded representation 140.  In particular aspects, the schema 150 is applied to each character of the string 120 that is to be converted\nto the positionally-encoded representation.  The schema 150 can be used to interconvert an individual character of a string 120 (or a rule relating to strings) to a positionally encoded representation.  The schema 150 can be represented as an n-tuple,\nwhere n is the number of possible characters in the string 120 (or that are recognized in the schema 150 and can be converted to the positionally-encoded representation 140).  Each element of the n-tuple in the encoding schema 150 can be a unique\ncharacter.  To form the positionally-encoded representation 140, the element of the binary tuple corresponding to a string character having the same position as the position of the character in the n-tuple is set to have a value indicating the presence\nof the character (e.g., 1 or true).  Positions of the binary n-tuple of the positionally-encoded representation 140 that cannot take a particular character value are set to a different value (e.g., 0 or false).\nThe encoding schema 150 can be an array having a length equal to a number of characters in a character set.  For instance, the modern English alphabet includes 26 letters, each having an uppercase form and a lowercase form.  In order to encode a\nstring 120 formed from uppercase letters in the English alphabet, the array of the encoding schema 150 would include 26 elements or positions.  However, the schema 150 is not limited to an alphabet, and can include letters, numbers, symbols, and\ncombinations thereof.  The schema 150 can use a recognized character set (e.g., the English alphabet, the alphabet of another language, a character set such as ASCII, Unicode, or the like), or can use a custom or arbitrarily constructed character set.\nIn the schema, the first letter of the alphabet is assigned to the first position in the array (e.g., element zero), the second letter of the alphabet is assigned to the second position, and so on.  When a string 120 is to be encoded, each\ncharacter in the string is associated with an array having the length of the encoding schema 150.  The position in the array corresponding to the character is set to one, while all other positions in the array are set to 0.  So for, example, for the\nstring \"DAB,\" applying the encoding schema 150 would produce the array (taking the first four positions of the array) of [0, 0, 0, 1, .  . . ] for \"D,\" [1, 0, 0, 0, .  . . ] for \"A\", and [0, 1, 0, 0, .  . . ] for \"B.\" The combination of these arrays\nproduces the positionally-encoded representation 140, a matrix having a number of rows equal to the length of the string 120 (either the actual length of the string, or the total possible length of the string) and a number of columns equal to the number\nof characters used in the encoding schema 150.\nIn cases where a string length is set (e.g., a program defines a string as having maximum length), and a string 120 has fewer actual characters in the set, the rows of the matrix 140 for which there are not actual string character values can be\ndenoted by setting all of the row elements to zero (e.g., [0, 0, 0, 0, .  . . ]), or a particular Boolean value (e.g., true).  The matrix 140 can also be used to indicate where multiple string values are possible.  For instance, for a \"wildcard\"\ncharacter, all of the row elements for that character (or, in some cases, multiple characters), can be designated by setting all row elements to one (e.g., [1, 1, 1, 1, .  . . ]).  In the case of a wildcard that is to apply to a range of string\ncharacters, such as remaining string characters, all of the remaining rows in the matrix 140 can have all of their elements set to one.  Taking \"A*\" as an example, the first row of the matrix 140 can be set to [1, 0, 0, 0, .  . . ], with the remaining\nrows set to [1, 1, 1, 1, .  . . ].\nIn addition to representing a single value, or all possible values, a row of the matrix 140 can be constructed to represent multiple discrete values.  In a row, all possible or acceptable values for a character can be indicated by setting the\ncorresponding elements to one, and setting all other elements to zero.  For instance, for a string 120 where the first character can be `B,` or `C,` the second character must be `A,` and the remaining characters can have any value, the matrix can be of\nthe form: [0, 1, 1, 0, .  . . ] [1, 0, 0, 0, .  . . ] [1, 1, 1, 1, .  . . ] .  . .\nThus, the positionally-encoded, matrix, representation 140 can represent strings that would be difficult or impossible to represent using a character representation.  Accordingly, rather than being formed from a particular string 120, a matrix\nrepresentation can be constructed in another manner, such as being defined by a user or being constructed using particular rules or criteria 170, optionally using the encoding schema 150.\nAs explained above, in at least some cases, the disclosed innovations can facilitate comparing strings, or analyzing a string for compliance with a rule.  The rule can be a rule 170, and can include wildcard values or may otherwise specify that\na particular element of a string 120 can have multiple acceptable values.  The string 120 can have a matrix representation 140 that includes a discrete value (e.g., a single element has a value of one) while the rule can have a matrix representation\nwhere one or more of the elements can have multiple values (e.g., multiple, and in some cases, all, elements in a row have a value of one).\nIn yet further cases, rather than comparing a string 120 to a rule 170, a matrix 140 can be produced that compares a rule with an object that can represent multiple strings.  For instance, an authorization object for a user may specify a set of\npermissions possessed by the user, which can be specified as a range or set of string values.  A rules object can specify a range, or set of string values, that are compliant with the rule.  The authorization object and the rules object can each be\nrepresented by a matrix 140 where, in at least some cases, one or more rows of the matrix can have multiple values.  The authorization object can be compared with the rules object to determine whether a set of values of the authorization object is a\nsubset of the rules object.  In other cases, the rules object can specify string values that are not authorized or compliant with a rule, and access or permission can be denied if the authorization object is a subset of the rules object (e.g., a\nblacklist versus a whitelist).\nIn at least some cases, permitting or denying access can be carried out in response to determining whether or not a positionally-encoded authorization object (such as an authorization object provided by a software application) matches a\npositionally-encoded representation of a rule (such as a rule provided by a software application); or, more generally, comparing two or more positionally-encoded strings.  The results of comparing the positionally-encoded authorization object to a\npositionally-encoded representation of a rule (or, more generally, comparing two or more positionally-encoded strings) can be used to select between providing access (or authorization) or denying access (or authorization).  The permitting or denying\naccess, or authorization, can be carried out, for example, by a software application.  The authorization object and authorization rule can be provided by a software application, and can be digital string representations (that in turn can be converted to\npositionally-encoded representations).\nAn instance 110 can have multiple strings 120, or a single string can have multiple string elements that are to be separately analyzed (e.g., a first set of characters is to be analyzed, a second set is not to be analyzed or encoded, and a third\nset of characters is to be analyzed).  FIG. 1 illustrates the instance 120 has having a string 120a and a string 120b.  While only string 120a is shown as encoded to produce the matrix 140, if desired, string 120b (or one or more other strings or string\nelements) can be encoded to produce a matrix.  Multiple matrices for an instance 110 can be associated, such as in a cube (e.g., implemented in a three-dimensional array), and the cube used for comparison or analysis purposes.\nA positionally-encoded representation 140 of a string 120, or string elements, can be described in one or more ways.  For instance, a matrix can be collapsed to one-dimensional representation and, generally, a cube, or multi-dimensional\nrepresentation of dimension n can be collapsed into a one-dimensional representation.  In the example above for `DAB,` the two-dimensional matrix can be collapsed by sequentially listing values according to the rows or columns of the matrix.  Collapsing\nthe example matrix by rows yields [0, 0, 0, 1, .  . . 1, 0, 0, 0, .  . . 0, 1, 0, 0, .  . . ]. When collapsed, the matrix can represent a single numerical value, which can easily be compared with another numerical value of another positionally-encoded\nrepresentation to determine whether the representations are the same and, optionally, the degree and nature of any differences.\n<BR><BR>Example 3--Example Cube Encoding Multiple Strings or String Elements\nAs described in Example 2, multiple matrices, each for a string or string element, also referred to as statements, can be combined into a single positionally-encoded representation.  FIG. 2 depicts cubes 200, 202, each having a y-axis 210\nrepresenting an index position in the array of characters from which the string is formed, a x-axis 220 representing the index positions in an encoding schema, and a z-axis 230 representing different encoded statements.  That is, each plane (e.g.,\ndefined by the x and y axes), at a particular value, is a matrix 234, analogous to the matrix 140 of FIG. 1, for the particular statement of the z-axis value.\nAs when a positionally-encoded representation of a string, or string elements, is a matrix, a cube 200, 202 can also have its dimensionality reduced, including to a one-dimensional representation that can be analyzed as a single numerical value. A cube 200, 202 can be collapsed by rows, by columns, or by statement (by statement matrix 234, where the matrix is collapsed by row or by column), e.g.: Collapse by rows: statement1Row1, statement2Row1, statement3Row1, statement1Row2 .  . . Collapse by\ncolumns: statement1Column1, statement2Column1, statement3Column1, statement1Column2 .  . . Collapse by statement/rows: statement1Row1, statement1Row2, statement1Row3, statement2Row1 .  . . Collapse by statement/columns: statement1Column1,\nstatement1Column2, statement1Column3, statement2Column1 .  . .\nFIG. 2 also illustrates a comparison point 240.  The comparison point 240 can be used to compare, or otherwise relate, elements at multiple layers of a single cube 202, 202 or one or more elements of a first cube with one or more elements of a\nsecond cube.  Although shown as relating elements at the same position in each matrix 234 of each cube 200, 202, in other cases, a comparison point 240 can relate elements at different layers in different cubes, or can relate elements having different\npositions in the x-axis 220 or the y-axis 210 between different matrix layers.  Also, although shown with cubes 200, 202, a comparison point 240 can be used with one or more positionally-encoded string representations that are not cubes (e.g., one or\nmore of the positionally-encoded string representations can be a matrix, such as a matrix represented as a two-dimensional array), or can be used with one-dimensional, collapsed, representation.\nThe cubes 200, 202 can be used, in some cases two relate words in different languages.  For instance, statement matrices 234, within the same cube 200, 202, or between different cubes, can be used to relate English and German versions of the\nsame word or concept.  In at least some examples, different classes of strings can be related using positionally-encoded string representations, provided that the same encoding schema is useable (e.g., the strings typically share the same character set).\n<BR><BR>Example 4--Example Process for Creating Matrix Cube Representing Multiple Strings\nFIG. 3 schematically depicts a process 300 for creating cube representations 306, 308 of a first object instance 312 and a second object instance 314, respectively, where the first and second object instances each have multiple strings or string\nelements 318.  In the example shown, first and last strings or string elements 318 of the first and second objects instances 312, 314 are used to construct the cube representations 306, 308.  For example, the second object instance 314 can represent a\nrule, where only the first and last string elements 318 of the first object instance 312 need be analyzed to determine whether authorization is present.\nEach of the strings or string elements 318 can be represented as an array 322 of characters 324.  The arrays 322 are converted to their matrix representations 338 using an encoding schema 342.  The encoding schema 342 can be implemented as\ndiscussed above with respect to FIG. 1.  The matrix representations 338 can be combined to form the cube representations 306, 308.\n<BR><BR>Example 5--Example Encoding Process\nFIG. 4 illustrates how an encoding schema 410 can be used to produce a matrix representation 415 of a string 420.  The string 420, \"MARVIN,\" has six characters.  Thus, six rows will be needed in the matrix representation 415.  However, in some\ncases a matrix representation 415 can include a larger number of rows than required for the actual number of characters in a string, such as when a string is defined to have a fixed size/number of elements, regardless of the number of actual (e.g.,\nmeaningful) characters or elements in a particular string.\nThe encoding schema 410 is shown as an array, where each element in the array is associated with a particular, unique, character.  The characters in the array can be numbers, letters, or symbols.  Each character in the string 420 to be encoded\ncan be represented by an array 425, having a number of elements equal to the number of elements in the encoding schema.  The identity of a character at a particular position of the string 420 can be indicated by setting the value of the element having a\nposition corresponding to the position of the character in the encoding schema 410 to one (or, setting all non-present characters to one and setting present characters to zero), setting all present characters to true and all non-present characters to\nfalse, setting all present characters to false and all present characters to true, or the like.\nEach array 425 can be included, sequentially, in the matrix representation 415.  If desired, the matrix representation 415 can be re-converted to the string 420, which can be a compressed representation of the string compared with the matrix\nrepresentation 415.  Re-conversion can simply involve forming the string 420 by adding a character to an array of characters, where the character is determined as the character of the encoding schema 410 associated with the index position of the element\nof the array 425 having the value of one.\n<BR><BR>Example 6--Example Comparison of Positionally-Encoded String Representations\nFIG. 5 schematically depicts how two positionally-encoded string representations 510, 514 can be compared using a comparison engine 518.  The positionally-encoded string representations 510, 514 are shown as cubes, or three dimensional,\nmatrices.  However, the positionally-encoded string representations 510, 514 can have other forms, such as a two-dimensional matrix or a collapsed matrix representation (e.g., a one-dimensional representation of a multi-dimensional matrix or array), and\ncan represent a larger or smaller number of string statements.\nThe comparison engine 518 can include comparison logic for conducting a comparison of the positionally-encoded string representations 510, 514, or selected portions thereof.  For instance, particular values or ranges of the positionally-encoded\nstring representations 510, 514 can be selected for comparison.  In at least some cases, the portions selected for comparison need not be the same between the positionally-encoded string representations 510, 514.  For instance, a two-dimensional array at\na first z-axis value of positionally-encoded string representation 510 can be compared with the two-dimensional array at a second z-axis value of the positionally-encoded string representation 514.  For a particular two-dimensional array, a single value,\nmultiple discrete values, a range (such as a contiguous range) of values, or multiple ranges of values can be specified for comparison.  In at least some cases, when multiple elements of a positionally-encoded string representation 510 are selected for\ncomparison, the same number, and, in at least some cases, arrangement, of elements is selected in the positionally-encoded string representation 514, even if the positions of the elements differs between the positionally-encoded string representations. \nIn other cases, the number of elements, or arrangement, of elements being compared can differ between the positionally-encoded string representations 510, 514, and logic of the comparison engine 518 can resolve any differences in the number of elements\nbeing compared, or their arrangement.\nThe comparison engine 518 can include a rules engine 522.  While the comparison engine 518 can determine the value of elements of the positionally-encoded string representations 510, 514 to be compared, the rules engine 522 can determine a\nrelation of the values, such as whether the values match, whether a value is greater or less than another value, or whether a value is within, or a subset of, another value (e.g., a set).\nThe rules engine 522 can communicate with a rules store 526 that includes rule definitions 530.  The rule definitions 530 can specify rules that determine if single elements match, can specify rules that determine whether particular subsections\nof the positionally-encoded string representations 510, 514 match, or whether the positionally-encoded string representations, overall, match.  In at least some cases, the rules store 526 can include multiple rule definitions 530, and one or more\nparticular rule definitions can be selected for a particular comparison (e.g., an application or user can specify one or more rules to be applied).  Rule definitions 530 can be, for example: is an element of positionally-encoded string representation 510\nthe same as an element of positionally-encoded string representation 514? is an element of positionally-encoded string representation 510 within a range of elements positionally-encoded string representation 514, including the endpoints? is an element of\npositionally-encoded string representation 510 within a range of elements of positionally-encoded string representation 514, excluding the endpoints? is a range of elements of positionally-encoded string representation 510 equal to, or a subset of, a\nrange of elements of positionally-encoded string representation 514? Rule definitions 530, or logic of the comparison engine 518, can also include logic for handling wildcard operations or cases where an element of a positionally-encoded string\nrepresentation 510, 514 can include multiple values.\nThe comparison engine 518 can be in communication with a rendering engine 540.  The rendering engine 540 can include one or more report templates 544.  Comparison results from the comparison engine 518 can be formatted into a report using a\nreport template, and the report rendered for output, such as to a display or printer.\nIn a specific example, the positionally-encoded string representations 510, 514 can represent an authorization object and an authorization rule.  An authorization object, or an authorization rule, can be defined in terms of an authorization\ninterval.  An authorization interval can be used to describe permissions possessed by a user (or another entity, such as a software application or computer system), or an authorization rule that defines what authorization objects are authorized (or are\nnot authorized).  An authorization interval can be a range, which can be a range that is bounded on one end (e.g., has a starting value but not an ending value, or has an ending value but not a starting value), or can be bounded at both ends by a\nstarting (or \"from\") value and an ending (or \"to\") value.  Values can be open ended, such as by using a wildcard symbol (e.g., `*`).\nWhen an authorization object is compared with an authorization rule, a single value of an authorization object can be compared with a single value of an authorization rule, one of the authorization object and authorization rule may have a range\nand the other a single value, or both the authorization object and the authorization rule may be specified in terms of ranges.  A comparison of the authorization object and authorization rule may determine whether two single values match, whether a\nsingle value is within a range, or whether two ranges overlap.  When a range is used, a comparison rule can specify whether or not the range endpoints count as a match.\nAs an example of ranges, a range may be specified as A* to Z* (which can be referred to as a pattern), which will match any string that begins with an uppercase letter (assuming case sensitivity).  A range may be specified as A*, in which the\nrange is strings that begin within an uppercase A. A range may be specified as AB01 to AL20 (which can be referred to as a character group), which is the range of strings that have the first character of A, a second character between B and L, and a\nnumber between 1 and 20.  In other aspects, ranges may be specified in different ways, an authorization object or rule is not specified as an authorization interval, or the innovations are applied to areas other than authorization objects and rule.\n<BR><BR>Example 7--Example Machine Learning Analysis of Positionally-Encoded String Representation\nA key benefit of the disclosed innovations is that strings can be converted to a numeric representation that can be an input to a machine learning algorithm.  Typically, machine learning algorithms are unable to use strings as input.  FIG. 6\nillustrates a process 600 for analyzing a positionally-encoded representation of a string 610 using a machine learning component 614 applying an algorithm 618.  The output 622 of the machine learning component 614 can be a classification of the input\npositionally-encoded string representation 610, such as into one or more categories 626.  The output 622 can be, for example, a yes/no (true/false) determination, or a classification into a category 626.\nThe output 622 can represent different aspects of the input positionally-encoded string representation 610, such as depending on the nature or meaning of the input positionally-encoded string representation 610 and the algorithm 618 (including\nany training provided to the algorithm).  In the case of a positionally-encoded string representation 610 of an authorization, the categories 626 can represent access to a resource, or authorization for an action, being authorized or not authorized.  Or,\nthe categories 626 can represent a risk level associated with providing access to a resource, or permitting an action to be executed, such as low, medium, or high risk.  In other cases, rather than categories 626, the machine learning component 614 can\nprovide a different type of output 622, such as a numeral value that can be associated with an aspect of the input 610, such as a numerical value that is associated with a risk level.\nThe machine learning component 614 can accept multiple inputs, including the input 610 and one or more additional inputs 630.  The additional inputs 630 may be positionally-encoded representations of other strings or objects that are to be\ncompared with the input 610, or can represent rules that are used to evaluate the input.  The machine learning component 614 can be used, for example, to compare the input 610 with one or more of the additional inputs 630.  In a specific example, the\ninput 610 is an authorization object and an additional input 630 is a definition for authorization objects that are allowed (or not allowed) to access a resource or perform an action.  In another example, the input 610 is an identifier for an entity\n(e.g., a person or an object, such as a product, piece of machinery, or information) and the additional input 630 is a rule that specifies what entities should, or should not be provided to another entity (for instance, determining whether a piece of\nmachinery should be sold to a company located in a country that may be subject to sanctions).  In other cases, rather than determining whether authorization is present, the additional inputs 630 can be used to determine a level of risk for enabling\naccess to a resource or allowing an action.\n<BR><BR>Example 8--Example Software Architecture\nFIG. 7 is a block diagram providing an example software architecture 700 that can be used in implementing at least certain embodiments of the present disclosure.  The architecture 700 includes a computing platform 708 and a database 710.  In\nspecific examples, the computing platform 708 can be the S/4 HANA platform, and the database 710 can be the HANA database system, both of SAP SE of Walldorf, Germany.\nThe computing platform 708 can include an analysis engine 714.  The analysis engine 714 can analyze one or more strings or object instances having at least one string component.  For instance, the analysis engine 714 can analyze a string\ncomponent to determine whether access to a particular resource, or taking a particular action, is authorized, or to determine a level of risk associated with the string component, such as a level of risk in authorizing access to a particular resource or\nto take a particular action.\nThe analysis engine 714 can communication with a comparison engine 718 which can be, for example, the comparison engine 518 of FIG. 5.  The comparison engine 718 can access a rules framework 722, which can be the rules engine 522 of FIG. 5.\nThe comparison engine 718 can also be in communication with a consumption view 724 that can be generated using data stored in a data store 726 of the database 710 and the results of analyzing a model associated with an analysis procedure of the\nanalysis engine 714.  The model can be maintained and defined in a model layer 730 of the database 710.  The consumption view 724 can represent a compilation of data and, optionally, transformation of the data into a format that can be read and\nmanipulated by the analysis engine 714 and other components of the architecture 700.  In particular implementations, the consumption view 724 provides an interface for requests, such as requests using the ODATA protocol, from the analysis engine 714,\nincluding requests originating at other components of the architecture 700.  In a specific example, the consumption view 724 can be a CORE DATA SERVICES CONSUMPTION VIEW provided by the HANA database system and S/4 HANA platform of SAP SE of Walldorf,\nGermany.\nThe consumption view 724 can be generated in part from a data view 734, such as a CORE DATA SERVICES VIEW provided by the HANA database system and S/4 HANA platform of SAP SE of Walldorf, Del.  The data view 734 can be a design-time object that\naggregates, formats, or otherwise manipulates or presents data from one or more data sources.  For example, the data view 734 can be constructed from one or more query views 738 provided by the database 710.  For example, a query view 738 may represent a\ntable, such as virtual table, generated from the response to a query request processed by the database 710, such as using structured query language (SQL) statements to retrieve data from the data store 726.  In a specific example, the query view 738 can\nbe a SQL VIEW provided by the SAP HANA database system, in particular the SAP HANA EXTENDED APPLICATION SERVICES, of SAP SE of Walldorf, Germany.\nThe consumption view 724 can also include data provided by predictive analytics (including machine learning algorithms) associated with an analysis being executed or managed by the analysis engine 714.  In a particular example, the consumption\nview 724 can include or be associated with functions that define data to be included in the consumption view.  The functions can be maintained by a data view functions component 742 which, in particular examples, can be a CDS TABLE FUNCTION of the S/4\nHANA platform of SAP SE of Walldorf, Del.  The data view function component 742 can, for example, allow structured query language functions to be included in the consumption view 724.\nA particular function of the data view function component 742 can be associated with an object implementing the function associated with, or stored in, a function implementation component 748.  In a particular example, the functions can be\nimplemented in the ABAP programming language, such as an ABAP managed database procedure that can be used to manage or call stored procedures maintained in the database 710.  More particularly, the function implementation 748 can be in communication\nwith, and call or manage procedures stored in, a query procedures store 752 of the database 710.\nAt least a portion of the query procedures of the query procedure store 752 can interact with the model layer 730.  For example, procedures of the query procedure store 752 can retrieve, and optionally manipulate, data associated with a model\nmaintained by the model layer 730.  In at least some cases, other components of the computing platform 708 can interact with the model layer 730, such to create, edit, manage, or delete models.  In other cases, another components of the architecture 700\n(including components not specifically illustrated in FIG. 7) can interact with the model layer 730 to carry out such actions.\nIn at least some cases, the model layer 730 can provide a framework for creating and manipulating models within the architecture 700, including models that can be used for predictive modeling, or models that can be used for other purposes.  In a\nparticular implementation, the model layer 730 is, or includes, the UMML4HANA or PREDICTIVE ANALYSIS INTEGRATION frameworks of SAP SE of Walldorf, Germany.  In some cases, a single model may be used to model one or more aspects of a particular analysis\nof the analysis engine 714 (e.g., to determine whether two or more strings match, if a string complies with a rule, or string analytics).  In other cases, multiple models can be used for a single analysis.  For instance, different models may be\nassociated with different steps of the analysis.\nThe model layer 730 can communicate with a predictive modeling engine 758 to carry out analyses associated with a model.  In some cases, the predictive modeling engine 758 can execute analysis procedures stored in an analysis library 762.  In\nparticular examples, the analysis library can be the AUTOMATED PREDICTIVE LIBRARY, the PREDICTIVE ANALYSIS LIBRARY, or the APPLICATION FUNCTION LIBRARY of SAP SE of Walldorf, Germany.  Non-limiting examples of predictive analysis techniques that can be\nused by the predictive modeling engine 758 include clustering, classification, regression, association, time series, preprocessing, statistics, social network analysis, and combinations thereof.  In particular cases, the predictive analysis can include a\nmachine learning component.  Suitable machine learning techniques can include decision trees, artificial neural networks, instance-based learning, Bayesian methods, reinforcement learning, inductive logic programming, genetic algorithms, support vector\nmachines, or combinations thereof.\nIn at least some cases, the predictive modeling engine 758 can apply multiple analysis techniques to a particular model.  The analysis technique, or techniques to be used, can, in some aspects, be directly specified, such as by a user when\nconstructing a model.  In other aspects the predictive modeling engine 758 can determine which analysis technique should be used, or can analyze the model with multiple techniques.  In a specific example, the predictive modeling engine 758 can apply an\nalgorithm, such as structured risk minimization, to determine one or more analysis techniques that provide useful information.\nThe analysis library 762 can include, or can be in communication with, matrix functions 766.  Matrix functions 766 include functions for creating positionally-encoded representations of strings, for converting positionally-encoded string\nrepresentations back to string form, and for comparing positionally-encoded string representations.  In at least some implementations, the matrix functions 766, and/or the analysis library 762, can be accessed without accessing the predictive modeling\nengine 758.  For instance, the analysis library 762 or matrix functions 766 can be accessed by query procedures 752 and or the model layer 730.\nThe model layer 730 can optionally be in communication with an analysis interface 768.  In some implementations, the analysis interface 768 can allow for creation or manipulation of models of the model layer 730.  For example, the analysis\ninterface 768 can be in communication with a client system 770.  In further implementations, the analysis interface 768 can provide access to additional analysis tools or components.  For instance, the analysis interface 768 can use machine learning\ncapabilities provided by AMAZON WEB SERVICES (Seattle, Wash.), GOOGLE CLOUD PLATFORM (Google Inc., Mountain View, Calif.), MICROSOFT COGNITIVE SERVICES (Microsoft Corp, Redmond, Wash.), HPE HAVEN ON DEMAND (Hewlett Packard Enterprise Development LP, Palo\nAlto, Calif.), and IBM WATSON SERVICES ON BLUEMIX (IBM Corp., Armonk, N.Y.).\nThe data store 726 of the database 710 can include data, such as data used in a process associated with the analysis engine 714.  For example, the data can be stored in data tables 774.  The data store 726 can also include data to be used by the\nmodel layer 730.  In some cases, the model layer 730 can directly access data in the data tables 774.  In other cases, the database 710 can maintain data associated with the model layer 730 in model tables 778.\nReturning to the computing platform 708, the analysis engine 714 can be in communication with an application server 782 or similar component for providing access to the analysis engine 714 to a user or external applications.  The application\nserver 782 can include specific components to execute functionality associated with the analysis engine 714.  The application server 782 can be in communication with a user interface component 790 that can send information to, and receive information\nfrom, the client system 770, such as through a network interface 792.  The user interface 790 can process commands for execution by the application server 782, or format information for consumption by the client system 770.\nThe client system 770 can include a network interface 794 for communicating with other components of the architecture 700, including the computing platform 708.  Although not shown, in some embodiments, the client system 770 can directly\ncommunicate with the database 710.  The network interface 794 can communicate with a user interface 796.  The user interface 796 can be used to display information to a user and to receive user commands in interacting with an analysis managed by the\nanalysis engine 714.\nThe architecture 700 can include more or fewer components than shown, and may be organized in other manners.  For example, functionality of a particular component can be carried out by another component.  In addition, in at least some cases,\nfunctionality can be carried out using multiple components.  In a specific example, the functionality of two or more of the client system 770, the computing platform 708, and the database 710 can be combined in a single system.\n<BR><BR>Example 9--Example Rules Framework for Positionally-Encoded String Representations\nFIG. 8 schematically depicts a system 800 illustrating how a collection of logical data objects 810 can interact with a rules framework 814 (e.g., a rules framework in communication with the comparison engine 714 of FIG. 7, which can be in\ncommunication with a rules engine, such as the rules engine 522 of FIG. 5) to create a rule set 818 that can be used by an analysis engine, such as the analysis engine 714, including to provide a determination of whether an access request or action is\nauthorized, or to determine a level of risk, such as a risk level associated with an access request or action (which can also be used to determine whether to allow the access or action).\nThe logical data objects 810 can be maintained in a repository 822.  In some examples, the repository 822 can be a database, such as the database 710.  In other examples, the repository 822 can be maintained in a different manner, including\nbeing maintained by another component of the architecture 700, such as a repository maintained in the computing platform 708.\nThe logical data objects 810 can represent or contain information related to various components, such as tasks, decision, or data items (e.g., individual data elements, or collections of data elements, such as information related to a particular\nindividual or a particular account) which can be analyzed, such as using the analysis engine 714.  In particular examples, the logical data objects 810 can be authorization objects, representing permissions for a particular user, for example, or can be\nrule objects, useable to determine what permissions are needed to access a particular resource or carry out a particular action.  The logical data objects 810 can serve as input to one or more function objects 826 of a function repository 830.  The\nfunction objects 826 can perform processing on one or more logical data objects 810, or other information, serving as input for the function object.  In at least some cases, a function object 826 can return a result object 834 that includes an outcome of\nrule processing (e.g., whether authorization exists, whether access or an action should be permitted, a risk level, etc.).  The logical data objects 810, function objects 826, and, optionally, results objects 834, can form a model, or model\nspecification, for a particular analysis.\nThe function objects 830 can be associated with one or more rules 838 of the ruleset 818.  The rules 838 can define actions to be taken in response to a particular function object 826, such as based on values associated with, or produced by, the\nfunction object.  The rules 838 can specify actions to be taken, as well as the conditions under which the rule should be applied.  A rule 838 can be, for example, can be a definition of when two positionally-encoded string representations are equivalent\nor meet other criteria.  Although not shown in FIG. 8, in at least some cases, the rules 838 can access additional data for use in carrying out the rule.  The rules 838 can also return information to be incorporated in the result objects 834.\nThe system 800 can allow for decoupling of the logical data objects 810, function objects 826, and rules 838, which can facilitate creating and modifying an analysis, or the ruleset 818 used in executing, evaluating, or monitoring the analysis. \nIn addition, the system 800 illustrates how an analysis, such as an analysis defined using the logical data objects 810, function objects 826, and result objects 834, can be decoupled from the basis of the ruleset 818.\n<BR><BR>Example 10--Example Source Code Implementing Encoding Schema\nFIG. 9 illustrates example source code (in C++) that defines an encoding schema or interface, and sets dimensions for a positionally-encoded representation of an object that includes at least one string.  In this case, the object includes at\nmost two strings that are to be positionally-encoded, and so the positionally-encoded representation is a matrix cube, or three-dimensional array, having a maximum depth of two.  The height of each matrix in the cube is limited to 50 characters, and the\nwidth is equal to the number of characters or elements in the encoding schema.\nFIG. 9 also illustrates function prototypes for determining whether a particular element of a cube is a letter or a number, as comparisons may be carried out differently depending on the nature of the element.  Although not shown, other\nfunctions can be included to determine the nature of an element, such as to determine the case of a letter or to determine whether an element is a symbol, or other type of non-letter/non-numerical element.\n<BR><BR>Example 11--Example Source Code for Creating Positionally-Encoded String Representation and for Reforming Character String Representation\nFIG. 10 illustrates an example function, digitalize, that converts one or more strings, provided in the \"from\" and \"to\" parameters, to a matrix cube (e.g., two-dimensional matrix, which can be represented as a three-dimensional array)\nrepresentation.  If one of the \"from\" and \"to\" parameters is not provided, the cube representation is changed to a matrix representation (e.g., the cube has a depth of one).\nThe function iterates through the string, each iteration representing a y-axis position of the cube (e.g., character or element or the string) corresponding to a string element, and determines the proper encoding for the array corresponding to\nthe x-axis.  Specifically, the element at the x-index corresponding to the encoded value of the string is set to one, all elements of the array having been initialized to zero.  Note that the function converts any lowercase letters to uppercase to\ndetermine the x-axis encoding (e.g., case information may be lost in the encoding process).\nFIG. 11 illustrates an example function, convertCubeToString, that takes a cube (or two-dimensional matrix) representation of a cube and returns the corresponding string (e.g., character representation).  The function includes a parameter,\nconvertBack, that can be used to determine whether the function returns the character version of the string (if true), or the collapsed (e.g., one-dimensional), binary representation of the cube or matrix (if false).  If the function returns the\ncharacter representation of the string, the function uses the xAxis[x] function to retrieve the character value at the index position of the encoding array corresponding to the index position of the x-axis of the matrix (or cube) where a value of `1` is\ndetermined.\n<BR><BR>Example 12--Example Source Code for Evaluating Values of Positionally-Encoded String Representations\nAs has been described, various rules can be implemented to determine whether a first positionally-encoded string representation \"matches\" a second positionally-encoded string representation.  The rules can include matching discrete values,\nmatching a discrete value to a range, and matching two ranges.  FIG. 12 illustrates source code (C++) for an example function, singleToSingle, that determines whether positionally-encoded string representations exactly match.  The function returns true\n(e.g. there is a match) if the last position in one of the cubes is reached without a mismatch being detected.  Thus, this rule will consider two cubes to be equivalent if one cube is a subset of the other.  If desired, the rule could be defined\ndifferently, such that both cubes must be exactly identical for a match to be returned, or that the cubes must have identical content, but can be of different sizes (e.g., one cube can have rows filled with `0` values that are not present in the other\ncube).\nNote that, if a wildcard character is defined and detected (e.g., `*`), the function calls a resolveWildcardIn function of the cube class.  Representative source code for the resolveWildcardIn function is presented in FIG. 13.  The\nresolveWildcardIn function compares the remaining elements of both cubes being compared, returning false if any mismatches are detected (including one cube having a wildcard character and the other cube not having the wildcard character), and calling the\nresolveWildcardIn function again if any further wildcard characters are detected.  In particular, the function looks to see if the character in the matrix without the wildcard is the same as the next non-wildcard character in the matrix with the wildcard\n(e.g., comparing `i*we` to `iwe` would return true, while comparing `i*me` with `iwe` would return false).\nFIG. 14 presents example source code (C++) for determining whether a single value of one cube is within a range for another cube.  Briefly, the function first determines whether the cubes are empty, and resolves any wildcard characters as\ndescribed above.  For the next non-wildcard character, it is determined whether the next characters of the cubes are numerical or alphabetical.  If the values are numerical, the number in each cube is determined by reading the cube elements until a\nnon-numerical value is encountered (e.g., *8320 k would be read as 8320).  One of the cubes should have a numerical range, that is, the matrix at layer 0 and the matrix at layer 1 should both have numbers (referred to as the reference cube).  The single\nnumerical value of the test cube is then compared with the range in the reference cube.  The function returns true if the function is within the range, and false otherwise.  The function implements similar functionality to determine whether a letter of\nthe test cube is within a range of the reference cube (e.g., `c` is in the range of `a`-'g').\nAlthough not shown, a \"range\" to \"range\" comparison can be carried out in a similar manner as a \"single to range\" comparison.  In particular, numerical or alphabetical ranges are determined for each cube in a similar manner as described above\n(e.g., using the values at layer 0 and layer 1 of a cube as the limits of the range).  The ranges can then be compared to see whether they overlap.  Overlap can be specified as including the endpoints or not including the endpoints (e.g., the range 2-5\noverlaps the range 5-8 if endpoints are included, but not if endpoints are excluded).\n<BR><BR>Example 13--Example String Comparisons and Results\nFIG. 15 presents a table 900 listing example strings for an authorization rule and an authorization token or object.  The table 900 provides a column listing the results of comparing the particular strings listed in a particular row.  In some\ncases, a row includes a single \"from\" value for the authorization rule and the authorization token, representing a rule where two discrete values are compared to determine whether there is a match between an authorization rule and an authorization token. In other cases, a row includes a \"from\" value for the authorization rule or the authorization token and both \"from\" and \"to\" values for the other.  In this case, a comparison can determine whether there is a match depending on whether the single value\n\"from\" value falls within the range of the \"from\" and \"to\" values.  Finally, some cases include \"from\" and \"to\" values for both the authorization rule and the authorization token.  In these cases a comparison can determine whether the \"from\" and \"to\"\nranges overlap (which, as described above, can be define as including endpoints or as excluding endpoints, as desired).\nIn carrying out the comparison, both the authorization rule and the authorization token are converted to positionally-encoded string representations, such as two-dimensional array (in the case of a \"from\" value only) or a three-dimensional array\n(in the case where both \"from\" and \"to\" values are provided).  As described above, arrays can be analyzed by reducing the dimensionality of the arrays, such as by collapsing a two- or three-dimensional array into a one-dimensional array, or using a\nnumber corresponding to the one-dimensional array (e.g., a number represented by the binary values of the one-dimensional array).\n<BR><BR>Example 14--Example Method for Determining String Compliance with an Authorization Rule\nFIG. 16 is a flowchart of an example method 1000 for determining whether authorization exists based on comparing a positionally-encoded string representation with an authorization rule.  At 1005, an authorization value is received.  The\nauthorization value includes at least one character string.  At least a portion of the character string is converted to a positionally-encoded string representation at 1010.  The converting includes, for each character to be converted, at 1015, comparing\nthe character with an encoding schema.  The encoding schema includes an array of unique characters, such as the unique characters from which a string may be constructed (e.g., letter, numbers, symbols, or combinations thereof).  The array has a length. \nEach unique character has a unique position in the array.\nAt 1020, a number of elements in a data structure are provided that is equal to the length of the array of unique characters.  The number of elements have an initial position.  A unique character in the array of unique characters is determined,\nat 1025, that is the same as the character of the character string.  At 1030, the position of the unique character in the array of unique characters is determined, the position is an index value compared with an initial position of the array of array of\nunique characters.  The position in the data structure is determined, at 1035, that is equal to the initial position plus the index value.  A value of the determined position of the data structure is set at 1040.  The value indicates that the character\nof the character string is the determined unique character.  Elements of the data structure that are not the unique character have a value that indicates that the unique character at the corresponding position in the array of unique characters is not the\ncharacter of the character string.\nAt 1045, the positionally-encoded string representation is compared with an authorization rule to determine whether the positionally-encoded string representation meets authorization criterial.  Authorization is provided or denied based on the\ncomparing at 1050.\n<BR><BR>Example 15--Example Method for Determining Set Membership of String\nFIG. 17 is a flowchart of an example method 1100 for determining whether a string is a member of a set.  At 1110, a positionally-encoded representation is created of at least a portion of a string.  The string includes a plurality of characters. The positionally-encoded representation is created by encoding a data structure based on the positions of a plurality of characters of the at least a portion of the string in an encoding schema.  The positionally-encoded representation can include\nnumerical digits or Boolean values.  The positionally-encoded string representation is provided to a machine learning algorithm at 1120.  The machine learning algorithm provides an output value.  At 1130, the output value is analyzed to determine whether\nthe string is a member of a set.\n<BR><BR>Example 16--Example Method for Determining Compliance of Authorization Token with Authorization Criteria\nFIG. 18 is a flowchart of an example method 1200 for determining whether an action is authorized.  At 1210, a positionally-encoded representation of an authorization token is created.  The authorization token includes at least one string.  The\npositionally-encoded representation is created by encoding a data structure based on the positions of a plurality of characters of at least a portion of the string in an encoding schema.  The positionally-encoded representation of the authorization token\nis compared at 1220 with a positionally-encoded representation of an authorization rule.  The authorization rule defines a set of authorized strings.  At 1230, it is determined if the positionally-encoded representation of the authorization token meets\nauthorization criteria of the authorization rule.  Responsive to the comparing, such as if (or when) the results of the comparing meet the authorization criteria, at 1240, an action is permitted to be executed.\n<BR><BR>Example 17--Example Technical Solution\nThe technical solution of the disclosed innovations can facilitate the processing of character strings, including comparing or analyzing strings using machine learning algorithms, or using non-machine learning based techniques.  However, the\ndisclosed innovations can be particularly useful in machine learning-based approaches, as traditional character representations of strings can be unsuitable as inputs for machine learning algorithms.  The disclosed positionally-encoded string\nrepresentations can also preserve the ability of strings to be compared, as all character information can be retained (including positional information), and a character string representation can be reconstructed by applying an encoding schema in a\nreverse manner as that used to form a positionally-encoded string representations.  The disclosed innovations can provide increased accuracy in comparing or otherwise analyzing strings, and can reduce processing time and required processor use.  The\ndisclosed innovations can also facilitate technical applications that would have been difficult or impossible to achieve with other approaches.\n<BR><BR>Example 18--Computing Systems\nFIG. 19 depicts a generalized example of a suitable computing system 1300 in which the described innovations may be implemented.  The computing system 1300 is not intended to suggest any limitation as to scope of use or functionality of the\npresent disclosure, as the innovations may be implemented in diverse general-purpose or special-purpose computing systems.\nWith reference to FIG. 19, the computing system 1300 includes one or more processing units 1310, 1315 and memory 1320, 1325.  In FIG. 13, this basic configuration 1330 is included within a dashed line.  The processing units 1310, 1315 execute\ncomputer-executable instructions.  A processing unit can be a general-purpose central processing unit (CPU), processor in an application-specific integrated circuit (ASIC), or any other type of processor.  In a multi-processing system, multiple\nprocessing units execute computer-executable instructions to increase processing power.  For example, FIG. 19 shows a central processing unit 1310 as well as a graphics processing unit or co-processing unit 1315.  The tangible memory 1320, 1325 may be\nvolatile memory (e.g., registers, cache, RAM), non-volatile memory (e.g., ROM, EEPROM, flash memory, etc.), or some combination of the two, accessible by the processing unit(s) 1310, 1315.  The memory 1320, 1325 stores software 1380 implementing one or\nmore innovations described herein, in the form of computer-executable instructions suitable for execution by the processing unit(s) 1310, 1315.  The memory 1320, 1325, may also store database data, such as data associated with the database 710 of FIG. 7.\nA computing system 1300 may have additional features.  For example, the computing system 1300 includes storage 1340, one or more input devices 1350, one or more output devices 1360, and one or more communication connections 1370.  An\ninterconnection mechanism (not shown) such as a bus, controller, or network interconnects the components of the computing system 1300.  Typically, operating system software (not shown) provides an operating environment for other software executing in the\ncomputing system 1300, and coordinates activities of the components of the computing system 1300.\nThe tangible storage 1340 may be removable or non-removable, and includes magnetic disks, magnetic tapes or cassettes, CD-ROMs, DVDs, or any other medium which can be used to store information in a non-transitory way and which can be accessed\nwithin the computing system 1300.  The storage 1340 stores instructions for the software 1380 implementing one or more innovations described herein.\nThe input device(s) 1350 may be a touch input device such as a keyboard, mouse, pen, or trackball, a voice input device, a scanning device, or another device that provides input to the computing system 1300.  The output device(s) 1360 may be a\ndisplay, printer, speaker, CD-writer, or another device that provides output from the computing system 1300.\nThe communication connection(s) 1370 enable communication over a communication medium to another computing entity.  The communication medium conveys information such as computer-executable instructions, audio or video input or output, or other\ndata in a modulated data signal.  A modulated data signal is a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal.  By way of example, and not limitation, communication media can use\nan electrical, optical, RF, or other carrier.\nThe innovations can be described in the general context of computer-executable instructions, such as those included in program modules, being executed in a computing system on a target real or virtual processor.  Generally, program modules or\ncomponents include routines, programs, libraries, objects, classes, components, data structures, etc. that perform particular tasks or implement particular abstract data types.  The functionality of the program modules may be combined or split between\nprogram modules as desired in various embodiments.  Computer-executable instructions for program modules may be executed within a local or distributed computing system.\nThe terms \"system\" and \"device\" are used interchangeably herein.  Unless the context clearly indicates otherwise, neither term implies any limitation on a type of computing system or computing device.  In general, a computing system or computing\ndevice can be local or distributed, and can include any combination of special-purpose hardware and/or general-purpose hardware with software implementing the functionality described herein.\nIn various examples described herein, a module (e.g., component or engine) can be \"coded\" to perform certain operations or provide certain functionality, indicating that computer-executable instructions for the module can be executed to perform\nsuch operations, cause such operations to be performed, or to otherwise provide such functionality.  Although functionality described with respect to a software component, module, or engine can be carried out as a discrete software unit (e.g., program,\nfunction, class method), it need not be implemented as a discrete unit.  That is, the functionality can be incorporated into a larger or more general purpose program, such as one or more lines of code in a larger or general purpose program.\nFor the sake of presentation, the detailed description uses terms like \"determine\" and \"use\" to describe computer operations in a computing system.  These terms are high-level abstractions for operations performed by a computer, and should not\nbe confused with acts performed by a human being.  The actual computer operations corresponding to these terms vary depending on implementation.\n<BR><BR>Example 19--Cloud Computing Environment\nFIG. 20 depicts an example cloud computing environment 1400 in which the described technologies can be implemented.  The cloud computing environment 1400 comprises cloud computing services 1410.  The cloud computing services 1410 can comprise\nvarious types of cloud computing resources, such as computer servers, data storage repositories, networking resources, etc. The cloud computing services 1410 can be centrally located (e.g., provided by a data center of a business or organization) or\ndistributed (e.g., provided by various computing resources located at different locations, such as different data centers and/or located in different cities or countries).  The cloud computing services 1410 are utilized by various types of computing\ndevices (e.g., client computing devices), such as computing devices 1420, 1422, and 1424.  For example, the computing devices (e.g., 1420, 1422, and 1424) can be computers (e.g., desktop or laptop computers), mobile devices (e.g., tablet computers or\nsmart phones), or other types of computing devices.  For example, the computing devices (e.g., 1420, 1422, and 1424) can utilize the cloud computing services 1410 to perform computing operators (e.g., data processing, data storage, and the like).\n<BR><BR>Example 20--Implementations\nAlthough the operations of some of the disclosed methods are described in a particular, sequential order for convenient presentation, it should be understood that this manner of description encompasses rearrangement, unless a particular ordering\nis required by specific language set forth below.  For example, operations described sequentially may in some cases be rearranged or performed concurrently.  Moreover, for the sake of simplicity, the attached figures may not show the various ways in\nwhich the disclosed methods can be used in conjunction with other methods.\nAny of the disclosed methods can be implemented as computer-executable instructions or a computer program product stored on one or more computer-readable storage media, such as tangible, non-transitory computer-readable storage media, and\nexecuted on a computing device (e.g., any available computing device, including smart phones or other mobile devices that include computing hardware).  Tangible computer-readable storage media are any available tangible media that can be accessed within\na computing environment (e.g., one or more optical media discs such as DVD or CD, volatile memory components (such as DRAM or SRAM), or nonvolatile memory components (such as flash memory or hard drives)).  By way of example, and with reference to FIG.\n19, computer-readable storage media include memory 1320 and 1325, and storage 1340.  The term computer-readable storage media does not include signals and carrier waves.  In addition, the term computer-readable storage media does not include\ncommunication connections (e.g., 1370).\nAny of the computer-executable instructions for implementing the disclosed techniques as well as any data created and used during implementation of the disclosed embodiments can be stored on one or more computer-readable storage media.  The\ncomputer-executable instructions can be part of, for example, a dedicated software application or a software application that is accessed or downloaded via a web browser or other software application (such as a remote computing application).  Such\nsoftware can be executed, for example, on a single local computer (e.g., any suitable commercially available computer) or in a network environment (e.g., via the Internet, a wide-area network, a local-area network, a client-server network (such as a\ncloud computing network), or other such network) using one or more network computers.\nFor clarity, only certain selected aspects of the software-based implementations are described.  Other details that are well known in the art are omitted.  For example, it should be understood that the disclosed technology is not limited to any\nspecific computer language or program.  For instance, the disclosed technology can be implemented by software written in C++, Java, Perl, JavaScript, Python, Ruby, ABAP, SQL, Adobe Flash, or any other suitable programming language, or, in some examples,\nmarkup languages such as html or XML, or combinations of suitable programming languages and markup languages.  Likewise, the disclosed technology is not limited to any particular computer or type of hardware.  Certain details of suitable computers and\nhardware are well known and need not be set forth in detail in this disclosure.\nFurthermore, any of the software-based embodiments (comprising, for example, computer-executable instructions for causing a computer to perform any of the disclosed methods) can be uploaded, downloaded, or remotely accessed through a suitable\ncommunication means.  Such suitable communication means include, for example, the Internet, the World Wide Web, an intranet, software applications, cable (including fiber optic cable), magnetic communications, electromagnetic communications (including\nRF, microwave, and infrared communications), electronic communications, or other such communication means.\nThe disclosed methods, apparatus, and systems should not be construed as limiting in any way.  Instead, the present disclosure is directed toward all novel and nonobvious features and aspects of the various disclosed embodiments, alone and in\nvarious combinations and sub combinations with one another.  The disclosed methods, apparatus, and systems are not limited to any specific aspect or feature or combination thereof, nor do the disclosed embodiments require that any one or more specific\nadvantages be present or problems be solved.\nThe technologies from any example can be combined with the technologies described in any one or more of the other examples.  In view of the many possible embodiments to which the principles of the disclosed technology may be applied, it should\nbe recognized that the illustrated embodiments are examples of the disclosed technology and should not be taken as a limitation on the scope of the disclosed technology.  Rather, the scope of the disclosed technology includes what is covered by the scope\nand spirit of the following claims.", "application_number": "15594086", "abstract": " String comparison, including comparing strings with a rule, can be time\n     consuming, and strings may not be suitable for analysis using machine\n     learning algorithms. The present disclosure provides positionally-encoded\n     representations of strings, such as binary matrices or arrays, that\n     provide an encoded representation of the string. The encoding, and\n     decoding, can be facilitated by an encoding schema, such as an encoding\n     schema stored in the memory of a computer device. The encoding schema\n     associates particular characters with particular positions in an array.\n     The positionally-encoded string representations can be used for security\n     applications, such as to determine whether an authorization token that\n     includes at least one string complies with an authorization rule. The\n     authorization rule can be specified as a positionally-encoded\n     representation that defines a set of strings that comply with the rule.\n", "citations": ["20030200212", "20030200235", "20040127682", "20040199241", "20050021348", "20050240943", "20050262551", "20080127162", "20080307488", "20090077618", "20100189251", "20100325418", "20110251930", "20120054142", "20120150859", "20130090977", "20130091066", "20130124292", "20130145419", "20130304535", "20140100910", "20160098572", "20160162820"], "related": []}, {"id": "20180332449", "patent_code": "10375542", "patent_name": "Data usage recommendation generator", "year": "2019", "inventor_and_country_data": " Inventors: \nAdinarayan; Geetha (Bangalore, IN), Shi; Shaw-Ben (Austin, TX), Sivakumar; Gandhi (Bentleigh, AU), Tsai; Meng Hong (Taipei, TW)  ", "description": "<BR><BR>BACKGROUND\nThe present invention relates generally to the field of cellular networks and more particularly to data usage.\nA cellular network or mobile network is a communication network where the last link is wireless.  Mobile and other computing devices access cellular networks in order to perform actions and functions, such as accessing the Internet,\ncommunicating, or viewing content.  When a computing device accesses a cellular network the computing device uses data.  Providers of cellular networks often restrict the amount of data a particular computing device can use during any period of time. \nUsers of computing devices may have difficulty monitoring data usage to avoid exceeding these data usage thresholds.\n<BR><BR>SUMMARY\nAspects of the present invention disclose a method, computer program product, and system for determining data usage access.  The method include receiving, by one or more computer processors, a request for data usage, the request for data usage\nbeing associated with a mobile device.  The method further includes identifying, by one or more computer processors, a current data usage quantity, the current data usage quantity being associated with the mobile device.  The method further includes\ndetermining, by one or more computer processors, whether the current data usage quantity is beyond a threshold data usage.  The method further includes responding to the data usage quantity being beyond the threshold data usage, by receiving, by one or\nmore computer processors, mobile device information associated with the mobile device.  The method further includes analyzing, by one or more computer processors, the mobile device information.  The method further includes determining, by one or more\ncomputer processors, access for the request based on the mobile device information and the current data usage quantity. <BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nFIG. 1 is a block diagram of a computing environment suitable for operation of a data access program, in accordance with at least one embodiment of the present invention;\nFIG. 2 is a flowchart depicting operational steps for a data access program, in accordance with at least one embodiment of the present invention; and\nFIG. 3 is a block diagram of components of a computing apparatus suitable for executing a data access program, in accordance with at least one embodiment of the present invention.\n<BR><BR>DETAILED DESCRIPTION\nFIG. 1 is a block diagram illustrating a distributed data processing environment, generally designated 100, in accordance with one embodiment of the present invention.  FIG. 1 provides only an illustration of one implementation and does not\nimply any limitations with regard to the environments in which different embodiments may be implemented.  Many modifications to the depicted environment may be made by those skilled in the art without departing from the scope of the invention.\nThe distributed data processing environment 100 includes a computing device 112 communicating with a network 110.  The network 110 represents, for example, a telecommunications network, a local area network (LAN), a wide area network (WAN), such\nas the Internet, or a combination of the three, and includes wires, wireless, and/or fiber optic connections.  Network 110 includes one or more wired and/or wireless networks that are capable of receiving and transmitting data, voice, and/or video\nsignals, including multimedia signals that include voice, data, and video information.\nIn the depicted embodiment, the computing device 112 is one or more of a management server, a web server, a client device, or any other electronic device or computing system capable of receiving, generating, and sending data.  In some\nembodiments, the computing device 112 is a computing system utilizing multiple computers as a server and/or client system, such as in a cloud computing environment.  In some embodiments, the computing device 112 is a laptop, a tablet computer, a netbook\ncomputer, a personal computer (PC), a desktop computer, a personal digital assistant (PDA), a smart phone, or any programmable electronic device capable of communicating with another computing device (not depicted) via the network 110.  In other\nembodiments, the computing device 112 is a computing system utilizing clustered computers and components to act as a single pool of seamless resources.  The computing device 112 may include components as depicted and described in further detail with\nrespect to FIG. 3, in accordance with embodiments of the present invention.  The computing device 112 includes a data access program 120, a database 130, and an analytics engine 124.\nThe analytics engine 124 is a software program capable of receiving input, generating results, and communicating the results with the data access program 120.  In some embodiments, the analytics engine 124 is an engine for cognitive computing. \nIn some embodiments, the analytics engine 124 is a general purpose analytics engine.  In other embodiments, the analytics engine 124 operates on a model tailored to data access programs (e.g., data access program 120).  The analytics engine 124 may be\nconfigured for finding and analyzing patterns and efficiencies at the macro level, understood in the context of data usage and access systems.  In various embodiments, the analytics engine 124 may be a function of data access program 120.  In an example,\ndata access program 120 may have multiple functions or models, one of which may be the analytics engine 124.\nIn the depicted embodiment, the analytics engine 124 resides on the computing device 112.  In other embodiments, the analytics engine 124 may reside elsewhere in the distributed data processing environment 100, such as within another computing\ndevice (not illustrated), a server device (not illustrated), or a client device (not illustrated).  In some embodiments, the data access program 120 is the analytics engine 124.\nIn the depicted embodiment, the database 130 resides on the computing device 112.  In another embodiment, the database 130 may reside elsewhere in the distributed data processing environment 100, such as within another computing device, a server\ndevice, a client device, or as a standalone database that is capable of communicating with the computing device 112 via the network 110.  The database 130 is an organized collection of data.  The Database 130 is implemented with any type of storage\ndevice capable of storing data that is accessed and utilized by the computing device 112, such as a database server, a hard disk drive, or a flash memory.  In some embodiments, the database 130 represents multiple storage devices within the computing\ndevice 112.  The database 130 stores information related to the computing device 112, such as the computing device 112's data usage history, data usage plan, data usage limit, application usage, location, times associated with activity, user preset\nthreshold data limits, and other information relevant to the usage of the computing device 112, such as availability of the network 110, type of connection available via the network 110, and signal strength of the network 110.\nIn the depicted embodiment, the data access program 120 resides on the computing device 112.  In another embodiment, the data access program 120 may reside elsewhere in the distributed data processing environment 100, such as within another\ncomputing device, a server device, or a client device that is capable of communicating with the computing device 112 via the network 110.  In some embodiments, the data access program 120 receives information from the computing device 112, the database\n130, and the analytics engine 124.  In some embodiments, the data access program 120 may reside on a server device and communicate with a corresponding data access program client device (not illustrated).  In such an embodiment, the data access program\nclient device may reside on a computing device, such as the computing device 112.  In such an embodiment, the data access program 120 may receive information from the data access program client device.  For example, the data access program 120 may reside\non a server device and receive information, such as user history, data usage, and other client device information, from an application on a cellular phone.\nFIG. 2 is a flowchart depicting operational steps for the data access program 120, in accordance with at least one embodiment of the present invention.  In some embodiments, the operational steps for the data access program 120 are initiated\nwhen a computing device is turned on.  In other embodiments, the operational steps for the data access program are initiated when the data access program 120 receives a request for data usage associated with a computing device.  In some embodiments, the\noperational steps for the data access program 120 may be initiated responsive to input from a user.\nAt step 200, the data access program 120 receives a request for data usage associated with a mobile device, such as the computing device 112.  Data usage refers to digital information the computing device 112 uses when accessing the cellular\nnetwork.  Actions requiring the computing device 112 to access a cellular network require the computing device 112 to use data.  Data usage may be measured in units of digital information, such as bytes.  Certain actions use more data than other actions. For example, streaming a video requires the computing device 112 to use more data than making a phone call.  In some embodiments, each time the computing device 112 begins performing an action that uses data, the data access program 120 may receive the\nrequest for data usage.  For example, the computing device 112 receives a request for data usage when the computing device 112 opens an email or a social media application.  The request for data usage may include meta-information about the request.  For\nexample, the request for data usage may include a time associated with when the request is made, a location associated with where the request is made, or the type of request being made.\nAt step 202, the data access program 120 identifies data usage associated with the mobile device.  The data usage is associated with the computing device 112.  Data usage for the computing device 112 may be measured and/or tracked as a total for\nthe lifespan of the device, in real time, or incremental totals (e.g., monthly, weekly, periodically, etc.).  In some embodiments, the data access program 120 may identify the data usage within a database, such as the database 130.  In some embodiments,\nthe database 130 may store data information for the computing device 112 incrementally.  For example, a data usage for the computing device 112 may be determined every week, and a log of data usage may be stored within the databased 130.  In some\nembodiments, the database 130 may store a record of each action performed by the computing device 112 and the data usage associated with each of the actions.  In such an embodiment, the data access program 120 may calculate the data usage by adding the\ndata usage stored within the database 130.  In such an embodiment, the data access program 120 may calculate the data usage by adding the data usage stored within the database 130 for a particular time period, such as for a current month or for a current\nbilling period.  In some embodiments, the data access program 120 may identify data usage from a user profile.  For example, a user may set up a profile that identifies different applications he or she uses.  In such an example, the data access program\n120 may identify data usage from the profile.\nAt step 204, the data access program 120 determines whether the data usage is beyond a threshold data usage.  The threshold data usage may be predetermined based on a subscription plan.  For example, a user of a mobile phone may pay ten dollars\na month for one gigabyte of data usage.  In such an embodiment, one gigabyte of data usage per month is the threshold data usage.  In other embodiments, a user may predetermine the threshold.  The threshold data usage may be stored in a database that is\ncapable of communicating with the data access program 120, such as the database 130.  The data access program 120 may determine whether the data usage is beyond the threshold data usage by comparing the two values to see which has a greater value.  If,\nno, the data usage is not beyond the threshold data usage, the data access program 120 exits.  If, yes, the data usage is beyond the threshold data usage, the data access program 120 proceeds to step 206.\nIn some embodiments, the data access program 120 may determine the threshold data usage based on information stored within the database 130.  In some embodiments the analytics engine 124 may determine the threshold data usage based on\ninformation stored within the database 130.  For example, the analytics engine 124 may determine the threshold data usage based on a predetermined amount set by a subscription plan in addition to mobile data use history.  For example, if a subscription\nplan sets a limit at twenty gigabytes of data and a user has a history utilizing large amounts of data in a short period of time (e.g., streaming videos) the threshold data usage may be that 18 gigabytes are allotted.  If, however, the user has a history\nof only utilizing small increments of data over time (e.g., 0.1 gigabytes per day) the threshold data usage may be 19.8 gigabytes.\nIn some embodiments, the threshold data usage is based on a percentage of usage of a total data usage for a billing cycle of a wireless subscription plan.  For example, a subscription plan may set a twenty gigabytes of data limit for a billing\ncycle.  In such an example, the threshold data usage may be ninety percent of the twenty gigabytes of data limit.\nAt step 206, the data access program 120 receives mobile device information associated with the mobile device.  The mobile device information is associated with the computing device 112 and may be stored within the database 130.  The mobile\ndevice information includes at least one artefact selected from the group consisting of: a timestamp, a location of a mobile device, a current data usage, a data usage history of the mobile device, a signal strength of the mobile device, a network\nconnection availability of the mobile device, and a mobile device application usage trail.\nThe computing device information may include information about time, a location for the computing device 112, current data usage for the computing device 112, data usage associated with the request, strength of the network 110's signal(s), type\nof the network 110's signals (wireless, cellular, etc.), a log of data usage history for the computing device 112, or a mobile device application usage trail.\nInformation about time may be a timestamp for when the request is made.  Information about time may also include timestamps for similar requests.  For example, information about time may be that the computing device 112 always accesses a social\nmedia website at or around 4 p.m.  Information about time may also be how long a data using action has lasted in the past.  For example, information about time may be that the computing device 112 typically spends ten minutes checking email or two hours\nstreaming videos.\nInformation about location may be a location for where the request is made.  For example, location information may be that the computing device 112 is at a particular address.  Location information may also include predetermined and/or\npre-identified frequent locations.  For example, a user of the computing device 112 may indicate that a specific location is his or her home or his or her place of employment.  Location information may include that the location where the request is being\nmade is a new location, or one the computing device 112 has never been to before.  Location information may include information about the location, such as whether the location is a restaurant, a shopping center, a hospital, etc.\nCurrent data usage for the computing device 112 may be associated with the data usage threshold.  For example, current data usage may be the amount of data that has been used for the current month, period, or billing cycle.  Current data usage\nmay also include information about the type of data that has been used over the past month.  For example, whether data was used to make phone calls, download videos, etc.\nData usage associated with the request from step 200 may be based on previous actions taken by the computing device 112.  For example, if the request is to access a social media website, and the computing device 112 typically uses one megabyte\nof data each time the social media website is accessed, the data usage associated with the request to access the social media website may be one megabyte of data.  In other embodiments, data usage associated with the request may vary based on time spent\nperforming the action.  For example, if the request is to stream episodes of a television show, information about the request may be that one hour of streaming will use one megabyte of data and two hours of streaming will use two megabytes of data, etc.\nIn such an example, the data access program 120 may indicate to a user of the computing device 112 information indicating that the data used for the request will vary based on the time spent performing the action.\nStrength of the network signal may include information about how easily the computing device 112 is able to access the network 110.  For example, in certain areas, access to a network is difficult based on proximity of available cellular towers\nand networks.  In such an example, strength of network signal may include a measurement of current signal strength.  Strength of signal information may also include current location information.  For example, the current signal strength may be low due to\nweather at the current location, or due to the current location being a tunnel.\nType of network signal available may include information about the type of network the network 110 provides the computing device 112.  In some embodiments, the network 110 provides multiple possible connections, such as a wireless or a cellular\nsignal.  Type of network signal available may include current location information.  For example, the type of network signal available may include that wireless access is restricted for the computing device 112 or that wireless access is available at a\nnearby location.\nA mobile device application usage trail is a grouping of commands, programs, and functions that have historically occurred together or in proximity.  For example, a mobile device application usage trail may include that using a camera\napplication is shortly followed by using an editing software within a social media website.\nAt step 208, the data access program 120 analyzes the mobile device information.  In some embodiments, the analysis involves the data access program 120 sending computing device information to the analytics engine 124, the analytics engine 124\nanalyzing the computing device information, and the data access program 120 receiving analyzed computing device information.\nThe analytics engine 124 may analyze the computing device information based on a static and/or dynamic process correlating the request received at step 200.  For example, the analytics engine 124 may be a big data analytics engine.  Big data\nanalytics is a process of analyzing and examining large volumes of data to discover patterns, correlations, and other information about the data.  Big data analytics analyzes higher volumes of data than conventional analytics and business intelligence\nsolutions would analyze.  For example, the big data analytics engine may access two hundred million pages of structured and unstructured content consuming over four terabytes of disk storage.  Big data analytics may include predictive analytics. \nPredictive analytics encompasses statistical techniques such as predictive modeling, machine learning, and data mining to analyze current and historical facts to make predictions about future, or otherwise unknown, events.  The analytics engine 124 may\nidentify patterns, correlation, and trends within the computing device information.  The analytics engine 124 may use the patterns, correlations, and trends within the computing device information to extrapolate or predict further actions based on the\nrequest and the computing device information.  The analytics engine 124 may analyze the computing device information using high-performance data mining, predictive analytics, machine learning, natural language processing, text mining, text analysis,\nforecasting, and/or optimization.\nFor example, the analytics engine 124 may analyze time information to determine a pattern of data accessing behavior.  For example, the analytics engine 124 may identify that a user checks his or her email around 8:00 a.m.  every morning.  In\nsuch an example the analytics engine 124 may identify the user checking his or her email around 8:00 a.m.  every morning as a pattern of behavior.\nIn another example, the analytics engine 124 may analyze mobile device information to determine how much data a particular request will require.  For example, the analytics engine 124 may use information about user history to determine how much\ndata a particular request will require.  In such an example, the analytics engine 124 may determine if a user typically streams videos for several hours at a time using approximately five gigabytes of data each time, the analytics engine 124 may\ndetermine that a request for streaming a video will use five gigabytes of data.  In another example the analytics engine 124 may use information about location.  For example, the analytics engine 124 may identify that at certain locations a request uses\nmore data than the same request would at another location due to the poor quality of network cellular reception.  In another example, the analytics engine 124 may use time information.  In such an example, the analytics engine 124 may identify the late\nafternoon as a high traffic time for a particular cellular phone application, and that the high traffic slows the server for the particular cellular phone application, in turn causing a request to use more data than that same request would at an earlier\nor later time.\nIn another example, the analytics engine may 124 analyze information about location.  For example, the analytics engine 124 may identify that a first time a user travels to a location, he or she accesses a map on his or her computing device, but\ndoes not use a map during subsequent visits.  The analytics engine 124 may also identify that a user downloads a weather report for a location approximately one hour before leaving towards the location.  In such an example, the analytics engine 124 may\nidentify a conditional pattern of behavior such that if a user downloads a weather report for a location the user has not previously visited the user will access a map in approximately an hour.\nIn some embodiments, the analytics engine 124 may identify these patterns based on input from a computer programmer.  For example, a computer programmer may identify potential patterns for the analytics engine 124 to look for.  In other\nembodiments, the analytics engine may compare information from regular time intervals, such as daily or weekly, looking for similarities and differences, and/or compare information from different locations looking for similarities and differences.\nAt step 210, the data access program 120 determines access for the request.  The request was received at step 200.  The access is whether or not, and the manner by which, the computing device 112 should perform the request for data usage.  In\nsome embodiments, access is allowing the request.  In other embodiments, access is prohibiting the request.\nIn some embodiments, access is based on whether or not a user would be required to purchase additional data as a part of his or her subscription plan.  For example, access may always be prohibited if additional data must be purchased to perform\na request.  In another embodiment, access may be prohibited unless a user is in a new location and downloading a map.  In another embodiment, access may be prohibited unless a user manually overrides the prohibition.\nIn other embodiments, access is deferring the request.  For example, the data access program 120 may identify a time for the request indicating a new billing cycle within a monthly subscription plan is about to begin.  In such an example, the\ndata access program 120 may determine that the request should be deferred.  In another embodiment, the data access program 120 may identify a location for the computing device 112 and identify that the computing device 112 is near a location with access\nto a wireless network (e.g., one block from home) and defer the request until the computing device 112 is at the location with access to the wireless network.  In another example, the data access program 120 may identify that a user will enter a new\nbilling cycle in one day and defer the request until the user and the computing device 112 are in a new billing cycle.\nIn another example, the data access program 120 may defer access to a social media site because the data access program 120 has determined the social media site is a lower priority activity (as opposed to making phone calls, accessing maps, or\nchecking emails).  Such a prioritization may be based on user input, input from a program developer, or a feature of the data access program 120.  For example, the data access program 120 may identify certain activities as having a higher priority based\non the time of day the activities are typically performed.  For example, activities performed between the hours of 8:30 a.m.  and 12:00 p.m.  may be ranked as having a higher priority than activities performed between the hours of 12:00 p.m.  and 1:00\np.m.  Deferring the request may be done so that the request is completed at a future time such as when the computing device 112 has access to a wireless network.\nIn other embodiments, access includes congesting the request.  Congesting the request may include making adjustments to the manner in which the request is carried out such that the action uses less data.  For example, the request may be to view\na video via a web browser within the computing device 112.  The data access program 120 may display the video with a lower quality.  In another example, the request may be to view a web page with images and graphics embedded within the web page.  The\ndata access program 120 may act by displaying the web page without the images and graphics such that the web page only contains plain text.  Congesting the request may be done so that the request uses less data than it would otherwise use.  The data\naccess program 120 may analyze previous requests similar to the request for data usage and compare data usage between requests of lower quality or with fewer images.\nIn other embodiments, access is prefetching the request.  The data access program 120 may identify within the data usage history that a computing device 112 uses several social media websites during a morning commute.  In such an embodiment, the\ndata access program 120 may download content for the social media websites when the computing device 112 has access to a wireless network, such as when the computing device 112 is at home or at a train station.  In the previous example, where a user\ndownloads weather information one hour before downloading map information if the user has never been to the location, prefetching the request may be downloading the map at a time earlier than when the user attempts to download the map, such as while the\nweather information is being downloaded if the computing device 112 has access to a wireless network at the time the weather information is downloaded.  Prefetching the request may be done so that the request and/or a predicted future request does not\nuse data since the request was completed when the computing device 112 had access to a wireless network.\nIn some embodiments, the data access program 120 has access to functions and applications within the computing device 112.  In such embodiments, the data access program 120 may allow access based on determinations made at step 204 automatically. For example, the data access program 120 may automatically download a video with lower quality or pre-fetch content from a social media website.  In some embodiments, the data access program 120 may generate an alert indicating the type of access for a\nparticular request and display the alert via the computing device 112.\nFIG. 3 is a block diagram depicting components of a computer 300 suitable for executing the data access program 120.  In some embodiments the computer 300 is the computing device 112.  FIG. 3 displays the computer 300, the one or more\nprocessor(s) 304 (including one or more computer processors), the communications fabric 302, the memory 306, the RAM 316, the cache 316, the persistent storage 308, the communications unit 310, the I/O interfaces 312, the display 320, and the external\ndevices 318.  It should be appreciated that FIG. 3 provides only an illustration of one embodiment and does not imply any limitations with regard to the environments in which different embodiments may be implemented.  Many modifications to the depicted\nenvironment may be made.\nAs depicted, the computer 300 operates over a communications fabric 302, which provides communications between the cache 316, the computer processor(s) 304, the memory 306, the persistent storage 308, the communications unit 310, and the\ninput/output (I/O) interface(s) 312.  The communications fabric 302 may be implemented with any architecture suitable for passing data and/or control information between the processors 304 (e.g., microprocessors, communications processors, and network\nprocessors, etc.), the memory 306, the external devices 318, and any other hardware components within a system.  For example, the communications fabric 302 may be implemented with one or more buses or a crossbar switch.\nThe memory 306 and persistent storage 308 are computer readable storage media.  In the depicted embodiment, the memory 306 includes a random access memory (RAM).  In general, the memory 306 may include any suitable volatile or non-volatile\nimplementations of one or more computer readable storage media.  The cache 316 is a fast memory that enhances the performance of computer processor(s) 304 by holding recently accessed data, and data near accessed data, from memory 306.\nProgram instructions for the data access program 120 may be stored in the persistent storage 308 or in memory 306, or more generally, any computer readable storage media, for execution by one or more of the respective computer processors 304 via\nthe cache 316.  The persistent storage 308 may include a magnetic hard disk drive.  Alternatively, or in addition to a magnetic hard disk drive, the persistent storage 308 may include, a solid state hard disk drive, a semiconductor storage device,\nread-only memory (ROM), electronically erasable programmable read-only memory (EEPROM), flash memory, or any other computer readable storage media that is capable of storing program instructions or digital information.\nThe media used by the persistent storage 308 may also be removable.  For example, a removable hard drive may be used for persistent storage 308.  Other examples include optical and magnetic disks, thumb drives, and smart cards that are inserted\ninto a drive for transfer onto another computer readable storage medium that is also part of the persistent storage 308.\nThe communications unit 310, in these examples, provides for communications with other data processing systems or devices.  In these examples, the communications unit 310 may include one or more network interface cards.  The communications unit\n310 may provide communications through the use of either or both physical and wireless communications links.  The data access program 120 may be downloaded to the persistent storage 308 through the communications unit 310.  In the context of some\nembodiments of the present invention, the source of the various input data may be physically remote to the computer 300 such that the input data may be received and the output similarly transmitted via the communications unit 310.\nThe I/O interface(s) 312 allows for input and output of data with other devices that may operate in conjunction with the computer 300.  For example, the I/O interface 312 may provide a connection to the external devices 318, which may include a\nkeyboard, keypad, a touch screen, and/or some other suitable input devices.  External devices 318 may also include portable computer readable storage media, for example, thumb drives, portable optical or magnetic disks, and memory cards.  Software and\ndata used to practice embodiments of the present invention may be stored on such portable computer readable storage media and may be loaded onto the persistent storage 308 via the I/O interface(s) 312.  The I/O interface(s) 312 may similarly connect to a\ndisplay 320.  The display 320 provides a mechanism to display data to a user and may be, for example, a computer monitor.\nThe programs described herein are identified based upon the application for which they are implemented in a specific embodiment of the invention.  However, it should be appreciated that any particular program nomenclature herein is used merely\nfor convenience, and thus the invention should not be limited to use solely in any specific application identified and/or implied by such nomenclature.\nThe present invention may be a system, a method, and/or a computer program product at any possible technical detail level of integration.  The computer program product may include a computer readable storage medium (or media) having computer\nreadable program instructions thereon for causing a processor to carry out aspects of the present invention.\nThe computer readable storage medium can be a tangible device that can retain and store instructions for use by an instruction execution device.  The computer readable storage medium may be, for example, but is not limited to, an electronic\nstorage device, a magnetic storage device, an optical storage device, an electromagnetic storage device, a semiconductor storage device, or any suitable combination of the foregoing.  A non-exhaustive list of more specific examples of the computer\nreadable storage medium includes the following: a portable computer diskette, a hard disk, a random access memory (RAM), a read-only memory (ROM), an erasable programmable read-only memory (EPROM or Flash memory), a static random access memory (SRAM), a\nportable compact disc read-only memory (CD-ROM), a digital versatile disk (DVD), a memory stick, a floppy disk, a mechanically encoded device such as punch-cards or raised structures in a groove having instructions recorded thereon, and any suitable\ncombination of the foregoing.  A computer readable storage medium, as used herein, is not to be construed as being transitory signals per se, such as radio waves or other freely propagating electromagnetic waves, electromagnetic waves propagating through\na waveguide or other transmission media (e.g., light pulses passing through a fiber-optic cable), or electrical signals transmitted through a wire.\nComputer readable program instructions described herein can be downloaded to respective computing/processing devices from a computer readable storage medium or to an external computer or external storage device via a network, for example, the\nInternet, a local area network, a wide area network and/or a wireless network.  The network may comprise copper transmission cables, optical transmission fibers, wireless transmission, routers, firewalls, switches, gateway computers and/or edge servers. \nA network adapter card or network interface in each computing/processing device receives computer readable program instructions from the network and forwards the computer readable program instructions for storage in a computer readable storage medium\nwithin the respective computing/processing device.\nComputer readable program instructions for carrying out operations of the present invention may be assembler instructions, instruction-set-architecture (ISA) instructions, machine instructions, machine dependent instructions, microcode, firmware\ninstructions, state-setting data, configuration data for integrated circuitry, or either source code or object code written in any combination of one or more programming languages, including an object oriented programming language such as Smalltalk, C++,\nor the like, and procedural programming languages, such as the \"C\" programming language or similar programming languages.  The computer readable program instructions may execute entirely on the user's computer, partly on the user's computer, as a\nstand-alone software package, partly on the user's computer and partly on a remote computer or entirely on the remote computer or server.  In the latter scenario, the remote computer may be connected to the user's computer through any type of network,\nincluding a local area network (LAN) or a wide area network (WAN), or the connection may be made to an external computer (for example, through the Internet using an Internet Service Provider).  In some embodiments, electronic circuitry including, for\nexample, programmable logic circuitry, field-programmable gate arrays (FPGA), or programmable logic arrays (PLA) may execute the computer readable program instructions by utilizing state information of the computer readable program instructions to\npersonalize the electronic circuitry, in order to perform aspects of the present invention.\nAspects of the present invention are described herein with reference to flowchart illustrations and/or block diagrams of methods, apparatus (systems), and computer program products according to embodiments of the invention.  It will be\nunderstood that each block of the flowchart illustrations and/or block diagrams, and combinations of blocks in the flowchart illustrations and/or block diagrams, can be implemented by computer readable program instructions.\nThese computer readable program instructions may be provided to a processor of a general purpose computer, special purpose computer, or other programmable data processing apparatus to produce a machine, such that the instructions, which execute\nvia the processor of the computer or other programmable data processing apparatus, create means for implementing the functions/acts specified in the flowchart and/or block diagram block or blocks.  These computer readable program instructions may also be\nstored in a computer readable storage medium that can direct a computer, a programmable data processing apparatus, and/or other devices to function in a particular manner, such that the computer readable storage medium having instructions stored therein\ncomprises an article of manufacture including instructions which implement aspects of the function/act specified in the flowchart and/or block diagram block or blocks.\nThe computer readable program instructions may also be loaded onto a computer, other programmable data processing apparatus, or other device to cause a series of operational steps to be performed on the computer, other programmable apparatus or\nother device to produce a computer implemented process, such that the instructions which execute on the computer, other programmable apparatus, or other device implement the functions/acts specified in the flowchart and/or block diagram block or blocks.\nThe flowchart and block diagrams in the Figures illustrate the architecture, functionality, and operation of possible implementations of systems, methods, and computer program products according to various embodiments of the present invention. \nIn this regard, each block in the flowchart or block diagrams may represent a module, segment, or portion of instructions, which comprises one or more executable instructions for implementing the specified logical function(s).  In some alternative\nimplementations, the functions noted in the blocks may occur out of the order noted in the Figures.  For example, two blocks shown in succession may, in fact, be executed substantially concurrently, or the blocks may sometimes be executed in the reverse\norder, depending upon the functionality involved.  It will also be noted that each block of the block diagrams and/or flowchart illustration, and combinations of blocks in the block diagrams and/or flowchart illustration, can be implemented by special\npurpose hardware-based systems that perform the specified functions or acts or carry out combinations of special purpose hardware and computer instructions.", "application_number": "16044978", "abstract": " The method include receiving, by one or more computer processors, a\n     request for data usage, the request for data usage being associated with\n     a mobile device. The method further includes identifying, by one or more\n     computer processors, a current data usage quantity, the current data\n     usage quantity being associated with the mobile device. The method\n     further includes determining, by one or more computer processors, whether\n     the current data usage quantity is beyond a threshold data usage. The\n     method further includes responsive to the data usage quantity being\n     beyond the threshold data usage, receiving, by one or more computer\n     processors, mobile device information associated with the mobile device.\n     The method further includes analyzing, by one or more computer\n     processors, the mobile device information. The method further includes\n     determining, by one or more computer processors, access for the request\n     based on the mobile device information and the current data usage\n     quantity.\n", "citations": ["6366780", "7415038", "8219062", "8577329", "8683042", "8886749", "9736671", "10070285", "20080244576", "20100017506", "20110151831", "20110276442", "20120155296", "20120173363", "20120272227", "20130172041", "20130196615", "20130196621", "20130346624", "20140195297", "20150065085", "20170208446", "20170279972", "20170311141"], "related": ["15643115", "14997717"]}, {"id": "20180337983", "patent_code": "10375160", "patent_name": "Mechanisms for programmable composition of factor graphs", "year": "2019", "inventor_and_country_data": " Inventors: \nLi; Li (Bridgewater, NJ), Ren; DaQi (Saratoga, CA)  ", "description": "<BR><BR>TECHNICAL FIELD\nThe present disclosure is related to machine learning and, in one particular embodiment, to mechanisms for programmable composition of factor graphs in statistical machine learning and probabilistic reasoning.\n<BR><BR>BACKGROUND\nA discrete factor graph is a directed acyclic bipartite graph representing the factorization of a joint probability distribution.  The nodes of the discrete factor graph are random variables with discrete values (e.g., Booleans, categories,\nintervals, or integers).  The edges of the discrete factor graph represent causal relationships between the connected random variables.  The factors of the discrete factor graph define conditional probability distributions of the random variables with\nmatrices.\nRealistic factor graphs for applications may have hundreds of nodes for which information is gathered from distinct sources.  Sub-graphs may be created for each group of nodes corresponding to each of the distinct sources, but it is difficult to\ncombine the separate sub-graphs into a coherent and conflict-free factor graph.\n<BR><BR>SUMMARY\nVarious examples are now described to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description.  The Summary is not intended to identify key or essential features of the claimed subject\nmatter, nor is it intended to be used to limit the scope of the claimed subject matter.\nAccording to one aspect of the present disclosure, a computer-implemented method of deploying a cloud application comprises: accessing, by one or more processors, a first factor graph that represents a first component of the cloud application,\nthe first factor graph comprising a first set of nodes; accessing, by the one or more processors, a second factor graph that represents a second component of the cloud application, the second factor graph comprising a second set of nodes; determining, by\nthe one or more processors, a third set of nodes that are present in both the first set of nodes and the second set of nodes; joining, by the one or more processors, the first factor graph and the second factor graph into a third factor graph, wherein\nthe joining includes unifying the third set of nodes in the first factor graph and the second factor graph; based on the third factor graph, selecting, by the one or more processors, one or more computing resources; and deploying, by the one or more\nprocessors, at least a portion of the cloud application to the selected computing resources.\nOptionally, in any of the preceding aspects, the selecting of the one or more computing resources comprises selecting a data center of a plurality of data centers.\nOptionally, in any of the preceding aspects, the selecting of the one or more computing resources comprises selecting one or more servers of a data center.\nOptionally, in any of the preceding aspects, the deploying is in response to a determination that resources allocated to the cloud application do not match the needs of the cloud application.\nOptionally, in any of the preceding aspects, the cloud application is running in a first data center of a plurality of data centers; the selecting of the one or more computing resources comprises simulating deployment of the cloud application to\none or more of the plurality of data centers; and the deploying of at least the portion of the cloud application to the selected computing resources comprises deploying at least the portion of the cloud application to a second data center of the\nplurality of data centers.\nOptionally, in any of the preceding aspects, the second factor graph further comprises a set of factors; and the third set of nodes excludes nodes that are associated with more than one factor in the set of factors.\nOptionally, in any of the preceding aspects, the method further includes: for each pair of nodes in the third set of nodes: determining if a first node of the pair is an ancestor node in the second factor graph of a second node of the pair;\ndetermining if the second node of the pair is an ancestor node in the second factor graph of the first node of the pair; and if the first node and the second node are each an ancestor node of the other in the second factor graph, removing the first node\nand the second node from the third set of nodes.\nOptionally, in any of the preceding aspects, a further implementation of the aspect provides that the joining of the first factor graph and the second factor graph into the third factor graph comprises: creating the third factor graph as an\nempty factor graph; and, for each node in the third set of nodes: removing a factor parent of the node from the second factor graph; copying the node into the third factor graph; and setting a parent of the copied node to a parent of the node in the\nfirst factor graph.\nOptionally, in any of the preceding aspects, a further implementation of the aspect provides that the joining of the first factor graph and the second factor graph into the third factor graph further comprises: for each node in the third set of\nnodes: setting the copied node as a parent of children of the node in the first factor graph and children of the node in the second factor graph.\nOptionally, in any of the preceding aspects, a further implementation of the aspect provides that the joining of the first factor graph and the second factor graph into the third factor graph further comprises: for each node in the first set of\nnodes that is not in the third set of nodes: copying the node from the first factor graph into the third factor graph; and for each node in the second set of nodes that is not in the third set of nodes: copying the node from the second factor graph into\nthe third factor graph.\nOptionally, in any of the preceding aspects, a further implementation of the aspect provides that the third factor graph comprises a set of factors; and the method further comprises: identifying a first factor in the third factor graph, a first\nset of edges comprising all edges connected to the first factor in the third factor graph, and a fourth set of nodes comprising all nodes connected by an edge to the first factor in the third factor graph; identifying a second factor in the third factor\ngraph, a second set of edges comprising all edges connected to the second factor in the third factor graph, and a fifth set of nodes comprising all nodes connected by an edge to the second factor in the third factor graph; removing the first factor and\nthe first set of edges from the third factor graph; removing the second factor and the second set of edges from the third factor graph; based on the first factor and the second factor, determining a third factor; and inserting the third factor into the\nthird factor graph, with edges connecting the third factor to the fourth set of nodes and the fifth set of nodes.\nOptionally, in any of the preceding aspects, a further implementation of the aspect provides that a node is in both the fourth set of nodes and the fifth set of nodes; the edge connecting the node to the first factor is directed from the node to\nthe first factor; the edge connecting the node to the second factor is directed from the second factor to the node; and the edge connecting the node to the third factor is directed from the third factor to the node.\nOptionally, in any of the preceding aspects, a further implementation of the aspect provides that the data center is a data center of a plurality of data centers; the method further comprises: accessing characteristics of each of the plurality\nof data centers; and the selecting of the data center is further based on the characteristics of each of the plurality of data centers.\nAccording to one aspect of the present disclosure, a composition server comprises a memory storage comprising instructions; and one or more processors in communication with the memory storage, wherein the one or more processors execute the\ninstructions to perform: accessing a first factor graph that represents a first component of a cloud application, the first factor graph comprising a first set of nodes; accessing a second factor graph that represents a second component of the cloud\napplication, the second factor graph comprising a second set of nodes; determining a third set of nodes that are present in both the first set of nodes and the second set of nodes; joining the first factor graph and the second factor graph into a third\nfactor graph, wherein the joining includes unifying the third set of nodes in the first factor graph and the second factor graph; based on the third factor graph, selecting one or more computing resources; and deploying at least a portion of the cloud\napplication to the selected computing resources.\nOptionally, in any of the preceding aspects, the selecting of the one or more computing resources comprises selecting a data center of a plurality of data centers.\nOptionally, in any of the preceding aspects, the selecting of the one or more computing resources comprises selecting one or more servers of a data center.\nOptionally, in any of the preceding aspects, the deploying is in response to a determination that resources allocated to the cloud application do not match the needs of the cloud application.\nOptionally, in any of the preceding aspects, the cloud application is running in a first data center of a plurality of data centers; the selecting of the one or more computing resources comprises simulating deployment of the cloud application to\none or more of the plurality of data centers; and the deploying of at least the portion of the cloud application to the selected computing resources comprises deploying at least the portion of the cloud application to a second data center of the\nplurality of data centers.\nOptionally, in any of the preceding aspects, a further implementation of the aspect provides that the second factor graph further comprises a set of factors; and the third set of nodes excludes nodes that are associated with more than one factor\nin the set of factors.\nOptionally, in any of the preceding aspects, a further implementation of the aspect provides that the processors further perform, for each pair of nodes in the third set of nodes: determining if a first node of the pair is an ancestor node in\nthe second factor graph of a second node of the pair; determining if the second node of the pair is an ancestor node in the second factor graph of the first node of the pair; and if the first node and the second node are each an ancestor node of the\nother in the second factor graph, removing the first node and the second node from the third set of nodes.\nOptionally, in any of the preceding aspects, a further implementation of the aspect provides that the joining of the first factor graph and the second factor graph into the third factor graph comprises: creating the third factor graph as an\nempty factor graph; and for each node in the third set of nodes: removing a factor parent of the node from the second factor graph; copying the node into the third factor graph; and setting a parent of the copied node to a parent of the node in the first\nfactor graph.\nOptionally, in any of the preceding aspects, a further implementation of the aspect provides that the joining of the first factor graph and the second factor graph into the third factor graph further comprises: for each node in the third set of\nnodes: setting the copied node as a parent of children of the node in the first factor graph and children of the node in the second factor graph.\nOptionally, in any of the preceding aspects, a further implementation of the aspect provides that the second factor graph further comprises a set of factors; and the third set of nodes excludes nodes that are associated with more than one factor\nin the set of factors.\nOptionally, in any of the preceding aspects, a further implementation of the aspect provides that the processors further perform: for each pair of nodes in the third set of nodes: determining if a first node of the pair is an ancestor node in\nthe second factor graph of a second node of the pair; determining if the second node of the pair is an ancestor node in the second factor graph of the first node of the pair; and if the first node and the second node are each an ancestor node of the\nother in the second factor graph, removing the first node and the second node from the third set of nodes.\nAccording to one aspect of the present disclosure, there is provided a non-transitory computer-readable medium that stores computer instructions for deploying a cloud application that, when executed by one or more processors, cause the one or\nmore processors to perform steps of: accessing a first factor graph that represents a first component of the cloud application, the first factor graph comprising a first set of nodes; accessing a second factor graph that represents a second component of\nthe cloud application, the second factor graph comprising a second set of nodes; determining a third set of nodes that are present in both the first set of nodes and the second set of nodes; joining the first factor graph and the second factor graph into\na third factor graph, wherein the joining includes unifying the third set of nodes in the first factor graph and the second factor graph; based on the third factor graph, selecting one or more computing resources; and deploying at least a portion of the\ncloud application to the selected computing resources.\nAny one of the foregoing examples may be combined with any one or more of the other foregoing examples to create a new embodiment within the scope of the present disclosure. <BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nFIG. 1 is a block diagram illustration of a factor graph, according to some example embodiments.\nFIG. 2 is a block diagram illustration of a data flow suitable for programmable composition of factor graphs, according to some example embodiments.\nFIG. 3 is a block diagram illustration of composition of factor graphs, according to some example embodiments.\nFIG. 4 is a block diagram illustration of a factor graph, according to some example embodiments.\nFIG. 5 is a block diagram illustration of a factor graph, according to some example embodiments.\nFIG. 6 is a block diagram illustration of a factor graph, according to some example embodiments.\nFIG. 7A is a block diagram illustration of factor graphs to be combined, according to some example embodiments.\nFIG. 7B is a block diagram illustration of combined factor graphs, according to some example embodiments.\nFIG. 8A is a block diagram illustration of factor graphs to be combined, according to some example embodiments.\nFIG. 8B is a block diagram illustration of combined factor graphs, according to some example embodiments.\nFIG. 9 is a block diagram illustrating circuitry for clients and servers that implement algorithms and perform methods, according to some example embodiments.\nFIG. 10 is a flowchart illustration of a method of a mechanism for programmable composition of factor graphs, according to some example embodiments.\nFIG. 11 is a flowchart illustration of a method for identifying a set of interface nodes, according to some example embodiments.\nFIG. 12 is a flowchart illustration of a method of a mechanism for programmable composition of factor graphs, according to some example embodiments.\nFIG. 13 is a flowchart illustration of a method of a mechanism for programmable composition of factor graphs, according to some example embodiments.\nFIG. 14 is a flowchart illustration of a method of a mechanism for programmable composition of factor graphs, according to some example embodiments.\nFIG. 15 is a flowchart illustration of a method of deploying a cloud application to computing resources, according to some example embodiments.\nFIG. 16 is a flowchart illustration of a method of deploying a cloud application to a data center, according to some example embodiments.\nFIG. 17 is a flowchart illustration of a method of deploying a cloud application to servers of a data center, according to some example embodiments.\nFIG. 18 is a flowchart illustration of a method of reallocating resources for a cloud application in a data center, according to some example embodiments.\n<BR><BR>DETAILED DESCRIPTION\nIn the following description, reference is made to the accompanying drawings that form a part hereof, and in which are shown, by way of illustration, specific embodiments which may be practiced.  These embodiments are described in sufficient\ndetail to enable those skilled in the art to practice the inventive subject matter, and it is to be understood that other embodiments may be utilized and that structural, logical, and electrical changes may be made without departing from the scope of the\npresent disclosure.  The following description of example embodiments is, therefore, not to be taken in a limiting sense, and the scope of the present disclosure is defined by the appended claims.\nThe functions or algorithms described herein may be implemented in software, in one embodiment.  The software may consist of computer-executable instructions stored on computer-readable media or a computer-readable storage device such as one or\nmore non-transitory memories or other types of hardware-based storage devices, either local or networked.  The software may be executed on a digital signal processor, application-specific integrated circuit (ASIC), programmable data plane chip,\nfield-programmable gate array (FPGA), microprocessor, or other type of processor operating on a computer system, such as a switch, server, or other computer system, turning such a computer system into a specifically programmed machine.\nA cloud application typically consists of many distributed software components that demand various computing resources and communicate over data center networks.  Existing tools are limited to analyzing and predicting resource demands of\nindividual components and do not scale well to forecasting the needs of entire cloud applications.  A computing resource is any resource that may be utilized by an executing computer program.  Example computing resources include, but are not limited to:\ncentral processing units (CPUs), processor cores, virtual memory, physical memory, storage, network bandwidth, file handles, network sockets, locks, cache memory, and random number generators.  A component of a cloud application is a process, group of\nprocesses, or communication between or among two or more processes or groups of processes.  Example components include, but are not limited to: virtual machines, instances of applications, and network connections between or among applications.\nThe resource demands of individual components are modeled using factor graphs.  Using a factor graph composition mechanism described herein, a composition server combines component factor graphs to generate a composite factor graph that\nrepresents an entire cloud application.  Using the composite factor graph, computing resources of the data center supporting the cloud application may be allocated.  The cloud application is deployed to the allocated computing resources.  Deploying a\ncloud application refers to the process of installing and activating each of the components of the cloud application.  For example, one or more executable files for each component are copied to a memory or storage device allocated to the component, and\none or more allocated CPUs are used to execute the component.\nFIG. 1 is a block diagram illustration of a factor graph 100, according to some example embodiments.  The factor graph 100 includes factors 110, 120, 130, 140, and 150 as well as nodes 160, 170, 180, 190, and 195.  Factors and nodes are\nconnected by directional edges, such that each of the factors 110-150 has only node parents and each of the nodes 160-195 has only factor parents.  In the factor graph 100, each of the nodes 160-195 is a Boolean value.  Thus, there is or is not an event\ncorresponding to each of the nodes.  The probability of the burglary node 160 being true is indicated by the independent factor 110.  The probability of the earthquake node 170 being true is indicated by the independent factor 120.  The probability of\nthe alarm node 180 being true is determined by the factor 130, based on the values of the burglary node 160 and the earthquake node 170.  The probability of the news node 190 being true is determined by the factor 140, based on the value of the\nearthquake node 170.  The probability of the dog bark node 195 being true is determined by the factor 150, based on the value of the alarm node 180.\nIn some example embodiments, each of the factors 110-150 defines a conditional probability distribution of its child nodes using an m.times.n matrix for each child node.  The matrix includes one column for each possible value of the child node\nand one row for each combination of values of the parent nodes.  When there are no parent nodes, the matrix reduces to a single row, the probability distribution of the value of the child node having the value corresponding to each column.  Thus the\nmatrix corresponding to the factor 130 would have four rows (one for each combination of true and false of the burglary node 160 and the earthquake node 170) and two columns (one for true and one for false).  The probability of the child node being true\nis the value of the element in the matrix corresponding to the states of the parent nodes of the matrix and the output value of true.  For example, if the matrix for the factor 130 is\n##EQU00001## the top row corresponds to the burglary node 160 and the earthquake node 170 both being true, the second row corresponds to the burglary node 160 being true and the earthquake node 170 being false, the third row corresponds to the\nburglary node 160 being false and the earthquake node 170 being true, the bottom row corresponds to the burglary node 160 and the earthquake node 170 both being false, the left column corresponds to the value of true, the right column corresponds to the\nvalue of false, the burglary node 160 is false, and the earthquake node 170 is true, then the probability of the alarm node 180 being true is 0.4.\nIn some example embodiments, the factors 110-150, the structure of the factor graph 100, or both are learned from training data.  Using the factor graph 100, the marginal probabilities and conditional probabilities of any subset of the random\nvariables may be estimated exactly or approximately.\nFIG. 2 is a block diagram illustration of a data flow 200 suitable for programmable composition of factor graphs, according to some example embodiments.  Shown in the data flow 200 are a template 210 and a factor graph repository 220, used as\ninputs to a composition translator 230.  The composition translator 230 produces component factor graphs 240 and a composition script 250, which are inputs for a composition server 260.  The composition server 260 produces a composite factor graph 270,\nwhich is provided to a factor graph inference engine 280.  The factor graph inference engine 280 produces resource analyses and predictions 290.\nThe template 210 describes relationships between or among components of a distributed cloud application.  The template 210 may be a topology and orchestration specification for cloud computing (TOSCA) template or a heat orchestration (Heat)\ntemplate.  Example TOSCA relations are \"Process X contain Process Y\" and \"Process X connect Process Y.\"\nEach component and relation in the template 210 may correspond to a component factor graph in the factor graph repository 220.  For example, u1 may be a component factor graph of Process X, u2 may be a component factor graph of Process Y, u3 may\nbe a component factor graph of the containment relation, and u4 may be a component factor graph of the network connection relation.\nThe composition translator 230 selects the appropriate factor graphs (e.g., the graphs u1-u4) from the factor graph repository 220 based on the relationships described in the template 210.  The selected factor graphs are produced as component\nfactor graphs 240.  The composition translator 230 also produces the composition script 250.  The composition script 250 defines relationships between or among the factor graphs that correspond to the relationships between or among the components and\nresources described by the template 210.\nIn an example embodiment, the composition script 250 is formed using the following pseudo-code:\nTABLE-US-00001 Script = Composition_Translaction(TOSCA graph, entry S) scripts = [ ] Breadth-First Traversal of TOSCA graph from entry S: for each parent P for each child C of P R = relationship from P to C if R == contain script_i = containment\nscript for P and C else if R == connect script_i = connection script for P and C else script_i = `` scripts.append(script_i) return scripts\nAn example composition script for \"Process X contain Process Y\" is:\ng1=Graph(`u1`)\ng2=Graph(`u2`)\ng3=Graph(`u3`)\nc=Composer( )\ng0=c.join(g1, g3, g2)\nAn example composition script for \"Process X connect Process Y\" is:\ng1=Graph(`u1`)\ng2=Graph(`u2`)\ng4=Graph(`u4`)\nc=Composer( )\ng0=c.join(g1, g4, g2)\nThe composition server 260 uses the composition script 250 to combine the component factor graphs 240 to produce the composite factor graph 270.  As compared to the component factor graphs 240, which each represent an individual component or\nrelationship, the composite factor graph 270 represents the entire distributed cloud application of the template 210.\nThe composition server 260 may provide an application programming interface (API) as described in the tables below.  \"OOP\" refers to object-oriented programming, such as Java or C++.  \"REST\" refers to representational state transfer, often used\nwith the hypertext transport protocol (HTTP).\nTABLE-US-00002 OOP REST Services Action Graph = new Graph URI = POST Create a new factor Graph(Reference) .  . . /graphs[Reference] graph from a file, stream, uniform resource identifier (URI), etc. Composer = new Composer URI = Create a\nComposer Composer( ) POST .  . . /composers [ ] Graph = Graph URI = POST Compose/Transform Composer.{operator} {Composer factor graphs with (Graph, .  . . ) URI}/{operator} {operator} [Graph URI, .  . . ]\nThe Composer may support the following operators:\nTABLE-US-00003 Operator Action Graph = Composer.join(Graphs, Combine N factor graphs into one at Nodes) the interface Nodes Graph = Composer.select(Graph, Modify the content of a factor node FX, map) FX with function map in Graph Graph =\nComposer.project(Graph, Remove random values from variable VX, [C1, .  . . , Cn]) node VX in Graph Graph = Composer.reduce(Graph, Combine multiple factors into one in [F1, .  . . , Fn]) Graph\nThe factor graph inference engine 280 generates the resource analyses and predictions 290 based on information about resource availability and the composite factor graph 270.\nThe composition translator 230, composition server 260, and factor graph inference engine 280 may each be implemented in a computing device 900, described below with respect to FIG. 9.\nFIG. 3 is a block diagram illustration 300 of composition of factor graphs 330, 332, and 334, according to some example embodiments.  The factor graph 330 includes factors 302, 304, and 306 as well as nodes 308, 310, and 312.  The factor graph\n332 includes factors 314 and 316 as well as nodes 318 and 320.  The factor graph 334 includes factors 322 and 324 as well as nodes 326 and 328.  The factor graphs 330 and 332 are combined via a join operation 336 to form a factor graph 338.  The factor\ngraphs 338 and 334 are combined via a join operation 340 to form a factor graph 342.\nIn FIG. 3, the factor graph 330 (labeled \"G1\"), the factor graph 332 (labeled \"G2\"), and the factor graph 334 (labeled \"G3\") are component factor graphs.  The factor graph 338 (labeled \"G12\") and the factor graph 342 (labeled \"G0\") are composite\nfactor graphs.  The composite factor graphs may be produced by the composition server 260 executing the composition script 250, described above with respect to FIG. 2.\nThe interface nodes between the factor graphs 330 and 332 are the nodes 312 and 318, both labeled \"Alarm.\" Thus, the join operation 336 will create the composite factor graph 338 with merged nodes 312 and 318 as well as merged factors 306 and\n314.\nThe interface nodes between the factor graphs 338 and 334 are the nodes 310 and 326, both labeled \"Earthquake.\" Thus, the join operation 340 will create the composite factor graph 342 with merged nodes 326 and 310 as well as merged factors 322\nand 304.  The resulting factor graph 342 is shown in FIG. 1 as the factor graph 100.\nAn example composition script is below, wherein [A] refers to nodes labeled \"Alarm\" and [Q] refers to nodes labeled \"Earthquake.\"\ng1=Graph(` .  . . /factor-graphs/1`)\ng2=Graph(` .  . . /factor-graphs/2`)\ng3=Graph(` .  . . /factor-graphs/3`)\nc=Composer( )\ng12=C.join(g1, g2, [A])\ng0=C.join(g12, g3, [Q])\nFIG. 4 is a block diagram illustration of a factor graph 400, according to some example embodiments.  The factor graph 400 includes factors 405, 410, 415, and 420 as well as nodes 425, 430, 435, 440, 445, 450, and 455.  The factor graph 400 may\nbe used as a component factor graph for a process or thread that models the probabilistic causal relations among the workload node 425, the machine capacity node 430, the process or thread priority node 435, and resulting resource allocations of CPU\ntime, memory, storage, and network bandwidth (represented by the nodes 440-455).\nFIG. 5 is a block diagram illustration of a factor graph 500, according to some example embodiments.  The factor graph 500 includes factors 510, 520, 530, 540, and 550 and nodes 560, 570, 580, 590, and 595.  The factor graph 500 may be used as a\ncomponent factor graph for a containment relation that models the probabilistic causal relations between available resources (represented by the nodes 560-590) and usable capacity (represented by the node 595).  Example containment relations include a\nphysical machine that contains a virtual machine, a virtual machine that contains Docker containers, a Docker container that contains processes, or any suitable combination thereof.  A Docker container is an executable software package that includes a\nsoftware application along with support libraries and data used by the software application.  For example, the Docker container may include a runtime library, system tools, system libraries, and a settings database.\nIn different example embodiments, different resources are modeled.  For example, in FIG. 5, the nodes 560-590 model CPU, memory, storage, and network resources.  Additional or alternate resources include packet loss frequency, connection error\nrate, message passing interface (MPI) transfer speed, data throughput, transmission control protocol (TCP) transfer speed, user datagram protocol (UDP) transfer speed, Internet protocol (IP) transfer speed, CPU load percentage, instance efficiency,\nbenchmark operations per second, benchmark efficiency, mean memory access time, mean memory put time, mean memory get time, memory speed, random memory update rate, intra-node memory access scaling, memory performance drop under increasing workload, Unix\nbenchmark utility score, one-byte data access rate of storage, benchmark input/output (I/O) operation speed, blob I/O operation speed, table I/O operation speed, queue I/O operation speed, I/O access retry rate, benchmark I/O speed, job computation time,\njob communication time, or any suitable combination thereof.\nFIG. 6 is a block diagram illustration of a factor graph 600, according to some example embodiments.  The factor graph 600 includes factors 610, 620, and 630 as well as nodes 640, 650, and 660.  The factor graph 600 may be used as a component\nfactor graph for a network connection that models the probabilistic causal relations among network demand (represented by the node 640), available bandwidth (represented by the node 650), and workload (represented by the node 660).\nFIG. 7A is a block diagram illustration 700A of factor graphs to be combined, according to some example embodiments.  FIG. 7A shows factor graphs 701, 713, and 724.  The factor graphs 701 and 724 are copies of the factor graph 400 and the factor\ngraph 713 is a copy of the factor graph 500.  The factor graph 701 includes factors 702, 703, 704, and 705 and nodes 706, 707, 708, 709, 710, 711, and 712.  The factor graph 713 includes factors 714, 715, 716, 717, and 718 and nodes 719, 720, 721, 722,\nand 723.  The factor graph 724 includes factors 725, 726, 727, and 728 and nodes 729, 730, 731, 732, 733, 734, and 735.  The factors of the factor graphs 701 and 724 correspond to the factors 405-420 and the nodes of the factor graphs 701 and 724\ncorrespond to the nodes 425-455, each of which are described above with respect to FIG. 4.  The factors of the factor graph 713 correspond to the factors 510-550 and the nodes of the factor graph 713 correspond to the nodes 560-595, each of which are\ndescribed above with respect to FIG. 5.  As can be seen in FIG. 7A, the nodes 709-712 of the factor graph 701 align with, and have the same labels as, the nodes 719-722 of the factor graph 713.  Additionally, the node 723 of the factor graph 713 aligns\nwith, and has the same label as, the node 730 of the lower factor graph 724.  Thus, the nodes 709-712 of the factor graph 701 and the nodes 719-722 of the factor graph 713 may be interface nodes for a join operation.  Likewise, the node 723 of the factor\ngraph 713 and the node 730 of the lower factor graph 724 may be interface nodes for a join operation.\nFIG. 7B is a block diagram illustration 700B of combined factor graphs, according to some example embodiments.  The block diagram illustration 700B shows a composite factor graph resulting from the combination of the factor graphs shown in FIG.\n7A.  The composite factor graph includes factors 736, 737, 738, 739, 740, 741, 742, and 743 as well as nodes 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, and 757.  The nodes 744-746 correspond to the nodes 706-708 of the factor graph\n701 of FIG. 7A.  The values of the factors 736-738 are the same as the values of the factors 702-704 of the factor graph 701 of FIG. 7A.  The nodes 747-750 result from the merger of the nodes 709-712 of the factor graph 701 of FIG. 7A with the nodes\n719-722 of the factor graph 713 of FIG. 7A.  The node 752 results from the merger of the node 723 of the factor graph 713 of FIG. 7A with the node 730 of the lower factor graph 724 of FIG. 7A.  The remaining nodes 751 and 753-757 correspond to the nodes\n729 and 731-735 of the factor graph 724 of FIG. 7A.\nIn some example embodiments, the factor graph 701 of FIG. 7A represents a first process (e.g., a Docker container), the factor graph 713 of FIG. 7A represents a containment relationship, and the factor graph 724 of FIG. 7A represents a second\nprocess (e.g., a virtual machine).  Thus, the composite factor graph of FIG. 7B represents the first process including details of its contained second process.\nFIG. 8A is a block diagram illustration 800A of factor graphs to be combined, according to some example embodiments.  FIG. 8A shows factor graphs 801, 813, and 820.  The factor graphs 801 and 820 are copies of the factor graph 400 and the factor\ngraph 813 is a copy of the factor graph 600.  The factor graph 801 includes factors 802, 803, 804, and 805 and nodes 806, 807, 808, 809, 810, 811, and 812.  The factor graph 813 includes factors 814, 815, and 816 and nodes 817, 818, and 819.  The factor\ngraph 820 includes factors 821, 822, 823, and 824 and nodes 825, 826, 827, 828, 829, 830, and 831.  The factors of the factor graphs 801 and 820 correspond to the factors 405-420 and the nodes of the factor graphs 801 and 820 correspond to the nodes\n425-455, each of which are described above with respect to FIG. 4.  The factors of the factor graph 813 correspond to the factors 610-630 and the nodes of the factor graph 813 correspond to the nodes 640-660, each of which are described above with\nrespect to FIG. 6.  As can be seen in FIG. 8A, the node 812 of the factor graph 801 has the same label as the node 817 of the factor graph 813.  Additionally, the node 819 of the factor graph 813 has the same label as the node 825 of the factor graph\n820.  Thus, the node 812 of the factor graph 800 and the node 817 of the factor graph 813 may be interface nodes for a join operation.  Likewise, the node 819 of the factor graph 813 and the node 825 of the factor graph 820 may be interface nodes for a\njoin operation.\nFIG. 8B is a block diagram illustration 800B of combined factor graphs, according to some example embodiments.  The block diagram illustration 800B shows a composite factor graph resulting from the combination of the factor graphs shown in FIG.\n8A.  The composite factor graph includes factors 832, 833, 834, 835, 836, 837, 838, 839, and 840 as well as nodes 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, and 855.  The nodes 841-846 correspond to the nodes 806-811 of the\nfactor graph 801 of FIG. 8A.  The values of the factors 832-834 are the same as the values of the factors 802-804 of the factor graph 801 of FIG. 8A.  The node 847 results from the merger of the node 812 of the factor graph 801 of FIG. 8A with the node\n817 of the factor graph 813 of FIG. 8A.  The node 848 corresponds to the node 818 of the factor graph 813 of FIG. 8A.  The value of the factor 836 is the same as the value of the factor 815 of the factor graph 813 of FIG. 8A.  The node 849 results from\nthe merger of the node 819 of the factor graph 813 of FIG. 8A with the node 825 of the factor graph 820 of FIG. 8A.  The remaining nodes 850-855 correspond to the nodes 826-831 of the factor graph 820 of FIG. 8A.\nIn some example embodiments, the factor graph 801 of FIG. 8A represents a first process (e.g., a web server), the factor graph 813 of FIG. 8A represents a connection relationship, and the factor graph 820 of FIG. 8A represents a second process\n(e.g., an application server).  Thus, the composite factor graph of FIG. 8B represents the first process in communication with the second process.  For a cloud application with many components and relations, the composition patterns of FIGS. 7A-8B may be\nrepeated many times, as determined by a composition script (e.g., the composition script 250 of FIG. 2).\nFIG. 9 is a block diagram illustrating circuitry for implementing algorithms and performing methods, according to example embodiments.  All components need not be used in various embodiments.  For example, clients, servers, and cloud-based\nnetwork resources may each use a different set of components, or, in the case of servers for example, larger storage devices.\nOne example computing device in the form of a computer 900 (also referred to as computing device 900 and computer system 900) may include a processor 905, memory storage 910, removable storage 915, and non-removable storage 920, all connected by\na bus 940.  Although the example computing device is illustrated and described as the computer 900, the computing device may be in different forms in different embodiments.  For example, the computing device may instead be a smartphone, a tablet, a\nsmartwatch, or another computing device including elements the same as or similar to those illustrated and described with regard to FIG. 9.  Devices such as smartphones, tablets, and smartwatches are generally collectively referred to as \"mobile devices\"\nor \"user equipment.\" Further, although the various data storage elements are illustrated as part of the computer 900, the storage may also or alternatively include cloud-based storage accessible via a network, such as the Internet, or server-based\nstorage.\nThe memory storage 910 may include volatile memory 945 and non-volatile memory 950, and may store a program 955.  The computer 900 may include--or have access to a computing environment that includes--a variety of computer-readable media, such\nas the volatile memory 945, the non-volatile memory 950, the removable storage 915, and the non-removable storage 920.  Computer storage includes random-access memory (RAM), read-only memory (ROM), erasable programmable read-only memory (EPROM) and\nelectrically erasable programmable read-only memory (EEPROM), flash memory or other memory technologies, compact disc read-only memory (CD ROM), digital versatile disks (DVD) or other optical disk storage, magnetic cassettes, magnetic tape, magnetic disk\nstorage or other magnetic storage devices, or any other medium capable of storing computer-readable instructions.\nThe computer 900 may include or have access to a computing environment that includes an input interface 925, an output interface 930, and a communication interface 935.  The output interface 930 may interface to or include a display device, such\nas a touchscreen, that also may serve as an input device.  The input interface 925 may interface to or include one or more of a touchscreen, a touchpad, a mouse, a keyboard, a camera, one or more device-specific buttons, one or more sensors integrated\nwithin or coupled via wired or wireless data connections to the computer 900, and other input devices.  The computer 900 may operate in a networked environment using the communication interface 935 to connect to one or more remote computers, such as\ndatabase servers.  The remote computer may include a personal computer (PC), server, router, network PC, peer device or other common network node, or the like.  The communication interface 935 may connect to a local-area network (LAN), a wide-area\nnetwork (WAN), a cellular network, a WiFi network, a Bluetooth network, or other networks.\nComputer-readable instructions stored on a computer-readable medium (e.g., the program 955 stored in the memory storage 910) are executable by the processor 905 of the computer 900.  A hard drive, CD-ROM, and RAM are some examples of articles\nincluding a non-transitory computer-readable medium such as a storage device.  The terms \"computer-readable medium\" and \"storage device\" do not include carrier waves to the extent that carrier waves are deemed too transitory.  \"Computer-readable\nnon-transitory media\" includes all types of computer-readable media, including magnetic storage media, optical storage media, flash media, and solid-state storage media.  It should be understood that software can be installed in and sold with a computer. Alternatively, the software can be obtained and loaded into the computer, including obtaining the software through a physical medium or distribution system, including, for example, from a server owned by the software creator or from a server not owned\nbut used by the software creator.  The software can be stored on a server for distribution over the Internet, for example.\nThe program 955 is shown as including a factor graph composition module 960 and a resource allocation module 965.  Any one or more of the modules described herein may be implemented using hardware (e.g., a processor of a machine, an ASIC, an\nFPGA, or any suitable combination thereof).  Moreover, any two or more of these modules may be combined into a single module, and the functions described herein for a single module may be subdivided among multiple modules.  Furthermore, according to\nvarious example embodiments, modules described herein as being implemented within a single machine, database, or device may be distributed across multiple machines, databases, or devices.\nThe factor graph composition module 960 is configured to compose factor graphs.  For example, the communication interface 935 may receive data defining component factor graphs and a composition script containing instructions for composing the\ncomponent factor graphs.  The factor graph composition module 960 generates one or more composite factor graphs from the component factor graphs based on the composition script.\nThe resource allocation module 965 is configured to allocate resources in a network computing environment.  For example, CPU time, memory, disk storage, network bandwidth, and other computing resources may be allocated to tasks, processes, or\nthreads.  In some example embodiments, the resource allocation module 965 allocates resources based on factor graphs (e.g., the one or more composite factor graphs generated by the factor graph composition module 960).\nFIG. 10 is a flowchart illustration of a method 1000 of a mechanism for programmable composition of factor graphs, according to some example embodiments.  The method 1000 includes operations 1010, 1020, 1030, and 1040.  By way of example and not\nlimitation, the method 1000 is described as being performed by elements of the computer 900, described above with respect to FIG. 9.\nIn operation 1010, the factor graph composition module 960 accesses a first factor graph comprising a first set of nodes.  For example, the factor graph 330 of FIG. 3 may be accessed by reading a representation of the factor graph 330 from the\nnon-removable storage 920 into the memory storage 910.\nIn operation 1020, the factor graph composition module 960 accesses a second factor graph comprising a second set of nodes.  For example, the factor graph 332 of FIG. 3 may be accessed.\nIn operation 1030, the factor graph composition module 960 determines a third set of nodes that are present in both the first set of nodes and the second set of nodes.  For example, each node in the first set of nodes may be compared to each\nnode in the second set of nodes to determine if the node of the first set of nodes has the same label as the node of the second set of nodes.  If a node with a matching label is found, a node with that label is added to the third set of nodes.\nIn operation 1040, the factor graph composition module 960 joins the first factor graph and the second factor graph into a third factor graph, unifying the third set of nodes in the first factor graph and the second factor graph in the process. \nFor example, the factor graphs 330 and 332 of FIG. 3 may be joined into the factor graph 338.  The factor graph 338 includes the nodes 308, 310, and 320 as well as a node unifying the nodes 312 and 318.\nFIG. 11 is a flowchart illustration of a method 1100 for identifying a set of interface nodes, according to some example embodiments.  The method 1100 includes operations 1110, 1120, 1130, and 1140.  By way of example and not limitation, the\nmethod 1100 is described as being performed by elements of the computer 900, described above with respect to FIG. 9.\nIn operation 1110, the factor graph composition module 960 determines, as a third set of nodes, an intersection between a set of allowable interface nodes and a first set of nodes of a first factor graph.  For example, the factor graph\ncomposition module 960 may have received an identifier of a first factor graph, an identifier of a second factor graph, and a set of allowable interface nodes as parameters to a function call that returns a set of interface nodes between the first factor\ngraph and the second factor graph, wherein the returned set of interface nodes is limited to the nodes in the set of allowable interface nodes.  Based on these input parameters, the third set of nodes comprising the intersection between the set of\nallowable interface nodes and the first set of nodes of the first factor graph is determined.\nIn operation 1120, the factor graph composition module 960 accesses a second set of nodes for a second factor graph.  For example, the second set of nodes for the second factor graph may have been passed as a parameter to a function of the\nfactor graph composition module 960.\nIn operation 1130, the factor graph composition module 960 iterates over each node in the intersection between the second set of nodes and the third set of nodes and, if the node has a single factor parent in the second factor graph, adds the\nnode to a set of interface nodes.\nIn operation 1140, the factor graph composition module 960 considers each pair of nodes in the set of interface nodes and, if there is a path between the pair of nodes in the second factor graph, removes the pair of nodes from the set of\ninterface nodes.  In this way, no loops (wherein a node is a direct or indirect parent or child of itself) are created when the first factor graph and the second factor graph are joined using the resulting set of interface nodes.  After each pair of\nnodes in the set of interface nodes has been considered, the remaining set of interface nodes may be returned by the factor graph composition module 960.\nIn some example embodiments, the method 1100 performs a method of finding interface nodes.  The method 1100 may be used as part of a join operation, described by the pseudo-code below.\nTABLE-US-00004 // output graph GZ = join of input graphs GX and GY, with permissible interface // nodes defined by the set Nodes GZ = join(GX, GY, Nodes) // find the interface nodes, e.g., by the method 1100 C = find_interface_nodes(GX, GY,\nNodes) // if there are no interface nodes, return an empty graph If C is empty, return { } // consider each node in the interface nodes For each node Ci in C Remove the factor parent fi of Ci in GY Copy Ci into GZ parent(Ci in GZ) = parent(Ci in GX)\nchildren(Ci in GZ) = children (Ci in GX) and children (Ci in GY) Copy GX without C into GZ Copy GY without C into GZ\nThe method above can be used to join an arbitrary number of factor graphs, as described by the pseudo-code below:\nTABLE-US-00005 G0 = join(G1, ..., Gn, Nodes) // create a stack of factor graphs, GS GS = [G1, ..., Gn] // merge the factor graphs one at a time until all factor graphs have been merged While length(GS) &gt; 1 GZ = join(pop(GS), pop(GS), Nodes)\npush(GZ, GS) // return the fully-merged factor graph Return pop(GS)\nFIG. 12 is a flowchart illustration of a method 1200 of a mechanism for programmable composition of factor graphs, according to some example embodiments.  The method 1200 includes operations 1210 and 1220.  By way of example and not limitation,\nthe method 1200 is described as being performed by elements of the computer 900, described above with respect to FIG. 9.  The method 1200 may implement a select operator that takes three parameters: a factor graph, a factor, and a map function.  The\nselect operator may return a new factor graph.\nIn operation 1210, the factor graph composition module 960 accesses a factor graph, a factor to be updated, and a map function.  For example, the factor graph, the factor to be updated, and the map function may be passed as parameters to a\nselect operator.  The map function is a function that maps a first matrix to a second matrix, where the two matrices have the same dimensions.\nIn operation 1220, the factor graph composition module 960 generates a new factor graph that includes a copy of the updated factor modified using the map function.  For example, the factor graph may be copied into a new factor graph.  The map\nfunction is performed on a matrix of the copied factor to be updated and the output matrix of the map function used to replace the matrix of the factor in the new factor graph.  For example, a map function that operates on a 3.times.3 matrix and keeps\nonly terms on the major diagonal is shown below:\n.function.  ##EQU00002##\nFIG. 13 is a flowchart illustration of a method 1300 of a mechanism for programmable composition of factor graphs, according to some example embodiments.  The method 1300 includes operations 1310, 1320, 1330, 1340, 1350, and 1360.  By way of\nexample and not limitation, the method 1300 is described as being performed by elements of the computer 900, described above with respect to FIG. 9.  The method 1300 may implement a project operator that takes three parameters: a factor graph, a node,\nand a list of columns.  The list of columns includes at least two columns, but fewer columns than a number of values currently available to the node.  The project operator may return a new factor graph in which the node can have only values corresponding\nto the columns in the list of columns.\nIn operation 1310, the factor graph composition module 960 accesses a factor graph, a node in the factor graph, and a list of allowable columns for factor matrices associated with the node.  For example, the factor graph, the node, and the list\nmay be sent as parameters to a project operator.\nIn operation 1320, the factor graph composition module 960 copies the accessed factor graph into a second factor graph.\nThe factor graph composition module 960 accesses, in the second factor graph, a factor parent of the node (operation 1330).  The factor parent is associated with a probability matrix.  For example, the probability matrix may be used in the\nsecond factor graph to determine the probability of each possible value of the node based on the value or values of node parents of the factor parent.\nIn operation 1340, the factor graph composition module 960 removes columns from the probability matrix that are not in the list of allowable columns.  For example, if the probability matrix initially is\n##EQU00003## and the list of allowable columns is [c.sub.1, c.sub.2], the resulting probability matrix excludes the third column:\n##EQU00004## Thus, the possibility of the node taking on the value previously represented by the third column is eliminated.\nIn operation 1350, the factor graph composition module 960 normalizes the probability matrix.  A normalized probability matrix is one in which the sum of values in each row of the matrix is 1.  For example, if the probability matrix, after\nremoving any columns not in the allowable columns list, is\n##EQU00005## the normalized probability matrix is\n.SIGMA..SIGMA..SIGMA..SIGMA..SIGMA..SIGMA.  ##EQU00006##\nIn operation 1360, the factor graph composition module 960 removes rows from child factors of the node that do not correspond to elements in the list of allowable columns.  This reflects the fact that the node can no longer take on the values\nthat were previously used with the removed rows.\nFIG. 14 is a flowchart illustration of a method 1400 of a mechanism for programmable composition of factor graphs, according to some example embodiments.  The method 1400 includes operations 1410, 1420, 1430, 1440, and 1450.  By way of example\nand not limitation, the method 1400 is described as being performed by elements of the computer 900, described above with respect to FIG. 9.  The method 1400 may implement a reduce operator that takes two parameters: a factor graph and a list of factors\nin the factor graph.  The reduce operator may return a new factor graph in which the listed factors have been combined into a single factor.\nIn operation 1410, the factor graph composition module 960 accesses a factor graph, a first factor in the factor graph, and a second factor in the factor graph.  The factor graph, the first factor, and the second factor may have been passed as\nparameters to a reduce operator.  In some example embodiments, the first factor and the second factor are part of a list of factors.\nIn operation 1420, the factor graph composition module 960 copies the accessed factor graph into a second factor graph.\nIn operation 1430, the factor graph composition module 960 removes the first factor and the second factor from the second factor graph, along with all edges connected to the removed factors.\nIn operation 1440, the factor graph composition module 960 adds a factor to the second factor graph, the matrix of the added factor being based on matrices of the first factor and the second factor.  For example, the matrix of the first factor\nmay be multiplied by the matrix of the second factor to generate the matrix of the added factor.\nIn operation 1450, the factor graph composition module 960 adds edges to the second factor graph that connect the added factor to the nodes previously connected to either or both of the first factor and the second factor.  Nodes that were\nchildren of either factor become children of the added factor.  Nodes that were otherwise connected to one or both of the first and second factors become parents of the added factor.\nFIG. 15 is a flowchart illustration of a method 1500 of deploying a cloud application to computing resources, according to some example embodiments.  The method 1500 includes operations 1510, 1520, and 1530.  By way of example and not\nlimitation, the method 1500 is described as being performed by elements of the computer 900, described above with respect to FIG. 9.\nIn operation 1510, the resource allocation module 965 accesses a composite factor graph that represents a cloud application.  For example, a composite factor graph generated using one or more of the methods 1000, 1100, 1200, 1300, or 1400 may be\naccessed.\nIn operation 1520, the resource allocation module 965 selects one or more computing resources, based on the composite factor graph.  For example, the composite factor graphs of the block diagram illustrations 700B and 800B generate probabilities\nof resource usages for CPU, memory, storage, and network bandwidth (e.g., the nodes 738-744 or the nodes 842-848) under various conditions of workload, capacity, and priority (e.g., the nodes 718, 720, 722, 732, 736, 820, 822, 824, 838, and 840). \nApplying a workload and capacity associated with the computing resources and a priority associated with the cloud application, a probability distribution of resource consumption is generated for the cloud application if deployed to each of the computing\nresources.  Using the probability distributions, the computing resources causing the lowest expected value of resource consumption may be selected.\nIn operation 1530, the resource allocation module 965 deploys at least a portion of the cloud application to the selected computing resources.  For example, one component of the cloud application may be deployed to a selected server.  As another\nexample, CPU time, memory resources, storage resources, and network communication resources may be assigned for use to the cloud application.  Additionally or alternatively, executable code for one or more components of the cloud application may be\ncopied to one or more of the allocated computing resources, one or more executable applications may be started in one or more servers of a data center, or any suitable combination thereof.\nFIG. 16 is a flowchart illustration of a method 1600 of deploying a cloud application to a data center, according to some example embodiments.  The method 1600 includes operations 1610, 1620, 1630, and 1640.  By way of example and not\nlimitation, the method 1600 is described as being performed by elements of the computer 900, described above with respect to FIG. 9.\nIn operation 1610, the resource allocation module 965 accesses a composite factor graph that represents a cloud application.  For example, a composite factor graph generated using one or more of the methods 1000, 1100, 1200, 1300, or 1400 may be\naccessed.\nIn operation 1620, the resource allocation module 965 accesses characteristics of a plurality of data centers.  For example, data for a set of available CPUs, a set of available memories, a set of available storage devices, and a set of\navailable network communication devices for each data center may be accessed.  As another example, data for a current or predicted workload and capacity for each data center may be accessed.\nIn operation 1630, the resource allocation module 965 selects a data center of the plurality of data centers, based on the composite factor graph and the characteristics.  For example, the composite factor graphs of the block diagram\nillustrations 700B and 800B generate probabilities of resource usages for CPU, memory, storage, and network bandwidth (e.g., the nodes 738-744 or the nodes 842-848) under various conditions of workload, capacity, and priority (e.g., the nodes 718, 720,\n722, 732, 736, 820, 822, 824, 838, and 840).  Applying a workload and capacity associated with each data center and a priority associated with the cloud application, a probability distribution of resource consumption is generated for the cloud\napplication in each data center.  Using the probability distributions, the data center causing the lowest expected value of resource consumption may be selected.\nIn operation 1640, the resource allocation module 965 deploys the cloud application to the selected data center.  For example, CPU time, memory resources, storage resources, and network communication resources may be assigned for use to the\ncloud application.  Additionally or alternatively, executable code for the cloud application may be copied to one or more of the allocated resources, one or more executable applications may be started in one or more servers of the data center, or any\nsuitable combination thereof.\nFIG. 17 is a flowchart illustration of a method 1700 of deploying a cloud application to servers of a data center, according to some example embodiments.  The method 1700 includes operations 1710, 1720, 1730, and 1740.  By way of example and not\nlimitation, the method 1700 is described as being performed by elements of the computer 900, described above with respect to FIG. 9.\nIn operation 1710, the resource allocation module 965 accesses a composite factor graph that represents a cloud application.  For example, a composite factor graph generated using one or more of the methods 1000, 1100, 1200, 1300, or 1400 may be\naccessed.\nIn operation 1720, the resource allocation module 965 accesses characteristics of a plurality of servers of a data center.  For example, each server in the data center may have CPU characteristics (e.g., number of processors, number of cores,\nprocessor speed, or any suitable combination thereof), memory characteristics (e.g., amount of memory, access speed, data transfer rate, or any suitable combination thereof), storage characteristics (e.g., amount of storage, access speed, data transfer\nrate, or any suitable combination thereof), and network characteristics (e.g., network response time, data transfer rate, or any suitable combination thereof).  As another example, data for a current or predicted workload and capacity for each server may\nbe accessed.\nIn operation 1730, the resource allocation module 965 selects one or more servers of the plurality of servers, based on the composite factor graph and the characteristics.  For example, the composite factor graphs of the block diagram\nillustrations 700B and 800B generate probabilities of resource usages for CPU, memory, storage, and network bandwidth (e.g., the nodes 738-744 or the nodes 842-848) under various conditions of workload, capacity, and priority (e.g., the nodes 718, 720,\n722, 732, 736, 820, 822, 824, 838, and 840).  Applying a workload and capacity associated with each server and a priority associated with the cloud application, a probability distribution of resource consumption is generated for the cloud application on\neach server.  Using the probability distributions, the server or set of servers causing the lowest expected value of resource consumption may be selected.  For example, a different server may be selected for each component of the cloud application.\nIn operation 1740, the resource allocation module 965 deploys the cloud application to the selected servers.  For example, executable code for the cloud application may be copied to the allocated servers and execution of the components of the\ncloud application begun.\nFIG. 18 is a flowchart illustration of a method 1800 of reallocating resources for a cloud application in a data center, according to some example embodiments.  The method 1800 includes operations 1810, 1820, and 1830.  By way of example and not\nlimitation, the method 1800 is described as being performed by elements of the computer 900, described above with respect to FIG. 9.\nIn operation 1810, the resource allocation module 965 determines that allocated resources for a cloud application running in a data center do not match the current needs of the cloud application.  For example, a cloud application may be\nassociated with a service level agreement that provides that the average response time of the cloud application must be below 100 ms or else the data center provider is subject to a financial penalty.  Accordingly, a monitoring application may\nperiodically check the responsiveness of the cloud application to determine the average response time.  When the average response time exceeds a predetermined threshold (e.g., the agreed response time of the service level agreement, a fraction (e.g.,\n90%) of the agreed response time of the service level agreement, or another threshold), the monitoring application triggers the remaining operations of the method 1800.\nIn operation 1820, the resource allocation module 965 accesses a composite factor graph that represents the cloud application.  For example, a composite factor graph generated using one or more of the methods 1000, 1100, 1200, 1300, or 1400 may\nbe accessed from a database of composite factor graphs representing cloud applications running in the data center.\nIn operation 1830, the resource allocation module 965 changes the resource allocation of the cloud application, based on the composite factor graph.  For example, operations 1720-1740 of the method 1700 may be performed to identify one or more\nservers to deploy the cloud application to and the cloud application may be deployed to the identified servers.  In some example embodiments, the changing of the resource allocation results in more or fewer resources being allocated to the cloud\napplication without changing servers on which cloud application components are run.  For example, network resources and storage resources may be allocated or deallocated without stopping and restarting applications.  In other example embodiments, the\nchanging of the resource allocation results in migration of one or more cloud application components (e.g., virtual machines) from one server to another.\nDevices and methods disclosed herein may reduce time, processor cycles, and power consumed in allocating resources to clients.  Devices and methods disclosed herein may also result in improved allocation of resources to clients, resulting in\nimproved throughput and quality of service.  Existing deployments may already use TOSCA or Heat for cloud application deployments.  Reusing TOSCA/Heat for generating composite factor graphs may result in efficiencies.\nAlthough a few embodiments have been described in detail above, other modifications are possible.  For example, the logic flows depicted in the figures do not require the particular order shown, or sequential order, to achieve desirable results. Other steps may be provided in, or steps may be eliminated from, the described flows, and other components may be added to, or removed from, the described systems.  Other embodiments may be within the scope of the following claims.", "application_number": "15596787", "abstract": " A computer-implemented method of deploying a cloud application comprises\n     accessing, by one or more processors, a first factor graph that\n     represents a first component of the cloud application, the first factor\n     graph comprising a first set of nodes; accessing a second factor graph\n     that represents a second component of the cloud application, the second\n     factor graph comprising a second set of nodes; determining a third set of\n     nodes that are present in both the first set of nodes and the second set\n     of nodes; joining, by the one or more processors, the first factor graph\n     and the second factor graph into a third factor graph, wherein the\n     joining includes unifying the third set of nodes in the first factor\n     graph and the second factor graph; based on the third factor graph,\n     selecting computing resources; and deploying at least a portion of the\n     cloud application to the selected computing resources.\n", "citations": ["8806487", "9070047", "20030229697", "20120304170"], "related": []}, {"id": "20180359562", "patent_code": "10375474", "patent_name": "Hybrid horn microphone", "year": "2019", "inventor_and_country_data": " Inventors: \nSkramstad; Rune (Drammen, NO), Sun; Haohai (Nesbru, NO)  ", "description": "<BR><BR>TECHNICAL FIELD\nThis present disclosure relates generally to microphones, and more particularly to a horn microphone utilizing beamforming signal processing.\n<BR><BR>BACKGROUND\nA Microphone converts air pressure variations of a sound wave into an electrical signal.  A variety of methods may be used to convert a sound wave into an electrical signal, such as use of a coil of wire with a diaphragm suspended in a magnetic\nfield, use of a vibrating diaphragm as a capacitor plate, use of a crystal of piezoelectric material, or use of a permanently charged material.  Conventional microphones may sense sound waves from all directions (e.g. omni microphone), in a 3D axis\nsymmetric figure of eight pattern (e.g. dipole microphone), or primarily in one direction with a fairly large pickup pattern (e.g. cardioid, super cardioid and hyper cardioid microphones).\nIn audio and video conferencing applications involving multiple participants in a given location, uni-directional microphones are undesired.  In addition, participants desire speech intelligibility and sound quality without requiring a multitude\nof microphones placed throughout a conference room.  Placing a plurality of microphones in varying locations within a room requires among other things, lengthy cables, cable management, and additional hardware.\nFurther, conventional microphone arrays require sophisticated and costly hardware, significant computing performance, complex processing, and may nonetheless lack adequate sound quality when compared to use of multiple microphones placed\nthroughout a room.  Moreover, conventional microphone arrays may experience processing artifacts caused by high-frequency spatial aliasing issues. <BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nThe embodiments herein may be better understood by referring to the following description in conjunction with the accompanying drawings in which like reference numerals indicate identical or functionally similar elements.  Understanding that\nthese drawings depict only exemplary embodiments of the disclosure and are not therefore to be considered to be limiting of its scope, the principles herein are described and explained with additional specificity and detail through the use of the\naccompanying drawings in which:\nFIG. 1 is a top view of a hybrid horn microphone, in accordance with various aspects of the subject technology.\nFIG. 2 is a front view of a hybrid horn microphone, in accordance with various aspects of the subject technology.\nFIG. 3 is a perspective view of a hybrid horn microphone array, in accordance with various aspects of the subject technology.\nFIG. 4 depicts a hybrid horn microphone array processing block diagram, in accordance with various aspects of the subject technology.\nFIG. 5 depicts an example method for processing signals representing sound waves, in accordance with various aspects of the subject technology.\n<BR><BR>DESCRIPTION OF EXAMPLE EMBODIMENTS\nThe detailed description set forth below is intended as a description of various configurations of embodiments and is not intended to represent the only configurations in which the subject matter of this disclosure can be practiced.  The\nappended drawings are incorporated herein and constitute a part of the detailed description.  The detailed description includes specific details for the purpose of providing a more thorough understanding of the subject matter of this disclosure. \nHowever, it will be clear and apparent that the subject matter of this disclosure is not limited to the specific details set forth herein and may be practiced without these details.  In some instances, structures and components are shown in block diagram\nform in order to avoid obscuring the concepts of the subject matter of this disclosure.\n<BR><BR>Overview\nConventional microphones may sense sound waves from all directions (e.g. omni microphone), in a 3D axis symmetric figure of eight pattern (e.g. dipole microphone), or primarily in one direction with a fairly large pickup pattern (e.g. cardioid,\nsuper cardioid and hyper cardioid microphones).  In applications where sensing of sound from various locations may be required, an array of microphones may be positioned in a central location, such as on the middle of a table in a room.  Conventional\nmicrophone arrays require sophisticated and costly hardware, significant computing performance, complex processing, and may lack adequate sound quality when compared to use of multiple microphones placed throughout a room or assigned to individual\nparticipants or users.  In addition, conventional microphone arrays may have a shorter critical distance, that is, the distance in which the microphone array may adequately sense sound due to the sound pressure level of the direct sound and the\nreverberant sound being equal when dealing with a directional source, when compared to the hybrid horn microphone of the subject technology.  Moreover, a conventional microphone array may experience processing artifacts caused by high-frequency spatial\naliasing issues.\nThe disclosed technology addresses the need in the art for providing a high-sensitive and anti-aliasing microphone by combining horn technology and beamforming signal processing.  In an array configuration, the hybrid horn microphone of the\nsubject technology requires less processing power compared to conventional microphone arrays.  In addition, the hybrid microphone of the subject technology has a higher signal to noise ratio and less high frequency spatial-aliasing issues than other\nimplementations.  The hybrid horn microphone array of the subject technology also has a longer critical distance and increased sound quality compared to conventional microphone arrays.\nIn addition, the hybrid horn microphone array of the subject technology does not require multiple arrays, may utilize a single output cable, and may be installed in a single location in a room, such as on or near the ceiling.  There is no need\nfor multiple microphones to be located, installed and wired throughout a room.  Further, users do not need to reposition table microphones to improve sound quality as the subject technology is capable of processing audio signals to create high quality\nsound.\n<BR><BR>DETAILED DESCRIPTION\nVarious aspects of the disclosure are discussed in detail below.  While specific implementations are discussed, it should be understood that this is done for illustration purposes only.  A person skilled in the relevant art will recognize that\nother components and configurations may be used without parting from the spirit and scope of the disclosure.\nFIG. 1 is a top view of a hybrid horn microphone 100, in accordance with various aspects of the subject technology.  Microphone 100 comprises a horn portion that is formed by a plurality of planar surfaces 110A-E. The planar surfaces 110A-E are\narranged in a converging orientation to form a shape having a first opening on a proximal end and a second opening on a distal end, the second opening at the distal end being smaller in area than the first opening at the proximal end.\nThe plurality of planar surfaces 110 may be substantially planar and devoid of curvature such that a cross-sectional area of the horn portion from the proximal end to the distal end decreases at a constant rate.  In some aspects, the planar\nsurfaces may include curvature such that the cross-sectional area of the horn portion from the proximal end to the distal end decreases with varying rates.\nThe plurality of planar surfaces 110 may be made of polymer, composite, metal, alloys, or a combination thereof.  It is understood that other materials may be used to form the horn portion without deviating from the scope of the subject\ntechnology.\nEach planar surface 110 of the plurality of planar surfaces 110A-E may have substantially the same thickness.  The thickness of each planar surface 110 may be 0.13'', 0.25'', 0.38'', or 0.5''.  It is understood that the planar surfaces 110 may\nhave other values for thickness without departing from the scope of the subject technology.\nIn some aspects, the length of the planar surface 110 may range from 4-6 inches, 6-8 inches, 8-10 inches, 10-12 inches or 12-14 inches.  It is understood that the planar surface 110 may have a longer length without departing from the scope of\nthe subject technology.  In one aspect, a width of the planar surface is similar to the length of the planar surface.\nIn one aspect, the horn portion may be formed by a single component, folded, cast, or molded into the desired shape.  For example, the horn portion may comprise sheet metal folded into a pentagonal pyramid having five planar surfaces 110A-E. In\nanother aspect, the horn portion may be assembled from multiple components with each component comprising the planar surface 110.\nFIG. 2 is a front view of the hybrid horn microphone 100, in accordance with various aspects of the subject technology.  The microphone 100 includes an instrument 120 disposed at the distal end of the horn portion 105.  The distal end is located\nwhere the planar surfaces 110A-E converge to form a narrow opening.  The instrument 120 is configured to detect sound waves and convert air pressure variations of a sound wave into an electrical signal.  The instrument 120 may comprise an electret\nmicrophone.  An electret microphone is a type of electrostatic capacitor-based microphone.\nSound waves emitted by a source, such as a user speaking at a telephonic or video conference, are directed or reflected towards the horn portion 105 and are directed to the instrument 120 by the shape of the planar surfaces 110A-E. In one\naspect, the size and shape of the horn portion 105 correlates to a frequency range or bandwidth of the sound waves desired for detection.\nIn another aspect, by utilizing the horn portion 105, the microphone 100 detects and senses sound waves directionally.  That is, the microphone 100 is capable of detecting sound waves from a source located within a detection range 115, while\nminimizing detection of sound waves from other sources that may be located at different locations from the source, outside of the detection range 115.  By utilizing the horn portion 105, the microphone 100 is also able to prevent detection of ambient\nnoise (typically greater than 10 dB) coming from sources located outside of the detection range.  In one aspect, the horn portion 105 of the microphone 100 significantly reduces detection of sound waves coming from angles outside of the direction of the\nmicrophone 100 because the sound waves from outside the direction of the microphone 100 are reflected away from the instrument 120 by the horn portion 105.  In another aspect, for sound waves coming from a source located within the detection range 115 of\nthe microphone 100, a Signal to Noise Ratio (SNR) of the sound wave is significantly higher (generally 9 dB or more) than conventional microphones resulting in increased sound quality.  In one aspect, for sound waves coming from a source within the\ndetection range 115, the microphone 100 has a very high directivity at frequencies above 2 kHz.\nIn some aspects, the horn portion 105 may have various shapes formed by the planar surfaces 110.  For example, the shape of the horn portion 105 formed by the plurality of planar surfaces 110 may comprise a triangular pyramid having three\ninterior faces.  In another example, the shape of the horn portion 105 formed by the plurality of planar surfaces 110 may comprise a square pyramid having four interior faces.  In yet another example, the shape of the horn portion 105 formed by the\nplurality of planar surfaces 110 may comprise a pentagonal pyramid having five interior faces.  In another example, the shape of the horn portion 105 formed by the plurality of planar surfaces 110 may comprise a hexagonal pyramid having six interior\nfaces.  In yet another example, the shape of the horn portion 105 formed by the plurality of planar surfaces 110 may comprise a heptagonal pyramid having seven interior faces.  In another example, the shape of the horn portion 105 formed by the plurality\nof planar surfaces 110 may comprise an octagonal pyramid having eight interior faces.  It is further understood that other shapes may be formed by the plurality of planar surfaces 110 as desired by a person of ordinary skill in the art.\nFIG. 3 is a perspective view of a hybrid horn microphone array 300, in accordance with various aspects of the subject technology.  In some aspects, the horn microphone 100 may be arranged in an array 300 to receive sound waves from one or more\nsources located within an area, such as a conference room.  For example, the array 300 of microphones 100 may be arranged to form a polyhedron shape, such as a full dodecahedron that may be formed by arranging twelve microphones 100 into a full sphere\ndodecahedron arrangement.  In another example, the polyhedron shape may comprise a half dodecahedron that may be formed by arranging six microphones 100 into a half dodecahedron arrangement (as shown in FIG. 3).  In yet another example, the polyhedron\nshape may comprise a quarter dodecahedron formed by arranging three microphones 100 into a quarter dodecahedron arrangement.  It is understood that the array 300 may comprise other shapes and may be formed of a multitude of microphones 100, including up\nto 120 microphones 100.  In one aspect, the higher the number of microphones 100 comprising the array, the narrower the detection of sound waves from the source.\nEach microphone 100 of the array 300 is pointed at a different direction, as shown in FIG. 3.  In some aspects, by forming the array 300 with the plurality of microphones 100 arranged so that each microphone 100 is pointed at a different\ndirection, each microphone 100 is configured to detect sound waves from the direction the microphone is pointed.\nFIG. 4 depicts a hybrid horn microphone array processing block diagram 400, in accordance with various aspects of the subject technology.  The microphone array 300 (shown in FIG. 3) may further comprise the hybrid horn microphone array\nprocessing block diagram 400 to process the electrical signals generated by the instrument 120 (shown in FIGS. 1 and 2) of each microphone 100.  In one aspect, the functions and operations depicted in the hybrid horn microphone array processing block\ndiagram 400 may be performed by components mounted to the array 300, components located at a remote location, or at an output device as discussed further below.\nThe hybrid horn microphone array processing block diagram 400 comprises a beamforming signal processing circuit 405 for creating a high-sensitivity and anti-aliasing microphone array 300.  The beamforming signal processing circuit 405 is\nelectrically coupled to each microphone 100 and is configured to receive the electrical signals from each instrument 120.  The beamforming signal processing circuit 405 is further configured to create beam signals corresponding to each microphone 100\nbased on the respective electrical signals.  In some aspects, the beam signals are indicative of a location of a source of the sound waves detected by each microphone 100.\nThe beamforming signal processing circuit 405 comprises a crossover filter 410, a delaying circuit 420, a processor 430, and a mixer 440.  Each electrical signal from the microphones 100A-N passes through respective cross over filters 410A-N.\nEach crossover filter 410A-N is configured to convert the respective electrical signals from the microphone 100A-N to a first signal 412 and a second signal 414, with the first and second signals, 412 and 414 respectively, having different frequencies or\nsub-bands.  For example, the frequency of each respective first signal 412 may be below 2 kHz and the frequency of each respective second signal 414 may be above 2 kHz.  In one aspect, the crossover frequency can be adapted to the size of the horn\nportion 105 (as shown in FIG. 2) of the microphone 100 in the array 300.\nFor example, with reference to a first microphone 100A, the electrical signal from the microphone 100A is received by the cross over filter 410A.  The cross over filter 410A converts the electrical signal from the microphone 100A into a first\nsignal 412A (Low Frequency or LF) and a second signal 414A (High Frequency or HF).  With reference to a second microphone 100B, the electrical signal from the microphone 100B is received by the cross over filter 410B.  The cross over filter 410B converts\nthe electrical signal from the microphone 100B into a first signal 412B (Low Frequency or LF) and a second signal 414B (High Frequency or HF).  With reference to a third microphone 100C, the electrical signal from the microphone 100C is received by the\ncross over filter 410C.  The cross over filter 410C converts the electrical signal from the microphone 100C into a first signal 412C (Low Frequency or LF) and a second signal 414C (High Frequency or HF).  With reference to a fourth microphone 100D, the\nelectrical signal from the microphone 100D is received by the cross over filter 410D.  The cross over filter 410D converts the electrical signal from the microphone 100D into a first signal 412D (Low Frequency or LF) and a second signal 414D (High\nFrequency or HF).  With reference to a fifth microphone 100E, the electrical signal from the microphone 100E is received by the cross over filter 410E.  The cross over filter 410E converts the electrical signal from the microphone 100E into a first\nsignal 412E (Low Frequency or LF) and a second signal 414E (High Frequency or HF).  In some aspects, any number of microphones 100N may be connected to the beamforming signal processing circuit 405, including the cross over filter 410N to convert the\nelectrical signal from the microphone 100N into a first signal 412N and a second signal 414N, without departing from the scope of the subject technology.\nThe delaying circuit 420 is configured to delay the second signal 414 from the crossover filter 410 to create a delayed second signal 422.  In some aspects, the delaying circuit is configured to sufficiently delay the second signal 414 so that\nupon mixing by the mixer 440, as discussed further below, the mixed signal is sufficiently aligned.  Each second signal 414A-N from the respective cross over filters 410A-N is received by corresponding delaying circuits 420A-N to create respective\ndelayed second signals 422A-N.\nFor example, with reference to the first microphone 100A, the second signal 414A from the cross over filter 410A is received by the delaying circuit 420A.  The delaying circuit 420A delays the second signal 414A to create a delayed second signal\n422A.  With reference to the second microphone 100B, the second signal 414B from the cross over filter 410B is received by the delaying circuit 420B.  The delaying circuit 420B delays the second signal 414B to create a delayed second signal 422B.  With\nreference to the third microphone 100C, the second signal 414C from the cross over filter 410C is received by the delaying circuit 420C.  The delaying circuit 420C delays the second signal 414C to create a delayed second signal 422C.  With reference to\nthe fourth microphone 100D, the second signal 414D from the cross over filter 410D is received by the delaying circuit 420D.  The delaying circuit 420D delays the second signal 414D to create a delayed second signal 422D.  With reference to the fifth\nmicrophone 100E, the second signal 414E from the cross over filter 410E is received by the delaying circuit 420E.  The delaying circuit 420E delays the second signal 414E to create a delayed second signal 422E.  In some aspects, any number of microphones\n100N may be connected to the beamforming signal processing circuit 405, including the delaying circuit 420N to delay the second signal 414N and create a delayed second signal 422N, without departing from the scope of the subject technology.\nThe processor 430 may be configured to downsample the first signal 412 from the crossover filter 410 to create a downsampled first signal, process the downsampled first signal to create a processed first signal that is indicative of the location\nof the source of the sound waves detected by the microphone 100, and upsample the processed first signal to create an upsampled first signal 432.  Each first signal 412A-N from the respective cross over filters 410A-N is received by the processor 430 to\ncreate the processed first signal 432A-N.\nIn some aspects, the processor 430 utilizes beamforming signal processing techniques to process the first signals 412A-N. Beam forming signal processing may be used to extract sound sources in an area or room.  This may be achieved by combining\nelements in a phased array in such a way that signals at particular angles experience constructive interference while others experience destructive interference.\nIn one aspect, because the horn portion 105 (as shown in FIG. 2) of the microphone 100 significantly reduces detection of sound waves coming from angles outside of the direction of the microphone 100, provides a high SNR for sound waves coming\nfrom a source located within the detection range 115 (as shown in FIG. 2), and provides a very high directivity at frequencies above 2 kHz; no processing is required by the processor 430 for the second signals 414A-N. In one aspect, because no processing\nis required for the second signals 414A-N, spatial aliasing issues are avoided.\nThe processor 430 may downsample each of the first signals 412A-N to a lower sampling rate such as from 48 kHz to 4 kHz, which may significantly reduce computational complexity by 90%.  The processor 430 may then filter and sum (or weight and\nsum in the frequency domain) each of the first signals 412A-N to create respective processed first signals representing acoustic beams pointing in the direction of each respective microphone.  In another example, the processer 430 may use spherical\nharmonics theory or sound field models to create respective processed first signals representing acoustic beams pointing in the direction of each respective microphone.  In one aspect, the processor 430 may measure the array response vectors for various\nsound arrival angles in an anechoic chamber.  In another aspect, the processor 430 may implement various types of beam pattern synthesis/optimization or machine learning.  The processor 430 may then upsample the processed first signals to obtain\nrespective upsampled first signals 432 with a desired sampling rate.\nFor example, with reference to the first microphone 100A, the first signal 412A from the cross over filter 410A is received by the processor 430.  The processor 430 may downsample the first signal 412A to create a first downsampled first signal. The processor 430 may then filter and sum (or weight and sum in the frequency domain) the first downsampled first signal to create a first processed first signal representing an acoustic beam pointing in the direction of microphone 100A.  The first\nprocessed first signal indicative of the location of the source of the sound waves detected by the microphone 100A.  The processor 430 may then upsample the first processed first signal to obtain an upsampled first signal 432A.  With respect to the\nsecond microphone 100B, the first signal 412B from the cross over filter 410B is received by the processor 430.  The processor 430 may downsample the first signal 412B to create a second downsampled first signal.  The processor 430 may then filter and\nsum (or weight and sum in the frequency domain) the second downsampled first signal to create a second processed first signal representing an acoustic beam pointing in the direction of microphone 100B.  The second processed first signal indicative of the\nlocation of the source of the sound waves detected by the microphone 100B.  The processor 430 may then upsample the second processed first signal to obtain an upsampled first signal 432B.  With respect to the third microphone 100C, the first signal 412C\nfrom the cross over filter 410C is received by the processor 430.  The processor 430 may downsample the first signal 412C to create a third downsampled first signal.  The processor 430 may then filter and sum (or weight and sum in the frequency domain)\nthe third downsampled first signal to create a third processed first signal representing an acoustic beam pointing in the direction of microphone 100C.  The third processed first signal indicative of the location of the source of the sound waves detected\nby the microphone 100C.  The processor 430 may then upsample the third processed first signal to obtain an upsampled first signal 432C.  With respect to the fourth microphone 100D, the first signal 412D from the cross over filter 410D is received by the\nprocessor 430.  The processor 430 may downsample the first signal 412D to create a fourth downsampled first signal.  The processor 430 may then filter and sum (or weight and sum in the frequency domain) the fourth downsampled first signal to create a\nfourth processed first signal representing an acoustic beam pointing in the direction of microphone 100D.  The fourth processed first signal indicative of the location of the source of the sound waves detected by the microphone 100D.  The processor 430\nmay then upsample the fourth processed first signal to obtain an upsampled first signal 432D.  With respect to the fifth microphone 100E, the first signal 412E from the cross over filter 410E is received by the processor 430.  The processor 430 may\ndownsample the first signal 412E to create a fifth downsampled first signal.  The processor 430 may then filter and sum (or weight and sum in the frequency domain) the fifth downsampled first signal to create a fifth processed first signal representing\nan acoustic beam pointing in the direction of microphone 100E.  The fifth processed first signal indicative of the location of the source of the sound waves detected by the microphone 100E.  The processor 430 may then upsample the fifth processed first\nsignal to obtain an upsampled first signal 432E.  In some aspects, any number of microphones 100N may be connected to the beamforming signal processing circuit 405, including the processor 430 to downsample, process and upsample the first signal 412N and\ncreate a upsampled first signal 432N, without departing from the scope of the subject technology.\nThe mixer 440 is configured to combine the upsampled first signal 432 from the processor 430 and the delayed second signal 422 from the delaying circuit 420 to create a full-band beam signal 442.  Each upsampled first signal 432A-N and delayed\nsecond signal 422A-N from the respective delaying circuits 420A-N is received by corresponding mixers 440A-N to create respective full-band beam signals 442A-N.\nFor example, with reference to the first microphone 100A, the upsampled first signal 432A from the processor 430 and the delayed second signal 422A from the delaying circuit 420A is received by the mixer 440A.  The mixer 440A combines the\nupsampled first signal 432A and the delayed second signal 422A to create a beam signal 442A.  With reference to the second microphone 100B, the upsampled first signal 432B from the processor 430 and the delayed second signal 422B from the delaying\ncircuit 420B is received by the mixer 440B.  The mixer 440B combines the upsampled first signal 432B and the delayed second signal 422B to create a beam signal 442B.  With reference to the third microphone 100C, the upsampled first signal 432C from the\nprocessor 430 and the delayed second signal 422C from the delaying circuit 420C is received by the mixer 440C.  The mixer 440C combines the upsampled first signal 432C and the delayed second signal 422C to create a beam signal 442C.  With reference to\nthe fourth microphone 100D, the upsampled first signal 432D from the processor 430 and the delayed second signal 422D from the delaying circuit 420D is received by the mixer 440D.  The mixer 440D combines the upsampled first signal 432D and the delayed\nsecond signal 422D to create a beam signal 442D.  With reference to the second microphone 100E, the upsampled first signal 432E from the processor 430 and the delayed second signal 422E from the delaying circuit 420E is received by the mixer 440E.  The\nmixer 440E combines the upsampled first signal 432E and the delayed second signal 422E to create a beam signal 442E.  In some aspects, any number of microphones 100N may be connected to the beamforming signal processing circuit 405, including the mixer\n440N to combine the upsampled first signal 432N and delayed second signal 422N to create the beam signal 442N, without departing from the scope of the subject technology.\nThe hybrid horn microphone array processing block diagram 400 may further comprise an audio processing circuit 450.  The audio processing circuit 450 may be configured to receive each of the beam signals 442A-N and perform at least one of an\necho control filter, a reverberation filter, or a noise reduction filter, to improve the quality of the beam signals 442A-N and create pre-mixed beam signals 452A-N.\nFor example, with reference to the first microphone 100A, the beam signal 442A from the mixer 440A is received by the audio processing circuit 450.  The audio processing circuit 450 performs operations such as echo modification, reverberation\nadjustment, or noise reduction, to improve the quality of the beam signal 442A, and thereby create a pre-mixed beam signal 452A.  With reference to the second microphone 100B, the beam signal 442B from the mixer 440B is received by the audio processing\ncircuit 450.  The audio processing circuit 450 performs operations such as echo modification, reverberation adjustment, or noise reduction, to improve the quality of the beam signal 442B, and thereby create a pre-mixed beam signal 452B.  With reference\nto the third microphone 100C, the beam signal 442C from the mixer 440C is received by the audio processing circuit 450.  The audio processing circuit 450 performs operations such as echo modification, reverberation adjustment, or noise reduction, to\nimprove the quality of the beam signal 442C, and thereby create a pre-mixed beam signal 452C.  With reference to the fourth microphone 100D, the beam signal 442D from the mixer 440D is received by the audio processing circuit 450.  The audio processing\ncircuit 450 performs operations such as echo modification, reverberation adjustment, or noise reduction, to improve the quality of the beam signal 442D, and thereby create a pre-mixed beam signal 452D.  With reference to the fifth microphone 100E, the\nbeam signal 442E from the mixer 440E is received by the audio processing circuit 450.  The audio processing circuit 450 performs operations such as echo modification, reverberation adjustment, or noise reduction, to improve the quality of the beam signal\n442E, and thereby create a pre-mixed beam signal 452E.  In some aspects, any number of microphones 100N may be connected to the audio processing circuit 450 to improve the quality of the beam signal 442N and create pre-mixed beam signal 452N, without\ndeparting from the scope of the subject technology.\nThe hybrid horn microphone array processing block diagram 400 may further comprise an automatic mixer 460.  The automatic mixer 460 may be configured to receive the plurality of pre-mixed beam signals 452A-N and identify one or more beam signals\nfrom the plurality of beam signals 452A-N to output to an output device 470 based on a characteristic of the beam signal 452A-N. The characteristic of the beam signal 452A-N may include, for example, quality, level, clarity, strength, SNR, signal to\nreverberation ratio, amplitude, wavelength, frequency, or phase.  In some aspects, the mixer 460 may be configured to review each incoming pre-mix beam signal 452A-N, identify one or more beam signals 452A-N based on one or more characteristic of the\nbeam signals 452A-N, select the one or more beam signals 452A-N, isolate signals representing speech, filter low signals that may not represent speech, and transmit an output signal 462 to the output device 470.  In one aspect, the mixer 460 may utilize\naudio selection techniques to generate the desired audio output signal 462 (e.g., mono, stereo, surround).\nThe output device 470 is configured to receive the output signal 462 from the mixer and may comprise a set top box, console, visual output device (e.g., monitor, television, display), or audio output device (e.g., speaker).\nFIG. 5 depicts an example method 500 for processing signals representing sound waves, in accordance with various aspects of the subject technology.  It should be understood that, for any process discussed herein, there can be additional, fewer,\nor alternative steps performed in similar or alternative orders, or in parallel, within the scope of the various embodiments unless otherwise stated.\nAt operation 510, a sound wave is received at an array of microphones.  The array of microphones comprise a plurality of microphones arranged in a polyhedron shape, as shown for example, in FIG. 3.  Each microphone may comprise a horn portion\nand an instrument, the instrument configured to generate an electrical signal based on the sound wave.  The horn portion may comprise a plurality of planar surfaces that are arranged to form the polyhedron shape.\nAt operation 520, a plurality of electrical signals are generated based on the received sound wave.  The plurality of electrical signals comprise the electrical signal generated by each instrument of the plurality of microphones.\nAt operation 530, each electrical signal of the plurality of electrical signals is converted into a high sub-band signal and a low sub-band signal.  The electrical signal generated by each instrument and microphone, is thus converted to two\nsignals, the high sub-band signal and the low sub-band signal.  Each of the low-band signals, together, comprise a plurality of low-band signals.  Similarly, each of the high-band signals, together, comprise a plurality of high-band signals.\nAt operation 540, beamforming signal processing is performed on the plurality of low sub-band signals to create a plurality of low sub-band beam signals.  Stated differently, each of the low-band signals undergoes beamforming signal processing\nto thereby create a low sub-band beam signal.  As described above, beamforming signal processing may comprise use of spherical harmonics theory or sound field models, use of array response vectors for various sound arrival angles in an anechoic chamber,\nand/or use of various types of beam pattern synthesis/optimization or machine learning.\nAt operation 550, each low-band beam signal of the plurality of low sub-band signals is combined with the respective high sub-band signal of the plurality of high sub-band signals to create a plurality of beam signals.  Each beam signal of the\nplurality of beam signals corresponds to each microphone of the plurality of microphones of the array.\nAt operation 560, one or more beam signals of the plurality of beam signals is elected for output to an output device.\nThe functions described above can be implemented using computer-executable instructions that are stored or otherwise available from computer readable media.  Such instructions can comprise, for example, instructions and data which cause or\notherwise configure a general purpose computer, special purpose computer, or special purpose processing device to perform a certain function or group of functions.  The computer executable instructions may be, for example, binaries, intermediate format\ninstructions such as assembly language, firmware, or source code.  Examples of computer-readable media that may be used to store instructions, information used, and/or information created during methods according to described examples include magnetic or\noptical disks, flash memory, USB devices provided with non-volatile memory, networked storage devices, and so on.\nDevices implementing the functions and operations according to these disclosures may comprise hardware, firmware and/or software, and can take any of a variety of form factors.  Typical examples of such form factors include laptops, smart\nphones, small form factor personal computers, personal digital assistants, rackmount devices, standalone devices, and so on.  Functionality described herein also can be embodied in peripherals or add-in cards.  Such functionality can also be implemented\non a circuit board among different chips or different processes executing in a single device, by way of further example.\nThe instructions, media for conveying such instructions, computing resources for executing them, and other structures for supporting such computing resources are means for providing the functions described in these disclosures.\nAlthough a variety of examples and other information was used to explain aspects within the scope of the appended claims, no limitation of the claims should be implied based on particular features or arrangements in such examples, as one of\nordinary skill would be able to use these examples to derive a wide variety of implementations.  Further and although some subject matter may have been described in language specific to examples of structural features and/or method steps, it is to be\nunderstood that the subject matter defined in the appended claims is not necessarily limited to these described features or acts.  For example, such functionality can be distributed differently or performed in components other than those identified\nherein.  Rather, the described features and steps are disclosed as examples of components of systems and methods within the scope of the appended claims.", "application_number": "15620169", "abstract": " The disclosed technology relates to a microphone array. The array\n     comprises a plurality of microphones with each microphone having a horn\n     portion. Each microphone of the array further comprises an instrument\n     disposed at a distal end of the horn portion. Each instrument of the\n     array is configured to convert sound waves into an electrical signal. The\n     microphone array further comprises a beamforming signal processing\n     circuit electrically coupled to each instrument and configured to create\n     a plurality of beam signals based on respective electrical signals.\n", "citations": ["4460807", "4890257", "4977605", "5293430", "5694563", "5699082", "5745711", "5767897", "5825858", "5874962", "5889671", "5917537", "5995096", "6023606", "6040817", "6075531", "6085166", "6191807", "6300951", "6392674", "6424370", "6463473", "6553363", "6554433", "6573913", "6646997", "6665396", "6700979", "6711419", "6754321", "6754335", "38609", "6816464", "6865264", "6938208", "6978499", "7046134", "7046794", "7058164", "7058710", "7062532", "7085367", "7124164", "7149499", "7180993", "7209475", "7340151", "7366310", "7418664", "7441198", "7478339", "7500200", "7530022", "7552177", "7577711", "7584258", "7587028", "7606714", "7606862", "7620902", "7634533", "7774407", "7792277", "7830814", "7840013", "7840980", "7881450", "7920160", "7956869", "7986372", "7995464", "8059557", "8081205", "8140973", "8169463", "8219624", "8274893", "8290998", "8301883", "8340268", "8358327", "8423615", "8428234", "8433061", "8434019", "8456507", "8462103", "8478848", "8520370", "8625749", "8630208", "8638354", "8645464", "8675847", "8694587", "8694593", "8706539", "8732149", "8738080", "8751572", "8831505", "8850203", "8860774", "8874644", "8890924", "8892646", "8914444", "8914472", "8924862", "8930840", "8947493", "8972494", "9003445", "9031839", "9032028", "9075572", "9118612", "9131017", "9137376", "9143729", "9165281", "9197701", "9197848", "9201527", "9203875", "9204099", "9219735", "9246855", "9258033", "9268398", "9298342", "9323417", "9335892", "9349119", "9367224", "9369673", "9407621", "9432512", "9449303", "9495664", "9513861", "9516022", "9525711", "9553799", "9563480", "9609030", "9609514", "9614756", "9640194", "9667799", "9674625", "9762709", "20010030661", "20020018051", "20020076003", "20020078153", "20020140736", "20020188522", "20030028647", "20030046421", "20030068087", "20030154250", "20030174826", "20030187800", "20030197739", "20030227423", "20040039909", "20040054885", "20040098456", "20040210637", "20040253991", "20040267938", "20050014490", "20050031136", "20050048916", "20050055405", "20050055412", "20050085243", "20050099492", "20050108328", "20050131774", "20050175208", "20050215229", "20050226511", "20050231588", "20050286711", "20060004911", "20060020697", "20060026255", "20060083305", "20060084471", "20060164552", "20060224430", "20060250987", "20060271624", "20070005752", "20070021973", "20070025576", "20070041366", "20070047707", "20070058842", "20070067387", "20070091831", "20070100986", "20070106747", "20070116225", "20070139626", "20070150453", "20070168444", "20070198637", "20070208590", "20070248244", "20070250567", "20080059986", "20080068447", "20080071868", "20080080532", "20080107255", "20080133663", "20080154863", "20080209452", "20080270211", "20080278894", "20090012963", "20090019374", "20090049151", "20090064245", "20090075633", "20090089822", "20090094088", "20090100142", "20090119373", "20090132949", "20090193327", "20090234667", "20090254619", "20090256901", "20090278851", "20090282104", "20090292999", "20090296908", "20090306981", "20090309846", "20090313334", "20100005142", "20100005402", "20100031192", "20100061538", "20100070640", "20100073454", "20100077109", "20100094867", "20100095327", "20100121959", "20100131856", "20100157978", "20100162170", "20100183179", "20100211872", "20100215334", "20100220615", "20100241691", "20100245535", "20100250817", "20100262266", "20100262925", "20100275164", "20100302033", "20100303227", "20100316207", "20100318399", "20110072037", "20110075830", "20110087745", "20110117535", "20110131498", "20110154427", "20110230209", "20110264928", "20110270609", "20110271211", "20110283226", "20110314139", "20120009890", "20120013704", "20120013768", "20120026279", "20120054288", "20120072364", "20120084714", "20120092436", "20120140970", "20120179502", "20120190386", "20120192075", "20120233020", "20120246229", "20120246596", "20120284635", "20120296957", "20120303476", "20120306757", "20120306993", "20120308202", "20120313971", "20120315011", "20120321058", "20120323645", "20120324512", "20130027425", "20130038675", "20130047093", "20130050398", "20130055112", "20130061054", "20130063542", "20130086633", "20130090065", "20130091205", "20130091440", "20130094647", "20130113602", "20130113827", "20130120522", "20130124551", "20130129252", "20130135837", "20130141371", "20130148789", "20130182063", "20130185672", "20130198629", "20130210496", "20130211826", "20130212202", "20130215215", "20130219278", "20130222246", "20130225080", "20130227433", "20130235866", "20130242030", "20130243213", "20130252669", "20130263020", "20130290421", "20130297704", "20130300637", "20130325970", "20130329865", "20130335507", "20140012990", "20140028781", "20140040404", "20140040819", "20140063174", "20140068452", "20140068670", "20140078182", "20140108486", "20140111597", "20140136630", "20140157338", "20140161243", "20140195557", "20140198175", "20140237371", "20140253671", "20140280595", "20140282213", "20140296112", "20140298210", "20140317561", "20140337840", "20140358264", "20140372908", "20150004571", "20150009278", "20150029301", "20150067552", "20150070835", "20150074189", "20150081885", "20150082350", "20150085060", "20150088575", "20150089393", "20150089394", "20150113050", "20150113369", "20150128068", "20150172120", "20150178626", "20150215365", "20150254760", "20150288774", "20150301691", "20150304120", "20150304366", "20150319113", "20150350126", "20150373063", "20150373414", "20160037304", "20160043986", "20160044159", "20160044380", "20160050079", "20160050160", "20160050175", "20160070758", "20160071056", "20160072862", "20160094593", "20160105345", "20160110056", "20160165056", "20160173537", "20160182580", "20160266609", "20160269411", "20160277461", "20160283909", "20160307165", "20160309037", "20160321347", "20170006162", "20170006446", "20170070706", "20170093874", "20170104961", "20170171260", "20170324850"], "related": []}, {"id": "20180376283", "patent_code": "10375515", "patent_name": "Location application program interface", "year": "2019", "inventor_and_country_data": " Inventors: \nJensen; Bradley Joel (San Francisco, CA), Huang; Ronald K. (San Jose, CA), Rhee; Stephen J. (San Jose, CA), Bruins; Jay N. (Los Altos, CA), Driscoll; Adam M. (Atherton, CA), Marti; Lukas M. (Santa Clara, CA), Dal Santo; Michael P. (San Francisco, CA), Dillon; Patrick Thomas (Mountain View, CA)  ", "description": "<BR><BR>TECHNICAL FIELD\nThis disclosure relates generally to location-based services.\n<BR><BR>BACKGROUND\nMany electronic devices have location-based functions.  For example, a mobile device can estimate a location of the mobile device using a satellite navigation system (e.g., global positioning system or GPS) or a cellular communications system. \nThe mobile device can perform various tasks that are location specific.  For example, a map application executing on the mobile device can cause the mobile device to display a map.  A marker on the map can indicate a current location of the mobile\ndevice.  Upon receiving a user input selecting the marker, the mobile device can display points of interests, e.g., restaurants or gas stations, that are close to the current location.  Upon receiving a user input specifying a destination, the mobile\ndevice can display a route from the current location to the destination, and an estimated time of arrival based on traffic information on the route.\n<BR><BR>SUMMARY\nTechniques for determining a location significant to a user for providing location-based services are described.  A significant user location is a geographic location that is determined to have a significant meaning to a user of a mobile device\nsuch that the user is likely to visit the location in the future.  The mobile device can determine that a geographic location is a significant user location based on how long the user has dwelled at the geographic location.  The length of time for\ndetermining a significant location can be hint based.  A hint can be a historical or present action performed on the mobile device or detected by the mobile device that indicates that the user may have an interest at the location.  Upon detecting a hint,\nthe mobile device can reduce a pre-specified threshold time for determining a significant location.\nTechniques for adaptive location clustering are described.  A mobile device can determine a size of a location cluster indicating a location that is significant to a user.  For a pre-specified period of time, the mobile device can record\nlocations, and determine a convergence rate of the recorded location.  The convergence rate can indicate how quickly the locations are clustered together.  A higher convergence rate corresponds to a smaller size.  The mobile device can measure a\ndeviation over a given convergence rate.  The mobile device can store the location cluster in association with the size.  The mobile device can determine a significant location based on locations in the location cluster and a size of the location\ncluster.\nTechniques for determining a location of a calendar item are described.  A mobile device can receive a calendar item including a description and a time.  The mobile device can determine that, at the time specified in the calendar item, the\nmobile device is located at a location that is estimated to be significant to a user.  The mobile device can store the description in association with the significant location.  Upon receiving a new calendar item containing at least one term in the\ndescription, the mobile device can predict that the user will visit the significant location at the time specified in the new calendar item.  The mobile device can provide user assistance based on the prediction.\nTechniques for determining a location of a mobile device using a location application programming interface (API) are described.  A mobile device can receive an input requesting the mobile device to monitor entry into and exit from a significant\nlocation.  The mobile device can call a start-monitoring instance function of an object of a location manager class as declared in the API to start monitoring, and call a stop-monitoring instance function of the object as declared in the API to stop\nmonitoring.  The mobile device can store the entry and exit, or provide a record of the entry or exit to a function that is conformant to the API for performing various tasks.\nThe features described in this specification can be implemented to achieve one or more advantages.  A mobile device can learn a movement pattern of the mobile device, and adapt itself to that movement pattern.  The mobile device can provide\npredictive user assistance based on the movement pattern without requiring additional user input, including, for example, alerting the user of traffic conditions while the user is en route to a significant location if the mobile device determines, based\non past movement patterns of the mobile device, that a user will visit the significant location, even when the mobile device did not receive a user inquiry.  Accordingly, a user of the mobile device may have a better experience using services, especially\nlocation-based services, of the mobile device.  For example, the mobile device can determine that a user usually goes from home to work at 8:00 am on weekdays and from home to a gymnasium at 8:00 am on weekends.  Upon being turned on shortly before 8:00\nam, on weekdays, the mobile device can automatically display traffic information on a route from home to work; whereas on weekends, the mobile device can automatically display traffic information on a route from home to the gymnasium.\nThe details of one or more implementations of the subject matter are set forth in the accompanying drawings and the description below.  Other features, aspects, and advantages of subject matter will become apparent from the description, the\ndrawings, and the claims. <BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nFIG. 1 is a diagram illustrating an exemplary implementation of predictive user assistance.\nFIG. 2A is a diagram illustrating exemplary techniques of determining location clusters.\nFIG. 2B is a diagram illustrating exemplary techniques of hint-based location clusters.\nFIG. 3A is a diagram illustrating exemplary techniques of identifying significant locations based on location clusters.\nFIG. 3B illustrates exemplary techniques of adaptive clustering.\nFIG. 4A is a diagram illustrating an exemplary state model determined based on the location clusters.\nFIG. 4B illustrates exemplary techniques for determining locations of calendar items.\nFIG. 5 is a diagram illustrating incremental changes to the state model.\nFIG. 6A is a diagram illustrating determining a transition probability density between exemplary states.\nFIG. 6B is a diagram illustrating determining an entry probability density of an exemplary state.\nFIG. 6C illustrates an exemplary user interface for displaying significant locations.\nFIGS. 7A, 7B, and 7C are block diagrams illustrating components of an exemplary mobile device implementing predictive user assistance.\nFIG. 7D is a block diagram illustrating exemplary location API.\nFIG. 8A is a flowchart illustrating an exemplary procedure of hint based location determination.\nFIG. 8B is a flowchart illustrating an exemplary procedure of adaptive location clustering.\nFIG. 8C is a flowchart illustrating an exemplary procedure of determining locations of calendar items.\nFIG. 8D is a flowchart illustrating an exemplary procedure of calling a location monitoring API.\nFIG. 9 is a flowchart illustrating an exemplary procedure of predicting a future location.\nFIG. 10 is a block diagram illustrating an exemplary device architecture of a mobile device implementing the features and operations of FIGS. 1-9.\nFIG. 11 is a block diagram of an exemplary network operating environment for the mobile devices implementing the features and operations of FIGS. 1-9.\nLike reference symbols in the various drawings indicate like elements.\n<BR><BR>DETAILED DESCRIPTION\n<BR><BR>Exemplary Predictive User Assistance\nFIG. 1 is a diagram illustrating an exemplary implementation of predictive user assistance.  Exemplary mobile device 102 can utilize past movements of mobile device 102 to predict a future location of mobile device 102.  Mobile device 102 can\nthen adapt behavior of mobile device 102 to perform services that are specific to the predicted future location.\nMobile device 102 can use machine learning and data mining techniques to learn the past movement of mobile device 102.  The past movement can be recorded as significant locations visited by mobile device 102 and movement of mobile device 102\nbetween the significant locations.  Mobile device 102 can determine that a place or region is a significant location upon determining that, with sufficient certainty, mobile device 102 has stayed at the place or region for a sufficient amount of time. \nThe amount of time can be sufficient if it satisfies various criteria, for example, when the amount of time satisfies a time length threshold (e.g., X hours) or a frequency threshold (e.g., X minutes per day, Y number of days per week).  Records of\nmovement of mobile device 102 can include a measured or calculated time of entry into each significant location and a measured or calculated time of exit from each significant location.  A significant location can be associated with multiple entries and\nexits.\nIn addition to significant locations, the records of movement can include transitions between the significant locations.  Each transition from a first significant location to a second significant location can be associated with a transition\nbegin timestamp indicating a time mobile device 102 leaves the first significant location and a transition end timestamp indicating a time mobile device 102 enters the second significant location.\nMobile device 102 can represent the records of movement as state model 104.  State model 104 can include states (e.g., state 106 and other states) each representing a significant location, and transitions (e.g., transition 107 and other\ntransition between the states) each representing a movement of mobile device 102 between significant locations.  Additional details of determining state model 104 are described below in reference to FIG. 2-5.\nBased on state model 104, mobile device 102 can determine (1) a transition probability density that, at a given time, mobile device 102 moves from a given significant location to each other significant location, or (2) an entry probability\ndensity that mobile device 102 enters a significant location from a previously unknown or unrepresented location.  A pattern analyzer of mobile device 102 can determine a daily, weekly, monthly, or annual movement pattern of mobile device 102 using state\nmodel 104.  A predictive engine of mobile device 102 can use transition probability density (or entry probability density) and the movement pattern to forecast a significant location that mobile device 102 will enter (or stay) at a future time.  Mobile\ndevice 102 can then use the forecast to provide predictive user assistance, e.g., to assist the user to plan for a future event.\nIn the example of FIG. 1, mobile device 102 can determine current location 108 using a location determination subsystem of mobile device 102.  Mobile device 102 can determine current time 110.  Based on the current location, current time, and\nthe probabilities and patterns of state model 104, mobile device 102 can determine that a most likely location of mobile device 102 at a given time in the future is a significant location represented by state 106.  Mobile device 102 can then perform a\nuser-assistance function corresponding to the significant location, or corresponding to a transition from the current location to the significant location.  For example, upon being turned on or unlocked, mobile device 102 can provide for display alert\n112 on a display surface of mobile device 102.  Alert 112 can include user assistance information 116.  User assistance information 116 can include, for example, a route from the current location to the likely future location, and traffic information\nalong the route.  Mobile device 102 can provide for display alert 112 and user assistance information 116 automatically, without requesting a user to input the likely future location as a destination.\nIn some implementations, mobile device 102 can provide a label associated with the likely future location.  The label can be an address or a name of a point of interest pre-specified by a user or determined by mobile device 102 through reverse\ngeocoding or through semantic analysis of movements of mobile device 102.  For example, mobile device 102 can determine that a first location is likely to be a home and a second location is likely to be a work place.  Accordingly, mobile device 102 can\nuse the terms \"home\" and \"work\" in user assistance information 116.\n<BR><BR>Exemplary Techniques of Constructing a State Model\nFIG. 2A is a diagram illustrating exemplary techniques of determining location clusters.  Exemplary mobile device 102 (of FIG. 1) can use the learning techniques to determine state model 104 (of FIG. 1).\nMobile device 102 can sequentially trace location data through time (T).  Sequentially tracing location data can be performed by piggybacking on another application to avoid or reduce cost of location data collection.  For example, mobile device\n102 can collect the location data when another service requests location from a location determination subsystem of mobile device 102.  Accordingly, collecting the location data can be \"free\" without having to activate the location determination\nsubsystem solely for determining a movement pattern of mobile device 102.\nMobile device 102 can collect locations 202, 204, 206, 208, 210, and 212 over time T. Collecting the locations can be on-going operations.  Locations older than a specified period can be purged.  The period can be specified by user preference or\nprivacy policies.  Locations 202, 204, 206, 208, 210, and 212 can each include latitude, longitude, and altitude coordinates and being associated with a timestamp indicating a time the corresponding location is collected.\nMobile device 102 can determine that some of locations 202, 204, 206, 208, 210, and 212 belong to location clusters that may indicate a significant location.  Mobile device 102 can determine that a location cluster is formed upon determining\nthat (1) at least a pre-specified threshold number (e.g., two) of consecutive locations are collected; (2) a time span of the consecutive locations satisfies a pre-specified threshold time window; and (3) these locations are identical, indicating that\nmobile device 102 is stationary, or sufficiently close to one another, indicating that mobile device 102 is located in a sufficiently small and defined area during the time the locations are collected.\nFor example, mobile device 102 can determine two location clusters, location cluster 218 and location cluster 220, over time T. Location cluster 218 can include locations 202, 204, and 206, which are collected over a time period [T1, T2] that is\nlonger than a threshold time window (e.g., a time window of 45 minutes).  Mobile device 102 can determine that location cluster 218 includes locations 202, 204, and 206 upon determining that a variance of locations 202, 204, and 206 is low enough to\nsatisfy a variance threshold.  Likewise, location cluster 220 can include locations 210 and 212, which are collected within time period [T3, T4].  Mobile device 102 can determine that location cluster 220 includes locations 210 and 212 upon determining\nthat a variance of locations 210 and 212 satisfies the variance threshold.\nAn outlier detection mechanism can filter out locations that do not belong to clusters.  For example, mobile device 102 can determine that location 208 is different from location 206 and location 210 (e.g., the distance between location 206 and\n208 and the distance between location 208 and location 210 exceeds a threshold).  In addition, mobile device 102 can determine that no other locations are (1) collected within the threshold time window before or after location 208 and (2) geographically\nclose to location 208.  In response, mobile device 102 can determine that location 208 is an outlier and discard location 208.  In addition, if a location in a time period is significantly different from many other locations in the time period, mobile\ndevice 102 can discard the different location as an outlier and determine the location cluster using other locations in the time window.  Mobile device 102 can use location clusters 218 and 220 to determine significant locations and states of state model\n104.\nFIG. 2B is a diagram illustrating exemplary techniques of hint-based location clusters.  In some implementations, one of the conditions for determining a location cluster is that a time span of the consecutive locations satisfies a variable\nthreshold time window.  The threshold can vary based on whether mobile device 102 has a hint of significance of a location.\nAt various times, mobile device 102 can be located at locations 232, 234, and 236.  Locations 232, 234, and 236 can be far apart from one another, indicating that mobile device 102 is moving.  Mobile device 102 can be located at locations 240\nthrough 248 during a continuous period of time.  Locations 240 through 248 can be identical or sufficiently close to one another.  Mobile device 102 can determine whether the period of time is sufficiently long such that locations 240 through 248 form a\nlocation cluster that indicates a significant location, based on whether the period of time satisfies a variable threshold.  Mobile device 102 can use various hints to determine the variable threshold.\nFor example, mobile device 102 can search locations where mobile device 102 visited previously.  Mobile device 102 can designate as a first hint a record indicating that mobile device 102 previously visited the location at or near locations 240\nthrough 248 as a first hint.  Mobile device 102 can examine a user search history performed on or through mobile device 102.  If the user searched for the location before, mobile device 102 can designate a search query including an address at or near\nlocations 240 through 248, or a business located at or near locations 240 through 248, as a second hint.  Mobile device 102 can designate a calendar item in a user calendar (e.g., an appointment or a meeting) located at or near locations 240 through 248\nas a third hint.\nUpon detecting one or more hints, mobile device 102 can use a shorter time period, e.g., five minutes, as a threshold for determining a location cluster or significant location.  More hints can correspond to shorter threshold.  Accordingly,\nmobile device 102 can determine a significant location upon detecting location 242 of the mobile device, when the short time threshold is satisfied.\nIf no hint is found, mobile device 102 can use a longer time period, e.g., 20 minutes, as a threshold for determining a location cluster or significant location.  Accordingly, when no hint is found, mobile device 102 can determine a location\ncluster or significant location upon detecting location 246 of mobile device 102, when the long time threshold is satisfied.  In either case, with or without a hint, mobile device 102 can determine a significant location in real time, e.g., 5 minutes or\n20 minutes after locations converge into a cluster.\nFIG. 3A is a diagram illustrating exemplary techniques of identifying significant locations based on location clusters.  Using the techniques described above in reference to FIG. 2, mobile device 102 can identify location clusters 218, 220, 302,\nand 303.  Mobile device 102 can determine significant locations 304, 306, and 308 based on location clusters 218, 220, 302, and 303.\nMobile device 102 can determine each of significant locations 304, 306, and 308 based on location clusters 218, 220, 302, and 303 using the locations in each of location clusters 218, 220, 302, and 303.  Determining significant locations 304,\n306, and 308 can be based on recursive filter with a constant gain.  Details of determining significant locations 304, 306, and 308 are provided below in the next paragraph.  Each of significant locations 304, 306, and 308 can include latitude,\nlongitude, and optionally, altitude coordinates.  Each of significant locations 304, 306, and 308 can be associated with one or more location clusters.  For example, significant location 304 can correspond to location cluster 218 in time period [T1, T2]\nand location cluster 303 during time period [T7, T8].  Location in location cluster 218 and location cluster 303 can be identical.  The length of time period [T1, T2] and time window [T7, T8] can be same or different.\nMobile device 102 can have an initial state model at time T2.  At time T2+k, mobile device 102 can receive incremental location data, where k is a difference between time T2 and the time the additional location data are received (in this\nexample, k=T7-T2).  Mobile device 102 can use the incremental location data to determine significant location 304 for use in the state model.  Mobile device 102 can determine that location cluster 218 corresponds to latitude and longitude coordinates X1. Mobile device 102 can determine that location cluster 303 corresponds to latitude and longitude coordinates X2.  Mobile device 102 can determine that a distance between X1 and X2 satisfies a threshold.  In response, mobile device 102 can determine that\nlocation cluster 218 and location cluster 303 belong to a same location (significant location 304).  Mobile device 102 can then add location cluster 303 to significant location 304 using constant gain filter as shown below in filter (1).\n.times..times..alpha..times..times..times..times..alpha..times..times..al- pha..gtoreq.  ##EQU00001##\nEach of significant locations 304, 306, and 308 can be associated with one or more entry timestamps and one or more exit timestamps.  Each entry timestamp can correspond to a time associated with a first location in a location cluster.  For\nexample, a first entry timestamp associated with significant location 304 can be a timestamp associated with location 202, which is the first location of location cluster 218.  A second entry timestamp associated with significant location 304 can be a\ntimestamp associated with a first location in location cluster 303.  Likewise, each exit timestamp can correspond to a time associated with a last location in a location cluster.  For example, a first exit timestamp associated with significant location\n304 can be a timestamp associated with location 206, which is the last location of location cluster 218.  A second entry timestamp associated with significant location 304 can be a timestamp associated with a last location in location cluster 303.\nEach of significant locations 304, 306, and 308 can be associated with a label.  The label can be designated by a user (e.g., \"Home,\" \"Gym,\" or \"Work\"), or automatically determined by mobile device 102 through reverse geocoding.  In some\nimplementations, the label can be derived from a semantic analysis of a pattern of the time of day and day of week of each location cluster associated with the significant locations.  The semantic analysis can be based on behaviors natural to human\nbeings.  Mobile device 102 can be programmed to apply pre-determined patterns that reflect the human behavior.  The behavior can include, for example, every human being needs to sleep for some time.  The time for sleeping can be a time mobile device 102\nis strictly stationary.  A user sleeps eight hours a day and eating dinner at home is likely to spend X hours (e.g., 10-12 hours) at home on weekdays, and Y hours on weekends.  A user can be at work Monday through Friday for regular hours.  Mobile device\n102 can leverage these patterns to determine that a significant location as \"home\" where (1) mobile device 102 spends more than a first threshold number of hours (e.g., 60 hours) per week; (2) mobile device 102 records most entries and exits; and (3)\nthose entries and exists indicate that mobile device stays at least a second threshold number of hours (e.g., eight hours) per day.\nFor example, mobile device 102 can determine that each location cluster associated with significant location 304 corresponds to a time period designated as evening during weekdays (e.g., from 7:00 pm to 8:00 am next day).  Mobile device 102 can\nthen designate significant location 304 as \"home\" and provide the designation as a label for significant location 304.\nMobile device 102 can determine transitions from one significant location to another.  For example, mobile device 102 can determine that, on a given weekday, mobile device 102 transitions (312) from significant location 304 (\"Home\") to\nsignificant location 308 (\"Work\") between time T2 and time T3.  Mobile device 102 can associate the transition with a transition begin timestamp (e.g., T2) and a transition end timestamp (e.g., T3).  Mobile device 102 can construct state model 104 based\non significant locations 304, 306, and 308 and transitions 312, 314, and 316.  Details of state model 104 are described below in reference to FIG. 4.\nFIG. 3B illustrates exemplary techniques of adaptive clustering.  Mobile device 102 (of FIG. 1) can record a location of mobile device 102 when mobile device 102 uses location based services.  Mobile device 102 can record locations and\ntimestamps.  Mobile device 102 can determine, based on the recorded locations and timestamps, if the locations converge to a cluster for a period of time.  For example, mobile device 102 can determine that mobile device 102 is located at location 332 at\na given time, e.g., 8:00 pm, and is located at location 334 at another time, e.g., 11:00 pm.  Mobile device 102 can determine that locations of mobile device 102 have not moved away from locations 332 and 334 between 8:00 pm and 11:00 pm.  Mobile device\n102 can determine that locations 332 and 334, and the locations recorded between 8:00 pm and 11:00 pm, converge into a location cluster having a size determined based on distance between locations 332 and 334.  Mobile device 102 can determine that\nsignificant location 304 has a first size corresponding to the size of the location cluster.\nMobile device 102 can determine that mobile device transitioned (335) to another location.  Mobile device 102 can determine that, during one or more time periods the total of which exceeds a threshold time, mobile device 102 is located at\nlocations 336, 338, 340, 342, 344, 346, 348, 350, and 352.  The time periods can include, for example, 8:00 am through 10:00 am on Monday, 8:00 am through 9:00 am on Tuesday, and 10:00 am through 12:00 pm on Wednesday.  The locations can be more \"spread\nout\" than the locations 332 and 334, due to movement of mobile device 102 between features of a work place including a parking lot, an office, a conference room, and a cafeteria, compared to movement of mobile device 102 between a living room and a\nbedroom of a home.  Mobile device 102 can determine that locations 336 through 352 converge into a location cluster having a size determined based on distance between locations 336 through 352 by measuring deviation among the locations in the location\nsamples.  Mobile device 102 can determine that significant location 308 has a second size corresponding to the size of the location cluster.  The second size can be bigger than the first size of significant location 304 resulting from the greater spread\namong locations 336 through 352.\nIn some implementations, mobile device 102 can match significant location 304 and significant location 308 with map data.  For example, mobile device 102 can determine that significant location 304 coincides with building 354 as represented in\nthe map data.  In response, mobile device 102 can snap a shape of significant location 304 to the shape of building 354.  Likewise, mobile device 102 can determine that significant location 308 matches a set of geographic features that includes parking\nlot 356, office 358, conference room 360, and cafeteria 362, as represented in the map data.  In response, mobile device 102 can determine a shape of significant location according to a bounding box of parking lot 356, office 358, conference room 360,\nand cafeteria 362.\nFIG. 4A is a diagram illustrating exemplary state model 104 determined based on the location clusters.  State model 104 can be a first order autoregressive process depicting states and state transitions where a transition into a state q is\nconditioned by a previous state r. The state and state transitions can be an abstraction of movement of mobile device 102 among significant locations.  Compared to a conventional Gauss-Markov model, state model 104 can be a sufficient model, retaining\nstochastic properties of the state transitions using distribution function in time and duration.\nState model 104 can include states 106, 402, and 404.  States 106, 402, and 404 can correspond to significant locations 304, 308, and 306, respectively.  Mobile device 102 can determine significant locations 304, 308, and 306 based on location\nclusters 218, 220, 302, and 303, as described above in reference to FIG. 3.  Each of states 106, 402, and 404 can be a representation of significant locations 304, 308, and 306, respectively.\nState model 104 can include multiple transitions from each state to each other state.  The transitions can include, for example, transition 406 from state 106 to state 402, and transition 408 from state 106 to state 402.  In state model 104,\neach transition from state 106 to state 402 can correspond to a transition from a location cluster of significant location 304 to a location cluster of significant location 308.  For example, transition 406 can represent transition 312 from location\ncluster 218 of significant location 304 to location cluster 220 of significant location 308.  Transition 408 can represent a transition from location cluster 303 of significant location 304 to a next location cluster of significant location 308.\nEach of transitions 406 and 408 can be associated with a transition begin timestamp and a transition end timestamp.  Each transition begin timestamp can be a time that mobile device 102 leaves significant location 304 represented by state 106. \nFor example, the transition begin timestamp of transition 406 can be Tuesday, 7:00 am; the transition begin timestamp of transition 408 can be Wednesday, 7:00 am.  Each transition end timestamp can be a time that mobile device 102 enters significant\nlocation 308 represented by state 402.  For example, the transition end timestamp of transition 406 can be Tuesday, 9:00 am; the transition end timestamp of transition 408 can be Wednesday, 9:00 am.\nEach state of state model 104 can be associated with one or more state entry timestamps and one or more state exit timestamps.  For example, a first state entry timestamp for state 106 can be a time associated with a first location (location\n202) of mobile device 102 located in location cluster 218 of significant location 304.  A first state exit timestamp can be a time associated with a last location (location 206) of mobile device 102 located in location cluster 218 of significant location\n304.  The first state entry timestamp and the first state exit timestamp can define first dwell time 412 of mobile device 102 staying at state 106.  A second state entry timestamp for state 106 can be a time associated with a first location of mobile\ndevice 102 located in location cluster 303 of significant location 304.  A second state exit timestamp can be a time associated with a last location of mobile device 102 in location cluster 303 of significant location 304.  The second state entry\ntimestamp and the second state exit timestamp can define second dwell time 414 of mobile device 102 staying at state 106.\nFIG. 4B illustrates exemplary techniques for determining locations of calendar items.  Mobile device 102 (of FIG. 1) can execute a calendar application program in which a user can specify calendar items for mobile device 102 to provide alerts or\nreminders.  Mobile device 102 can determine, from a user input or from an application program (e.g., an email program), calendar items 422, 424, and 426.  Each of calendar items 422, 424, and 426 can be associated with a respective text string, e.g.,\n\"Cedar,\" \"Sequoia,\" and \"Dentist.\" Each text string can be a subject line of a respective calendar item or a body of the respective calendar item.  Each of calendar items 422, 424, and 426 can be associated with a respective time, e.g., 9:00 am through\n10:30 am Wednesday, 11:00 am through 12:00 noon Wednesday, and 2:00 pm through 3:30 pm Wednesday.  Mobile device 102 can make the association over multiple instances to increase a certainty that the association is correct.  For example, a calendar\napplication program may have multiple calendar items including a string \"Sequoia\" indicating a conference room.  Mobile device 102 may or may not always be in the \"Sequoia\" conference room at time as indicated in the calendar items.  By making the\nassociation over multiple instances, mobile device 102 can determine a most visited location to be the location of the \"Sequoia\" conference room.\nMobile device 102 can determine that, during the time period associated with calendar items 422 and 424, mobile device 102 is located at significant location 308 designated as \"work\" and that, during the time period associated with calendar item\n426, mobile device 102 is located at significant location 428 designated as \"Palo Alto.\" Accordingly, mobile device 102 can store each of the text strings \"Cedar,\" \"Sequoia,\" and \"Dentist\" in association with a respective location.  For example, mobile\ndevice 102 can store, in a text database, each of the text strings \"Cedar\" and \"Sequoia\" in association with geographic coordinates of significant location 308, and store text string \"Dentist\" in associate with geographic coordinates of significant\nlocation 428.  Mobile device 102 can provide the stored information to a location service for providing various user assistances.\nFor example, mobile device 102 can receive a calendar item specifying a time in the future, e.g., 5:00 pm on a given day six months later.  The calendar item can include a text string \"visit dentist.\" Mobile device 102 can determine that the\ntext string matches one that is stored in the text database.  Accordingly, mobile device 102 can determine that a user is likely to visit significant location 428 at 5:00 pm on that given day.  On that day, mobile device 102 can determine an estimated\ntravel time from a location of mobile device 102 to significant location 428, e.g., 25 minutes.  Accordingly, mobile device 102 can automatically provide an alert for display at least 25 minutes before 5:00 pm on that day and indicating to the user that\nthe user should start heading for significant location 428 to be on time for the calendar item.\nFIG. 5 is a diagram illustrating incremental changes to state model 104.  State model 104 can have a variable topology, allowing incremental addition of new states and deletion of obsolete states.\nMobile device 102 can determine new state 502.  For example, mobile device 102 can determine that a series of location readings indicate that mobile device 102 is located at a place for a sufficiently long duration that, with sufficient\ncertainty, that the place is a significant location.  Mobile device 102 can determine that the significant location is not represented in state model 104.  In response, mobile device 102 can create new state 502, and add (504) new state 502 to state\nmodel 104.  Mobile device 102 can add transitions to state 502 based on a last significant location visited by mobile device 102 prior to visiting state 502.  Mobile device 102 can associate state 502 with a state entry timestamp of a first location\nreading indicating mobile device 102 is located at the significant location of state 502.  Mobile device 102 can associate state 502 with a state exit timestamp of a last location reading indicating mobile device 102 is at the significant location\nrepresented by state 502 before mobile device 102 enters another significant location.  Mobile device 102 can add transitions from state 502 based on the next significant location visited by mobile device 102 and represented in state model 104.\nIn addition to adding states, mobile device 102 can periodically remove states from state model 104.  Mobile device 102 can determine that, for a sufficiently long time (e.g., exceeding an X day or week threshold), mobile device 102 has not\nvisited a significant location represented by state 404.  Accordingly, mobile device 102 can remove (506) state 404 from state model 104.  Removing state 404 can include removing transitions into state 404 and transitions from state 404.\nMobile device 102 can use state model 104 to predict a future location of mobile device 102.  Predicting the future location can be based at least in part on a current location of mobile device 102.  The current location can be \"in state,\" where\nthe current location is represented by a state of state model 104.  Upon determining that the current location is in state, mobile device 102 can predict the future location based on transition probability densities between states.  The current location\ncan be \"out of state,\" where the current location is not represented by a state of state model 104.  Upon determining that the current location is out of state, mobile device 102 can predict the future location based on entry probability densities of\nentering a state of state model 104 from the current location.  Details on determining the transition probability densities and entry probability densities are described below in reference to FIGS. 6A and 6B.\nFIG. 6A is a diagram illustrating determining a transition probability density 602 between exemplary states 106 and 402.  Transition probability density 602 can indicate a probability distribution of mobile device 102 transitions from state 106\nto state 402 of state model 104.  Mobile device 102 can determine transition probability density 602 upon receiving a request to predict a future location of mobile device 102.  The request can be associated with a current time and a future time.  At the\ncurrent time, mobile device 102 can be located at a significant location corresponding to state 106.  The future time can be a point in time or a time window.\nTransition probability density 602 can be a distribution over a time period, e.g., [Ta, Tb], where Ta is a starting time, and Tb is an ending time of the time period.  The time period [Ta, Tb] can be a window of forecast.  In some\nimplementations, the starting time Ta can correspond to the current time, or the current time with a bias (e.g., X minutes before or after the current time); the ending time Tb can correspond to the future time, or the future time with a bias (e.g., Y\nminutes before or after the future time).  In some implementations, the starting time Ta and ending time Tb can correspond to a beginning and an ending of a time window (e.g., a day or a week), respectively.\nMobile device 102 can determine transition probability density 602 based on past transitions of mobile device 102 from state 106 to state 402.  At a given time between Ta and Tb, (1) more transitions from state 106 to state 402 in the past at\nthe given time can correspond to a higher probability density value; (2) more certainty on the transitions in the past at the given time can correspond to a higher probability density value; and (3) a more stable pattern of transitions in the past at the\ngiven time can correspond to a higher probability density value.\nFor example, t0 corresponds to 8:00 am, and t1 corresponds to 3:00 pm.  In the past, and as recorded in state model 104, X number of transitions occurred between state 106 and state 402 between 7:00 am and 9:00 am; and Y number of transitions\noccurred between 2:00 pm and 4:00 pm.  If X is greater than Y, t0 can correspond to comparatively higher probability density value 604, whereas t1 can correspond to comparatively lower probability density value 606.\nIn addition, the certainty of the transitions can be relevant.  If a mean time of the transition start timestamps of the X transitions is closer to t0 than a mean time of the transition start timestamps of the Y transition is closer to t1, t0\ncan correspond to comparatively higher probability density value 604, whereas t1 can correspond to comparatively lower probability density value 606.  If a variance of the transition start timestamps of the X transitions is smaller than a variance of the\ntransition start timestamps of the Y transitions, t0 can correspond to comparatively higher probability density value 604, whereas t1 can correspond to comparatively lower probability density value 606.\nIn addition, stability of patterns of transitions in the past can be relevant.  Mobile device 102 can determine a pattern of movement based on time.  For example, mobile device 102 can determine, based on transitions in state model 104, that\nmovement of mobile device 102 follows a weekly pattern.  On weekdays, mobile device 102 transitions from state 106 to state 402 between 7:00 am and 9:00 am.  On weekends, mobile device 102 transitions from state 106 to state 402 between 2:00 pm and 4:00\npm.  Based on this identified weekly pattern, mobile device 102 can associate a comparatively higher probability density value 604 for time t0 if t0 is in a weekday, or associate a comparatively lower probability density value for time t0 if t0 is in a\nweekend day.\nTransition probability density 602 can be discrete or continuous.  Upon determining transition probability density 602 and other transition probability densities between states of state model 104, mobile device 102 can determine a time-based\nlikelihood of mobile device 102 transitioning from a current state (e.g., state 106) to each other state directly or indirectly (e.g., through one or more intermediate states).  Mobile device 102 can determine a predicted future location of mobile device\n102 based on the current location, the future time, and the probabilities of mobile device 102 transitioning to each state.\nFIG. 6B is diagram illustrating determining entry probability density 620 of exemplary state 106.  Entry probability density 620 can indicate a probability distribution that mobile device 102 enters state 106 from a current location that is not\nrepresented in state model 104.  Mobile device 102 can determine entry probability density 620 upon receiving a request to predict a future location of mobile device 102.  The request can be associated with a current time and a future time.  At the\ncurrent time, mobile device 102 can be located at the un-represented current location.  The future time can be a point in time or a time window.\nEntry probability density 620 can be a distribution over a time period, e.g., [Tc, Td], where Tc is a starting time, and Td is an ending time of the time period.  The time period [Tc, Td] can be a window of forecast.  In some implementations,\nthe starting time Tc can correspond to the current time, or the current time with a bias (e.g., X minutes before or after the current time); the ending time Td can correspond to the future time, or the future time with a bias (e.g., Y minutes before or\nafter the future time).  In some implementations, the starting time Tc and ending time Td can correspond to a beginning and ending of a time window (e.g., a day or a week), respectively.\nMobile device 102 can determine entry probability density 620 based on dwell time of mobile device 102 in state 106.  The dwell time, e.g., dwell time 412, 414, and 622, can be determined as described above in reference to FIG. 4.\nAt a given time between Tc and Td, (1) more number of stays of mobile device 102 in state 106 in the past at the given time can correspond to a higher probability density value; (2) more certainty on the entry into the state 106 in the past can\ncorrespond to a higher probability density value; and (3) a more stable pattern of entry into state 106 in the past can correspond to a higher probability density value.\nFor example, t2 corresponds to 10:00 am, and t2 corresponds to 3:00 pm.  In the past, and as recorded in state model 104 by dwell time 412, 414, and 622, on X number occasions, mobile device 102 is located in state 106 at time t2; and on Y\nnumber occasions, mobile device 102 is in state 106 at time t3.  If X is less than Y (e.g., in this example, X=2, Y=3), t2 can correspond to comparatively lower probability density value 624, whereas t3 can correspond to comparatively lower probability\ndensity value 626.\nAdditionally or alternatively, mobile device 102 can determine, based on state dwelling time determined from state model 104, that location of mobile device 102 follows a weekly pattern.  For example, mobile device 102 can determine that dwell\ntime 414, and a number of other dwell times occur only on weekdays, whereas dwell times 412 and 622 occur only on weekends.  Based on this identified weekly pattern, mobile device 102 can associate lower probability density value 624 to time t2 and\nhigher probability density value 624 to time t3 if time t2 and time t3 fall on a weekday.  Mobile device 102 can associate equal probability density values to time t2 and time t3 fall on a weekend day.\nEntry probability density 620 can be discrete or continuous.  Upon determining entry probability density 620 and other entry probability densities between states of state model 104, mobile device can determine a time-based likelihood of mobile\ndevice 102 enters from a current location to each other state directly or indirectly (e.g., through one or more intermediate states).  Mobile device 102 can determine a predicted future location of mobile device 102 based on the current location, the\nfuture time, and the probabilities of mobile device 102 entering each state.\nMobile device 102 can filter out states from state model 104 before, during, or after calculating the entry probability densities based on various factors.  Filtering out a state can include preventing the state being used for a particular\nlocation prediction without removing the state from state model 104.  The factors for filtering out a state can include a distance between the current location and the location represented by the state in state model 104.  Mobile device 102 can filter\nout a state upon determining that, during the forecast time window, mobile device 102 is unlikely to reach from the current location to the location of that state.  Mobile device can perform the filtering based on a time difference between the current\ntime and the starting time or the ending time of the time window, and a pre-specified maximum speed of movement of mobile device 102.\nFor example, mobile device 102 can determine that the time difference between the current time and the closing time Td of the forecasting time window is X hours.  Mobile device can determine that a distance between the current location and the\nsignificant location represented by state 106 is Y kilometers.  Based on a pre-specified maximum speed of Z kilometers per hour, mobile device 102 can filter out state 106 upon determining that X*Z&lt;Y, indicating that mobile device 102 cannot reach the\nlocation represented by state 106 in X hours, even if travelling at maximum speed.\nFIG. 6C illustrates an exemplary user interface 604 for displaying significant locations.  User interface 604 can be displayed on mobile device 102.  User interface 604 can include a map display for displaying significant locations, e.g.,\nsignificant locations indicated by markers 608, 610, and 612.  User interface 604 can include search box 614, where a user can enter a search query for significant locations.  In the example shown, the user entered the date/time span query \"Mar.  23,\n2014, my locations.\" This example query is requesting a search for significant locations that the user visited on Mar.  23, 2014.  Upon receiving an input initiating a search (e.g., selecting a \"Search\" button), the mobile device searches a significant\nlocation data store, determines significant locations visited on Mar.  23, 2014, generates markers 608, 610, and 612 that represent the significant locations, and displays markers 608, 610, and 612 on a map.  Markers 608, 610, and 612 can have different\nsizes, corresponding to different sizes of the represented significant locations.\n<BR><BR>Exemplary Device Components\nFIG. 7A is a block diagram illustrating components of exemplary mobile device 102 implementing predictive user assistance.  Each component of mobile device 102 can include hardware and software components.\nMobile device 102 can include state model determination subsystem 702.  State model determination subsystem 702 can be a component of mobile device 102 programmed to determine a state model (e.g., state model 104) using location data from\nlocation determination subsystem 704.  The location data can include a series of one or more location readings, each being associated with a timestamp.  The location readings can include latitude, longitude, and optionally, altitude coordinates.\nLocation determination subsystem 704 is a component of mobile device 102 programmed to determine a location of mobile device 102 using a satellite navigation system (e.g., GPS), a cellular communications system (e.g., by triangulation using\ncellular towers), or wireless access gateways (e.g., by triangulation using known access point locations).\nMobile device 102 can include one or more services 706.  Services 706 can include functions of an operating system of mobile device 102 or one or more application programs.  Services 706 can request location data from location determination\nsubsystem 704.  The request can activate location determination subsystem 704.\nState model determination subsystem 702 can be configured to read location data provided by location determination subsystem 704 upon activation of location determination subsystem 704 by services 706.  Triggering reading location data by\nactivation of location determination subsystem 704 can avoid or minimize consumption of battery power by operations of determining the state model.  Based on the location data, state model determination subsystem 702 can determine a state model and store\nthe state model in state model database 708.  State model database 708 can include a storage device on mobile device 102 or on a server located remotely from mobile device 102.\nMobile device 102 can include forecasting subsystem 710.  Forecasting subsystem 710 is a component of mobile device 102 configured to determine a predicted future location of mobile device 102 based on the state model stored in state model\ndatabase 708.  One or more services 712 or other devices 714 can request a forecast from forecasting subsystem 710.  The request can be associated with a future time point or time window.  In response, forecasting subsystem 710 can provide one or more\npredicted future locations corresponding to the future time or time window.\nFIG. 7B is a block diagram illustrating components of exemplary state model determination subsystem 702 of FIG. 7A.  Each component of state model determination subsystem 702 can include hardware and software components.\nState model determination subsystem 702 can include location listener 720.  Location listener 720 is a component of state model determination subsystem 702 configured to read location data from location determination subsystem 704 upon being\ntriggered by an activation of location determination subsystem 704.  In some implementations, location listener 720 can be programmed to activate location determination subsystem 704 periodically to obtain the location data.\nLocation listener 720 can store the location data received from location determination subsystem 704 to raw location data store 722.  Raw location data store 722 can be a storage device of mobile device 102 programmed to store raw location data\nas read from location determination subsystem 704.  Raw location data store 722 can enforce a persistency policy where the raw location data are purged after a specified persistency period based on user request or privacy policy.\nState model determination subsystem 702 can include abstraction engine 724.  Abstraction engine 724 is a component of state model determination subsystem 702 configured to access the location data stored in raw location data store 722.  Based on\nthe location data, abstraction engine 724 can determine location clusters based on one or more pre-specified conditions.  The conditions can include a minimum number of locations for establishing a significant location (e.g., two), a threshold time\nwindow (e.g., minimum of X minutes), and outlier criteria.  Abstraction engine 724 can determine the significant locations visited by generating abstractions of the location clusters.  Abstraction engine 724 can store the significant locations in\nlocation data store 726.\nLocation data store 726 is a storage device of state model determination subsystem 702 configured to store significant locations determined by abstraction engine 724.  Location data store 726 can enforce a persistency policy where the\nsignificant locations are purged after a specified persistency period.  The persistence policy for location data store 726 can be different from the persistence policy for raw location data store 722.\nState model determination subsystem 702 can include state model construction engine 728.  State model construction engine 728 is a component of state model determination subsystem 702 configured to read the significant locations from location\ndata store 726, and generate state model 104.  In addition, state model construction engine 728 can be configured to maintain state model 104 by adding and removing states to state model 104.\nFIG. 7C is a block diagram illustrating components of exemplary forecasting subsystem 710 of FIG. 7A.  Each component of forecasting subsystem 710 can include hardware and software components.\nForecasting subsystem 710 can include probability modeler 740.  Probability modeler 740 is a component of forecasting subsystem 710 configured to determine probability densities (e.g., transition probability density 602 and entry probability\ndensity 620) based on states and transitions of a state model (e.g., state model 104).  Probability modeler 740 can determine the probability densities for transitions and entries over a time window.\nForecasting subsystem 710 can include pattern analyzer 742.  Pattern analyzer 742 is a component of forecasting subsystem 710 configured to determine a pattern of movement of mobile device 102 over a time period.  The time period can be a day, a\nweek, a month, or a year.  Pattern analyzer 742 can determine whether to determine a pattern based on a day, a week, a month, or a year based on longevity of state model 104.  For example, pattern analyzer 742 can determine whether state model 104 has\nsatisfied a longevity threshold (e.g., contains at least X weeks of data).\nUpon determining that state model 104 satisfies the threshold, pattern analyzer 742 can determine a weekly pattern.  The weekly pattern can include a probability distribution calculated for each day of week, where, for example, a probability\ndistribution for Monday is determined separately from a probability distribution for Sunday.  Upon determining that state model 104 does not satisfy the threshold, pattern analyzer 742 can determine a daily pattern.  The daily pattern can include a\nprobability distribution calculated for each hour of day, where, for example, a probability distribution for 9:00 am to 10:00 am is determined separately from a probability distribution for 5:00 pm to 6:00 pm.\nIn some implementations, pattern analyzer 742 can determine a daily pattern upon determining that mobile device 102 has moved to a new place.  For example, pattern analyzer 742 can determine that, the distances between each of the last X number\nof new states and each state older than the last X number of new states exceed a local threshold (e.g., Y kilometers), indicating that mobile device 102 has recently traveled to a new location (e.g., to a vacation place).  Upon the determination, pattern\nanalyzer 742 can determine the daily pattern, starting from the last X number of states.\nForecasting subsystem 710 can include prediction engine 744.  Prediction engine 744 is a component of forecasting subsystem 710 configured to receive a current time and a current location and determine a forecast location.  Prediction engine 744\ncan determine a predicted location of mobile device 102 based on the probability densities for transitions and entries provided by probability modeler 740 and the movement patterns provided from pattern analyzer 742.  Prediction engine 744 can identify\nmultiple candidate future locations based on the probability densities and the movement patterns.  Prediction engine 744 can then rank the candidate future locations using various attributes.\nThe attributes used by prediction engine 744 to rank the candidate future locations can include a last visit to a candidate future location as represented by a state, where a more recent visit can be associated with a higher ranking.  The\nattributes can include data longevity of the state associated with the candidate location, where a state having a longer data history can be associated with a higher ranking.  The attribute can include a likelihood associated with a forecast time window,\nwhich is determined based on a current location, a future time of the forecast time window, and a length of the forecast time window.  The attributes can include an aggregated dwell time, where a state having longer aggregated dwell time can be ranked\nhigher.  The attributes can include a number of visits to the state of the candidate location, where more visits or a higher frequency of visits to the state can be ranked higher.  Prediction engine 744 can provide one or more candidate future locations,\nincluding the highest ranked candidate future location, to prediction engine interface 746 as a forecast.\nPrediction engine interface 746 can be a component of mobile device 102 configured to implement an application programming interface (API) to prediction engine 744 such that an application program, function, or device complying with the API can\naccess the forecast determined by prediction engine 744.  In some implementations, prediction engine interface 746 can include an interface to other devices 714, e.g., external display screens or GPS devices, and provide the forecast location to other\ndevices 714.\nForecasting subsystem 710 can include semantic analyzer 748.  Semantic analyzer 748 is a component of forecasting subsystem 710 configured to determine a meaning of each significant location based on pattern of visit to the significant location. Semantic analyzer 748 can generate labels (e.g., \"work\" or \"home\") based on the meaning and provide the labels to prediction engine interface 746 to be associated with the forecast.\nFIG. 7D is a block diagram illustrating exemplary location API.  The API can be implemented on mobile device 102.  The API can include location function declarations 760.  Location function declarations 760 can be implemented using a header\nfile, e.g., a .h file in an object oriented C programming language, e.g., Objective-C or C++.\nLocation function declarations 760 can include start monitoring visit function declaration 762 and stop monitoring visit declaration 764.  Each of the declarations 762 and 764 can declare a name of a respective function, the name being\nindicative of the operations of the function.  Each of the declarations 762 and 764 can declare a return type of a respective function.  Each of the declarations 762 and 764 can declare whether a respective function is a class method or an instance\nmethod.  For example, a class method can be represented by a plus (+) sign before the function name.  An instance method can be represented by a minus (-) sign before the function name.  Each of the declarations 762 and 764 can declare parameters of the\nrespective function.\nThe functions can be defined in location function library 766.  Location function library 766 can include definition 768 for the start monitoring visit function and definition 770 for the stop monitoring visit function.  Each of definition 768\nand definition 770 can include programming instructions for start and stop monitoring a location visit.  A location visit can be an event that includes at least one of an arrival at or a departure from a location.\nApplication program 772 can call the start monitoring visit function and the stop monitoring visit function through the API.  For example, application program can be programmed to include location function declarations 760 by including the\nheader file for compilation.  Application program 772 can include location function 774.  Location function 774 can include, for example, computer instructions operable to cause a processor of mobile device 102 to determine location clusters and\nsignificant locations.  Location function 774 can call the start monitoring visit function and the stop monitoring visit function as declared in location function declarations 760 and as defined in location library 766.\n<BR><BR>Exemplary Procedures\nFIG. 8A is a flowchart illustrating an exemplary procedure 800 of hint based location determination.  Procedure 800 can be performed by a system including one or more processors.  The system can include mobile device 102.\nThe system can determine (802) multiple locations of the mobile device.  Each location can be associated with a timestamp indicating a time the location was determined by a location determination subsystem.  The locations can be ordered\nsequentially based on timestamps of the locations.  Determining the locations can include reading the location (e.g., latitude and longitude) from the location determination subsystem one at a time.  Each reading of the location determination subsystem\ncan be triggered by an activation of the location determination subsystem by an application program requesting a location-based service.  The system can filter the locations from the location determination subsystem by removing outliers using a\nstatistical filter.\nThe system can identify (804) a hint indicating that a user of the mobile device had shown interest in performing one or more acts at, or in proximity with, at least a portion of the locations.  The hint can include at least one of a present act\nperformed on the mobile device or detected by the mobile device, or a historical record of an act performed on the mobile device or detected by the mobile device.\nFor example, the hint can include a present act.  The present act can include a change in motion mode detected by the mobile device indicating that the user has entered or exited a vehicle.  The present act can include a power plugin event\nindicating that the mobile device is plugged into a power charger.  The present act can include a network handshake indicating that the mobile device is being connected to a wired or wireless communications network.  Additionally or alternatively, the\nhint can include a historical record.  The historical record can include a record of a search, the search including a search input on the mobile device and a search result including an address of the geographic location.  The historical record can\ninclude a calendar item indicating an appointment is to occur at the geographic location.  The historical record can include a record indicating that the mobile device established a wireless connection to a wireless device located at the geographic\nlocation.  The historical record can include a record indicating that the mobile device was plugged into a charger device or a computing device at the geographic location.  The historical record can include a record of a previous visit by the mobile\ndevice at the geographic location.\nThe system can determine (806) a hint-based time threshold for recognizing a location cluster, including reducing, upon the identified hint, a pre-specified time threshold for establishing the location cluster.\nThe system can determine (808) that a set of consecutive locations in the ordered locations form a location cluster upon determining that a time difference among the set of consecutive locations is longer than the hint-based time threshold.  The\nlocation cluster can indicate that the mobile device has dwelled at a geographic location sufficiently long to indicate a sufficient location for the user.  Determining that the consecutive locations form the location cluster can occur in real time while\nthe mobile device moves into or out of the geographic location, or in batch mode, e.g., once every day at 2:00 am.\nThe system can store (810) the significant location on the mobile device in association with a label of the significant location.  The system can designate the significant location as a state in the state model for estimating a place that the\nuser is likely to move to at a future time and for providing predictive user assistance according to the estimated place.  The state model can represent each movement of the mobile device from a first significant location to a second significant location\nas a transition from a first state representing the first significant location to a second state representing the second significant location.  The transition being associated with a transition start time and a transition end time.\nThe system can provide the state model to a forecasting subsystem of the mobile device for generating a forecast that a future location of the mobile device at a given future time is one of the significant locations represented in the state\nmodel.  The forecast can be based on a current time, the future time, a current location, and a probability density function determined based on the states and transitions of the state model.  The system can predict that the mobile device will move to\nthe significant location at a future time based on a past movement pattern of the mobile device.\nFIG. 8B is a flowchart illustrating an exemplary procedure 820 of adaptive location clustering.  Procedure 820 can be performed by a system including one or more processors.  The system can include mobile device 102.\nThe system can determine (822) that a series of locations of a mobile device that are recorded in a pre-specified convergence threshold amount of time, e.g., five hours, converge into a location cluster.  The location cluster can indicate that a\ngeographic location of the location cluster is a significant location to a user of the mobile device.  The series of locations of the mobile device can be recorded during multiple time periods, e.g., 7:00-8:00 am on every weekday, where each time period\nis disconnected with another time period.  As long as a total amount of time of the time periods, when summed up, satisfies the pre-specified convergence threshold amount of time, the system can move to the next stage of operations.\nThe system can determine (824) a convergence rate of the locations in the location cluster, the convergence rate indicating how quickly the locations are clustered together.  Determining the convergence rate can occur real time and include\ndetermining the convergence rate using locations recorded in a present time period and each prior time period.  Determining that the series of locations converge into a location cluster can include determining an initial location X[0] indicating an entry\ninto the location cluster.  The system can then receive a series of subsequent locations X[1], X[2] .  . . X[n].  The system can determine whether each respective subsequent location is included in the location cluster using a statistical filter\nconfigured to filter out outliers that are too far away from locations already in the location cluster.  The statistical filter can include a type of Kalman filter.  The system can determine that the series of locations converge into the location cluster\nupon determining at least a portion of the subsequent locations are included in the location cluster.  Determining the convergence rate includes determining a statistical deviation among the locations in the location cluster, e.g., by calculating a\nstandard deviation of the locations std(X[0], X[1] .  . . X[n]).\nThe system can determine (826) a size of the location cluster based on the convergence rate.  For example, a higher convergence rate, as indicated by a smaller standard deviation, can correspond to a smaller size.  After determining the size of\nthe location cluster, the system can adjust the size according to additional locations of the mobile device.  An increased convergence among the additional locations can reduce the size of the location cluster.\nThe system can store (828) the size in association with the location cluster.  In some implementations, the system can designate the significant location as a state in a state model for estimating a place that the user is likely to move to at a\nfuture time and for providing predictive user assistance according to the estimated place.  The significant location can be associated with the size of the location cluster and representable in a virtual map by a marker having a display size\ncorresponding to the size of the location cluster.  Determining the convergence rate, determining the size of the location cluster, and designating the significant location as the state in the state model can occur in real time while the mobile device\ndetermines and records locations of the mobile device.  In various implementations, the system can identify a venue from map data.  The venue can have a location that matches the significant location and have a size that matches the size of the location\ncluster.  The system can snap the significant location to the venue, including designating a shape of the venue as a shape of the significant location.\nFIG. 8C is a flowchart illustrating an exemplary procedure 840 of determining locations of calendar items.  Procedure 840 can be performed by a system including one or more processors.  The system can include mobile device 102.\nThe system can receive (842), from a calendar management application program, a record of a calendar item.  The record can include a text string describing an event of the calendar item and a time specification of the event.  The text string can\ninclude a subject line of the calendar item or a text body of the calendar item.\nThe system can determine (844) a geographic overlap between the significant location and the calendar item.  Determining the geographic overlap can include determining that, at a time designated in the time specification, the mobile device\ndwells at a significant location of a user of the mobile device.  The significant location can include a location that is estimated to have a significant meaning to the user of the mobile device.  The significant location can be determined using a\nlocation cluster of the mobile device as detected from historical data.  Determining that the mobile device dwells at the significant location can include determining that the mobile device is located in a location cluster for at least a threshold amount\nof time.  The location cluster can include detected locations of the mobile device filtered by a statistical filter.  In response, the system can determine the significant location based on the location cluster.\nIn response to determining the geographic overlap, the system can associate (846) the text string with the significant location.  Associating the subject text with the significant location can include storing the text string in association with\nthe significant location on a storage device.  Associating the subject text with the significant location can include associating the subject text with the significant location in the calendar application program.\nThe system can provide (848) a location-based service that corresponds to the significant location for a second calendar item of the calendar management application program ahead of a time designated in a time specification of the second\ncalendar item.  The system can provide the location-based service upon determining that the second calendar item includes at least one term in the text string.  Providing the location-based service can occur at a time that is determined using the time\ndesignated in the time specification of the second calendar item minus an estimated travel time from a current location to the significant location\nIn some implementations, the location-based service can include determining that the calendar item is outside of a set of locations associated with a daily routine of the user.  The daily routine can include a respective set of likelihood values\nthat the user is located at each of the locations at various times of a day.  In response to determining that the calendar item is outside of the set of locations, the system can switch a predictive user assistance model from one that is based on the\ndaily routine (e.g., work) to one that is based on the significant location (e.g., a vacation resort in East Palo Alto, Calif.).\nIn some implementations, the location-based service can include determining that the user will visit the second location at the time designated in a time specification of the second calendar item.  In response, the system can provide an alert to\nthe user before the user visits the second location.  For example, a mobile device can determine, based on readings of a sensor of the mobile device, a mode of transport of the mobile device, e.g., walking, biking, driving, or on public transit.  The\nmobile device can then determine a travel time corresponding to the mode of transport, and provide the alert that corresponds to the travel time.  In some implementations, the mobile device can receive motion classifiers from the mobile device or from a\nserver indicating that the mobile device is traveling in a particular mode.  The mobile device can reclassify the classifier using the mode of transport as context information.  Accordingly, the mobile device can use the context information to filter out\none or more motion classifiers that misclassifies a motion.\nFIG. 8D is a flowchart illustrating an exemplary procedure 860 of calling a location monitoring API.  Procedure 860 can be performed by a mobile device including one or more processors.  The mobile device can be mobile device 102 of FIG. 1.\nThe mobile device can receive (862) an input.  The input can request the mobile device to monitor locations of the mobile device to determine a length of time the mobile device has dwelled at a location.  The mobile device can determine that the\nlocation is a significant location for a user of the mobile device upon determining that the length of time satisfies a configurable threshold.\nResponsive to the input, the mobile device can monitor (864) the locations through an API.  Monitoring the locations can include calling a start-monitoring instance function, also referred to as a start-monitoring instance method, of an object\nof a location manager class.  The start-monitoring instance function can be declared in the API and configured to perform actions of recording detected visits of the mobile device at the locations.  Each detected visit can be associated with a respective\nset of geographic coordinates of a location that the mobile device visited.  Recording the detected visits can include storing the detected visits as data objects on a storage device, or sending a visit callback to a pre-specified function to notify the\npre-specified function of an aspect of a detected visit.  The aspect of the detected visit can include at least one of an arrival of the mobile device at a location or a departure of the mobile device from the location.\nIn some implementations, each detected visit can be recorded as an object of a location visit class, the object having an arrival date attribute storing a date the visit began, a departure date attribute storing a date the visit ended, a\ncoordinate attribute storing geographic coordinates of a center of a region visited by the mobile device, and a horizontal accuracy attribute storing an estimated radius of the region visited.  The object of the location visit class can be specified in a\nclass declaration to conform to a secure coding protocol and a copying protocol, each of the secure coding protocol and copying protocol defining a manner that the object sends a message to another object.\nResponsive to a trigger event, the mobile device can stop (866) the monitoring.  Stopping the monitoring can include calling a stop-monitoring instance function, also referred to as a stop-monitoring instance method, of the object.  The\nstop-monitoring instance function can be declared in the API and being operable to stop the object of the location manager class to record the visits.  The trigger event can include a user input, a timeout event, or an interruption event.\nEach of the start-monitoring instance function and the stop-monitoring instance function can be an asynchronous function that, once called, performs their respective operations without requiring a caller to wait for a result before performing\nother actions.  Each of the start-monitoring instance function and the stop-monitoring instance function is associated with a compiler hint in the API.  The compiler hint indicating a compatible version of an operating system for the API.\nThe mobile device can provide (868) the recorded visits to location consumer.  The location consumer can be a significant location determination engine for determining location coordinates of the significant location and a size of the\nsignificant location using the sets of geographic coordinates in the recorded visits.\nThe API can be defined in an object oriented programming language, e.g., Objective-C or C++ programming language in a header file.  The start-monitoring instance function can declared in the API as having a name of startMonitoringVisits and a\nvoid type.  The stop-monitoring instance function can be declared in the API as having a name of stopMonitoringVisits and a void type.  Each name of the respective instance function is indicative of underlying operations of the respective function to a\ndeveloper programming using the API.  Pseudo code for the API is provided below in Listing 1.\nTABLE-US-00001 Listing 1: Location API @interface LocationManager (LocationVisitExtensions) /* startMonitoring Visits * Begin monitoring for visits.  All LocationManagers allocated by an * application can deliver detected visits to their\ndelegates.  The delivery * can continue until -stopMonitoringVisits is sent to any such * LocationManager, even across application re-launch events.  * Detected visits can be sent to the delegate's -locationManager:didVisit: * method.  */ - (void)\nstartMonitoringVisits COMPILER_HINT(OS_VERSION); /* stopMonitoringVisits * Stop monitoring for visits.  To resume visit monitoring, send * -startMonitoringVisits.  * Stopping and starting can be asynchronous operations and may or may * not immediately\nreflect in delegate callback patterns.  */ - (void) stopMonitoringVisits COMPILER_HINT(OS_VERSION); @end /* LocationVisit * An instance of this class can represent a possibly open-ended event * during which a mobile device was at a specified coordinate. \n*/ COMPILER_HINT(OS_VERSION) @interface LocationVisit : Object &lt;SecureCoding, Copying&gt; /* arrivalDate - A date, including time, when the visit began.  This value * may equal to [Date_Distant_Past] if the true arrival date is not available.  */\n@property (nonatomic, readonly, copy) Date *arrivalDate; /* departureDate - A date when the visit ended.  This value may equal to * [Date_Distant_Future] if the mobile device has not left a location yet.  */ @property (nonatomic, readonly, copy) Date\n*departureDate; /*coordinate - A center of a region which the mobile device is visiting.  */ @property (nonatomic) LocationCoordinate2D coordinate; /* horizontalAccuracy - An estimate of a radius, e.g., in meters of the region which the mobile device is\nvisiting.  */ @property (nonatomic) CLLocationAccuracy horizontalAccuracy; @end\nFIG. 9 is a flowchart illustrating exemplary procedure 900 of predicting a future location.  Procedure 900 can be performed by mobile device 102, for example, using forecasting subsystem 710 of mobile device 102.\nMobile device 102 can receive (902), from a storage device (e.g., state model database 708) coupled to mobile device 102, a state model.  The state model can include multiple states and transitions between the states.  Each state can correspond\nto a location.  Each transition from a first state to a second state can indicate that, in the past, mobile device 102 moved from a corresponding first location to a corresponding second location.  Each location and transition can be associated with one\nor more timestamps.\nMobile device 102 can receive (904), from an application program or a device, a request for predicting a future location of mobile device 102.  The request can specify a future time and, optionally, a current location of mobile device 102.  The\nfuture time can include a point in time in the future or a time window in the future.\nMobile device 102 can determine (906), using a current time, the future time, and a current location of the mobile device as inputs, a probability for associating with each state in the state model.  If the request does not include the current\nlocation, mobile device 102 can determine the current location using location determination subsystem 704.  Mobile device 102 can determine the probabilities based on the states, transitions, and associating timestamps.  The probabilities can indicate a\nlikelihood that mobile device 102 will be located at each respective location corresponding to a state at the future time.\nDetermining (906) the probability for associating with each state can include determining that the current location is in state, where the current location is represented as a state in the state model.  Determining the probability for each state\ncan include determining a transition probability density of mobile device 102 moving from the state representing current location to a location corresponding to the state in one or more transitions.  The transition probability density can satisfy\nproperties of a Markov process.  Determining the transition probability density can be based on the transitions between states and a transition begin timestamp and a transition end timestamp associated with each of the transitions.\nDetermining (906) the probability for associating with each state can include determining that the current location is out of state, where the current location is not represented as a state in the state model.  Determining the probability to be\nassociated with each state can include determining an entry probability density of mobile device 102 entering a location corresponding to each state from the out-of-state current location.  Determining the entry probability density can be based on a\ndwell time mobile device 102 is in each state.  Mobile device 102 can determine the dwell time based on one or more entry timestamps and one or more exit timestamps associated with the respective state.\nIn some implementations, determining (906) the probability for associating with each state can be based on a daily, weekly, monthly, or annual pattern.  Mobile device 102 can determine whether the state model satisfies a longevity threshold\n(e.g., X weeks).  Mobile device 102 can determine a first activity pattern upon determining the state model satisfies the longevity threshold.  The first activity pattern can correspond to a first time span (e.g., a week).  Alternatively, mobile device\n102 can determine a second activity pattern upon determining that the state model does not satisfy the longevity threshold.  The second activity pattern can correspond to a second time span (e.g., a day).  The first time span can be longer than the\nsecond time span.  Mobile device 102 can determine the probability based on the current time, the future time, and the first activity pattern or second activity pattern.  Mobile device 102 can then determine the probability for associating with each\nstate based on the current time, the future time, and the first activity pattern or second activity pattern.\nIn some implementations, mobile device 102 can filter the states in the state model based on a distance between the current location and each location represented in the state model and a difference between the current time and the future time. \nMobile device 102 can filter out the states that, given the difference in time, and given a moving speed of mobile device 102, a likelihood that mobile device 102 reaches the state from the current location falls below a threshold value.\nBased on the probabilities, mobile device 102 can provide (908) at least one location associated with a state as a predicted future location of mobile device 102 in response to the request.  In some implementations, providing the location as the\npredicted future location can include identifying a state associated with a highest probability, and designating the location associated with the state associated with the highest probability as the predicted future location.  In some implementations,\nproviding the location as the predicted future location can include ranking the states based on the probabilities and one or more forecast attributes, and designating the location associated with a highest rank as the predicted future location.\nThe forecast attributes can include a time of last visit to each corresponding location.  The forecast attributes can include a derived likelihood for a forecast window based on the current location, the current time, and a forecast window\nlength.  The forecast attributes can include a temporal length of the state model.  The forecast attributes can include an aggregated dwell time at each state.  The forecast attributes can include a number of visits at each state.\nIn some implementations, mobile device 102 can determine that a data density of the state model satisfies a sparse model threshold.  In response, mobile device 102 can determine the probability for associating with each state in a sparse\noperating mode.  In the sparse operating mode, probability density calculations and rankings can be performed in a less stringent matter than the calculations and rankings in normal operating mode.\n<BR><BR>Exemplary Mobile Device Architecture\nFIG. 10 is a block diagram illustrating exemplary device architecture 1000 of a mobile device implementing the features and operations of category-based geofence.  A mobile device (e.g., mobile device 102) can include memory interface 1002, one\nor more data processors, image processors and/or processors 1004, and peripherals interface 1006.  Memory interface 1002, one or more processors 1004 and/or peripherals interface 1006 can be separate components or can be integrated in one or more\nintegrated circuits.  Processors 1004 can include application processors, baseband processors, and wireless processors.  The various components in mobile device 102, for example, can be coupled by one or more communication buses or signal lines.\nSensors, devices, and subsystems can be coupled to peripherals interface 1006 to facilitate multiple functionalities.  For example, motion sensor 1010, light sensor 1012, and proximity sensor 1014 can be coupled to peripherals interface 1006 to\nfacilitate orientation, lighting, and proximity functions of the mobile device.  Location processor 1015 (e.g., GPS receiver) can be connected to peripherals interface 1006 to provide geopositioning.  Electronic magnetometer 1016 (e.g., an integrated\ncircuit chip) can also be connected to peripherals interface 1006 to provide data that can be used to determine the direction of magnetic North.  Thus, electronic magnetometer 1016 can be used as an electronic compass.  Motion sensor 1010 can include one\nor more accelerometers configured to determine change of speed and direction of movement of the mobile device.  Barometer 1017 can include one or more devices connected to peripherals interface 1006 and configured to measure pressure of atmosphere around\nthe mobile device.\nCamera subsystem 1020 and an optical sensor 1022, e.g., a charged coupled device (CCD) or a complementary metal-oxide semiconductor (CMOS) optical sensor, can be utilized to facilitate camera functions, such as recording photographs and video\nclips.\nCommunication functions can be facilitated through one or more wireless communication subsystems 1024, which can include radio frequency receivers and transmitters and/or optical (e.g., infrared) receivers and transmitters.  The specific design\nand implementation of the communication subsystem 1024 can depend on the communication network(s) over which a mobile device is intended to operate.  For example, a mobile device can include communication subsystems 1024 designed to operate over a GSM\nnetwork, a GPRS network, an EDGE network, a Wi-Fi.TM.  or WiMAX.TM.  network, and a Bluetooth.TM.  network.  In particular, the wireless communication subsystems 1024 can include hosting protocols such that the mobile device can be configured as a base\nstation for other wireless devices.\nAudio subsystem 1026 can be coupled to a speaker 1028 and a microphone 1030 to facilitate voice-enabled functions, such as voice recognition, voice replication, digital recording, and telephony functions.  Audio subsystem 1026 can be configured\nto receive voice commands from the user.\nI/O subsystem 1040 can include touch surface controller 1042 and/or other input controller(s) 1044.  Touch surface controller 1042 can be coupled to a touch surface 1046 or pad.  Touch surface 1046 and touch surface controller 1042 can, for\nexample, detect contact and movement or break thereof using any of a plurality of touch sensitivity technologies, including but not limited to capacitive, resistive, infrared, and surface acoustic wave technologies, as well as other proximity sensor\narrays or other elements for determining one or more points of contact with touch surface 1046.  Touch surface 1046 can include, for example, a touch screen.\nOther input controller(s) 1044 can be coupled to other input/control devices 1048, such as one or more buttons, rocker switches, thumb-wheel, infrared port, USB port, and/or a pointer device such as a stylus.  The one or more buttons (not shown)\ncan include an up/down button for volume control of speaker 1028 and/or microphone 1030.\nIn one implementation, a pressing of the button for a first duration may disengage a lock of the touch surface 1046; and a pressing of the button for a second duration that is longer than the first duration may turn power to mobile device 102 on\nor off.  The user may be able to customize a functionality of one or more of the buttons.  The touch surface 1046 can, for example, also be used to implement virtual or soft buttons and/or a keyboard.\nIn some implementations, mobile device 102 can present recorded audio and/or video files, such as MP3, AAC, and MPEG files.  In some implementations, mobile device 102 can include the functionality of an MP3 player.  Mobile device 102 may,\ntherefore, include a pin connector that is compatible with the iPod.  Other input/output and control devices can also be used.\nMemory interface 1002 can be coupled to memory 1050.  Memory 1050 can include high-speed random access memory and/or non-volatile memory, such as one or more magnetic disk storage devices, one or more optical storage devices, and/or flash memory\n(e.g., NAND, NOR).  Memory 1050 can store operating system 1052, such as Darwin, RTXC, LINUX, UNIX, OS X, WINDOWS, iOS, or an embedded operating system such as VxWorks.  Operating system 1052 may include instructions for handling basic system services\nand for performing hardware dependent tasks.  In some implementations, operating system 1052 can include a kernel (e.g., UNIX kernel).\nMemory 1050 may also store communication instructions 1054 to facilitate communicating with one or more additional devices, one or more computers and/or one or more servers.  Memory 1050 may include graphical user interface instructions 1056 to\nfacilitate graphic user interface processing; sensor processing instructions 1058 to facilitate sensor-related processing and functions; phone instructions 1060 to facilitate phone-related processes and functions; electronic messaging instructions 1062\nto facilitate electronic-messaging related processes and functions; web browsing instructions 1064 to facilitate web browsing-related processes and functions; media processing instructions 1066 to facilitate media processing-related processes and\nfunctions; GPS/Navigation instructions 1068 to facilitate GPS and navigation-related processes and instructions; camera instructions 1070 to facilitate camera-related processes and functions; magnetometer data 1072 and calibration instructions 1074 to\nfacilitate magnetometer calibration.  The memory 1050 may also store other software instructions (not shown), such as security instructions, web video instructions to facilitate web video-related processes and functions, and/or web shopping instructions\nto facilitate web shopping-related processes and functions.  In some implementations, the media processing instructions 1066 are divided into audio processing instructions and video processing instructions to facilitate audio processing-related processes\nand functions and video processing-related processes and functions, respectively.  An activation record and International Mobile Equipment Identity (IMEI) or similar hardware identifier can also be stored in memory 1050.  Memory 1050 can store\nsignificant location instructions 1076 that include modeling instructions and forecasting instructions.  The modeling instructions, upon execution, can cause processor 1004 to perform the operations of state model determination subsystem 702, including\nprocedure 800.  The forecasting instructions, upon execution, can cause processor 1004 to perform the operations of forecasting subsystem 710.  The operations can include procedure 900.\nEach of the above identified instructions and applications can correspond to a set of instructions for performing one or more functions described above.  These instructions need not be implemented as separate software programs, procedures, or\nmodules.  Memory 1050 can include additional instructions or fewer instructions.  Furthermore, various functions of the mobile device may be implemented in hardware and/or in software, including in one or more signal processing and/or application\nspecific integrated circuits.\n<BR><BR>Exemplary Operating Environment\nFIG. 11 is a block diagram of exemplary network operating environment 1100 for the mobile devices implementing the features and operations of category-based geofence.  Mobile devices 1102a and 1102b can, for example, communicate over one or more\nwired and/or wireless networks 1110 in data communication.  For example, a wireless network 1112, e.g., a cellular network, can communicate with a wide area network (WAN) 1114, such as the Internet, by use of a gateway 1116.  Likewise, an access device\n1118, such as an 802.11g wireless access point, can provide communication access to the wide area network 1114.  Each of mobile devices 1102a and 1102b can be mobile device 102.\nIn some implementations, both voice and data communications can be established over wireless network 1112 and the access device 1118.  For example, mobile device 1102a can place and receive phone calls (e.g., using voice over Internet Protocol\n(VoIP) protocols), send and receive e-mail messages (e.g., using Post Office Protocol 3 (POP3)), and retrieve electronic documents and/or streams, such as web pages, photographs, and videos, over wireless network 1112, gateway 1116, and wide area network\n1114 (e.g., using Transmission Control Protocol/Internet Protocol (TCP/IP) or User Datagram Protocol (UDP)).  Likewise, in some implementations, the mobile device 1102b can place and receive phone calls, send and receive e-mail messages, and retrieve\nelectronic documents over the access device 1118 and the wide area network 1114.  In some implementations, mobile device 1102a or 1102b can be physically connected to the access device 1118 using one or more cables and the access device 1118 can be a\npersonal computer.  In this configuration, mobile device 1102a or 1102b can be referred to as a \"tethered\" device.\nMobile devices 1102a and 1102b can also establish communications by other means.  For example, wireless device 1102a can communicate with other wireless devices, e.g., other mobile devices, cell phones, etc., over the wireless network 1112. \nLikewise, mobile devices 1102a and 1102b can establish peer-to-peer communications 1120, e.g., a personal area network, by use of one or more communication subsystems, such as the Bluetooth.TM.  communication devices.  Other communication protocols and\ntopologies can also be implemented.\nMobile device 1102a or 1102b can, for example, communicate with one or more services 1130, 1140, and 1150 over the one or more wired and/or wireless networks.  For example, one or more location services 1130 can provide location data associated\nwith cellular towers or wireless access gateways to mobile devices 1102a and 1102b such that mobile device 1102a and 1102b can determine a current location using triangulation.  Location service 1130 can receive a series of current locations from mobile\ndevices 1102a or 1102b and determine a significant location for mobile devices 1102a or 1102b or both, based on hints and based on adaptive location clustering technologies.  Travel planning services 1140 can provide traffic information based on a\ncurrent time, current location, and a forecast location to assist a user planning a route to the forecast location and an estimated time of arrival.  Calendar services 1150 can store, on a user's storage space, the user's calendar items and their\nrespective locations for access by multiple user devices of a same user.\nMobile device 1102a or 1102b can also access other data and content over the one or more wired and/or wireless networks.  For example, content publishers, such as news sites, Really Simple Syndication (RSS) feeds, web sites, blogs, social\nnetworking sites, developer networks, etc., can be accessed by mobile device 1102a or 1102b.  Such access can be provided by invocation of a web browsing function or application (e.g., a browser) in response to a user touching, for example, a Web object.\nAs described above, some aspects of the subject matter of this specification include gathering and use of data available from various sources to improve services a mobile device can provide to a user.  The present disclosure contemplates that in\nsome instances, this gathered data may include personal information data that uniquely identifies or can be used to contact or locate a specific person.  Such personal information data can include demographic data, location-based data, telephone numbers,\nemail addresses, twitter ID's, home addresses, or any other identifying information.\nThe present disclosure recognizes that the use of such personal information data, in the present technology, can be used to the benefit of users.  For example, the personal information data can be used to deliver targeted content that is of\ngreater interest to the user.  Accordingly, use of such personal information data enables calculated control of the delivered content.  Further, other uses for personal information data that benefit the user are also contemplated by the present\ndisclosure.\nThe present disclosure further contemplates that the entities responsible for the collection, analysis, disclosure, transfer, storage, or other use of such personal information data will comply with well-established privacy policies and/or\nprivacy practices.  In particular, such entities should implement and consistently use privacy policies and practices that are generally recognized as meeting or exceeding industry or governmental requirements for maintaining personal information data\nprivate and secure.  For example, personal information from users should be collected for legitimate and reasonable uses of the entity and not shared or sold outside of those legitimate uses.  Further, such collection should occur only after receiving\nthe informed consent of the users.  Additionally, such entities would take any needed steps for safeguarding and securing access to such personal information data and ensuring that others with access to the personal information data adhere to their\nprivacy policies and procedures.  Further, such entities can subject themselves to evaluation by third parties to certify their adherence to widely accepted privacy policies and practices.\nDespite the foregoing, the present disclosure also contemplates embodiments in which users selectively block the use of, or access to, personal information data.  That is, the present disclosure contemplates that hardware and/or software\nelements can be provided to prevent or block access to such personal information data.  For example, in the case of advertisement delivery services, the present technology can be configured to allow users to select to \"opt in\" or \"opt out\" of\nparticipation in the collection of personal information data during registration for services.\nTherefore, although the present disclosure broadly covers use of personal information data to implement one or more various disclosed embodiments, the present disclosure also contemplates that the various embodiments can also be implemented\nwithout the need for accessing such personal information data.  That is, the various embodiments of the present technology are not rendered inoperable due to the lack of all or a portion of such personal information data.  For example, content can be\nselected and delivered to users by inferring preferences based on non-personal information data or a bare minimum amount of personal information, such as the content being requested by the device associated with a user, other non-personal information\navailable to the content delivery services, or publically available information.\nA number of implementations of the subject matter have been described.  Nevertheless, it will be understood that various modifications can be made without departing from the spirit and scope of the subject matter.", "application_number": "16120029", "abstract": " Systems, methods, and program products for determining a location of a\n     mobile device using a location application programming interface (API)\n     are described. A mobile device can receive an input requesting the mobile\n     device to monitor entry into and exit from a significant location. The\n     mobile device can call a start-monitoring instance function of an object\n     of a location manager class as declared in the API to start monitoring,\n     and call a stop-monitoring instance function of the object as declared in\n     the API to stop monitoring. The mobile device can store the entry and\n     exit, or provide a record of the entry or exit to a function that is\n     conformant to the API for performing various tasks.\n", "citations": ["6889138", "8090383", "8385944", "8990107", "9076009", "9243911", "20020161587", "20030148771", "20070016553", "20080001276", "20080012761", "20090196267", "20100304756", "20110046881", "20110137834", "20110184768", "20110195727", "20110219328", "20120052871", "20120058782", "20120088525", "20120108259", "20120149394", "20120149415", "20130066548", "20130159234", "20130252633", "20130252638", "20130315042", "20140058778", "20140094199", "20140099921", "20140141808", "20140162693", "20140164358", "20140171013", "20140229099", "20150087264", "20150199380", "20150201307", "20150237470", "20150304437", "20150350828", "20150350841", "20150350842", "20150350843", "20160003637"], "related": ["14502816", "62005897"]}, {"id": "20190007797", "patent_code": "10375522", "patent_name": "Mobile device inference and location prediction of a moving object of\n     interest", "year": "2019", "inventor_and_country_data": " Inventors: \nFolco; Rafael C. S. (Santa Barbara d'Oeste, BR), Leitao; Breno H. (Campinas, BR), Nunes do Rosario; Desnes A. (Pinheiros, BR), Santiago Filho; Jose F. (Campinas, BR)  ", "description": "<BR><BR>BACKGROUND\nThis disclosure relates generally to object tracking systems, and more specifically, to inferring a mobile device of a particular object of interest that has been identified and predicting a location of where the particular object of interest\nwill move at a future time.\nLaw enforcement officials, public safety officials, or other entities may be continually searching for objects of interest such as people.  These people may have, for example, committed serious felonies or have been reported as missing.  Whether\nthe searching includes a hot pursuit of such individuals or an extended search, it is important to catch these people as soon as possible for various reasons such as keeping a community safe, helping these people, and/or making the capturing of these\npeople as effortless and accurate as possible.\n<BR><BR>SUMMARY\nOne or more embodiments are directed to a computer-implemented method, a system, and a computer program product.  A first set of data may be received indicating that an object of interest has been identified.  A second set of data may be\nreceived indicating a first location of where the object of interest was identified.  The first location may correspond to a geographical area.  In response to the receiving of the first set of data and the second set of data, the first location may be\nassociated with a first transceiver base station.  In response to the associating, a first list of one or more mobile devices may be obtained that are within an active range of the first transceiver base station. <BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nFIG. 1 is a block diagram of a computing environment, according to embodiments.\nFIG. 2 is a diagram of an example cellular network illustrating how mobile device identifiers may be obtained in order to infer a mobile device of an object of interest, according to embodiments.\nFIG. 3A is a diagram illustrating how an object of interest's mobile device may be inferred based on identifying the object of interest in multiple cell sites, according to embodiments.\nFIG. 3B is a more detailed view of the mobile device inference of the object of interest of FIG. 3A, according to embodiments.\nFIG. 4A is a diagram illustrating how an object of interest location prediction and/or route may be estimated, according to embodiments.\nFIG. 4B is a diagram illustrating in more detail how the location prediction and/or route the object of interest in FIG. 4A may be estimated, according to embodiments.\nFIG. 5 is a flow diagram of an example process for inferring a mobile device identifier associated with an object of interest and generating a location prediction estimation or route estimation of an object of interest, according to embodiments.\nFIG. 6 is a flow diagram of an example process illustrating how a tracking system identifies an object of interest, according to embodiments.\nFIG. 7 is a block diagram of a computing device that includes a device inference module and a prediction module, according to embodiments.\nWhile the invention is amenable to various modifications and alternative forms, specifics thereof have been shown by way of example in the drawings and will be described in detail.  It should be understood, however, that the intention is not to\nlimit the invention to the particular embodiments described.  On the contrary, the intention is to cover all modifications, equivalents, and alternatives falling within the spirit and scope of the invention.\n<BR><BR>DETAILED DESCRIPTION\nAspects of the present disclosure relate to tracking systems, and more specifically, to inferring a mobile device of a particular object that has been identified.  While the present disclosure is not necessarily limited to such applications,\nvarious aspects of the disclosure may be appreciated through a discussion of various examples using this context.  For example, the aspects of the present disclosure also relate to generating a location prediction estimate of where an object of interest\nwill arrive.\nAlthough particular tracking systems, such as surveillance systems may be configured to capture images of particular objects, these data may be static and have limited use for entities such as law enforcement officials.  For example, a\nsurveillance system may capture an image of a crime.  Law enforcement officials may first have to determine that the crime occurred in a particular area and then query every nearby business entity to determine whether the business entities utilize\nsecurity cameras.  The officials may then have to run through hours of surveillance for each of the business entities to find the crime in question.  When the officials have captured the crime, they then may have to determine that the image is clear\nenough to generate suspects of the crime.  In the meantime, the person of interest who committed the crime may have rapidly moved far from the scene of the crime by the time the law enforcement officials begin serious pursuit of the person.  It may be\nuseful for tracking systems to thus quickly identify objects of interest and notify appropriate parties.\nIt may also be useful for tracking systems to estimate a route or location that a fleeing object of interest will take and/or arrive at in order to quickly capture the objects of interest and keep them from endangering a community for example.\nFurther, a mobile device identifier (e.g., a phone number) of an object of interest may be useful for entities to obtain for various reasons.  For example, if a person of interest was associated with a wanted suspect and that person of interest\nwas identified, government officials may desire to deactivate the person's mobile device, obtain Equipment Identity Register (EIR) information in order to track the mobile device (and thus the person of interest), and/or infiltrate (gain unauthorized\naccess to) the mobile device to obtain mobile data or intercept communication with other mobile devices in order to obtain knowledge of or prevent future crimes.  In another example, a runaway teenager, lost child, or other missing person (e.g., an\nelderly person with Alzheimer's) may be identified and their associated mobile device EIR data may be utilized to track and find them.\nAccordingly, embodiments of the present disclosure are directed to tracking systems that efficiently identify objects of interest, generate location prediction estimations and/or route estimations of objects of interest, and/or infer mobile\ndevice identifiers of objects of interest for the reasons stated above.  As disclosed herein, the terms \"object(s),\" \"object(s) of interest,\" or the like refers to any apparatus, a living being or any feature thereof (e.g., a human face), a machine, an\narticle of manufacture, a sound pattern (e.g., a human speech pattern), or any other suitable tangible item that a user (e.g., a person or computing device) has stored information about (e.g., speech recognition and/or facial recognition digital\nfingerprints associated with the object of interest).  For example, in some embodiments, the object of interest may be a particular missing person and/or a particular license plate that is fastened to a stolen vehicle.\nFIG. 1 is a block diagram of a computing environment 100, according to embodiments.  The computing environment 100 may include one or more of: an object of interest information provider 106, a device inference and prediction system 102 coupled\nto a datastore 104, the mobile devices 118, 114, and 116, a security camera 112 configured to identify the object of interest 110, a transceiver base station 120 (i.e., a base station, cell tower, and/or an access point in a cellular network, etc.) each\nor some of which may be communicatively coupled (e.g., via the network 128 and/or the network 108).\nThe computing environment 100 may also include various other base stations and/or security cameras that are associated with particular locations or geographical areas.  Each cell site (e.g., cell site 103) may correspond to a particular range\nthat a base station may transmit and/or receive particular signals (above or below a signal strength threshold).  The particular range may be associated with a particular location/geographical area and distance (e.g., 2 square miles of a parcel of land). In some embodiments, each base station and/or security camera of each cell site may further be communicatively coupled to the object of interest information provider 106 and/or the device inference and prediction system 102.  Accordingly, the components\nas shown to be associated with cell site 101 may be for illustrative purposes only, as the other cells' components may also communicate via the networks 108 and/or 128.\nThe computing environment 100 may also be associated with the cell sites 103, 101, 113, 111, 109, 105, and 107, as each or some of these cell sites may include their own respective base stations and/or security cameras.  In some embodiments, the\ncomputing environment 100 includes/does not include one or more components as illustrated.  For example, microphones may be utilized instead of or in addition to the security camera 112 to perform voice recognition analysis to identify the object of\ninterest 110, as described in more detail below.  In another example, the computing environment 100 may not include the object of interest information provider 106.  In some embodiments, the computing environment 100 may be implemented within a cloud\ncomputing environment, or use one or more cloud computing services.  Consistent with various embodiments, a cloud computing environment may include a network-based, distributed/data processing system that provides one or more cloud computing services. \nFurther, a cloud computing environment may include many computers, hundreds or thousands of them or more, disposed within one or more data centers and configured to share resources over the network 108 and/or 128.\nIn various embodiments, some or each of the components--e.g., the device inference and prediction system 102, the object of interest information provider 106, and the security camera 112--represent separate computing devices.  In some\nembodiments, some or each of the components represent particular compute instances of a single computing device (e.g., program modules, computing components within a chassis, a blade server within a blade enclosure, an I/O drawer, a processor chip,\netc.).  In some embodiments, some or each of the components as illustrated in FIG. 1 may represent a single computing system that includes multiple computing devices responsible for a given task.  For example, the device inference and prediction system\n102 may represent a first computing device that infers a mobile device of the object of interest 110 and also represent a second computing device that predicts what route the object of interest 110 will take for future endeavors.\nIn some embodiments, the object of interest information provider 106 corresponds to a law enforcement computing device or other system that defines what a particular object of interest is and then provides the object of interest data (e.g., to\nthe device inference and prediction system 102, the security camera 112, and/or a system associated with the base station 120).  For example, a prisoner may have escaped from prison.  In response, a user may upload the prisoner's picture (e.g., a digital\nfingerprint) to the object of interest information provider 106.  The object of interest's 110 digital picture may then be transmitted to the security camera 112 such that the security camera 112 may identify the object of interest 110 based on matching\nthe digital picture to the object of interest 110.\nIn some embodiments, the device inference and prediction system 102 corresponds to a central processing system that infers what mobile device the object of interest 110 is carrying or associated with and/or predicts where the object of interest\n110 will travel, which is described in more detail below.  These data may be based on information received from one or more security cameras and/or base stations, as described in more detail below.\nThe device inference and prediction system 102, the security camera 112, and/or the object of interest information provider 106 may communicate with each other via the network 108, which may be any suitable network such as a Personal Area\nNetwork (PAN), a local area network (LAN), a general wide area network (WAN), and/or a public network (e.g., the internet).  In some embodiments, a system of the base station 120 may also communicate with any of the components (e.g., security camera 112,\nthe object of interest information provider, etc.) via any suitable network.  In some embodiments, the device inference and prediction system 102 may communicate with a system of the base station 120 via the network 128, which may also be any suitable\nnetwork such as a cellular network or internet.  In some embodiments, the networks 128 and 108 are the same networks.\nThe security camera 112 (or other non-security camera, such as a web cam) may represent a surveillance system.  This system may be or include a computing device that stores one or more program instructions or data (e.g., facial recognition\nfingerprints, computer vision tracking algorithms, etc.).  The security camera 112 and other security cameras may be scattered throughout and area and fastened to different objects such as in traffic lights, train stations, buses, parks, telephone poles,\netc.\nFIG. 1 illustrates that the object of interest 110 may be identified and an initiation of inferring an associated mobile device and/or generating a location prediction estimation or route estimation may be made.  For example, in some\nembodiments, the object of interest information provider 106 first provides a digital fingerprint of a missing person (corresponding to the object of interest 110) to the security camera 112 (e.g., via the network 108).  The security camera 112 may then\nmatch the digital fingerprint to an object to identify the object of interest 110.  The security camera 112 may then transmit (e.g., via the network 108) a set of data to the device inference and prediction system 102 indicating that an object of\ninterest has been identified (which may include an identifier (e.g., name) of the object of interest), and that the object of interest was identified at a particular location (corresponding to cell 101 and a geographical location).  The particular\nlocation may correspond to an identifier (e.g., location 1) that is transmitted from the security camera and stored to the datastore 104.  The device inference and prediction system 102 may then associate, in response to receiving the location\nidentifier, the location with the transceiver base station 120.  For example, the datastore 104 may include a database table that specifies for every location identifier record, what the corresponding base station is in an adjacent field of the record. \nA component of the datastore 104 (e.g., a database manager) may then read the record to obtain the information.  The device inference and prediction system 102 may then transmit (e.g., via the network 128) a query message to the base station 120 system\nin order to determine active subscribers that are within a transmit/receive range (corresponding to a threshold value of signal strength) of the base station 120 (e.g., within the cell site 101).  A system of the base station 120 may then transmit and\nthe device inference and prediction system 102 may then obtain a list of the mobile devices--mobile device 114, 116, and 118--that are within an active range of the base transceiver station 120.\nFIG. 2 is a diagram of an example cellular network illustrating how mobile device identifiers may be obtained in order to infer a mobile device of an object of interest, according to embodiments.  FIG. 2 includes the mobile devices 114, 116, and\n118 of FIG. 1, a base station subsystem 205, which includes the base station 120 of FIG. 1 and a base station controller 201, a core network 207, which includes an active subscriber datastore 210, and a switching center 203.  FIG. 2 also includes the\ndevice inference and prediction system 102 of FIG. 1, which may be communicatively coupled (e.g., via the networks 128 and 208) to the base station subsystem 205.  In some embodiments, some or each cell site (e.g., cell sites 103, 105, 109, 11, 113, and\n107) may also be associated with its own base station controller, switching center, and active subscriber datastore, as illustrated in FIG. 2 for cell site 101.\nThe base station controller 201 controls one or more base stations and may perform multiple tasks.  For example, the base station controller 201 may manage radio network tasks (e.g., radio frequency control) and control handover data between\nbase stations that are under the base station controller's control.  \"Handover\" data may be data that is passed from one base station to another.  For example, when a person drives from one cell site to another while talking on a phone and/or while a\nperson has a phone in possession and the phone is emitting an active or roaming signal, handover data may be passed from base station to base station.\nThe base station controller 201 may also be the intermediary between the base station 120 and the switching center 203.  The switching center 203 (e.g., a mobile switching center (MSC)) performs communication switching functions such as call\nset-up, release of calls, routing of particular calls, routing of particular Short Message Server (SMS) text messages, communicating with other networks, and completing handover operations.  For example, the base station controller 201 may detect that\nthe mobile device 114 is falling below a signal strength threshold (i.e. approaching the edge of the cell site it is currently in) via information received from the base station 120.  The base station controller 201 may then request handover assistance\nfrom the switching center 203.  The switching center may then scan a list of adjacent cells and their associated base station controller to hand off data to the appropriate base station controller (i.e., the base station controller of where the mobile\ndevice 114 is now in or will be in).\nThe mobile switching center 203 may be communicatively couple to various datastores, such as the active subscriber datastore 210 in order to determine subscriber data (e.g., a mobile telephone number) and other information for call routing,\nobtaining active or roaming data, completing handoff operations, etc. The active subscriber datastore 210 (e.g., a Visitor Location Register (VLR) datastore) may include a list of subscribers (e.g., mobile device identifiers) associated with users (e.g.,\nthe object of interest 110) that are active or are roaming in a particular location associated with the base station 120 (i.e., the mobile devices are within a signal strength threshold of the base station 120).  To be \"active\" means that one or more\nmobile devices register (e.g., broadcast) their location, identifier, and/or signal strength (and the signal strength is above a threshold and/or within a range) to the base station 120, which is then stored in the active subscriber datastore 210. \nAccordingly, active information may specify/infer where a current location of a mobile device is, particularly where mobile device triangulation occurs via signals obtained by a plurality of base stations.  This registering may occur several times (e.g.,\n20 times) a minute to determine whether a mobile device is active in a particular cell site.  Moreover, this registering may not be able to be turned off as long as the base station 120 is receiving an active signal above a threshold regardless of\nwhether or not a mobile device is currently processing a phone call.  Accordingly, the switching center 203 may be able to determine which mobile devices are currently roaming or are active in a particular cell site.\nIn some embodiments, the switching center 203 may also include other datastores, such as a Home Location Register (HLR) database, an authentication center (AuC), and an Equipment Identity Register (EIR).  The HLR database may be associated with\nvarious subscriber details specifying which subscribers are authorized to use the core network 207.  These subscriber details may include a customer ID, current customer VLR data, subscriber status, etc. The HLR database may also store details of every\nSubscriber Identity Module (SIM) card issued by a mobile phone operation, which includes International Mobile Subscriber Identity (IMSI) of mobile devices.  These may be telephone numbers used by mobile phones to make and receive calls.  The AuC may\nauthenticate each mobile identifier attempting to connect to the core network 207.  The EIR may store a list of mobile phone identifiers (e.g., International Mobile Station Equipment Identity (IMEIs)) that are banned and/or monitored from the cellular\nnetwork.  For example, a network may monitor a mobile device IMEI in order to track a stolen mobile phone.\nFIG. 2 illustrates that the device inference and prediction system 102 may obtain some or all of the data contained in the active subscriber datastore 210.  For example, at a first time the mobile devices 114, 116, and 118 may register their\nactive location, identifiers, and/or signal strength to the base station 120.  The base station 120 may then relay this information to the switching center 203.  The switching center 203 may then store this information in the active subscriber datastore\n210.  At a second time, the device inference and prediction system 102 may query the cellular network in order to obtain the active subscriber information stored in the active subscriber datastore 210.  For example, the device inference and prediction\nsystem 102 may send a query message (e.g., via the network 128) to the switching center 203 in order to obtain all the active subscribers (mobile identifiers) within a range or signal strength threshold of the base station 120.\nAs illustrated in FIG. 1, this query message may in some embodiments be in response to the security camera 112 identifying the object of interest 110.  In order to transmit a message back to the device inference and prediction system 102, the\nswitching center 203 and/or the base station controller 201 may send a packet of active subscribers 230 to the device inference and prediction system 102 (e.g., via the network 208).  The packet of active subscribers 230 may be routed through different\nnetwork in order to, for example, send the packet over the internet.  For example, the packet of active subscribers 230 may be routed (e.g., via the node(s) 213) from the cellular network 128 or base station controller 201 to the network 208.  The node\n213 may represent one or more routing components, such as a Serving GPRS Support Node (SGSN) and a Gateway GPRS Support Node (GGSN) in a General Packet Radio Service (GPRS) network.\nFIG. 3A is a diagram illustrating how an object of interest's mobile device may be inferred based on identifying the object of interest in multiple cell sites, according to embodiments.  FIG. 3A includes cell sites 103, 101, 113, 111, 109, 105,\nand 107.  FIG. 3A also includes the object of interest 110, mobile devices 114, 116, and 118, base stations 120, 320, and 322, and security cameras 112, 312, and 314.  In some embodiments, the cell sites may be relatively small (e.g., microcells or\npicocells) such that there are more base stations and/or security cameras for a particular area.  This infrastructure may allow for better mobile device inference accuracy (or location prediction estimations) because the more an object of interest moves\nthrough more cell sites, the more mobile device inference accuracy and/or estimation accuracy a tracking system will have as explained in more detail below.\nFIG. 3A illustrates that in some situations the object of interest 110 may be a person and that person may moving from cell site to cell site (e.g., because he/she is running away), which may make mobile device inference of the object of\ninterest 110 stronger.  For example, as illustrated, at a first time the security camera 112 may identify the object of interest 110 (e.g., via pre-stored facial recognition fingerprints).  The security camera 112 may then transmit a first set of data to\nthe device inference and prediction system 102 indicating that the object of interest 110 has been identified and a second set of data indicating a first location (e.g., cell site 101) of where the object of interest was identified.  The device inference\nand prediction system 102 may then associate and in response to the receiving of the first set of data and the second set of data, the first location with the base station 120.  The device inference and prediction system 102 may then obtain a list of one\nor more mobile devices that are active within a range of the base station 120 (e.g., via the active subscriber datastore 210 of FIG. 2).  Accordingly, and as illustrated, the device inference and prediction system 102 may then determine that mobile\ndevices 114, 116, and 118 are within the active range of the base station 120, without knowing which mobile device belongs to or is associated with the object of interest 110.\nThe same process that occurred for cell site 101 may then be repeated as the object of interest 110 moves from cell site to cell site--from cell site 107 to 109.  Therefore, when the object of interest 110 moves into cell site 107, the security\ncamera 312 may identify the object of interest 110 and the mobile devices that are within an active range of the base station 320 may be obtained.  The device inference and prediction system 102 may then determine which mobile device identifiers\nassociated with base station 120 are in common (are shared with/are the same as/are identical to) with the base station 320.  As illustrated, mobile devices 116 and 118 are within an active range of the base station 320, which were also within an active\nrange of the base station 120 (i.e., they are in common with each other).  The object of interest 110 may yet again move into another cell site--cell site 109.  The security camera 314 may then identify the object of interest 110 and the mobile devices\nthat are within an active range of the base station 322 may be obtained.  The device inference and prediction system 102 may then determine which device identifiers associated with the base station 120 and/or 320 are in common with the base station 322. \nAs illustrated, the only mobile device that is in common with base stations 101 and 107 is mobile device 116.  Accordingly, a strong inference can be made that the object of interest 110 is in possession of or associated with mobile device 116. \nTherefore, entities, such as law enforcement agencies may then inactivate, track, and/or infiltrate the mobile device 116.\nIn some embodiments, instead of or in addition to determining which mobile device identifiers associated with a particular base station are in common with another base station, timestamp intervals between identification events may be utilized to\ninfer mobile devices associated with objects of interest.  For example, if the security camera 112 identified the object of interest 110 at a first time, and then the security camera 312 identified the object of interest 110 at a second time, the time\nthat has passed between the first and second time (e.g., 5 minutes) may be utilized to filter the mobile devices that handed off from base stations 120 to 320 within/between the first and second time intervals.  Accordingly, a snapshot (an identification\nof) of each active mobile device identifier that was handed off from bases stations 120 to 320 may be take in order to analyze or identify (e.g., by a user) what mobile device identifiers may be associated with the object of interest 110.\nFIG. 3B is a more detailed view of the mobile device inference of the object of interest of FIG. 3A, according to embodiments.  Each mobile device--mobile device 114, mobile device 116, and mobile device 118--within an active range of the base\nstation 120 may broadcast or send an active signal to the base station 120 (which is then relayed and stored to the active subscriber datastore 210 as shown in FIG. 2).  Each of these mobile devices may be possessed by various people as illustrated.  At\nthe first time, when the security camera 112 identifies the object of interest 110, the device inference and prediction system 102 may then obtain (query the cellular network of FIG. 2) each active subscriber or mobile device identifier (e.g., IMSI) from\nthe cellular network associated with base station 120.  As illustrated mobile devices 114, 116, and 118 may each be within an active signal strength range of the base station 120.\nWhen the object of interest 110 walks into cell site 107, the security camera 312 may then identify the object of interest 110 and another query may be made to the cellular network associated with the base station 320.  Mobile devices 424, 116,\n118, 420, and 422 may each register their active location to the cellular network, which is then obtained by the device inference and prediction system 102.  The base station 120 may have provided handoff information (e.g., mobile devices 114 and 116) to\nthe base station 320 as a signal strength fell below a threshold for the base station 120 and rose above a threshold for the base station 320.  The device inference and prediction system 102 may then determine that the mobile device identifiers\nassociated with base station 120 that are in common with base station 320 are mobile devices 116 and 118.\nFurther, when the object of interest 110 walks into the cell site 109, the security camera 314 may then identify the object of interest 110 and yet another query may be made to the cellular network associated with the base station 322.  Mobile\ndevices 428, 116, 426, 432, and 430 may each register their active location to the cellular network, which is then obtained by the device inference and prediction system 102.  The base station 320 may have provided handoff information (e.g., mobile\ndevice 116) to the base station 322 as a signal strength fell below a threshold for the base station 320 and rose above a threshold for the base station 322.  The device inference and prediction system 102 may then determine that the only mobile device\nidentifier that was originally associated or registered with base station 120 and/or 320 that are in common with the base station 322 is mobile device 116.  Accordingly, the device inference and prediction system 102 may then infer that the object of\ninterest 110 is possessing the mobile device 116.  In some embodiments, after the device inference and prediction system 102 infers the mobile device 116 it may then notify or alert the object of interest provider 106, which may correspond to a law\nenforcement agency computing device.  In some embodiments, the notification may include various data, such as the mobile device identifier (mobile telephone number) of the object of interest, the identity of the object of interest, the general location\nof the object of interest 110 (e.g., somewhere within cell site 109), the exact location of the object of interest 110 (e.g., via triangulation methods), a location and/or route estimate of the object of interest 110, etc. Each of these notification data\nare described in more detail below.  In some embodiments, the operations described in FIGS. 3A and 3B may instead be performed by computing devices that include security cameras.\nFIG. 4A is a diagram illustrating how an object of interest location prediction and/or route may be estimated, according to embodiments.  FIG. 4A includes cell sites 403, 401, 413, 411, 409, 405, and 407.  There are also streets that cross\nthrough or over these cell sites--Power Rd.  S. starts from cell site 411 and goes through cell site 407 and becomes Power Rd.  N. in cell site 403.  Two streets fork off Power Rd.  at cell site 407--Brown Rd.  forks off from Power Rd.  starting from\ncell site 407 and goes into cell site 401.  And Ellsworth Rd.  also forks off from Power Rd.  starting from cell site 407 and goes into cell site 405.  The object of interest may be a license plate identifier that is coupled to the object of interest\nvehicle 420.  The object of interest vehicle 420 may be identified to be starting in a position south of cell site 411 and heading in a northern direction on Power Rd.  S. As described in more detail below, a system associated with the law enforcement\nvehicle (e.g., device inference and prediction system 102 and/or the object of interest information provider 106) may have provided information to an occupant of the law enforcement vehicle 422 indicating that the object of interest vehicle 420 will most\nlikely keep travelling north on Power Rd.  S. and take a right on Brown Rd., instead of a left on Ellsworth Rd.  or north on Power Rd.  N. Accordingly, the law enforcement vehicle 422 may be oriented in a blocking or stopping position outside of cell 401\nat Brown Rd.  so as to provide a major road block, and/or be in position to utilize any other halting methods such as placing spike strips on the street to stop the object of interest vehicle 420.\nThe object of interest license plate number or identifier may first be located by a security camera south of cell site 411.  The license plate number may correspond to a stolen vehicle, vehicle of a person of interest, etc. In some embodiments,\na specific location estimate may be generated (e.g., by the device inference and prediction system 102) after the object of interest and/or a mobile device of the object of interest has been identified and associated with a particular location and base\nstation or cell site.  For example, after a mobile device of an object of interest has been identified or inferred (e.g. via the methods described in FIGS. 3A and 3B), the tracking system may infiltrate the mobile device and obtain Global Positioning\nSystems (GPS) data such that coordinates of where the mobile device is located (which may be in possession by a person in the object of interest vehicle 420) may be obtained.  Alternatively, once the mobile device is inferred the mobile identifier may be\nblacklisted and EIR data (e.g., IMEIs) may be obtained in order to monitor or track the inferred mobile device IMEI such that a particular position of the object of interest may be inferred.\nIn some embodiments cell tower triangulation methods may be utilized to infer a particular position of a mobile device associated with an object of interest.  This is based on the principle that multiple base stations may receive signals from a\nparticular mobile device regardless of what cell site the mobile device is in. And based on a signal strength connection between particular base stations and the mobile device and the base station sectors that receive the signals, a particular location\ncan be inferred.  For example, there may be three base stations that are picking up a signal from a mobile device and each base station may have three triangular sectors (i.e., Alpha, Beta, and Gamma sectors) that receive signals associated with a\nparticular orientation range (e.g., Northeast).  The Gamma sector of a first base station may be picking up a signal at a particular signal strength value (or may be picking up a time delay value a signal takes to return to the Gamma sector).  This may\ncorrespond to a distance between the mobile device and the first base station.  A Beta sector of a second base station may be picking up a signal at a particular signal strength value for the same mobile device.  Further, an Alpha sector of a third base\nstation may be picking up a signal at a particular signal strength value for the same mobile device.  Each of these signal strength value readings, sector orientations, and distance inferences may be consolidated in order to pinpoint a location of a\nmobile device (which an object of interest may be in possession of).\nIn some embodiments, once an object of interest and/or inferred mobile device has been located (e.g., via GPS coordinates or by cell tower triangulation), various other information may be obtained (e.g., GPS data) that specifies what\ngeographical features (e.g., Power Rd.  S., Brown Rd., Power Rd.  N., Ellsworth Rd., etc.) surround the object of interest and/or inferred mobile device.  Other data (e.g., speed of the object of interest, the direction of travel of the object of\ninterest, etc.) may also be obtained (e.g., by a radar within a security camera or an infrared range finder, etc.) in order to generate a location prediction estimate of where the object of interest will move at a future time, as described in more detail\nbelow.\nFIG. 4B is a diagram illustrating in more detail how a location prediction and/or rout the object of interest in FIG. 4A may be estimated, according to embodiments.  As disclosed herein a \"location prediction estimation\" may be an estimated\ncalculation of where (e.g., specific GPS coordinates, base station location, triangulated position, etc.) an object of interest will move to or arrive at a future time given a particular set of data (e.g., the starting point, direction of travel, the\nroad an object of interest is on, etc.).  A \"route estimation\" as disclosed herein may refer to a route (streets driven on, direction traveled, etc.) that an object of interest will take in the midst of travelling to a particular destination.  The\nlocation prediction or route estimation may be made in any suitable manner using various data mining techniques, such as hidden Markov models, Kalman filters, or any other method.\nFor example, the location prediction estimation and/or route estimation may be calculated using a Dynamic Bayesian Network (DBN), as illustrated by the graph 400.  The graph 400 includes various nodes--411, 409, 413, 407, 405, 403, and 401--that\ndirectly correspond to the cell cites 411, 409, 413, 407, 405, 403, and 401 of FIG. 4A.  DBNs utilize probability theory to predict an outcome and is premised on the paradigm that an environment can be viewed as a series of snapshots or time slices (t). \nDBN assumes that a same subset of variables is observable in each time slice.  The variables (Xt) denote the set of variables (i.e., nodes 411, 409, 413, 407, 405, 403, or 401) at time t, which may be assumed to be unobservable.  The variables e.sub.t\n(or E.sub.t) denote a set of observable evidence variables (e.g., security camera identification matches in each cell site), which may include multiple types of evidence variables (e.g., vehicle speed, traffic data, road layout, etc.), as described in\nmore detail below.\nIn a DBN, the location prediction estimation and/or route estimation may be calculated by inference techniques such as filtering and prediction.  \"Filtering\" is associated with the principle of computing a \"belief state\"--the posterior\ndistribution over a most recent state--given all the evidence to date.  \"Prediction\" is the principle of computing the posterior distribution over a future state, given all of the evidence to date.  Filtering and prediction in the context of FIGS. 4A and\n4B may be represented as follows: P(X.sub.(t+k)|e.sub.(1:t)), where k&gt;0 and is the quantity of steps predicted into the future, and where P is the probability.  Thus, the device inference and prediction system 102 or other component may compute, for\nexample, the probability that the object of interest vehicle 420 will arrive in cell site 405 in 5 minutes compared to other cell sites, given all of the evidence observations.  Prediction is therefore useful for evaluation possible courses of action\nbased on their expected outcomes.\nThe location prediction estimation and/or route estimation may further be estimated through a \"most likely explanation\" model in addition to or instead of the filtering and prediction methods described above.  These models provide that given a\nsequence of observations, it may be desirable to find the sequence of states that is most likely to have generated one or more of the observations.  This may be represented by computing: argmax.sub.(x.sub.(1:t).sub.)P(X.sub.(1:t)|e.sub.(1:t)).\nThese calculations may be utilized to, for example, explain why the object of interest license plate attached to the object of interest vehicle 420 was identified in some cell sites but not others.  For example, if it was observed that the\nobject of interest vehicle 420 was identified going north on Power Rd.  S. through the cell sites 411 and 407 at slow speeds (e.g., 30 MPH), but then accelerated to a fast speed (e.g., 90 MPH) at the northern end of cell site 407, and was beginning to\nveer right from cell cite 407 onto Brown Rd., yet was not identified in cell site 401, then a most likely explanation may be that the object of interest vehicle 420 was travelling too fast to be identified within cell site 401.  And if this occurs, in\nsome embodiments, the device inference and prediction system 102 or other component may still generate location prediction estimations and/or route estimations, particularly where the system is making these \"most likely explanation\" inferences.  Thus an\nobject of interest does not necessarily need to be identified in every cell site it enters to make inferences.\nBecause DBNs may assume the probability of future conditionally independent events P(X.sub.t|X.sub.(0: t-1))=P(X.sub.t|X.sub.(t-1)), it can also be considered that after identifying an object of interest on G.sub.t, the inferring of\nP(A.sub.T|G.sub.T), P(F.sub.T|G.sub.T) and P(E.sub.T|G.sub.T) would not depend on C.sub.t, B.sub.t, or D.sub.t, where G=node 407, A=node 405, F=node 403, E=node 401, C=node 411, B=node 409, and D=node 413.  This may narrow down considerably the\npossibilities of where the object of interest vehicle 420 will go.  For example, given that the object of interest vehicle 420 is travelling north on Power Rd.  S., and is approaching cell site 411, and there is only the road configuration available as\nillustrated, it must follow that the object of interest must either take a left at Ellsworth, keep going straight on Power Rd.  N., or take a right at Brown Rd.  Given these three possibilities, the system can make even more inferences within this\nthree-choice pool, as described in more detail below.  Accordingly, instead of predicting that the object of interest vehicle 420 has a 33.33 percent chance (an equal chance) of taking one of these three destinations, more calculations may be done to\nfurther generate a location prediction estimate and/or route estimate.\nFIGS. 4B (and 4A) illustrates how a location prediction estimate and/or route estimate predicts that the object of interest vehicle 420 will keep travelling north through cell sites 411 and 407 and then through cell site 401.  The graph 400\nillustrates a relationship between any two variables denoted by an arrow connecting one node with another node.  The \"strength\" of the relationship may be denoted by the thickness of the arrow connecting the nodes.  The graph 400 illustrates that the\nstrongest relationship is between nodes 411 and 407.  The graph 400 also illustrates that there is no relationship between any node and nodes 409 or 413.  Accordingly, for example, given that the object of interest vehicle 420 is within the cell site\n411, there is a strong probability (e.g., 95%) that the object of interest vehicle will travel north into cell site 407 and not deviate into cell sites 409 or 413.  This strong prediction may be made by making particular observations and compounding\nvarious evidence variables.  For example, observations may be made (e.g., by a security camera and/or device inference and prediction system 102) that the object of interest vehicle 420 is currently heading north at 50 miles per hour, that there is only\none major road that connects cell site 411 to 407 (i.e., Power Rd.  S.), that the object of interest has been spotted in the past heading to cell site 407 via Power Rd.  S. (instead of taking side residential roads), etc.\nFIGS. 4B (and 4A) also illustrates that there is varying relationships between node 407 and node 405 (the weakest relationship), node 407 and node 403, and node 407 and node 401 (a stronger relationship than nodes 407 and 405).  Accordingly,\nwhen the object of interest vehicle 420 is travelling north and is in cell site 407, it may: travel northwest (via Ellsworth Rd.) into cell site 405, travel north (via Power Rd.  N.) into cell site 403, or travel northeast (via Brown Rd.) into cell site\n401.\nEven though theoretically the object of interest 420 may take any of these three roads, various data may be obtained in order to infer which route/destination the object of interest vehicle 420 will likely take.  Such data may include one or\nmore of: traffic data, traffic light data (e.g., whether a traffic light on a particular street is or will turn red), speed limit(s) posted on particular street(s), directions of location interest(s) associated with the object of interest (e.g., an\naddress of the object of interest), real time data provided by a mobile device associated with the object of interest (e.g., text messages, phone calls, etc.), a history of where an object of interest has been, etc. In some embodiments, these data may\nweighted or scored differently in order to rank or prioritize the importance of data.  For example, real time data may be scored higher or be weighted more than any other data because it may reflect an interest or intent of a person associated with the\nobject of interest 420 at a particular moment in time.\nIn an example illustration, the object of interest vehicle 420 may have a 60% chance of taking a right on Brown Rd.  and thus going through cell site 401 (node 401) instead of a 30% chance of going straight on Power Rd.  N. and a 10% chance of\ngoing left at Ellsworth because of various data.  For example, the traffic data (e.g., obtained by the device inference and prediction system 102 and/or security camera) may indicate that Brown Rd.  has the least amount of traffic (e.g., a least quantity\nof cars that have passed over a given point (a point where a security camera is) for a particular duration of time).  A location interest, such as a home address, associated with a person driving the object of interest 420 may be off of Brown Rd.  or be\nat cell site 401.  This location interest may be obtained in various manners.  For example, a mobile phone identifier associated with an object of interest may be inferred (e.g., via methods analogous to FIG. 3B).  After inference, a system (e.g., the\ndevice inference and prediction system 102 and/or the object of interest information provider 106) may query a registration provider or network associated with the mobile device in order to get a home address.  For example, referring back to FIG. 2, in\nsome embodiments, subscriber datastores may also include a home address associated with each user or mobile device.  Accordingly, if the device inference and prediction system 102 queried the cellular network, the active subscriber datastore 210 may\ninclude a home address, which may then be forwarded to the device inference and prediction system 102, the security camera 112, and/or the object of interest information provider 106.  Alternatively, an object of interest may be a person's face.  The\ncorresponding facial digital fingerprint may be stored and associated with a particular person of interest's home address (e.g., via the object of interest information provide 106).\nIn some embodiments, real time data may indicate that the object of interest vehicle 420 will go down Brown Rd.  instead of any other road.  For example, if the mobile device associated with the object of interest was inferred, the mobile phone\nmay be infiltrated to obtain data.  Accordingly, for example, an object of interest associated with the mobile phone may text or call another person indicating that he/she is currently on her way to a location, which may be at or associated with cell\nsite 401.  And this data may be accessed (e.g., by the device inference and prediction system 102) and utilized as a factor to generate a location prediction estimation and/or route estimation.\nIn some embodiments, historical data indicating where an object of interest has been identified in the past may be utilized to generate a location prediction estimation and/or route estimation.  For example, the object of interest vehicle 420\nmay have been identified two weeks ago and four weeks ago heading northeast on Brown Rd.  around the intersection of Power Rd and may not have historically been identified taking Ellsworth Rd.  or Power Rd.  N. It may therefore be inferred that the\nobject of interest will once again travel down Brown Rd.  In some embodiments, this historical data may also or instead be utilized to infer a mobile device identifier.  For example, using the illustration above, a mobile device identifier for a\nparticular cell site(s) that was in common with the two week identification, the four week identification, and the current identification, may be inferred to be associated with an object of interest.  Therefore, for each reason stated above, node 407 may\nindicate a strong relationship with node 401 thereby providing enough data such that the law enforcement vehicle 422 may be located outside of cell site 401 in order to capture or stop the object of interest vehicle 420.\nFIG. 5 is a flow diagram of an example process for inferring a mobile device identifier associated with an object of interest and generating a location prediction estimation or route estimation of an object of interest, according to embodiments. In some embodiments, the example process may begin at block 502 when data is received (e.g., by the device inference and prediction system 102 and/or the security camera 112) corresponding to an identity of an object of interest.  For example, a security\ncamera system may receive a digital fingerprint associated with an object of interest's facial identity from the object of interest information provider 106.\nPer block 504, a first set of data may be received (e.g., by the device inference and prediction system 102 and/or the security camera 112) indicating that the object of interest has been identified.  For example, a security camera system may\nmatch the digital fingerprint data received at block 502 to an object of interest and transmit the first set of data to the device inference and prediction system 102.  Per block 506, a second set of data may be received indicating a first location of\nwhere the object of interest was identified.  The first location may correspond to a geographical area.  For example, a security camera may have matched a digital fingerprint with an object of interest along with a location identifier indicating where\nthe matching occurred.\nPer block 508, the first location may be associated with a first base station (e.g., by the security camera 112 and/or the device inference and prediction system 102).  This may be in response to the receiving of the first set of data and the\nsecond set of data.  For example, the datastore 104 of FIG. 1 may include a record that specifies the location that it was provided by the security camera and the corresponding base station ID.  Per block 510, it may be determined whether any mobile\ndevices are active within the first base station range (e.g., a signal strength threshold).  If no mobile devices are active within the first base station, a counter may be set (e.g., for 2 minutes) to continually poll a cellular network to determine\nwhether any mobile devices become active.  If no mobile device are still found to be active, then block 514 may occur.  Per block 512, if one or more mobile devise are active within the first base station range, then a first list of one or more mobile\ndevices within the active range of the first base station may be obtained (e.g., by the security camera 112 and/or the device inference and prediction system 102).  This may be in response to the associating at block 508.  The \"list\" of mobile devices\nmay include mobile device identifiers (e.g., telephone numbers) of the mobile devices that are active.\nPer block 514 various estimation data may be generated (e.g., by the security camera 112 and/or the device inference and prediction system 102).  For example, one or more components may: generate a location estimation to estimate where exactly\nthe object of interest is (e.g., via cell tower triangulation, GPS coordinates, etc.), infer a mobile device identifier of the object of interest or generate a mobile device identifier estimate of the object of interest, generate a location prediction\nestimation/route estimation of where the object of interest will go.  In some embodiments, a time estimate of how fast the object of interest will arrive at a particular location may be generated as well.  For example, the security camera 112 may include\na speed radar module configured to estimate a velocity of an object of interest and based on the velocity and the direction of the velocity, the time estimate of where the object of interest will go may be generated.  The time estimate may also be\nincluded in the operations specified in block 536.\nPer block 516, a history of where the object of interest has been identified may be stored (e.g., to the security camera 112, the object of interest information provider 106 and/or the device inference and prediction system 102).  This\ninformation may be utilized to generate a location prediction estimation or route estimation of an object of interest at a later time.  For example, if the object of interest was spotted several times going from area 1 to area 2, this may be stored data. The next time the object of interest is identified in area 1, it may be predicted that the object of interest will go into area 2.  This historical data may be utilized for \"machine learning\" in intelligent systems.  Machine learning is the ability learn\nor recognize patterns without receiving explicit user input.\nPer block 518, a computing device (e.g., the object of interest information provider 106 and/or the device inference and prediction system 102) may be notified that the object of interest has been identified.  For example, after a security\ncamera identifies a particular object of interest, the security camera may notify the object of interest information provider 106 corresponding to law enforcement.  This notification may include various sets of information, such as what and where the\nobject of interest was located, a location prediction estimation, mobile device identifier inferences, etc.\nPer block 520, it may be determined whether the object of interest was identified at a second location.  Per block 522, if the object of interest was identified at the second location then another set of data may be received indicating that the\nobject of interest has been identified at the second location.  Per block 524, the second location may be associated with a second base station.  Per block 526, a second list of one or more mobile devices within an active range of the second base station\nmay be obtained.\nPer block 528, it may be determined (e.g., by the device inference and prediction system 102 and/or the security camera 112) whether any mobile devices are in common (e.g., the same as) between the first and second base stations (e.g., via the\noperations specified in FIG. 3B).  Per block 530, if there are any mobile devices in common between the first and second base stations then a mobile device identifier associated with the object of interest (a mobile device in possession of the object of\ninterest) may be inferred based on the determining which mobile device identifiers generated by the first base station are in common with the second base station.  And based on the receiving the set (the third set) of data at block 522.  For example,\nreferring back to FIG. 3B, if only one mobile device identifier--mobile device 116--was found to be in common with those identified around the base station 120, then it may be inferred that mobile device 116 is associated with an object of interest. \nAlternatively, per block 532, a mobile device identifier estimate of the object of interest may be generated.  For example, referring back to FIG. 3B, if by the time the object of interest arrived at the base station 322 and both mobile devices 116 and\n118 were both identified in cell site 109, then it may be estimated that either the mobile devices 116 or 118 may belong to the object of interest 110, as these two mobile device identifiers were the only common identifiers identified at cell site 101.\nPer block 534, a location estimation to estimate where exactly the object of interest is may be generated (e.g., by the security camera 112, the object of interest provider 106, and/or the object of interest information provider 106).  For\nexample, as described above, the exact (or near exact) location may be generated via GPS coordinates, cell tower triangulation, etc. Per block 536, a location prediction estimate may be generated (e.g., by the device inference and prediction system 102,\nthe security camera 112, and/or the object of interest information provider) of where the object of interest will move at a future time may be made.  As discussed above, this estimation may be generated using various data such as traffic data, handoff\ndata, history of where the object of interest has been identified, etc.\nFIG. 6 is a flow diagram of an example process 600 illustrating how a tracking system identifies an object of interest, according to embodiments.  The process 600 may begin at block 602 when a device (e.g., a surveillance system or security\ncamera) identifies an object of interest.  The identification may occur in any suitable manner according to various embodiments.  For example, the security camera 112 may be a computing device that stores various object of interest fingerprints.  These\nfingerprints may include digital patterns that directly correspond to a person of interest's facial pattern or other objects.  For example, these fingerprints may be or include a computer-aided design (CAD) that may be a graphical representation of an\nobject of interest.  The security camera 112 may continuously monitor or scan its environment in order to potentially match the fingerprints to an object of interest.  The matching or visual identification may occur in any suitable manner.  For example,\nthe security camera 112 may continuously run computer vision tracking or object recognition algorithms such as Principal Component Analysis (PCA), Hidden Markov Model, dynamic link matching, etc. in order to identify an object of interest.\nIn some embodiments, the identification of the object of interest at block 602 may occur via microphone instead of or in addition to security cameras.  For example, the security camera 112 may include a microphone configured to pick up voice\npatterns of objects or people.  The security camera 112 may include a speech recognition or natural language processing (NLP) module configured to identify words or sentences a person is saying based on a sound input (e.g., processed by a \"most likely\nexplanation\" algorithm described above for DBN networks) of that person.  In an example illustration, the security camera may store coined phrases or voice fingerprints associated with an object of interest.  For example, an object of interest may always\nsay a particular unique phrase.  That unique phrase may be stored to the security such that any time the security camera identifies the particular phrase (e.g., via NLP and a microphone), the object of interest may be identified.  In other examples, the\nobject of interest's voice itself regardless of the content may be identified via voice matching algorithms (e.g., via Mel Frequency Cepstral Coefficient (MFCC) and/or Dynamic Time Warping (DTW) algorithms).\nIn some embodiments, the object of interest may already be known (e.g., by law enforcement officers) before the object of interest is identified by a tracking system.  For example, particular objects of interest may be missing or wanted for a\ncrime.  Accordingly, the object of interest information provider 106, for example, may upload object of interest fingerprints to the security camera 112 so that the security camera 112 knows what to identify.  Alternatively, objects of interest may not\nbe known by entities but may be dynamically identified based on one or more real time input data that the tracking system receives from an object.  For example, the security camera 112 may identify (e.g., via NLP and a microphone) particular words or\nsentences that are associated with particular crimes.  For example, a NLP module and microphone may identify a string of words \"I'm going to rob the store tonight,\" and associate this string with an actionable response such as notifying the object of\ninterest information provider 106 and/or the device inference and prediction system 102.  The person saying this phrase could then become an object of interest.  The security camera 112 could then, for example, take a snapshot digital picture of the\nobject of interest and store (or transmit) such information in order to track such individual through various cell sites, as described above.\nPer block 606, and in some embodiments, data may be transmitted indicating the identity of the object of interest and that the object of interest was identified at a first location.  For example, the security camera 112 may capture image\ninformation of the object of interest to identify the object of interest and then transmit a location identifier and identity of the object of interest to the device inference and prediction system 102.  In other embodiments however, per block 604 the\nfirst location may be associated with a first base station.  And per block 608, a first list of one or more mobile device within an active range of the first base station may be obtained.  Accordingly, the security camera 112, for example may be the\ncomponent (e.g., instead of or in addition to the device inference and prediction system 102) that does the associating or the obtaining of lists.\nFIG. 7 is a block diagram of a computing device 700 that includes a device inference module 702, a prediction module 704, and object of interest fingerprints 706, according to embodiments.  In some embodiments, the computing device 700\nrepresents the device inference and prediction system 102, the security camera 112, and/or the object of interest information provider 106.  The components of the computing device 700 can include one or more processors 06, a memory 12, a terminal\ninterface 18, a storage interface 20, an Input/Output (\"I/O\") device interface 22, and a network interface 24, all of which are communicatively coupled, directly or indirectly, for inter-component communication via a memory bus 10, an I/O bus 16, bus\ninterface unit (\"IF\") 08, and an I/O bus interface unit 14.\nThe computing device 700 may include one or more general-purpose programmable central processing units (CPUs) 06A and 06B, herein generically referred to as the processor 06.  In an embodiment, the computing device 700 may contain multiple\nprocessors; however, in another embodiment, the computing device 700 may alternatively be a single CPU device.  Each processor 06 executes instructions stored in the memory 12 (e.g., the device inference module 702).\nThe computing device 700 may include a bus interface unit 08 to handle communications among the processor 06, the memory 12, the display system 04, and the I/O bus interface unit 14.  The I/O bus interface unit 14 may be coupled with the I/O bus\n16 for transferring data to and from the various I/O units.  The I/O bus interface unit 14 may communicate with multiple I/O interface units 18, 20, 22, and 24, which are also known as I/O processors (IOPs) or I/O adapters (IOAs), through the I/O bus 16. The display system 04 may include a display controller, a display memory, or both.  The display controller may provide video, audio, or both types of data to a display device 02.  The display memory may be a dedicated memory for buffering video data. \nThe display system 04 may be coupled with a display device 02, such as a standalone display screen, computer monitor, television, a tablet or handheld device display, or another other displayable device.  In an embodiment, the display device 02 may\ninclude one or more speakers for rendering audio.  Alternatively, one or more speakers for rendering audio may be coupled with an I/O interface unit.\nIn alternate embodiments, one or more functions provided by the display system 04 may be on board an integrated circuit that also includes the processor 06.  In addition, one or more of the functions provided by the bus interface unit 08 may be\non board an integrated circuit that also includes the processor 06.\nThe I/O interface units support communication with a variety of storage and I/O devices.  For example, the terminal interface unit 18 supports the attachment of one or more user I/O devices, which may include user output devices (such as a video\ndisplay devices, speaker, and/or television set) and user input devices (such as a keyboard, mouse, keypad, touchpad, trackball, buttons, light pen, or other pointing devices).  A user may manipulate the user input devices using a user interface, in\norder to provide input data and commands to the user I/O device 26 and the computing device 700, may receive output data via the user output devices.  For example, a user interface may be presented via the user I/O device 26, such as displayed on a\ndisplay device, played via a speaker, or printed via a printer.\nThe storage interface 20 supports the attachment of one or more disk drives or direct access storage devices 28 (which are typically rotating magnetic disk drive storage devices, although they could alternatively be other storage devices,\nincluding arrays of disk drives configured to appear as a single large storage device to a host computer, or solid-state drives, such as a flash memory).  In another embodiment, the storage device 28 may be implemented via any type of secondary storage\ndevice.  The contents of the memory 12, or any portion thereof, may be stored to and retrieved from the storage device 28 as needed.  The storage devices 28 may be employed to store any of the databases or data store data described herein.  The I/O\ndevice interface 22 provides an interface to any of various other I/O devices or devices of other types, such as printers or fax machines.  The network interface 24 provides one or more communication paths from the computing device 700 to other digital\ndevices and computer systems.\nAlthough the computing device 700 shown in FIG. 7 illustrates a particular bus structure providing a direct communication path among the processors 06, the memory 12, the bus interface 08, the display system 04, and the I/O bus interface unit\n14, in alternative embodiments the computing device 700 may include different buses or communication paths, which may be arranged in any of various forms, such as point-to-point links in hierarchical, star or web configurations, multiple hierarchical\nbuses, parallel and redundant paths, or any other appropriate type of configuration.  Furthermore, while the I/O bus interface unit 14 and the I/O bus 08 are shown as single respective units, the computing device 700, may include multiple I/O bus\ninterface units 14 and/or multiple I/O buses 16.  While multiple I/O interface units are shown, which separate the I/O bus 16 from various communication paths running to the various I/O devices, in other embodiments, some or all of the I/O devices are\nconnected directly to one or more system I/O buses.\nIn various embodiments, the computing device 700 is a multi-user mainframe computer system, a single-user system, or a server computer or similar device that has little or no direct user interface, but receives requests from other computer\nsystems (clients).  In other embodiments, the computing device 700 may be implemented as a desktop computer, portable computer, laptop or notebook computer, tablet computer, pocket computer, telephone, smart phone, smart watch, or any other suitable type\nof electronic device.\nIn an embodiment, the memory 12 may include a random-access semiconductor memory, storage device, or storage medium (either volatile or non-volatile) for storing or encoding data and programs.  In another embodiment, the memory 12 represents the\nentire virtual memory of the computing device 700, and may also include the virtual memory of other computer systems coupled to the computing device 700 or connected via a network 30.  The memory 12 may be a single monolithic entity, but in other\nembodiments the memory 12 may include a hierarchy of caches and other memory devices.  For example, memory may exist in multiple levels of caches, and these caches may be further divided by function, so that one cache holds instructions while another\nholds non-instruction data, which is used by the processor.  Memory 12 may be further distributed and associated with different CPUs or sets of CPUs, as is known in any various so-called non-uniform memory access (NUMA) computer architectures.\nThe memory 12 may store all or a portion of the components and data (e.g., the device inference module 702) shown in FIG. 7.  These programs and data are illustrated in FIG. 7 as being included within the memory 12 in the computing device 700;\nhowever, in other embodiments, some or all of them may be on different computer systems and may be accessed remotely, e.g., via a network 30.  The computing device 700 may use virtual addressing mechanisms that allow the programs of the computing device\n700 to behave as if they only have access to a large, single storage entity instead of access to multiple, smaller storage entities.  Thus, while the components and data shown in FIG. 7 are illustrated as being included within the memory 12, these\ncomponents and data are not necessarily all completely contained in the same storage device at the same time.  Although the components and data shown in FIG. 7 are illustrated as being separate entities, in other embodiments some of them, portions of\nsome of them, or all of them may be packaged together.\nIn an embodiment, the components and data shown in the memory 12 of FIG. 7 (the device inference module 702, the prediction module 704, and/or the object of interest fingerprints 702) may include instructions or statements that are\nexecutable/readable on the processor 06 or instructions or statements that are interpreted by instructions or statements that execute/read on the processor 06 to carry out the functions as described above.  In another embodiment, the components shown in\nFIG. 7 may be implemented in hardware via semiconductor devices, chips, logical gates, circuits, circuit cards, and/or other physical hardware devices in lieu of, or in addition to, a processor-based system.  In an embodiment, the components shown in\nFIG. 5 may include data in addition to instructions or statements.\nIn some embodiments, the device inference module 702 is configured to infer a mobile device or generate a mobile device estimate of an object of interest.  The device inference module 702 may be further configured to perform some or each of the\noperations and/or block functions as specified in FIGS. 1, 2, 3A, 3B, 4A, 4B, 5, and/or 6.  In some embodiments, the prediction module 704 may be configured to generate a location prediction estimate of where the object of interest is and/or will be. \nThe prediction module 704 may be further configured to perform some or each of the operations and/or block functions as specified in FIGS. 1, 2, 3A, 3B, 4A, 4B, 5, and/or 6.  The object of interest fingerprints 706 may be digital pictures/fingerprints\nthat correspond and are used to identify an object of interest.  The object of interest fingerprints 706 may in addition or instead be stored to the storage device 28 (e.g., a database).  In some embodiments, the device inference module 702, the\nprediction module 704, and/or the object of interest fingerprints 706 may be combined to form a single module instead of separate modules.  This single module may perform each of the operations and/or blocks as specified in each figure.\nFIG. 7 is intended to depict representative components of the computing device 700.  Individual components, however, may have greater complexity than represented in FIG. 7.  In FIG. 7, components other than or in addition to those shown may be\npresent, and the number, type, and configuration of such components may vary.  Several particular examples of additional complexity or additional variations are disclosed herein; these are by way of example only and are not necessarily the only such\nvariations.  The various program components illustrated in FIG. 7 may be implemented, in various embodiments, in a number of different ways, including using various computer applications, routines, components, programs, objects, modules, data pages etc.,\nwhich may be referred to herein as \"software,\" \"computer programs,\" or simply \"programs.\"\nAspects of the present invention may be a system, a method, and/or a computer program product.  The computer program product may include a computer readable storage medium (or media) having computer readable program instructions thereon for\ncausing a processor to carry out aspects of the various embodiments.\nThe computer readable storage medium can be a tangible device that can retain and store instructions for use by an instruction execution device.  The computer readable storage medium may be, for example, but is not limited to, an electronic\nstorage device, a magnetic storage device, an optical storage device, an electromagnetic storage device, a semiconductor storage device, or any suitable combination of the foregoing.  A non-exhaustive list of more specific examples of the computer\nreadable storage medium includes the following: a portable computer diskette, a hard disk, a random access memory (RAM), a read-only memory (ROM), an erasable programmable read-only memory (EPROM or Flash memory), a static random access memory (SRAM), a\nportable compact disc read-only memory (CD-ROM), a digital versatile disk (DVD), a memory stick, a floppy disk, a mechanically encoded device such as punch-cards or raised structures in a groove having instructions recorded thereon, and any suitable\ncombination of the foregoing.  A computer readable storage medium, as used herein, is not to be construed as being transitory signals per se, such as radio waves or other freely propagating electromagnetic waves, electromagnetic waves propagating through\na waveguide or other transmission media (e.g., light pulses passing through a fiber-optic cable), or electrical signals transmitted through a wire.\nComputer readable program instructions described herein can be downloaded to respective computing/processing devices from a computer readable storage medium or to an external computer or external storage device via a network, for example, the\nInternet, a local area network, a wide area network and/or a wireless network.  The network may comprise copper transmission cables, optical transmission fibers, wireless transmission, routers, firewalls, switches, gateway computers and/or edge servers. \nA network adapter card or network interface in each computing/processing device receives computer readable program instructions from the network and forwards the computer readable program instructions for storage in a computer readable storage medium\nwithin the respective computing/processing device.\nComputer readable program instructions for carrying out operations of embodiments of the present invention may be assembler instructions, instruction-set-architecture (ISA) instructions, machine instructions, machine dependent instructions,\nmicrocode, firmware instructions, state-setting data, or either source code or object code written in any combination of one or more programming languages, including an object oriented programming language such as Smalltalk, C++ or the like, and\nconventional procedural programming languages, such as the \"C\" programming language or similar programming languages.  The computer readable program instructions may execute entirely on the user's computer, partly on the user's computer, as a stand-alone\nsoftware package, partly on the user's computer and partly on a remote computer or entirely on the remote computer or server.  In the latter scenario, the remote computer may be connected to the user's computer through any type of network, including a\nlocal area network (LAN) or a wide area network (WAN), or the connection may be made to an external computer (for example, through the Internet using an Internet Service Provider).  In some embodiments, electronic circuitry including, for example,\nprogrammable logic circuitry, field-programmable gate arrays (FPGA), or programmable logic arrays (PLA) may execute the computer readable program instructions by utilizing state information of the computer readable program instructions to personalize the\nelectronic circuitry, in order to perform aspects of embodiments of the present invention.\nAspects of the present invention are described herein with reference to flowchart illustrations and/or block diagrams of methods, apparatus (systems), and computer program products according to embodiments of the invention.  It will be\nunderstood that each block of the flowchart illustrations and/or block diagrams, and combinations of blocks in the flowchart illustrations and/or block diagrams, can be implemented by computer readable program instructions.\nThese computer readable program instructions may be provided to a processor of a general purpose computer, special purpose computer, or other programmable data processing apparatus to produce a machine, such that the instructions, which execute\nvia the processor of the computer or other programmable data processing apparatus, create means for implementing the functions/acts specified in the flowchart and/or block diagram block or blocks.  These computer readable program instructions may also be\nstored in a computer readable storage medium that can direct a computer, a programmable data processing apparatus, and/or other devices to function in a particular manner, such that the computer readable storage medium having instructions stored therein\ncomprises an article of manufacture including instructions which implement aspects of the function/act specified in the flowchart and/or block diagram block or blocks.\nThe computer readable program instructions may also be loaded onto a computer, other programmable data processing apparatus, or other device to cause a series of operational steps to be performed on the computer, other programmable apparatus or\nother device to produce a computer implemented process, such that the instructions which execute on the computer, other programmable apparatus, or other device implement the functions/acts specified in the flowchart and/or block diagram block or blocks.\nThe flowchart and block diagrams in the figures illustrate the architecture, functionality, and operation of possible implementations of systems, methods, and computer program products according to various embodiments of the present invention. \nIn this regard, each block in the flowchart or block diagrams may represent a module, segment, or portion of instructions, which comprises one or more executable instructions for implementing the specified logical function(s).  In some alternative\nimplementations, the functions noted in the block may occur out of the order noted in the figures.  For example, two blocks shown in succession may, in fact, be executed substantially concurrently, or the blocks may sometimes be executed in the reverse\norder, depending upon the functionality involved.  It will also be noted that each block of the block diagrams and/or flowchart illustration, and combinations of blocks in the block diagrams and/or flowchart illustration, can be implemented by special\npurpose hardware-based systems that perform the specified functions or acts or carry out combinations of special purpose hardware and computer instructions.\nThe descriptions of the various embodiments of the present invention have been presented for purposes of illustration, but are not intended to be exhaustive or limited to the embodiments disclosed.  Many modifications and variations will be\napparent to those of ordinary skill in the art without departing from the scope and spirit of the described embodiments.  The terminology used herein was chosen to explain the principles of the embodiments, the practical application or technical\nimprovement over technologies found in the marketplace, or to enable others of ordinary skill in the art to understand the embodiments disclosed herein.", "application_number": "16121697", "abstract": " A first set of data may be received indicating that an object of interest\n     has been identified. A second set of data may be received indicating a\n     first location of where the object of interest was identified. The first\n     location may correspond to a geographical area. In response to the\n     receiving of the first set of data and the second set of data, the first\n     location may be associated with a first transceiver base station. In\n     response to the associating, a first list of one or more mobile devices\n     may be obtained that are within an active range of the first transceiver\n     base station.\n", "citations": ["5434617", "6690374", "6947073", "7197320", "9408083", "9781565", "20060285723", "20090268030", "20090304229", "20110217962", "20110298930", "20130201329", "20130259302", "20140192205", "20140362225", "20150341602", "20150381940", "20160380820", "20180014159"], "related": ["15709966", "15169938"]}, {"id": "20190082311", "patent_code": "10375541", "patent_name": "Methods and systems for social networking with autonomous mobile agents", "year": "2019", "inventor_and_country_data": " Inventors: \nRonnau; Andrew (Los Alamitos, CA)  ", "description": "<BR><BR>BACKGROUND OF THE DISCLOSURE\n1.  Field of the Disclosure\nThe disclosure relates generally to social networking methods and systems and specifically in certain embodiments to methods and systems for proximity-driven social networking applications implemented with autonomous mobile agents incorporating\ndata mining and machine learning classification.\n2.  General Background\nMobile device users (agents) provide a platform for a social networking interaction that is motivated by groupings of proximate users, independent of other typical social network connections.  Geographical proximity, and additional features\nproximity, provides categorizations which establish a set of proximate agents which are thus associated for interaction.\nThe information exchange between any two agents may be performed with open identity, or with partial or complete anonymity\nTypes of data exchanged may be user-driven communication, user-defined auto-categorizations, and automatic machine classification operating on feature data inputs from associated agents.\nData mining and machine learning technologies may be used to develop automatic computer constructs for agents whom can operate on feature data sets from associated neighbor agents as input.  Computer machine constructs within an agent perform\nclassification operations on feature data sets from associated agents and the classification results may be communicated back to associated agents.  This information exchange may be autonomously derived.\nAs an example, facial recognition technology (FRT) provides a means to derive feature data to describe the facial appearance of a user agent.  Machine learning provides a technology by which the facial attraction preferences of an associated\n(proximate group) agent may be modeled.  The computer machine of the associated agent may operate on the feature data set from the parent agent and thus classify the associated agent as attracted, or not attracted to the parent agent.  This process\nallows the associated agents to be thusly classified.  Classification using facial recognition technology (FRT) and machine learning is one example.  Other classifications may be implemented into this structure.\nIt is desirable to address the limitations in the art, e.g., to apply methods and systems for proximity-driven social networking applications implemented with autonomous mobile agents incorporating data mining and machine learning\nclassification. <BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nBy way of example, reference will now be made to the accompanying drawings, which are not to scale.\nFIG. 1A illustrates an exemplary networked environment and its relevant components according to certain embodiments.\nFIG. 1B is an exemplary block diagram of a computing device that may be used to implement certain embodiments.\nFIG. 2 is an exemplary block diagram of a network that may be used to implement certain embodiments.\nFIG. 3 illustrates an exemplary environment of agents in which the proximate group (neighbor group) is shown within the discrimination domain.\nFIG. 4 illustrates an implementation in which the proximate zone is established with locally provided log-on key.\nFIG. 5 illustrates an implementation in which the proximate zone is established with a log-on key provided at more than one location.\nFIG. 6 illustrates and exemplary environment of agents created by joining two remote proximate zones.\nFIG. 7 illustrates the inter-agent communication in a proximate subgroup, independent of any classification operation.\nFIG. 8 illustrates the inter-agent communication for the classification operation in an exemplary environment proximate subgroup of agents in which the proximate group (neighbor group) has been classified.\nFIG. 9 is a schematic diagram of a prototypical agent structure as implemented herein.\nFIG. 10 is a diagram of an agent layer structure for an example binary classification.\nFIG. 11 is a diagram of an agent layer structure for an example binary multiplex classification.\nFIG. 12 is a diagram of an agent layer structure for an example multivariate classification.\nFIG. 13 depicts the agent initialization process.\nFIG. 14 depicts the network discrimination process to determine a proximate group.\nFIG. 15 shows an exemplary process in which parent agent feature data is classified by an associated neighbor agent.\nFIG. 16 shows the use of additional computer resources by the computer machine of an agent.\nFIG. 17 shows the use of a proximity zone to request data from a target location.\nFIG. 18 is an exemplary block diagram of a process to detect and identify a proximate group of agents and establish communication, and classification amongst the agents.\nFIG. 19A is a detailed exemplary block diagram of the first portion of the processes depicted in FIG. 18.\nFIG. 19B is a detailed exemplary block diagram of the second portion of the processes depicted in FIG. 18.\nFIG. 19C is a detailed exemplary block diagram of the third portion of the processes depicted in FIG. 18.\n<BR><BR>DETAILED DESCRIPTION\nThose of ordinary skill in the art will realize that the following description of the present invention is illustrative only and not in any way limiting.  Other embodiments of the invention will readily suggest themselves to such skilled\npersons, having the benefit of this disclosure.  Reference will now be made in detail to specific implementations of the present invention as illustrated in the accompanying drawings.  The same reference numbers will be used throughout the drawings and\nthe following description to refer to the same or like parts.\nFurther, certain figures in this specification are flow charts illustrating methods and systems.  It will be understood that each block of these flow charts, and combinations of blocks in these flow charts, may be implemented by computer program\ninstructions.  These computer program instructions may be loaded onto a computer or other programmable apparatus to produce a machine, such that the instructions which execute on the computer or other programmable apparatus create structures for\nimplementing the functions specified in the flow chart block or blocks.  These computer program instructions may also be stored in a computer-readable memory that can direct a computer or other programmable apparatus to function in a particular manner,\nsuch that the instructions stored in the computer-readable memory produce an article of manufacture including instruction structures which implement the function specified in the flow chart block or blocks.  The computer program instructions may also be\nloaded onto a computer or other programmable apparatus to cause a series of operational steps to be performed on the computer or other programmable apparatus to produce a computer implemented process such that the instructions which execute on the\ncomputer or other programmable apparatus provide steps for implementing the functions specified in the flow chart block or blocks.\nAccordingly, blocks of the flow charts support combinations of structures for performing the specified functions and combinations of steps for performing the specified functions.  It will also be understood that each block of the flow charts,\nand combinations of blocks in the flow charts, can be implemented by special purpose hardware-based computer systems which perform the specified functions or steps, or combinations of special purpose hardware and computer instructions.\nFor example, any number of computer programming languages, such as C, C++, C# (CSharp), Perl, Ada, Python, Pascal, SmallTalk, FORTRAN, assembly language, and the like, may be used to implement aspects of the present invention.  Further, various\nprogramming approaches such as procedural, object-oriented or artificial intelligence techniques may be employed, depending on the requirements of each particular implementation.  Compiler programs and/or virtual machine programs executed by computer\nsystems generally translate higher level programming languages to generate sets of machine instructions that may be executed by one or more processors to perform a programmed function or set of functions.\nThe term \"machine-readable medium\" should be understood to include any structure that participates in providing data which may be read by an element of a computer system.  Such a medium may take many forms, including but not limited to,\nnon-volatile media, volatile media, and transmission media.  Non-volatile media include, for example, optical or magnetic disks and other persistent memory.  Volatile media include dynamic random access memory (DRAM) and/or static random access memory\n(SRAM).  Transmission media include cables, wires, and fibers, including the wires that comprise a system bus coupled to processor.  Common forms of machine-readable media include, for example, a floppy disk, a flexible disk, a hard disk, a magnetic\ntape, any other magnetic medium, a CD-ROM, a DVD, any other optical medium.\nFIG. 1A depicts an exemplary networked environment 100 in which systems and methods, consistent with exemplary embodiments, may be implemented.  As illustrated, networked environment 100 may include a content server 105, a receiver 115, and a\nnetwork 110.  The exemplary simplified number of content servers 105, receivers 115, and networks 110 illustrated in FIG. 1A can be modified as appropriate in a particular implementation.  In practice, there may be additional content servers 105,\nreceivers 115, and/or networks 110.\nIn certain embodiments, a receiver 115 may include any suitable form of multimedia playback device, including, without limitation, a cable or satellite television set-top box, a DVD player, a digital video recorder (DVR), or a digital\naudio/video stream receiver, decoder, and player.  A receiver 115 may connect to network 110 via wired and/or wireless connections, and thereby communicate or become coupled with content server 105, either directly or indirectly.  Alternatively, receiver\n115 may be associated with content server 105 through any suitable tangible computer-readable media or data storage device (such as a disk drive, CD-ROM, DVD, or the like), data stream, file, or communication channel.\nNetwork 110 may include one or more networks of any type, including a Public Land Mobile Network (PLMN), a telephone network (e.g., a Public Switched Telephone Network (PSTN) and/or a wireless network), a local area network (LAN), a metropolitan\narea network (MAN), a wide area network (WAN), an Internet Protocol Multimedia Subsystem (IMS) network, a private network, the Internet, an intranet, and/or another type of suitable network, depending on the requirements of each particular\nimplementation.\nOne or more components of networked environment 100 may perform one or more of the tasks described as being performed by one or more other components of networked environment 100.\nFIG. 1B is an exemplary diagram of a computing device 150 that may be used to implement aspects of certain embodiments of the present invention, such as aspects of content server 105 or of receiver 115.  Computing device 150 may include a bus\n190, one or more processors 175, a main memory 170, a read-only memory (ROM) 180, a storage device 185, one or more input devices 155, one or more output devices 160, and a communication interface 165.  Bus 190 may include one or more conductors that\npermit communication among the components of computing device 150.\nProcessor 175 may include any type of conventional processor, microprocessor, or processing logic that interprets and executes instructions.  Main memory 170 may include a random-access memory (RAM) or another type of dynamic storage device that\nstores information and instructions for execution by processor 175.  ROM 180 may include a conventional ROM device or another type of static storage device that stores static information and instructions for use by processor 175.  Storage device 185 may\ninclude a magnetic and/or optical recording medium and its corresponding drive.\nInput device(s) 155 may include one or more conventional mechanisms that permit a user to input information to computing device 150, such as a keyboard, a mouse, a pen, a stylus, handwriting recognition, voice recognition, biometric mechanisms,\nand the like.  Output device(s) 160 may include one or more conventional mechanisms that output information to the user, including a display, a projector, an A/V receiver, a printer, a speaker, and the like.  Communication interface 165 may include any\ntransceiver-like mechanism that enables computing device/server 150 to communicate with other devices and/or systems.  For example, communication interface 165 may include mechanisms for communicating with another device or system via a network, such as\nnetwork 110 as shown in FIG. 1A.\nAs will be described in detail below, computing device 150 may perform operations based on software instructions that may be read into memory 170 from another computer-readable medium, such as data storage device 185, or from another device via\ncommunication interface 165.  The software instructions contained in memory 170 cause processor, 175, to perform processes that will be described later.  Alternatively, hardwired circuitry may be used in place of or in combination with software\ninstructions to implement processes consistent with the present invention.  Thus, various implementations are not limited to any specific combination of hardware circuitry and software.\nA web browser comprising a web browser user interface may be used to display information (such as textual and graphical information) on the computing device 150.  The web browser may comprise any type of visual display capable of displaying\ninformation received via the network 110 shown in FIG. 1A, such as Microsoft's Internet Explorer browser, Netscape's Navigator browser, Mozilla's Firefox browser, PalmSource's Web Browser, Google's Chrome browser or any other commercially available or\ncustomized browsing or other application software capable of communicating with network 110.  The computing device 150 may also include a browser assistant.  The browser assistant may include a plug-in, an applet, a dynamic link library (DLL), or a\nsimilar executable object or process.  Further, the browser assistant may be a toolbar, software button, or menu that provides an extension to the web browser.  Alternatively, the browser assistant may be a part of the web browser, in which case the\nbrowser would implement the functionality of the browser assistant.\nThe browser and/or the browser assistant may act as an intermediary between the user and the computing device 150 and/or the network 110.  For example, source data or other information received from devices connected to the network 110 may be\noutput via the browser.  Also, both the browser and the browser assistant are capable of performing operations on the received source information prior to outputting the source information.  Further, the browser and/or the browser assistant may receive\nuser input and transmit the inputted data to devices connected to network 110.\nSimilarly, certain embodiments of the present invention described herein are discussed in the context of the global data communication network commonly referred to as the Internet.  Those skilled in the art will realize that embodiments of the\npresent invention may use any other suitable data communication network, including without limitation direct point-to-point data communication systems, dial-up networks, personal or corporate Intranets, proprietary networks, or combinations of any of\nthese with or without connections to the Internet.\nThe building blocks of a system according to certain embodiments consist of a universe of agents (typically embodied as mobile users), a proximity discrimination module, a classification module, and notification interface.  The structure and\nimplementation of the system in certain embodiments, and of each of its building blocks, are described below and depicted in the accompanying figures.\nFIG. 2 depicts an exemplary networked environment 200 in which systems and methods, consistent with exemplary embodiments, may be implemented.  As illustrated, networked environment, 200, will include a proximity discrimination module\n(discrimination module), 220, and a classification module, 230.  The parent agent, 205, initiates a query in which the discrimination module, 220, discriminates the group of all agents, 215, to a proximate group, 225.  The discrimination parameters may\nbe refined in an iterative process, 240.  The classification module, 230, further operates on the proximate group to yield the classified proximate group, 235.  An abbreviated structure may be utilized in which classification is not performed, 245.\nThe group of all agents, 215, consists of all members participating in the system.  Members of the system may consist of, but are not limited to, users with cell phones, tablets, desktops, and shared environment computational devices.\nThe parent agent, 205, is a subgroup of all agents which typically consists of a single member.  An agent may be designated as parent agent by specifying a set of parameters for proximity discrimination.  In implementations where a single agent\nfunctions as parent agent, one or more agents, or all agents, may independently serve as the parent agent.\nThe functional concept according to certain embodiments is that a neighborhood of users can be established about some location.  Communication between the users in the proximate subgroup may be established.  Additional operations\n(classifications) may be performed and communicated openly or anonymously so that the parent (or other) agent knows some characteristic of the associated neighbor agents.  For example, the parent agent may know that there are 20 Republican in the\nproximate group and 10 others.  Relative geographic data may be included in the inter-agent exchange to be communicated to an agent.  For example, the parent agent may see that of the 30 agents in the room, 20 of them are Republicans, and they are all at\nthe front of the room.\nReferring back to the exemplary embodiment depicted in FIG. 2, the discrimination module, 220, consists of a system by which the proximate subgroup, 225, is determined.\nThe proximate group, 225 consists of the subgroup of all users that are within specified proximity to geographical position data and/or proximity input parameters specified by the parent agent, 205.\nThe proximate group, 225, is the group of agents amongst which inter-agent communication may occur.\nFIG. 3 depicts an exemplary proximate group of associated agents consisting of the parent agent, 205, and associated neighbor agents, 210, that are all within the discrimination zone, 250, and excluded agents, 255, that are not part of the\nassociated neighbor group.\nNote that for each of the agents that functions as a parent agent, the resultant proximate group may be different.\nIt may be possible to specify parameters such that the discrimination zone does not contain the parent agent.\nProximity parameters may be of any qualitative nature.  Examples of such proximity parameters include, but are not limited to, hair color, age range, height range, political affiliation, and the like.\nProximity parameters may be geographical, e.g., at (or within 0.25 mile of) latitude/longitude coordinates 34.8940-117.0472.  Other parameters may be used, depending on the requirements of each particular implementation.  For example, the\nproximity parameters may specify teenage males within the boundaries of Disneyland, Calif.  As another example, the proximity parameters may specify a domain that is within 50 feet of the centerline of Colorado Blvd, in Pasadena, Ca, during the period\nfrom 8:00 AM to 11:00 AM on Jan.  1, 2016.\nThe parameters may be specified which create a collection of domains which are not contiguous.  For example, the proximity parameters may specify the collection of domains defined as within 100 feet of any of the In-N-Out.RTM.  Burger\nrestaurants in Long Beach, Calif.\nThe discrimination operation may be parametric only, utilizing agent inputs.\nIn one exemplary implementation, the discrimination operation may perform in whole or in part using agent position data.  For example, individual agents, or groups of agents may be added to or removed from the proximate group using a map display\ndepicting the relative position of agents.\nIn certain embodiments, the discrimination module, 220, may consist of a remote server which receives geographic position data and/or additional proximity input parameters from all agents, 215, to discriminate the subgroup of all agents that are\nwithin specified proximity to geographical position data and/or proximity input parameters of the parent agent, 205.\nIn certain embodiments, the discrimination module, 220, may consist of a set of operations that are performed on one or more agents without communication with a remote device to perform the discrimination operation.\nIn certain embodiments, the discrimination module, 220, may consist, in part, of a proximity key, e.g., a log-on displayed at a bar, to facilitate an associative collection of participating agents at that location.  This is depicted in FIG. 4,\nin which a proximate group of neighbor agents, 210, are established at some locality, 300, using the locally provided log-on key, 305.  Agents that do not apply the log-on key comprise the group of excluded agents, 255.\nThe element that provides the log-on key, 305, may serve as a parent agent, or may simply be a mechanism to provide a log-on process initiated by an agent within the local group, or by an agent external to the local group.\nA log-on key may be provided at different locations to establish a proximate group consisting of geographically remote subgroups, as depicted in FIG. 5, which shows two remote localities, 301 and 302, in which a log-on key, 305, is provided at\nmore than one location via a communication link, 350, to establish a proximate group of neighbor agents, 210.  Agents that do not apply the log-on key comprise the group of excluded agents, 255.\nReferring back to exemplary diagram in FIG. 2, upon development of a proximate group, 225, the proximity parameters may be modified, 240, whereupon the proximate group is re-evaluated.  This process may be repeated and adjusted any number of\ntimes.\nRemote proximate groups may be joined into one proximate group comprised of remote groups of associated agents, as shown in FIG. 6, which shows the proximate group within the discrimination zone, 250, composed of a parent agent, 205 and\nassociated neighbor agents, 210, and the proximate group within the discrimination zone, 251, composed of a parent agent, 206 and associated neighbor agents, 211, joined, 400, to form one effective associated group\nThe joined group thus formed may have either parent agent, 205, or 206, assume the role as the joined group parent agent.\nA proximate group may have more than one parent agent.\nThe role of parent agent may be switched amongst agents, or shared amongst agents.\nA proximate group of associated agents may be broken into more than one smaller disjoint proximate groups.\nThe grouping thus established may be temporary, with associations expiring in a fixed time, or expiring after there has been a lapse of inter-agent communication for a fixed time.\nThe groupings thus established may be saved, e.g., the group that was at the bar on Friday night.\nThe proximity group may be geographically transient, in which the group consists of agents that are within a certain geographical proximity to a location, e.g., the parent agent's location, which may be moving.  As an example, consider agents in\ncars on the highway, wherein the proximity group consists of agents that are sufficiently near to the car transporting the parent agent.  In essence, the discrimination zone, 250, as shown in FIG. 3 moves with a moving parent agent.\nIn another exemplary implementation, the proximity parameters may be specified such that the proximity region does not follow a parent agent, but does move.  As an example, a zone may move with a float in a parade, even though no parent agent is\nmoving with the float.\nIn one embodiment, communication and notification may commence, and continue between agents in the proximate group, 225, with our without classification.  This is shown in FIG. 7, in which the parent agent, 205, and associated neighbor agents,\n210, within the discrimination zone, 250, communicate, 450.  Agents, 255, outside of the discrimination zone are excluded from the communication.\nIn one implementation, each neighbor agent communicates only with the parent agent (restricted communication).\nIn one implementation, all agents may communicate to each other agent in the proximate group (open communication).\nIn one implementation, communication permissions may be different for different agents (structured communication).\nIn one implementation the communication structure may be initialized, but is not locked such that the communication structure may emerge based on agent level decisions (free communication)\nThe inter-agent communication may include notification to the parent agent of the existence of each neighbor agent (anonymous notification).\nThe communication to the parent agent may also include relative positional data, and possible additional neighbor agent data (non-anonymous notification).\nThe inter-agent communication may include notification to the neighbor agent that the neighbor agent has been discriminated into a proximate group (anonymous notification).\nThe communication to the neighbor agent may also include relative positional data, and possible additional neighbor agent data (non-anonymous notification).\nThe inter-agent communication structure may be configured such that combinations of one-way, two-way, anonymous, and non-anonymous communication are used within an agent group.\nThe communication may also include any combination of directed communications, not limited to, text messages, emails, file transfers, and stored photo exchanges, real time photo exchange, stored video exchange and real time video.\nThe classification module, 150, comprises hardware and or software that transmits and receives feature data, and that operates on feature data which classifies the agents relative to the feature data.\nThe classification information which may be exchanged between agents is in addition to the inter-agent communication described herein.\nIn one embodiment, the classification operates on feature data from an agent of the proximate group, generally the parent agent, which classifies the associated neighbor agent relative to the feature data.\nIn one embodiment, the classification operates on directed inputs from one or more participants in the associated neighbor group (auto-classification).\nIn one embodiment, the classification operates on directed inputs or data mined from about one or more sources that are not in the associated neighbor group (external auto-classification).  For example, the agents may be classified as members of\nan organization, e.g., The Sierra Club.\nThe classification module may consist of classification operations executed on a distributed platform.  Such computations may be performed in residence on the agent, or in part or in whole utilizing computational resources external to the agent.\nDepending on the requirements of each particular implementation, the classification module may consist of combinations of the classification module examples described above.\nThe classification operation may be one that provides results which may are qualitative.\nThe classification operation may be one that provides results which may are binary.\nThe classification operation may be one that provides results which may are variable.\nThe classification operation may be one that provides results which are a derived types with any combination of types described herein.\nThe classification operation may be dynamically driven via inter-agent communication.  For example, a member of an active proximate subgroup at a lecture may send out a classification query, e.g., \"likes speaker.\" The complementary members of\nthe proximate subgroup may actively auto-classify into the binary groupings, e.g., \"like the current speaker,\" or \"dislike the current speaker.\"\nThe classification may be parametrically stored in the agent structure.  The information may be transient and stored only during the period of participation in the particular associated proximate group.\nIn one implementation, the classification query, for selected layers, is initiated by the parent agent.  The feature data is transmitted to the associated proximate subgroup neighbors, and the classification results are obtained for each member\nof the proximate group.  The classification results are transmitted to the parent agent.\nThis classification is shown in FIG. 8, which shows the subgroup of elements in a binary classification operation.  A classification query, 500, from the parent agent, 205, to elements within the discrimination zone, 250, which activates the\nclassification process on each associated neighbor agent.  In FIG. 8, the binary classification results are depicted schematically as classified neighbor agents with either a solid border, 210, or with a dashed border, 510.  The classification results\nare transmitted, 500, back to the parent agent, 205.\nAgents, 255, outside of the discrimination zone are excluded from the classification operation.\nThe data exchange and classification may not provide notification to the associated neighbor agent that the classification occurs (one-way anonymous classification).\nThe data exchange and classification in may also include relative positional data, and possible additional neighbor agent data (one-way non-anonymous notification).\nThe data exchange and classification may include notification to the neighbor agent that the classification occurs (anonymous classification).\nThe communication to the neighbor agent may also include relative positional data, and possible additional neighbor agent data (non-anonymous classification).\nThe data exchange and classification structure may be configured such that combinations of one-way, two-way, anonymous, and non-anonymous communication and exchange are used within an agent group.\nIn certain embodiments, neighbor member agents may initiate a classification query within the active existing proximate group, and receive classification data, as acting parent agent.\nFIG. 9 depicts an exemplary structure of a prototypical agent, 550, including the communication/notification interface, 555, and three data layers, 561, 571, and 581.  Each of the three layers has a feature data component, 562, 572, and 582,\nrespectively, and corresponding computer machine components, 563, 573, and 583, respectively.\nThe executive information principal to each data layer contains the layer name, and status, i.e.; active/inactive.\nNote that although the data layers are shown in the figure, the number of data layers may vary by agent, and by implementation.\nThe communication/notification interface, 555, may send and receive data from associated agents, from external entities, and from direct user input/output.\nCommunication to and from other agents may be in the form of wireless network transmissions, or hardwired network transmissions.\nCommunication to and from other agents may be in the form of wireless, or hardwired signals independent of any internet, or intranet.\nCommunication to and from other agents may by way of audible or inaudible sound or vibration or other form of transducer interaction.\nCommunication to and from other agents may by way of infrared, or any other band of EMF.\nThe communication/notification interface to the user may consist of any suitable combination of audible, sensory, text, touch, and graphical input and display used to facilitate the implementations described herein.\nIn one exemplary implementation, the interface consists of a graphical map display which shows the relative positional data, as available, of agents in the proximate group.\nClassification and notification data may be displayed as coloration, text tagging, or other visual indications associated with proximate agents.\nCommunications and data may be associated and accessible with user interaction, For example, classification results may be displayed in a drop down layer type display structure.\nDifferent communication and notification interface implementations may be used for different aspects of inter-agent classification and communication.\nCombinations of different notification and input modes may be used.\nCommunication and notification may be implemented on any suitable electronic device.  Note that the communication and notification may be exchanged using a device other than the agent to or from which said communication is directed.  For\nexample, a laptop computer may be used to view a map like environment representing the proximate group of agents as displayed for one of the member agents, although said member agent may be embodied by a user with a cell phone.\nThe layer structure, consisting of feature data and computer machine may be used to develop classification data beyond communications established between agents in the proximate group.\nThe feature element of each layer has the data structure for the classification, and contains the relevant data that may be necessary for the classification.\nThe layer structure may be dynamic.\nThe computer machine has the construct necessary to classify the feature data input from an agent.\nThe feature data component and computer machine component of each classification layer may be completely different, or may have the same structure as for other layers.\nThe classification may be dynamic (performed in real time upon the request query), or it may be stored as part of the agent structure.\nIn one implementation, the classification layer may consist of feature data and computer machine to provide a binary classification, i.e.; and is or is not classification relative to a category per the feature data.\nThe binary classification may be a self-categorization (auto classification), e.g.; male.  The feature data set consists of the data descriptor (which may even be the same as the layer name), and the computer machine consists of the\ncategorization which may be set dynamically or may be pre-set.\nIn one implementation, the feature data will contain the data name, and the categorization will simply have the result of the auto-categorization.  An exemplary layer structure for a binary feature/classification layer is shown in FIG. 10, in\nwhich the data layer, 600, shows executive structure, i.e.; layer name and status.  The feature data component, 605, contains a binary classification element, \"Republican\".  The computer machine component, 610, holds the binary classification switch for\neach corresponding element of the feature data.\nThe computer machine may be simply an external categorization, by another agent or organization.  For example, the layer may be Sierra Club membership, and the feature data consists of personal identity information, and the classification (is a\nmember, or is not a member) is provided to the layer by the Sierra Club.\nIn an additional implementation, the classification layer may consist of feature data and computer machine to provide a multiplex classification, of several possible variables.  Such a layer is similar to the construct of multiple binary layers. An exemplary structure is shown in FIG. 11, showing a multiple binary classification layer, 615.  The feature data component, 620, contains three binary classification elements, \"Republican\", \"Democrat\", and \"Independent\".  The computer machine\ncomponent, 625, holds the binary classification switches for each corresponding element of the feature data.\nIn another implementation, the classification layer may consist of feature data and computer machine to provide a multivariate classification.  Such a layer may classify in a manner that is not binary, but rather has multiple possible answers. \nAn exemplary structure is shown in FIG. 12, showing a multivariate classification data layer, 630.  The feature data component, 635, and computer machine component, 640, are structured appropriately for a classification that admits some range of possible\nanswers.\nThe multivariate example, \"Political Affiliation\", shown In FIG. 12 admits a discrete multivariate classification, i.e.; \"Republican\", or \"Democrat\", or \"Independent\", etc. It is also possible to have a continuous multivariate classification,\nfor example, \"Height\", in which case the possible classification is number from a continuum.\nThe classification structures may consist of auto-categorizations, or externally derived classifications.\nIn one embodiment, the classification is performed by the computer machine using data mining techniques.\nIn one embodiment, the computer machine may consist of statistically derived group preferences using agent data,\nFeature data may consist of sufficient agent (personal) information from which the computer machine may make data mining inquiries over external data bases.  The computer machine may extract data from internet sources, social networks, and other\nsocial media, e.g., Twitter, from which the computer machine may classify the agent.\nFeature data in the form of other classifications or categorizations may be used by the computer machine to classify using data mining techniques.  For example, a computer machine may suggest that there is a strong correlation between National\nRifle Association membership and Republican political registration.  Thus, if the feature data from an agents contains the classification \"National Rifle Association\", then the computer machine may thus classify the agent as \"Republican\".\nThe feature data may serve as input to a computer machine that is statistically constructed.  For example, the computer machine for an associated neighbor agent may contain the statistical construct that the agent has a preference for blond\nwomen between 20-25 years old.  That associated agent would thus be classified as attracted to a parent agent that is a 22-year old blond.\nCombinations of said feature data and classification operations may be used.\nFeature data may consist of descriptive elements that are specifically tailored to special classification computer machines.  For example, an attraction classification may use feature data that is abstract, such as eigenvectors from image\nprinciple component analysis, or other image processing feature extractions.\nThe computer machine to operate on such abstract feature extractions may also rely on training and parameterization using abstract feature data sets.\nThe agent structure and classification mechanisms described permit additional implementations somewhat outside the notion of communication and classification within the proximate agent group.\nEntities mentioned herein may be people, corporations, groups, products, institutions, etc.\nSuch an exemplary implementation is one in which entities rather than participating agents are classified for the parent agent.  For example natural language processing may be utilized to perform sentiment analysis on a collection of\npublications to determine the political disposition of a newspaper author.\nIn such an exemplary implementation, news articles may be tagged following the inclinations (as derived by the analysis) of the author.\nOther data mining and analysis methodologies may be implemented.  For example, correlative models may associate membership in some specific group with a certain political affiliation.\nSuch data mining and analysis models may be implemented on the system level, or may be implemented at the agent level.  Such models may be individually derived, or may be made available by the system institution.\nOther similar versions of such implementations are possible using data mining and natural language processing analysis directed at entities to be classified.\nAgent input may also be used to verify or refute such classifications.  Data obtained from such processes provide relative classification placements.\nSuch models described herein, embodied in the computer machine element within an agent, may be stored, shared, exchanged, created, and modified as utility objects.\nSuch classification may consist of classification operations executed on a distributed platform, on the agent level, or in part or in whole utilizing computational resources external to the agent.\nThe body of agents for the implementations described herein is created through an initialization process shown in FIG. 13.\nA potential agent, 650, downloads the application software, 660, from a content server, 655.  The potential agent with the software, 665, then registers, 675, with the software registration server, 670, whereupon the agent becomes a member\nagent, 680, and is a member of the collective universe of member agents, 215.\nThe collective universe of agents, 215, is the group from which all parent agents and proximate groups may be drawn.\nIt is possible for an agent to reside in a refined sub-universe of agents.  Said refinement is a semi-permanent step of group discrimination, as described above.\nNote that the content server, 655, and the registration server, 670, are process entities, which may be embodied on the same device, or on multiple devices.\nThe download, 660, and registration, 675, processes may be accomplished in one step or multiple steps.\nThe potential agents consist of, but are not limited to, users with cell phones, tablets, desktops, and shared environment computational devices.\nIt may be possible for an agent to have more than one member presence on a single device, i.e., different user profiles.\nFIG. 14 schematically depicts the process of forming a proximate agent subgroup.  The parent agent, 205, initiates a discrimination request, 705, which is transmitted to the network agent, 700.\nThe discrimination request may contain any combination of geographic location data, and discrimination parameters.  The network agent, 700, may forward the discrimination request to other network agents, 701, 702.\nOne or more of the network agents may then initiate communication, 720, to and or from the group of agents covered by the network agents.  The location parameters, and or any additional discrimination parameters are then used to distinguish the\nappropriate proximate neighbor subgroup agents, 210.\nIt certain implementations, it may be possible for agents in the universe group to utilize preference filters to prevent their incorporation into certain parent agent proximate groupings.\nIn certain embodiments, preference filters on the proximate group restrict inclusion to certain features.  Said preference filters may be provided by the parent agent.  Said preference filters may be implemented at the system level.\nAgents, 255, that do not meet the proximity classification requirements and/or preference filters are excluded from the associated neighbor group.\nIn one exemplary embodiment, an existing proximate group will have an associated log-on key so that agents may be joined in to the proximate group following the structure in FIG. 4.\nSuch log-on key usage may be open (all agents), or limited (some agents), or restricted (parent agent) only.  Such log-on usage may be determined at the system level.  Combinations of the usage structure may be used.\nIn certain implementations, it may be possible for agents that have been discriminated into a proximate group to sever ties with (to leave) the proximate group.\nThe neighbor agent subgroup(s) and communication links thus established are then used, in part, for subsequent communication and classifications as shown in FIG. 7 and FIG. 8.\nThe proximate group associations and communications channels may be stored for semi-permanent use.  In such instances, the associates are maintained even if a subsequent discrimination request, using the exact same parameters, would not result\nin the same proximate group.\nThe proximate group associations may be programmed to terminate in a specified time, or may be terminated instantaneously by the parent agent.\nIn one implementation, and agent may be removed from the proximate group following a period of inactivity.\nThe temporal nature of the agent membership in the proximate neighbor group is established by combinations of preference settings of the parent agent, the neighbor agent, and the system parameters.\nThe settings established during the initialization process determine the communications that transpire during the discrimination operation.\nIn certain implementations, the communication settings may be adjusted by some or all of the user agents.\nThe communication settings may be different for different agents in the proximate group.\nUpon discrimination, the presence of an agent in the proximate group may be transmitted back to the parent agent.\nUpon discrimination, the presence and any combination of location and other identifying features may be transmitted from one or more of the neighbor agent to the parent agent.\nOpen discrimination, notification of the discrimination operation may be transmitted to one or more potential neighbor agents.\nOpen discrimination, notification of the discrimination operation any combination of location and other identifying features may be transmitted from the parent agent to one or more potential neighbor agents.\nThe transmitted information to/from agents may be representative in that the information may be stored on one or more network entities and transmitted to or from said network entity.\nIn one implementation, the discrimination operation is reinitiated periodically, as the parent agent location moves.\nIn another implementation, the discrimination operation is periodically reinitiated as the proximate zone following the moving path of the parameterized discrimination target.\nIn one implementation, the structure of the communication (restricted communication, open communication, structured communication, and free communication) is established at the outset by the parent agent, as part of the permissions in the query.\nIn one implementation, the structure of the communication (restricted communication, open communication, structured communication, and free communication) is established at the system level.\nCombinations of the communication structuring process may be used, depending on the requirements of each particular implementation.\nThe layer structure may be initialized at a system level, or may be set up by the user during the initialization stage, or may be initially tailored by implementation at inception.\nIn one implementation, the layer structure may be altered using a list of layers available at the system level.  In such an implementation, layers may be initialized (populated with relevant data by the user).\nIn one implementation, layers may be created and added by user agents.\nIt may be possible to send or receive layer structures, as in the form of an invite.\nLayer structures may be added, as in layer allowances, using a trade metric.\nLayers may be active or inactive and have permission preferences.  For example, an agent may have initialized the auto-classification layers.  \"Republican\", and \"Sierra Club\".  Said agent may have the layer \"Republican\" active and the layer\n\"Sierra Club\" inactive.  In such an implementation, for example, a parent agent query would reveal the agent's \"Republican\" classification, but would not reveal the agent's \"Sierra Club\" classification.\nDifferent levels of information transmitted, or restricted based on agent layer user preferences.  For example, that the agent has a \"Sierra Club\" layer at all may be transmitted, but not the related classification.  As another example, the\nagent may appear invisible to other agents viewing by layer, and choosing to view the \"Sierra Club\".\nIn one implementation, permissions and preferences on layers may be determined at an agent level and controlled by the agent.\nIn another implementation, permissions and preferences are controlled at a system level.\nLayer permissions and preferences may be established on a layer by layer basis.\nCombinations of the approaches to layer permissions and usage are possible.\nSeveral exemplary embodiments are now described to illustrate the potential utilities available within the present structure.  The examples are in no way restrictive.  They are constructions that demonstrate a variety of implementations.\nIn one exemplary implementation, the structure described herein is utilized to create a local communication group.  A query is initiated to develop a proximate group of agents, amongst which communication may occur.  Following the process shown\nin FIG. 14, for example, a proximate group of agents is established.  The group, exemplified as shown in FIG. 7, then has an established communication link motivated by certain commonality, i.e., all at the same bar.\nThe discrimination operation is periodically performed to establish and refine the group of agents that are presently within the proximate region.  In this manner, new agents within the locality are added to the proximate group and agents that\nhave left the locality are removed from the proximate group.\nIn one exemplary implementation, potential member agents have their agent presence set up so that any proximity query results in notification of the member agent's classification status upon discrimination.  The structure results in a form of a\ndigital bumper sticker in which agents prominently display certain traits upon discrimination.  For example, the agents may have the binary classification layer \"Republican\" with the computer machine set as \"yes\", reference FIG. 10.\nDuring a query by any agent, the discrimination communication from the neighbor agent back to the parent agent, shown as 3046 in FIG. 14, contains trigger information to automatically notify the parent agent regarding the classification status\nof the neighbor agent.\nThe notification is independent of any direct communication, or subsequent classification queries.  In essence, said classification information is transmitted at first contact, thus forcing the classification to prominence.\nSuch classification notification is the first part of any communication to the parent agent or to other member agents.\nAn extension of the implementation may incorporate notification for more than one classification layer.\nAn extension of the implementation may incorporate notification for more than one classification layer\nIn one exemplary implementation, a parent agent initiates a selective search in which a refined proximate group is determined containing only those neighbor agents that have a particular layer and classification status.  For example, the parent\nagent query for \"Republican\" agents will indicate the existence of proximate agents that have the layer \"Republican\" and an auto classification \"yes\".\nSuch an implementation may proceed as shown in FIG. 14, in which the selective layer status is an additional parameter utilized within the operations performed by network agent, 3040, to further discriminate the proximate group to satisfy the\nselective classification request, i.e., \"Republican\".\nSuch an implementation may also proceed by first performing a proximity query without the aforementioned layer selectivity.  The base proximate group is then divided by a network agent into a selective discriminant subgroup, per the initial\nrequest, and a complementary proximate subgroup of agents that do not meet the selective search criterion of the initial search.\nIn another exemplary implantation a parent agent may initiate a selective search in which a refined proximate group is determined containing only those neighbor agents that do not have a particular layer and classification status.\nThe implementation may be extended to selective searches that are for combinations of attributes within the proximate subgroup.\nIn one exemplary implementation using the structure described herein, a parent agent initiates a query to develop a proximate subgroup from which classification information is gleaned.  As one particular example, an attraction layer is utilized\nso that the parent agent may determine if there are agents in the proximate group which are likely to find the parent agent attractive.  The situation is as depicted in FIG. 8\nThe process is proceeds as depicted in FIG. 14, in which a parent agent query discriminates neighbor agents.  The neighbor agents in the proximate group might be, for example, all participating agents with 100 feet of a certain locale.\nUpon discrimination, the neighbor agents are classified as shown in FIG. 15, which depicts the classification interaction between the parent agent, 205, and an associated neighbor agent, 210.  The parent agent is shown with one active layer,\n750, and two inactive layers 765.  The associated neighbor agent is shown with one active layer, 775, and two inactive layers, 765.\nThe parent agent feature data, 755, from the active layer, 750, is transmitted, 770, to the computer machine, 785, of the associated active layer, 775, of the associated neighbor agent, 210.\nThe computer machine, 785, on the associated neighbor agent active layer, 775 operates on the feature data to create a classification result which is transmitted, 790, back to the communication/notification interface, 555, of the parent agent. \nThe feature data, 780, of the associated neighbor agent, may or may not be utilized in the classification process.\nThe classification may involve notification and communication through the associated agent communication/notification module, 555.\nThe process depicted in FIG. 15 is repeated for each of the associated neighbor agents in the proximate group.\nUsing the aforementioned example of attraction, the feature data of the parent agent may consist of demographic data, descriptive data, and statistical data that have been previously generated as part of the training process for the attraction\nmetric.\nThe computer machine in the corresponding layer of each associated neighbor agent may consist of a computer construct generated to statistically capture the neighbor agent's attraction preferences.  The construct has been previously generated as\npart of the training process for the attraction metric.\nAdditional attraction metric structures outside of the present example are available.\nThe classification process, using the computer machine, 785, is symbolically contained in the associated neighbor agent, 210.  The classification may be performed in part or in whole on an external device.\nFIG. 16 depicts an agent, 800, with one active layer, 775, and two inactive layers, 765, utilizing external computing resource to process a classification operation for the active layer.  Relevant data, i.e., feature data from the parent agent,\nis transmitted, 810, from the agent computer machine, 785, to a network agent, 700, that provides computational resource to perform the classification operation.\nSome classification operations utilize externally provided data.  Such externally available data may obtain through a query to an external source.  The request is transmitted, 810, from the computer machine, 785, to the network agent, 700, and\nthen transmitted, 815, to the network resource server, 805, The data from the external source is downloaded, 820, to the network agent, 700, for processing.  The data and is then transmitted, 825, to the agent computer machine, 785.  Additional cycles of\nthe data search and retrieval may be performed as part of the classification operation.\nExternal data and external computer processing may be performed in any combination by the network agent, 700, and network resource server, 805.  Processing may also be performed in whole or in part in combination by the computer machine, 785,\nand the external resources.\nFeature data, 780, from the agent active layer, may or may not be utilized in the classification process.\nNotification to the parent agent may use any combination of the agent interaction types, anonymous, non-anonymous, one-way, and two-way.\nIn one exemplary implementation, the agent structure and classification mechanisms described herein are used to create a mechanism by which the political predisposition of manuscript authors are determined and used to tag (classify) the\nassociated work.  The process is different from other example implementations described herein in that the classification operation is not performed on an agent\nThe classification uses the same process as shown in FIG. 16, where the name of the author of the article is supplied to the computer machine, 785, by the agent communication/notification interface, 555.  The computer machine, 785, transmits,\n810, the author name to a network agent, 700.  A data query, 815 is made to network resource server, 805, where data mining techniques and natural language processing are used to assess the sentiment bias of the article author.  The bias assessment\nresults are transmitted back, 820, to the network agent, 800, and then back, 825, the computer machine, 785.  The final result is transmitted to the notification interface, 555, where the article is tagged according to the author bias.\nClassification results may be stored as multivariate classification structures, reference FIG. 12 to save on future computational overhead.  The classification bias may be refreshed at any time, or may be performed every time.\nNumerous data mining and classification operations are available using existing art, for example, natural language processing and sentiment analysis, or a statistical preference model following a market basket analysis.\nThe present structures and processes permit the implementation of peer to peer data exchange with dynamic demand.\nIn one exemplary embodiment, a peer to peer exchange for real time video with proximity driven dynamic demand.\nThe process is depicted in FIG. 17, which shows a parent agent, 205, which queries for video at the target location, 850.  Information concerning the request is transmitted, 870, to an adjacent network agent, 700, and then to another network\nagent, 701, adjacent to the target location.  A zone of proximity, 860, is established and scanned, 880, for candidate source agents therein.\nShould no appropriate agent exist in the zone of proximity, the zone is monitored until a member agent which is a potential neighbor agent, 685, enters the zone and becomes neighbor agent, 210, which is a candidate source agent.\nThe parent agent request is enunciated to candidate source agents.\nWhen a candidate source agent accepts the parent agent request, the video is uploaded, 885, to the network agent, 701, and transmitted via the agent network back to the parent agent.\nThe video thus sourced and transmitted may be archived as part of a library structure for future availability, or may be abandoned.\nData other than video may be exchanged in the same fashion.  The intent is to provide dynamic demand for data of geographic and temporal importance.  For example, data may be requested, in real time on location at breaking news story.  As\nanother example, real time video may be requested following a Rose Parade float.\nLocation categorized information about agents creates an environment from which a game may be created.  The present exemplary implementation presumes that proximate group has been established at some location, i.e., a bar, and geographic\nposition data of each proximate group member agent has been openly broadcast.  Additionally, it is presumed that one or more agents have been classified, and the classification has been has been enunciated openly to the group anonymously.\nThe example configuration thus described is one in which the communication/notification module of each element displays the approximate location of each agent in the proximate group.  The classification data, not being linked to any geographic\ndata, is not attached to any particular one of the proximate agents.\nThe exemplary game challenge is for an agent to guess, based upon appearance, for instance, which of the proximate agents fit the available classification data.  The guess can be directed to a particular agent using the positional aspect of the\ncommunication/notification interface.\nThe guess can be verified as correct or incorrect using the available system agent data and the result can be communicated to the agent making the guess.\nVariations of the game are possible.  As another example, a proximate agent's classification with regard to some layer, i.e., \"Republican\" is made.  The guess is tested against system agent information, and the result is transmitted back to the\nagent making the guess.\nCertain embodiments described herein may generally be described through the process diagram shown in FIG. 18.\nThe proximity query, 1800, is the operation by which the parent agent initiates the formation of a proximate group.  The parent agent uses the communication/notification interface, 555, to select and enter proximity parameters, and communication\nand notification preferences.  The classification option is specified and active layers for classification are selected.  The proximity query parameters, communication/notification preferences, and classification options, are entered into the parent\nagent communication/notification interface to form a data input set that used as input in subsequent operations.  The communication/notification interface is used to commence the discrimination operation, 1805.\nWhen the discrimination option is commenced, 1805, the proximity query parameters, communication/notification preferences, and classification options that had been entered into the parent agent communication/notification interface are uploaded\nto a network agent.  The network agent uses the proximity parameters as input for the discrimination operation.  In certain implementations, other parts of the data entered during the proximity query, 1800, are also used as input to the discrimination\noperation, 1805.  The network agent uses network resources to determine which member agents, 685, satisfy the proximity parameters selected in 800.  The network identity and additional agent information relevant to the data input set entered in the\nproximity query, 1800, are obtained and utilized in the subsequent processes shown in FIG. 18.  This completes the discrimination operation, 805.\nThe notification process, 1810, provides announcements and indications to the agents concerning various aspects of the discrimination process, 1805.  The existences of each of the neighbor agents, 210, that are detected and discriminated during\nthe discrimination process, 1805, are transmitted to the parent agent, 205, from the network agent, 700.  Depending upon the communication/notification preferences, geographic information and/or additional neighbor agent, 210, data are also transmitted\nfrom the network agent, 700, to the parent agent.  Depending upon the communication/notification preferences, the member agents, 685, that are queried as part of the discrimination process are notified of the discrimination process.  Depending upon the\ncommunication/notification preferences, the neighbor agents, 210, that are established as part of the discrimination process are notified of their inclusion in the proximate group, and possibly geographic information and/or additional information about\nthe parent agent.  The notification process communicates the proximate group structure to the parent agent, 205, and neighbor agents, 210, and indicates the classification, 1820, and/or communication and data exchange, 1830, operations to follow.\nThe communication/classification decision point, 1815, shows the process branch, from which, the classification and communication operations begin.  If neighbor agent classification has been requested as part of the query, 1800, then the parent\nagent, 205, communication/notification interface, 555, provides a prompt for the classification operation, 1820, to commence, and also for communication and data exchange amongst agents, 1830, to commence.  Each of the operations, classification, 1820,\nand communication, 1830, commences upon execution by the parent agent, 205, at the decision point, 1815.  The classification operation, 1820, and communication operation, 1830, may be performed in either order.  Additional cycles of each may be\nperformed, as shown on FIG. 18.  If neither operation can be performed, the process stops, 1835.  Neither operation can be performed when the proximate group is vacated, or expires, or is terminated by the parent agent or network agent.\nUpon commencement of the classification operation, 1820, from the decision point, 1815, the classification operation, 1820, evaluates neighbor agents, 210, relative to feature data from parent agent, 755, or relative to other data independent of\nparent agent feature data, 755.  Using the communication links established as part of the discrimination operation, 1805, the selected classification layer data are transmitted from the parent agent, 205, to the network agent, 700, and then to the\nneighbor agents, 210.  The neighbor agents, 210 operate on the feature data, 755, and are classified accordingly.  The classification results are transmitted to the network agent, 700, and then to the parent agent, 205, where the results are displayed on\nthe communication/notification interface, 555.  Depending upon the communication/notification preferences, the neighbor agents, 210, may or may not be notified of the classification process.  Subsequent classifications may be initiated within the same\nprocess block, 1820, when the parent agent, 205, selects a different data layer for classification, and the classification process just described is repeated.  Each classification operation is completed when the neighbor agents, 210, are classified, and\nthe classification process results have been transmitted to the agents.  From the classification process block, 1820, control returns to the decision point, 1815.\nThe communication process, 1830, provides notifications and data exchange between agents in the proximate group, 225.  Using the communication links established as part of the discrimination operation, 1805, agents within the proximate group\ncommunicate and transmit data within the confines of the permissions and preferences established during the discrimination process, 1805.  The communication and data exchange is performed (or initiated) using the agent communication/notification\ninterface, 555.  The communication process, 1830, is completed when the message, email, text, video, or data file to or from the agent has been transmitted and received, or the process has been interrupted by the parent agent, 205, neighbor agent, 210,\nor network agent, 700.  From the communication block, 1830, control returns to the decision point, 1815.\nAdditional process structures are possible, describing implementation variations permitted by processes and structures described herein.\nThe process depicted on FIG. 18 is shown in greater detail in FIGS. 19A, 19B, and 19C.\nThe parent agent proximity parameters, 1900, are the operation by which the parent agent initiates the formation of a proximate group.  The parent agent uses the communication/notification interface to select and enter proximity parameters, and\ncommunication and notification preferences.  The classification option is specified and active layers for classification are selected.  Such parameters and such preferences may be stored as profile data and selected for subsequent queries.\nProximity parameters include geometric proximity parameters, and also additional parametric proximity parameters.  Geometric proximity parameters include location data, i.e.; centroid and radius of the proximity zone.  Additional proximity\nparameters include parametric data as permissible age, or sex.  Such parameters are part of each agent's information profile and/or part of (or not) an agent's layer data structure\nCommunication preferences indicate the level of notification and communication to be provided to and from agents during the discrimination and classification options, such as one-way non-anonymous communication in which the parent agent is to be\nnotified of the existence of agents in the proximate region, and also of the location of each agent in the proximate region.\nThe classification option indicates whether or not any classification operation will be performed on the agents in the proximate region.  The option to classify and to also establish a communication link is also provided.\nAdditional temporal and geographical specifications, such as the duration of the proximate grouping or multiple proximity region locations are also indicated.\nThe communication/notification interface is used to commence the query, upon which the proximity query data is uploaded to a network agent, 1905.  The data entered to establish the proximate zone discrimination and other communication\npreferences and permissions, are transmitted to a network agent, 700, in preparation for the subsequent operations.\nThe network agent, 700, transmits the proximity parameters, as appropriate to other network agents, 700, to facilitate agent detection, 1910.  Member agents, 685, that are within the geometric limits of the discrimination zone, 250, are detected\nand identified, 1910, by network agents, 700.  The network identity and agent data for agents within the geometric limits of the proximity zone are used in the subsequent discrimination operation, 1915.\nThe discrimination operation is shown in 1915.  Using the agent data obtained in 1910, the additional proximity parameters, beyond geometric proximity, are applied to each of the agents detected in 1910, to refine the group of agents.  Agents\nthat satisfy the geometric proximity parameters and that also satisfy the additional proximity parameters are categorized as neighbor agents, 210.  Agents that do not satisfy all of the proximity parameters are categorized as excluded agents, 255.  The\ndiscrimination operation, 1915, stops when all of the agents detected in 1910 have been adjudicated relative to all additional proximity parameters.  The network identities, agent data, and categorization (neighbor agent, 210, or excluded agent, 255) are\nutilized as part of the input for the notification process, 1920.\nThe parent agent, 205, is notified regarding the process and results of the detection, 1910, and discrimination, 1915, operations.  The existence of each of the neighbor agents, 210, that are detected, 1910, and discriminated, 1920, are\ntransmitted to the parent agent, 205, from the network agent, 700.  Depending upon the communication/notification preferences, geographic information and/or additional neighbor agent, 210, data are also transmitted from the network agent, 700, to the\nparent agent.  The notifications are utilized as input for the subsequent parent agent, 205, decision point, 1925.\nThe notifications provided in 1920 are displayed on the communication/notification interface, 555, and reviewed by the parent agent, 205, from which a decision, 1925, to revise the proximity parameters is made.  If the resultant proximate group\nof agents, 225, is not satisfactory, the proximity parameters may be revised, to restart the process, 1900.  A prompt on the communication/notification interface is provided to either revise the proximate group, 225, or to accept the proximate group,\n225.  If the resultant proximate group of agents, 225, is satisfactory, the neighbor agent data and proximity query preferences are utilized as input for the neighbor agent notification decision point, 1930.\nThe decision to notify neighbor agents, or not, is shown in 1930.  The neighbor agents, 210, are notified of their inclusion in the proximate group, 225, if the proximity query preferences input in 1900 indicate notification to neighbor agents. \nIf the notification is indicated, a message is sent from the network agent, 700, to each of the neighbor agents notifying them of their inclusion into a proximate neighbor group.  Depending upon the preferences set forth as part when establishing the\ndiscrimination parameters, geographical and/or other data about the parent agent, 205, is transmitted from the network agent, 700, to the neighbor agents, 210.  Following notification, the process control flows to the decision point in which the parent\nchooses, 1940, to classify the neighbor agents, or communicate with the group of neighbor agents.\nThe communication/classification decision point, 1940, shows the process branch, from which, the classification and communication operations begin.  If neighbor agent classification has been requested as part of the query parameters, 1900, then\nthe parent agent, 205, communication/notification interface, 555, provides a prompt for the classification operations to commence, and also for communication and data exchange amongst agents, 1945, to commence.  Each of the operations, classification,\nand communication commences upon execution by the parent agent, 205, at the decision point, 1940.  The classification operations, and communication operations may be performed in either order.  Additional cycles of each may be performed, as shown on FIG.\n19.  If neither operation can be performed, the process stops, 1980.  Neither operation can be performed when the proximate group is vacated, or expires, or is terminated by the parent agent or network agent.\nIf classification is chosen at the decision point, 1940, the process control flows to process block 1950, where the parent agent, 205, information is transmitted, 1950, to each neighbor agent, 210.  The feature data, 755, from the parent agent,\n205 is transmitted to a network agent, 700, and then to the computer machine, 785, of each neighbor agent, 210.  The feature data is used as input for the subsequent classification, 1955.  The data transfer, 1950, is complete when the feature data has\nbeen transmitted and received by each of the neighbor agents.\nProcess control transfers to the classification operation, 1950, the computer machine, 785, of each neighbor agent, 210, operates on the parent agent, 205, feature data, 755, to obtain a classification result.  The process is complete when each\nof the neighbor agent's classification is established.  Process control then flows to process block 1960 where the classification results are communicated.\nThe classification result of each neighbor agent is transmitted to a network agent, 700, from when it is transmitted to the communication/notification interface, 555, of the parent agent, 205.  Depending upon the preferences established by the\ninitial proximity parameters used to initiate the formation of the proximate group, 1900, the notification may or may not be provided to neighbor agents that they have been classified.  The classification results are viewed on the parent agent, 205,\ncommunication/notification interface, 555 Tithe classification transmission, 1960, is complete when all of the neighbor classification results have been transmitted to the parent agent, 205.  The process then returns to the classify or communicate\ndecision point, 1940.\nIf communication is chosen at the classify or communicate decision point, 1940, the process control flows to process block 1945, to commence inter-agent communication amongst agents in the proximate group, 225.  The communication process, 1945,\nprovides notifications and data exchange between agents in the proximate group, 225.  Using the communication links established as part of the discrimination operation, 1915, agents within the proximate group, 225, communicate and transmit data within\nthe confines of the permissions and preferences that are established during formation of the proximate group, 225.  The communication and data exchange is performed (or initiated) using the agent communication/notification interface, 555.  The\ncommunication process, 1945, is completed when the message, email, text, video, or data file to or from the agent has been transmitted and received, or the process has been interrupted by the parent agent, 205, neighbor agent, 210, or network agent, 700. From the communication block, 1945, control returns to the classify or communicate decision point, 1940.\nEmbodiments are not limited to processes described in FIGS. 18, 19A, 19B, and 19C.\nAccording to exemplary embodiments of the present invention, three novel implementations of facial recognition and data management technology for social applications are presented; a facial rolodex, a reverse facial rolodex, and a set social\nnetworking applications.  Each of the three implementations may be utilized independently for use on a computer or mobile device, for example.\nThe third implementation (social networking applications) requires a metric (or combination of metrics) according to certain embodiments.  A particularly suitable metric may be derived from FRT.  The third implementation is described\nindependently from the choice of metric (to admit application with any suitable metrics).  The novel metric derived from FRT is subsequently described.\nThe facial rolodex application is a facial data management application, that may or may not be facilitated using FRT.  The concept is to have an application in which a name may be associated with a face in an organized fashion.  By using a\nlinked face-name database, the user may call up a picture of a name that is either typed, spoken using suitable voice recognition technology (VRT), or in the text of an existing document.  The essence of this application is to simply place a face with a\nname.  The following features or design elements are presented:\nFacial look-up.  The application retrieves an associated facial image by typing a name, or by speaking a name, or by scrolling through a list or document to find a name.\nTagged text in a document.  The application presents a face image corresponding to a name in a text.  The image is either being permanent or transient, and may be toggled on or off.  The face image retrieval may either be passive (as when the\ncursor or selector icon is floated above the text) or active (as when the name text is highlighted and a keystroke or button is used to activate the facial image retrieval).  The application may be embedded upon existing text, or may be applied over text\nas it is typed (as when a face is called up simply by typing the name, as the name is typed).\nText tagging.  The application is used to create a real time pop-up of an associated facial image for text messages or emails received.\nConversation tagging.  The application is used to retrieve an associated facial image (and biographical information) for the names of people that are associated to snips of conversation.  This application may be used for ambient conversations,\nor for telephone or radio conversations.  It associates a face with the speaker.  For example, during conference calls, the application uses VRT to identify the person speaking, and retrieves the associated facial image (if the speaker is in the database\nlibrary).\nConversation Capture.  The application identifies names that were spoken during conversations and retrieve associated images for the spoken names.  The kernel of the application functions just as does conversation tagging, however the feature\nthat triggers the data retrieval would be a spoken name.\nOrganizational Structure.  The name-face database library may be augmented by the user in the same fashion that contact information is added and edited.  The libraries may be shared or merged and may be organized with associations, i.e.; a\nname-picture for your neighbor may have tree-like connection to the neighbor's wife, etc. The organizational structure may be presented in different fashions, as in a tree diagram with connections (relationships) and nodes (entries).\nSocial Networking.  The facial rolodex application and organizational structure may be implemented along with social media to allow a social media presence to be linked to a face-name data pairing.  The application may be tasked with data mining\nthe present webpage to create new, and develop existing, name-face-data data association's database.\nDatabase building.  Any element of the associations, name-face-data, for an entry in the database may be added to an existing entry, or entered to create a new database entry.  The database entries may be modified, completed, augmented or edited\nusing additional information that becomes available through the processes to which the application is directed.\nThe reverse facial rolodex application is a facial data management tool that uses FRT to identify faces and retrieve social contact and other information attached to a face in the name-face library.  It allows the user to user to identify a\nfacial image and allow a user to also retrieve data associated to a person.  The essence is to simply place a name to a face.  The following features or design elements are presented.\nDirect image sensing.  The application may be utilized to access archival information linked to a facial image in the database that is entered with a cell phone.  The cell phone may be used to scan a face or a crowd and thus retrieve information\nabout any faces from the images that are in the facial image database.\nEmail image input.  The application may be used to recall linked information attached to a facial image in a picture embedded in or attached to an email.\nWebsite image capture.  The application may be used to retrieve linked information associated with any facial image from the database that is contained in a photograph that is attached to, linked to, or embedded in a webpage.  The application\nmay be tasked with drilling down and seeking facial images at some level of association with the present webpage.\nDocument image capture.  The application may be used to capture images embedded, linked, or tagged to a document and retrieve associated information for any image in the database.\nVideo image capture.  The application may be used to retrieve linked information associated with any facial image from the database that is contained in a video.  Any of the resources, from which an image is made available to the application may\nalso be used to convey or provide video to the application.\nSocial Networking.  The reverse facial rolodex application and organizational structure may be implemented along with social media to allow a social media presence to be linked to a name-face data pairing.  The application may be tasked with\ndata mining the present webpage to develop and populate associated data linked to an image in the database, and to develop new name-face-data associations to add to the database.\nDatabase building.  Any element of the associations, name-face-data, for an entry in the database may be added to an existing entry, or entered to create a new database entry.  The database entries may be modified, completed, augmented or edited\nusing additional information that becomes available through the processes to which the application is directed.\nMethods have been developed for facial recognition using Eigen space analysis.  The Eigen face approach, casts the identification and classification of facial images as a mathematical problem, utilizing vector space analysis, as used in numerous\nfields of science, math and engineering.\nMost young people (in particular) are concerned with their attractiveness to other people.  When any individual describes attractiveness (how attractive someone is), there is some continuum; extremely attractive, very attractive, somewhat\nattractive, somewhat unattractive, etc. Thus, a metric of attraction can be described, and the metric may be used in a social application.\nSince the Eigen face concept codifies a facial image as an element of a large dimensional vector space, which is axiomatically also a metric space (with a Euclidean metric), it provides a mathematical structure by which attractiveness may\nquantified.  This notion permits the development of the broader idea of a social application that addresses the issue of attractiveness according to certain embodiments.\nThe following discussion describes applications according to certain embodiments, which ultimately may be used with a variety of metrics, or combinations of metrics.  Following the discussion of several applications, a specific metric obtained\nby using facial recognition Eigen space is described in detail according to certain embodiments.  An additional suitable metric using neural networks is also described, according to other embodiments.\nThe application is addresses the issue of attractiveness, and the perception, by others, of one's attractiveness.  A person may determine how attractive he or she is to another individual without direct input or interaction with that individual. Additionally, an individual may get some measure of how attractive he or she would find another person without visually making that assessment.  The ideas are extended according to certain embodiments to situations that arise from the information that\nbecomes available with a suitable attractiveness metric.\nIf there exists a suitable metric of attractiveness, and if information or parameters necessary to establish the metric which describes their attraction to others is available, then certain measurements may be performed.  More precisely,\ndifferent parties could make the measurement of attraction.  For example:\nAn individual could measure how attractive any other person would be (A measures how attractive B is to A).\nA person could measure how attractive an individual would find that person (B measures how attractive B is to A).\nA third party measures attraction (C measures how attractive B is to A and vice versa).\nProvided that the individual's metric can be parameterized in some sufficiently convenient manner, then the metric may be reconstructed and thus applied as a test of any person's attractiveness.  Thus, a virtual device is created that determines\nhow attractive person B is to person A.\nThe application of the metric concept according to certain embodiments may be utilized for any social application website such as Facebook or Instagram.  The utilization may take several forms, as listed below in certain exemplary embodiments:\nImage Classification--(A measures) The user may actively or automatically classify incoming images (or linked persons) by attractiveness to the user.  The resulting classification may simply be private, as a convenience to the user.  The\nresulting classification may also be made public (open to all), or semi-public (shared with chosen users).\nReverse Image Classification--(B measures) The user may measure how attractive he or she is to the person depicted in incoming images (or linked persons).  The resulting classification may be private (to B) or shared with the owner of the image\n(or linked person).\nThe application of the metric concept may be utilized as a search engine, or to modify a search engine or search results.  The utilization may be used in a dating website search, a social networking search, or on a generic search engine search.\nAttraction Search--(A measures) A search may be made, using the metric, for people that are attractive to A.\nReverse Attraction Search--(B measures) A search may be made, using the metric, for people that would find An attractive.\nSearch Modification--A search may be made using traditional methods (keywords, AI, for instance) and modified using the metric.  For example, search for brunettes, named Elizabeth, that find a certain person attractive.\nThe application of the metric concept could be used by a third party (C measures) in fixed or mobile applications--(C measures).  For example, person A may test to see if a person that person A meets at work would be attractive to person A's\nneighbor.  As a different example, person A may test to see if a person A meets at work would find person A's neighbor attractive.\nThe metric may be used as a stand-alone website or mobile application, in which users may test themselves by other users, and test other users.  They may also search users for attractive or attracted users.  The stand alone may serve as a dating\nwebsite, or simply as an interesting waypoint, by which, members could judge their attractiveness.\nThe application of the metric concept may be utilized for a mobile application that utilizes position information to augment the measurement available from the metric.  The essence of the application is that someone may see that there are people\nthat find that person attractive in the same locality (at a party, for instance).\nBlind Annunciation--The metric parameters of an individual's attraction metric are broadcast locally according to certain embodiments.  The metric is then used by other users in the same locality to test their own appearance.  Thus, a person can\nsee if there are other individuals in the location that would find that person attractive.  No specific information about either person or either person's precise location is exchanged, according to certain embodiments.\nOpen Annunciation--The metric parameters of an individual's attraction metric are broadcast locally, according to certain embodiments.  The metric may then use by other users in the same locality to test their own appearance.  Thus, a person can\nsee if there are other individuals in the location that would find that person attractive.  However, in contrast to the blind annunciation described above, some or all available information about one person, or the other (including, possibly, precise\nlocation) is transferred, according to certain embodiments.\nThe application is essentially the same as the mobile application in certain embodiments, except the information is integrated into a network so that a user can view information about the people at a remote location, such as at a restaurant.\nThe application according to certain embodiments provides an objective test for attractiveness that is based upon a metric derived from a group of people.  The group metric may then be used to test the attractiveness of an individual to a group. This utilization may thus be used to see if a particular individual is attractive to a group.  This implementation may be used for hiring and selection in professions where appearance is important.\nBy nature, the utilizations described herein present themselves to gathering all manner of statistics regarding attractiveness.  The statistics may be used to determine if a person is attractive to some particular demographic.  It may also be\nused to extract trends.\nOne particular formulation of FRT, Eigen faces, is cast in a mathematical formulation that permits allows the development of an attractiveness metric that is particularly well suited for use in the social networking application described above. \nThe Eigen face concept treats facial recognition as a mathematical problem (eigen-analysis) in large scale, finite-dimensional vector space.\nIn certain embodiments, a photograph of a face is depicted as a rectangular array of intensity values.  It can thus be represented as a vector of intensity values.  For example, a 256.times.256 image can be expressed as a vector in\n65,536-dimensional space.  Prior artists have noted that a group of such images can be sufficiently well represented when the images are projected onto an appropriate subspace of the original 65,536-dimensional space.  The appropriate subspace is\ndetermined by performing an eigen-analysis on the covariance matrix of the group of images.  Further, images that are not members of the sample group can be classified as facial images, or as images of something else by determining the proximity of the\ntested image to the subspace comprised of the sample images.  The entire formulation is developed as a facial recognition technology that is effective and robust, that requires relatively low computer storage and relatively low computational effort.\nThese formulations demonstrate the practical use of the mathematical structure (vector space) that is available when a facial image is described as a finite dimensional vector.  The present development according to certain embodiments\n(attractiveness metric) capitalizes on those very same mathematical elements; finite dimensional vector space, subspace, eigen-analysis, norm, and metric.  For sake of description, the following discussion is presented in the context of 256.times.256\nfacial image representation.  Note that the ideas may be directly extended to other image sizes, depending on the particular requirements of each implementation.\nA finite dimensional real vector space has a Euclidean norm, and by simple extension, has a metric (norm of the difference of two vectors).  This is a familiar metric, however it must be noted that it is not the only metric that can be\nconstructed which will satisfy the definitional property of a metric.  Utilizing the subspace of images screened for attractiveness and/or the subspace of images screened for un-attractiveness presents an opportunity to utilize any such metrics available\nwithin the vector space construction, plus any combination of metrics and other features (brunette, Catholic, etc.).\nA metric for attractiveness may be formed for an individual by presenting the individual with a test set of 256.times.256 facial images which the individual categorizes as attractive, or not attractive.  The attractive images form a subspace in\n256.times.256 face space.  The attraction metric is created by utilizing the vector space metric that is available as part of the mathematical structure (vector space) utilized.  The distance of the test image vector from the attractive image subspace is\nthe attraction metric; a very close image is deemed attractive, and a very distant image is deemed not attractive.\nA metric for attractiveness may be formed for an individual by presenting the individual with a test set of 256.times.256 facial images which the individual categorizes as attractive, or not attractive.  The non-attractive images form a subspace\nin 256.times.256 face space.  The attraction metric is created by utilizing the vector space metric that is available as part of the mathematical structure (vector space) utilized.  The distance of the test image vector from the non-attractive image\nsubspace is the attraction metric; a very close image is deemed attractive, and a very distant image is deemed not attractive.\nA metric for attractiveness may be formed for an individual by presenting the individual with a test set of 256.times.256 facial images which the individual categorizes as attractive, or not attractive.  The attractive images form a subspace in\n256.times.256 face space and the images that are non-attractive form the complementary subspace in 256.times.256 face space.  The attraction metric is created by utilizing the vector space metric that is available as part of the mathematical structure\n(vector space) utilized.  The distance of the test image vector from the attractive image subspace and the distance of the of the test image vector from the non-attractive image subspace provide two numbers used to gauge the attractiveness of the test\nimage.\nA metric for attractiveness may be formed for an individual by presenting the individual with a test set of 256.times.256 facial images to which the individual assigns attraction values (1 to 10, for example).  This scaling is simply used as a\ndiscrimination parameter for the division into attractive, and not-attractive subspaces used in the metrics described above.\nA metric for attractiveness may be formed for an individual by presenting the individual with a test set of 256.times.256 facial images to which the individual assigns attraction values (1 to 10, for example).  The test images form a subspace\nwith an associated scaling (in essence an additional dimension--think of the three dimensional plot in which the test vectors are in the X-Y plane, and the scaling is the associated Z-value).  The test image may be projected onto the sample space (closer\nto the attractive images, or closer to the un-attractive images) to see what the associated scale (attractiveness value) is for the projected test image.\nNeural networks are a structural form of artificial intelligence that are commonly applied to pattern recognition problems, such as facial recognition technology.  Neural networks are a kind of black box approach in which a network of simple\nbehavioral cells (perceptron's, for instance) are trained so that the response of the system adheres sufficiently well to a desired algorithm.\nIn the same fashion that a neural network may be utilized to identify an image as a face, and select a particular face from a training set, a neural network may be trained to identify a face as attractive or non-attractive according to an\nappropriate training set of images.  A neural network may therefore be used as attractiveness metric.  The network, a back propagation, feed forward network, for example, would provide discrete output signals such as attractive and non-attractive (two\noutput cells), or attractive, non-attractive, neutrally attractive (three output cells), etc.\nIn this manner, a neural network may be trained to create attractiveness metric for use in any of the aforementioned components of the attractiveness application.\nThere may be other combinations not explicitly presented here.  Therefore it is understood that the invention is not to be limited to the specific embodiments disclosed, and that modifications and embodiments are intended to be included as\nreadily appreciated by those skilled in the art.\nWhile the above description contains many specifics and certain exemplary embodiments have been described and shown in the accompanying drawings, it is to be understood that such embodiments are merely illustrative of and not restrictive on the\nbroad invention, and that this invention not be limited to the specific constructions and arrangements shown and described, since various other modifications may occur to those ordinarily skilled in the art, as mentioned above.  The invention includes\nany combination or sub combination of the elements from the different species and/or embodiments disclosed herein.", "application_number": "16186149", "abstract": " Exemplary methods and systems are presented for social networking\n     applications using autonomous mobile agents. Communication links are\n     established based on geographic proximity and distance described as a\n     domain in which resident agents are detected and identified. The\n     communication links thus established allow platform independent\n     communication along communication channels that are dynamically derived.\n     Incorporation of computer machines and feature data sets permit agent\n     classification and inter-agent notification of classification results.\n", "citations": ["6826414", "7574606", "8315211", "8554172", "9100709", "20030009589", "20070070935", "20100015991", "20100280837", "20100322215", "20110039539", "20120028623", "20120322484", "20130028163", "20140016494", "20160119768"], "related": ["14217173", "61800009"]}, {"id": "20190082312", "patent_code": "10375555", "patent_name": "Disaster event management", "year": "2019", "inventor_and_country_data": " Inventors: \nNeybert; Karl (Overland Park, KS), Deshaies; Stephen (Snoqualmie, WA), Husby; Barry (Lawrenceville, GA), Steven; Daniel (Troy, IL), Happ; Martha (Reno, NV)  ", "description": "<BR><BR>TECHNICAL FIELD\nThe present application relates generally to a field of mobile communication and, more specifically to accounting for people and connected devices during a disaster or other emergency situation.\n<BR><BR>BACKGROUND\nWhen disaster strikes and other emergency events occur, keeping track of people and determining who is safe is important, not just for public safety authorities, but for employers as well.  Knowledge of where employees are, other connected\ndevices (e.g., internet of things \"IOT\" devices) and sensors and their statuses can be used to inform family members and authorities, but also can be used to determine best practices for future events. <BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nNon-limiting and non-exhaustive embodiments of the subject disclosure are described with reference to the following figures, wherein like reference numerals refer to like parts throughout the various views unless otherwise specified.\nFIG. 1 illustrates an example disaster event management system in accordance with various aspects and embodiments of the subject disclosure.\nFIG. 2 illustrates an example disaster event management system in accordance with various aspects and embodiments of the subject disclosure.\nFIG. 3 illustrates exemplary communication pathways for a disaster event management system accordance with various aspects and embodiments of the subject disclosure.\nFIG. 4 illustrates an example disaster event management system and cell site or worksite in accordance with various aspects and embodiments of the subject disclosure.\nFIG. 5 illustrates another example disaster event management system and cell site or worksite in accordance with various aspects and embodiments of the subject disclosure.\nFIG. 6 illustrates an example disaster event management system in accordance with various aspects and embodiments of the subject disclosure.\nFIG. 7 illustrates an example method for tracking employees during an emergency event in accordance with various aspects and embodiments of the subject disclosure.\nFIG. 8 illustrates an example method for tracking employees during an emergency event in accordance with various aspects and embodiments of the subject disclosure.\nFIG. 9 illustrates an example block diagram of an example user equipment that can be a mobile handset operable to provide a format indicator in accordance with various aspects and embodiments of the subject disclosure.\nFIG. 10 illustrates an example block diagram of a computer that can be operable to execute processes and methods in accordance with various aspects and embodiments of the subject disclosure.\n<BR><BR>DETAILED DESCRIPTION\nOne or more embodiments are now described with reference to the drawings, wherein like reference numerals are used to refer to like elements throughout.  In the following description, for purposes of explanation, numerous specific details are\nset forth in order to provide a thorough understanding of the various embodiments.  It is evident, however, that the various embodiments can be practiced without these specific details (and without applying to any particular networked environment or\nstandard).\nVarious embodiments disclosed herein provide for a disaster event management system that can track locations of employees and others affected during disaster events and other emergency situations and determine their safety status.  The disaster\nevent management system can determine when an emergency event has occurred, and determine which employees are likely to be affected by the emergency, based on their location at the time and other directory information.  The system can provide an\ninterface on user equipment devices operated by the employees (mobile devices, laptops, computers, tablets, etc) to provide their status along with an identification code to verify their identities.  In an embodiment, the system can prompt the user\nequipment devices to provide a status in response to determining that the employee may be affected by the emergency situation.\nThe disaster event management system can enable employers and other agencies to rapidly account for thousands of employees following a disaster or other emergency event.  This can be particular useful when worksites (e.g., office buildings) and\nother facilities have to be evacuated, making it harder for supervisors to track their employees to ensure their safety.  The integrated disaster event solution can also enable enterprise customers to target critical information pushes, preparedness\nexercises and best practices, as well as generate reports on evacuation and safety metrics and data visualization.  The disaster event management system can also enable people afflicted by the emergency to rapidly distribute information and provide for\nemployee to employee resource sharing.\nSimilarly, note that for simplicity we use the radio network node or simply network node is used for gNB.  It refers to any type of network node that serves UE and/or connected to other network node or network element or any radio node from\nwhere UE receives signal.  Examples of radio network nodes are Node B, base station (BS), multi-standard radio (MSR) node such as MSR BS, gNB, eNode B, network controller, radio network controller (RNC), base station controller (BSC), relay, donor node\ncontrolling relay, base transceiver station (BTS), access point (AP), transmission points, transmission nodes, RRU, RRH, nodes in distributed antenna system (DAS) etc.\nLikewise, for reception we use the term user equipment (UE).  It refers to any type of wireless device that communicates with a radio network node in a cellular or mobile communication system.  Examples of UE are target device, device to device\n(D2D) UE, machine type UE or UE capable of machine to machine (M2M) communication, PDA, Tablet, mobile terminals, smart phone, laptop embedded equipped (LEE), laptop mounted equipment (LME), USB dongles etc. Note that the terms element, elements and\nantenna ports are also interchangeably used but carry the same meaning in this disclosure.\nIn various embodiments, a system can comprise a processor and a memory that stores executable instructions that, when executed by the processor facilitate performance of operations.  The operations can comprise determining that an event that\nsatisfies a criterion related to public safety has occurred.  The operations can also include determining that a user equipment associated with a user account is at a location associated with the event.  The operations can also comprise sending a status\nupdate request to the user equipment The operations can also comprise, in response to receiving status update feedback from the user equipment, updating a safety status of the user account, the safety status of the user account relating to a safety of a\nperson associated with the user account.\nIn another embodiment, method comprises receiving, by device comprising a processor, a notification that an emergency event has occurred within a first defined distance from a worksite.  The method can also comprise determining, by the device,\nthat a user equipment associated with an employee is within a second defined distance from the worksite.  The method can also comprise receiving, by the device, a status update from the user equipment, wherein the status update comprises a validation\ncode.  The method can also comprise determining, by the device, whether the validation code is a match with directory information associated with the user equipment.  The method can also comprise in response to the receiving the status update and the\nvalidation code being determined to be the match with the directory information, updating, by the device, a status associated with the employee.\nIn another embodiment machine-readable storage medium, comprising executable instructions that, when executed by a processor of a device, facilitate performance of operations.  The operations can comprise determining that a mobile device\nassociated with an employee is within a predetermined distance of an emergency event based on a network location of the mobile device.  The operations can also comprise receiving a status update from the mobile device, wherein the status update comprises\na validation code and status information.  The operations can also comprise matching the validation code to directory information associated with the mobile device.  The operations can also comprise in response to the matching the validation code to the\ndirectory information, updating a status of the employee in an emergency tracking database with the status information.\nAs used in this disclosure, in some embodiments, the terms \"component,\" \"system\" and the like are intended to refer to, or comprise, a computer-related entity or an entity related to an operational apparatus with one or more specific\nfunctionalities, wherein the entity can be either hardware, a combination of hardware and software, software, or software in execution.  As an example, a component may be, but is not limited to being, a process running on a processor, a processor, an\nobject, an executable, a thread of execution, computer-executable instructions, a program, and/or a computer.  By way of illustration and not limitation, both an application running on a server and the server can be a component.\nOne or more components may reside within a process and/or thread of execution and a component may be localized on one computer and/or distributed between two or more computers.  In addition, these components can execute from various computer\nreadable media having various data structures stored thereon.  The components may communicate via local and/or remote processes such as in accordance with a signal having one or more data packets (e.g., data from one component interacting with another\ncomponent in a local system, distributed system, and/or across a network such as the Internet with other systems via the signal).  As another example, a component can be an apparatus with specific functionality provided by mechanical parts operated by\nelectric or electronic circuitry, which is operated by a software application or firmware application executed by a processor, wherein the processor can be internal or external to the apparatus and executes at least a part of the software or firmware\napplication.  The mechanical parts can include sensors on a float, tilt monitors, and etc. As yet another example, a component can be an apparatus that provides specific functionality through electronic components without mechanical parts, the electronic\ncomponents can comprise a processor therein to execute software or firmware that confers at least in part the functionality of the electronic components.  While various components have been illustrated as separate components, it will be appreciated that\nmultiple components can be implemented as a single component, or a single component can be implemented as multiple components, without departing from example embodiments.\nFurther, the various embodiments can be implemented as a method, apparatus or article of manufacture using standard programming and/or engineering techniques to produce software, firmware, hardware or any combination thereof to control a\ncomputer to implement the disclosed subject matter.  The term \"article of manufacture\" as used herein is intended to encompass a computer program accessible from any computer-readable (or machine-readable) device or computer-readable (or\nmachine-readable) storage/communications media.\nComputer-readable storage media can include, but are not limited to, random access memory (RAM), read only memory (ROM), electrically erasable programmable read only memory (EEPROM), flash memory or other memory technology, solid state drive\n(SSD) or other solid-state storage technology, compact disk read only memory (CD ROM), digital versatile disk (DVD), Blu-ray disc or other optical disk storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices or\nother tangible and/or non-transitory media which can be used to store desired information.  In this regard, the terms \"tangible\" or \"non-transitory\" herein as applied to storage, memory or computer-readable media, are to be understood to exclude only\npropagating transitory signals per se as modifiers and do not relinquish rights to all standard storage, memory or computer-readable media that are not only propagating transitory signals per se.\nIn addition, the words \"example\" and \"exemplary\" are used herein to mean serving as an instance or illustration.  Any embodiment or design described herein as \"example\" or \"exemplary\" is not necessarily to be construed as preferred or\nadvantageous over other embodiments or designs.  Rather, use of the word example or exemplary is intended to present concepts in a concrete fashion.  As used in this application, the term \"or\" is intended to mean an inclusive \"or\" rather than an\nexclusive \"or\".  That is, unless specified otherwise or clear from context, \"X employs A or B\" is intended to mean any of the natural inclusive permutations.  That is, if X employs A; X employs B; or X employs both A and B, then \"X employs A or B\" is\nsatisfied under any of the foregoing instances.  In addition, the articles \"a\" and \"an\" as used in this application and the appended claims should generally be construed to mean \"one or more\" unless specified otherwise or clear from context to be\ndirected to a singular form.\nMoreover, terms such as \"mobile device equipment,\" \"mobile station,\" \"mobile,\" subscriber station,\" \"access terminal,\" \"terminal,\" \"handset,\" \"communication device,\" \"mobile device\" (and/or terms representing similar terminology) can refer to a\nwireless device utilized by a subscriber or mobile device of a wireless communication service to receive or convey data, control, voice, video, sound, gaming or substantially any data-stream or signaling-stream.  The foregoing terms are utilized\ninterchangeably herein and with reference to the related drawings.  Likewise, the terms \"access point (AP),\" \"Base Station (BS),\" BS transceiver, BS device, cell site, cell site device, \"Node B (NB),\" \"evolved Node B (eNode B),\" \"home Node B (HNB)\" and\nthe like, are utilized interchangeably in the application, and refer to a wireless network component or appliance that transmits and/or receives data, control, voice, video, sound, gaming or substantially any data-stream or signaling-stream from one or\nmore subscriber stations.  Data and signaling streams can be packetized or frame-based flows.\nFurthermore, the terms \"device,\" \"communication device,\" \"mobile device,\" \"subscriber,\" \"customer entity,\" \"consumer,\" \"customer entity,\" \"entity\" and the like are employed interchangeably throughout, unless context warrants particular\ndistinctions among the terms.  It should be appreciated that such terms can refer to human entities or automated components supported through artificial intelligence (e.g., a capacity to make inference based on complex mathematical formalisms), which can\nprovide simulated vision, sound recognition and so forth.\nEmbodiments described herein can be exploited in substantially any wireless communication technology, comprising, but not limited to, wireless fidelity (Wi-Fi), global system for mobile communications (GSM), universal mobile telecommunications\nsystem (UMTS), worldwide interoperability for microwave access (WiMAX), enhanced general packet radio service (enhanced GPRS), third generation partnership project (3GPP) long term evolution (LTE), third generation partnership project 2 (3GPP2) ultra\nmobile broadband (UMB), high speed packet access (HSPA), machine to machine, satellite, microwave, laser, Z-Wave, Zigbee and other 802.XX wireless technologies and/or legacy telecommunication technologies.\nFIG. 1 illustrates an example embodiment 100 of a disaster event management system 104 in accordance with various aspects and embodiments of the subject disclosure.  In one or more embodiments, disaster event management system 104 can track a\nlocation of a UE 102 and if it is determined the UE 102 is within a predetermined range of a disaster event, or other emergency situation (e.g., weather related, earthquake, volcano, act of terror, war, etc.) and if so, update a status of the employee\nassociated with the UE 102 in response to receiving feedback from the UE 102.  The UE 102 can be a mobile device such as a cellular phone, a smartphone, a tablet computer, a wearable device, a virtual reality (VR) device, a heads-up display (HUD) device,\na smart car, a machine-type communication (MTC) device, and the like.  User equipment UE 102 can also comprise IOT devices that communicate wirelessly.\nIn an embodiment, the UE 102 can be associated with an employee account.  For example, the UE 102 can be a device issued by an employer, or is otherwise registered with the employer.  In other embodiments, the UE 102 can be a personal device,\nnot otherwise associated with the employer, but still used by an employee associated with the employer system 106.\nThe disaster event management system 104 can determine when an emergency situation has occurred (e.g., from public safety organizations, alerts, social media data, etc.) and determine whether UE 102 is in a location that may be affected by the\nemergency situation.  In an embodiment, the disaster event management system 104 can determine from an employee database 112 in the employer system 106 whether an employee is assigned to a worksite (e.g., facility, office building, region, etc) that is\naffected by the emergency situation.  In an embodiment, the disaster event management system 104 can also determine locations of customers, vendors, visitors, and others that may be in area affected by the emergency.  As an example, a property manager or\nowner could use the system to get input from people in their apartments, hotels, malls, multi-tenant office buildings, RV parks, hospitals, education institutions, city buildings, and etc.\nIn other embodiments, the disaster event management system 104 can determine the location of the UE 102 from GPS data on the UE 102 or based on which network node the UE 102 is attached.  In an embodiment, the disaster event management system\n104 can determine a location of the UE 102 from a last known GPS coordinate location.  This can be helpful when a device has been turned off, or is in a building.  In other embodiments, the disaster event management system 104 can determine a location of\nthe UE 102 from an internet protocol (IP) address associated with a data transmission from the UE 102 if the UE 102 is connected to a WiFi signal or is otherwise connected to a local area network or wide area network with a known location.  In other\nembodiments, the disaster event management system 104 can determine a location of the UE 102 by utilizing calendar data, e.g., if the employee user account has calendar data indicating a meeting or travel plans at or to a location affected by the\nemergency situation.  In yet another embodiment, the disaster event management system 104 can integrate with an employer access management system and determine who is present in a building to be evacuated based on the employees that swiped badges to\nenter the building.  The administrative component 110 can provide access to lists of employees present in a building at the time of the emergency, and disaster event management system 104 can solicit status updates from UEs associated with the employees\npresent.\nData on visitors can be collected from multiple sources such as building/location access (not just employer access) management systems and detection systems that can recognize mobile devices such as smart phones.  For example, if a person is in\na building with their cell phone on (not airplane mode) then some information like phone numbers or SIM ID can be captured and used to augment building occupant counts, and last known location (especially if multiple sensors throughout a location is\nused).  Information on employees and contractors can come from human resource system, directory database, etc. and even data from corporate travel indicating an employee from one location will be in the geographic region impacted by an emergency\nsituation.\nOnce the disaster event management system 104 determines that the UE 102 is present, the disaster event management system 104 can send a prompt to the UE 102 to provide an updated status.  In an embodiment, the prompt can be sent via short\nmessage service (SMS) message to the UE 102, or by voice call, or by a notification in an application on the UE 102.  The employee can enter a status (or use a predefined status: \"Safe\", \"Okay\", and etc.), and the disaster event management system 104 can\nreceive the status update, and return the status update to the employer system.  In an embodiment, the user can send back the status to the disaster event management system 104 without having received a prompt.  For instance, the user can send an SMS to\na predefined number, or call a number and use the phone system's dual tone multi-frequency (DTMF) signaling to send status updates.  In other embodiments, the user can send a status update to the disaster event management system 104 via an app without\nhaving been prompted.\nIn an embodiment, the status update can include a verification code identifying the employee.  The verification code can be an employee identification code or other code issued by the employer that may identify the employee, and/or the business\ngroup to which the employee belongs.  The disaster event management system 104 can match the verification code provided in the status update to a verification code associated with the employee in the employee database 112 before updating the status of\nthe employee.\nIn an embodiment, the status update can include one or more short message codes that can inform a diverse group of carriers where to send the status update and provide insight into which company a person sending the message works for.  Companies\ncould also share a 5 digit short message code, but then would need to have an application or user input to provide a unique customer code and employee identification code.  In an embodiment, the unique customer (company) and employee (worker) codes\nenable message validation (weeding out spam) and correlation to individuals regardless of the device used to send in the message.  For example, an employee could use a company provided or a personal mobile phone to text the designated number, as long as\nthey identified their unique ID.  That is especially important during disasters where the person may not have ready access to mobile phone listed in their contact information.\nIn an embodiment, the verification code may not be necessary if the employee is providing the status update via a device that is already registered with the employer system 106.  For instance, if the phone number of a received SMS message\nproviding a status report matches the phone number of an employee, the disaster event management system 104 can update the status of the employee.  Likewise, the UE 102 may be a device not registered with the employer system 106, with no records\nassociated with the UE 102 in the employee database 112, such as if the phone were a personal device, or a pay-phone, or other device.  The verification code can be used to authenticate the employee status update in such a case.\nIn an embodiment where the verification code is not provided, and/or the device is not registered, or when the verification code does not match the code in the employee database 112, the disaster event management system 104 can resend a request\nfor update or request the user to re-enter the verification code.\nRules would be set up in GSMS to validate if incoming mobile-originated messages contain an employee's unique identifier or not.  Response messages would indicate if a valid employee identifier was found in the incoming message such as a text\nstring that matches the company employee ID schema (e.g., two letters followed by four numbers) before the disaster event management system 104 forwards to the reporting component 108, else the disaster event management system 104 would reply back with a\nnegative response notice informing the sender to try again with a valid ID.\nThe disaster event management system 104 can automatically validate a uniqueness of the employee identifier, ignoring capitalization, each time the customer attempts to uploads new/changed employee identifier files.  Employer systems 106 could\nadd an optional--yes or--no after the employee ID if they wanted to indicate a basic company defined condition status code.\nAn administrative component 110 on the employer system 106 can track the responses and reporting component 108 can be used to generate reports for supervisors and public safety officials to determine the status of employees and others affected\nby the emergency.  The administrative component 110 can track data associated with how employees are reporting back via the disaster event management system 104 and the information can be used to identify best practices and improved response protocols\nIn an embodiment, the disaster event management system 104 can facilitate two way communications to and from the UE 102.  The disaster event management system 104 can be used to broadcast a message to all UEs belonging to employees affected by\nan emergency situation.  For example, the administrative component 110 can be configured to transmit, via the disaster event management system 104 a message or set of messages to a single UE (e.g., UE 102) or group of UEs (e.g., all UEs associated with\nemployees at a worksite, or in a business group).  The message(s) can contain emergency instructions to both the people who have responded with status updates and/or to people who are currently unaccounted for.  The messages can be different depending on\nwhether the status update has been provided by the person or whether they are unaccounted for.  The messages can be sent via SMS message, voice call, email, or via notifications in an application on the UEs.  During a disaster event, if SMS messaging is\nnot available or is limited in its availability, IP messaging may provide an alternate method to account for employees.  The disaster event management system 104 can support 2-way SMS messaging, with IP message failover to help ensure that employees have\na robust, multi-modal messaging option to account for themselves following a disaster in their area.  IP messaging may require an application to be loaded and active on the employee's mobile device.\nIn an embodiment, two way communication can be established with a person checking in. For instance, if UE 102 checked in and provided their status to disaster event management system 104, they could be accounted for and transferred to an\nemergency responder system like 911 if needed.  Assets that are responding to inquiries or sending self-initiated notices, such as an emergency defibrillator being activated, could include chips to report in and also enable responders to talk with people\nnear the device.  The same could be done for emergency exits and fire extinguishers.\nIn an embodiment, the administrative component 110 can store copies of the messages for automated accounting of employees following the emergency situation.  The disaster event management system 104 can use standard and flexible APIs that feed\nreal-time data into their reporting systems.  This can enable the employer system 106 to quickly determine the accounted for status of their employees following a disaster and create reports as needed via the reporting component 108.  In an embodiment,\nthe messages can be stored for at least 90 days.\nIn an embodiment, the administrative component 110 can use a secure web enabled portal for company administrators that supports basic messaging, contact information maintenance (i.e. uploads of target phone number lists), and reporting or data\ndownload into a spreadsheet.  This is particularly important for organizations without their own reporting system that can leverage an API automated input feed.\nIn an embodiment, each message can be tracked, searchable, downloadable, and can include both the message text and metadata device's automatic number identification (ANI--i.e. caller ID), date/time stamp, and message type (SMS or IP).  In an\nembodiment, messages can continue be sent to the UEs affected by the emergency situation at periodic intervals until a validated response is received.\nNote: Customer reporting tools or potential future cloud based reporting solution could include a function to cross-reference mobile number with employee contact information, but that is not native in the GSMS messaging solution.\nTurning now to FIG. 2, illustrated is an embodiment 200 showing an example disaster event management system 204 in accordance with various aspects and embodiments of the subject disclosure.\nIn an embodiment, the disaster event management system 204 can receive status updates from UE 202 in response to there being an emergency situation that may affect an employee or person associated with the UE 202.  The disaster event management\nsystem 204 can solicit status updates from UE 202 or can otherwise receive and update the statuses with employer system 210 when the disaster event management system 204 determines that an event has occurred that warrants the response.\nDisaster event management system 204 can determine that an event is serious enough based on several different sources of information.  A weather system 206 can provide weather updates and notifications to the disaster event management system 204\nand if a weather update is serious enough (e.g., tsunami, tornado warning, or actual reports of a tornado touching down near an office building worksite, etc.) then the disaster event management system 204 can send a prompt for status updates to the UE\n202.  The weather system 206 can be sourced by active advisory statements from the National Weather Service or other governmental and private agencies that provide weather alerts.  In an embodiment, disaster event management system 204 can relay status\nupdates and other contextual information received from the UE 202 back to the weather system 206.  In this way, forecasts, and other advisory statements can include contextual information from the scene that may be useful to others.\nIn an embodiment, the disaster event management system 204 can determine an emergency has occurred in response to a plurality of tracked devices that are no longer reporting status information.  For example, if multiple IoT or connected devices\nor sensors stop reporting, to the disaster event management system 204, then the disaster event management system 204 could determine that an emergency has occurred.  Building alarms and environmental sensors could be correlated to trigger an alert via\nalerts system 208 to a monitoring station or individuals who would then determine if a notification would go out to individuals in a given building or location.  Multiple no power indicators or emergency generators starting can help indicate the\ngeographic scope of an emergency.  Automated asset accounting can also include IoT devices reporting if an asset is moving and has its emergency lights on, which indicates its status to mapping correlation tools that can help especially if 911 dispatch\nis experiencing heavy volumes.  First responders could initiate a request to see if any assets are near them to provide backup.  On the people side, knowing someone's phone is moving or outside a building can also be helpful\nAn alerts system 208 can also provide information relating to emergency situation.  An alerts system 208 can be provided with information from first responders systems (e.g., police, fire service, National Guard, Homeland Security and etc).  If\nan emergency situation is declared by any organization, the alert can be forwarded to disaster event management system 204 which can start soliciting updates from UEs associated with employees that may be affected by the emergency.  Similarly, the alerts\nsystem 208 can also be provided with contextual data about the scene of the emergency from the disaster event management system 204.  The information can help first responders determine how best to respond to the emergency situation.  For example, if\nthere is an emergency, and the disaster event management system 204 can track the locations of the UEs that have responded to the status updates, and also track locations of UEs that have not responded, or can determine areas where there have been no\nresponses from, that location data can be passed to the first responders via the alerts system 208 to inform the first responders about where there might be areas of greater danger.  As an example, if an office building has been evacuated due to a fire,\nthe disaster event management system 204 can determine which business groups or employees associated with floors or offices have and have not responded.  The disaster event management system 204 can determine that a group of people may be trapped in a\nlocation or otherwise have not been able to respond and based on the employee directory information or the location data, can indicate where the first responders should search via the alerts system 208.  The directory information can include information\nincluding employee personnel data, contact information, employee identification number and job description data.  The directory information can also include information about job hierarchy (supervisors, direct reports, etc), and job location (e.g., where\nis the employee assigned, which office building, office number, etc).  The directory information can be stored in a database managed by a human resource department, or in a lightweight directory access protocol directory which is an open, vendor neutral\napplication protocol for accessing and maintaining distributed directory information services over an IP network.\nIn an embodiment, notifications to evacuate, or notifications about emergencies can be propagated by internal company mechanisms via the employer system 210.\nTurning now to FIG. 3, illustrated in embodiment 300 are exemplary communication pathways for a disaster event management system 304 in accordance with various aspects and embodiments of the subject disclosure.\nIn an embodiment, the disaster event management system 304 can be communicably coupled to a UE 302 via a variety of means, including via a mobile network 314 that uses a radio access network to communicate with the UE 302 via SMS messages 306,\nvoice calls 308, and/or IP data 310.  The disaster event management system 304 can also transmit and receive IP data to and from the UE 302 via a WiFi network 312 or other access network not related to the mobile network 314.\nIn an embodiment, the disaster event management system 304 can have a preferred mechanism of communication (e.g., IP data via an application on the UE 302) and then switch to another form if preferred means is unavailable.  In an embodiment, the\ncommunication preferences can be set by the employer system.  In other embodiments, the mobile network can set the preference order based on network conditions.  In still other embodiments, the disaster event management system 304 can set the preference\nbased on the type of disaster or emergency situation.\nIn an embodiment, the disaster event management system 304 can facilitate 2 way communications with the UE 302, allowing voice calls, data transfers (audio, images, video, etc) to and from the UE 302.  In an embodiment, the disaster event\nmanagement system 304 can facilitate communications to and from other UEs and UE 302.  In an embodiment, the disaster event management system 304 can facilitate asymmetric communications (e.g., broadband in one direction, and SMS in another) depending on\nthe network conditions, emergency situation, and other contextual information (employer preferences, and etc.).\nTurning now to FIG. 4, illustrated is an embodiment 400 of an example disaster event management system 402 and cell site or worksite 404 in accordance with various aspects and embodiments of the subject disclosure.\nIn one or more embodiments, disaster event management system 402 can determine that an emergency 412 has occurred in a region 404.  Region 404 can be an office building, town, city, region, or cell site serviced by network node 406.  In response\nto determining that an emergency has occurred, the disaster event management system 402 can track the locations of UEs 408 and 410 and determine that they are with the area 404 that might be affected by emergency 412.  The disaster event management\nsystem 402 can send a status request to one or more of UE 408 or 410, and update their status with an employer system in response to receiving feedback from the UE 408 and 410.  In an embodiment, the disaster event management system 402 may receive the\nstatus update without having sent the status request.\nThe disaster event management system 402 can track the location of UE 408 and 410 and if it is determined the UEs are within a region 404, update a status of the employee associated with the UE 102 in response to receiving feedback from the UE\n102.  The disaster event management system 402 can determine the location of the UE 408 and 410 based on GPC data received from the UE 408 and 410 or from network node 406 which can indicate to disaster event management system 402 that the UE 408 and 410\nare attached to the network node 406.  In an embodiment, the disaster event management system 402 can determine the location of the UEs from the network node 406 and other network nodes which can perform multilateration in order to determine the location\nof the mobile devices.\nThe disaster event management system 402 can determine when an emergency situation has occurred (e.g., from public safety organizations, alerts, social media data, etc.) based on alerts from the first responders system 414.  In an embodiment,\nthe disaster event management system 104 can determine from an employee directory 416 whether an employee is assigned to a worksite (e.g., facility, office building, region, etc) that is affected by the emergency 412.\nIn an embodiment, the disaster event management system 402 can start to receive status reports from UE 408 and/or UE 410 even before the disaster event management system 402 determines that an emergency 412 has occurred.  This information can\nindicate that there is some type of emergency, and other information provided with the status updates can indicate the nature of the emergency 412 (e.g., what type of emergency it is, where it is, when it happened, etc).  This information can be passed\nto a first responders system 414 (e.g., 911 service) which can alert the authorities to the emergency 412.  The information can also be passed to other monitoring and/or reporting persons or systems.  Images, audio, video, and other contextual\ninformation received from the UEs 408 and 410 can also be passed to the first responders system as well as to other UEs in the area to alert them about the emergency 412.\nIn an embodiment, the location of the UEs 408 and 410 can be passed to the first responder system 414 as well to indicate the locations of people that have responded, and not responded.  This can indicate to the authorities where people might\nneed rescuing, or areas of greater or lesser danger.\nIn an embodiment, the disaster event management system 402 can send specialized alerts to UEs depending on their location in area 404 as well as their jobs and roles (as defined by employee directory 416).  For instance, if an employee account\nassociated with UE 408 has a job as a lineman, a weather report indicating potential lightning can trigger the emergency response by the disaster event management system 402 for UE 408, when it may not trigger it for a user account associated with UE\n410.  The disaster event management system 402 can also ensure that mobile workers location is known and send a check-in request at certain time intervals or if moving outside a geo-fence of where they were scheduled to go.\nDeviations from planned routes can also trigger a status request from the disaster event management system 402.  If a company vehicle left a certain area or the real estate person left the region where they were going to show homes, then an\nautomated alert could be sent by disaster event management system 402 to the affected UE.\nIn an embodiment, the disaster event management system 402 can activate a mobile device camera or microphone (e.g., for an employer issued UE) if the UE has not responded to the status requests.  Information could be captured and recorded for\nuse by the first responders system 414 such as police, but privacy safeguards can be used to ensure inadvertent recording did not occur.  Automated sensors on video or still images could detect if large amounts of skin were showing without a related\nviolent act.  The disaster event management system 402 could incorporate AI image processing; machine learning and big unstructured data filtering in order to make inferences about the safety and status of employees.  The disaster event management system\n402 could also automatically contact supervisors if there have been no check-ins by the employee.\nIn an embodiment, UE 408 can send a push status update (e.g., initiated by the employee associated with 408) in response to local conditions.  For instance, the employee may be walking to their car, and feel threatened by somebody following\nthem.  The alert can be passed from the disaster event management system 402 to the first responders system 414, and a service can be offered to escort the employee associated with UE 408 to their car or building.\nThe disaster event management system 402 can also integrate a reverse 911 service where first responders can be told the locations of UEs (e.g., UE 408 or 410) that have not responded to status updates.  The reverse 911 service can especially be\noffered for those populations that are at-risk (e.g., elderly, sickly, etc).\nIn an embodiment, the disaster event management system 402 can facilitate informing others when a person has responded with their status or has not responded.  Each employee can have a list of numbers or email addresses to notify in case of\nemergency.  When the disaster event management system 402 receives a status report from the UE, the disaster event management system 402 can notify each of the numbers or email addresses via SMS, voice call, email, etc, that the employee associated with\nthe UE is ok.  The disaster event management system 402 can extract the information about the contacts to notify from the employee directory 416 (e.g., a LDAP directory).\nIn an embodiment, disaster event management system 402 can also update the employee status for status updates received within a predetermined time period after the emergency 412 has been determined to start.\nIn an embodiment, the disaster event management system 402 can also integrate wearable devices such as heart rate monitors, step counters, and other devices with the status.  Other smart devices that have onboard and/or attachable sensors such\nas mobile devices can also be integrated.  In one or more embodiments, implantable sensors and other devices can also be used to gather and transmit status information and contextual information.  If the disaster event management system 402 does not\nreceive a status update from UE 408, but does receive an indication that UE 408 and/or person associated with UE 408 has a heartbeat and/or is moving, then the disaster event management system 402 can automatically update the status of the employee based\non the received data.  The disaster event management system 402 can mark the status as temporary or interim until a formal status update is received from UE 408.  If the authorities are trying to rescue the employee associated with UE 408, the health\ninformation and other information can be passed to the first responders system 414 to give the authorities some context regarding the wellbeing of the person associated with UE 408.\nThe disaster event management system 402 can also integrate data from sensors and other device in area 404 and provide that information to the first responders system 414.  Sensors can include smoke detectors, fire alarms, thermostats, and etc.\nThe sensor information can be utilized by the first responders system to facilitate route planning and other response procedures.\nTurning now to FIG. 5, illustrated is an embodiment 500 of another example disaster event management system and cell site or worksite in accordance with various aspects and embodiments of the subject disclosure.\nIn the embodiment shown in disaster event management system 502, disaster event management system 502 can facilitate sharing data between UE 508 and 510, as well as other sources via data sharing system 512.\nThe disaster event management system 502 can provide a platform for informal and validated information sharing with all users in the company and/or between and within specific groups of the company, e.g., business groups, etc. For instance, UE\n508, can send image data or other contextual data about an emergency situation to disaster event management system 502 via network node 506.  The disaster event management system 502 can make the information shareable via data sharing system 512, and UE\n510 can access the images and other contextual data directly from data sharing system 512 via the network node 506 or other access network.  As an example, the data sharing system 512 can facilitate crowdsourcing data that can be compiled for all\ndisaster event management system users, regardless of which employer, then be displayed to users with an indicator of how many people reported similar findings or validated.  The reliability of the information can be rated by other users, so that over\ntime, more reliable information can be determined to be more trustworthy while information determined to be not trustworthy will not be propagated.  In an embodiment, the reliability of the information can be based on whether a message is marked as\n\"official\" or not or based on the sender of the message.  Such data can be used for data mining, statistics, and analytics that can provide insights based on word frequency, location, job role of person inputting information, etc. For example, if lots of\npeople are talking about needing blankets or bottled water, then emergency management team can know to provide such resources in a given area.  Data (including chats, timelines, requests, pictures, and etc., can be offloaded for post event analysis\nand/or summarized data can be maintained until after the emergency operation is completed.\nIn an embodiment, the data sharing system 512 can also enable a private or semiprivate chatroom access or communications channels between UEs 508 and 510.  In another embodiment, the data sharing system 512 can also enable UEs belonging to\ndifferent companies to communicate with each other and share status updates.  In this way, contextual information about an emergency situation can be shared among employees and employer systems for different companies that may be affected by an emergency\nevent.  For instance, different companies that share an office building can share information in response to the office building being evacuated during a fire or other emergency.\nAt least benefit of the person to person sharing of data is that companies can automate metrics collection, see what are key needs, and matching/searches for help do not rely on one's one personal network of contacts, clutter email, or result in\neach group having to create their own means of helping each other out.  Knowing a person also works for the same company can add a degree of safety in that they are more of a known entity than a total stranger.  Employees knowing they have a place to\nhouse their pets may also be more likely to head evacuation warnings\nThe data sharing system 512 can also enable direct person to person assistance of goods and services to fellow employees.  This can supplement donations to relief agencies and can be healing as it is more personal than a donation to a relief\nagency .  . . . Can give more of a sense of \"I personally helped and was able to take charge over something in my life during a time of crisis.  The disaster event management system 502 can also enables efficient tracking and searches to match what is\nbeing offered and people wanting what is being offered.  Individuals can offer to donate even before they know what employee would need the donation and potentially before the employee realizes their need.  As an example, employees in a first region\ncould begin identifying donations in anticipation of person from a second region being evacuated due to a hurricane.  The persons being evacuated do not need to just rely on their network of contacts, but can leverage everyone in the company donating\nlocally or even mailing if they are going to be out of their home area for a while due to disaster.  This could have also helped in wild fires to match people with animals, especially large animals like horses, with others who could help board the\nanimals.\nTurning now to FIG. 6, illustrated is an embodiment 600 of a disaster event management system 602.  Disaster event management system 602 can include an alerts component 608 that determines when an emergency situation has occurred.  The alerts\ncomponent 608 can determine that an emergency situation has occurred based on active advisories received from the National Weather Service, or based on alerts from government emergency management agencies (state authorities, police, fire, FEMA, and etc). The alerts component 608 can also determine that an emergency has occurred based on social media feedback.  If there is a lot of activity on social media accounts that have keywords matching one or more tracked keywords (\"tornado\", \"fire\", \"terrorist\",\n\"shooting\", \"earthquake\", and etc) along with a location matching an office site or route, or other location that may affect one or more employees, the alerts component 608 can send a status update request to the mobile devices of users that may be\naffected.\nThe tracking component 606 can track the location of mobile devices and other devices associated with the employees to determine whether the employees may be affected.  The tracking component 606 can use location data received from a mobile\nnetwork or mobile device, access management data (e.g., card swipes at buildings), calendar data (planned meetings at specified locations), and directory information (e.g., to which office an employee is assigned).\nAuthentication component 604 can verify the identification code received from the mobile device with employee directory information in an employee directory database to determine whether or not the information is valid.  If the ID matches, the\nupdate component 610 can mark the employee as safe.  If the ID does not match, or is not present the authentication component 604 can transmit a revalidation request to the mobile device.\nIn an embodiment, the update component 610 can also map customer data (HR, LDAP, etc .  . . ) to a standardized database that can be used for reporting.  The update component 610 can stream status update responses received back to the end\ncustomer who would then be responsible for correlation and reporting.\nFIGS. 7-8 illustrates processes in connection with the aforementioned systems.  The process in FIGS. 7-8 can be implemented for example by the systems in FIGS. 1-6 respectively.  While for purposes of simplicity of explanation, the methods are\nshown and described as a series of blocks, it is to be understood and appreciated that the claimed subject matter is not limited by the order of the blocks, as some blocks may occur in different orders and/or concurrently with other blocks from what is\ndepicted and described herein.  Moreover, not all illustrated blocks may be required to implement the methods described hereinafter.\nFIG. 7 illustrates an example method 700 for tracking employees during an emergency event in accordance with various aspects and embodiments of the subject disclosure.\nMethod 700 can begin at 702 wherein the method includes determining that an event that satisfies a criterion related to public safety has occurred.\nAt 704, the method can include determining that a user equipment associated with a user account is at a location associated with the event.\nAt 706, the method can include s sending a status update request to the user equipment.\nAt 708, the method can include, in response to receiving status update feedback from the user equipment, updating a safety status of the user account, the safety status of the user account relating to a safety of a person associated with the\nuser account.\nFIG. 8 illustrates an example method 800 for tracking employees during an emergency event in accordance with various aspects and embodiments of the subject disclosure.\nMethod 800 can begin at 802 wherein the method includes receiving, by device comprising a processor, a notification that an emergency event has occurred within a first defined distance from a worksite.\nAt 804, the method can include determining, by the device, that a user equipment associated with an employee is within a second defined distance from the worksite.\nAt 806, the method can include receiving, by the device, a status update from the user equipment, wherein the status update comprises a validation code.\nAt 808, the method includes determining, by the device, whether the validation code is a match with directory information associated with the user equipment.\nAt 810, the method can include in response to the receiving the status update and the validation code being determined to be the match with the directory information, updating, by the device, a status associated with the employee.\nReferring now to FIG. 9, illustrated is a schematic block diagram of an example end-user device such as a user equipment) that can be a mobile device 900 capable of connecting to a network in accordance with some embodiments described herein. \nAlthough a mobile handset 900 is illustrated herein, it will be understood that other devices can be a mobile device, and that the mobile handset 900 is merely illustrated to provide context for the embodiments of the various embodiments described\nherein.  The following discussion is intended to provide a brief, general description of an example of a suitable environment 900 in which the various embodiments can be implemented.  While the description includes a general context of\ncomputer-executable instructions embodied on a machine-readable storage medium, those skilled in the art will recognize that the various embodiments also can be implemented in combination with other program modules and/or as a combination of hardware and\nsoftware.\nGenerally, applications (e.g., program modules) can include routines, programs, components, data structures, etc., that perform particular tasks or implement particular abstract data types.  Moreover, those skilled in the art will appreciate\nthat the methods described herein can be practiced with other system configurations, including single-processor or multiprocessor systems, minicomputers, mainframe computers, as well as personal computers, hand-held computing devices,\nmicroprocessor-based or programmable consumer electronics, and the like, each of which can be operatively coupled to one or more associated devices.\nA computing device can typically include a variety of machine-readable media.  Machine-readable media can be any available media that can be accessed by the computer and includes both volatile and non-volatile media, removable and non-removable\nmedia.  By way of example and not limitation, computer-readable media can comprise computer storage media and communication media.  Computer storage media can include volatile and/or non-volatile media, removable and/or non-removable media implemented in\nany method or technology for storage of information, such as computer-readable instructions, data structures, program modules or other data.  Computer storage media can include, but is not limited to, RAM, ROM, EEPROM, flash memory or other memory\ntechnology, CD ROM, digital video disk (DVD) or other optical disk storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium which can be used to store the desired information and which can\nbe accessed by the computer.\nCommunication media typically embodies computer-readable instructions, data structures, program modules or other data in a modulated data signal such as a carrier wave or other transport mechanism, and includes any information delivery media. \nThe term \"modulated data signal\" means a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal.  By way of example, and not limitation, communication media includes wired media such as a\nwired network or direct-wired connection, and wireless media such as acoustic, RF, infrared and other wireless media.  Combinations of the any of the above should also be included within the scope of computer-readable media.\nThe handset 900 includes a processor 902 for controlling and processing all onboard operations and functions.  A memory 904 interfaces to the processor 902 for storage of data and one or more applications 906 (e.g., a video player software, user\nfeedback component software, etc.).  Other applications can include voice recognition of predetermined voice commands that facilitate initiation of the user feedback signals.  The applications 906 can be stored in the memory 904 and/or in a firmware 908,\nand executed by the processor 902 from either or both the memory 904 or/and the firmware 908.  The firmware 908 can also store startup code for execution in initializing the handset 900.  A communications component 910 interfaces to the processor 902 to\nfacilitate wired/wireless communication with external systems, e.g., cellular networks, VoIP networks, and so on.  Here, the communications component 910 can also include a suitable cellular transceiver 911 (e.g., a GSM transceiver) and/or an unlicensed\ntransceiver 913 (e.g., Wi-Fi, WiMax) for corresponding signal communications.  The handset 900 can be a device such as a cellular telephone, a PDA with mobile communications capabilities, and messaging-centric devices.  The communications component 910\nalso facilitates communications reception from terrestrial radio networks (e.g., broadcast), digital satellite radio networks, and Internet-based radio services networks.\nThe handset 900 includes a display 912 for displaying text, images, video, telephony functions (e.g., a Caller ID function), setup functions, and for user input.  For example, the display 912 can also be referred to as a \"screen\" that can\naccommodate the presentation of multimedia content (e.g., music metadata, messages, wallpaper, graphics, etc.).  The display 912 can also display videos and can facilitate the generation, editing and sharing of video quotes.  A serial I/O interface 914\nis provided in communication with the processor 902 to facilitate wired and/or wireless serial communications (e.g., USB, and/or IEEE 1394) through a hardwire connection, and other serial input devices (e.g., a keyboard, keypad, and mouse).  This\nsupports updating and troubleshooting the handset 900, for example.  Audio capabilities are provided with an audio I/O component 916, which can include a speaker for the output of audio signals related to, for example, indication that the user pressed\nthe proper key or key combination to initiate the user feedback signal.  The audio I/O component 916 also facilitates the input of audio signals through a microphone to record data and/or telephony voice data, and for inputting voice signals for\ntelephone conversations.\nThe handset 900 can include a slot interface 918 for accommodating a SIC (Subscriber Identity Component) in the form factor of a card Subscriber Identity Module (SIM) or universal SIM 920, and interfacing the SIM card 920 with the processor 902. However, it is to be appreciated that the SIM card 920 can be manufactured into the handset 900, and updated by downloading data and software.\nThe handset 900 can process IP data traffic through the communication component 910 to accommodate IP traffic from an IP network such as, for example, the Internet, a corporate intranet, a home network, a person area network, etc., through an\nISP or broadband cable provider.  Thus, VoIP traffic can be utilized by the handset 800 and IP-based multimedia content can be received in either an encoded or decoded format.\nA video processing component 922 (e.g., a camera) can be provided for decoding encoded multimedia content.  The video processing component 922 can aid in facilitating the generation, editing and sharing of video quotes.  The handset 900 also\nincludes a power source 924 in the form of batteries and/or an AC power subsystem, which power source 924 can interface to an external power system or charging equipment (not shown) by a power I/O component 926.\nThe handset 900 can also include a video component 930 for processing video content received and, for recording and transmitting video content.  For example, the video component 930 can facilitate the generation, editing and sharing of video\nquotes.  A location tracking component 932 facilitates geographically locating the handset 900.  As described hereinabove, this can occur when the user initiates the feedback signal automatically or manually.  A user input component 934 facilitates the\nuser initiating the quality feedback signal.  The user input component 934 can also facilitate the generation, editing and sharing of video quotes.  The user input component 934 can include such conventional input device technologies such as a keypad,\nkeyboard, mouse, stylus pen, and/or touch screen, for example.\nReferring again to the applications 906, a hysteresis component 936 facilitates the analysis and processing of hysteresis data, which is utilized to determine when to associate with the access point.  A software trigger component 938 can be\nprovided that facilitates triggering of the hysteresis component 938 when the Wi-Fi transceiver 913 detects the beacon of the access point.  A SIP client 940 enables the handset 900 to support SIP protocols and register the subscriber with the SIP\nregistrar server.  The applications 906 can also include a client 942 that provides at least the capability of discovery, play and store of multimedia content, for example, music.\nThe handset 900, as indicated above related to the communications component 810, includes an indoor network radio transceiver 913 (e.g., Wi-Fi transceiver).  This function supports the indoor radio link, such as IEEE 802.11, for the dual-mode\nGSM handset 900.  The handset 900 can accommodate at least satellite radio services through a handset that can combine wireless voice and digital radio chipsets into a single handheld device.\nReferring now to FIG. 10, there is illustrated a block diagram of a computer 1000 operable to execute the functions and operations performed in the described example embodiments.  For example, a network node (e.g., network node 406) may contain\ncomponents as described in FIG. 10.  The computer 1000 can provide networking and communication capabilities between a wired or wireless communication network and a server and/or communication device.  In order to provide additional context for various\naspects thereof, FIG. 10 and the following discussion are intended to provide a brief, general description of a suitable computing environment in which the various aspects of the embodiments can be implemented to facilitate the establishment of a\ntransaction between an entity and a third party.  While the description above is in the general context of computer-executable instructions that can run on one or more computers, those skilled in the art will recognize that the various embodiments also\ncan be implemented in combination with other program modules and/or as a combination of hardware and software.\nGenerally, program modules include routines, programs, components, data structures, etc., that perform particular tasks or implement particular abstract data types.  Moreover, those skilled in the art will appreciate that the inventive methods\ncan be practiced with other computer system configurations, including single-processor or multiprocessor computer systems, minicomputers, mainframe computers, as well as personal computers, hand-held computing devices, microprocessor-based or\nprogrammable consumer electronics, and the like, each of which can be operatively coupled to one or more associated devices.\nThe illustrated aspects of the various embodiments can also be practiced in distributed computing environments where certain tasks are performed by remote processing devices that are linked through a communications network.  In a distributed\ncomputing environment, program modules can be located in both local and remote memory storage devices.\nComputing devices typically include a variety of media, which can include computer-readable storage media or communications media, which two terms are used herein differently from one another as follows.\nComputer-readable storage media can be any available storage media that can be accessed by the computer and includes both volatile and nonvolatile media, removable and non-removable media.  By way of example, and not limitation,\ncomputer-readable storage media can be implemented in connection with any method or technology for storage of information such as computer-readable instructions, program modules, structured data, or unstructured data.  Computer-readable storage media can\ninclude, but are not limited to, RAM, ROM, EEPROM, flash memory or other memory technology, CD-ROM, digital versatile disk (DVD) or other optical disk storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or\nother tangible and/or non-transitory media which can be used to store desired information.  Computer-readable storage media can be accessed by one or more local or remote computing devices, e.g., via access requests, queries or other data retrieval\nprotocols, for a variety of operations with respect to the information stored by the medium.\nCommunications media can embody computer-readable instructions, data structures, program modules or other structured or unstructured data in a data signal such as a modulated data signal, e.g., a carrier wave or other transport mechanism, and\nincludes any information delivery or transport media.  The term \"modulated data signal\" or signals refers to a signal that has one or more of its characteristics set or changed in such a manner as to encode information in one or more signals.  By way of\nexample, and not limitation, communication media include wired media, such as a wired network or direct-wired connection, and wireless media such as acoustic, RF, infrared and other wireless media.\nWith reference to FIG. 10, implementing various aspects described herein with regards to the end-user device can include a computer 1000, the computer 1000 including a processing unit 1004, a system memory 1006 and a system bus 1008.  The system\nbus 1008 couples system components including, but not limited to, the system memory 1006 to the processing unit 1004.  The processing unit 1004 can be any of various commercially available processors.  Dual microprocessors and other multi-processor\narchitectures can also be employed as the processing unit 1004.\nThe system bus 1008 can be any of several types of bus structure that can further interconnect to a memory bus (with or without a memory controller), a peripheral bus, and a local bus using any of a variety of commercially available bus\narchitectures.  The system memory 1006 includes read-only memory (ROM) 1027 and random access memory (RAM) 1012.  A basic input/output system (BIOS) is stored in a non-volatile memory 1027 such as ROM, EPROM, EEPROM, which BIOS contains the basic\nroutines that help to transfer information between elements within the computer 1000, such as during start-up.  The RAM 1012 can also include a high-speed RAM such as static RAM for caching data.\nThe computer 1000 further includes an internal hard disk drive (HDD) 1014 (e.g., EIDE, SATA), which internal hard disk drive 1014 can also be configured for external use in a suitable chassis (not shown), a magnetic floppy disk drive (FDD) 1016,\n(e.g., to read from or write to a removable diskette 1018) and an optical disk drive 1020, (e.g., reading a CD-ROM disk 1022 or, to read from or write to other high capacity optical media such as the DVD).  The hard disk drive 1014, magnetic disk drive\n1016 and optical disk drive 1020 can be connected to the system bus 1008 by a hard disk drive interface 1024, a magnetic disk drive interface 1026 and an optical drive interface 1028, respectively.  The interface 1024 for external drive implementations\nincludes at least one or both of Universal Serial Bus (USB) and IEEE 1394 interface technologies.  Other external drive connection technologies are within contemplation of the subject embodiments.\nThe drives and their associated computer-readable media provide nonvolatile storage of data, data structures, computer-executable instructions, and so forth.  For the computer 1000 the drives and media accommodate the storage of any data in a\nsuitable digital format.  Although the description of computer-readable media above refers to a HDD, a removable magnetic diskette, and a removable optical media such as a CD or DVD, it should be appreciated by those skilled in the art that other types\nof media which are readable by a computer 1000, such as zip drives, magnetic cassettes, flash memory cards, cartridges, and the like, can also be used in the example operating environment, and further, that any such media can contain computer-executable\ninstructions for performing the methods of the disclosed embodiments.\nA number of program modules can be stored in the drives and RAM 1012, including an operating system 1030, one or more application programs 1032, other program modules 1034 and program data 1036.  All or portions of the operating system,\napplications, modules, and/or data can also be cached in the RAM 1012.  It is to be appreciated that the various embodiments can be implemented with various commercially available operating systems or combinations of operating systems.\nA user can enter commands and information into the computer 1000 through one or more wired/wireless input devices, e.g., a keyboard 1038 and a pointing device, such as a mouse 1040.  Other input devices (not shown) may include a microphone, an\nIR remote control, a joystick, a game pad, a stylus pen, touch screen, or the like.  These and other input devices are often connected to the processing unit 1004 through an input device interface 1042 that is coupled to the system bus 1008, but can be\nconnected by other interfaces, such as a parallel port, an IEEE 1394 serial port, a game port, a USB port, an IR interface, etc.\nA monitor 1044 or other type of display device is also connected to the system bus 1008 through an interface, such as a video adapter 1046.  In addition to the monitor 1044, a computer 1000 typically includes other peripheral output devices (not\nshown), such as speakers, printers, etc.\nThe computer 1000 can operate in a networked environment using logical connections by wired and/or wireless communications to one or more remote computers, such as a remote computer(s) 1048.  The remote computer(s) 1048 can be a workstation, a\nserver computer, a router, a personal computer, portable computer, microprocessor-based entertainment device, a peer device or other common network node, and typically includes many or all of the elements described relative to the computer, although, for\npurposes of brevity, only a memory/storage device 1050 is illustrated.  The logical connections depicted include wired/wireless connectivity to a local area network (LAN) 1052 and/or larger networks, e.g., a wide area network (WAN) 1054.  Such LAN and\nWAN networking environments are commonplace in offices and companies, and facilitate enterprise-wide computer networks, such as intranets, all of which may connect to a global communications network, e.g., the Internet.\nWhen used in a LAN networking environment, the computer 1000 is connected to the local network 1052 through a wired and/or wireless communication network interface or adapter 1056.  The adapter 1056 may facilitate wired or wireless communication\nto the LAN 1052, which may also include a wireless access point disposed thereon for communicating with the wireless adapter 1056.\nWhen used in a WAN networking environment, the computer 1000 can include a modem 1058, or is connected to a communications server on the WAN 1054, or has other means for establishing communications over the WAN 1054, such as by way of the\nInternet.  The modem 1058, which can be internal or external and a wired or wireless device, is connected to the system bus 1008 through the input device interface 1042.  In a networked environment, program modules depicted relative to the computer, or\nportions thereof, can be stored in the remote memory/storage device 1050.  It will be appreciated that the network connections shown are exemplary and other means of establishing a communications link between the computers can be used.\nThe computer is operable to communicate with any wireless devices or entities operatively disposed in wireless communication, e.g., a printer, scanner, desktop and/or portable computer, portable data assistant, communications satellite, any\npiece of equipment or location associated with a wirelessly detectable tag (e.g., a kiosk, news stand, restroom), and telephone.  This includes at least Wi-Fi and Bluetooth.TM.  wireless technologies.  Thus, the communication can be a predefined\nstructure as with a conventional network or simply an ad hoc communication between at least two devices.\nWi-Fi, or Wireless Fidelity, allows connection to the Internet from a couch at home, a bed in a hotel room, or a conference room at work, without wires.  Wi-Fi is a wireless technology similar to that used in a cell phone that enables such\ndevices, e.g., computers, to send and receive data indoors and out; anywhere within the range of a base station.  Wi-Fi networks use radio technologies called IEEE802.11 (a, b, g, n, etc.) to provide secure, reliable, fast wireless connectivity.  A Wi-Fi\nnetwork can be used to connect computers to each other, to the Internet, and to wired networks (which use IEEE802.3 or Ethernet).  Wi-Fi networks operate in the unlicensed 2.4 and 5 GHz radio bands, at an 11 Mbps (802.11b) or 54 Mbps (802.11a) data rate,\nfor example, or with products that contain both bands (dual band), so the networks can provide real-world performance similar to the basic \"10BaseT\" wired Ethernet networks used in many offices.\nAs used in this application, the terms \"system,\" \"component,\" \"interface,\" and the like are generally intended to refer to a computer-related entity or an entity related to an operational machine with one or more specific functionalities.  The\nentities disclosed herein can be either hardware, a combination of hardware and software, software, or software in execution.  For example, a component may be, but is not limited to being, a process running on a processor, a processor, an object, an\nexecutable, a thread of execution, a program, and/or a computer.  By way of illustration, both an application running on a server and the server can be a component.  One or more components may reside within a process and/or thread of execution and a\ncomponent may be localized on one computer and/or distributed between two or more computers.  These components also can execute from various computer readable storage media having various data structures stored thereon.  The components may communicate\nvia local and/or remote processes such as in accordance with a signal having one or more data packets (e.g., data from one component interacting with another component in a local system, distributed system, and/or across a network such as the Internet\nwith other systems via the signal).  As another example, a component can be an apparatus with specific functionality provided by mechanical parts operated by electric or electronic circuitry that is operated by software or firmware application(s)\nexecuted by a processor, wherein the processor can be internal or external to the apparatus and executes at least a part of the software or firmware application.  As yet another example, a component can be an apparatus that provides specific\nfunctionality through electronic components without mechanical parts, the electronic components can comprise a processor therein to execute software or firmware that confers at least in part the functionality of the electronic components.  An interface\ncan comprise input/output (I/O) components as well as associated processor, application, and/or API components.\nFurthermore, the disclosed subject matter may be implemented as a method, apparatus, or article of manufacture using standard programming and/or engineering techniques to produce software, firmware, hardware, or any combination thereof to\ncontrol a computer to implement the disclosed subject matter.  The term \"article of manufacture\" as used herein is intended to encompass a computer program accessible from any computer-readable device, computer-readable carrier, or computer-readable\nmedia.  For example, computer-readable media can include, but are not limited to, a magnetic storage device, e.g., hard disk; floppy disk; magnetic strip(s); an optical disk (e.g., compact disk (CD), a digital video disc (DVD), a Blu-ray Disc.TM.  (BD));\na smart card; a flash memory device (e.g., card, stick, key drive); and/or a virtual device that emulates a storage device and/or any of the above computer-readable media.\nAs it employed in the subject specification, the term \"processor\" can refer to substantially any computing processing unit or device comprising, but not limited to comprising, single-core processors; single-processors with software multithread\nexecution capability; multi-core processors; multi-core processors with software multithread execution capability; multi-core processors with hardware multithread technology; parallel platforms; and parallel platforms with distributed shared memory. \nAdditionally, a processor can refer to an integrated circuit, an application specific integrated circuit (ASIC), a digital signal processor (DSP), a field programmable gate array (FPGA), a programmable logic controller (PLC), a complex programmable logic\ndevice (CPLD), a discrete gate or transistor logic, discrete hardware components, or any combination thereof designed to perform the functions described herein.  Processors can exploit nano-scale architectures such as, but not limited to, molecular and\nquantum-dot based transistors, switches and gates, in order to optimize space usage or enhance performance of user equipment.  A processor also can be implemented as a combination of computing processing units.\nIn the subject specification, terms such as \"store,\" \"data store,\" \"data storage,\" \"database,\" \"repository,\" \"queue\", and substantially any other information storage component relevant to operation and functionality of a component, refer to\n\"memory components,\" or entities embodied in a \"memory\" or components comprising the memory.  It will be appreciated that the memory components described herein can be either volatile memory or nonvolatile memory, or can comprise both volatile and\nnonvolatile memory.  In addition, memory components or memory elements can be removable or stationary.  Moreover, memory can be internal or external to a device or component, or removable or stationary.  Memory can comprise various types of media that\nare readable by a computer, such as hard-disc drives, zip drives, magnetic cassettes, flash memory cards or other types of memory cards, cartridges, or the like.\nBy way of illustration, and not limitation, nonvolatile memory can comprise read only memory (ROM), programmable ROM (PROM), electrically programmable ROM (EPROM), electrically erasable ROM (EEPROM), or flash memory.  Volatile memory can\ncomprise random access memory (RAM), which acts as external cache memory.  By way of illustration and not limitation, RAM is available in many forms such as synchronous RAM (SRAM), dynamic RAM (DRAM), synchronous DRAM (SDRAM), double data rate SDRAM (DDR\nSDRAM), enhanced SDRAM (ESDRAM), Synchlink DRAM (SLDRAM), and direct Rambus RAM (DRRAM).  Additionally, the disclosed memory components of systems or methods herein are intended to comprise, without being limited to comprising, these and any other\nsuitable types of memory.\nIn particular and in regard to the various functions performed by the above described components, devices, circuits, systems and the like, the terms (including a reference to a \"means\") used to describe such components are intended to\ncorrespond, unless otherwise indicated, to any component which performs the specified function of the described component (e.g., a functional equivalent), even though not structurally equivalent to the disclosed structure, which performs the function in\nthe herein illustrated example aspects of the embodiments.  In this regard, it will also be recognized that the embodiments comprises a system as well as a computer-readable medium having computer-executable instructions for performing the acts and/or\nevents of the various methods.\nComputing devices typically comprise a variety of media, which can comprise computer-readable storage media and/or communications media, which two terms are used herein differently from one another as follows.  Computer-readable storage media\ncan be any available storage media that can be accessed by the computer and comprises both volatile and nonvolatile media, removable and non-removable media.  By way of example, and not limitation, computer-readable storage media can be implemented in\nconnection with any method or technology for storage of information such as computer-readable instructions, program modules, structured data, or unstructured data.  Computer-readable storage media can comprise, but are not limited to, RAM, ROM, EEPROM,\nflash memory or other memory technology, CD-ROM, digital versatile disk (DVD) or other optical disk storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or other tangible and/or non-transitory media which\ncan be used to store desired information.  Computer-readable storage media can be accessed by one or more local or remote computing devices, e.g., via access requests, queries or other data retrieval protocols, for a variety of operations with respect to\nthe information stored by the medium.\nOn the other hand, communications media typically embody computer-readable instructions, data structures, program modules or other structured or unstructured data in a data signal such as a modulated data signal, e.g., a carrier wave or other\ntransport mechanism, and comprises any information delivery or transport media.  The term \"modulated data signal\" or signals refers to a signal that has one or more of its characteristics set or changed in such a manner as to encode information in one or\nmore signals.  By way of example, and not limitation, communications media comprise wired media, such as a wired network or direct-wired connection, and wireless media such as acoustic, RF, infrared and other wireless media\nFurther, terms like \"user equipment,\" \"user device,\" \"mobile device,\" \"mobile,\" station,\" \"access terminal,\" \"terminal,\" \"handset,\" and similar terminology, generally refer to a wireless device utilized by a subscriber or user of a wireless\ncommunication network or service to receive or convey data, control, voice, video, sound, gaming, or substantially any data-stream or signaling-stream.  The foregoing terms are utilized interchangeably in the subject specification and related drawings. \nLikewise, the terms \"access point,\" \"node B,\" \"base station,\" \"evolved Node B,\" \"cell,\" \"cell site,\" and the like, can be utilized interchangeably in the subject application, and refer to a wireless network component or appliance that serves and receives\ndata, control, voice, video, sound, gaming, or substantially any data-stream or signaling-stream from a set of subscriber stations.  Data and signaling streams can be packetized or frame-based flows.  It is noted that in the subject specification and\ndrawings, context or explicit distinction provides differentiation with respect to access points or base stations that serve and receive data from a mobile device in an outdoor environment, and access points or base stations that operate in a confined,\nprimarily indoor environment overlaid in an outdoor coverage area.  Data and signaling streams can be packetized or frame-based flows.\nFurthermore, the terms \"user,\" \"subscriber,\" \"customer,\" \"consumer,\" and the like are employed interchangeably throughout the subject specification, unless context warrants particular distinction(s) among the terms.  It should be appreciated\nthat such terms can refer to human entities, associated devices, or automated components supported through artificial intelligence (e.g., a capacity to make inference based on complex mathematical formalisms) which can provide simulated vision, sound\nrecognition and so forth.  In addition, the terms \"wireless network\" and \"network\" are used interchangeable in the subject application, when context wherein the term is utilized warrants distinction for clarity purposes such distinction is made explicit.\nMoreover, the word \"exemplary\" is used herein to mean serving as an example, instance, or illustration.  Any aspect or design described herein as \"exemplary\" is not necessarily to be construed as preferred or advantageous over other aspects or\ndesigns.  Rather, use of the word exemplary is intended to present concepts in a concrete fashion.  As used in this application, the term \"or\" is intended to mean an inclusive \"or\" rather than an exclusive \"or\".  That is, unless specified otherwise, or\nclear from context, \"X employs A or B\" is intended to mean any of the natural inclusive permutations.  That is, if X employs A; X employs B; or X employs both A and B, then \"X employs A or B\" is satisfied under any of the foregoing instances.  In\naddition, the articles \"a\" and \"an\" as used in this application and the appended claims should generally be construed to mean \"one or more\" unless specified otherwise or clear from context to be directed to a singular form.\nIn addition, while a particular feature may have been disclosed with respect to only one of several implementations, such feature may be combined with one or more other features of the other implementations as may be desired and advantageous for\nany given or particular application.  Furthermore, to the extent that the terms \"includes\" and \"including\" and variants thereof are used in either the detailed description or the claims, these terms are intended to be inclusive in a manner similar to the\nterm \"comprising.\"\nThe above descriptions of various embodiments of the subject disclosure and corresponding figures and what is described in the Abstract, are described herein for illustrative purposes, and are not intended to be exhaustive or to limit the\ndisclosed embodiments to the precise forms disclosed.  It is to be understood that one of ordinary skill in the art may recognize that other embodiments having modifications, permutations, combinations, and additions can be implemented for performing the\nsame, similar, alternative, or substitute functions of the disclosed subject matter, and are therefore considered within the scope of this disclosure.  Therefore, the disclosed subject matter should not be limited to any single embodiment described\nherein, but rather should be construed in breadth and scope in accordance with the claims below.", "application_number": "15698785", "abstract": " Various embodiments disclosed herein provide for a disaster event\n     management system that can track location of employees and other affected\n     during disaster events and other emergency situations and determine their\n     safety status. The disaster event management system can determine when an\n     emergency event has occurred, and determine which employees are likely to\n     be affected by the emergency, based on their location at the time and\n     other directory information. The system can provide an interface on user\n     equipment devices operated by the employees (mobile devices, laptops,\n     computers, tablets, etc) to provide their status along with an\n     identification code to verify their identification. In an embodiment, the\n     system can prompt the user equipment devices to provide a status in\n     response to determining that the employee may be affected by the\n     emergency situation.\n", "citations": ["5560852", "5793882", "6956474", "7468658", "7605696", "8903424", "9116230", "9848313", "9947199", "20140230030", "20150317809", "20150358794", "20150364017", "20160005293", "20160044625", "20160050037", "20160371966", "20170191843", "20170195833", "20170230786", "20170251347", "20180098206", "20180176362"], "related": []}, {"id": "20190090334", "patent_code": "10375808", "patent_name": "Discharge lamp drive device, light source device, projector, and discharge\n     lamp drive method", "year": "2019", "inventor_and_country_data": " Inventors: \nSuzuki; Junichi (Matsumoto, JP), Kono; Masaru (Shimoina-gun, JP)  ", "description": "<BR><BR>BACKGROUND\n<BR><BR>1.  Technical Field\nThe present invention relates to a discharge lamp drive device, a light source device, a projector, and a discharge lamp drive method.\n<BR><BR>2.  Related Art\nAs shown in, for example, JP-A-2016-018746, there has been known a discharge lamp lighting device for changing a pulse of an AC current supplied to a discharge lamp in accordance with a value of an applied voltage applied to the discharge lamp.\nHowever, there is an individual difference between the discharge lamps, and the change in the applied voltage (inter-electrode voltage) applied to the discharge lamp differs by the individual discharge lamp.  Therefore, depending on the drive\nmethod which is not capable of considering the individual difference between the discharge lamps, the life of the discharge lamp cannot sufficiently be extended in some cases.\n<BR><BR>SUMMARY\nAn advantage of some aspects of the invention is to provide a discharge lamp drive device capable of extending the life of the discharge lamp irrespective of the individual difference between the discharge lamps, a light source device equipped\nwith such a discharge lamp drive device, and a projector equipped with such a light source device.  Another advantage of some aspects of the invention is to provide a discharge lamp drive method capable of extending the life of the discharge lamp\nirrespective of the individual difference between the discharge lamps.\nA discharge lamp drive device according to an aspect of the invention includes a discharge lamp driver adapted to supply a drive current to a discharge lamp having a first electrode and a second electrode, a control section adapted to control\nthe discharge lamp driver, and a storage section adapted to store a plurality of drive patterns of the drive current, the drive patterns each have a plurality of drive parameters, the plurality of drive patterns includes a first drive pattern and a\nsecond drive pattern different in a value of at least one of the drive parameters from each other, and the control section executes the first drive pattern and the second drive pattern in accordance with at least one of accumulated lighting time of the\ndischarge lamp and an individual of the discharge lamp in a case in which an inter-electrode voltage of the discharge lamp is at a predetermined voltage value.\nAccording to the discharge lamp drive device related to the aspect of the invention, in the case in which the control section executes the drive pattern in accordance with the accumulated lighting time, it is possible to make the drive pattern\nto be executed differ by the drive time of the discharge lamp even in the same inter-electrode voltage.  Thus, it is easy to execute the preferable drive pattern even in the case in which the individual difference exists between the discharge lamps. \nMeanwhile, in the case in which the control section executes the drive pattern in accordance with the individual of the discharge lamp, it is possible to make the drive pattern to be executed different between the individuals of the discharge lamp even\nin the same inter-electrode voltage.  Thus, it is easy to execute the preferable drive pattern even in the case in which the individual difference exists between the discharge lamps.  Therefore, according to the discharge lamp drive device related to\nthis aspect of the invention, the life of the discharge lamp can be extended irrespective of the individual difference between the discharge lamps.\nThe discharge lamp drive device may be configured such that the control section selects any one of the drive patterns based on machine learning, and executes the drive pattern thus selected.\nAccording to this configuration, by performing the machine learning, even in the case in which the individual difference exists between the discharge lamps, it is possible to select the preferable drive pattern from the first drive pattern and\nthe second drive pattern in accordance with the individual difference between the discharge lamps.  Therefore, according to this configuration, it is possible to further extend the life of the discharge lamp irrespective of the individual difference\nbetween the discharge lamps.\nThe discharge lamp drive device may be configured such that in an execution period in which the first drive pattern and the second drive pattern are each executed at least one or more times, a proportion of execution time during which the first\ndrive pattern is executed to a length of the execution period and a proportion of execution time during which the second drive pattern is executed to the length of the execution period are different from each other.\nAccording to this configuration, it is possible to elongate the time in which more preferable one of the drive patterns is executed in accordance with the condition of the discharge lamp.  Thus, it is possible to further extend the life of the\ndischarge lamp.\nThe discharge lamp drive device may be configured such that, in an execution period in which the first drive pattern and the second drive pattern are each executed at least one or more times, a number of times that the first drive pattern is\nexecuted and a number of times that the second drive pattern is executed are different from each other.\nAccording to this configuration, it is possible to increase the number of times that more preferable one of the drive patterns is executed in accordance with the condition of the discharge lamp.  Thus, it is possible to further extend the life\nof the discharge lamp.\nThe discharge lamp drive device may be configured such that, in a case in which the inter-electrode voltage is at a predetermined voltage value, a probability that the first drive pattern is executed and a probability that the second drive\npattern is executed are different from each other.\nAccording to this configuration, it is possible to make it easy to execute more preferable one of the drive patterns in accordance with the condition of the discharge lamp.  Thus, it is possible to further extend the life of the discharge lamp.\nThe discharge lamp drive device may be configured such that the control section executes the first drive pattern in a case in which the inter-electrode voltage is at the predetermined voltage value and the accumulated lighting time is equal to\nfirst accumulated lighting time, and the second drive pattern in a case in which the inter-electrode voltage is at the predetermined voltage value and the accumulated lighting time is equal to second accumulated lighting time different from the first\naccumulated lighting time.\nAccording to this configuration, even in the same inter-electrode voltage, it is possible to make the drive pattern to be executed differ by the drive time of the discharge lamp.  Thus, it is possible to extend the life of the discharge lamp.\nThe discharge lamp drive device may be configured such that the control section executes the first drive pattern in a case in which the inter-electrode voltage is at the predetermined voltage value and the discharge lamp is a first individual,\nand the second drive pattern in a case in which the inter-electrode voltage is at the predetermined voltage value and the discharge lamp is a second individual different from the first individual.\nAccording to this configuration, even in the same inter-electrode voltage, it is possible to make the drive pattern to be executed different between the individuals of the discharge lamp.  Thus, it is possible to extend the life of the discharge\nlamp.\nThe discharge lamp drive device may be configured such that a number of the drive patterns is no less than 20 and no more than 30.\nAccording to this configuration, it is easy to select the preferable drive pattern corresponding to the state of the discharge lamp, and the life of the discharge lamp can further be extended.\nA light source device according to an aspect of the invention includes a discharge lamp adapted to emit light, and any one of the discharge lamp drive devices described above.\nAccording to the light source device related to the aspect of the invention, since the discharge lamp drive device described above is provided, the life of the discharge lamp can be extended similarly to the aspects described above.\nA projector according to an aspect of the invention includes the light source device described above, a light modulation device adapted to modulate light emitted from the light source device in accordance with an image signal, and a projection\noptical system adapted to project the light modulated by the light modulation device.\nAccording to the projector related to the aspect of the invention, since the light source device described above is provided, the life of the discharge lamp can be extended similarly to the aspects described above.\nA discharge lamp drive method according to an aspect of the invention is a discharge lamp drive method adapted to supply a drive current to a discharge lamp having a first electrode and a second electrode to drive the discharge lamp, the method\nincluding the steps of supplying the discharge lamp with the drive current in accordance with a plurality of drive patterns of the drive current, the plurality of drive patterns each having a plurality of drive parameters, and executing a first drive\npattern and a second drive pattern which are included in the plurality of drive patterns and different in a value of at least one of the drive parameters from each other in accordance with at least one of accumulated lighting time of the discharge lamp\nand an individual of the discharge lamp in a case in which an inter-electrode voltage of the discharge lamp is at a predetermined voltage value.\nAccording to the discharge lamp drive method related to the aspect of the invention, the life of the discharge lamp can be extended similarly to the aspects described above. <BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nThe invention will be described with reference to the accompanying drawings, wherein like numbers reference like elements.\nFIG. 1 is a schematic configuration diagram showing a projector according to an embodiment of the invention.\nFIG. 2 is a diagram showing a discharge lamp in the embodiment.\nFIG. 3 is a block diagram showing a variety of constituents of the projector according to the embodiment.\nFIG. 4 is a circuit diagram of a discharge lamp lighting device of the embodiment.\nFIG. 5 is a block diagram showing a configuration example of a control section of the embodiment.\nFIG. 6A is a diagram showing an appearance of a projection at a tip of an electrode of the discharge lamp.\nFIG. 6B is a diagram showing an appearance of the projection at the tip of the electrode of the discharge lamp.\nFIG. 7 is a diagram showing an example of a drive current waveform supplied to the discharge lamp in an AC drive operation of the embodiment.\nFIG. 8A is a diagram showing an example of a drive current waveform supplied to the discharge lamp in a DC drive operation of the embodiment.\nFIG. 8B is a diagram showing an example of the drive current waveform supplied to the discharge lamp in the DC drive operation of the embodiment.\nFIG. 9 is a diagram showing an example of a drive pattern of a drive current supplied to the discharge lamp in the embodiment.\nFIG. 10 is a flowchart showing an example of a control procedure of a control section in an initial learning period of the embodiment.\nFIG. 11 is a flowchart showing an example of a control procedure of the control section in a steady learning period of the embodiment.\nFIG. 12A is a diagram showing an example of a drive current waveform supplied to the discharge lamp in an imbalanced drive operation of the embodiment.\nFIG. 12B is a diagram showing an example of the drive current waveform supplied to the discharge lamp in the imbalanced drive operation of the embodiment.\nFIG. 13 is a diagram showing an example of a drive current waveform supplied to the discharge lamp in a jumping drive operation of the embodiment.\n<BR><BR>DESCRIPTION OF AN EXEMPLARY EMBODIMENT\nA projector according to an embodiment of the invention will hereinafter be described with reference to the accompanying drawings.\nIt should be noted that the scope of the invention is not limited to the embodiment hereinafter described, but can arbitrarily be modified within the technical idea or the technical concept of the invention.  Further, in the drawings described\nbelow, the actual structures and the structures of the drawings are made different from each other in scale size, number, and so on of each of the constituents in some cases in order to make the constituents easy to understand.\nFIG. 1 is a schematic configuration diagram showing the projector 500 according to the present embodiment.  As shown in FIG. 1, the projector 500 according to the present embodiment is provided with a light source device 200, a collimating lens\n305, an illumination optical system 310, a color separation optical system 320, three liquid crystal light valves (light modulation devices) 330R, 330G, and 330B, a cross dichroic prism 340, and a projection optical system 350.\nThe light emitted from the light source device 200 passes through the collimating lens 305, and then enters the illumination optical system 310.  The collimating lens 305 collimates the light from the light source device 200.\nThe illumination optical system 310 adjusts the illuminance of the light emitted from the light source device 200 so as to be homogenized on the liquid crystal light valves 330R, 330G, and 330B.  Further, the illumination optical system 310\nuniforms the polarization direction of the light emitted from the light source device 200 into one direction.  The reason therefor is to effectively utilize the light emitted from the light source device 200 in the liquid crystal light valves 330R, 330G,\nand 330B.\nThe light adjusted in the illuminance and the polarization direction enters the color separation optical system 320.  The color separation optical system 320 separates the incident light into three colored light beams, namely a red light beam\n(R), a green light beam (G), and a blue light beam (B).  The liquid crystal light valves 330R, 330G, and 330B associated with the respective colored light beams modulate the three colored light beams, respectively, in accordance with an image signal. \nThe liquid crystal light valves 330R, 330G, and 330B are provided with liquid crystal panels 560R, 560G, and 560B described later, and polarization plates (not shown), respectively.  The polarization plates are disposed on the light incident side and the\nlight exit side of each of the liquid crystal panels 560R, 560G, and 560B.\nThe three colored light beams thus modulated are combined with each other by the cross dichroic prism 340.  The composite light enters the projection optical system 350.  The projection optical system 350 projects the incident light on a screen\n700 (see FIG. 3).  Thus, an image is displayed on the screen 700.  It should be noted that a known configuration can be adopted as a configuration of each of the collimating lens 305, the illumination optical system 310, the color separation optical\nsystem 320, the cross dichroic prism 340, and the projection optical system 350.\nFIG. 2 is a cross-sectional view showing a configuration of the light source device 200.  The light source device 200 is provided with a light source unit 210 and a discharge lamp lighting device (discharge lamp drive device) 10.  FIG. 2 shows a\ncross-sectional view of the light source unit 210.  The light source unit 210 is provided with a main reflecting mirror 112, a discharge lamp 90, and a sub-reflecting mirror 113.\nThe discharge lamp lighting device 10 supplies the discharge lamp 90 with a drive current I to thereby light the discharge lamp 90.  The main reflecting mirror 112 reflects the light, which is emitted from the discharge lamp 90, toward an\nirradiation direction D. The irradiation direction D is parallel to an optical axis AX of the discharge lamp 90.\nThe discharge lamp 90 has a rod-like shape extending along the irradiation direction D. One end part of the discharge lamp 90 is defined as a first end portion 90e1, and the other end part of the discharge lamp 90 is defined as a second end\nportion 90e2.  A material of the discharge lamp 90 is a light transmissive material such as quartz glass.  A central portion of the discharge lamp 90 bulges to have a spherical shape, and a discharge space 91 is formed inside the central portion.  In the\ndischarge space 91, there is encapsulated a gas as a discharge medium including a noble gas, a metallic halide, or the like.\nIn the discharge space 91, there are projected the tips of the first electrode 92 and the second electrode 93.  The first electrode 92 is disposed on the first end portion 90e1 side of the discharge space 91.  The second electrode 93 is disposed\non the second end portion 90e2 side of the discharge space 91.  Each of the first electrode 92 and the second electrode 93 has a rod-like shape extending along the optical axis AX.  In the discharge space 91, there are disposed electrode tip portions of\nthe first electrode 92 and the second electrode 93 so as to be opposed to each other with a predetermined distance.  The material of the first electrode 92 and the second electrode 93 is metal such as tungsten.\nThe first end portion 90e1 of the discharge lamp 90 is provided with a first terminal 536.  The first terminal 536 and the first electrode 92 are electrically connected to each other with a conductive member 534 penetrating the inside of the\ndischarge lamp 90.  Similarly, the second end portion 90e2 of the discharge lamp 90 is provided with a second terminal 546.  The second terminal 546 and the second electrode 93 are electrically connected to each other with a conductive member 544\npenetrating the inside of the discharge lamp 90.  The material of the first terminal 536 and the second terminal 546 is metal such as tungsten.  As the material of the conductive members 534, 544, there is used, for example, molybdenum foil.\nThe first terminal 536 and the second terminal 546 are connected to the discharge lamp lighting device 10.  The discharge lamp lighting device 10 supplies the first terminal 536 and the second terminal 546 with the drive current I for driving\nthe discharge lamp 90.  As a result, arc discharge is caused between the first electrode 92 and the second electrode 93.  The light (discharge light) generated by the arc discharge is emitted from the discharge position in all directions as indicated by\nthe dashed arrows.\nThe main reflecting mirror 112 is fixed to the first end portion 90e1 of the discharge lamp 90 with a fixation member 114.  Out of the discharge light, the light proceeding toward the opposite direction to the irradiation direction D is\nreflected by the main reflecting mirror 112 toward the irradiation direction D. The shape of the reflecting surface (the surface on the discharge lamp 90 side) of the main reflecting mirror 112 is not particularly limited as long as the discharge light\ncan be reflected toward the irradiation direction D, and can also be, for example, a spheroidal shape or a paraboloidal shape.  In the case of, for example, adopting the paraboloidal shape as the shape of the reflecting surface of the main reflecting\nmirror 112, the main reflecting mirror 112 is capable of converting the discharge light into the light roughly parallel to the optical axis AX.  Thus, the collimating lens 305 can be eliminated.\nThe sub-reflecting mirror 113 is fixed to the second end portion 90e2 side of the discharge lamp 90 with a fixation member 522.  A reflecting surface (a surface on the discharge lamp 90 side) of the sub-reflecting mirror 113 has a spherical\nshape surrounding a part on the second end portion 90e2 side of the discharge space 91.  Out of the discharge light, the light proceeding toward the opposite side to the side where the main reflecting mirror 112 is disposed is reflected by the\nsub-reflecting mirror 113 toward the main reflecting mirror 112.  Thus, the utilization efficiency of the light radiated from the discharge space 91 can be improved.\nThe material of the fixation members 114, 522 is not particularly limited as long as the material is a heat-resistant material tolerable to the heat generated by the discharge lamp 90, and is, for example, an inorganic adhesive.  As the method\nof fixing the arrangement of the main reflecting mirror 112 and the sub-reflecting mirror 113 with respect to the discharge lamp 90, an arbitrary method can be adopted besides the method of fixing the main reflecting mirror 112 and the sub-reflecting\nmirror 113 to the discharge lamp 90.  For example, it is also possible to fix the discharge lamp 90 and the main reflecting mirror 112 independently to a housing (not shown) of the projector 500.  The same applies to the sub-reflecting mirror 113.\nA circuit configuration of the projector 500 will hereinafter be described.\nFIG. 3 is a diagram showing an example of the circuit configuration of the projector 500 according to the present embodiment.  The projector 500 is provided with an image signal conversion section 510, a DC power supply device 80, liquid crystal\npanels 560R, 560G, and 560B, an image processing device 570, and a central processing unit (CPU) 580 besides the optical system shown in FIG. 1.\nThe image signal conversion section 510 converts an image signal 502 (e.g., a luminance/color-difference signal or an analog RGB signal) input from the outside into a digital RGB signal of a predetermined word length to thereby generate image\nsignals 512R, 512G, and 512B, and then supplies the image signals 512R, 512G, and 512B to the image processing device 570.\nThe image processing device 570 performs image processing on each of the three image signals 512R, 512G, and 512B.  The image processing device 570 supplies the liquid crystal panels 560R, 560G, and 560B with drive signals 572R, 572G, and 572B\nfor driving the liquid crystal panels 560R, 560G, and 560B, respectively.\nThe DC power supply device 80 converts the AC voltage supplied from an external AC power supply 600 into a constant DC voltage.  The DC power supply device 80 supplies the DC voltage to the image signal conversion section 510 and the image\nprocessing device 570 located on the secondary side of a transformer (not shown, but included in the DC power supply device 80) and the discharge lamp lighting device 10 located on the primary side of the transformer.\nThe discharge lamp lighting device 10 generates a high voltage between the electrodes of the discharge lamp 90 at the time of startup to cause insulation breakdown to thereby form a discharge path.  Thereafter, the discharge lamp lighting device\n10 supplies the drive current I for the discharge lamp 90 to keep the discharge.\nThe liquid crystal panels 560R, 560G, and 560B are provided respectively to the liquid crystal light valves 330R, 330G, and 330B described above.  The transmittance (luminance) of the colored light beams entering the liquid crystal panels 560R,\n560G, and 560B via the optical system described above is modulated by liquid crystal panels 560R, 560G, and 560B based on the drive signals 572R, 572G, and 572B, respectively.\nThe CPU 580 controls a variety of operations of the projector 500 from the start of lighting to the extinction.  For example, in the example shown in FIG. 3, a lighting command and an extinction command are output to the discharge lamp lighting\ndevice 10 via a communication signal 582.  The CPU 580 receives lighting information of the discharge lamp 90 from the discharge lamp lighting device 10 via the communication signal 584.\nA configuration of the discharge lamp lighting device 10 will hereinafter be described.\nFIG. 4 is a diagram showing an example of a circuit configuration of the discharge lamp lighting device 10.\nAs shown in FIG. 4, the discharge lamp lighting device 10 is provided with a power control circuit 20, a polarity inverting circuit 30, a control section 40, an operation detection section 60, and an igniter circuit 70.\nThe power control circuit 20 generates drive power Wd to be supplied to the discharge lamp 90.  In the present embodiment, the power control circuit 20 is formed of a down-chopper circuit receiving a voltage from the DC power supply device 80 as\nan input, and stepping down the input voltage to output a DC current Id.\nThe power control circuit 20 is configured including a switch element 21, a diode 22, a coil 23, and a capacitor 24.  The switch element 21 is formed of, for example, a transistor.  In the present embodiment, one end of the switch element 21 is\nconnected to a positive voltage side of the DC power supply device 80, and the other end thereof is connected to the cathode terminal of the diode 22 and one end of the coil 23.\nOne end of the capacitor 24 is connected to the other end of the coil 23, and the other end of the capacitor 24 is connected to an anode terminal of the diode 22 and a negative voltage side of the DC power supply device 80.  A current control\nsignal is input to the control terminal of the switch element 21 from the control section 40 described later, and thus, ON/OFF of the switch element 21 is controlled.  As the current control signal, a pulse width modulation (PWM) control signal can be\nused, for example.\nWhen the switch element 21 is switched ON, a current flows through the coil 23, and energy is stored in the coil 23.  Subsequently, when the switch element 21 is switched OFF, the energy stored in the coil 23 is released in the path passing\nthrough the capacitor 24 and the diode 22.  As a result, the DC current Id according to a proportion of the ON time of the switch element 21 is generated.\nThe polarity inverting circuit 30 inverts the polarity of the DC current Id input from the power control circuit 20 at a predetermined timing.  Thus, the polarity inverting circuit 30 generates and outputs the drive current I as a DC current\nlasting for the controlled time, or the drive current I as an AC current with an arbitrary frequency.  In the present embodiment, the polarity inverting circuit 30 is formed of an inverter bridge circuit (a full bridge circuit).\nThe polarity inverting circuit 30 includes a first switch element 31, a second switch element 32, a third switch element 33, and a fourth switch element 34 each formed of, for example, a transistor.  The polarity inverting circuit 30 has a\nconfiguration in which the first switch element 31 and the second switch element 32 connected in series to each other, and the third switch element 33 and the fourth switch element 34 connected in series to each other are connected in parallel to each\nother.  Polarity inverting control signals are input from the control section 40 to control terminals of the first switch element 31, the second switch element 32, the third switch element 33, and the fourth switch element 34, respectively.  Based on the\npolarity inverting control signals, ON/OFF operations of the first switch element 31, the second switch element 32, the third switch element 33, and the fourth switch element 34 are controlled, respectively.\nIn the polarity inverting circuit 30, there is repeated an operation of alternately switching ON/OFF a pair of the first switch element 31 and the fourth switch element 34 and a pair of the second switch element 32 and the third switch element\n33.  Thus, the polarity of the DC current Id output from the power control circuit 20 is alternately inverted.  The polarity inverting circuit 30 generates and then outputs the drive current I as a DC current keeping the same polarity state for a\ncontrolled time, or the drive current I as an AC current with a controlled frequency from a common connection point to the first switch element 31 and the second switch element 32, and a common connection point to the third switch element 33 and the\nfourth switch element 34.\nSpecifically, the polarity inverting circuit 30 is controlled so that the second switch element 32 and the third switch element 33 are in the OFF state while the first switch element 31 and the fourth switch element 34 are in the ON state, and\nthe second switch element 32 and the third switch element 33 are in the ON state while the first switch element 31 and the fourth switch element 34 are in the OFF state.  Therefore, while the first switch element 31 and the fourth switch element 34 are\nin the ON state, there is generated the drive current I flowing from one end of the capacitor 24 through the first switch element 31, the discharge lamp 90, and the fourth switch element 34 in this order.  While the second switch element 32 and the third\nswitch element 33 are in the ON state, there is generated the drive current I flowing from one end of the capacitor 24 through the third switch element 33, the discharge lamp 90, and the second switch element 32 in this order.\nIn the present embodiment, a part obtained by combining the power control circuit 20 and the polarity inverting circuit 30 with each other corresponds to a discharge lamp driver 230.  In other words, the discharge lamp driver 230 supplies the\ndischarge lamp 90 with the drive current I for driving the discharge lamp 90.\nThe control section 40 controls the discharge lamp driver 230.  In the example shown in FIG. 4, the control section 40 controls the power control circuit 20 and the polarity inverting circuit 30 to thereby control parameters such as the holding\ntime during which the drive current I lasts in the same polarity, and the current value (the amount of electrical power of the drive power Wd) and the frequency of the drive current I. The control section 40 performs the polarity inverting control for\ncontrolling the holding time during which the drive current I lasts in the same polarity, and the frequency and so on of the drive current I on the polarity inverting circuit 30 based on the polarity inverting timing of the drive current I. The control\nsection 40 performs, on the power control circuit 20, the current control for controlling the current value of the DC current Id output from the power control circuit 20.\nIn the present embodiment, the control section 40 is capable of performing, for example, the AC drive operation and the DC drive operation.  The AC drive operation is the drive operation in which the AC current is supplied to the discharge lamp\n90.  The DC drive operation is the drive operation in which the DC current is supplied to the discharge lamp 90.  A drive current waveform of the drive current I supplied to the discharge lamp 90 due to each of the types of the discharge lamp drive\noperation will be described later in detail.\nThe configuration of the control section 40 is not particularly limited.  In the present embodiment, the control section 40 is configured including a system controller 41, a power control circuit controller 42, and a polarity inverting circuit\ncontroller 43.  It should be noted that it is also possible to configure a part or the whole of the control section 40 with a semiconductor integrated circuit.\nThe system controller 41 controls the power control circuit controller 42 and the polarity inverting circuit controller 43 to thereby control the power control circuit 20 and the polarity inverting circuit 30.  It is also possible for the system\ncontroller 41 to control the power control circuit controller 42 and the polarity inverting circuit controller 43 based on a lamp voltage (inter-electrode voltage) Vla and the drive current I detected by the operation detection section 60.\nIn the present embodiment, a storage section 44 is connected to the system controller 41.\nIt is also possible for the system controller 41 to control the power control circuit 20 and the polarity inverting circuit 30 based on the information stored in the storage section 44.  The storage section 44 stores a plurality of drive\npatterns DW of the drive current I. More specifically, the storage section 44 stores, for example, information related to the drive parameters such as length of time during which the drive operation is performed, and the current value, the frequency, the\nnumber of periods, the polarity, the waveform, the modulation pattern and so on of the drive current I related to each of the drive operations constituting each of the drive patterns DW.  Each of the drive patterns DW of the drive current I includes at\nleast one of the AC drive operation and the DC drive operation described above.  The details of the drive patterns DW will be described later in detail.\nThe power control circuit controller 42 outputs the current control signal to the power control circuit 20 based on the control signal from the system controller 41 to thereby control the power control circuit 20.\nThe polarity inverting circuit controller 43 outputs the polarity inverting control signal to the polarity inverting circuit 30 based on the control signal from the system controller 41 to thereby control the polarity inverting circuit 30.\nThe control section 40 performs machine learning.  The control section 40 selects either one of the drive patterns DW stored in the storage section 44 based on the machine learning, and then executes the drive pattern DW thus selected.  The\ndetails of the machine learning will be described later in detail.\nThe control section 40 is realized using a dedicated circuit, and can be arranged to perform the control described above and a variety of types of control of processes described later.  In contrast, it is also possible to arrange the control\nsection 40 so that, for example, the CPU executes a control program stored in the storage section 44 to thereby function as a computer to perform a variety of types of control of these processes.\nFIG. 5 is a diagram for explaining another configuration example of the control section 40.  As shown in FIG. 5, the control section 40 can also be configured so as to function as a current controller 40-1 for controlling the power control\ncircuit 20, and a polarity inverting controller 40-2 for controlling the polarity inverting circuit 30 due to the control program.\nIn the example shown in FIG. 4, the control section 40 is configured as a part of the discharge lamp lighting device 10.  In contrast, it is also possible to adopt a configuration in which the CPU 580 assumes a part of the function of the\ncontrol section 40.\nIn the present embodiment, the operation detection section 60 includes a voltage detection section for detecting the lamp voltage Vla of the discharge lamp 90 to output lamp voltage information to the control section 40.  Further, it is also\npossible for the operation detection section 60 to include a current detection section for detecting the drive current I to output drive current information to the control section 40, and so on.  In the present embodiment, the operation detection section\n60 is configured including a first resistor 61, a second resistor 62, and a third resistor 63.\nIn the present embodiment, the voltage detection section of the operation detection section 60 detects the lamp voltage Vla using the voltage obtained by voltage dividing with the first resistor 61 and the second resistor 62 connected in series\nto each other and connected in parallel to the discharge lamp 90.  Further, in the present embodiment, the current detection section detects the drive current I using the voltage generated in the third resistor 63 connected in series to the discharge\nlamp 90.\nThe igniter circuit 70 operates only when starting to light the discharge lamp 90.  The igniter circuit 70 supplies a high voltage (a voltage higher than the voltage applied in the normal lighting of the discharge lamp 90) which is necessary for\ncausing the dielectric breakdown between the electrodes (between the first electrode 92 and second electrode 93) of the discharge lamp 90 to form the discharge path when starting to light the discharge lamp 90, between the electrodes (between the first\nelectrode 92 and second electrode 93) of the discharge lamp 90.  In the present embodiment, the igniter circuit 70 is connected in parallel to the discharge lamp 90.\nFIG. 6A and FIG. 6B show tip portions of the first electrode 92 and the second electrode 93.  At the tips of the first electrode 92 and the second electrode 93, there are respectively formed projections 552p, 562p.  FIG. 6A shows a first\npolarity state in which the first electrode 92 acts as an anode, and the second electrode 93 acts as a cathode.  In the first polarity state, electrons migrate from the second electrode 93 (the cathode) to the first electrode 92 (the anode) due to the\ndischarge.  The electrons are emitted from the cathode (the second electrode 93).  The electrons emitted from the cathode (the second electrode 93) collide with the tip of the anode (the first electrode 92).  The collision causes heat, and the\ntemperature of the tip (the projection 552p) of the anode (the first electrode 92) rises.\nFIG. 6B shows a second polarity state in which the first electrode 92 acts as the cathode, and the second electrode 93 acts as the anode.  In the second polarity state, in contrast to the first polarity state, electrons migrate from the first\nelectrode 92 to the second electrode 93.  As a result, the temperature of the tip (the projection 562p) of the second electrode 93 rises.\nAs described above, by the drive current I being supplied to the discharge lamp 90, the temperature of the anode with which the electrons collide rises.  In contrast, the temperature of the cathode emitting the electrons drops while emitting the\nelectrons toward the anode.\nAn inter-electrode distance between the first electrode 92 and the second electrode 93 increases with the deterioration of the projections 552p, 562p.  This is because the projections 552p, 562p wear.  When the inter-electrode distance\nincreases, the resistance between the first electrode 92 and the second electrode 93 increases, and therefore, the lamp voltage Vla rises.  Therefore, by referring to the lamp voltage Vla, it is possible to detect the change in the inter-electrode\ndistance, namely a degree of deterioration of the discharge lamp 90.\nIt should be noted that since the first electrode 92 and the second electrode 93 have substantially the same configurations, only the first electrode 92 will be described as a representative in some cases in the following description.  Further,\nsince the projection 552p of the tip of the first electrode 92 and the projection 562p of the tip of the second electrode 93 have substantially the same configurations, only the projection 552p will be described as a representative in some cases in the\nfollowing description.\nThe control of the discharge lamp driver 230 by the control section 40 according to the present embodiment will hereinafter be described.  In the present embodiment, the control section 40 controls the discharge lamp driver 230 using at least\none of the AC drive operation and the DC drive operation.\nIn the present embodiment, the control section 40 is capable of executing a plurality of drive patterns DW including one or more drive operations described later combined with each other.  In the present embodiment, each of the drive patterns DW\nhas drive current waveforms different from each other in at least one of the drive parameters in each of the drive operations constituting the drive pattern DW.\nEach of the drive operations will hereinafter be described.  FIG. 7 is a diagram showing an example of the drive current waveform supplied to the discharge lamp 90 in the AC drive operation.  FIG. 8A and FIG. 8B are each a diagram showing an\nexample of the drive current waveform supplied to the discharge lamp 90 in the DC drive operation.  In FIG. 7, FIG. 8A and FIG. 8B, the vertical axis represents the drive current I, and the horizontal axis represents time T. The drive current I is shown\ndefining the case of the first polarity state as positive, and the case of the second polarity state as negative.\nThe drive current I supplied to the discharge lamp 90 in the AC drive operation shown in FIG. 7 is, for example, a rectangular wave AC current having the polarity inverted a plurality of times between the current value Im and the current value\n-Im.  In the AC current shown in FIG. 7, the length of the period C1 is constant.  The duty ratio of the AC current shown in FIG. 7 is 0.5 (50%).\nThe drive current I supplied to the discharge lamp 90 in the DC drive operation shown in FIG. 8A is a DC current with the first polarity having a constant current value Im.  The drive current I supplied to the discharge lamp 90 in the DC drive\noperation shown in FIG. 8B is a DC current with the second polarity having a constant current value -Im.\nFIG. 9 is a diagram showing an example of the drive pattern DW of the drive current I supplied to the discharge lamp 90 in the present embodiment.  In FIG. 9, the vertical axis represents the drive current I, and the horizontal axis represents\ntime T.\nThe drive pattern DW shown in FIG. 9 is constituted by the AC drive operation and the DC drive operation.  More specifically, the drive pattern DW shown in FIG. 9 is constituted by a first AC drive operation AC1, a first DC drive operation DC1,\na second AC drive operation AC2, and a second DC drive operation DC2.  Further, the drive pattern DW has a plurality of drive parameters with respect to each of the AC drive operations and each of the DC drive operations.  For example, the first AC drive\noperation AC1 has a length ta1 of the execution time of the AC drive operation and a first frequency f1 of the AC drive operation as the drive parameters.  The first DC drive operation DC1 has a length td1 of the execution time of the DC drive operation\nand the first polarity as the drive parameters.  The second AC drive operation AC2 has a length ta2 of the execution time of the AC drive operation and a second frequency f2 of the AC drive operation as the drive parameters.  The second DC drive\noperation DC2 has a length td2 of the execution time of the DC drive operation and the second polarity as the drive parameters.\nIt should be noted that in the case of the drive pattern DW shown in FIG. 9, it is assumed that the length ta1 of the execution time of the first AC drive operation AC1 and the length ta2 of the execution time of the second AC drive operation\nAC2 are the same, and further, it is also assumed that the length td1 of the execution time of the first DC drive operation DC1 and the length td2 of the execution time of the second DC drive operation DC2 are the same.  Further, in the case of the drive\npattern DW shown in FIG. 9, it is assumed that the first frequency f1 of the AC current in the first AC drive operation AC1 and the second frequency f2 of the AC current in the second AC drive operation AC2 are the same.\nThe first frequency f1 and the second frequency f2 are, for example, no lower than 100 Hz and no higher than 1 kHz.  The length ta1 of the execution time of the first AC drive operation AC1 and the length ta2 of the execution time of the second\nAC drive operation AC2 are, for example, no less than 10 ms (milliseconds), and no more than 10 s (seconds).  The length td1 of the execution time of the first DC drive operation DC1 and the length td2 of the execution time of the second DC drive\noperation DC2 are, for example, no less than 10 ms (milliseconds), and no more than 40 ms (milliseconds).\nThe plurality of drive patterns DW is configured by, for example, arbitrarily combining a plurality of numerical values selected from the numerical value ranges of the respective drive parameters in each of the drive operations.  For example,\nthe number of types of the drive parameters in each of the drive operations used in the combination is preferably no less than 2 and no more than 6, and the number of numerical values prepared for each of the types of the drive parameters is preferably\nno less than 2 and no more than 6.  By combining these to configure the plurality of drive patterns DW, it is possible to obtain a preferable number of drive patterns DW.\nFor example, the drive parameters described in the drive pattern DW shown in FIG. 9 are the length of the execution time of the AC drive operation, the frequency of the AC current in the AC drive operation, the length of the execution time of\nthe DC drive operation, and the polarity of the DC drive operation, and in this case, the total number of the types of the drive parameters in each of the drive operations is 4.\nThe drive patterns DW are different from each other in a value of at least one of the drive parameters described above.  The number of the drive patterns DW is, for example, no less than 3 and no more than 150.  The number of the drive patterns\nDW is preferably no less than 10 and no more than 100.  The number of the drive patterns DW is more preferably no less than 20 and no more than 30.  By setting the number of the drive patterns DW in such a manner, the life of the discharge lamp 90 can\nfurther be extended.\nThen, switching between the drive patterns DW by the control section 40 of the present embodiment will be described.  The control section 40 switches between the drive patterns DW based on the machine learning.  In the present embodiment, the\ncontrol section 40 makes an evaluation of the drive pattern DW based on the change of the lamp voltage Vla, and then makes a selection of the drive pattern DW based on the evaluation of the drive pattern DW.\nIn the present embodiment, there are provided an initial learning period in which initial evaluations of the drive patterns DW are made, and a steady learning period set after the initial learning period.  FIG. 10 is a flowchart showing an\nexample of a control procedure of the control section 40 in the initial learning period.  It should be noted that in the following description, it is assumed that N drive patterns DW are provided, and the numbers from first through Nth are assigned to\nthe respective drive patterns DW.\nAs shown in FIG. 10, the control section 40 starts (step S11) the initial learning period, and then selects (step S12) the drive pattern DW which has not been previously selected in the initial learning period out of the first through Nth drive\npatterns DW.  The control section 40 selects, for example, the drive pattern DW not having been previously selected at random.  Since none of the drive patterns DW has been selected immediately after starting the initial learning period, the control\nsection 40 selects one drive pattern DW from the first through Nth drive patterns DW.  Then, the voltage detection section of the operation detection section 60 detects (step S13) the lamp voltage Vla1 of the discharge lamp 90, and the control section 40\nstores the lamp voltage Vla thus detected to the storage section 44.  Then, the control section 40 executes (step S14) the drive pattern DW thus selected.\nAfter starting the execution of the drive pattern DW, the control section 40 determines (step S15) whether or not the initial learning time has elapsed after the execution of the drive pattern DW presently selected has been started.  The length\nof the initial learning time is, for example, no less than 10 min (minutes) and no more than 120 min (minutes).  In the case in which the initial learning time has not elapsed from when the execution of the drive pattern DW presently selected has been\nstarted (NO in the step S15), the control section 40 continues to execute the drive pattern DW presently selected.\nIn contrast, in the case in which the initial learning time has elapsed from when the execution of the drive pattern DW presently selected has been started (YES in the step S15), the voltage detection section of the operation detection section\n60 detects (step S16) the lamp voltage Vla2 of the discharge lamp 90, and then the control section 40 stores the lamp voltage Vla2 thus detected in the storage section 44.  Then, the control section 40 makes (step S17) an evaluation of the drive pattern\nDW presently selected.\nIn the present embodiment, the evaluation of the drive pattern DW is made based on the change in the lamp voltage Vla.  Specifically, the control section 40 makes an evaluation of the drive pattern DW based on a value of the lamp voltage Vla2\nobtained after the drive pattern DW thus selected is executed for the initial learning time, and a difference of the lamp voltage Vla2 obtained after the drive pattern DW thus selected is executed for the initial learning time from the lamp voltage Vla1\nobtained before executing the drive pattern DW thus selected.  In the following description, the difference of the lamp voltage Vla2 obtained after executing the drive pattern DW for the initial learning time from the lamp voltage Vla1 obtained before\nexecuting the drive pattern DW is called a first variation voltage value.\nHere, a target numerical value range is set for the lamp voltage Vla.  The control section 40 selects and executes the drive patterns DW so that the lamp voltage Vla can be kept in the target numerical value range if at all possible.  The target\nnumerical value range is, for example, no lower than 60 V, and no higher than 65 V. The cases in which the evaluation of the drive pattern DW becomes relatively high are, for example, the case in which the lamp voltage Vla (the lamp voltage Vla2 obtained\nafter one drive pattern DW is executed for the initial learning time) falls within the target numerical value range due to the execution of the one drive pattern DW, the case in which the lamp voltage Vla comes closer to the target numerical value range\ndue to the execution of one drive pattern DW, and the case in which the lamp voltage Vla can be kept within the target numerical value range before and after executing one drive pattern DW.  Further, the cases in which the evaluation of the drive pattern\nDW is relatively low are, for example, the case in which the lamp voltage Vla runs off (move out of) the target numerical value range due to the execution of one drive pattern DW, and the case in which the lamp voltage Vla gets away from the target\nnumerical value range due to the execution of one drive pattern DW.\nAs an example, in the case in which the lamp voltage Vla2 obtained after executing one drive pattern DW for the initial learning time is higher than the target numerical value range, and at the same time, the first variation voltage value is a\nnegative value, the evaluation of the one drive pattern DW thus selected is relatively high.  Further, in the case in which the lamp voltage Vla2 obtained after executing one drive pattern DW for the initial learning time is higher than the target\nnumerical value range, and at the same time, the first variation voltage value is a positive value, the evaluation of the one drive pattern DW thus selected is relatively low.  In contrast, in the case in which the lamp voltage Vla2 obtained after\nexecuting one drive pattern DW for the initial learning time is lower than the target numerical value range, and at the same time, the first variation voltage value is a negative value, the evaluation of the one drive pattern DW thus selected is\nrelatively low.  Further, in the case in which the lamp voltage Vla2 obtained after executing one drive pattern DW for the initial learning time is lower than the target numerical value range, and at the same time, the first variation voltage value is a\npositive value, the evaluation of the one drive pattern DW thus selected is relatively high.  Further, in the case in which the lamp voltage Vla2 obtained after executing one drive pattern DW for the initial learning time is within the target numerical\nvalue range, the smaller the absolute value of the first variation voltage value is, the relatively higher the evaluation of the one drive pattern DW thus selected is, and in contrast, the larger the absolute value of the first variation voltage value\nis, the relatively lower the evaluation of the one drive pattern DW thus selected is.\nIt should be noted that the fact that the first variation voltage value is a negative value means the fact that the lamp voltage Vla has dropped due to one drive pattern DW executed for the initial learning time.  The fact that the first\nvariation voltage value is a positive value means the fact that the lamp voltage Vla has risen due to one drive pattern DW executed for the initial learning time.\nAfter evaluating the drive pattern DW thus selected, the control section 40 determines (step S18) whether or not all of the first through Nth drive patterns DW have been executed in the initial learning period.  In the case in which there is a\ndrive pattern DW which has not been executed in the initial learning period in the first through Nth drive patterns DW (NO in the step S18), the control section 40 selects and then executes another drive pattern DW, and then evaluates the drive pattern\nDW thus selected (steps S12 through S17).  In contrast, in the case in which all of the N patterns, namely the first through Nth drive patterns DW have been executed in the initial learning period (YES in the step S18), the control section 40 terminates\nthe initial learning period to make (step S19) the transition to the steady learning period.  The length of the initial learning period is, for example, shorter than 10 h (hours).\nIn the present embodiment, it is assumed that the lamp voltage Vla of the discharge lamp 90 is detected by the voltage detection section of the operation detection section 60 as the lamp voltage Vla1 obtained before executing the drive pattern\nDW thus selected after selecting the drive pattern DW not having been selected from the plurality of drive patterns DW in the step S12, but this is not a limitation.  The lamp voltage Vla1 obtained before executing the Xth drive pattern DW thus selected\ncan be set to, for example, the lamp voltage Vla2 detected after the (X-1)th drive pattern DW selected immediately before the Xth drive pattern DW thus selected is executed for the initial learning time.  By adopting such control, the detection of the\nlamp voltage Vla1 in the step S13 becomes unnecessary, and thus, the process of the initial evaluation can further be simplified.\nFIG. 11 is a flowchart showing an example of a control procedure of the control section 40 in the steady learning period.  FIG. 11 shows one cycle in the steady learning period.  In the steady learning period, the control section 40 repeatedly\nexecutes one cycle shown in FIG. 11.  As shown in FIG. 11, the control section 40 starts (step S21) the steady learning period, and then selects either one of the drive pattern DW not having been selected in the steady learning period and the drive\npattern DW having a relatively high rating out of the first through Nth drive patterns DW (steps S22 through S24).  It should be noted that the control section 40 randomly selects the drive pattern DW from the first through Nth drive patterns DW, for\nexample.\nMore specifically, for example, the control section 40 determines (step S22) whether or not former one (i.e., the drive pattern DW not having been selected in the steady learning period) of the drive pattern DW not having been selected in the\nsteady learning period and the drive pattern DW having a relatively high rating is selected from the first through Nth drive patterns DW, and in the case in which the drive pattern DW having a relatively high rating is selected (NO in the step S22), the\ncontrol section 40 selects (step S23) the drive pattern DW having a relatively high rating from the first through Nth drive patterns DW.  For example, the control section 40 selects the drive pattern DW having the highest rating, namely the drive pattern\nDW which makes the lamp voltage Vla the closest to the target numerical value range (the predetermined voltage value) of the lamp voltage Vla, from the first through Nth drive patterns DW.  Then, the control section 40 executes (step S26) the drive\npattern DW thus selected in the step S23.\nIn contrast, in the case of selecting the former one, namely the drive pattern DW not having been selected in the steady learning period (YES in the step S22), the control section 40 selects (step S24) the drive pattern DW not having been\nselected from the first through Nth drive patterns DW.  Then, in the case in which the drive pattern DW not having been selected in the steady learning period is selected, the control section 40 determines (step S25) whether or not the drive pattern DW\nthus selected fulfills the execution condition.  The execution condition includes, for example, the fact that the drive pattern DW thus selected is not switched to another drive pattern DW in the step S28 described later last time the drive pattern DW\nthus selected is selected and then executed.\nIn the case in which the drive pattern DW selected in the step S24 fulfills the execution condition (YES in the step S25), the process makes the transition to the step S26, and the control section 40 executes the drive pattern DW thus selected. \nIn contrast, in the case in which the drive pattern DW thus selected fails to fulfill the execution condition (NO in the step S25), the process makes the transition to the step S22, and the control section 40 selects another drive pattern DW from the\nfirst through Nth drive patterns DW, and then performs substantially the same determination as described above.\nThen, after starting the execution of the drive pattern DW thus selected, the control section 40 determines (step S27) whether or not the steady learning time has elapsed after the execution of the drive pattern DW presently selected has been\nstarted.  The steady learning time determined in the step S27 is the same as, for example, the initial learning time determined in the step S15 in the initial learning period.  Therefore, the length of the steady learning time is, for example, no less\nthan 10 min (minutes) and no more than 120 min (minutes).  In the case in which the steady learning time has not elapsed from when the execution of the drive pattern DW presently selected has been started (NO in the step S27), the control section 40\ndetermines (step S28) whether or not the present drive pattern DW fulfills a switching condition (a first predetermined condition).\nThe switching condition includes, for example, the fact that either one of a first switching condition and a second switching condition is fulfilled.  The first switching condition is that the absolute value of the variation (the variation\nvoltage value) of the lamp voltage Vla detected within the steady learning time becomes equal to or larger than a first predetermined value, and at the same time the lamp voltage Vla thus detected runs off the target numerical value range during the\nexecution of the present drive pattern DW.  The second switching condition includes the fact that the absolute value of the variation of the lamp voltage Vla becomes equal to or larger than a second predetermined value in the case in which the time\nhaving elapsed from when the execution of the present drive pattern DW has started is equal to or shorter than a first time.  The first time is shorter than the steady learning time, and is, for example, 5 min (minutes).  The second predetermined value\nis smaller than the first predetermined value.  The first predetermined value is, for example, 5 V. The second predetermined value is, for example, 3 V.\nSpecifically, it is assumed that in the case in which the elapsed time is equal to or shorter than the first time, the switching condition (the second switching condition) is fulfilled even in the case in which the absolute value of the\nvariation of the lamp voltage Vla has become equal to or larger than the second predetermined value smaller than the first predetermined value, and in the case in which the elapsed time exceeds the first time, the switching condition (the first switching\ncondition) is not fulfilled unless the variation of the lamp voltage Vla becomes equal to or larger than the first predetermined value larger than the second predetermined value.  By adopting such a relationship, the control section 40 determines the\nswitching of the drive pattern DW presently selected in a phased manner based on the execution time of the drive pattern DW presently selected and the lamp voltage Vla.\nIn the case in which the drive pattern DW presently selected fulfills the switching condition (YES in the step S28), the control section 40 determines that the drive pattern DW presently selected is an undesirable drive pattern DW for extending\nthe life of the discharge lamp 90 in the present state of the discharge lamp 90.  Then, the control section 40 degrades the rating of the drive pattern DW presently selected.\nSubsequently, the control section 40 performs the step S22 through the step S26 in substantially the same manner as described above to perform the selection and the execution of the next drive pattern DW.  As described above, in the case in\nwhich the variation of the lamp voltage Vla fulfills the switching condition when executing the drive pattern DW, the control section 40 switches from the drive pattern DW presently selected to another drive pattern DW.\nIn contrast, in the case in which the present drive pattern DW does not fulfill the switching condition (NO in the step S28), the control section 40 executes the drive pattern DW presently selected until the steady learning time elapses. \nFurther, in the case in which the steady learning time has elapsed from when the execution of the present drive pattern DW has started (YES in the step S27), the voltage detection section of the operation detection section 60 detects (step S29) the lamp\nvoltage Vla of the discharge lamp 90, and then the control section 40 stores the lamp voltage Vla thus detected in the storage section 44.  Subsequently, the control section 40 makes (step S30) an evaluation of the drive pattern DW presently selected.\nThe evaluation of the drive pattern DW in the step S30 is substantially the same as, for example, the evaluation of the drive pattern DW in the step S17 in the initial learning period.  Specifically, the control section 40 makes an evaluation of\nthe drive pattern DW based on a value of the lamp voltage Vla obtained after the drive pattern DW thus selected is executed for the steady learning time, and a difference of the lamp voltage Vla obtained after the drive pattern DW is executed for the\nsteady learning time from the lamp voltage Vla obtained before executing the drive pattern DW thus selected.  In the following description, the difference of the lamp voltage Vla obtained after executing the drive pattern DW for the steady learning time\nfrom the lamp voltage Vla obtained before executing the drive pattern DW is called a second variation voltage value.\nIn the step S30, the control section 40 makes a re-evaluation of the drive pattern DW thus selected in the steady learning period.  Specifically, the control section 40 updates evaluations of each of the drive patterns DW evaluated both in the\ninitial learning period and in the steady learning period before the present moment.\nSubsequently, the control section 40 determines (step S31) whether or not the drive pattern DW presently selected fulfills a continuous execution condition (a second predetermined condition).  The continuous execution condition includes the fact\nthat either one of a first continuous execution condition, a second continuous execution condition, and a third continuous execution condition is fulfilled.  Each of the first continuous execution condition, the second continuous execution condition, and\nthe third continuous execution condition includes the fact that the number of times of the continuous execution is equal to or smaller than a predetermined number of times.  The predetermined number of times related to the number of times of the\ncontinuous execution is, for example, no smaller than twice and no larger than 15 times.\nFurther, the first continuous execution condition is that the lamp voltage Vla obtained after executing the drive pattern DW thus selected for the steady learning time is higher than the target numerical value range, and at the same time, the\nsecond variation voltage value is a negative value.  The second continuous execution condition is that the lamp voltage Vla is included in the target numerical value range before and after the execution of the drive pattern DW thus selected.  The third\ncontinuous execution condition is that the lamp voltage Vla obtained after executing the drive pattern DW thus selected for the steady learning time is lower than the target numerical value range, and at the same time, the second variation voltage value\nis a positive value.\nIn the case in which the present drive pattern DW fulfills the continuous execution condition (YES in the step S31), the control section 40 determines that the drive pattern DW presently selected is a preferable drive pattern DW for extending\nthe life of the discharge lamp 90 in the present state of the discharge lamp 90.  Then, the control section 40 selects (step S32) once again the drive pattern DW presently selected as the drive pattern DW to be subsequently executed.  Then, the process\nmakes the transition to the step S26, and the control section 40 continuously executes the previous drive pattern DW selected as the drive pattern DW to be executed this time.\nAs described hereinabove, in the present embodiment, in the case in which the variation of the lamp voltage Vla before and after executing one drive pattern DW thus selected for the steady learning time fulfills the continuous execution\ncondition, the control section 40 executes the same drive pattern DW a plurality of times continuously.\nIn contrast, in the case in which the present drive pattern DW fails to fulfill the continuous execution condition (NO in the step S31), the control section 40 determines (step S33) whether or not the selection and the execution of the drive\npattern DW are performed a predetermined number of times in the step S22 through the step S26.\nIn the case in which the selection and the execution of the drive pattern DW are not performed the predetermined number of times (NO in the step S33), the process makes the transition to the step S22 to perform the selection of the drive pattern\nDW once again.  The predetermined number of times related to the selection and the execution of the drive pattern DW in every cycle of the steady learning period is, for example, larger than the number N of the drive patterns DW.\nIn the case in which the selection and the execution of the drive pattern DW have been performed the predetermined number of times (YES in the step S33), the control section 40 terminates (step S34) the cycle of the steady learning period.  The\ncontrol section 40 repeats the cycle described above to continue to execute the steady learning period.  The subsequent cycle is executed in the state of taking over the parameters from the previous cycle of the steady learning period except the fact\nthat the number of times of the selection and the execution of the drive pattern DW is reset.\nIn such a manner as described above, the control section 40 performs the machine learning with the initial learning period and the steady learning period to select the drive pattern DW to be executed.  The initial learning period is executed\njust one time after, for example, the projector 500 is lit for the first time.  The steady learning period is always provided during the period in which the projector 500 is in the lighting state after, for example, the initial learning period is\nterminated.  For example, in the case in which the projector 500 is powered OFF, and is then powered ON once again, the control section 40 resumes the period which has been executed last time the power has been switched OFF from the point where the\nperiod has been interrupted.\nFor example, in the related art, since the drive pattern DW corresponding to the value of the lamp voltage Vla and the value of the drive power Wd is set in advance, in the case in which the lamp voltage Vla and the drive power Wd have the same\nvalues, the drive pattern DW to be executed is the unique drive pattern DW set in advance.\nIn contrast, in the present embodiment, since the drive pattern DW is selected due to the machine learning, in the case in which, for example, the lamp voltage Vla is at a predetermined voltage value, and the drive power Wd is at a predetermined\npower value, a plurality of drive patterns DW different in at least one of the drive parameters from each other is executed.  Specifically, in the case in which, for example, the drive power Wd supplied to the discharge lamp 90 is in a predetermined\npower band, and the lamp voltage Vla of the discharge lamp 90 has a predetermined voltage value, the control section 40 executes at least two (in the present embodiment, e.g., at least three) drive patterns DW.  The at least two drive patterns DW are the\ndrive patterns DW different from each other in the value of at least one of the drive parameters in the drive operation constituting one drive pattern DW.  In other words, in the case of detecting the drive patterns DW of the drive current I supplied to\nthe discharge lamp 90 setting the lamp voltage Vla constant and setting the power band of the drive power Wd constant, it is possible to detect at least two or more drive patterns DW different from each other.  It should be noted that the predetermined\npower band means, for example, a numerical value range of the drive power Wd having a width no larger than about 10 W.\nIn the present embodiment, in the case in which the drive power Wd to be supplied to the discharge lamp 90 is in a predetermined power band, and the lamp voltage Vla of the discharge lamp 90 has a predetermined voltage value, at least three\ndrive patterns DW are executed.  It is assumed that, for example, the three drive patterns DW correspond to a first drive pattern, a second drive pattern, and a third drive pattern, and are each constituted by the AC drive operation and the DC drive\noperation as shown in FIG. 9.  In the case of assuming the drive parameters in this case as three types, namely the frequency of the AC current, the length of the execution time of the AC drive operation, and the length of the execution time of the DC\ndrive operation, for example, regarding the first drive pattern, the frequency of the AC current is 400 Hz, the length of the execution time of the AC drive operation is 100 ms, and the length of the execution time of the DC drive operation is 10 ms. \nRegarding the second drive pattern, the frequency of the AC current is 400 Hz, the length of the execution time of the AC drive operation is 50 ms, and the length of the execution time of the DC drive operation is 10 ms.  Regarding the third drive\npattern, the frequency of the AC current is 300 Hz, the length of the execution time of the AC drive operation is 100 ms, and the length of the execution time of the DC drive operation is 10 ms.  The first drive pattern, the second drive pattern, and the\nthird drive pattern are the drive patterns DW in which any two of these three drive patterns DW are different in the value of at least one of the drive parameters from each other.\nFurther, if the selection and the execution of the drive pattern DW are executed using the machine learning as described above, in the case in which the lamp voltage Vla of the discharge lamp 90 has a predetermined voltage value, the drive\npattern DW to be selected and executed is different due to the difference in accumulated lighting time of the discharge lamp 90 or the individual of the discharge lamp 90.  Specifically, in the case in which the lamp voltage Vla of the discharge lamp 90\nis at the predetermined voltage value, the control section 40 executes at least two drive patterns DW, for example, the first drive pattern and the second drive pattern, in accordance with at least one of the accumulated lighting time of the discharge\nlamp 90 and the individual of the discharge lamp 90.  In this case, the power band of the drive power Wd in the two drive patterns DW can be a predetermined power value, or can also be different.\nThe first drive pattern and the second drive pattern are any two of the drive patterns DW.  The first drive pattern and the second drive pattern are different from each other in the value of at least one of the drive parameters in the drive\noperations constituting each of the drive patterns.  It should be noted that the accumulated lighting time is the total sum of the time for which the discharge lamp 90 has been lit.  In other words, the accumulated lighting time is the lighting time of\nthe discharge lamp 90 accumulated from when the discharge lamp 90 has been lit for the first time.\nSpecifically, for example, the control section 40 executes the first drive pattern in the case in which the lamp voltage Vla has a predetermined voltage value and the accumulated lighting time is a first accumulated lighting time, or executes\nthe second drive pattern in the case in which the lamp voltage Vla has the predetermined voltage value and the accumulated lighting time is a second accumulated lighting time different from the first accumulated lighting time.  In other words, in the\ncase of detecting the drive patterns DW of the drive current I supplied to the discharge lamp 90 setting the lamp voltage Vla constant, if the accumulated lighting time of the discharge lamp 90 varies, it is possible to detect at least two or more drive\npatterns DW different from each other.\nFurther, for example, the control section 40 executes the first drive pattern in the case in which the lamp voltage Vla has the predetermined voltage value and the discharge lamp 90 is a first individual (a first discharge lamp), or executes the\nsecond drive pattern in the case in which the lamp voltage Vla has the predetermined voltage value and the discharge lamp 90 is a second individual (a second discharge lamp) different from the first individual.  In other words, in the case of detecting\nthe drive patterns DW of the drive current I supplied to the discharge lamp 90 setting the lamp voltage Vla constant, if the individual of the discharge lamp 90 changes, it is possible to detect at least two or more drive patterns DW different from each\nother.\nThe discharge lamp lighting device 10 provided with the control section 40 for performing the control described above can also be expressed as a discharge lamp drive method.  Specifically, one aspect of the discharge lamp drive method according\nto the present embodiment is a discharge lamp drive method of supplying the drive current I to the discharge lamp 90 having the first electrode 92 and the second electrode 93 to thereby drive the discharge lamp 90, and is characterized in that the drive\ncurrent I is supplied to the discharge lamp 90 in accordance with a plurality of drive patterns DW of the drive current I, the plurality of drive patterns DW each has a plurality of drive parameters, and in the case in which the lamp voltage Vla of the\ndischarge lamp 90 has the predetermined voltage value, the first drive pattern and the second drive pattern different from each other in the value of at least one of the drive parameters are executed in accordance with at least one of the accumulated\nlighting time of the discharge lamp 90 and the individual of the discharge lamp 90.\nIt should be noted that the fact that the drive patterns are different in the value of at least one of the drive parameters from each other includes the case in which, for example, the drive operation constituting the drive pattern DW is\ndifferent between the two drive patterns DW.  More specifically, also in the case in which, for example, the first drive pattern is constituted by the AC drive operation, the second drive pattern is constituted by the DC drive operation and the AC drive\noperation, and the drive parameters related to the AC drive operation in the first drive pattern and the drive parameters related to the AC drive operation in the second drive pattern are the same, the first drive pattern and the second drive pattern are\nregarded as the drive patterns DW different in at least one of the drive parameters from each other.  In this case, since the DC drive operation is not included in the first drive pattern, the drive parameters related to the DC drive operation in the\nfirst drive pattern are regarded as zero.  Thus, in this case, the first drive pattern and the second drive pattern are different from each other in the drive parameters related to the DC drive operation.\nAccording to the present embodiment, in such a manner as described above, in the case in which the lamp voltage Vla of the discharge lamp 90 is at the predetermined voltage value, the control section 40 executes the first drive pattern and the\nsecond drive pattern different in the value of at least one of the drive parameters from each other in accordance with at least one of the accumulated lighting time of the discharge lamp 90 and the individual of the discharge lamp 90.  Therefore, in the\ncase in which the control section 40 executes the drive pattern DW in accordance with the accumulated lighting time, it is possible to make the drive pattern DW to be executed differ by the drive time of the discharge lamp 90 even in the same lamp\nvoltage Vla.  Thus, it is easy to execute the preferable drive pattern DW even in the case in which the individual difference exists between the discharge lamps 90.  Meanwhile, in the case in which the control section 40 executes the drive pattern DW in\naccordance with the individual of the discharge lamp 90, it is possible to make the drive pattern DW to be executed different between the individuals of the discharge lamp 90 even in the same lamp voltage Vla.  Thus, it is easy to execute the preferable\ndrive pattern DW even in the case in which the individual difference exists between the discharge lamps 90.  Therefore, according to the present embodiment, it is possible to extend the life of the discharge lamp 90 irrespective of the individual\ndifference between the discharge lamps 90.\nFurther, according to the present embodiment, in such a manner as described above, the control section 40 selects any one of the drive patterns DW based on the machine learning, and then executes the drive pattern DW thus selected.  Therefore,\nby performing the machine learning, even in the case in which the individual difference exists between the discharge lamps 90, it is possible to select the preferable drive pattern DW from the first drive pattern and the second drive pattern in\naccordance with the individual difference between the discharge lamps 90.  Therefore, according to the present embodiment, it is possible to further extend the life of the discharge lamp 90 irrespective of the individual difference between the discharge\nlamps 90.\nFurther, if the drive power Wd supplied to the discharge lamp 90 changes, the melting state and the growth state of the projection 552p of the first electrode 92 change.  Therefore, in the past, it was necessary to determine the preferable drive\npatterns DW of the drive current I to be supplied to the discharge lamp 90 in accordance with the lamp voltage Vla and store the preferable drive patterns in the storage section 44 for each of the values of the drive power Wd to be supplied to the\ndischarge lamp 90.  Therefore, it was difficult to set the drive patterns DW corresponding to the lamp voltage Vla for each of the values of the drive power Wd, and there is adopted a specification in which the drive power Wd supplied to the discharge\nlamp 90 can only be changed to predetermined several levels of the drive power Wd.\nIn contrast, according to the present embodiment, since the drive pattern DW is selected based on the machine learning, even in the case in which the drive power Wd is changed, the preferable drive pattern DW can be selected in accordance with\nthe change in the drive power Wd.  Thus, it becomes possible to easily change the drive power Wd to be supplied to the discharge lamp 90 in multiple levels.  Therefore, for example, it becomes possible for the user to arbitrarily change the drive power\nWd to change the luminance of the picture projected from the projector 500 at will.  Therefore, it becomes also possible to extend the life of the discharge lamp 90 while setting the drive power Wd relatively low to preferably reduce the power\nconsumption of the projector 500.\nFurther, since it becomes possible to arbitrarily change the drive power Wd, it becomes possible to use the drive power Wd as one of the drive parameters of the drive pattern DW to be changed when extending the life of the discharge lamp 90. \nThus, it is possible to further extend the life of the discharge lamp 90.  For example, it is also possible to provide an operation section for changing the drive power Wd on the housing of the projector 500.\nFurther, according to the present embodiment, the control section 40 performs the selection of the drive pattern DW based on the change in the lamp voltage Vla.  Therefore, by detecting the lamp voltage Vla, it is possible to perform the\nselection of the drive pattern DW, and it is possible to preferably and easily perform the machine learning.\nFurther, according to the present embodiment, in the case in which the variation of the lamp voltage Vla thus detected fulfills the switching condition when executing the drive pattern DW thus selected, the control section 40 switches from the\ndrive pattern DW thus selected to another drive pattern DW.  Therefore, in the case in which the drive pattern DW thus selected is an undesirable drive pattern DW for extending the life with respect to the state of the discharge lamp 90 at that moment,\nit is possible to switch the drive pattern DW to another desirable drive pattern DW.  Therefore, it is possible to prevent the life of the discharge lamp 90 from being shortened.\nFurther, as described above, the drive pattern DW having fulfilled the switching condition is degraded in rating, and is determined not to fulfill the execution condition when the drive pattern DW having fulfilled the switching condition is\nsubsequently selected in the step S24 and then whether or not the drive pattern DW having fulfilled the switching condition fulfills the execution condition is determined in the step S25.  In other words, the control section 40 does not execute the drive\npattern DW having fulfilled the switching condition for a predetermined period.  Therefore, according to the present embodiment, it is possible to prevent the drive pattern DW having a high possibility of shortening the life of the discharge lamp 90 from\nbeing executed, and thus, the life of the discharge lamp 90 can further be extended.\nAs described above, in the case in which the steady learning time has elapsed from when starting the execution of the drive pattern DW thus selected, the control section 40 selects the next drive pattern DW.  Therefore, the length of the\nexecution time (the predetermined time) of the drive pattern DW basically becomes equal to the length of the steady learning time.  However, since the drive pattern DW selected in the steady learning period is switched to another drive pattern DW in some\ncases halfway before the steady learning time elapses depending on the state of the discharge lamp 90, the execution time (the predetermined time) of one drive pattern DW thus selected, namely a period from when starting the execution of the one drive\npattern DW to when the one drive pattern DW is switched to the next drive pattern DW, changes in some cases even in the same drive pattern DW.\nIn other words, the control section 40 changes the length of the execution time (the predetermined time) in which the predetermined drive pattern DW out of the plurality of drive patterns DW is executed based on the lamp voltage Vla.  Therefore,\nit is possible to arbitrarily switch between the drive patterns DW in accordance with the change in the lamp voltage Vla, and thus, it is possible to further extend the life of the discharge lamp 90.\nFurther, the length of the execution time in which a predetermined one of the drive patterns DW is executed changes in such a manner as described above, and therefore changes in accordance with the accumulated lighting time of the discharge lamp\n90.  Further, the length of the execution time in which the predetermined drive pattern out of the plurality of drive patterns DW is executed changes in such a manner as described above, and therefore changes in accordance with the individual of the\ndischarge lamp 90.  In the case in which the drive pattern DW selected has been switched halfway to another drive pattern DW, the execution time (the predetermined time) of the drive pattern DW thus selected is shorter than the steady learning time.\nFurther, in the case of, for example, always executing the same drive pattern DW even if the drive pattern DW has the highest rating, namely the drive pattern DW makes the lamp voltage Vla of the discharge lamp 90 the closest to the target\nnumerical value range, it becomes difficult for the projection 552p of the first electrode 92 to grow in some cases, and it becomes difficult to sufficiently extend the life of the discharge lamp 90 in some cases.  Further, for example, the melting state\nof the projection 552p of the first electrode 92 of the discharge lamp 90 changes with the deterioration of the discharge lamp 90, namely an increase in the accumulated lighting time.  Therefore, even the drive pattern DW preferable for extending the\nlife of the discharge lamp 90 at a certain time point becomes the drive pattern DW undesirable for extending the life of the discharge lamp 90 at another time point in some cases.\nIn contrast, according to the present embodiment, the control section 40 selects and executes one of the drive pattern DW not having been selected from the N drive patterns DW and the drive pattern DW having a relatively high rating in one cycle\nof the steady learning period.  Therefore, in one cycle, both of the drive pattern DW having a relatively high rating including the drive pattern DW having the highest rating and the other drive pattern DW are executed.  Specifically, the control section\n40 of the present embodiment executes both of the drive pattern DW (hereinafter referred to as a high-rating drive pattern DWm) having a relatively high rating including the drive pattern DW having the highest rating of the plurality of drive patterns\nDW, and the drive pattern DW (hereinafter referred to as another drive pattern DWe) having a rating lower than the rating of the high-rating drive pattern DWm out of the plurality of the drive patterns DW in a certain period.  Thus, it is possible to\nexecute the other drive pattern DWe having a rating lower than the rating of the high-rating drive pattern DWm having a high rating between the high-rating drive pattern DWm, and it is easy to dramatically vary a stimulus of a thermal load applied to the\nfirst electrode 92.  Therefore, it is easy to grow the projection 552p, and thus, it is easy to further extend the life of the discharge lamp 90.\nFurther, according to the present embodiment, in the case in which the variation of the lamp voltage Vla before and after executing the drive pattern DW selected fulfills the continuous execution condition, the control section 40 executes that\ndrive pattern DW a plurality of times continuously.  Here, in the present embodiment, there is adopted the configuration in which a plurality of drive patterns DW different in at least one of the drive parameters from each other is executed in the case\nin which the lamp voltage Vla is at a predetermined voltage value, and the drive power Wd is at a predetermined power value.  In other words, the present embodiment has a random nature that one of two or more drive patterns DW is selected and executed in\none condition on the one hand, and also has a nature that the same drive pattern is to be continuously executed in the case in which the drive pattern DW selected fulfills the continuous execution condition on the other hand.  Therefore, it is possible\nto continuously execute the drive pattern DW preferable for extending the life of the discharge lamp 90 a plurality of times, and it is easy to further extend the life of the discharge lamp 90.  Further, in the present embodiment, the continuous\nexecution condition includes the fact that the number of continuous execution times is no larger than a predetermined number of times.  Therefore, it is possible to continuously execute the preferable drive pattern DW the plurality of times in such\nexecution time that the state of the discharge lamp 90 does not dramatically change.  Therefore, it is easy to further extend the life of the discharge lamp 90.\nFurther, according to the present embodiment, the control section 40 performs the determination on whether or not the drive pattern DW fulfills the execution condition as shown in the step S25, and does not execute the drive pattern DW in the\ncase in which the drive pattern DW does not fulfill the execution condition.  Thus, it is difficult for the drive pattern DW having a relatively low rating to be executed.  Therefore, it is easy to further extend the life of the discharge lamp 90.\nSince the drive pattern DW is selected and then executed based on such machine learning as described above, in the present embodiment, the probability that the drive pattern DW is executed is different between the drive patterns DW irrespective\nof the lamp voltage Vla.  Specifically, in the case in which the lamp voltage Vla of the discharge lamp 90 is at a predetermined voltage value, the probability that the first drive pattern is executed and the probability that the second drive pattern is\nexecuted are different from each other.\nAs described above, according to the present embodiment, the probability that the first drive pattern is executed and the probability that the second drive pattern is executed are different from each other.  Therefore, it is possible to make it\neasy to execute more preferable one of the drive patterns DW (e.g., the high-rating drive pattern DWm) in accordance with the condition of the discharge lamp 90.  Thus, it is possible to further extend the life of the discharge lamp 90.\nFurther, in the execution period in which the first drive pattern and the second drive pattern are each executed at least one or more times, the proportion of the execution time in which the first drive pattern is executed to the length of the\nexecution period and the proportion of the execution time in which the second drive pattern is executed to the length of the execution period are different from each other.  The execution period corresponds to, for example, one cycle of the steady\nlearning period.\nAs described above, according to the present embodiment, the proportion of the execution time in which the first drive pattern is executed and the proportion of the execution time in which the second drive pattern is executed are different from\neach other in the execution period.  Therefore, it is possible to elongate the time in which more preferable one of the drive patterns DW (e.g., the high-rating drive pattern DWm) is executed in accordance with the condition of the discharge lamp 90. \nThus, it is possible to further extend the life of the discharge lamp 90.\nFurther, in the execution period in which the first drive pattern and the second drive pattern are each executed at least one or more times, namely one cycle of the steady learning period, for example, the number of times of the execution of the\nfirst drive pattern and the number of times of the execution of the second drive pattern are different from each other.  In the case of defining the first drive pattern as the high-rating drive pattern DWm including the drive pattern DW having the\nhighest rating, and the second drive pattern as one drive pattern DW of other drive patterns DWe, the number of times of the execution of the first drive pattern is larger than the number of times of the execution of the second drive pattern.\nAs described above, according to the present embodiment, the number of times that the first drive pattern is executed and the number of times that the second drive pattern is executed are different from each other in the execution period. \nTherefore, it is possible to increase the number of times that more preferable one of the drive patterns DW (e.g., the high-rating drive pattern DWm) is executed in accordance with the condition of the discharge lamp 90.  Thus, it is possible to further\nextend the life of the discharge lamp 90.\nFurther, for example, if the number (N) of the drive patterns DW to be selected is too small, the preferable drive pattern DW corresponding to the state of the discharge lamp 90 fails to be included in some cases.  In contrast, if the number of\nthe drive patterns DW is too large, it takes time to execute the initial learning period, and it takes time until the preferable drive pattern DW is selected.  Further, in the steady learning period, the proportion of executing the drive pattern other\nthan the preferable drive pattern DW becomes large.  Further, even in the case in which, for example, the control section 40 does not perform the machine learning, if the number of the drive patterns DW is too large, the probability that the preferable\ndrive pattern DW is selected from the plurality of drive patterns DW becomes low in some cases.\nIn contrast, if the number of the drive patterns DW is made no less than 10 and no more than 100, it is possible to make it easy to select the preferable drive pattern DW corresponding to the state of the discharge lamp 90, and to shorten the\nlength of the initial learning period.  Further, since the proportion of the preferable drive pattern DW can be made large in the steady learning period, the life of the discharge lamp 90 can further be extended.  Further, even in the case in which the\ncontrol section 40 does not perform the machine learning, it is possible to make it easy to select the preferable drive pattern DW, and it is possible to further extend the life of the discharge lamp 90.  Further, if the number of the drive patterns DW\nis made no smaller than 20 and no larger than 30, there advantages can be obtained in an enhanced manner.\nIt should be noted that in the present embodiment, it is also possible to adopt the configurations and methods described below.\nAs long as the control section 40 executes the first drive pattern and the second drive pattern different in the value of at least one of the drive parameters from each other in accordance with at least one of the accumulated lighting time of\nthe discharge lamp 90 and the individual of the discharge lamp 90 in the case in which the lamp voltage Vla is at the predetermined voltage value, the selection method of the drive pattern DW and so on are not particularly limited.  For example, the\ncontrol section 40 is not required to perform machine learning.  In this case, for example, it is possible to select and then execute the drive pattern DW based only on a predetermined condition, or it is also possible to randomly select and then execute\nthe drive pattern DW.\nFurther, in the case in which the control section 40 selects and then executes the drive pattern DW based on the machine learning, the method of the machine learning is not particularly limited.  The evaluation method of the drive pattern DW\ndescribed above is not particularly limited.  The initial learning time and the steady learning time can also be different from each other.\nFurther, it is possible for the control section 40 to switch the drive pattern DW to the drive pattern DW executed last time in the case in which the lamp voltage Vla has dropped as much as a third predetermined value (a predetermined value) or\nmore.  More specifically, for example, in the case in which a selected one of the drive patterns DW is executed, the control section 40 determines whether or not the lamp voltage Vla drops as much as the third predetermined value or more based on the\nlamp voltage Vla having been detected within the steady learning time, and performs the switching to the drive pattern DW executed last time in the case in which the lamp voltage Vla has dropped as much as the third predetermined value or more. \nAccording to this configuration, for example, in the case in which the projection 552p moves, and the lamp voltage Vla drops rapidly, it is possible to perform switching to the drive pattern DW executed before the projection 552p moves.  Thus, it is easy\nto correct the position of the projection 552p to the position before moving.  Further, it is possible for the control section 40 to switch the drive pattern DW to the drive pattern DW different from the drive pattern DW executed last time in the case in\nwhich the lamp voltage Vla has dropped as much as the third predetermined value (the predetermined value) or more.\nFurther, it is also possible for the control section 40 to change the length of the steady learning time based on the lamp voltage Vla.  For example, if the discharge lamp 90 deteriorates, the time until the change in the lamp voltage Vla due to\nthe drive pattern DW occurs becomes long in some cases.  In such a case, if the execution time of the drive pattern DW is short, the drive pattern DW cannot appropriately be evaluated in some cases.  To cope with the above, by changing the length of the\nsteady learning time based on the lamp voltage Vla, it is possible to elongate the steady learning time to elongate the execution time (the predetermined time) of the drive pattern DW in the case in which the discharge lamp 90 has deteriorated. \nTherefore, it is easy to appropriately evaluate the drive pattern DW, and as a result, the life of the discharge lamp 90 can be extended.\nFurther, it is also possible for the control section 40 to change the number of the drive patterns DW, or change the types of the drive parameters in each of the drive operations of the drive patterns DW based on the lamp voltage Vla.  In these\ncases, it is also possible for the control section 40 to change the number of the types of the drive parameters which are different between the drive patterns DW based on the lamp voltage Vla.  For example, it is also possible for the control section 40\nto increase the number of the types of the drive parameters which are different between the drive patterns DW in the case in which the lamp voltage Vla is higher than a first voltage.  According to this configuration, in the case in which the discharge\nlamp 90 has deteriorated, it is easy to increase the stimulus due to the change in the thermal load applied to the first electrode 92, and it is possible to further extend the life of the discharge lamp 90.\nFurther, it is also possible for the control section 40 to select the drive pattern DW based on the change in the lamp voltage Vla occurring until just before the selection.  Further, it is also possible for the control section 40 to arrange\nthat each of all of the drive patterns DW is executed one or more times without fail in each cycle in the steady learning period.  Further, it is also possible for the control section 40 to create the drive pattern DW other than the plurality of drive\npatterns DW set in advance in the steady learning period.  In this case, it is also possible for the control section 40 to combine the drive parameters to create the new drive pattern DW based on the ratings of the respective drive patterns DW set in\nadvance.\nFurther, in the step S15 in the initial learning period, it is also possible for the control section 40 to determine whether or not the drive pattern DW presently selected fulfills the switching condition as in the step S28 in the steady\nlearning period.  For example, in the case in which the drive pattern DW presently selected fulfills the switching condition, it is also possible for the control section 40 to lower the rating of the drive pattern DW presently selected to perform the\nswitching from the drive pattern DW presently selected to another of the drive patterns DW.  In contrast, in the case in which the drive pattern DW presently selected does not fulfill the switching condition, it is also possible for the control section\n40 to execute the drive pattern DW presently selected until the initial learning time elapses.  It should be noted that the switching condition on this occasion can be the same as the switching condition in the step S28, or can also be different\ntherefrom.\nFurther, the drive current waveform of the drive pattern DW is not particularly limited.  For example, the drive current waveform of the drive pattern DW can also include the drive current waveforms shown in FIG. 12A, FIG. 12B and FIG. 13.  FIG.\n12A, FIG. 12B and FIG. 13 are each a diagram showing another example of the drive current waveform supplied to the discharge lamp 90 in the AC drive operation.  In FIG. 12A, FIG. 12B and FIG. 13, the vertical axis represents the drive current I, and the\nhorizontal axis represents time T. The drive current I is shown defining the case of the first polarity state as positive, and the case of the second polarity state as negative.\nThe AC drive operations shown in FIG. 12A and FIG. 12B are each an imbalanced drive operation with the duty ratio smaller than 0.5 (50%).  The drive current I supplied to the discharge lamp 90 in each of the imbalanced drive operations shown in\nFIG. 12A and FIG. 12B is, for example, a rectangular wave AC current having the polarity inverted a plurality of times between the current value Im and the current value -Im.\nIn the imbalanced drive operation shown in FIG. 12A, the length of the period C2 is constant.  In the period C2, there are provided a first polarity period C21 realizing the first polarity state, and a second polarity period C22 realizing the\nsecond polarity state.  The length of the first polarity period C21 is longer than the length of the second polarity period C22.  Specifically, the imbalanced drive operation shown in FIG. 12A has the number of cycles, the length of the period C2, the\nduty ratio, the length of the first polarity period C21 and the length of the second polarity period C22 as the drive parameters.\nIn the imbalanced drive operation shown in FIG. 12B, the length of the period varies.  In the example shown in FIG. 12B, there are provided a first period C3, a second period C4, and a third period C5.  The length of the first period C3 is\nlonger than the length of the third period C5.  The length of the second period C4 is longer than the length of the first period C3.\nIn the first period C3, there are provided a first polarity period C31 realizing the first polarity state, and a second polarity period C32 realizing the second polarity state.  The length of the second polarity period C32 is shorter than the\nlength of the first polarity period C31.  In the second period C4, there are provided a first polarity period C41 realizing the first polarity state, and a second polarity period C42 realizing the second polarity state.  The length of the second polarity\nperiod C42 is shorter than the length of the first polarity period C41.  In the third period C5, there are provided a first polarity period C51 realizing the first polarity state, and a second polarity period C52 realizing the second polarity state.  The\nlength of the second polarity period C52 is shorter than the length of the first polarity period C51.\nThe length of the first polarity period C31 is longer than the length of the first polarity period C51.  The length of the first polarity period C41 is longer than the length of the first polarity period C31.  The length of the second polarity\nperiod C32, the length of the second polarity period C42, and the length of the second polarity period C52 are the same as each other.  The lengths of the first polarity periods C31, C41, and C51 are, for example, no less than 5.0 ms (millisecond) and no\nmore than 20 ms (millisecond).  The lengths of the second polarity periods C32, C42, and C52 are shorter than 0.5 ms (millisecond).\nSpecifically, the imbalanced drive operation shown in FIG. 12B has the number of cycles, the lengths of the respective periods, the duty ratios, the lengths of the first polarity periods and the lengths of the second polarity periods as the\ndrive parameters similarly to the imbalanced drive operation shown in FIG. 12A.\nThe AC drive operation shown in FIG. 13 is a jumping drive operation in which the absolute value of the drive current I jumps in each of the polarity states.  The drive current I supplied to the discharge lamp 90 in the jumping drive operation\nshown in FIG. 13 is, for example, a rectangular wave AC current having the current value changing between Im1, Im2, -Im1, -Im2.  The absolute value of Im2 and -Im2 is larger than the absolute value of Im1 and -Im1.  In the jumping drive operation shown\nin FIG. 13, the length of the period C6 is constant.  The duty ratio of the AC current shown in FIG. 13 is 0.5 (50%).\nIn the period C6, there are provided a first polarity period C61 realizing the first polarity state, and a second polarity period C62 realizing the second polarity state.  The length of the first polarity period C61 and the length of the second\npolarity period C62 are the same as each other.  The first polarity period C61 includes a low current period C61a and a high current period C61b.  The low current period C61a is a period in which the drive current I is at a current value Im1.  The high\ncurrent period C61b is a period in which the drive current I is at a current value Im2.  The length of the high current period C61b is shorter than the length of the low current period C61a.\nThe second polarity period C62 includes a low current period C62a and a high current period C62b.  The low current period C62a is a period in which the drive current I is at a current value -Im1.  The high current period C62b is a period in\nwhich the drive current I is at a current value -Im2.  The length of the high current period C62b is shorter than the length of the low current period C62a.  The length of the low current period C61a and the length of the low current period C62a are the\nsame as each other.  The length of the high current period C61b and the length of the high current period C62b are the same as each other.\nTherefore, the jumping drive operation shown in FIG. 13 has the number of cycles, the length of the period C6, the absolute value of the current values Im1 and -Im1, the absolute value of the current values Im2 and -Im2, the length of the first\npolarity period C61, the length of the second polarity period C62, the length of the low current period C61a and the length of the high current period C61b in the first polarity period C61, the length of the low current period C62a and the length of the\nhigh current period C62b in the second polarity period C62, the proportion of the low current period C61a or the high current period C61b in the first polarity period C61, the proportion of the low current period C62a or the high current period C62b in\nthe second polarity period C62, the ratio of the absolute value of the current values Im2 and -Im2 to the absolute value of the current values Im1 and -Im1, and so on as the drive parameters.\nIt should be noted that in the above description, the drive operations shown in FIG. 12A, FIG. 12B, and FIG. 13 are described as examples of the AC drive operation, but the AC drive operation is not limited to these examples.  For example, it is\nalso possible to assume the drive operations shown in FIG. 12A, FIG. 12B, and FIG. 13 as examples of the DC drive operation.  In this case, the polarity of the DC drive operation and the length of the execution time of the DC drive operation arbitrarily\nchange to thereby form the drive current waveforms shown in the drawings.\nFurther, although in the embodiment described above, an example of the case in which the invention is applied to the transmissive projector is explained, the invention can also be applied to reflective projectors.  Here, \"transmissive\" denotes\nthat the liquid crystal light valve including the liquid crystal panel and so on is a type of transmitting the light.  Further, \"reflective\" denotes that the liquid crystal light valve is a type of reflecting the light.  It should be noted that the light\nmodulation device is not limited to the liquid crystal panel or the like, but can also be a light modulation device using, for example, micro-mirrors.\nFurther, although in the embodiment described above, there is cited the example of the projector 500 using the three liquid crystal panels 560R, 560G, and 560B (the liquid crystal light valves 330R, 330G, and 330B), the invention can be applied\nto a projector using a single liquid crystal panel alone, or a projector using four or more liquid crystal panels.\nFurther, the configurations described hereinabove can arbitrarily be combined with each other within a range in which the configurations do not conflict with each other.\n<BR><BR>SPECIFIC EXAMPLES\nSpecific Examples 1, 2 and Comparative Examples 1, 2 were compared with each other to confirm the availability of the invention.  In Specific Examples 1, 2, the plurality of drive patterns DW were assumed to be based on the drive patterns DW\nhaving the drive current waveform shown in FIG. 9.  Further, the drive parameters of the AC drive operation and the DC drive operation in the plurality of drive patterns DW are assumed to be the three types, namely the first frequency f1 and the second\nfrequency f2, the lengths ta1, ta2 of the execution time of the AC drive operation, and the lengths td1, td2 of the execution time of the DC drive operation.  Further, it was assumed that as the numerical values of each of the types of the drive\nparameters, the first frequency f1 and the second frequency f2 were selected from 200 Hz, 300 Hz, and 400 Hz, the lengths ta1, ta2 of the execution time of the AC drive operations were selected from 0.1 ms (millisecond), 0.2 ms (millisecond) and 0.5 ms\n(millisecond), and the lengths td1, td2 of the execution time of the DC drive operations were selected from 10 ms (millisecond), 20 ms (millisecond), and 30 ms (millisecond).  Further, as the number of the drive patterns DW, totally 27 patterns were\nprepared by combining the three types of drive parameters described above and the numerical values thereof with each other.\nIn the Specific Example 1, the execution time of the drive pattern DW was set to 20 min (minutes), and it was assumed that if the change in the lamp voltage Vla due to the fact that the drive pattern DW was executed for 20 min (minutes) is\nsmaller than +3 V, the same drive pattern DW was executed once again, and if the change in the lamp voltage Vla is not smaller than +3V, a different drive pattern DW was executed.  It should be noted that the discharge lamp driver was controlled so that\nthe switching to a different drive pattern was performed irrespective of the change in the lamp voltage Vla in the case in which the same drive pattern DW was executed 6 times in a row.  By executing the drive pattern DW in such a manner as described\nabove, the different drive pattern DW is executed in accordance with the accumulated lighting time in the case of the same lamp voltage Vla.\nIn Specific Example 2, the 27 drive patterns DW were made to arbitrarily be selected and executed using the machine learning of the embodiment described above.  In Comparative Example 1, one preferable drive pattern DW was set in advance every 5\nV of the lamp voltage Vla from the 27 drive patterns DW, and was executed.  In Comparative Example 2, two preferable drive patterns DW were set in advance every 5 V of the lamp voltage Vla from the 27 drive patterns DW, and one drive pattern DW was made\nto arbitrarily be selected from the two drive patterns DW and then executed.\nAs the discharge lamp 90, a high-pressure mercury lamp rated at 200 W was used.  The drive power Wd supplied to the discharge lamp was set to 200 W. In all of Specific Examples 1, 2 and Comparative Examples 1, 2, lighting for 2 h (hours) and\nextinction for 15 min (minutes) were made to alternately be repeated, and the illuminance of the discharge lamp was measured every time the accumulated lighting time of 500 h (hours) elapsed.  The illuminance keeping ratio was calculated from the\nilluminance thus measured, and in the case in which the illuminance keeping ratio thus calculated became lower than 50%, the accumulated lighting time obtained when performing the measurement last time was obtained as the value of the life.  The\nilluminance keeping ratio is the ratio of the present illuminance of the discharge lamp to the illuminance of the discharge lamp when lighting at the first time.\nThe measurement and calculation described above were performed on 10 discharge lamps in all of Specific Examples 1, 2 and Comparative Examples 1, 2, and then the average values of the life obtained were compared with each other.  As a result,\nthe life was 5000 h (hours) in Comparative Examples 1, 2 on the one hand, the life was 7000 h (hours) in Specific Example 1, and the life was 10000 h (hours) in Specific Example 2 on the other hand.  Therefore, it was confirmed that it was possible to\nextend the life of the discharge lamp by executing the different drive patterns DW in accordance with the accumulated lighting time in the case in which the lamp voltage Vla was at the predetermined voltage value.  Further, since the life in Specific\nExample 2 was longer than the life in Specific Example 1, it was confirmed that the life of the discharge lamp could more efficiently be extended by performing the machine learning.  According to the above, the availability of the invention was\nconfirmed.\nThe entire disclosure of Japanese Patent Application No. 2017-180309, filed Sep. 20, 2017 is expressly incorporated by reference herein.", "application_number": "16135274", "abstract": " A discharge lamp drive device includes a discharge lamp driver adapted to\n     supply a drive current to a discharge lamp, a control section adapted to\n     control the discharge lamp driver, and a storage section adapted to store\n     a plurality of drive patterns of the drive current. Each of the drive\n     patterns has a plurality of drive parameters. The plurality of drive\n     patterns includes a first drive pattern and a second drive pattern\n     different from each other in a value of at least one of the plurality of\n     drive parameters. The control section is configured to execute the first\n     drive pattern and the second drive pattern in accordance with at least\n     one of accumulated lighting time of the discharge lamp and an individual\n     of the discharge lamp in a case in which an inter-electrode voltage of\n     the discharge lamp is at a predetermined voltage value.\n", "citations": ["6160361", "8120285", "9730304", "9785041", "20090237009", "20100134033", "20120074858", "20120162611", "20160320693", "20170142816", "20170219919"], "related": []}, {"id": "20190116485", "patent_code": "10375565", "patent_name": "Dynamic rerouting of wireless traffic based on input from machine\n     learning-based mobility path analysis", "year": "2019", "inventor_and_country_data": " Inventors: \nVasseur; Jean-Philippe (Saint Martin D'uriage, FR), Kolar; Vinay Kumar (San Jose, CA), Pandey; Santosh (Fremont, CA)  ", "description": "<BR><BR>TECHNICAL FIELD\nThe present disclosure relates generally to computer networks, and, more particularly, to the dynamic rerouting of wireless traffic based on input from machine learning-based mobility path analysis.\n<BR><BR>BACKGROUND\nIn most wireless networks, such as Wi-Fi networks, roaming is a fairly common event.  Generally, roaming refers to a client device transitioning from one wireless access point (AP) to another.  Notably, roaming is often caused by the client\ndevice attempting to connect to the \"best\" AP available in the location of the client.  The \"best\" AP from the standpoint of the client device may change over time due to movement of the client, changes in the environment that affect the signal (e.g., in\nterms of strength, signal to noise ratio, etc.), or other such factors. <BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nThe embodiments herein may be better understood by referring to the following description in conjunction with the accompanying drawings in which like reference numerals indicate identically or functionally similar elements, of which:\nFIGS. 1A-1B illustrate an example communication network;\nFIG. 2 illustrates an example network device/node;\nFIG. 3 illustrates an example network assurance system;\nFIGS. 4A-4E illustrate examples of a mobility path in a network;\nFIG. 5 illustrates an example architecture for performing mobility path analysis in a network assurance system;\nFIG. 6 illustrates examples of mobility paths in three dimensions;\nFIGS. 7A-7B illustrate examples of an assessment of client trajectories; and\nFIG. 8 illustrates an example simplified procedure for triggering a mobility path reroute by a client device.\n<BR><BR>DESCRIPTION OF EXAMPLE EMBODIMENTS\n<BR><BR>Overview\nAccording to one or more embodiments of the disclosure, a service receives data indicative of roaming failures along mobility paths in a network.  The mobility paths represent ordered series of wireless access points via which wireless clients\nhave accessed the network over time.  The service uses, based on the data indicative of the roaming failures, a machine learning-based model to associate mobility path failure metrics with portions of the mobility paths.  The service identifies, for a\nfirst mobility path, an alternate mobility path that has a lower mobility path failure metric than that of the first mobility path.  The service triggers a mobility path reroute for a particular client device in the network on the first mobility path to\nthe alternate mobility path.\n<BR><BR>DESCRIPTION\nA computer network is a geographically distributed collection of nodes interconnected by communication links and segments for transporting data between end nodes, such as personal computers and workstations, or other devices, such as sensors,\netc. Many types of networks are available, with the types ranging from local area networks (LANs) to wide area networks (WANs).  LANs typically connect the nodes over dedicated private communications links located in the same general physical location,\nsuch as a building or campus.  WANs, on the other hand, typically connect geographically dispersed nodes over long-distance communications links, such as common carrier telephone lines, optical lightpaths, synchronous optical networks (SONET), or\nsynchronous digital hierarchy (SDH) links, or Powerline Communications (PLC) such as IEEE 61334, IEEE P1901.2, and others.  The Internet is an example of a WAN that connects disparate networks throughout the world, providing global communication between\nnodes on various networks.  The nodes typically communicate over the network by exchanging discrete frames or packets of data according to predefined protocols, such as the Transmission Control Protocol/Internet Protocol (TCP/IP).  In this context, a\nprotocol consists of a set of rules defining how the nodes interact with each other.  Computer networks may be further interconnected by an intermediate network node, such as a router, to extend the effective \"size\" of each network.\nSmart object networks, such as sensor networks, in particular, are a specific type of network having spatially distributed autonomous devices such as sensors, actuators, etc., that cooperatively monitor physical or environmental conditions at\ndifferent locations, such as, e.g., energy/power consumption, resource consumption (e.g., water/gas/etc. for advanced metering infrastructure or \"AMI\" applications) temperature, pressure, vibration, sound, radiation, motion, pollutants, etc. Other types\nof smart objects include actuators, e.g., responsible for turning on/off an engine or perform any other actions.  Sensor networks, a type of smart object network, are typically shared-media networks, such as wireless or PLC networks.  That is, in\naddition to one or more sensors, each sensor device (node) in a sensor network may generally be equipped with a radio transceiver or other communication port such as PLC, a microcontroller, and an energy source, such as a battery.  Often, smart object\nnetworks are considered field area networks (FANs), neighborhood area networks (NANs), personal area networks (PANs), etc. Generally, size and cost constraints on smart object nodes (e.g., sensors) result in corresponding constraints on resources such as\nenergy, memory, computational speed and bandwidth.\nFIG. 1A is a schematic block diagram of an example computer network 100 illustratively comprising nodes/devices, such as a plurality of routers/devices interconnected by links or networks, as shown.  For example, customer edge (CE) routers 110\nmay be interconnected with provider edge (PE) routers 120 (e.g., PE-1, PE-2, and PE-3) in order to communicate across a core network, such as an illustrative network backbone 130.  For example, routers 110, 120 may be interconnected by the public\nInternet, a multiprotocol label switching (MPLS) virtual private network (VPN), or the like.  Data packets 140 (e.g., traffic/messages) may be exchanged among the nodes/devices of the computer network 100 over links using predefined network communication\nprotocols such as the Transmission Control Protocol/Internet Protocol (TCP/IP), User Datagram Protocol (UDP), Asynchronous Transfer Mode (ATM) protocol, Frame Relay protocol, or any other suitable protocol.  Those skilled in the art will understand that\nany number of nodes, devices, links, etc. may be used in the computer network, and that the view shown herein is for simplicity.\nIn some implementations, a router or a set of routers may be connected to a private network (e.g., dedicated leased lines, an optical network, etc.) or a virtual private network (VPN), such as an MPLS VPN thanks to a carrier network, via one or\nmore links exhibiting very different network and service level agreement characteristics.  For the sake of illustration, a given customer site may fall under any of the following categories:\n1.) Site Type A: a site connected to the network (e.g., via a private or VPN link) using a single CE router and a single link, with potentially a backup link (e.g., a 3G/4G/LTE backup connection).  For example, a particular CE router 110 shown\nin network 100 may support a given customer site, potentially also with a backup link, such as a wireless connection.\n2.) Site Type B: a site connected to the network using two MPLS VPN links (e.g., from different Service Providers), with potentially a backup link (e.g., a 3G/4G/LTE connection).  A site of type B may itself be of different types:\n2a.) Site Type B1: a site connected to the network using two MPLS VPN links (e.g., from different Service Providers), with potentially a backup link (e.g., a 3G/4G/LTE connection).\n2b.) Site Type B2: a site connected to the network using one MPLS VPN link and one link connected to the public Internet, with potentially a backup link (e.g., a 3G/4G/LTE connection).  For example, a particular customer site may be connected to\nnetwork 100 via PE-3 and via a separate Internet connection, potentially also with a wireless backup link.\n2c.) Site Type B3: a site connected to the network using two links connected to the public Internet, with potentially a backup link (e.g., a 3G/4G/LTE connection).\nNotably, MPLS VPN links are usually tied to a committed service level agreement, whereas Internet links may either have no service level agreement at all or a loose service level agreement (e.g., a \"Gold Package\" Internet service connection that\nguarantees a certain level of performance to a customer site).\n3.) Site Type C: a site of type B (e.g., types B1, B2 or B3) but with more than one CE router (e.g., a first CE router connected to one link while a second CE router is connected to the other link), and potentially a backup link (e.g., a\nwireless 3G/4G/LTE backup link).  For example, a particular customer site may include a first CE router 110 connected to PE-2 and a second CE router 110 connected to PE-3.\nFIG. 1B illustrates an example of network 100 in greater detail, according to various embodiments.  As shown, network backbone 130 may provide connectivity between devices located in different geographical areas and/or different types of local\nnetworks.  For example, network 100 may comprise local/branch networks 160, 162 that include devices/nodes 10-16 and devices/nodes 18-20, respectively, as well as a data center/cloud environment 150 that includes servers 152-154.  Notably, local networks\n160-162 and data center/cloud environment 150 may be located in different geographic locations.\nServers 152-154 may include, in various embodiments, a network management server (NMS), a dynamic host configuration protocol (DHCP) server, a constrained application protocol (CoAP) server, an outage management system (OMS), an application\npolicy infrastructure controller (APIC), an application server, etc. As would be appreciated, network 100 may include any number of local networks, data centers, cloud environments, devices/nodes, servers, etc.\nIn some embodiments, the techniques herein may be applied to other network topologies and configurations.  For example, the techniques herein may be applied to peering points with high-speed links, data centers, etc.\nIn various embodiments, network 100 may include one or more mesh networks, such as an Internet of Things network.  Loosely, the term \"Internet of Things\" or \"IoT\" refers to uniquely identifiable objects (things) and their virtual representations\nin a network-based architecture.  In particular, the next frontier in the evolution of the Internet is the ability to connect more than just computers and communications devices, but rather the ability to connect \"objects\" in general, such as lights,\nappliances, vehicles, heating, ventilating, and air-conditioning (HVAC), windows and window shades and blinds, doors, locks, etc. The \"Internet of Things\" thus generally refers to the interconnection of objects (e.g., smart objects), such as sensors and\nactuators, over a computer network (e.g., via IP), which may be the public Internet or a private network.\nNotably, shared-media mesh networks, such as wireless or PLC networks, etc., are often on what is referred to as Low-Power and Lossy Networks (LLNs), which are a class of network in which both the routers and their interconnect are constrained:\nLLN routers typically operate with constraints, e.g., processing power, memory, and/or energy (battery), and their interconnects are characterized by, illustratively, high loss rates, low data rates, and/or instability.  LLNs are comprised of anything\nfrom a few dozen to thousands or even millions of LLN routers, and support point-to-point traffic (between devices inside the LLN), point-to-multipoint traffic (from a central control point such at the root node to a subset of devices inside the LLN),\nand multipoint-to-point traffic (from devices inside the LLN towards a central control point).  Often, an IoT network is implemented with an LLN-like architecture.  For example, as shown, local network 160 may be an LLN in which CE-2 operates as a root\nnode for nodes/devices 10-16 in the local mesh, in some embodiments.\nIn contrast to traditional networks, LLNs face a number of communication challenges.  First, LLNs communicate over a physical medium that is strongly affected by environmental conditions that change over time.  Some examples include temporal\nchanges in interference (e.g., other wireless networks or electrical appliances), physical obstructions (e.g., doors opening/closing, seasonal changes such as the foliage density of trees, etc.), and propagation characteristics of the physical media\n(e.g., temperature or humidity changes, etc.).  The time scales of such temporal changes can range between milliseconds (e.g., transmissions from other transceivers) to months (e.g., seasonal changes of an outdoor environment).  In addition, LLN devices\ntypically use low-cost and low-power designs that limit the capabilities of their transceivers.  In particular, LLN transceivers typically provide low throughput.  Furthermore, LLN transceivers typically support limited link margin, making the effects of\ninterference and environmental changes visible to link and network protocols.  The high number of nodes in LLNs in comparison to traditional networks also makes routing, quality of service (QoS), security, network management, and traffic engineering\nextremely challenging, to mention a few.\nFIG. 2 is a schematic block diagram of an example node/device 200 that may be used with one or more embodiments described herein, e.g., as any of the computing devices shown in FIGS. 1A-1B, particularly the PE routers 120, CE routers 110,\nnodes/device 10-20, servers 152-154 (e.g., a network controller located in a data center, etc.), any other computing device that supports the operations of network 100 (e.g., switches, etc.), or any of the other devices referenced below.  The device 200\nmay also be any other suitable type of device depending upon the type of network architecture in place, such as IoT nodes, etc. Device 200 comprises one or more network interfaces 210, one or more processors 220, and a memory 240 interconnected by a\nsystem bus 250, and is powered by a power supply 260.\nThe network interfaces 210 include the mechanical, electrical, and signaling circuitry for communicating data over physical links coupled to the network 100.  The network interfaces may be configured to transmit and/or receive data using a\nvariety of different communication protocols.  Notably, a physical network interface 210 may also be used to implement one or more virtual network interfaces, such as for virtual private network (VPN) access, known to those skilled in the art.\nThe memory 240 comprises a plurality of storage locations that are addressable by the processor(s) 220 and the network interfaces 210 for storing software programs and data structures associated with the embodiments described herein.  The\nprocessor 220 may comprise necessary elements or logic adapted to execute the software programs and manipulate the data structures 245.  An operating system 242 (e.g., the Internetworking Operating System, or IOS.RTM., of Cisco Systems, Inc., another\noperating system, etc.), portions of which are typically resident in memory 240 and executed by the processor(s), functionally organizes the node by, inter alia, invoking network operations in support of software processors and/or services executing on\nthe device.  These software processors and/or services may comprise a network assurance process 248, as described herein, any of which may alternatively be located within individual network interfaces.\nIt will be apparent to those skilled in the art that other processor and memory types, including various computer-readable media, may be used to store and execute program instructions pertaining to the techniques described herein.  Also, while\nthe description illustrates various processes, it is expressly contemplated that various processes may be embodied as modules configured to operate in accordance with the techniques herein (e.g., according to the functionality of a similar process). \nFurther, while processes may be shown and/or described separately, those skilled in the art will appreciate that processes may be routines or modules within other processes.\nNetwork assurance process 248 includes computer executable instructions that, when executed by processor(s) 220, cause device 200 to perform network assurance functions as part of a network assurance infrastructure within the network.  In\ngeneral, network assurance refers to the branch of networking concerned with ensuring that the network provides an acceptable level of quality in terms of the user experience.  For example, in the case of a user participating in a videoconference, the\ninfrastructure may enforce one or more network policies regarding the videoconference traffic, as well as monitor the state of the network, to ensure that the user does not perceive potential issues in the network (e.g., the video seen by the user\nfreezes, the audio output drops, etc.).\nIn some embodiments, network assurance process 248 may use any number of predefined health status rules, to enforce policies and to monitor the health of the network, in view of the observed conditions of the network.  For example, one rule may\nbe related to maintaining the service usage peak on a weekly and/or daily basis and specify that if the monitored usage variable exceeds more than 10% of the per day peak from the current week AND more than 10% of the last four weekly peaks, an insight\nalert should be triggered and sent to a user interface.\nAnother example of a health status rule may involve client transition events in a wireless network.  In such cases, whenever there is a failure in any of the transition events, the wireless controller may send a reason_code to the assurance\nsystem.  To evaluate a rule regarding these conditions, the network assurance system may then group 150 failures into different \"buckets\" (e.g., Association, Authentication, Mobility, DHCP, WebAuth, Configuration, Infra, Delete, De-Authorization) and\ncontinue to increment these counters per service set identifier (SSID), while performing averaging every five minutes and hourly.  The system may also maintain a client association request count per SSID every five minutes and hourly, as well.  To\ntrigger the rule, the system may evaluate whether the error count in any bucket has exceeded 20% of the total client association request count for one hour.\nIn various embodiments, network assurance process 248 may also utilize machine learning techniques, to enforce policies and to monitor the health of the network.  In general, machine learning is concerned with the design and the development of\ntechniques that take as input empirical data (such as network statistics and performance indicators), and recognize complex patterns in these data.  One very common pattern among machine learning techniques is the use of an underlying model M, whose\nparameters are optimized for minimizing the cost function associated to M, given the input data.  For instance, in the context of classification, the model M may be a straight line that separates the data into two classes (e.g., labels) such that\nM=a*x+b*y+c and the cost function would be the number of misclassified points.  The learning process then operates by adjusting the parameters a, b, c such that the number of misclassified points is minimal.  After this optimization phase (or learning\nphase), the model M can be used very easily to classify new data points.  Often, M is a statistical model, and the cost function is inversely proportional to the likelihood of M, given the input data.\nIn various embodiments, network assurance process 248 may employ one or more supervised, unsupervised, or semi-supervised machine learning models.  Generally, supervised learning entails the use of a training set of data, as noted above, that is\nused to train the model to apply labels to the input data.  For example, the training data may include sample network observations that do, or do not, violate a given network health status rule and are labeled as such.  On the other end of the spectrum\nare unsupervised techniques that do not require a training set of labels.  Notably, while a supervised learning model may look for previously seen patterns that have been labeled as such, an unsupervised model may instead look to whether there are sudden\nchanges in the behavior.  Semi-supervised learning models take a middle ground approach that uses a greatly reduced set of labeled training data.\nExample machine learning techniques that network assurance process 248 can employ may include, but are not limited to, nearest neighbor (NN) techniques (e.g., k-NN models, replicator NN models, etc.), statistical techniques (e.g., Bayesian\nnetworks, etc.), clustering techniques (e.g., k-means, mean-shift, etc.), neural networks (e.g., reservoir networks, artificial neural networks, etc.), support vector machines (SVMs), logistic or other regression, Markov models or chains, principal\ncomponent analysis (PCA) (e.g., for linear models), multi-layer perceptron (MLP) ANNs (e.g., for non-linear models), replicating reservoir networks (e.g., for non-linear models, typically for time series), random forest classification, or the like.\nThe performance of a machine learning model can be evaluated in a number of ways based on the number of true positives, false positives, true negatives, and/or false negatives of the model.  For example, the false positives of the model may\nrefer to the number of times the model incorrectly predicted whether a network health status rule was violated.  Conversely, the false negatives of the model may refer to the number of times the model predicted that a health status rule was not violated\nwhen, in fact, the rule was violated.  True negatives and positives may refer to the number of times the model correctly predicted whether a rule was violated or not violated, respectively.  Related to these measurements are the concepts of recall and\nprecision.  Generally, recall refers to the ratio of true positives to the sum of true positives and false negatives, which quantifies the sensitivity of the model.  Similarly, precision refers to the ratio of true positives the sum of true and false\npositives.\nFIG. 3 illustrates an example network assurance system 300, according to various embodiments.  As shown, at the core of network assurance system 300 may be a cloud service 302 that leverages machine learning in support of cognitive analytics for\nthe network, predictive analytics (e.g., models used to predict user experience, etc.), troubleshooting with root cause analysis, and/or trending analysis for capacity planning.  Generally, architecture 300 may support both wireless and wired network, as\nwell as LLNs/IoT networks.\nIn various embodiments, cloud service 302 may oversee the operations of the network of an entity (e.g., a company, school, etc.) that includes any number of local networks.  For example, cloud service 302 may oversee the operations of the local\nnetworks of any number of branch offices (e.g., branch office 306) and/or campuses (e.g., campus 308) that may be associated with the entity.  Data collection from the various local networks/locations may be performed by a network data collection\nplatform 304 that communicates with both cloud service 302 and the monitored network of the entity.\nThe network of branch office 306 may include any number of wireless access points 320 (e.g., a first access point AP1 through nth access point, APn) through which endpoint nodes may connect.  Access points 320 may, in turn, be in communication\nwith any number of wireless LAN controllers (WLCs) 326 located in a centralized datacenter 324.  For example, access points 320 may communicate with WLCs 326 via a VPN 322 and network data collection platform 304 may, in turn, communicate with the\ndevices in datacenter 324 to retrieve the corresponding network feature data from access points 320, WLCs 326, etc. In such a centralized model, access points 320 may be flexible access points and WLCs 326 may be N+1 high availability (HA) WLCs, by way\nof example.\nConversely, the local network of campus 308 may instead use any number of access points 328 (e.g., a first access point AP1 through nth access point APm) that provide connectivity to endpoint nodes, in a decentralized manner.  Notably, instead\nof maintaining a centralized datacenter, access points 328 may instead be connected to distributed WLCs 330 and switches/routers 332.  For example, WLCs 330 may be 1:1 HA WLCs and access points 328 may be local mode access points, in some\nimplementations.\nTo support the operations of the network, there may be any number of network services and control plane functions 310.  For example, functions 310 may include routing topology and network metric collection functions such as, but not limited to,\nrouting protocol exchanges, path computations, monitoring services (e.g., NetFlow or IPFIX exporters), etc. Further examples of functions 310 may include authentication functions, such as by an Identity Services Engine (ISE) or the like, mobility\nfunctions such as by a Connected Mobile Experiences (CMX) function or the like, management functions, and/or automation and control functions such as by an APIC-Enterprise Manager (APIC-EM).\nDuring operation, network data collection platform 304 may receive a variety of data feeds that convey collected data 334 from the devices of branch office 306 and campus 308, as well as from network services and network control plane functions\n310.  Example data feeds may comprise, but are not limited to, management information bases (MIBS) with Simple Network Management Protocol (SNMP)v2, JavaScript Object Notation (JSON) Files (e.g., WSA wireless, etc.), NetFlow/IPFIX records, logs reporting\nin order to collect rich datasets related to network control planes (e.g., Wi-Fi roaming, join and authentication, routing, QoS, PHY/MAC counters, links/node failures), traffic characteristics, and other such telemetry data regarding the monitored\nnetwork.  As would be appreciated, network data collection platform 304 may receive collected data 334 on a push and/or pull basis, as desired.  Network data collection platform 304 may prepare and store the collected data 334 for processing by cloud\nservice 302.  In some cases, network data collection platform may also anonymize collected data 334 before providing the anonymized data 336 to cloud service 302.\nIn some cases, cloud service 302 may include a data mapper and normalizer 314 that receives the collected and/or anonymized data 336 from network data collection platform 304.  In turn, data mapper and normalizer 314 may map and normalize the\nreceived data into a unified data model for further processing by cloud service 302.  For example, data mapper and normalizer 314 may extract certain data features from data 336 for input and analysis by cloud service 302.\nIn various embodiments, cloud service 302 may include a machine learning-based analyzer 312 configured to analyze the mapped and normalized data from data mapper and normalizer 314.  Generally, analyzer 312 may comprise a power machine\nlearning-based engine that is able to understand the dynamics of the monitored network, as well as to predict behaviors and user experiences, thereby allowing cloud service 302 to identify and remediate potential network issues before they happen.\nMachine learning-based analyzer 312 may include any number of machine learning models to perform the techniques herein, such as for cognitive analytics, predictive analysis, and/or trending analytics as follows:\nCognitive Analytics Model(s): The aim of cognitive analytics is to find behavioral patterns in complex and unstructured datasets.  For the sake of illustration, analyzer 312 may be able to extract patterns of Wi-Fi roaming in the network and\nroaming behaviors (e.g., the \"stickiness\" of clients to APs 320, 328, \"ping-pong\" clients, the number of visited APs 320, 328, roaming triggers, etc).  Analyzer 312 may characterize such patterns by the nature of the device (e.g., device type, OS)\naccording to the place in the network, time of day, routing topology, type of AP/WLC, etc., and potentially correlated with other network metrics (e.g., application, QoS, etc.).  In another example, the cognitive analytics model(s) may be configured to\nextract AP/WLC related patterns such as the number of clients, traffic throughput as a function of time, number of roaming processed, or the like, or even end-device related patterns (e.g., roaming patterns of iPhones, IoT Healthcare devices, etc.).\nPredictive Analytics Model(s): These model(s) may be configured to predict user experiences, which is a significant paradigm shift from reactive approaches to network health.  For example, in a Wi-Fi network, analyzer 312 may be configured to\nbuild predictive models for the joining/roaming time by taking into account a large plurality of parameters/observations (e.g., RF variables, time of day, number of clients, traffic load, DHCP/DNS/Radius time, AP/WLC loads, etc.).  From this, analyzer\n312 can detect potential network issues before they happen.  Furthermore, should abnormal joining time be predicted by analyzer 312, cloud service 312 will be able to identify the major root cause of this predicted condition, thus allowing cloud service\n302 to remedy the situation before it occurs.  The predictive analytics model(s) of analyzer 312 may also be able to predict other metrics such as the expected throughput for a client using a specific application.  In yet another example, the predictive\nanalytics model(s) may predict the user experience for voice/video quality using network variables (e.g., a predicted user rating of 1-5 stars for a given session, etc.), as function of the network state.  As would be appreciated, this approach may be\nfar superior to traditional approaches that rely on a mean opinion score (MOS).  In contrast, cloud service 302 may use the predicted user experiences from analyzer 312 to provide information to a network administrator or architect in real-time and\nenable closed loop control over the network by cloud service 302, accordingly.  For example, cloud service 302 may signal to a particular type of endpoint node in branch office 306 or campus 308 (e.g., an iPhone, an IoT healthcare device, etc.) that\nbetter QoS will be achieved if the device switches to a different AP 320 or 328.\nTrending Analytics Model(s): The trending analytics model(s) may include multivariate models that can predict future states of the network, thus separating noise from actual network trends.  Such predictions can be used, for example, for\npurposes of capacity planning and other \"what-if\" scenarios.\nMachine learning-based analyzer 312 may be specifically tailored for use cases in which machine learning is the only viable approach due to the high dimensionality of the dataset and patterns cannot otherwise be understood and learned.  For\nexample, finding a pattern so as to predict the actual user experience of a video call, while taking into account the nature of the application, video CODEC parameters, the states of the network (e.g., data rate, RF, etc.), the current observed load on\nthe network, destination being reached, etc., is simply impossible using predefined rules in a rule-based system.\nUnfortunately, there is no one-size-fits-all machine learning methodology that is capable of solving all, or even most, use cases.  In the field of machine learning, this is referred to as the \"No Free Lunch\" theorem.  Accordingly, analyzer 312\nmay rely on a set of machine learning processes that work in conjunction with one another and, when assembled, operate as a multi-layered kernel.  This allows network assurance system 300 to operate in real-time and constantly learn and adapt to new\nnetwork conditions and traffic characteristics.  In other words, not only can system 300 compute complex patterns in highly dimensional spaces for prediction or behavioral analysis, but system 300 may constantly evolve according to the captured\ndata/observations from the network.\nCloud service 302 may also include output and visualization interface 318 configured to provide sensory data to a network administrator or other user via one or more user interface devices (e.g., an electronic display, a keypad, a speaker,\netc.).  For example, interface 318 may present data indicative of the state of the monitored network, current or predicted issues in the network (e.g., the violation of a defined rule, etc.), insights or suggestions regarding a given condition or issue\nin the network, etc. Cloud service 302 may also receive input parameters from the user via interface 318 that control the operation of system 300 and/or the monitored network itself.  For example, interface 318 may receive an instruction or other\nindication to adjust/retrain one of the models of analyzer 312 from interface 318 (e.g., the user deems an alert/rule violation as a false positive).\nIn various embodiments, cloud service 302 may further include an automation and feedback controller 316 that provides closed-loop control instructions 338 back to the various devices in the monitored network.  For example, based on the\npredictions by analyzer 312, the evaluation of any predefined health status rules by cloud service 302, and/or input from an administrator or other user via input 318, controller 316 may instruct an endpoint client device, networking device in branch\noffice 306 or campus 308, or a network service or control plane function 310, to adjust its operations (e.g., by signaling an endpoint to use a particular AP 320 or 328, etc.).\nAs noted above, in many wireless networks, such as Wi-Fi networks, roaming is a fairly common event.  Roaming can be triggered by mobility of the client device whereby the client tries to always connect to the best Access point (AP) or,\nsometimes, simply because the client determines that the current AP is not the best AP.  Notably, even when the client is not currently moving, signal strength, signal to noise ratio (SNR), etc. may change (e.g., due to changing environmental conditions. In almost all forms of wireless networks, with the exception of DETNET and the like, roaming decisions are made by the client device.\nThe process of roaming in a wireless network is by far not \"free\" and could be highly subject to issues, thus leading to connectivity loss and application disruption.  There are different types of roaming (e.g., intra-WLC, layer-2, layer-3,\netc.) that potentially require a series of steps to successfully complete for the roaming to succeed: association, (re)authentication, rekeying the Group Temporal Key (GTK), de-authentication, DHCP operations, and the like.  In other words, there are a\nnumber of possible ways in which roaming can fail in a wireless network.  Accordingly, the large number of roaming events that typically occur in a wireless network, as well as the numerous conditions that can lead to roaming failures, can often impinge\non the user experience.\nDynamic Rerouting of Wireless Traffic Based on Input from Machine Learning-Based Mobility Path Analysis\nThe techniques herein introduce a mechanism that helps to reduce and/or eliminate roaming failures in a wireless network.  In some aspects, the techniques herein introduce the concept of mobility path metrics used to evaluate the risk of failure\nwhen roaming in a wireless network.  In another aspect, a central path computation engine (PCE) may gather information regarding the roaming events in the network and leverage machine learning to compute and associate a mobility failure metric with the\nmobility path.  Such a metric may then be used as a signal to constrain the mobility paths based on their risk of failure.  In further aspects, information regarding the mobility paths and their failure metrics may be provided to client devices on\nrequest, such as when the client joins the wireless network or on detection of a special event (e.g., to perform a fast reroute using the techniques herein).  Note that although the techniques herein are described primarily with respect to Wi-Fi\nnetworks, the techniques herein are equally applicable in other networks such as, but not limited to, cellular network (e.g., 4G, 5G, LTE, etc.), IoT networks that use 802.15.4 with the techniques herein adapted to take into account the local ETX of such\nlinks, and any other wireless network that supports roaming.\nSpecifically, according to one or more embodiments of the disclosure as described in detail below, a service receives data indicative of roaming failures along mobility paths in a network.  The mobility paths represent ordered series of wireless\naccess points via which wireless clients have accessed the network over time.  The service uses, based on the data indicative of the roaming failures, a machine learning-based model to associate mobility path failure metrics with portions of the mobility\npaths.  The service identifies, for a first mobility path, an alternate mobility path that has a lower mobility path failure metric than that of the first mobility path.  The service triggers a mobility path reroute for a particular client device in the\nnetwork on the first mobility path to the alternate mobility path.\nIllustratively, the techniques described herein may be performed by hardware, software, and/or firmware, such as in accordance with the network assurance process 248, which may include computer executable instructions executed by the processor\n220 (or independent processor of interfaces 210) to perform functions relating to the techniques described herein.\nOperationally, the concept of a mobility path in a network is introduced herein.  In contrast with a data plane path, a mobility path generally refers to the list of APs that a client device visits/attaches to in a given period.  As would be\nappreciated, an access point may be a Wi-Fi AP, a gateway in the context of the IoT, a base station, or any other networking device that communicates wirelessly with a client device and provides the client device access to the wireless network.  In some\ncases, a given mobility path may be defined as an ordered set of three or more AP nodes.  In other words, a mobility path is a control plane path that is not followed by the data, but by the client device itself.  Accordingly, a mobility path failure\nherein generally refers to an unsuccessful roaming event.  Such failed roaming events often lead to traffic disruptions, similar to a data path failure.\nFIGS. 4A-4E illustrate examples of a mobility path in a network, according to various embodiments.  In FIG. 4A, assume that a client device 402 is a mobile device that is traveling along a path of travel 406.  As would be appreciated, while path\nof travel 406 is depicted as a linear path, the movement of a mobile device in most situations will not be linear and may vary in one, two, or even three dimensions.  For purposes of illustration, assume that the local network comprises APs 404, such as\nAPs 404a-404c (e.g., APs A-C), as shown.  At time T=t.sub.0, client device 402 may be connected to the wireless network via AP 404a, which may be the closest AP 404 to client device 402 at this time or, alternatively, offer the best characteristics in\nterms of signal strength, SNR, etc.\nIn FIG. 4B, assume now that client device 402 has moved along path of travel 406 and is now closer to AP 404b at time T=t.sub.1.  If the wireless characteristics of AP 404b, from the standpoint of client device 402, are now better than that of\nAP 404a, client device 402 may initiate roaming.  As a result, client device 402 may attach itself to AP 404b and detach itself from AP 404a, thereby completing the roaming operation.  After attaching to AP 404b, client device 402 may continue to\ncommunicate with the network as normal via AP 404b.\nIn FIG. 4C, client device 402 may perform a similar operation as in FIG. 4C, but with AP 404c.  Notably, assume now that at time T=t.sub.2, client device 402 is now within closest proximity to AP 404c and/or that AP 404c offers the best\ncharacteristics, from the perspective of client device 402.  In such a case, client device 402 may initiate another roaming operation, thereby switching its access from AP 404b to AP 404c.\nFIG. 4D illustrates the concept of a mobility path 408, according to various embodiments.  Based on the movement and wireless roaming operations of client device 402 over time (e.g., between times T=t.sub.0 and T=t.sub.2), as depicted in FIG.\n4A-4C, client device 402 can be viewed as having traversed mobility path 408.  More specifically, mobility path 408 may be a directed (or directionless) set of nodes/APs 404 through which the client device 402 roamed in the network.  In this sense,\nroaming events between the APs 404 can be viewed akin to hops between nodes in a communication data path.\nFIG. 4E illustrates an example of a roaming failure, according to various embodiments.  Assume that the network assurance system has identified mobility path 408 by monitoring the roaming events for the various client devices in the network. \nFurther, assume that client device 412 first attached to AP 404a and then to AP 404b.  Thus, the network assurance system may determine that client device 412 is on mobility path 408 and will next roam to AP 404c.  However, instead of roaming to AP 404c,\nas expected, client device 412 newly attaches to AP 404d because of an inability to connect to AP 404c.  In this case, mobility path 408 can be said to have experienced a roaming failure 410, since client device 412 was unable to successfully roam to AP\n404c.  If client device 412 was participating in an online session at the time, roaming failure 410 could result in a loss of connectivity for client device 412 and impact the user experience of the user of client device 402.  For example, if client\ndevice 412 is participating in an online conference, loss of network connectivity due to roaming failure 410 could cause client device 412 to stop receiving the conference stream.\nFIG. 5 illustrates an example architecture 500 for performing mobility path analysis in a network assurance system, according to various embodiments.  In general, architecture 500 may include any or all of the following components: a mobility\npath analyzer 506 and/or a mobility path failure modeler 508.  In various embodiments, the components of architecture 500 may be implemented within a network assurance system, such as system 300 shown in FIG. 3.  Accordingly, the components 506-508 of\narchitecture 500 shown may be implemented as part of cloud service 302, as part of network data collection platform 304, and/or on one or more network elements/entities 502 within the monitored network itself.  For example, mobility path analyzer 506 and\nmobility path failure modeler 508 may be implemented as part of machine learning-based analyzer 312, in some embodiments, as shown.  Further, these components may be implemented in a distributed manner or implemented as its own stand-alone service,\neither as part of the local network under observation or as a remote service.  In addition, the functionalities of the components of architecture 500 may be combined, omitted, or implemented as part of other processes, as desired.\nDuring operation, a client device 504 may leverage one or more of network entities 502, to communicate wirelessly with the local network.  For example, network entities 502 may include wireless APs, WLC, switches, routers, or the like, that\nprovide network connectivity to client device 504.  In turn, network entities 502 may report information regarding the roaming and other wireless conditions associated with client device 504 to network data collection platform 304 as part of data 334. \nNetwork data collection platform 304 may then pass this data on to cloud service 302 for analysis by machine learning (ML)-based analyzer 312.\nA function of architecture 500 involves the notion of a mobility path objective function and a mobility failure metric for such paths.  With respect to communicating data throughout a network, an objective function may control the routing paths\ntraversed by the data.  Notably, in the context of IP or MPLS networks, routing metrics can be used in objective functions (e.g., find the shortest constrained path given a specific metric, which can reflect the bandwidth, jitter, link quality, etc.)\nand/or as a path constraint (e.g., prune paths for which a given constraint such as a color of minimum bandwidth, etc.), if not satisfied.  In a somewhat similar manner and in the context of a mobility path that represents the roaming transitions of a\nclient device, architecture 500 may utilize mobility path failure metrics to represent the probability of a mobility/roaming failure along a given mobility path.  In some embodiments, architecture 500 may also use these metrics as a constraint or in some\nobjective functions, as detailed below.\nAnother functionality of architecture 500 is the computation of mobility path failure metrics which can be performed, in some embodiments, by a single computational element, such as a PCE.  In the case of a wireless Wi-Fi network, roaming events\nmay be tracked by the WLC(s) (e.g., network entities 502) to which the APs are connected.  Note that a client device, such as client device 504, may roam between APs connected to different WLCs, in which case, more than one WLC may be involved in\ntracking roaming events.  All roaming events are then collected (e.g., by data collection platform 304) for each client device, to be able to compute mobility paths.\nUsing the collected data, mobility path analyzer 506 may identify the mobility paths that exist in the network.  In some cases, these paths can be distinguished in architecture 500 by user ID, thanks to the authentication steps taken by the\nmobile devices (e.g., leveraging an ISE), based on MAC addresses, or the like.  In one embodiment, mobility path analyzer 506 may first compute the most frequent sub-trajectories of the client devices in the network.  For example, mobility path analyzer\n506 may apply a machine learning-based clustering approach to the observed client trajectories, such as by using a time series of APs traversed by the clients and computing the most prominently occurring sequences of APs.\nSaid differently, mobility path analyzer 506 may transform the wireless traces from the monitored network into a mobility graph in which each node represents an AP, and the client device is represented as a hyper-edge on the graph.  Using this\ngraph notation, mobility path analyzer 506 may extract the trajectories of the client devices users from wireless traces using heuristic and machine learning approaches.  From the extracted trajectories, we leverage ML and other graph algorithms to\neliminate noisy paths where the device is oscillating between few APs.  In turn, mobility path analyzer 506 may identify the most-frequent sub-trajectories that have been traversed by a large number of clients.  Data mining approaches such as frequent\npattern mining (e.g., TKS and TPS), can be used to extract most frequent sub-paths.\nReferring briefly to FIG. 6, examples of mobility paths in three dimensions are shown.  Notably, plot 600 illustrates in three dimensions the mobility paths 602a-602e observed during testing of a wireless network.  In some cases, such as with\nmobility path 602b and 602d, the client devices may tend to stay at the same z-coordinate, indicating that the client device is likely to roam along a mobility path on a single floor.  However, such as in the case of mobility path 602e, the client device\nmay roam between APs with different z-coordinates, indicating that the user of the device may have traveled to a different floor.  For each of these trajectories (e.g., links between APs in a mobility path), architecture 500 may assess the observed\nroaming failures and compute mobility failure metrics from these observations.  For example, one insight from plot 600 is that the third floor (z=3) has many more failed paths concentrated in one area than that of the first and second floors.  In this\nway, architecture 500 can associate mobility failure metrics to the identified mobility paths.\nFor each sub-trajectory identified by mobility path analyzer 506, mobility path failure modeler 508 may compute mobility path failure metrics based on the failure events observed over these sub-trajectories (e.g., by examining the distributions\nof failures, clusters of failures that are commonly occurring, etc.).  In some embodiments, the mobility path failure metrics may also be client-specific or associated with a group of clients.  In other words, and in sharp contrast to data plane metrics\nthat are independent of the client type, the mobility path failure metrics may also vary with the type of client (e.g., some client devices may have better reception than others, may be more prone to roam, etc.).\nReferring again to FIG. 5, in various embodiments, architecture 500 may be further configured to drive client device mobility based on the mobility path failure metrics modeled by mobility path failure modeler 508.  In some embodiments, mobility\npath failure modeler 508 may leverage automation & feedback controller 316 to upload the mobility path information and failure metrics to network entities 502.  For example, mobility path failure modeler 508 may upload this information to tables\nmaintained by an AP, gateway, base station, or the like.\nIn general, given the knowledge of the frequent traversal paths and failed paths of the client devices in the monitored network, architecture 500 can also trigger local reroutes in the mobility paths of the client devices.  In one embodiment,\nmobility path failure modeler 508 predicts the approximate path of travel of client device 504 and its final destination.  Based on this predicted path, modeler 508 can compute a list of the most effective alternative mobility paths for client device 504\nthat are the most failure resistant.  In one embodiment, this can be computed by running a shortest path algorithm on the path graph from mobility path analyzer 506.  For example, the graph of the network can be computed with edges being weighted using\nthe failure metrics.  In turn, modeler 508 can compute \"weighted shortest paths\" from the same source and destination as that of the failed path.  This will yield probable mobility paths with low failures.\nAnother approach would be to infer the best paths by using machine learning and data-mining.  For example, ML-based analyzer 312 can query the client's history of all paths between the client's current AP and final destination.  In turn,\nanalyzer 312 can then mine these set of paths to infer the most-frequent paths which had low failures.  Local rerouting and next-hop selection can be done based on the shortest path information.\nOn joining the network, client device 504 may receive mobility path information 510 from the AP to which client device 504 attaches.  For example, mobility path information 510 may include a list of mobility paths along with their respective\nmobility path failure metrics for all potential next hops/APs, to aid client device 504 in its roaming decisions.  In particular, client device 504 may use this mobility path failure metric, along with other characteristics, such as a measured RSSI of\nanother AP, to determine whether client device 504 should roam to that AP.\nIn some cases, mobility path information 510 may be provided to client device 504 on expiration of a given timer, or when there are substantial changes in the mobility path failure metrics.  In another embodiment, network entities 502 may\ndynamically provide mobility path information 510 on being explicitly requested by client device 504.  For example, when client device 504 moves from AP.sub.x to AP.sub.y, client device 504 may be provided data indicative of the next hop AP and its\nassociated probability of failure or may be provided a list of mobility paths with their respective mobility path failure metrics.\nIn yet another embodiment, the AP may anticipate which next hops are likely to be visited by client device 504, by using machine learning to predict the mobility path that client device 504 is likely to take.  Note that the set of mobility\npath(s) that client device 504 is predicted to take may also be constrained to APs for which the expected signal quality from client device 504 will be above a given threshold.  For example, the most probable, low failure paths can be inferred by mining\nclient device trajectories with a weight on each path that corresponds to the wireless failures observed on that path.  Later, sequence mining, such as identifying the top-K subsequences, can be altered not to only account for the most probable\nsub-sequences, but also for the failures along this path.  One adaption of such an approach may be to exclude the edges in the sub-sequence that have faced more than n %, in another embodiment.  In turn, these sub-paths can be provided to client device\n504 as part of mobility path information 510.\nIn yet another mode of operation, client device 504 may explicitly request mobility path information 510 from network entities 502 using a custom signaling extension, such as 802.1k/v in the case of a Wi-Fi network.  Such a request may indicate,\nfor example, a requested set of mobility paths that are constrained based on their mobility path failure metrics.  For example, if client device 504 is associated with a given AP1, client device 504 may request a listing of all of the relevant mobility\npaths that have a mobility path failure metric that is less than a threshold value.  On receiving this listing (e.g., as part of mobility path information 510), client device 504 may use the listing to enhance its AP roaming decisions (e.g., by roaming\nalong a constrained mobility path that satisfies the specified failure metric constraint).  Note that such a mode of operation introduces a new approach for mobility in wireless networks whereby machine learning is used by a central PCE to govern\nmobility, thus effectively influencing/overriding local decisions made by client device 504.\nA further aspect of the techniques herein is the ability for architecture 500 to trigger mobility path reroutes for client device 504.  In particular, mobility path failure modeler 508 may determine that there is a high chance of mobility\nfailure along the mobility path of client device 504.  In turn, mobility path failure modeler 508 may push mobility path information 510 to client device 504 that causes client device 504 to deviate from its current mobility path to a different mobility\npath.\nBy way of example, plots 700 and 710 in FIGS. 7A-7B illustrate examples of the top-K subsequences/trajectories that affect clients in the network.  Notably, plot 700 illustrates a plot of the most failed sub-trajectories along a set of central\nnodes and plot 710 illustrates the paths in a distributed manner.  In other words, plot 700 provides insight that only a few central nodes/APs in the network are responsible for many of the mobility path failures.  In plot 710, however, the failures are\nmore distributed among different nodes.  From plots 700-710, it can be seen how diverse mobility paths can be in a wireless network and this insight can be leveraged by the network assurance system to assess and predict mobility path failures.  As would\nbe appreciated, failures can also be subdivided for purposes of predicting path failure metrics, such as by distinguishing between authentication-related failures and failures related to DHCP server timeouts.\nIn some embodiments, different approaches for updating mobility path failure metrics may be adopted according to the network topology, so as to avoid oscillation and instability.  For example, rerouting client devices along different mobility\npaths may be load balanced across clients, so as to avoid rerouting too many clients along the same set of APs.  In particular, rerouting clients to the same APs may trigger addition mobility path failures because of the additional burden of control\nplane messages sent when roaming takes place.\nReferring again to FIG. 5, another aspect of the techniques herein relates to performing next hop selection on client device 504, according to a set of active applications running on client device 504.  Indeed, in most end devices, the action of\nroaming is not always tied to the set of active applications, but is instead strictly a function of the wireless signal characteristics.  In some embodiments, the roaming decision by client device 504 may be governed, not only by the quality of the\nmobility path measured by the path failure metric introduced herein, but also based on one or more service level agreements (SLAs) of the active application(s) on client device 504.  For example, client device 504 may decide not to roam when a potential\nnext hop candidate (providing a better signal strength) belongs to a mobility path with a low path failure metric and real-time applications are active on client device 504 (e.g. a video call, etc.).  In one embodiment, ML-based analyzer 312 may compute\nthe path failures and attach the application performance and the failure attributes to the edges.  Machine learning approaches, such as trajectory clustering, can then be applied on top of these edges, to infer the most-promising paths for client device\n504 for a given set of applications.\nIn various embodiments, architecture 500 may also leverage feedback regarding any detected roaming failures, to modify the models used in ML-based analyzer 312.  Such feedback may be provided by the WLC, as in the case of Wi-Fi networks, but\ncould also be provided by client device 504, itself.  In such cases, a protocol extension may be used to signal back when roaming has failed for client device 504.  In either case, feedback can then be used to adjust the mobility path failure metric\npredictions and potentially the underlying ML model used to compute such metrics.  Techniques, such as reinforcement learning and active learning, can be used to strengthen the model based on the feedback.\nFIG. 8 illustrates an example simplified procedure for triggering a mobility path reroute by a client device in a network, in accordance with one or more embodiments described herein.  For example, a non-generic, specifically configured device\n(e.g., device 200) may perform procedure 800 by executing stored instructions (e.g., process 248) to provide a service to the network.  The procedure 800 may start at step 805, and continues to step 810, where, as described in greater detail above, the\nservice may receive data indicative of roaming failures along mobility paths in a network.  In various embodiments, the mobility paths represent ordered series of wireless access points via which wireless clients have accessed the network over time.  For\nexample, if client devices in the network are observed to roam from AP 1, to AP 2, to AP 3, a corresponding mobility path may represent these APs as nodes and the roaming transitions as edges between the nodes.\nAt step 815, as detailed above, the service may use, based on the data indicative of the roaming failures, a machine learning-based model to associate mobility path failure metrics with portions of the mobility paths.  In various embodiments,\nthe mobility path failure metrics may quantify the likelihood of a client device traversing a given mobility path experiencing a roaming failure between APs on the mobility path.\nAt step 820, the service may identify, for a first mobility path, an alternate mobility path that has a lower mobility path failure metric than that of the first mobility path.  For example, if the mobility path on which a client device is\ntraversing has a mobility path failure metric above a predefined threshold, this may indicate that the client device is likely to experience a roaming failure on the current mobility path.  In turn, the service may identify another mobility path that has\na lower path failure metric as an alternate mobility path for the client device.\nAt step 825, the service may trigger a mobility path reroute for a particular client device in the network on the first mobility path to the alternate mobility path, as described in greater detail above.  In some embodiments, the service may\ntrigger the mobility path reroute by sending mobility path information to the client device.  For example, such information may indicate one or more mobility paths and their associated mobility path failure metrics, thereby allowing the client device to\nlocally switch to the other mobility path.  In further embodiments, the service may receive an indication of a threshold mobility path failure metric for the client device and trigger the reroute based on the first path's failure metric being below this\nthreshold.  For example, the types of applications running on the device may require a certain degree of continuous connectivity (e.g., conferencing applications, etc.), thus requiring a mobility path having a low chance of roaming failures.  Procedure\n800 then ends at step 830.\nIt should be noted that while certain steps within procedure 800 may be optional as described above, the steps shown in FIG. 8 are merely examples for illustration, and certain other steps may be included or excluded as desired.  Further, while\na particular order of the steps is shown, this ordering is merely illustrative, and any suitable arrangement of the steps may be utilized without departing from the scope of the embodiments herein.\nThe techniques described herein, therefore, dramatically improve the user experience in wireless networks, effectively avoiding a large number of roaming failures.\nWhile there have been shown and described illustrative embodiments that provide for dynamic rerouting of wireless traffic based on input from ML-based mobility path analysis, it is to be understood that various other adaptations and\nmodifications may be made within the spirit and scope of the embodiments herein.  For example, while certain embodiments are described herein with respect to using certain models for purposes of predicting mobility path failure metrics, the models are\nnot limited as such and may be used for other functions, in other embodiments.  In addition, while certain wireless protocols are shown, such as Wi-Fi, other suitable protocols may be used, accordingly.\nThe foregoing description has been directed to specific embodiments.  It will be apparent, however, that other variations and modifications may be made to the described embodiments, with the attainment of some or all of their advantages.  For\ninstance, it is expressly contemplated that the components and/or elements described herein can be implemented as software being stored on a tangible (non-transitory) computer-readable medium (e.g., disks/CDs/RAM/EEPROM/etc.) having program instructions\nexecuting on a computer, hardware, firmware, or a combination thereof.  Accordingly, this description is to be taken only by way of example and not to otherwise limit the scope of the embodiments herein.  Therefore, it is the object of the appended\nclaims to cover all such variations and modifications as come within the true spirit and scope of the embodiments herein.", "application_number": "15783342", "abstract": " In one embodiment, a service receives data indicative of roaming failures\n     along mobility paths in a network. The mobility paths represent ordered\n     series of wireless access points via which wireless clients have accessed\n     the network over time. The service uses, based on the data indicative of\n     the roaming failures, a machine learning-based model to associate\n     mobility path failure metrics with portions of the mobility paths. The\n     service identifies, for a first mobility path, an alternate mobility path\n     that has a lower mobility path failure metric than that of the first\n     mobility path. The service triggers a mobility path reroute for a\n     particular client device in the network on the first mobility path to the\n     alternate mobility path.\n", "citations": ["6603968", "7400605", "7808918", "9363714", "20150189554"], "related": []}, {"id": "20190132341", "patent_code": "10375100", "patent_name": "Identifying anomalies in a network", "year": "2019", "inventor_and_country_data": " Inventors: \nGrayson; Mark (Berkshirem, GB), Patil; Santosh Ramrao (Santa Clara, CA), Pularikkal; Gangadharan Byju (San Jose, CA)  ", "description": "<BR><BR>TECHNICAL\nFIELD\nThe present disclosure relates generally to networks, and in particular, to identifying anomalies in a network.\n<BR><BR>BACKGROUND\nThe number of devices that are connected to networks has increased exponentially in recent years.  As deployment of the Internet of things (IoT) advances, the number of network-connected devices will further increase.  Some networks are\nsusceptible to security attacks via the devices that are part of the network.  As the number of devices in a network increase, the network often becomes more vulnerable to security attacks.  Furthermore, some IoT devices lack hardware and/or software\ncapability to prevent or thwart security attacks.  Given the various device types that are currently connected to networks and additional device types that will likely connect to networks in the future, networks that rely heavily on static rules for\nsecurity protection often fail at identifying anomalies. <BR><BR>BRIEF DESCRIPTION OF THE DRAWINGS\nSo that the present disclosure can be understood by those of ordinary skill in the art, a more detailed description may be had by reference to aspects of some illustrative implementations, some of which are shown in the accompanying drawings.\nFIG. 1 is a schematic diagram of a network environment in accordance with some implementations.\nFIG. 2 is a block diagram of an anomaly detector in accordance with some implementations.\nFIG. 3 is a sequence diagram illustrating radio resource control (RRC) messages being exchanged in accordance with some implementations.\nFIG. 4 is a sequence diagram illustrating non-access stratum (NAS) messages being exchanged in accordance with some implementations.\nFIG. 5 is a flowchart representation of a method of identifying anomalies in accordance with some implementations.\nFIG. 6 is a block diagram of a server system enabled with various modules that are provided to identify anomalies in a network in accordance with some implementations.\nIn accordance with common practice the various features illustrated in the drawings may not be drawn to scale.  Accordingly, the dimensions of the various features may be arbitrarily expanded or reduced for clarity.  In addition, some of the\ndrawings may not depict all of the components of a given system, method or device.  Finally, like reference numerals may be used to denote like features throughout the specification and figures.\n<BR><BR>DESCRIPTION OF EXAMPLE EMBODIMENTS\nNumerous details are described herein in order to provide a thorough understanding of the illustrative implementations shown in the accompanying drawings.  However, the accompanying drawings merely show some example aspects of the present\ndisclosure and are therefore not to be considered limiting.  Those of ordinary skill in the art will appreciate from the present disclosure that other effective aspects and/or variants do not include all of the specific details of the example\nimplementations described herein.  While pertinent features are shown and described, those of ordinary skill in the art will appreciate from the present disclosure that various other features, including well-known systems, methods, components, devices,\nand circuits, have not been illustrated or described in exhaustive detail for the sake of brevity and so as not to obscure more pertinent aspects of the example implementations disclosed herein.\n<BR><BR>OVERVIEW\nVarious implementations disclosed herein enable identifying anomalies in a network.  For example, in various implementations, a method of identifying anomalies in a network is performed by a network node.  In various implementations, the network\nnode includes one or more processors, and a non-transitory memory.  In various implementations, the method includes generating a characteristic indicator that characterizes a device type based on communications associated with a first device of the\ndevice type.  In various implementations, the method includes determining, based on communications associated with the first device, a performance indicator that indicates a performance of the first device.  In various implementations, the method\nincludes synthesizing an anomaly indicator as a function of the performance indicator in relation to the characteristic indicator.\n<BR><BR>EXAMPLE EMBODIMENTS\nFIG. 1 is a schematic diagram of a network environment 10.  While certain specific features are illustrated, those of ordinary skill in the art will appreciate from the present disclosure that various other features have not been illustrated for\nthe sake of brevity and so as not to obscure more pertinent aspects of the example implementations disclosed herein.  To that end, the network environment 10 includes devices 22 of various device types 20a, 20b .  . . 20n, and a cellular network 70 with\nvarious cellular network nodes 72a, 72b .  . . 72n.  In some implementations, the cellular network nodes 72a, 72b .  . . 72n include corresponding anomaly detectors 74a, 74b .  . . 74n.  In some implementations, the anomaly detectors 74a, 74b .  . . 74n,\nindividually or in combination, detect anomalies in the cellular network 70.  For example, in some implementations, the anomaly detectors 74a, 74b .  . . 74n, individually or in combination, identify devices 22 that exhibit anomalous behavior.  In some\nimplementations, the anomaly detectors 74a, 74b .  . . 74n protect the cellular network 70 from security threats by detecting anomalies.  For example, in some implementations, the anomaly detectors 74a, 74b .  . . 74n prevent and/or thwart security\nattacks on the cellular network nodes 72a, 72b .  . . 72n by devices 22 that are malicious.  In some implementations, the anomaly detectors 74a, 74b .  . . 74n collectively form a distributed anomaly detector 76.  In some implementations, the anomaly\ndetectors 74a, 74b .  . . 74n are collectively referred to as the distributed anomaly detector 76.\nIn various implementations, the cellular network 70 operates in accordance with 3GPP standards.  For example, in some implementations, the cellular network 70 includes a fifth-generation (5G) cellular network.  In various implementations, the\ncellular network nodes 72a, 72b .  . . 72n perform various cellular-related functions.  In some implementations, at least some of the cellular network nodes 72a, 72b .  . . 72n form the core network architecture for the cellular network 70 (e.g., as\ndefined by the 3GPP standards).  In some implementations, at least some of the cellular network nodes 72a, 72b .  . . 72n form the Evolved Packet Core (EPC) portion of the cellular network 70.  In some implementations, the cellular network node 72a\nincludes a base station (e.g., an eNode B (eNB)) that provides the devices 22 access to the cellular network 70.  In some implementations, the cellular network node 72b includes a Mobility Management Entity (MME) that serves as a signaling node in the\ncellular network 70.  For example, in some implementations, as the MME, the cellular network node 72b initiates paging and authentication of the devices 22.\nIn various implementations, one or more of the anomaly detectors 74a, 74b .  . . 74n identifies anomalies in the cellular network 70.  Briefly, in various implementations, one or more of the anomaly detectors 74a, 74b .  . . 74n identifies\nanomalies by grouping the devices 22 into the device types 20a, 20b .  . . 20n, generating corresponding characteristic indicators for the device types 20a, 20b .  . . 20n, determining corresponding performance indicators for the devices 22, and\nsynthesizing an anomaly indicator based on the performance indicators and the characteristic indicators.\nIn various implementations, one or more of the anomaly detectors 74a, 74b .  . . 74n groups the devices 22 into the device types 20a, 20b .  . . 20n based on one or more characteristics of the devices 22.  For example, in some implementations,\none or more of the anomaly detectors 74a, 74b .  . . 74n groups the devices 22 into the device types 20a, 20b .  . . 20n based on functions of the devices 22, identifiers of the devices 22 (e.g., IMEI of the devices 22), and/or classmarks of the devices\n22.  In the example of FIG. 1, the device type 20a includes devices 22 that are appliances (e.g., microwave 22a, washing machine 22b, refrigerator 22c, stoves, ovens, air conditioners, etc.).  In the example of FIG. 1, the device type 20b includes\ndevices 22 that are vehicles (e.g., automobiles 22d and 22e, trains, airplanes, ships, etc.).  In the example of FIG. 1, the device type 20c includes devices 22 that are communication devices (e.g., mobile devices such as laptop 22f and smartphone 22g). \nIn some implementations, the device types 20a, 20b .  . . 20n indicate operational characteristics of corresponding devices 22.  In the example of FIG. 1, since the device type 20a includes appliances that are typically immobile, the device type 20a\ncorresponds to no/low mobility devices.  In the example of FIG. 1, since the device type 20n includes communication devices such as the smartphone 22g that can be taken across continents, the device type 20n corresponds to high mobility devices.\nIn various implementations, one or more of the anomaly detectors 74a, 74b .  . . 74n generates the corresponding characteristic indicators for the device types 20a, 20b .  . . 20n based on messages 24 communicated by the devices 22.  For\nexample, in some implementations, one or more of the anomaly detectors 74a, 74b .  . . 74n generates a characteristic indicator for the device type 20a based on messages 24a communicated by the microwave 22a, the washing machine 22b and/or the\nrefrigerator 22c.  In some implementations, one or more of the anomaly detectors 74a, 74b .  . . 74n generates a characteristic indicator for the device type 20b based on messages 24b communicated by the automobile 22d and/or the automobile 22e.  In some\nimplementations, one or more of the anomaly detectors 74a, 74b .  . . 74n generates a characteristic indicator for the device type 20n based on messages 24n communicated by the laptop 22f and/or the smartphone 22g.  In some implementations, the\ncharacteristic indicators define thresholds for mobility, data throughput, types of messages communicated and/or number/frequency of messages communicated.\nIn various implementations, one or more of the anomaly detectors 74a, 74b .  . . 74n generates the corresponding performance indicators for the devices 22 based on the messages 24 communicated by the devices 22.  In some implementations, the\nperformance indicators indicate respective performances of the devices 22.  In some implementations, the performance indicators indicate the mobility of the devices 22.  In some implementations, the performance indicators indicate the data throughput of\nthe devices 22.  In some implementations, the performance indicators indicate the messages (e.g., a number of messages and/or a type of messages) communicated by the devices 22.\nIn various implementations, one or more of the anomaly detectors 74a, 74b .  . . 74n synthesize an anomaly indicator based on the characteristic indicator(s) and the performance indicator(s).  In some implementations, one or more of the anomaly\ndetectors 74a, 74b .  . . 74n synthesize the anomaly indicator in response to the performance indicator(s) breaching the characteristic indicator(s).  For example, in some implementations, one or more of the anomaly detectors 74a, 74b .  . . 74n\nsynthesize the anomaly indicator in response to a difference between the performance indicator(s) and the characteristic indicator(s) being greater than a threshold.  More generally, in various implementations, one or more of the anomaly detectors 74a,\n74b .  . . 74n identify an anomaly based on the characteristic indicator(s) and the performance indicator(s), and synthesize the anomaly indicator in response to identifying the anomaly.\nIn some implementations, one or more of the anomaly detectors 74a, 74b .  . . 74n transmit the anomaly indicator to one of the devices 22 whose performance indicator breaches the characteristic indicator of the device type.  In some\nimplementations, one or more of the anomaly detectors 74a, 74b .  . . 74n transmit the anomaly indicator to a network administration device in order to identify one of the devices 22 whose performance indicator breaches the characteristic indicator of\nthe device type.  In some implementations, one or more of the anomaly detectors 74a, 74b .  . . 74n transmit a command that limits an operation of at least one of the devices 22 whose performance indicator breaches the characteristic indicator of the\ndevice type (e.g., by deactivating the device, powering-down the device and/or quarantining the device).\nIn various implementations, the devices 22 includes transmitters and/or receivers (e.g., transceivers) that allow the devices 22 to communicate with the cellular network 70.  In some implementations, the devices 22 communicate with the cellular\nnetwork 70 periodically (e.g., once a day, once a month, etc.).  In some implementations, the devices 22 utilize a low power mode to communicate with the cellular network 70 (e.g., in order to conserve power).  In various implementations, the messages 24\ncommunicated between the devices 22 and the cellular network 70 include network layer messages.  For example, in some implementations, the messages 24 communicated between the devices 22 and the cellular network 70 include non-access stratum (NAS)\nmessages (e.g., update messages, attach messages, authentication messages, service requests, etc.).  In some implementations, the messages 24 communicated between the devices 22 and the cellular network 70 include radio resource control (RRC) messages\n(e.g., messages related to connection establishment, connection release, broadcast of system information, radio bearer establishment, reconfiguration, etc.).  In some implementations, at least a portion of the messages 24 are transported by a protocol\nthat operates below the Internet Protocol (IP) layer thereby circumventing security protections available at the IP layer.  In various implementations, the methods, devices and/or systems discussed herein for anomaly detection operate at a layer below\nthe IP layer (e.g., at the network layer) thereby providing security protection from the portion of messages 24 that are transported by protocols operating below the IP layer.\nIn various implementations, one or more of the anomaly detectors 74a, 74b .  . . 74n detect anomalous NAS messages and/or anomalous RRC messages communicated by the devices 22.  In some implementations, detecting anomalous NAS messages and/or\nanomalous RRC messages allows the anomaly detector(s) 74a, 74b .  . . 74n to prevent and/or thwart security attacks on the cellular network 70.  For example, in some implementations, in response to detecting anomalous NAS messages and/or anomalous RRC\nmessages being communicated by a particular device 22, the anomaly detector(s) 74a, 74b .  . . 74n limit an operation of that particular device 22 to reduce (e.g., stop) the anomalous NAS/RRC messages.  In various implementations, reducing the anomalous\nNAS/RRC messages conserves resources of the cellular network 70 that would otherwise be utilized in processing the anomalous NAS/RRC messages.  In various implementations, the anomaly detector(s) 74a, 74b .  . . 74n improve the operability of the\ncellular network 70 by detecting anomalous messages and forgoing processing of the anomalous messages thereby conserving resources (e.g., processing resources of the cellular network 70) and improving the efficiency of the cellular network 70.  In some\nimplementations, anomalous messages interrupt the operation of the cellular network 70.  As such, detecting anomalies and reducing the anomalous messages improves the operation of the cellular network 70 by increasing availability of the cellular network\n70.\nFIG. 2 illustrates a block diagram of an anomaly detector 200 (e.g., the anomaly detectors 74a, 74b .  . . and/or 74n).  In some implementations, the anomaly detector 200 includes a characteristic determiner 210, a datastore 220, a performance\nanalyzer 230, and an anomaly synthesizer 240.  Briefly, in various implementations, the characteristic determiner 210 determines (e.g., generates) corresponding characteristic indicators 212a, 212b .  . . 212n for the device types 20a, 20b .  . . 20n\nbased on the messages 24a, 24b .  . . 24n, the performance analyzer 230 generates a performance indicator 232 that indicates the performance of at least one of the devices 22, and the anomaly synthesizer 240 synthesizes an anomaly indicator 242 based on\nthe performance indicator 232 and the characteristic indicators 212a, 212b .  . . 212n.\nIn various implementations, the characteristic determiner 210 determines corresponding characteristic indicators 212a, 212b .  . . 212n for the device types 20a, 20b .  . . 20n based on the messages 24a, 24b .  . . 24n, respectively.  For\nexample, in some implementations, the characteristic determiner 210 determines the characteristic indicator 212a for the device type 24a based on the messages 24a communicated by at least one of the devices of the device type 24a.  For example, the\ncharacteristic determiner 210 determines the characteristic indicator 212a for the device type 24a based on the messages 24a communicated by the microwave 22a, the washing machine 22b and/or the refrigerator 22c.  Similarly, in some implementations, the\ncharacteristic determiner 210 determines the characteristic indicator 212b for the device type 20b based on the messages 24b communicated by at least one of the devices of the device type 20b.  For example, the characteristic determiner 210 determines\nthe characteristic indicator 212b for the device type 20b based on the messages 24b communicated by the automobile 22d and/or the automobile 22e.  Similarly, in some implementations, the characteristic determiner 210 determines the characteristic\nindicator 212n for the device type 20n based on the messages 24n communicated by at least one of the devices of the device type 20n.  For example, the characteristic determiner 210 determines the characteristic indicator 212n for the device type 20n\nbased on the messages 24n communicated by the laptop 22f and/or the smartphone 22g.\nIn various implementations, the characteristic indicators 212a, 212b .  . . 212n include alphanumeric values.  For example, in some implementations, the characteristic indicators 212a, 212b .  . . 212n include numerical values.  In some\nimplementations, the characteristic indicators 212a, 212b .  . . 212n include labels.  In some implementations, the characteristic indicators 212a, 212b .  . . 212n include flags (e.g., binary values including 1 and 0).\nIn some implementations, the characteristic determiner 210 determines the corresponding characteristic indicators 212a, 212b .  . . 212n for the device types 20a, 20b .  . . 20n based on a function of individual characteristic indicators of\ndevices in the device types 20a, 20b .  . . 20n.  For example, in some implementations, the characteristic determiner 210 determines the characteristic indicator 212a for the device type 20a based on a function of individual characteristic indicators of\ndevices of the device type 20a.  As an example, the characteristic determiner 210 determines the characteristic indicator 212a for the device type 20a based on a function of a characteristic indicator of the microwave 22a, a characteristic indicator of\nthe washing machine 22b, and a characteristic indicator of the refrigerator 22c.  In some implementations, the characteristic indicator 212a for the device type 20a is an average of the characteristic indicator of the microwave 22a, the characteristic\nindicator of the washing machine 22b, and the characteristic indicator of the refrigerator 22c.  In some implementations, the characteristic indicator 212a for the device type 20a is a mean of the characteristic indicator of the microwave 22a, the\ncharacteristic indicator of the washing machine 22b, and the characteristic indicator of the refrigerator 22c.\nIn various implementations, the characteristic indicators 212a, 212b .  . . 212n indicate a target operation (e.g., an expected operation) for devices of the device types 20a, 20b .  . . 20n, respectively.  In various implementations, the\ncharacteristic indicators 212a, 212b .  . . 212n indicate a target performance (e.g., an expected performance) for devices of the device types 20a, 20b .  . . 20n, respectively.  In various implementations, the characteristic indicators 212a, 212b .  . .\n212n indicate a target number of communications and/or a target type of communications (e.g., an expected number of communications and/or an expected type of communications) for devices of the device types 20a, 20b .  . . 20n, respectively.\nIn some implementations, the characteristic indicators 212a, 212b .  . . 212n indicate target mobility levels (e.g., mobility thresholds) for devices of the device types 20a, 20b .  . . 20n, respectively.  For example, in some implementations,\nthe characteristic indicators 212a, 212b .  . . 212n indicate how much the devices of the device types 20a, 20b .  . . 20n, respectively, are expected to move.  In some implementations, the target mobility levels defined by the characteristic indicators\n212a, 212b .  . . 212n are based on a function of (e.g., an average of, or a mean of) individual characteristic indicators of devices of the device types 20a, 20b .  . . 20n, respectively.  For example, in some implementations, the target mobility level\ndefined by the characteristic indicator 212a for the device type 20a is a function of (e.g., an average of, or a mean of) mobility levels of the devices of the device type 20a (e.g., the microwave 22a, the washing machine 22b and the refrigerator 22c\nshown in FIG. 1).  In the example of FIG. 1, the device type 20a includes appliances that are typically immobile.  As such, in some implementations, the characteristic indicator 212a indicates that the target mobility level for the device type 20a is no\nmobility or low mobility (e.g., exactly zero, or approximately zero).\nIn some implementations, the characteristic indicators 212a, 212b .  . . 212n indicate target data throughputs (e.g., data throughput thresholds) for devices of the device types 20a, 20b .  . . 20n, respectively.  For example, in some\nimplementations, the characteristic indicators 212a, 212b .  . . 212n indicate how much data the devices of the device types 20a, 20b .  . . 20n, respectively, are expected to receive/transmit.  In some implementations, the target data throughputs\ndefined by the characteristic indicators 212a, 212b .  . . 212n are based on a function of (e.g., an average of, or a mean of) individual data throughputs of devices of the device types 20a, 20b .  . . 20n, respectively.  For example, in some\nimplementations, the target data throughput defined by the characteristic indicator 212a for the device type 20a is a function of (e.g., an average of, or a mean of) data throughputs of the devices of the device type 20a (e.g., the microwave 22a, the\nwashing machine 22b and the refrigerator 22c shown in FIG. 1).  In the example of FIG. 1, the device type 20a includes appliances that typically do not receive/transmit a significant amount of data.  As such, in some implementations, the characteristic\nindicator 212a indicates that the target data throughput for the device type 20a is relatively low (e.g., less than 1 MB/day, less than 3 MB/week, less than 10 MB/month, etc.).\nIn some implementations, the characteristic indicators 212a, 212b .  . . 212n indicate a target communication type (e.g., an expected communication type) for devices of the device types 20a, 20b .  . . 20n, respectively.  In some\nimplementations, the target communication type indicates types of communications that are not permitted/expected.  For example, in some implementations, the characteristic indicators 212a, 212b .  . . 212n indicate a type of messages that devices of the\ndevice types 20a, 20b .  . . 20n, respectively, are expected to receive/transmit.  In some implementations, the target communication type defined by the characteristic indicators 212a, 212b .  . . 212n is based on individual communication types of\ndevices of the device types 20a, 20b .  . . 20n, respectively.  For example, in some implementations, the target communication type defined by the characteristic indicator 212a for the device type 20a includes types of messages that are communicated by\nat least a portion of the devices of the device type 20a (e.g., the microwave 22a, the washing machine 22b and/or the refrigerator 22c shown in FIG. 1).  In the example of FIG. 1, the device type 20a includes appliances that typically do not communicate\nmessages related to cell handover.  As such, in some implementations, the characteristic indicator 212a indicates that the target communication type for the device type 20a excludes cell handover messages.\nIn some implementations, the characteristic indicators 212a, 212b .  . . 212n indicate a target number/frequency of communications (e.g., communication number/frequency threshold) for devices of the device types 20a, 20b .  . . 20n,\nrespectively.  For example, in some implementations, the characteristic indicators 212a, 212b .  . . 212n indicate a number/frequency of messages that devices of the device types 20a, 20b .  . . 20n, respectively, are expected to receive/transmit.  In\nsome implementations, the target number/frequency of communications defined by the characteristic indicators 212a, 212b .  . . 212n is based on individual number/frequency of communications by devices of the device types 20a, 20b .  . . 20n,\nrespectively.  For example, in some implementations, the target number/frequency of communications defined by the characteristic indicator 212a for the device type 20a is a function of (e.g., an average of, or a mean of) the number/frequency of\ncommunications by at least a portion of the devices of the device type 20a (e.g., the microwave 22a, the washing machine 22b and/or the refrigerator 22c shown in FIG. 1).  In the example of FIG. 1, the device type 20a includes appliances that typically\ndo not communicate messages often.  As such, in some implementations, the characteristic indicator 212a indicates that the target number/frequency of communications for the device type 20a is less than one message per hour, one message per day, ten\nmessages per month, etc.\nIn some implementations, the characteristic indicators 212a, 212b .  . . 212n indicate a target direction of communication (e.g., an expected direction of communication) for devices of the device types 20a, 20b .  . . 20n, respectively.  In some\nimplementations, the target direction of communication indicates whether a majority of the communications (e.g., all the communications) are incoming (e.g., messages received) or outgoing (e.g., messages transmitted).  For example, in some\nimplementations, the characteristic indicators 212a, 212b .  . . 212n indicate whether the devices of the device types 20a, 20b .  . . 20n, respectively, are expected to receive or transmit data (e.g., messages).  In some implementations, the target\ndirection of communication defined by the characteristic indicators 212a, 212b .  . . 212n is based on individual directions of communication of devices of the device types 20a, 20b .  . . 20n, respectively.  For example, in some implementations, the\ntarget direction of communication defined by the characteristic indicator 212a for the device type 20a is based on the directions of communication of at least a portion of the devices of the device type 20a (e.g., the microwave 22a, the washing machine\n22b and/or the refrigerator 22c shown in FIG. 1).  In some implementations, the target direction of communication includes a ratio of incoming and outgoing data.  In the example of FIG. 1, the device type 20a includes appliances that typically do not\nrequest significant amounts of data.  As such, in some implementations, the characteristic indicator 212a indicates that the target direction of communication for the device type 20a is predominantly outgoing (e.g., 90% outgoing and 10% incoming).\nIn various implementations, the characteristic determiner 210 generates the characteristic indicators 212a, 212b .  . . 212n periodically (e.g., once a day, once a week, once a month, etc.).  In some implementations, the characteristic\ndeterminer 210 generates the characteristic indicators 212a, 212b .  . . 212n in response to a threshold change in the number/percentage of devices 22.  For example, in some implementations, the characteristic determiner 210 generates the characteristic\nindicator 212a for the device type 20a in response to a threshold number/percentage of new devices of device type 20a being initialized.  In some implementations, the characteristic determiner 210 generates the characteristic indicator 212a for the\ndevice type 20a in response to a threshold number/percentage of existing devices of device type 20a being deactivated/decommissioned.\nIn some implementations, the characteristic determiner 210 updates the characteristic indicators 212a, 212b .  . . 212n after generating the characteristic indicators 212a, 212b .  . . 212n.  For example, in some implementations, the\ncharacteristic determiner 210 updates the characteristic indicators 212a, 212b .  . . 212n based on additional messages communicated by the devices 22.\nIn various implementations, the characteristic determiner 210 utilizes devices, methods and/or systems associated with machine learning to generate the characteristic indicators 212a, 212b .  . . 212n.  For example, in some implementations, the\ncharacteristic determiner 210 utilizes a learning agent to generate the characteristic indicators 212a, 212b .  . . 212n.  In various implementations, the characteristic determiner 210 generates the characteristic indicators 212a, 212b .  . . 212n\nautomatically (e.g., without human intervention, for example, without a sequence of user inputs).\nIn various implementations, the characteristic determiner 210 stores the characteristic indicators 212a, 212b .  . . 212n in the datastore 220.\nIn various implementations, the performance analyzer 230 generates a performance indicator 232 that indicates a performance of at least one of the devices 22.  In the example of FIG. 2, the performance indicator 232 indicates a performance of\nthe refrigerator 22c.  As shown in FIG. 2, the performance indicator 232 for the refrigerator 22c is based on messages 24ac communicated (e.g., transmitted and/or received) by the refrigerator 22c.\nIn various implementations, the performance indicator 232 indicates a mobility level of the refrigerator 22c.  In some implementations, the performance indicator 232 indicates a data throughput of the refrigerator 22c.  For example, in some\nimplementations, the performance indicator 232 indicates an amount of data that the refrigerator 22c is receiving/transmitting.  In some implementations, the performance indicator 232 indicates a type of communication associated with the refrigerator\n22c.  For example, in some implementations, the performance indicator 232 indicates whether the refrigerator 22c is communicating messages related to cell handovers.  In some implementations, the performance indicator 232 indicates a number/frequency of\ncommunications associated with the refrigerator 22c.  For example, in some implementations, the performance indicator 232 indicates a number of messages being transmitted/received by the refrigerator 22c.  In some implementations, the performance\nindicator 232 indicates a direction of communication associated with the refrigerator 22c.  For example, in some implementations, the performance indicator 232 indicates whether the refrigerator 22c is predominantly receiving data or transmitting data. \nIn some implementations, the performance indicator 232 indicates a ratio of messages being transmitted and received by the refrigerator 22c.  In various implementations, the performance analyzer 230 provides the performance indicator 232 to the anomaly\nsynthesizer 240.\nIn various implementations, the anomaly synthesizer 240 synthesizes an anomaly indicator 242 based on the performance indicator 232 and a corresponding one of the characteristic indicators 212a, 212b .  . . 212n.  In the example of FIG. 2, since\nthe refrigerator 22c is of device type 20a, the anomaly synthesizer 240 retrieves the characteristic indicator 212a for the device type 20a.  As illustrated in FIG. 2, the anomaly synthesizer 240 synthesizes the anomaly indicator 242 based on the\nperformance indicator 232 and the characteristic indicator 212a.  In various implementations, the anomaly indicator 242 indicates that there is an anomaly at one of the devices 22 (e.g., at the refrigerator 22c).\nIn various implementations, the anomaly synthesizer 240 synthesizes the anomaly indicator 242 in response to the performance indicator 232 breaching a function of the characteristic indicator 212a.  For example, in some implementations, the\nanomaly synthesizer 240 synthesizes the anomaly indicator 242 in response to a difference between the performance indicator 232 and the characteristic indicator 212a being greater than a threshold.  In some implementations, the anomaly synthesizer 240\nsynthesizes the anomaly indicator 242 in response to the performance indicator 232 being greater than the characteristic indicator 212a.\nIn some implementations, the anomaly synthesizer 240 synthesizes the anomaly indicator 242 in response to a mobility level indicated by the performance indicator 232 breaching (e.g., being greater than) the target mobility level (e.g., the\nmobility threshold) defined by the characteristic indicator 212a.  In some implementations, the anomaly synthesizer 240 synthesizes the anomaly indicator 242 in response to a data throughput indicated by the performance indicator 232 breaching (e.g.,\nbeing greater than) the target data throughput (e.g., the data throughput threshold) defined by the characteristic indicator 212a.  In some implementations, the anomaly synthesizer 240 synthesizes the anomaly indicator 242 in response to a communication\ntype indicated by the performance indicator 232 breaching (e.g., being different from) the target communication type(s) defined by the characteristic indicator 212a.  In some implementations, the anomaly synthesizer 240 synthesizes the anomaly indicator\n242 in response to a number/frequency of communications indicated by the performance indicator 232 breaching (e.g., exceeding) the target number/frequency of communications (e.g., communication number/frequency threshold) defined by the characteristic\nindicator 212a.  In some implementations, the anomaly synthesizer 240 synthesizes the anomaly indicator 242 in response to a direction of communication indicated by the performance indicator 232 breaching (e.g., being different from) the target direction\nof communication defined by the characteristic indicator 212a.  In various implementations, the anomaly synthesizer 240 synthesizes the anomaly indicator 242 in response to a device operation indicated by the performance indicator 232 breaching (e.g.,\nbeing different from) a target operation (e.g., a threshold operation) defined by the characteristic indicator 212a.\nIn some implementations, the anomaly detector 200 (e.g., the anomaly synthesizer 240) transmits the anomaly indicator 242 to one of the devices 22 (e.g., to the refrigerator 22c).  In some implementations, the anomaly detector 200 transmits the\nanomaly indicator 242 to an operating entity that controls operation of at least a portion of the cellular network 70.  In some implementations, the anomaly detector 200 transmits the anomaly indicator 242 to a network/system administration device (e.g.,\na network/system administration computer that controls operation of the devices 22).\nAs illustrated in FIG. 2, in some implementations, the anomaly detector 200 (e.g., the anomaly synthesizer 240) generates and transmits a command 244 to the refrigerator 22c.  In some implementations, the command 244 limits an operation of the\nrefrigerator 22c.  For example, in some implementations, the command 244 deactivates the refrigerator 22c, so that the refrigerator 22c is unable to communicate with the cellular network 70.  In some implementations, the command 244 quarantines the\nrefrigerator 22c, so that communications transmitted by the refrigerator 22c are not processed by the cellular network 70.  In some implementations, the command 244 shuts down (e.g., powers-down) the refrigerator 22c.  In some implementations, the\ncommand 244 causes the refrigerator 22c to download and execute computer-readable instructions corresponding to a security patch.  More generally, in various implementations, the anomaly detector 200 (e.g., the anomaly synthesizer 240) generates and\ntransmits the command 244 in order to limit an operation of one or more of the devices 22 that are associated with the performance indicator 232 in response to the performance indicator 232 breaching a corresponding one of the characteristic indicators\n212a, 212b .  . . 212n.\nFIG. 3 is a sequence diagram illustrating various RRC and NAS messages 34 being exchanged in accordance with some implementations.  In some implementations, the anomaly detector 200 (e.g., characteristic determiner 210) utilizes one or more of\nthe RRC and NAS messages 34 to determine the characteristic indicators 212a, 212b .  . . 212n.  In the example of FIG. 3, the RRC and NAS messages 34 corresponds to a RRC exchange.  As illustrated in FIG. 3, in some implementations, the RRC and NAS\nmessages 34 include messages related to: RRC connection establishment 302; NAS attach request and/or packet data network (PDN) connectivity request 304; NAS identity, authentication, security mode command (SMC) procedures 306; update location request\n308; update location response 310; create session request 312; create session response 314; initial context setup request 316; AS security mode command 318; AS security mode complete 320; RRC connection reconfiguration 322; RRC reconfiguration complete\n324; initial context setup response 326; modify bearer request 328; modify bearer response 330; NAS attach acc and/or act def bearer context request 332; and NAS attach complete and/or act def bearer context acc 334.\nIn the example of FIG. 3, some of the RRC and NAS messages 34 include messages between the device 22 and one of the cellular network nodes 72a (e.g., eNB), 72b (e.g., MME), 72c (e.g., home subscriber base (HSB), 72d (e.g., serving gateway (SGW))\nand 72e (e.g., packet gateway (PGW)), while other RRC and NAS messages 34 includes messages exchanged between the cellular network nodes 72a, 72b, 72c, 72d and 72e.\nFIG. 4 is a sequence diagram illustrating non-access stratum (NAS) messages 44 being exchanged in accordance with some implementations.  In some implementations, the anomaly detector 200 (e.g., characteristic determiner 210) utilizes one or more\nof the NAS messages 44 to determine the characteristic indicators 212a, 212b .  . . 212n.  In the example of FIG. 4, the NAS messages 44 corresponds to a NAS exchange.  As illustrated in FIG. 4, in some implementations, the NAS messages 44 include\nmessages related to: NAS identity request 402; NAS identity response 404; NAS authentication request 406; NAS authentication response 408; NAS security mode command 410; and NAS security mode complete 412.\nIn the example of FIG. 4, some of the NAS messages 44 include messages between the device 22 and the cellular network node 72b (e.g., MME).\nFIG. 5 is a flowchart representation of a method 500 of identifying anomalies in accordance with some implementations.  In various implementations, the method 500 is implemented as a set of computer readable instructions that are executed at an\nanomaly detector (e.g., the anomaly detector 200 shown in FIG. 2, and/or the anomaly detectors 74a, 74b .  . . 74n shown in FIG. 1).  Briefly, the method 500 includes generating a characteristic indicator that characterizes a device type based on\ncommunications associated with a first device of the device type, determining a performance indicator that indicates a performance of the first device, and synthesizing an anomaly indicator based on the performance indicator and the characteristic\nindicator.\nAs represented by block 510, in various implementations, the method 500 includes generating a characteristic indicator that characterizes a device type based on communications associated with a first device of the device type (e.g., the\ncharacteristic indicators 212a, 212b .  . . 212n for device types 20a, 20b .  . . 20n, respectively).  As represented by block 510a, in some implementations, the method 500 includes generating the characteristic indicator based on individual\ncharacteristic indicators of respective devices in the device type (e.g., generating the characteristic indicator 212a based on individual characteristic indicators of the microwave 22a, the washing machine 22b and the refrigerator 22c shown in FIG. 1). \nAs represented by block 510b, in some implementations, the method 500 includes generating the characteristic indicator based on NAS messages and/or RRC messages communication by the first device (e.g., as shown in FIGS. 3 and 4).  As represented by block\n510c, in some implementations, the method 500 includes generating a characteristic indicator that defines targets/thresholds for mobility, data throughput, types of communication, number/frequency of communications, and/or direction of communication.  In\nsome implementations, the method 500 includes updating the characteristic indicator based on additional messages communicated by the first device.  As described herein, in various implementations, the method 500 includes generating the characteristic\nindicator by utilizing methods, devices and/or systems associated with machine learning.  For example, in some implementations, the method 500 includes utilizing a learning agent (e.g., a distributed learning agent) to generate the characteristic\nindicator.\nAs represented by block 520, in various implementations, the method 500 includes determining a performance indicator (e.g., the performance indicator 232 shown in FIG. 2) that indicates a performance of the first device.  In some\nimplementations, the method 500 includes determining the performance indicator based on communications associated with the first device.  For example, in some implementations, the method 500 includes determining the performance indicator based on\nmessages transmitted/received by the first device.  As represented by block 520a, in some implementations, the method 500 includes determining the performance indicator based on NAS/RRC messages communicated by the first device (e.g., the NAS/RRC\nmessages shown in FIGS. 3 and 4).  As represented by block 520b, in some implementations, the method 500 includes determining a performance indicator that indicates a mobility level, a data throughput, types of communications, number/frequency of\ncommunications, direction of communication associated with the first device.\nAs represented by block 530, in various implementations, the method 500 includes synthesizing an anomaly indicator (e.g., the anomaly indicator 242 shown in FIG. 2) based on the performance indicator and the characteristic indicator.  As\nrepresented by block 530a, in some implementations, the method 500 includes synthesizing the anomaly indicator in response to the performance indicator breaching a function of the characteristic indicator.  For example, in some implementations, the\nmethod 500 includes synthesizing the anomaly indicator in response to the performance indicator being greater than the characteristic indicator.  In some implementations, the method 500 includes synthesizing the anomaly indicator in response to a\ndifference between the performance indicator and the characteristic indicator being greater than a threshold.  As represented by block 530b, in some implementations, the method 500 includes transmitting the anomaly indicator.  In some implementations,\nthe method 500 includes transmitting the anomaly indicator to a network/system administration device (e.g., to an operator entity that controls at least a portion of the cellular network).  As represented by block 530c, in some implementations, the\nmethod 500 includes transmitting a command (e.g., the command 244) to the first device.  In some implementations, the command limits an operation of the first device (e.g., by deactivating/quarantining/shutting off the first device).\nFIG. 6 is a block diagram of a server system 600 enabled with one or more components of an anomaly detector (e.g., the anomaly detector 200 shown in FIG. 2 and/or the anomaly detectors 74a, 74b .  . . 74n shown in FIG. 1) in accordance with some\nimplementations.  While certain specific features are illustrated, those of ordinary skill in the art will appreciate from the present disclosure that various other features have not been illustrated for the sake of brevity, and so as not to obscure more\npertinent aspects of the implementations disclosed herein.  To that end, as a non-limiting example, in some implementations the server system 600 includes one or more processing units (CPUs) 601, a network interface 602, a programming interface 603, a\nmemory 604, and one or more communication buses 605 for interconnecting these and various other components.\nIn some implementations, the network interface 602 is provided to, among other uses, establish and maintain a metadata tunnel between a cloud hosted network management system and at least one private network including one or more compliant\ndevices.  In some implementations, the communication buses 605 include circuitry that interconnects and controls communications between system components.  The memory 604 includes high-speed random access memory, such as DRAM, SRAM, DDR RAM or other\nrandom access solid state memory devices; and may include non-volatile memory, such as one or more magnetic disk storage devices, optical disk storage devices, flash memory devices, or other non-volatile solid state storage devices.  The memory 604\noptionally includes one or more storage devices remotely located from the CPU(s) 601.  The memory 604 comprises a non-transitory computer readable storage medium.\nIn some implementations, the memory 604 or the non-transitory computer readable storage medium of the memory 604 stores the following programs, modules and data structures, or a subset thereof including an optional operating system 606, a\ncharacteristic generating module 608, a performance analysis module 610, an anomaly synthesis module 612, and the datastore 220.  In various implementations, the characteristic generating module 608, the performance analysis module 610 and the anomaly\nsynthesis module 612 perform substantially the same operations as the characteristic determiner 210, the performance analyzer 230 and the anomaly synthesizer 240, respectively, shown in FIG. 2.  For example, in various implementations, the characteristic\ngenerating module 608 generates characteristic indicators for corresponding device types (e.g., the characteristic indicators 212a, 212b .  . . 212n for the device types 20a, 20b .  . . 20n, respectively).  To that end, in various implementations, the\ncharacteristic generating module 608 includes instructions and/or logic 608a, and heuristics and metadata 608b.  In some implementations, the characteristic generating module 608 stores the characteristic indicators 212a, 212b .  . . 212n in the\ndatastore 220.  In some implementations, the performance analysis module 610 determines a performance indicator that indicates a performance of at least one device (e.g., the performance indicator 232 shown in FIG. 2).  To that end, in various\nimplementations, the performance analysis module 610 includes instructions and/or logic 610a, and heuristics and metadata 610b.  In some implementations, the anomaly synthesis module 612 synthesizes an anomaly indicator (e.g., the anomaly indicator 242\nshown in FIG. 2) based on the performance indicator and the characteristic indicator.  To that end, in various implementations, the anomaly synthesis module 612 includes instructions and/or logic 612a, and heuristics and metadata 612b.\nWhile various aspects of implementations within the scope of the appended claims are described above, it should be apparent that the various features of implementations described above may be embodied in a wide variety of forms and that any\nspecific structure and/or function described above is merely illustrative.  Based on the present disclosure one skilled in the art should appreciate that an aspect described herein may be implemented independently of any other aspects and that two or\nmore of these aspects may be combined in various ways.  For example, an apparatus may be implemented and/or a method may be practiced using any number of the aspects set forth herein.  In addition, such an apparatus may be implemented and/or such a\nmethod may be practiced using other structure and/or functionality in addition to or other than one or more of the aspects set forth herein.\nIt will also be understood that, although the terms \"first,\" \"second,\" etc. may be used herein to describe various elements, these elements should not be limited by these terms.  These terms are only used to distinguish one element from another. For example, a first contact could be termed a second contact, and, similarly, a second contact could be termed a first contact, which changing the meaning of the description, so long as all occurrences of the \"first contact\" are renamed consistently and\nall occurrences of the second contact are renamed consistently.  The first contact and the second contact are both contacts, but they are not the same contact.\nThe terminology used herein is for the purpose of describing particular embodiments only and is not intended to be limiting of the claims.  As used in the description of the embodiments and the appended claims, the singular forms \"a\", \"an\" and\n\"the\" are intended to include the plural forms as well, unless the context clearly indicates otherwise.  It will also be understood that the term \"and/or\" as used herein refers to and encompasses any and all possible combinations of one or more of the\nassociated listed items.  It will be further understood that the terms \"comprises\" and/or \"comprising,\" when used in this specification, specify the presence of stated features, integers, steps, operations, elements, and/or components, but do not\npreclude the presence or addition of one or more other features, integers, steps, operations, elements, components, and/or groups thereof.\nAs used herein, the term \"if\" may be construed to mean \"when\" or \"upon\" or \"in response to determining\" or \"in accordance with a determination\" or \"in response to detecting,\" that a stated condition precedent is true, depending on the context. \nSimilarly, the phrase \"if it is determined [that a stated condition precedent is true]\" or \"if [a stated condition precedent is true]\" or \"when [a stated condition precedent is true]\" may be construed to mean \"upon determining\" or \"in response to\ndetermining\" or \"in accordance with a determination\" or \"upon detecting\" or \"in response to detecting\" that the stated condition precedent is true, depending on the context.", "application_number": "15795670", "abstract": " Various implementations disclosed herein enable identifying anomalies in\n     a network. For example, in various implementations, a method of\n     identifying anomalies in a network is performed by a network node. In\n     various implementations, the network node includes one or more\n     processors, and a non-transitory memory. In various implementations, the\n     method includes generating a characteristic indicator that characterizes\n     a device type based on communications associated with a first device of\n     the device type. In various implementations, the method includes\n     determining, based on communications associated with the first device, a\n     performance indicator that indicates a performance of the first device.\n     In various implementations, the method includes synthesizing an anomaly\n     indicator as a function of the performance indicator in relation to the\n     characteristic indicator.\n", "citations": ["9298494", "9369476", "9472084", "9705762", "20040025044", "20050027818", "20050185666", "20070143851", "20100197315", "20110125989", "20130276125", "20140244552", "20150163121", "20150201424", "20160036844", "20170063656", "20170099309", "20170155566", "20180359095"], "related": []}, {"id": "20190159125", "patent_code": "10375631", "patent_name": "State change enabled by a hierarchical class of a radio access network\n     device", "year": "2019", "inventor_and_country_data": " Inventors: \nSukumaran; Jayesh (Woodinville, WA), Orloff; David (Sammamish, WA), Reiger; Susan (Redmond, WA)  ", "description": "<BR><BR>TECHNICAL FIELD\nThe disclosed subject matter relates to enabling a changing of a state of a device based on a hierarchical class of the device, e.g., enabling changing to, or from, a lower power state for a radio access network (RAN) device based on a\nhierarchical class of the RAN device, for example wherein the device is a femtocell, picocell, other small cell, access point (AP), etc.\n<BR><BR>BACKGROUND\nA conventional heterogeneous radio access network (RAN) environment can encompasses conventional macro cells, small cells, e.g., femtocells, picocells, etc., and Wi-Fi access points.  These conventional heterogeneous RAN environments are\nbecoming increasingly common to provide mobile or wireless devices access to a network over a wireless link.  Small cells can be a common part of 4G networks and can be expected to similarly be common in the evolution of 5G networks.  Some estimates\npropose that heterogeneous networks may be composed of 85% small cells and 15% macro cells by as early as 2020.  Moreover, operation in the unlicensed spectral regions/bands is also becoming increasingly common in conventional heterogeneous networks. \nThis can suggest that, increasingly, heterogeneous network architectures, possibly operating in the unlicensed band, can place a high value on interference management.  Similarly, energy conservation and management can become increasingly important as\nmore and more small devices are deployed in in these heterogeneous networks.  As such, businesses can be expected to strive for efficient and be a responsible operation of network devices, e.g., RAN devices, small cells, access points, etc. <BR><BR>BRIEF\nDESCRIPTION OF DRAWINGS\nFIG. 1 is an illustration of an example system that can enable changing a device state based on a hierarchical class of the device, in accordance with aspects of the subject disclosure.\nFIG. 2 is an illustration of an example system that can facilitate changing a device state based on a hierarchical class of the device among a plurality of devices comprising the device, in accordance with aspects of the subject disclosure.\nFIG. 3 is an illustration of an example system that can facilitate changing a device state based on a hierarchical class of the device and use of another device in a network comprising the device and the other device, in accordance with aspects\nof the subject disclosure.\nFIG. 4 is an illustrates an example system that can facilitate changing a device state based on a hierarchical class of the device and neighboring device(s) state(s), in accordance with aspects of the subject disclosure.\nFIG. 5 is an illustration of an example system that can facilitate changing a device state based on a hierarchical class of the device and received data, in accordance with aspects of the subject disclosure.\nFIG. 6 illustrates a flowchart depicting adapting a state of a device based on a hierarchical class of the device, a state of a neighboring device, and a current state of the device, in accordance with aspects of the subject disclosure.\nFIG. 7 illustrates an example method facilitating changing a device state based on a hierarchical class of the device, in accordance with aspects of the subject disclosure.\nFIG. 8 illustrates an example method enabling time-delayed changing of a device state based on a hierarchical class of the device, in accordance with aspects of the subject disclosure.\nFIG. 9 depicts an example schematic block diagram of a computing environment with which the disclosed subject matter can interact.\nFIG. 10 illustrates an example block diagram of a computing system operable to execute the disclosed systems and methods in accordance with an embodiment.\n<BR><BR>DETAILED DESCRIPTION\nThe subject disclosure is now described with reference to the drawings, wherein like reference numerals are used to refer to like elements throughout.  In the following description, for purposes of explanation, numerous specific details are set\nforth in order to provide a thorough understanding of the subject disclosure.  It may be evident, however, that the subject disclosure may be practiced without these specific details.  In other instances, well-known structures and devices are shown in\nblock diagram form in order to facilitate describing the subject disclosure.\nModern radio access network (RANs) can comprise heterogeneous types of RAN equipment in the RAN environment.  These heterogeneous RAN devices can encompasses conventional macro cells, small cells, e.g., femtocells, picocells, etc., Wi-Fi access\npoints (Wi-Fi APs), millimeter-wave devices, etc. Conventional heterogeneous RAN environments are becoming increasingly common to provide mobile or wireless devices access to a network over a wireless link.  Small cells can be a common part of 4G\nnetworks and can be expected to similarly be common in the evolution of 5G networks.  Moreover, operation in the unlicensed spectral regions/bands is also becoming increasingly common in conventional heterogeneous networks.  It can therefore be expected\nthat efficient power and efficient spectral operation of network devices is desirable.  Generally, in the context of this disclosure, the term `RAN device(s)`, or similar terms, can refer to non-macro cell devices, e.g., small cells, Wi-Fi APs, other\nAPs, etc., such as those associated with providing enterprise level service(s), etc., and accordingly macro cell RAN devices will typically be referred to as `macro cell(s),` `macro RAN device(s)`, or similar terms to distinguish them from non-macro cell\nRAN devices.\nConventional RAN devices, e.g., deployed in enterprise environments, etc., can remain powered on and transmit wireless signals continuously, 24 hours a day, even if there are no users in the building (e.g., after hours, holidays and on\nweekends).  This can needlessly waste resources and can generate spectrum interference, e.g., interfering with nearby RAN devices and/or nearby macro RAN devices, because walls can fail to attenuate interfering radio frequency (RF) signals sufficiently. \nAs such, there can be noteworthy leakage of RF signals from buildings that can interfere with other RAN and/or macro RAN devices.  Devices that generate interfering RF signals can be termed `interferes`.  Moreover, these same interferers can also consume\npower even where they are not needed to support current UE traffic.\nA modern building can have multiple floors and each floor can have multiple RAN devices to provide wireless coverage, e.g., wireless coverage to indoor spaces, nearby outdoor spaces affiliated with the building, etc., hereinafter termed `indoor\nspace(s)` or similar terms, for clarity and brevity.  Mobile device users enjoy seamless wireless service transitions between outdoor spaces and indoor spaces and to enable mobility from an outdoor space, associated with a macro RAN device, to an indoor\nspace, associated with a RAN device network, certain RAN devices in a building can be designated as \"gateway\" RAN device(s).  A gateway RAN device can be designated as a RAN device located near an entrance point of a building.  The gateway RAN device can\ntherefore be viewed as a RAN device that can have a neighbor relation to a macro RAN device to facilitate transitioning a mobile device from the macro service to the RAN service of the building.  Moreover, other RAN devices of the building that are not\ndesignated as gateway RAN devices can be termed field RAN devices.  Field RAN devices generally will not have a neighbor relation to a macro RAN device.  Field RAN devices can, however, have a neighbor relationship with other RAN devices, e.g., a gateway\nRAN device, another field RAN device, etc. These neighbor relationships between RAN devices can be created, maintained, destroyed, modified, etc., manually and/or autonomously, based on various methods devised to neighbor relationships, e.g.,\nself-organizing behavior, self-optimizing behavior, etc.\nIn an aspect, determining a hierarchical class of a RAN device can enable adaptation of a state of the device.  In an embodiment, RAN devices can be classed as gateway RAN devices, field RAN devices, and intermediate RAN devices.  A gateway RAN\ndevice can be affiliated with an entry point to a network, e.g., an entry to a building, an entry to a park, an entry to a mine, etc. A gateway RAN device can typically have a neighbor relation to a macro RAN device to enable transition of a user\nequipment from a macro network to a network attached to the macro network via the gateway RAN device.  An intermediate RAN device can have a neighbor relationship with a gateway RAN device and a field RAN device.  A field RAN device can have neighbor\nrelations with other field RAN device(s) or an intermediate RAN device, but will not have a neighbor relation with a gateway RAN device or a macro RAN device, e.g., a field device that has a neighbor relation with a gateway RAN device is typically an\nintermediate RAN device, a field device that has a neighbor relation to a macro RAN device is typically a gateway RAN device.  It will be noted that a gateway RAN device, an intermediate RAN device, and/or a field RAN device can be identical devices that\nhave different neighbor relations, can be different devices, e.g., different types of device, differently configured same devices, etc., that have different neighbor relations.  In an aspect, a gateway RAN device can be a first hierarchical class, an\nintermediate RAN device can be a second hierarchical class, and a field RAN device can be a third hierarchical class, wherein the classes differ based on neighbor relationships and corresponding differences in functionality, rather than explicitly by the\ntype or configuration of a device.  As an example, a first RAN device can be a field RAN device that can be moved to a building entry point and can be designated as a gateway RAN device allowing the first device to have a neighbor relationship with a\nmacro RAN device.  As a second example, a gateway RAN device can be re-designated as a field RAN device allowing only neighbor relationships with other field RAN devices and intermediate RAN devices.\nThe use of hierarchical RAN device classes can enable state transitions of RAN devices.  These state transitions can in some embodiments reduce the likelihood that a RAN device is an interferer, e.g., the RAN device can be an interferer less\noften, etc. These state transitions can in some embodiments reduce the power consumed by RAN devices.  As an example, a state transition can be from a full power state, e.g., receive/transmit at full strength, etc., to a sleep state, e.g., reduced\ntransmission of RF signals, etc., to an off state, e.g., the RAN device powers off and draws no power.  In this example, power consumption is reduced and the RAN device can be less of an interferer.\nIn an aspect, state transitions can be controlled by the RAN device itself, can be controlled by a local controller communicatively coupled to the RAN device, can be centrally controlled by a remotely located controller via a macro communication\nnetwork, etc. As an example, each small cell or AP can report its traffic load to a smart controller.  The example smart controller can be a logical entity and could be a localized in building, could be centralized in a network operator's location,\nvirtualized in a cloud-based environment, etc. In some embodiments, control can be integrated as one of the functions in an existing system, e.g., a self-organizing network (SON) controller, within a RAN device itself, etc. A controller, based on a\nparameter, such as the hierarchical class of the RAN device, use of a RAN device and/or a coupled RAN device, supplementary data, e.g., date (weekday, weekend, national or company holiday), time (regular working hours, after hours), historical use data,\netc., rules, received indicia, etc., can determine if a state transition is allowed, can determine to initiate a state transition, can adapt a RAN device state, etc. As an example, a virtualized controller can activate a `power saving` mode on a RAN\ndevice, via a signal sent to the RAN device over a communication framework, based on the RAN device being a field RAN device and all neighboring RAN devices indicating that they are without a camped or in use UE.  In some embodiments, a controller can\n`learn` or form inferences based on historical usage, e.g., usage patterns of a RAN device at a particular location, etc., and can use learned behavior(s), inference(s), etc., as an input to a decision making process regarding the state transition.  As\nan example, a field RAN device can learn that it is typically used between 6 am and 6 pm, Monday to Friday, and can accordingly not check for possibilities of a state transition during those hours.  In an aspect, over a period of time, more especially in\na large location with potentially tens to thousands of RAN devices, the disclosed subject matter can result in notable energy savings and, in some embodiments, can concurrently provide a less cluttered spectrum.\nTo the accomplishment of the foregoing and related ends, the disclosed subject matter, then, comprises one or more of the features hereinafter more fully described.  The following description and the annexed drawings set forth in detail certain\nillustrative aspects of the subject matter.  However, these aspects are indicative of but a few of the various ways in which the principles of the subject matter can be employed.  Other aspects, advantages, and novel features of the disclosed subject\nmatter will become apparent from the following detailed description when considered in conjunction with the provided drawings.\nFIG. 1 is an illustration of a system 100, which can facilitate changing a device state based on a hierarchical class of the device, in accordance with aspects of the subject disclosure.  System 100 can comprise macro RAN device 102.  Macro RAN\ndevice 102 can be an access point to a macro network, e.g., a macro cellular device, a NodeB, an eNodeB, a wide area network (WAN) AP, etc. In some embodiments, macro RAN device 102 can be connected to a communications network associated with a wireless\nnetwork provider identity.  Typically, a user equipment (UE) can communicate via a wireless interface to macro RAN device 102.  The UE can be transferred from macro RAN device 102 to another macro RAN device as the UE moves between a location served by\nmacro RAN device 102 and a location served by the other macro RAN device, e.g., a cellular handover, etc. In an aspect, macro RAN device 102 can be a neighbor of a gateway RAN device, e.g., RAN device 110.\nSystem 100 can comprise RAN device 110.  Ran device 110 can be a gateway RAN device.  A gateway RAN device, e.g., RAN device 110, can have a neighbor relationship with macro RAN device 102.  Accordingly, a UE can transition, e.g., be handed\nover, between RAN device 110 and macro RAN device 102.  As an example, where a UE is transported into an office building, over the air communication to macro RAN device 102 can degrade due to RF attenuation associated with the building, etc. The example\nUE can seek to transition to another RAN access device to maintain UE services.  Accordingly, where the example building comprises RAN device 110, the UE can be handed from macro RAN device 102 to RAN device 110 based on the neighbor relationship between\nmacro RAN device 102 and RAN device 110.  In contrast, for example where a neighbor relationship does not exist between macro RAN device 102 and RAN device 110, the UE can be dropped from macro RAN device 102 before acquiring service via RAN device 110,\nwhich can result in an interruption of UE services.  Where RAN device 110 is a gateway RAN device, in some embodiments, RAN device 110 can be have a neighbor relationship with RAN device 120.\nRAN device 120, of system 100, can be an intermediate RAN device.  In an aspect, an intermediate RAN device can have a neighbor relation with a field RAN device, another intermediate RAN device, and/or a gateway RAN device.  In an aspect, an\nintermediate RAN device, e.g., RAN device 120, can be interposed between a field RAN device and a gateway RAN device, wherein the gateway RAN device can be interposed between macro RAN device 102 and the intermediate RAN device.  In some embodiments,\nintermediate RAN device, e.g., RAN device 102, etc., can be a first layer of RAN devices that can support a transition to/from a gateway RAN device.\nSystem 100 can further comprise RAN device 130 that can have a neighbor relationship with RAN device 120.  RAN device 130 can be a field RAN device.  In an aspect, a field RAN device, e.g., RAN device 120, etc., can have a neighbor relationship\nwith other field RAN devices and/or an intermediate RAN device, e.g., RAN device 120.  As such, in some embodiments, a field RAN device, e.g., RAN device 120, etc., can be at least one hierarchical layer away from a gateway RAN device, e.g., RAN device\n110, etc.\nIn an aspect, a RAN device can be enabled to undergo a state transition based on satisfying a hierarchical class rule.  The hierarchical class rule can, for example, reserve state transitions for field RAN device(s) only, e.g., an intermediate\nRAN device and/or a gateway RAN device can be deemed ineligible to undergo a state transition.  Moreover, eligibility to undergo a state transition can be distinct from determining if a state transition is to be initiated or performed.  As an example, a\nfield RAN device, e.g., RAN device 130, etc., can be determined to be eligible for a state transition but may, or may not, undergo a state transition based on a further criterion, e.g., the field RAN device is not in use by a UE, etc.\nIn an embodiment, where RAN device 130 is eligible to undergo a state transition, a state transition of RAN device 130 can be initiated in response to determining that a self-use rule related to use of RAN 130 has been satisfied and that a\nneighbor-use rule related to use of RAN 120 has been satisfied.  In an aspect, the neighbor use rule is related to use of a neighbor RAN device to RAN device 130 rather than to use of an intermediate RAN device, e.g., RAN device, etc., specifically.  As\nan example, where RAN device 130 has a neighbor relationship to another field RAN device, rather than to an intermediate RAN device like 120, and the other field RAN device is in use by a UE, then a state transition may not be initiated based on a\npossibility of the UE transitioning from the neighboring field RAN device to RAN device 130.  Use of a RAN device can include active and idle use, e.g., active use can be based on a radio access bearer resource being established for the UE via the RAN\ndevice while idle use can lack the radio access bearer resource but still have the UE logically camped on the RAN device.\nFIG. 2 is an illustration of a system 200, which can facilitate changing a device state based on a hierarchical class of the device among a plurality of devices comprising the device, in accordance with aspects of the subject disclosure.  System\n200 can comprise macro RAN device 202.  Macro RAN device 202 can be an access point to a macro network.  In some embodiments, macro RAN device 202 can be connected to a communications network associated with a wireless network provider identity. \nTypically, a UE can communicate via a wireless interface to macro RAN device 202.  The UE can be transferred from macro RAN device 202 to another macro RAN device as the UE moves between a location served by macro RAN device 202 and a location served by\nthe other macro RAN device.\nSystem 200 can comprise RAN device 210.  Ran device 210 can be a gateway RAN device.  A gateway RAN device, e.g., RAN device 210, can have a neighbor relationship with macro RAN device 202.  Accordingly, a UE can transition between RAN device\n210 and macro RAN device 202.  In some embodiments, RAN device 210 can further have a neighbor relationship with RAN device(s) 221-223, etc.\nRAN device(s) 221-223, of system 200, can be intermediate RAN device(s).  In an aspect, an intermediate RAN device can have a neighbor relation with a field RAN device, another intermediate RAN device, and/or a gateway RAN device, as\nillustrated.  As an example, RAN device 221 can have a neighbor relationship with gateway RAN device 210 and field RAN device 231.  As another example, RAN device 222 can have a neighbor relationship with gateway RAN device 210 and field RAN devices 232\nand 233, as well as to another intermediate RAN device, e.g., RAN device 223.  As a further example, RAN device 223 can have a neighbor relationship with gateway RAN device 210 and intermediate RAN device 222.  Of note, intermediate RAN device(s) 221-223\ndo not have a neighbor relationship with macro RAN device 202, e.g., in some embodiments, only a gateway RAN device, e.g., 210, can have a neighbor relationship with a macro RAN device, e.g., 202.  In an aspect, an intermediate RAN device, 221-223, can\nbe interposed between a field RAN device, e.g., 231-237, and a gateway RAN device, e.g., 210.  Moreover, a gateway RAN device, e.g., 210, can be interposed between macro RAN device 202 and intermediate RAN device(s), e.g., 221-223.  In some embodiments,\nintermediate RAN devices can be a first layer of RAN devices that can support a transition between a gateway RAN device and a field RAN device.\nSystem 200 can further comprise field RAN device(s) 231-237 that can have a neighbor relationship with intermediate RAN device(s) 221-223, as illustrated.  A field RAN device, 231-237, can have a neighbor relationship with other field RAN\ndevices 231-237, and/or intermediate RAN device(s), 221-223.  As an example, field RAN device 231 can have a neighbor relationship with intermediate RAN device 221 and field RAN device 232.  As another example, field RAN device 234 can have a neighbor\nrelationship with field RAN device 233, field RAN device 236, field RAN device 237, field RAN device 235, and field RAN device 233, but not have one with an intermediate RAN device, 221-223, nor with a gateway RAN device 210, nor macro RAN device 202. \nAs such, in some embodiments, a field RAN device, 231-237, can be at least one hierarchical layer away from a gateway RAN device, 210, e.g., separated by at least intermediate RAN device 221-223.\nIn an aspect, a RAN device can be enabled to undergo a state transition based on satisfying a hierarchical class rule.  The hierarchical class rule can, for example, reserve state transitions for field RAN device(s) 231-237 only, e.g., an\nintermediate RAN device 221-223, and/or a gateway RAN device 210 can be deemed ineligible to undergo a state transition.  Moreover, eligibility to undergo a state transition can be distinct from determining if a state transition is to be initiated or\nperformed.  In an embodiment, where a field RAN device, 231-237, is eligible to undergo a state transition, the state transition of the field RAN device, 231-237, can be initiated in response to determining that a self-use rule related to UE use of field\nRAN device, 231-237, has been satisfied and that a neighbor-use rule related to UE use of a neighboring RAN device, e.g., field RAN device, 231-237, and/or intermediate RAN device 221-223, has been satisfied.  In an aspect, the neighbor use rule is\nrelated to use of a neighbor RAN device to RAN device 230 rather than to use of an intermediate RAN device, e.g., RAN device, etc., specifically.  As a first example, where RAN device 231 has a neighbor relationship to field RAN device 232 and to\nintermediate RAN device 221, then UE use of field RAN device 231 or 232, or UE use of field RAN device 221, can restrict implementation of a state transition for filed RAN device 231 even where field RAN device 231 is eligible for a state transition.  As\na second example, where RAN device 235 has a neighbor relationship to field RAN devices 233, 234, and 237, then UE use of field RAN device 233, 234, 235, or 237, can restrict implementation of a state transition for field RAN device 235 even where field\nRAN device 235 is eligible to otherwise undergo a state transition.\nFIG. 3 is an illustration of a system 300, which can facilitate changing a device state based on a hierarchical class of the device and use of another device in a network comprising the device and the other device, in accordance with aspects of\nthe subject disclosure.  System 300 can comprise macro RAN device 302.  Macro RAN device 302 can be an access point to a macro network.  In some embodiments, macro RAN device 302 can be connected to a communications network associated with a wireless\nnetwork provider identity.  Typically, a UE can communicate via a wireless interface to macro RAN device 302.  The UE can be transferred from macro RAN device 302 to another macro RAN device as the UE moves between a location served by macro RAN device\n302 and a location served by the other macro RAN device.\nSystem 300 can comprise gateway RAN device 310.  Gateway RAN device can have a neighbor relationship with macro RAN device 302.  Accordingly, a UE can transition between gateway RAN device 310 and macro RAN device 302.  In some embodiments,\ngateway RAN device 310 can further have a neighbor relationship with intermediate RAN device 320.\nIntermediate RAN device 320, of system 300, can have a neighbor relation with a field RAN device, e.g., 332-338, another intermediate RAN device, not illustrated, and/or a gateway RAN device, e.g., 310, as illustrated.  As an example,\nintermediate RAN device 320 can have a neighbor relationship with gateway RAN device 310 and field RAN device 332.  Of note, intermediate RAN device 320 does not have a neighbor relationship with macro RAN device 302, e.g., in some embodiments, only\ngateway RAN device 310 can have a neighbor relationship with macro RAN device 302.  In an aspect, an intermediate RAN device 320 can be interposed between a field RAN device, e.g., 332-338, and gateway RAN device 310.  Moreover, gateway RAN device 310\ncan be interposed between macro RAN device 302 and intermediate RAN device 320.  In some embodiments, intermediate RAN devices can be a first layer of RAN devices that can support a UE transition between a gateway RAN device and a field RAN device via\nthe intermediate RAN device.\nSystem 300 can further comprise field RAN device(s) 332-338 that can have a neighbor relationship with intermediate RAN device 320, or with other field RAN devices 332-338, as illustrated.  As an example, field RAN device 332 can have a neighbor\nrelationship with intermediate RAN device 320 and field RAN device 334.  As another example, field RAN device 334 can have a neighbor relationship with field RAN device 332, field RAN device 336, but not have a neighbor relationship with an intermediate\nRAN device, 320, with a gateway RAN device, 310, nor with a macro RAN device, 302.  As such, in some embodiments, a field RAN device, 332-338, can be at least one hierarchical layer away from gateway RAN device 310, e.g., separated by at least\nintermediate RAN device 320.\nIn an aspect, a RAN device can be enabled to undergo a state transition based on satisfying a hierarchical class rule.  The hierarchical class rule can, for example, reserve state transitions for field RAN device(s) 332-338 only, e.g.,\nintermediate RAN device 320, and/or a gateway RAN device 310 can be deemed ineligible to undergo a state transition.  Moreover, eligibility to undergo a state transition can be distinct from determining if a state transition is to be initiated or\nperformed.  In an embodiment, where field RAN device, 332-338, are eligible to undergo a state transition, the state transition of the field RAN device, 332-338, can be initiated in response to determining that a self-use rule related to UE use of field\nRAN device, 332-338, has been satisfied and that a neighbor-use rule related to UE use of a neighboring RAN device, e.g., field RAN device, 332-338, and/or intermediate RAN device 320, has been satisfied.  As a first example, RAN device 332 can have a\nneighbor relationship to field RAN device 334 and to intermediate RAN device 320, such that UE use of field RAN device 332, 334, or UE use of field RAN device 320, can cause either the self-use or neighbor-use rule to not be satisfied, thwarting\nimplementation of a state transition for field RAN device 331.  System 300 illustrates this by showing UE 304 using field RAN device 332, which can cause the self-use rule to be unsatisfied and can result in no state transition being initiated.  As a\nsecond example, where RAN device 336 has a neighbor relationship to field RAN devices 334 and 338, and UE 304 uses field RAN device 332 rather than 334 or 338, then both the self-use and neighbor-use rule can be satisfied.  It will be noted that in some\nembodiments, the neighbor-use rule can be adapted to reflect more than a first layer of neighbor RAN device.  In these embodiments, UE 304 use of 332 can affect satisfaction of the neighbor-use rule, for example for field RAN device 336, 338, etc. In\nsome embodiments, the neighbor-use rule can be based on the density of RAN devices, e.g., where the devices are dense, it can be more likely that a transition between a used and unused RAN device can occur rapidly enough in time that it can be desirable\nto avoid state transitions for a plurality of layers between an in use RAN device and an eligible RAN device.\nFIG. 4 is an illustration of a system 400, which can facilitate changing a device state based on a hierarchical class of the device and neighboring device(s) state(s), in accordance with aspects of the subject disclosure.  System 400 can\ncomprise macro RAN device 402.  Macro RAN device 402 can be an access point to a macro network.  In some embodiments, macro RAN device 402 can be connected to a communications network associated with a wireless network provider identity.  Typically, a UE\ncan communicate via a wireless interface to macro RAN device 402.  The UE can be transferred from macro RAN device 402 to another macro RAN device as the UE moves between a location served by macro RAN device 402 and a location served by the other macro\nRAN device.\nSystem 400 can comprise gateway RAN device 410.  Gateway RAN device can have a neighbor relationship with macro RAN device 402.  Accordingly, a UE can transition between gateway RAN device 410 and macro RAN device 402.  In some embodiments,\ngateway RAN device 410 can further have a neighbor relationship with intermediate RAN device 420.  Intermediate RAN device 420, of system 400, can have a neighbor relation with a field RAN device(s), e.g., 430, 432, 434, etc. another intermediate RAN\ndevice, not illustrated, and/or a gateway RAN device, e.g., 410, as illustrated.\nSystem 400 can further comprise field RAN device(s) 430, 432, 434, etc., that can have a neighbor relationship with other field RAN devices 430, 432, 434, etc., as illustrated.  In an embodiment, field RAN device 434 can have neighbor relations\nwith a first layer of other field RAN devices as indicated by the medium grey band of field RAN devices 432, surrounding field RAN device 434.  Field RAN device 434 can have extended neighbor relations with a second layer of other field RAN devices,\ne.g., the neighbors of the first layer can be extended neighbors of 434, as indicated by the light grey band of field RAN devices 430, surrounding the first layer of field RAN 432.  As such, in some embodiments, field RAN device 434 can be at least one\nhierarchical layer away from gateway RAN device 410, e.g., separated by at least intermediate RAN device 420, and generally one or more layers of filed RAN devices, e.g., 430, 432, 434, etc.\nIn an aspect, a RAN device can be enabled to undergo a state transition based on satisfying a hierarchical class rule.  The hierarchical class rule can, for example, reserve state transitions for field RAN device(s) 430-434 only, e.g.,\nintermediate RAN device 420, and/or a gateway RAN device 410 can be deemed ineligible to undergo a state transition.  Moreover, eligibility to undergo a state transition can be distinct from determining if a state transition is to be initiated or\nperformed.  In an embodiment, where field RAN device 434 is eligible to undergo a state transition, the state transition of the field RAN device 434 can be initiated in response to determining that a self-use rule related to UE use of field RAN device\n434 has been satisfied and that a neighbor-use rule related to UE use of a neighboring RAN device, e.g., field RAN device, 432 has been satisfied.  As such, UE use of any of the field RAN devices 432 can prevent initiation of a state transition for the\nfield RAN device 434, for example, because there is a possibility of a UE transitioning from a neighboring field RAN device 432 to field RAN device 434.  This can be understood more clearly where the state transition is putting field RAN device 434 into\na low-power state, in that if a UE attached to field RAN device 432 attempts to transition to field RAN device 434, the time it takes to bring field RAN device 434 out of a low-power state can cause transition problems and, as such, a direct neighbor\nthat has a UE attached can be used to prevent an eligible RAN device from undergoing a state transition.  In some embodiments, the neighbor-user rule can extend beyond the first layer of neighbor RAN device, e.g., field RAN devices 432 are the first\nlayer to field RAN device 434.  In these embodiments, use of a second, third, etc., layer RAN device can affect the implementation of a state transition for an eligible RAN device.  As an example, the UE use of the second layer field RAN devices 430 to\nfield RAN device 434 can prevent a state transition for field RAN device 434.\nFIG. 5 is an illustration of a system 500, which can facilitate changing a device state, based on a hierarchical class of the device and received data, in accordance with aspects of the subject disclosure.  System 500 can comprise small\ncell/AP/RAN device 540, hereinafter RAN device 540.  RAN device 540 can be the same as or similar to RAN device(s) 110, 120, 130, 210, 221-223, 231-237, 310, 320, 332-338, 410, 420, 430, 432, 434, etc. RAN device 540 can comprise hierarchy analysis\ncomponent (HAC) 542.  HAC 542 can determine a hierarchical class of a RAN device, e.g., 540, etc. In an embodiment, the hierarchical class can be a gateway RAN device, an intermediate RAN device, a field RAN device, etc., as disclosed herein.  In an\naspect, additional hierarchical classes can be employed, e.g., a first layer of neighbor RAN device, a second layer of RAN devices, etc. In an aspect, HAC 542 can determine a hierarchical level of RAN 540, of other RAN devices, e.g., first layer\nneighbors, second layer neighbors, etc. In an embodiment, HAC 542 can determine a hieratical map of RAN devices in a device network, a portion of a device network, etc. The device map and/or hierarchical class information determined by HAC 542 can be\nemployed in determining eligibility for a state transition of a RAN device.  Moreover, the device map and/or hierarchical class information determined by HAC 542 can be employed in self-use and neighbor-use rule analysis.\nRAN device 540 can further comprise state determination component (SDC) 544.  SDC 544 can determine the current or historical state of RAN 540, via self-state component 550, or of another RAN device, via neighbor-state component 552.  A state\ndiagram or model thereof can then be used to determine possible state transitions from the current state determined by SDC 544.  In an aspect, a current state can comprise information indicative of UE use of a RAN device.  As an example, SDC 544 can\ndetermine, via self-state component 550, if RAN device 540 is in use by a UE, e.g., a UE is camped on RAN device 540, is actively using RAN device 540, etc. As another example, SDC 544 can determine, via neighbor-state component 552, if another RAN\ndevice, e.g., a neighbor to RAN Device 540, etc., is in use by a UE.\nIn an embodiment, HAC 542 and SDC 544 can be coupled to rule component 546.  Rule component 546 can receive hierarchical data from HAC 542 and can receive state data from SDC 544.  Rule component 546 can further receive input and/or supplemental\ndata 570 from another device.  In an embodiment, data 570 can comprise data that allows implementation of new rules, deletion of existing rules, modification of existing rules, etc., comprised in rule component 546.  In some embodiments, data 570 can\ncomprise information relation to historical RAN device use, events data, time, date, location, etc. In an embodiment, rule component 546 can enable determining state transition eligibility of RAN device 540, or of another RAN device.  Moreover, rule\ncomponent 546 can determine if a state transition in to be performed, initiated, etc., based on the RAN device being deemed eligible and satisfaction of rules related to use, time/date/place, historical data, etc. As an example, historical data\nindicating regular use of a RAN device during working hours can prevent a RAN device from going into a sleep mode during business hours even though it is otherwise eligible and unused.  As a further example, historical data indicating irregular use of a\nRAN device during working hours can limit the RAN device to a `snoozing` mode rather than a `deep sleep` mode during business hours where it is eligible and unused, e.g., the RAN can stay active but broadcast an SSID less frequently that when at full\npower, which can save power and reduce interference but allow the RAN device to rapidly support an incoming UE.\nRAN device 540 can comprise state adjustment component (SAC) 548.  SAC 548 can initiate, implement, or perform a state transition based on information from rule component 546.  In an aspect, rule component 546 can indicate which rules are\nsatisfied and SAC 546 can, accordingly, determine what state transition(s) to implement.  In an embodiment, SAC 548 can comprise snooze component 560, sleep component 562, off component 546, etc. Snooze component 560 can enable putting a RAN device,\ne.g., 540, etc., into a snooze mode that can be more power and interference friendly than a full power mode but less so than a sleep mode.  Sleep component 562 can enable putting a RAN device, e.g., 540, etc., into a sleep mode that can be more power and\ninterference friendly than either a full power mode or a snooze mode, but less so than an off mode.  Off component 564 can enable putting a RAN device, e.g., 540, etc., into an off state that can be consume little to no power and cause little to no\ninterference but can be much slower to transition out of in comparison to a sleep or snooze state.  It will be appreciated that other states can be represent by components of SAC 548 and that all such other states are within the scope of the instant\ndisclosure despite not being explicitly recited for the sake of clarity and brevity.\nIn view of the example system(s) described above, example method(s) that can be implemented in accordance with the disclosed subject matter can be better appreciated with reference to flowcharts in FIG. 6-FIG. 8.  For purposes of simplicity of\nexplanation, example methods disclosed herein are presented and described as a series of acts; however, it is to be understood and appreciated that the claimed subject matter is not limited by the order of acts, as some acts may occur in different orders\nand/or concurrently with other acts from that shown and described herein.  For example, one or more example methods disclosed herein could alternatively be represented as a series of interrelated states or events, such as in a state diagram.  Moreover,\ninteraction diagram(s) may represent methods in accordance with the disclosed subject matter when disparate entities enact disparate portions of the methods.  Furthermore, not all illustrated acts may be required to implement a described example method\nin accordance with the subject specification.  Further yet, two or more of the disclosed example methods can be implemented in combination with each other, to accomplish one or more aspects herein described.  It should be further appreciated that the\nexample methods disclosed throughout the subject specification are capable of being stored on an article of manufacture (e.g., a computer-readable medium) to allow transporting and transferring such methods to computers for execution, and thus\nimplementation, by a processor or for storage in a memory.\nFIG. 6 is an illustration of a flowchart 600, which can facilitate adapting a state of a device based on a hierarchical class of the device, a state of a neighboring device, and a current state of the device, in accordance with aspects of the\nsubject disclosure.  Flowchart 600 can comprise a controller determining if a RAN device is a gateway device at decision 610.  A gateway RAN device can be designated as a RAN device that has a neighbor relation to a macro RAN device.  Where the RAN\ndevice is determined to be a gateway device the RAN device can be set to type `A` at 605.  Where the RAN device is determined to not be a gateway device, e.g., the RAN device does not have a neighbor relation with a macro RAN device, then the device type\ncan be set to type `B` at 615.\nAt 620, flowchart 600 can comprise the controller determining if the RAN device is coupled to a gateway RAN device.  Devices coupled to a gateway RAN device can be type `B` devices or other RAN devices.  Where the RAN device is determined to be\ncoupled to a gateway RAN device the flow can return to decision block 610.  Where the RAN device is determined by the controller to not be coupled to a gateway RAN device, the type can be set to type `C` at 625.  In an aspect, type A can be a gateway RAN\ndevice, type B can be an intermediate RAN device, and type C can be a field RAN device, as disclosed hereinabove.\nAt 630, it can be determined by the controller if another RAN device neighboring the RAN device is supporting a UE.  In an aspect, supporting a UE can comprise a UE being camped on the neighboring RAN device.  In another aspect, supporting a UE\ncan comprise the UE having an active session with the neighboring RAN device, e.g., having a radio access bearer established via the neighboring RAN device.  Where the neighboring RAN device is supporting a UE, the controller can return to 610.  Where\nthe neighboring RAN device is not supporting a UE, an indication that a state change is available can be made by the controller at 635.\nAt 640, it can be determined by the controller if the RAN device itself is supporting a UE.  If the RAN device is itself supporting a UE, the controller can return to 630.  This can allow the controller to continue to monitor for UE use of the\nRAN device itself or of a neighboring RAN device before determining to initiate a state transition.  Where the RAN device itself is not supporting a UE, it can be determined by the controller, at 650, if a current use rule is satisfied.  Satisfaction of\nthe current use rule can be based on historical use data, supplementary data, e.g., time/date/location, an event occurrence, etc. Again, if the current use rule is not satisfied, the controller can return to 630 to allow the controller to continue to\nmonitor relevant conditions of the RAN device prior to determining to initiate a state transition.  If the current use rule is satisfied at 650, the state of the RAN device can be adapted by the controller via a state transition at 655.\nAt 660, the controller can monitor for a recovery indicator related to the RAN device.  Where a recovery indicator has been received, e.g., an indicator indicating that the RAN device should return to a full power state, that the RAN device\nshould transition to another state, etc., then the controller can adapt the state of the device via a state transition at 665, then the controller can return to 610.  Where a recovery indicator has not been received at the controller, then the controller\ncan return to 660 to continue to monitor of a recovery indicator.\nFIG. 7 illustrates example method 700 that facilitates changing a device state based on a hierarchical class of the device, in accordance with aspects of the subject disclosure.  Method 700, at 710, can comprise, in response to determining that\na device state change is supported, determining that a rule related to a present use of the device has been satisfied.  The device state change can be determined to be supported based on the hierarchical class of the device.  The hierarchical class can\nbe related to a logical functionality of a device or to a type of a device.  In an aspect, logical function of a device can be determined to act as a gateway device, an intermediate device, or a field device, even where the devices are otherwise the same\nor similar.  In another aspect, different types of devices can be used for the different hierarchical classes, e.g., a first type of device can be used as a gateway device, a second type of device can be used for an intermediate device, and a third type\nof device can be used for a field device, wherein the first, second and third types of device can be different types.\nIn response to determining that the hierarchical class of device enables the device to undergo a state change, rules relating to a current use of the device can be employed to determine that the state change of the device should be initiated. \nIn an aspect, the current use can relate to the device supporting a UE, e.g., an idle UE camped on the device, an active UE having a radio access bearer resource allocated via the device, etc. In some embodiments, current use can be based on UE use of\nneighboring devices, e.g., indicating a potential us of the current device due to the proximity of a supported UE.\nAt 720, method 700 can comprise selecting a device state update based on an input related to an operation profile, historical use data, and supplementary data.  Once the device is determined to be eligible for a state transition and that the\ncurrent use satisfies the related rules for imitating the state transition, the method at 720 can determine what that transition is based on inputs.  An input can be an operator profile that comprises information relating to what state transition should\nbe implemented for determined conditions.  The conditions can be determined from supplemental data, e.g., time, date, place, nearby events, historical use data, etc.\nAt 730, the device state can be adapted, e.g., transitioned, based on the device state update determined at 720.  At this point method 700 can end.  The state transition determined at 720 can be applied at 730 in accord with the eligibility and\nsatisfaction of the rules at 710.  As such, for example, a field RAN device can be eligible, the field RAN device can be unused and a first layer of neighboring RAN devices can also be unused by a UE resulting in the determination that the field RAN\ndevice satisfies the use rules.  It can then be determined, for example, that the field RAN device should enter a snooze state rather than an off or sleep state due to historical use data and the current time and day.\nFIG. 8 illustrates example method 800 facilitating time-delayed changing of a device state based on a hierarchical class of the device, in accordance with aspects of the subject disclosure.  Method 800, at 810, can comprise receiving, by a small\ncell device, e.g., a RAN device, etc., an indication of a change in use of a neighboring small cell device, wherein the neighboring small cell device is a neighbor of the small cell device, e.g., it can be indicated that a UE is now using a neighboring\nRAN device to a RAN device of interest.  In the alternative, an indication of a change in the use of the small cell device itself can be received, e.g., a UE can be attempting to directly use the small cell device itself rather than being handed over\nfrom another device.\nAt 820, the small cell device state can be adapted in response to the receiving the indication at 810.  As an example, where a first small cell device is in sleep mode, a controller device can receive an indication that a second small cell\ndevice, that is a neighbor to the first small cell device, has started to support a UE.  The example controller can then adapt the state of the first small cell device, e.g., from the sleep state to another state, such as a wake state, full power state,\netc. In the example, the first small cell device can be woken up in response to a UE moving closer to the first small cell device, wherein closer is indicative of a number of hops between the UE and the first small cell device.  In an aspect, closer can\nalso indicate that the UE is physically closer to the first small cell device because neighboring small cells can be closer together in distance as well as in number of hops between them.  In some instances, a closer neighbor in hops will have a greater\ndistance, but this can still be considered moving closer to the first small cell device due to the decrease in hops to the UE.\nAt 830, a timer value can be updated.  At this point method 800 can end.  The time value can be related to restricting subsequent adaptations of the small cell device state.  This can reduce `fluttering` where the RAN device cyclically\ntransitions between states without remaining in any one state long enough to be meaningfully useful in the network of devices comprising the RAN device.  As an example, where a small cell device is adapted to a full power state, the time can keep the\ndevice in full power state for a few seconds to a few hours, etc. Thus, where a neighboring small cell starts supporting a UE, the first small cell can be activated and remain on in accord with the timer.  Thus, if the UE moves away and then comes closes\nagain, such as where the user is walking out of their office to use the restroom and then comes back, the first small cell can be prevented from going to sleep during that short period where the UE was outside of the service area of the neighboring small\ncell.  This can be similar to preventing your desktop computer from hibernating the second you stop using it which, if it occurred, could waste more time going in and out of hibernation that is acceptable, e.g., it can be cheaper to waste a short period\nof energy than to waste time waiting for state transitions to occur.  The timer value can be adapted based on input associated with a user identity, e.g., the timer can be set by a systems administrator, an operator, can be adapted based on inference or\nmachine learning techniques, can be based on historical use data, etc.\nFIG. 9 is a schematic block diagram of a computing environment 900 with which the disclosed subject matter can interact.  The system 900 comprises one or more remote component(s) 910.  The remote component(s) 910 can be hardware and/or software\n(e.g., threads, processes, computing devices).  In some embodiments, remote component(s) 910 can core network devices associated with a network provider identity, can be a macro RAN device, e.g., 102, 202, 302, 402, etc., UE 304, a device generating\ninput(s) 570, etc.\nThe system 900 also comprises one or more local component(s) 920.  The local component(s) 920 can be hardware and/or software (e.g., threads, processes, computing devices).  In some embodiments, local component(s) 920 can comprise RAN devices,\n110-130, 210-237, 310-338, 410-434, 540, etc., local controller devices, controller devices of RAN devices, etc.\nOne possible communication between a remote component(s) 910 and a local component(s) 920 can be in the form of a data packet adapted to be transmitted between two or more computer processes.  Another possible communication between a remote\ncomponent(s) 910 and a local component(s) 920 can be in the form of circuit-switched data adapted to be transmitted between two or more computer processes in radio time slots.  The system 900 comprises a communication framework 940 that can be employed\nto facilitate communications between the remote component(s) 910 and the local component(s) 920, and can comprise an air interface, e.g., Uu interface of a UMTS network, via a long-term evolution (LTE) network, etc. Remote component(s) 910 can be\noperably connected to one or more remote data store(s) 950, such as a hard drive, solid state drive, SIM card, device memory, etc., that can be employed to store information on the remote component(s) 910 side of communication framework 940.  Similarly,\nlocal component(s) 920 can be operably connected to one or more local data store(s) 930, that can be employed to store information on the local component(s) 920 side of communication framework 940.  As examples, historical use data, supplementary data,\ncurrent use data, neighboring use data, user input(s), etc., can be stored on remote data store(s) that can be comprised in or connected to a macro RAN device, e.g., 102, 202, 302, 402, etc., a UE 304, a device generating input(s) 570, etc.\nIn order to provide a context for the various aspects of the disclosed subject matter, FIG. 10, and the following discussion, are intended to provide a brief, general description of a suitable environment in which the various aspects of the\ndisclosed subject matter can be implemented.  While the subject matter has been described above in the general context of computer-executable instructions of a computer program that runs on a computer and/or computers, those skilled in the art will\nrecognize that the disclosed subject matter also can be implemented in combination with other program modules.  Generally, program modules comprise routines, programs, components, data structures, etc. that performs particular tasks and/or implement\nparticular abstract data types.\nIn the subject specification, terms such as \"store,\" \"storage,\" \"data store,\" data storage,\" \"database,\" and substantially any other information storage component relevant to operation and functionality of a component, refer to \"memory\ncomponents,\" or entities embodied in a \"memory\" or components comprising the memory.  It is noted that the memory components described herein can be either volatile memory or nonvolatile memory, or can comprise both volatile and nonvolatile memory, by\nway of illustration, and not limitation, volatile memory 1020 (see below), non-volatile memory 1022 (see below), disk storage 1024 (see below), and memory storage 1046 (see below).  Further, nonvolatile memory can be included in read only memory,\nprogrammable read only memory, electrically programmable read only memory, electrically erasable read only memory, or flash memory.  Volatile memory can comprise random access memory, which acts as external cache memory.  By way of illustration and not\nlimitation, random access memory is available in many forms such as synchronous random access memory, dynamic random access memory, synchronous dynamic random access memory, double data rate synchronous dynamic random access memory, enhanced synchronous\ndynamic random access memory, Synchlink dynamic random access memory, and direct Rambus random access memory.  Additionally, the disclosed memory components of systems or methods herein are intended to comprise, without being limited to comprising, these\nand any other suitable types of memory.\nMoreover, it is noted that the disclosed subject matter can be practiced with other computer system configurations, comprising single-processor or multiprocessor computer systems, mini-computing devices, mainframe computers, as well as personal\ncomputers, hand-held computing devices (e.g., personal digital assistant, phone, watch, tablet computers, netbook computers, .  . . ), microprocessor-based or programmable consumer or industrial electronics, and the like.  The illustrated aspects can\nalso be practiced in distributed computing environments where tasks are performed by remote processing devices that are linked through a communications network; however, some if not all aspects of the subject disclosure can be practiced on stand-alone\ncomputers.  In a distributed computing environment, program modules can be located in both local and remote memory storage devices.\nFIG. 10 illustrates a block diagram of a computing system 1000 operable to execute the disclosed systems and methods in accordance with an embodiment.  Computer 1012, which can be, for example, a macro RAN device, e.g., 102, 202, 302, 402, etc.,\na UE 304, a device generating input(s) 570, etc., a RAN device, 110-130, 210-237, 310-338, 410-434, 540, etc., a local controller device, a controller device of a RAN devices, etc., can comprise a processing unit 1014, a system memory 1016, and a system\nbus 1018.  System bus 1018 couples system components comprising, but not limited to, system memory 1016 to processing unit 1014.  Processing unit 1014 can be any of various available processors.  Dual microprocessors and other multiprocessor\narchitectures also can be employed as processing unit 1014.\nSystem bus 1018 can be any of several types of bus structure(s) comprising a memory bus or a memory controller, a peripheral bus or an external bus, and/or a local bus using any variety of available bus architectures comprising, but not limited\nto, industrial standard architecture, micro-channel architecture, extended industrial standard architecture, intelligent drive electronics, video electronics standards association local bus, peripheral component interconnect, card bus, universal serial\nbus, advanced graphics port, personal computer memory card international association bus, Firewire (Institute of Electrical and Electronics Engineers 1194), and small computer systems interface.\nSystem memory 1016 can comprise volatile memory 1020 and nonvolatile memory 1022.  A basic input/output system, containing routines to transfer information between elements within computer 1012, such as during start-up, can be stored in\nnonvolatile memory 1022.  By way of illustration, and not limitation, nonvolatile memory 1022 can comprise read only memory, programmable read only memory, electrically programmable read only memory, electrically erasable read only memory, or flash\nmemory.  Volatile memory 1020 comprises read only memory, which acts as external cache memory.  By way of illustration and not limitation, read only memory is available in many forms such as synchronous random access memory, dynamic read only memory,\nsynchronous dynamic read only memory, double data rate synchronous dynamic read only memory, enhanced synchronous dynamic read only memory, Synchlink dynamic read only memory, Rambus direct read only memory, direct Rambus dynamic read only memory, and\nRambus dynamic read only memory.\nComputer 1012 can also comprise removable/non-removable, volatile/non-volatile computer storage media.  FIG. 10 illustrates, for example, disk storage 1024.  Disk storage 1024 comprises, but is not limited to, devices like a magnetic disk drive,\nfloppy disk drive, tape drive, flash memory card, or memory stick.  In addition, disk storage 1024 can comprise storage media separately or in combination with other storage media comprising, but not limited to, an optical disk drive such as a compact\ndisk read only memory device, compact disk recordable drive, compact disk rewritable drive or a digital versatile disk read only memory.  To facilitate connection of the disk storage devices 1024 to system bus 1018, a removable or non-removable interface\nis typically used, such as interface 1026.\nComputing devices typically comprise a variety of media, which can comprise computer-readable storage media or communications media, which two terms are used herein differently from one another as follows.\nComputer-readable storage media can be any available storage media that can be accessed by the computer and comprises both volatile and nonvolatile media, removable and non-removable media.  By way of example, and not limitation,\ncomputer-readable storage media can be implemented in connection with any method or technology for storage of information such as computer-readable instructions, program modules, structured data, or unstructured data.  Computer-readable storage media can\ncomprise, but are not limited to, read only memory, programmable read only memory, electrically programmable read only memory, electrically erasable read only memory, flash memory or other memory technology, compact disk read only memory, digital\nversatile disk or other optical disk storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or other tangible media which can be used to store desired information.  In this regard, the term \"tangible\" herein\nas may be applied to storage, memory or computer-readable media, is to be understood to exclude only propagating intangible signals per se as a modifier and does not relinquish coverage of all standard storage, memory or computer-readable media that are\nnot only propagating intangible signals per se.  In an aspect, tangible media can comprise non-transitory media wherein the term \"non-transitory\" herein as may be applied to storage, memory or computer-readable media, is to be understood to exclude only\npropagating transitory signals per se as a modifier and does not relinquish coverage of all standard storage, memory or computer-readable media that are not only propagating transitory signals per se.  Computer-readable storage media can be accessed by\none or more local or remote computing devices, e.g., via access requests, queries or other data retrieval protocols, for a variety of operations with respect to the information stored by the medium.  As such, for example, a computer-readable medium can\ncomprise executable instructions stored thereon that, in response to execution, can cause a system comprising a processor to perform operations, comprising determining, by a RAN device (110-130, 210-237, 310-338, 410-434, 540, etc.), a hierarchical\nclass, a next state, satisfaction of a current use rule for the RAN device itself, or of a neighboring RAN device, and employing those determinations to effect a state transition for the RAN device.\nCommunications media typically embody computer-readable instructions, data structures, program modules or other structured or unstructured data in a data signal such as a modulated data signal, e.g., a carrier wave or other transport mechanism,\nand comprises any information delivery or transport media.  The term \"modulated data signal\" or signals refers to a signal that has one or more of its characteristics set or changed in such a manner as to encode information in one or more signals.  By\nway of example, and not limitation, communication media comprise wired media, such as a wired network or direct-wired connection, and wireless media such as acoustic, RF, infrared and other wireless media.\nIt can be noted that FIG. 10 describes software that acts as an intermediary between users and computer resources described in suitable operating environment 1000.  Such software comprises an operating system 1028.  Operating system 1028, which\ncan be stored on disk storage 1024, acts to control and allocate resources of computer system 1012.  System applications 1030 take advantage of the management of resources by operating system 1028 through program modules 1032 and program data 1034 stored\neither in system memory 1016 or on disk storage 1024.  It is to be noted that the disclosed subject matter can be implemented with various operating systems or combinations of operating systems.\nA user can enter commands or information into computer 1012 through input device(s) 1036.  In some embodiments, a user interface can allow entry of user preference information, etc., and can be embodied in a touch sensitive display panel, a\nmouse/pointer input to a graphical user interface (GUI), a command line controlled interface, etc., allowing a user to interact with computer 1012.  Input devices 1036 comprise, but are not limited to, a pointing device such as a mouse, trackball,\nstylus, touch pad, keyboard, microphone, joystick, game pad, satellite dish, scanner, TV tuner card, digital camera, digital video camera, web camera, cell phone, smartphone, tablet computer, etc. These and other input devices connect to processing unit\n1014 through system bus 1018 by way of interface port(s) 1038.  Interface port(s) 1038 comprise, for example, a serial port, a parallel port, a game port, a universal serial bus, an infrared port, a Bluetooth port, an IP port, or a logical port\nassociated with a wireless service, etc. Output device(s) 1040 use some of the same type of ports as input device(s) 1036.\nThus, for example, a universal serial bus port can be used to provide input to computer 1012 and to output information from computer 1012 to an output device 1040.  Output adapter 1042 is provided to illustrate that there are some output devices\n1040 like monitors, speakers, and printers, among other output devices 1040, which use special adapters.  Output adapters 1042 comprise, by way of illustration and not limitation, video and sound cards that provide means of connection between output\ndevice 1040 and system bus 1018.  It should be noted that other devices and/or systems of devices provide both input and output capabilities such as remote computer(s) 1044.\nComputer 1012 can operate in a networked environment using logical connections to one or more remote computers, such as remote computer(s) 1044.  Remote computer(s) 1044 can be a personal computer, a server, a router, a network PC, cloud\nstorage, a cloud service, code executing in a cloud-computing environment, a workstation, a microprocessor based appliance, a peer device, or other common network node and the like, and typically comprises many or all of the elements described relative\nto computer 1012.  A cloud computing environment, the cloud, or other similar terms can refer to computing that can share processing resources and data to one or more computer and/or other device(s) on an as needed basis to enable access to a shared pool\nof configurable computing resources that can be provisioned and released readily.  Cloud computing and storage solutions can store and/or process data in third-party data centers which can leverage an economy of scale and can view accessing computing\nresources via a cloud service in a manner similar to a subscribing to an electric utility to access electrical energy, a telephone utility to access telephonic services, etc.\nFor purposes of brevity, only a memory storage device 1046 is illustrated with remote computer(s) 1044.  Remote computer(s) 1044 is logically connected to computer 1012 through a network interface 1048 and then physically connected by way of\ncommunication connection 1050.  Network interface 1048 encompasses wire and/or wireless communication networks such as local area networks and wide area networks.  Local area network technologies comprise fiber distributed data interface, copper\ndistributed data interface, Ethernet, Token Ring and the like.  Wide area network technologies comprise, but are not limited to, point-to-point links, circuit-switching networks like integrated services digital networks and variations thereon, packet\nswitching networks, and digital subscriber lines.  As noted below, wireless technologies may be used in addition to or in place of the foregoing.\nCommunication connection(s) 1050 refer(s) to hardware/software employed to connect network interface 1048 to bus 1018.  While communication connection 1050 is shown for illustrative clarity inside computer 1012, it can also be external to\ncomputer 1012.  The hardware/software for connection to network interface 1048 can comprise, for example, internal and external technologies such as modems, comprising regular telephone grade modems, cable modems and digital subscriber line modems,\nintegrated services digital network adapters, and Ethernet cards.\nThe above description of illustrated embodiments of the subject disclosure, comprising what is described in the Abstract, is not intended to be exhaustive or to limit the disclosed embodiments to the precise forms disclosed.  While specific\nembodiments and examples are described herein for illustrative purposes, various modifications are possible that are considered within the scope of such embodiments and examples, as those skilled in the relevant art can recognize.\nIn this regard, while the disclosed subject matter has been described in connection with various embodiments and corresponding Figures, where applicable, it is to be understood that other similar embodiments can be used or modifications and\nadditions can be made to the described embodiments for performing the same, similar, alternative, or substitute function of the disclosed subject matter without deviating therefrom.  Therefore, the disclosed subject matter should not be limited to any\nsingle embodiment described herein, but rather should be construed in breadth and scope in accordance with the appended claims below.\nAs it employed in the subject specification, the term \"processor\" can refer to substantially any computing processing unit or device comprising, but not limited to comprising, single-core processors; single-processors with software multithread\nexecution capability; multi-core processors; multi-core processors with software multithread execution capability; multi-core processors with hardware multithread technology; parallel platforms; and parallel platforms with distributed shared memory. \nAdditionally, a processor can refer to an integrated circuit, an application specific integrated circuit, a digital signal processor, a field programmable gate array, a programmable logic controller, a complex programmable logic device, a discrete gate\nor transistor logic, discrete hardware components, or any combination thereof designed to perform the functions described herein.  Processors can exploit nano-scale architectures such as, but not limited to, molecular and quantum-dot based transistors,\nswitches and gates, in order to optimize space usage or enhance performance of user equipment.  A processor may also be implemented as a combination of computing processing units.\nAs used in this application, the terms \"component,\" \"system,\" \"platform,\" \"layer,\" \"selector,\" \"interface,\" and the like are intended to refer to a computer-related entity or an entity related to an operational apparatus with one or more\nspecific functionalities, wherein the entity can be either hardware, a combination of hardware and software, software, or software in execution.  As an example, a component may be, but is not limited to being, a process running on a processor, a\nprocessor, an object, an executable, a thread of execution, a program, and/or a computer.  By way of illustration and not limitation, both an application running on a server and the server can be a component.  One or more components may reside within a\nprocess and/or thread of execution and a component may be localized on one computer and/or distributed between two or more computers.  In addition, these components can execute from various computer readable media having various data structures stored\nthereon.  The components may communicate via local and/or remote processes such as in accordance with a signal having one or more data packets (e.g., data from one component interacting with another component in a local system, distributed system, and/or\nacross a network such as the Internet with other systems via the signal).  As another example, a component can be an apparatus with specific functionality provided by mechanical parts operated by electric or electronic circuitry, which is operated by a\nsoftware or firmware application executed by a processor, wherein the processor can be internal or external to the apparatus and executes at least a part of the software or firmware application.  As yet another example, a component can be an apparatus\nthat provides specific functionality through electronic components without mechanical parts, the electronic components can comprise a processor therein to execute software or firmware that confers at least in part the functionality of the electronic\ncomponents.\nIn addition, the term \"or\" is intended to mean an inclusive \"or\" rather than an exclusive \"or.\" That is, unless specified otherwise, or clear from context, \"X employs A or B\" is intended to mean any of the natural inclusive permutations.  That\nis, if X employs A; X employs B; or X employs both A and B, then \"X employs A or B\" is satisfied under any of the foregoing instances.  Moreover, articles \"a\" and \"an\" as used in the subject specification and annexed drawings should generally be\nconstrued to mean \"one or more\" unless specified otherwise or clear from context to be directed to a singular form.  Moreover, the use of any particular embodiment or example in the present disclosure should not be treated as exclusive of any other\nparticular embodiment or example, unless expressly indicated as such, e.g., an first embodiment that has aspect A and a second embodiment that has aspect B does not preclude a third embodiment that has aspect A and aspect B. The use of granular examples\nand embodiments is intended to simplify understanding of certain features, aspects, etc., of the disclosed subject matter and is not intended to limit the disclosure to said granular instances of the disclosed subject matter or to illustrate that\ncombinations of embodiments of the disclosed subject matter were not contemplated at the time of actual or constructive reduction to practice.\nFurther, the term \"include\" is intended to be employed as an open or inclusive term, rather than a closed or exclusive term.  The term \"include\" can be substituted with the term \"comprising\" and is to be treated with similar scope, unless\notherwise explicitly used otherwise.  As an example, \"a basket of fruit including an apple\" is to be treated with the same breadth of scope as, \"a basket of fruit comprising an apple.\"\nMoreover, terms like \"user equipment (UE),\" \"mobile station,\" \"mobile,\" subscriber station,\" \"subscriber equipment,\" \"access terminal,\" \"terminal,\" \"handset,\" and similar terminology, refer to a wireless device utilized by a subscriber or user\nof a wireless communication service to receive or convey data, control, voice, video, sound, gaming, or substantially any data-stream or signaling-stream.  The foregoing terms are utilized interchangeably in the subject specification and related\ndrawings.  Likewise, the terms \"access point,\" \"base station,\" \"Node B,\" \"evolved Node B,\" \"eNodeB,\" \"home Node B,\" \"home access point,\" and the like, are utilized interchangeably in the subject application, and refer to a wireless network component or\nappliance that serves and receives data, control, voice, video, sound, gaming, or substantially any data-stream or signaling-stream to and from a set of subscriber stations or provider enabled devices.  Data and signaling streams can comprise packetized\nor frame-based flows.  Data or signal information exchange can comprise technology, such as, single user (SU) multiple-input and multiple-output (MIMO) (SU MIMO) radio(s), multiple user (MU) MIMO (MU MIMO) radio(s), long-term evolution (LTE), LTE\ntime-division duplexing (TDD), global system for mobile communications (GSM), GSM EDGE Radio Access Network (GERAN), Wi Fi, WLAN, WiMax, CDMA2000, LTE new radio-access technology (LTE-NX), massive MIMO systems, etc.\nAdditionally, the terms \"core-network\", \"core\", \"core carrier network\", \"carrier-side\", or similar terms can refer to components of a telecommunications network that typically provides some or all of aggregation, authentication, call control and\nswitching, charging, service invocation, or gateways.  Aggregation can refer to the highest level of aggregation in a service provider network wherein the next level in the hierarchy under the core nodes is the distribution networks and then the edge\nnetworks.  UEs do not normally connect directly to the core networks of a large service provider but can be routed to the core by way of a switch or radio access network.  Authentication can refer to determinations regarding whether the user requesting a\nservice from the telecom network is authorized to do so within this network or not.  Call control and switching can refer determinations related to the future course of a call stream across carrier equipment based on the call signal processing.  Charging\ncan be related to the collation and processing of charging data generated by various network nodes.  Two common types of charging mechanisms found in present day networks can be prepaid charging and postpaid charging.  Service invocation can occur based\non some explicit action (e.g. call transfer) or implicitly (e.g., call waiting).  It is to be noted that service \"execution\" may or may not be a core network functionality as third party network/nodes may take part in actual service execution.  A gateway\ncan be present in the core network to access other networks.  Gateway functionality can be dependent on the type of the interface with another network.\nFurthermore, the terms \"user,\" \"subscriber,\" \"customer,\" \"consumer,\" \"prosumer,\" \"agent,\" and the like are employed interchangeably throughout the subject specification, unless context warrants particular distinction(s) among the terms.  It\nshould be appreciated that such terms can refer to human entities or automated components (e.g., supported through artificial intelligence, as through a capacity to make inferences based on complex mathematical formalisms), that can provide simulated\nvision, sound recognition and so forth.\nAspects, features, or advantages of the subject matter can be exploited in substantially any, or any, wired, broadcast, wireless telecommunication, radio technology or network, or combinations thereof.  Non-limiting examples of such technologies\nor networks comprise broadcast technologies (e.g., sub-Hertz, extremely low frequency, very low frequency, low frequency, medium frequency, high frequency, very high frequency, ultra-high frequency, super-high frequency, extremely high frequency,\nterahertz broadcasts, etc.); Ethernet; X.25; powerline-type networking, e.g., Powerline audio video Ethernet, etc.; femtocell technology; Wi-Fi; worldwide interoperability for microwave access; enhanced general packet radio service; second generation\npartnership project (2G or 2GPP); third generation partnership project (3G or 3GPP); fourth generation partnership project (4G or 4GPP); long term evolution (LTE); fifth generation partnership project (5G or 5GPP); third generation partnership project\nuniversal mobile telecommunications system; third generation partnership project 2; ultra mobile broadband; high speed packet access; high speed downlink packet access; high speed uplink packet access; enhanced data rates for global system for mobile\ncommunication evolution radio access network; universal mobile telecommunications system terrestrial radio access network; or long term evolution advanced.  As an example, a millimeter wave broadcast technology can employ electromagnetic waves in the\nfrequency spectrum from about 30 GHz to about 300 GHz.  These millimeter waves can be generally situated between microwaves (from about 1 GHz to about 30 GHz) and infrared (IR) waves, and are sometimes referred to extremely high frequency (EHF).  The\nwavelength (.lamda.) for millimeter waves is typically in the 1-mm to 10-mm range.\nThe term \"infer\" or \"inference\" can generally refer to the process of reasoning about, or inferring states of, the system, environment, user, and/or intent from a set of observations as captured via events and/or data.  Captured data and events\ncan include user data, device data, environment data, data from sensors, sensor data, application data, implicit data, explicit data, etc. Inference, for example, can be employed to identify a specific context or action, or can generate a probability\ndistribution over states of interest based on a consideration of data and events.  Inference can also refer to techniques employed for composing higher-level events from a set of events and/or data.  Such inference results in the construction of new\nevents or actions from a set of observed events and/or stored event data, whether the events, in some instances, can be correlated in close temporal proximity, and whether the events and data come from one or several event and data sources.  Various\nclassification schemes and/or systems (e.g., support vector machines, neural networks, expert systems, Bayesian belief networks, fuzzy logic, and data fusion engines) can be employed in connection with performing automatic and/or inferred action in\nconnection with the disclosed subject matter.\nWhat has been described above includes examples of systems and methods illustrative of the disclosed subject matter.  It is, of course, not possible to describe every combination of components or methods herein.  One of ordinary skill in the art\nmay recognize that many further combinations and permutations of the claimed subject matter are possible.  Furthermore, to the extent that the terms \"includes,\" \"has,\" \"possesses,\" and the like are used in the detailed description, claims, appendices and\ndrawings such terms are intended to be inclusive in a manner similar to the term \"comprising\" as \"comprising\" is interpreted when employed as a transitional word in a claim.", "application_number": "16239515", "abstract": " Enabling a changing of a state of a device based on a hierarchical class\n     of the device is disclosed. Non-macro radio access network (RAN) devices\n     can be comprised in a network connected to a macro RAN device via a\n     gateway RAN device. The gateway RAN device can be connected to a field\n     RAN device via an intermediate RAN device. A state of a field RAN device\n     can be altered based on a criterion. In an embodiment the criterion can\n     be use of the hierarchical class of the RAN device, e.g., a field class\n     RAN device, by an active or idle UE. In an embodiment the criterion can\n     be use, by an active or idle UE, of another RAN device that is a logical\n     neighbor to the field RAN device. Altering the state can result in a\n     power savings or improved interference characteristics of the network.\n", "citations": ["8423094", "8923874", "9088949", "9185647", "9232479", "9264993", "9930617", "10182397", "20110237239", "20130028157", "20130295932", "20160262063", "20160335111"], "related": ["15909972", "15462775"]}]