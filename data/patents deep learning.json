[{"id": "20150248608", "patent_code": "10373047", "patent_name": "Deep convolutional neural networks for automated scoring of constructed\n     responses", "year": "2019", "inventor_and_country_data": " Inventors: \nHiggins; Derrick (Chicago, IL), Chen; Lei (Pennington, NJ), Heilman; Michael (Princeton, NJ), Zechner; Klaus (Princeton, NJ), Madnani; Nitin (Princeton, NJ)  ", "description": "FIELD\n The technology described in this patent document relates generally to computer-based test scoring systems and more particularly to a system and method for automatically scoring a constructed response using a deep convolutional neural network.\nBACKGROUND\n To evaluate the understanding, comprehension, or skill of students in an academic environment, the students are tested.  Typically, educators rely on multiple-choice examinations to evaluate students.  Multiple-choice examinations quickly\nprovide feedback to educators on the students' progress.  However, multiple-choice examinations may reward students for recognizing an answer versus constructing or recalling an answer.  Thus, another method of evaluating students utilizes test questions\nthat require a constructed response.  Examples of constructed responses include free-form, non-multiple choice responses such as essays, spoken responses, or show-your-work math responses.  For some educators, use of a constructed response examination is\npreferred versus a multiple-choice examination because the constructed response examination requires the student to understand and articulate concepts in the tested subject matter.  However, a length of time required to grade a constructed response may\nbe considerable.\nSUMMARY\n The present disclosure is directed to a computer-implemented method, system, and non-transitory computer-readable storage medium for automatically scoring a constructed response using a convolutional neural network.  In an example\ncomputer-implemented method of automatically scoring a constructed response using a convolutional neural network, a constructed response is obtained.  The constructed response may be a textual response or a spoken response, for example.  The constructed\nresponse is processed with a processing system to generate a plurality of first vectors containing numeric values, the plurality of first vectors being representative of the constructed response.  Using the processing system, a convolution layer of a\nconvolutional neural network is applied to the plurality of first vectors, where the convolution layer includes a first plurality of nodes that each receive input from the plurality of first vectors.  Using the processing system, a hidden layer of the\nconvolutional neural network is applied to an output of the convolution layer in order to generate a plurality of second vectors.  The hidden layer includes a second plurality of nodes that each receive input from at least one of the first plurality of\nnodes.  Using the processing system, an output layer of the convolutional neural network is applied to an output of a top-most hidden layer in order to generate a score for the constructed response.\n An example system for automatically scoring a constructed response using a convolutional neural network includes a processing system and computer-readable memory in communication with the processing system encoded with instructions for\ncommanding the processing system to execute steps.  In executing the steps, a constructed response is obtained.  The constructed response is processed to generate a plurality of first vectors containing numeric values, the plurality of first vectors\nbeing representative of the constructed response.  A convolution layer of a convolutional neural network is applied to the plurality of first vectors, where the convolution layer includes a first plurality of nodes that each receive input from the\nplurality of first vectors.  A hidden layer of the convolutional neural network is applied to an output of the convolution layer in order to generate a plurality of second vectors.  The hidden layer includes a second plurality of nodes that each receive\ninput from at least one of the first plurality of nodes.  An output layer of the convolutional neural network is applied to an output of a top-most hidden layer in order to generate a score for the constructed response.\n An example non-transitory computer-readable storage medium for automatically scoring a constructed response using a convolutional neural network comprises computer executable instructions which, when executed, cause a processing system to\nexecute steps.  In executing the steps, a constructed response is obtained.  The constructed response is processed to generate a plurality of first vectors containing numeric values, the plurality of first vectors being representative of the constructed\nresponse.  A convolution layer of a convolutional neural network is applied to the plurality of first vectors, where the convolution layer includes a first plurality of nodes that each receive input from the plurality of first vectors.  A hidden layer of\nthe convolutional neural network is applied to an output of the convolution layer in order to generate a plurality of second vectors.  The hidden layer includes a second plurality of nodes that each receive input from at least one of the first plurality\nof nodes.  An output layer of the convolutional neural network is applied to an output of a top-most hidden layer in order to generate a score for the constructed response.\n The present disclosure is also directed to a computer-implemented method, system, and non-transitory computer-readable storage medium for automatically scoring a constructed response generated by a user.  In an example computer-implemented\nmethod of automatically scoring a constructed response generated by a user, a constructed response generated by a user is received, the constructed response being based on a given item.  The constructed response is parsed with a processing system to\nidentify in the constructed response a plurality of multi-character sequences.  The plurality of multi-character sequences is processed with the processing system to generate a plurality of numerical vectors that is representative of the constructed\nresponse.  A convolutional neural network model associated with the given item is applied to the plurality of numerical vectors to determine a score for the constructed response.  The convolutional neural network model includes an input layer configured\nto receive the plurality of numerical vectors, the input layer being connected to a following layer of the convolutional neural network model via a first plurality of connections.  Each of the connections has an associated first weight and passes a\nportion of the plurality of numerical vectors to the following layer.  At least a subset of the connections have a same first weight.  The convolutional neural network model also includes a convolution layer including a plurality of nodes, where each\nnode of the convolution layer receives input from an immediately-preceding layer of the convolutional neural network model.  The convolutional neural network model also includes a hidden layer of nodes configured to receive inputs from the convolution\nlayer via a second plurality of connections, each of the second plurality of connections having an associated second weight.  At least a subset of the second plurality of connections have a same second weight.  Each node of the hidden layer generates an\noutput based on a weighted summation of received inputs.  The convolutional neural network model further includes an output layer connected to the hidden layer via a third plurality of connections, each of the third plurality of connections having an\nassociated third weight and passing one of the outputs from the hidden layer to the output layer.  The output layer is configured to generate a score for the constructed response based on the received outputs and the third weights.\n An example system for automatically scoring a constructed response generated by a user includes a processing system and a computer-readable memory in communication with the processing system.  The computer-readable memory is encoded with\ninstructions for commanding the processing system to execute steps.  In executing the steps, a constructed response generated by a user is received, the constructed response being based on a given item.  The constructed response is parsed to identify in\nthe constructed response a plurality of multi-character sequences.  The plurality of multi-character sequences is processed to generate a plurality of numerical vectors that is representative of the constructed response.  A convolutional neural network\nmodel associated with the given item is applied to the plurality of numerical vectors to determine a score for the constructed response.  The convolutional neural network model includes an input layer configured to receive the plurality of numerical\nvectors, the input layer being connected to a following layer of the convolutional neural network model via a first plurality of connections.  Each of the connections has an associated first weight and passes a portion of the plurality of numerical\nvectors to the following layer.  At least a subset of the connections have a same first weight.  The convolutional neural network model also includes a convolution layer including a plurality of nodes, where each node of the convolution layer receives\ninput from an immediately-preceding layer of the convolutional neural network model.  The convolutional neural network model also includes a hidden layer of nodes configured to receive inputs from the convolution layer via a second plurality of\nconnections, each of the second plurality of connections having an associated second weight.  At least a subset of the second plurality of connections have a same second weight.  Each node of the hidden layer generates an output based on a weighted\nsummation of received inputs.  The convolutional neural network model further includes an output layer connected to the hidden layer via a third plurality of connections, each of the third plurality of connections having an associated third weight and\npassing one of the outputs from the hidden layer to the output layer.  The output layer is configured to generate a score for the constructed response based on the received outputs and the third weights.\n In an example non-transitory computer-readable storage medium for automatically scoring a constructed response generated by a user, the computer-readable storage medium includes computer executable instructions which, when executed, cause a\nprocessing system to execute steps.  In executing the steps, a constructed response generated by a user is received, the constructed response being based on a given item.  The constructed response is parsed to identify in the constructed response a\nplurality of multi-character sequences.  The plurality of multi-character sequences are processed to generate a plurality of numerical vectors that is representative of the constructed response.  A convolutional neural network model associated with the\ngiven item is applied to the plurality of numerical vectors to determine a score for the constructed response.  The convolutional neural network model includes an input layer configured to receive the plurality of numerical vectors, the input layer being\nconnected to a following layer of the convolutional neural network model via a first plurality of connections.  Each of the connections has an associated first weight and passes a portion of the plurality of numerical vectors to the following layer.  At\nleast a subset of the connections have a same first weight.  The convolutional neural network model also includes a convolution layer including a plurality of nodes, where each node of the convolution layer receives input from an immediately-preceding\nlayer of the convolutional neural network model.  The convolutional neural network model also includes a hidden layer of nodes configured to receive inputs from the convolution layer via a second plurality of connections, each of the second plurality of\nconnections having an associated second weight.  At least a subset of the second plurality of connections have a same second weight.  Each node of the hidden layer generates an output based on a weighted summation of received inputs.  The convolutional\nneural network model further includes an output layer connected to the hidden layer via a third plurality of connections, each of the third plurality of connections having an associated third weight and passing one of the outputs from the hidden layer to\nthe output layer.  The output layer is configured to generate a score for the constructed response based on the received outputs and the third weights.\n The present disclosure is also directed to a computer-implemented method, system, and non-transitory computer-readable storage medium for constructing a model to automatically score a constructed response.  In an example computer-implemented\nmethod of constructing a model to automatically score a constructed response, a model associated with a given item is specified.  The model includes an input layer configured to receive a plurality of numerical vectors that is representative of a\nconstructed response, the input layer being connected to a following layer of the model via a first plurality of connections.  Each of the connections has an associated first weight and passes a portion of the plurality of numerical vectors to the\nfollowing layer.  The model also includes a first hidden layer of nodes configured to receive inputs from an immediately-preceding layer of the model via a second plurality of connections, each of the second plurality of connections having an associated\nsecond weight.  Each node of the first hidden layer generates an output based on a weighted summation of received inputs.  The model further includes an output layer connected to the first hidden layer via a third plurality of connections, each of the\nthird plurality of connections having an associated third weight and passing one of the outputs from the first hidden layer to the output layer.  The output layer is configured to generate a score for the constructed response based on the received\noutputs and the third weights.  Multiple reference responses are received for the given item.  Each reference response has been given a reference score, and the reference responses span a range of reference scores.  The multiple reference responses are\nprocessed to generate, for each reference response, a plurality of numerical vectors that is representative of the reference response.  The model is trained with a processing system using the numerical vectors representative of the reference responses\nand the reference scores to determine values for each of the first, second, and third weights.  The model is configured with the determined values of the first, second, and third weights to receive a plurality of numerical vectors that is representative\nof an actual constructed response to be scored so as to generate a score for the actual constructed response.\n An example system for constructing a model to automatically score a constructed response includes a processing system and a computer-readable memory in communication with the processing system.  The computer-readable memory is encoded with\ninstructions for commanding the processing system to execute steps.  In executing the steps, a model associated with a given item is specified.  The model includes an input layer configured to receive a plurality of numerical vectors that is\nrepresentative of a constructed response, the input layer being connected to a following layer of the model via a first plurality of connections.  Each of the connections has an associated first weight and passes a portion of the plurality of numerical\nvectors to the following layer.  The model also includes a first hidden layer of nodes configured to receive inputs from an immediately-preceding layer of the model via a second plurality of connections, each of the second plurality of connections having\nan associated second weight.  Each node of the first hidden layer generates an output based on a weighted summation of received inputs.  The model further includes an output layer connected to the first hidden layer via a third plurality of connections,\neach of the third plurality of connections having an associated third weight and passing one of the outputs from the first hidden layer to the output layer.  The output layer is configured to generate a score for the constructed response based on the\nreceived outputs and the third weights.  Multiple reference responses are received for the given item.  Each reference response has been given a reference score, and the reference responses span a range of reference scores.  The multiple reference\nresponses are processed to generate, for each reference response, a plurality of numerical vectors that is representative of the reference response.  The model is trained using the numerical vectors representative of the reference responses and the\nreference scores to determine values for each of the first, second, and third weights.  The model is configured with the determined values of the first, second, and third weights to receive a plurality of numerical vectors that is representative of an\nactual constructed response to be scored so as to generate a score for the actual constructed response.\n An example non-transitory computer-readable storage medium for constructing a model to automatically score a constructed response includes computer executable instructions.  When executed, the computer executable instructions cause a processing\nsystem to execute steps.  In executing the steps, a model associated with a given item is specified.  The model includes an input layer configured to receive a plurality of numerical vectors that is representative of a constructed response, the input\nlayer being connected to a following layer of the model via a first plurality of connections.  Each of the connections has an associated first weight and passes a portion of the plurality of numerical vectors to the following layer.  The model also\nincludes a first hidden layer of nodes configured to receive inputs from an immediately-preceding layer of the model via a second plurality of connections, each of the second plurality of connections having an associated second weight.  Each node of the\nfirst hidden layer generates an output based on a weighted summation of received inputs.  The model further includes an output layer connected to the first hidden layer via a third plurality of connections, each of the third plurality of connections\nhaving an associated third weight and passing one of the outputs from the first hidden layer to the output layer.  The output layer is configured to generate a score for the constructed response based on the received outputs and the third weights. \nMultiple reference responses are received for the given item.  Each reference response has been given a reference score, and the reference responses span a range of reference scores.  The multiple reference responses are processed to generate, for each\nreference response, a plurality of numerical vectors that is representative of the reference response.  The model is trained using the numerical vectors representative of the reference responses and the reference scores to determine values for each of\nthe first, second, and third weights.  The model is configured with the determined values of the first, second, and third weights to receive a plurality of numerical vectors that is representative of an actual constructed response to be scored so as to\ngenerate a score for the actual constructed response. BRIEF DESCRIPTION OF THE DRAWINGS\n FIG. 1 is a block diagram illustrating an example system for automatically scoring a constructed response generated by a user.\n FIG. 2A is a block diagram depicting an example architecture of a convolutional neural network configured to score a constructed response.\n FIG. 2B depicts an example convolutional neural network configured to automatically score a constructed response generated by a user.\n FIG. 3 is a flowchart depicting operations of an example method for constructing a model to automatically score a constructed response.\n FIG. 4 depicts an example convolutional neural network configured to automatically score a constructed response generated by a user.\n FIG. 5 is a flowchart depicting operations of an example computer-implemented method of automatically scoring a constructed response generated by a user.\n FIG. 6 depicts example operations for processing a constructed response to generate a plurality of numerical vectors that is representative of the constructed response.\n FIGS. 7A, 7B, and 7C depict example systems for automatically scoring a constructed response generated by a user.\nDETAILED DESCRIPTION\n FIG. 1 is a block diagram 100 illustrating an example computer-based system for automatically scoring a constructed response 102 generated by a user.  In an example, the constructed response 102 is a textual response that is provided by the user\nin response to a given item (e.g., a test question, task, etc.).  In an example, the given item includes a prompt that requests that the user generate a constructed response that is a short answer (e.g., a single word or phrase not comprising a complete\nsentence), one or more complete sentences, and/or an essay (e.g., comprising multiple sentences and/or paragraphs).  In an example, the given item includes a prompt that requests a spoken response from the user.  In an example, the given item is any type\nof open-ended question that requests a free-form, non-multiple choice response from the user.  In the example, the user is a human that generates the constructed response 102.\n In an example, the given item presented to the user is used in assessing the user's reading comprehension, as demonstrated by the constructed response 102.  Thus, in an example, the given item includes a passage of text, and a prompt requests\nthat the user summarize the passage of text.  In other examples, the given item is used in assessing the user's understanding of a concept, as demonstrated by the constructed response 102.  For example, the given item may include a prompt that requests\nthat the user explain a concept (e.g., \"Describe the process of osmosis.\").  In another example, the given item is used in assessing the user's vocabulary knowledge, as demonstrated by the constructed response 102.  For example, the given item may\ninclude a prompt that requests that the user define a word or write one or more sentences using the word.  Thus, a score 118 generated by the computer-based system of FIG. 1 may be intended to provide a measure of one or more of these abilities of the\nuser (e.g., the user's reading comprehension, understanding of a concept, vocabulary knowledge, etc.).\n The constructed response 102 generated by the user is received at a text processing module 104 of the computer-based system, in an embodiment.  Text processing performed on the constructed response 102 at the text processing module 104 may\ninclude parsing the constructed response 102 with a processing system to identify in the constructed response 102 a plurality of individual characters.  The text processing performed at the text processing module 104 may further include parsing the\nconstructed response 102 with the processing system to identify in the constructed response 102 a plurality of multi-character sequences.  In an example, the text processing module 104 identifies in the constructed response 102 sequences of characters\nhaving a fixed length.  Thus, in an example where the constructed response 102 includes nine (9) characters (e.g., \"dog cat bob\"), three 3-character sequences (e.g., \"dog,\" \"cat,\" \"bob\") may be identified in the constructed response 102, for instance.\n The text processing performed at the text processing module 104 may further include parsing the constructed response 102 with the processing system to identify in the constructed response 102 a plurality of words.  In an example, the text\nprocessing performed at the text processing module 104 may also include parsing the constructed response 102 with the processing system to identify in the constructed response 102 a plurality of multi-word sequences.  In an example, the text processing\nmodule 104 identifies in the constructed response 102 sequences of words having a fixed number of words.  Thus, in an example where the constructed response 102 includes nine (9) words (e.g., \"The boy and his dog walked down the street\"), three 3-word\nsequences (e.g., \"The boy and,\" \"his dog walked,\" \"down the street\") may be identified in the constructed response 102, for instance.\n The parsing performed at the text processing module 104 may be carried out using conventional automated, computer-based text parsing algorithms known to those of ordinary skill in the art.  Various other processing and analysis may be performed\non the constructed response 102 at the text processing module 104, such as correction of spelling errors in the constructed response 102, using conventional automated, computer-based algorithms known to those of ordinary skill in the art.  The use of\nspelling correction algorithms can be beneficial to improve the quality of the assessment being carried out by reducing the likelihood of complications in the assessment caused by the presence of spelling errors.\n An encoding module 105 receives an output of the text processing module 104, which may include a plurality of (i) single characters of the constructed response 102, (ii) multi-character sequences of the constructed response 102, (iii) words of\nthe constructed response 102, and/or (iv) multi-word sequences of the constructed response 102.  In an example, the encoding module 105 utilizes an encoding algorithm to transform one or more components of the output of the text processing module into\nnumerical vectors 108A, 108B, 108C.  In an example, such numerical vectors 108A, 108B, 108C comprise one-dimensional arrays, with each element of the arrays storing a number.  In an example, the encoding module 105 utilizes the \"word2vec\" tool known to\nthose of ordinary skill in the art.  The word2vec tool is configured to receive a text input (e.g., one or more words or multi-word sequences) and generate one or more numerical vectors as an output.\n The numerical vectors 108A, 108B, 108C comprise a vector-based representation of the constructed response 102.  In an example where the text processing module 104 generates an output that is a plurality of sequences of characters having a fixed\nlength, the encoding module 105 may transform each sequence of characters into a numerical vector.  Thus, for instance, in the example described above, where the three 3-character sequences \"dog,\" \"cat,\" and \"bob\" are identified in the constructed\nresponse 102, each of these 3-character sequences may be transformed into a numerical vector.  The numerical vectors for the three 3-character sequences may be the numerical vectors 108A, 108B, 108C illustrated in FIG. 1.\n The plurality of numerical vectors 108A, 108B, 108C that is representative of the constructed response 102 is received at a scoring engine 112.  The scoring engine 112 includes an automated scoring system configured to determine the score 118\nfor the constructed response 102.  The score 118 may be a point score (e.g., 87 points out of 110 points possible), a percentage or decimal score (e.g., 95% correct), a classification (e.g., \"high,\" \"medium,\" \"low,\" etc.), one or more probabilities\n(e.g., probability of 0.90 that user's understanding of a concept is high, probability of 0.07 that user's understanding is medium, and probability of 0.03 that user's understanding is low), or a ranking, for example.  In an example, the automated\nscoring system is a computer-based system for automatically scoring the constructed response 102 that requires no human intervention or minimal human intervention.  The scoring engine 112 may determine the score 118 for the constructed response 102 based\non the plurality of numerical vectors 108A, 108B, 108C that is representative of the constructed response 102 and a convolutional neural network model.\n A convolutional neural network is a type of neural network that uses many identical copies of the same neuron (i.e., the same node).  This allows the convolutional neural network to include a large number of neurons and to express\ncomputationally large models while keeping a number of parameters (e.g., parameter values that describe how the neurons behave, such as weights) that need to be learned fairly small.  Thus, as described below, connections between layers of the\nconvolutional neural network are associated with weights, and in examples, at least a subset of the connections share a same weighting factor.\n As described in further detail below, the convolutional neural network model may include multiple layers that are connected via a plurality of connections.  Each of the connections is used in passing information between layers of the\nconvolutional neural network model, in an example.  Further, each of the connections is associated with a weight (e.g., a weighting factor), and the weights of the convolutional neural network model are determined based on a plurality of human-scored\nconstructed responses 114, in an example.  In an example, the convolutional neural network model includes a convolutional neural network that is configured to receive the numerical vectors 108A, 108B, 108C and to determine the score 118 that provides a\nmeasure of one or more abilities of the user (e.g., the user's reading comprehension, understanding of a concept, vocabulary knowledge, etc.).\n To generate the convolutional neural network model used in the scoring engine 112, a model generation module 106 may be used.  The model generation module 106 receives the plurality of human-scored constructed responses 114 with associated\nscores for each of the constructed responses 114 and uses the plurality of human-scored constructed responses 114 to determine the weights for the model.  The model generation module 106 may perform text processing similar to that performed by the text\nprocessing module 104, and the model generation module 106 may also utilize an encoding algorithm to generate a plurality of numerical vectors for each of the human-scored constructed responses 114.  Thus, for example, the model generation module 106 may\nparse each of the human-scored constructed responses 114 to identify in the human-scored constructed response a plurality of (i) single characters, (ii) multi-character sequences, (iii) words, and/or (iv) multi-word sequences.  The encoding algorithm\nutilized by the model generation module 106 may be used to transform one or more of the single characters, multi-character sequences, words, and/or multi-word sequences into a plurality of numerical vectors that is representative of the human-scored\nconstructed response.\n In an example, the plurality of human-scored constructed responses 114 may span a range of reference scores, and the constructed responses 114 may be scored constructed responses that have been accepted as usable for training the convolutional\nneural network model.  In an example, the weights of the model are determined using an optimization procedure for training convolutional neural networks, such as stochastic gradient descent (SGD).  In an example, values for the weights are iteratively\nmodified in order to reduce a loss function associated with scoring accuracy, such as the root-mean-squared error.  As illustrated in FIG. 1, the model generation module 106 provides the model to the scoring engine 112.  With the convolutional neural\nnetwork model in place, the constructed response 102 may be scored by applying the convolutional neural network model as noted above.\n It should be appreciated that under the approaches described herein, one or more computer-based models are used in determining the score 118 for the constructed response 102.  As described above, such computer-based models may be trained via an\noptimization procedure for convolutional neural networks (e.g., stochastic gradient descent) in order to determine weights for the models.  By contrast, conventional human scoring techniques for determining a score for a constructed response would\ninclude none of these steps.  Conventional human scoring techniques involve one or more human graders reviewing constructed responses and manually assigning scores to the constructed responses.\n It should also be appreciated that the approaches described herein differ from conventional techniques for automated scoring of constructed responses.  Such conventional techniques are based on human-engineered features that are extracted from a\nconstructed response and then provided to a scoring model or scoring equation to determine a score for the constructed response.  The human-engineered features are developed by one or more humans based on the humans' belief that such features provide\nuseful information about a construct to be measured.  For example, in measuring a user's vocabulary knowledge as presented in a constructed response, a human may believe that an \"average word length\" feature provides useful information about the user's\nvocabulary knowledge.  This feature may then be used within the context of a conventional technique for automated scoring of a constructed response.  Such conventional techniques for automated scoring generally include a \"training\" phase whereby (i)\nhuman-engineered features are extracted from human-scored responses, and (ii) the extracted features and the scores assigned to the responses are used to train a scoring model using a machine-learning application (e.g., using linear regression, Support\nVector Machine (SVM), or other machine-learning methods).  The training of the model may include determining weights that are associated with the human-engineered features.  In the conventional techniques for automated scoring, the trained scoring model\ngenerates scores for the responses based on the extracted human-engineered features.\n By contrast, the approaches described herein do not utilize manually-defined, human-engineered features.  Rather, in the approaches described herein, any features used in scoring are engineered (i.e., chosen, designed) by the computer-based\nsystem (i.e., and not by a human), based on directly-observable elements of constructed responses (e.g., words, characters, sequences of characters, acoustic frames, etc.).  Thus, the approaches described herein enable responses to open-ended questions\nto be scored by allowing a computer-based system to induce the characteristics of appropriate response types from a scored sample of responses, rather than by requiring manual encoding of features or response patterns by humans.  The conventional\ntechniques described above may utilize features that encode the presence of sequences of characters and words, and one limitation of the conventional techniques is that such sequences of characters and words must be determined according to some\nheuristic, such as their frequency in a data set.  By contrast, the approaches described herein utilize a \"deep learning\" technique that obviates the need for heuristic encoding of an input feature space and allows more appropriate and general features\nto be induced by the computer-based system.\n As described in further detail below, the approaches described herein may utilize a non-linear mapping of an input (e.g., a plurality of numerical vectors that is representative of a constructed response) into an embedded space (e.g., a\ncompressed vector representation of the input) that is represented by a plurality of hidden nodes of a convolution neural network.  This mapping allows the computer-based system to design higher-level features representing meaningful and predictive\ncharacteristics of constructed responses.  Such higher-level features may be difficult or impossible for a human to design (e.g., features that encode character co-occurrence patterns in a response above and beyond strictly adjacent sequences, etc.). \nThe computer-engineered features are built upon simple, directly-observable elements of constructed responses (e.g., words, characters, acoustic frames, etc.) and are not based upon human-engineered features.  The approaches described herein, including\nthe computer-based system that operates directly on such directly-observable elements of constructed responses, varies from the conventional automated scoring techniques, which require that any directly-observable elements be pre-processed into\n\"features\" that are not directly-observable elements.  Such features include lower-level features (e.g., n-grams of a constructed response) and higher-order features (e.g., features that relate to a presence of a particular concept or pattern in a\nconstructed response).\n As described above with reference to FIG. 1, a convolutional neural network model utilized by the scoring engine 118 may include a convolutional neural network that is configured to receive the numerical vectors 108A, 108B, 108C and to determine\nthe score 118 for the constructed response 102.  A convolutional neural network includes multiple nodes organized in layers.  In an example, each node in a layer is connected with all nodes of an immediately preceding layer.  Each connection between\nnodes (which may be referred to as an \"edge\") has an associated strength or weight.  In an example, the weights of the connections encode the knowledge of the convolutional neural network.\n The convolutional neural network may be configured to generate multiple scores for the constructed response.  In an example, each of the multiple scores represents a probability.  For example, a given item may include a prompt that requests that\nthe user generate a constructed response that explains the process of osmosis.  The constructed response may be scored using the convolutional neural network to generate the multiple scores for the constructed response, where each of the multiple scores\nrepresents a probability (e.g., a first score represents a probability that the user's understanding of osmosis is very high, a second score represents a probability that the user's understanding of osmosis is high, a third score represents a probability\nthat the user's understanding of osmosis is average, a fourth score represents a probability that the user's understanding of osmosis is low, and a fifth score represents a probability that the user's understanding of osmosis is very low).\n In another example, each of the multiple scores represents a binary number that classifies the user (e.g., a first binary score has a first logical value if the convolutional neural network determines that the user's understanding of osmosis is\nvery high and a second logical value if the convolutional neural network determines that the user's understanding of osmosis is not very high, and so on).  The multiple scores may represent other values in other examples.  Further, in other examples, the\nconvolutional neural network generates a single score.  In these other examples, an output layer of the convolutional neural network may have a single node, in contrast to the multiple nodes of the output layer that may be used when multiple scores are\ngenerated.\n Input data is received at nodes of an input layer of the convolutional neural network, and the data passes through the convolutional neural network, layer-by-layer, until the data arrives at the nodes of the output layer.  There is no feedback\nbetween the layers of the network, in an example.  The layers of the convolutional neural network include the input layer, the output layer, one or more convolutional layers, and one or more hidden layers.  In an example, each convolution layer includes\na plurality of nodes (e.g., convolution units).  Hidden layers of the network may be referred to herein as \"intermediate layers.\" The convolutional neural network may have any number of hidden layers.\n The input layer of the convolutional neural network may be configured to receive a plurality of numerical vectors that is representative of a constructed response to be scored.  The input layer is connected to a following layer of the\nconvolutional neural network via a first plurality of connections.  Each of the connections has an associated first weight and passes a portion of the plurality of numerical vectors 202 to the following layer.  In an example, at least a subset of the\nconnections have a same first weight.\n A hidden layer of nodes of the convolutional neural network may be configured to receive inputs from the convolution layer of the network via a second plurality of connections.  In an example, the convolution layer is configured to store a\nnumerical vector that is based on a convolution of other numerical vectors generated by another layer of the convolutional neural network.  An example convolution layer is described in further detail below with reference to FIGS. 2A, 2B, and 4.  Each of\nthe second plurality of connections that connect the hidden layer to the convolution layer has an associated second weight.  In an example, at least a subset of the second plurality of connections have a same second weight.\n Each node of the hidden layer generates an output based on (i) a weighted summation of received inputs, and (ii) an activation function.  Specifically, a node of the hidden layer receives inputs from nodes of the convolution layer of the model\nvia one or more weighted connections.  The node of the hidden layer performs a weighted summation of the inputs, and the result of the weighted summation is then transformed by the activation function.  In an example, the functionality of the node of the\nhidden layer is described by:\n .sigma..function..times..times.  ##EQU00001## where n is the number of inputs received at the node of the hidden layer, x.sub.1 .  . . x.sub.n are values of the inputs received at the node, {w.sub.j, b.sub.j} are weights for an input j received\nat the node, and .sigma.  is the activation function.  .sigma.  is a nonlinear activation function, in an embodiment.  In an example, the nonlinear activation function may be a sigmoidal function represented by:\n .function.  ##EQU00002##\n The output layer of the convolutional neural network is connected to a top-most hidden layer of the network via a third plurality of connections.  Each of the third plurality of connections has an associated third weight and passes one of the\noutputs from the top-most hidden layer to the output layer.  The output layer is configured to generate one or more scores for the constructed response based on the outputs from the top-most hidden layer and the third weights.  In an example, each node\nof the output layer generates a score based on a weighted summation of inputs received from the top-most hidden layer and an activation function.\n In an example, each of the first, second, and third weights of the convolutional neural network are determined via an optimization procedure for neural networks that utilizes a plurality of human-scored constructed responses.  In an example, a\nplurality of numerical vectors representative of a human-scored constructed response is received at the input layer of the convolutional neural network.  The input data passes through the convolutional neural network and reaches the output layer.  The\noutputs of the convolutional neural network present at the nodes of the output layer are compared with the human-assigned scores for the constructed response.  On the basis of this comparison, one or more of the first, second, and third weights are\nmodified, such that if the same plurality of numerical vectors is subsequently received at the input layer, then the outputs of the convolutional neural network present at the nodes of the output layer will correspond better with the human-assigned\nscores.  Each of the plurality of human-scored constructed responses may be used to train the convolutional neural network in a similar manner.\n FIG. 2A is a block diagram depicting an example architecture of a convolutional neural network 251 configured to score a constructed response.  The convolutional neural network 251 may be configured to automatically score a constructed response\n(e.g., a textual response, a spoken response).  Thus, the convolutional neural network 251 may be applied to the constructed response, and an output of an output layer 256 may be a score 257 for the constructed response.  An input processing module 252\nprocesses the constructed response with a processing system to generate a plurality of first vectors containing numeric values.  The plurality of first vectors are representative of the constructed response.  In an example, the input processing module\n252 is configured to parse the constructed response to identify in the constructed response a plurality of words.  The input processing module 252 may then be configured to transform each of the words into a first vector containing numeric values (e.g.,\nusing an encoding algorithm).\n With reference to FIG. 2B, a constructed response including \"I am a boy living in\" may be parsed to identify each of the individual words included in the constructed response.  The input processing module 252 may apply an encoding algorithm to\ntransform each of the individual words into a first vector 220 containing numeric values.  The first vectors 220 may be input to an input layer 214 of a convolutional neural network model 213, as shown in FIG. 2B.  The input layer 214 may include a\nplurality of nodes or sets of nodes 222, where each node or set of nodes 222 is configured to receive a first vector 220 associated with a particular word of the constructed response, in an example.  The transforming of words into first vectors may be\nknown as \"word embedding\" or \"embedding at a word level.\" In examples where the constructed response is a spoken response, the input processing module 252 may generate the first vectors using framing (e.g., identifying acoustic frames in the constructed\nresponse) and additional signal processing (e.g., running a filter bank analysis to identify cepstral features associated with acoustic frames, etc.).  Regardless of whether the constructed response is a textual response or a spoken response, the first\nvectors 220 representing the response are received at the nodes or sets of nodes 222 of the input layer 214 of the network 213.\n As shown in FIG. 2A, the convolutional neural network 251 comprises at least one pair of layers 253, where each pair 253 includes a convolution layer 255 and a hidden layer 254.  The convolution layer 255 is applied to the plurality of first\nvectors generated by the input processing module 252.  The convolution layer 255 includes a first plurality of nodes (e.g., a first plurality of convolution units) that each receive input from the plurality of first vectors.  With reference to FIG. 2B, a\nconvolution layer 216 is applied to the plurality of first vectors 220 received at the input layer 214.  The convolution layer 216 includes a first plurality of nodes 224 that each receive input from the plurality of first vectors 220 via the input layer\n214.  In an example, each node of the convolution layer 216 applies a set of kernels that operate on the input received from the plurality of first vectors 220.\n In FIG. 2A, the hidden layer 254 is applied to an output of the convolution layer 255 in order to generate a plurality of second vectors.  In an example, the plurality of second vectors comprise a representation of informative patterns included\nin the constructed response that may be useful in scoring the constructed response.  The hidden layer 254 may include a second plurality of nodes (e.g., pooling units) that each receive input from at least one of the first plurality of nodes of the\nconvolution layer 255.  With reference to FIG. 2B, a hidden layer 218 is applied to an output of the convolution layer 216 in order to generate a plurality of second vectors.  The hidden layer 218 includes a second plurality of nodes (e.g., pooling\nunits) 226 that each receive input from at least one of the nodes 224 of the convolution layer 216.\n In an example, the hidden layer 218 applies a hidden layer function to one or more outputs of the convolution layer 216.  The hidden layer function may be an average or a maximum function or any other function that aggregates multiple values\ninto a single value.  Thus, as shown in the example of FIG. 2B, a node 226 of the hidden layer 218 applies a hidden layer function which is a maximum of outputs H.sub.1, H.sub.2, H.sub.3, H.sub.4 from the convolution layer 216.  With reference again to\nFIG. 2A, an output layer 256 of the convolutional neural network 251 may be applied to an output of a top-most hidden layer of the network 251 in order to generate the score 257 for the constructed response.  In an example, the output layer 256 may be\napplied to combine outputs from nodes of the top-most hidden layer.\n Although the example of FIG. 2A depicts a single pair 253 of hidden layer 254 and convolution layer 255, it should be appreciated that there may be multiple pairs of these layers in a convolutional neural network.  In an example, a second\nconvolution layer of the convolutional neural network 251 is applied to the plurality of second vectors generated by the hidden layer 254.  The second convolution layer may include a third plurality of nodes (e.g., convolution units) that each receive\ninput from the plurality of second vectors.\n Referring again to FIG. 2B, the convolution layer 216 is applied to the plurality of first vectors 220 using the input layer 214, which passes the first vectors 220 to the convolution layer 216 via a plurality of first connections.  Each of the\nfirst connections has an associated weight.  As illustrated in FIG. 2B, at least a subset of the plurality of first connections may have a same weight.  Such weights may be used in computing outputs (e.g., activations) of the convolution layer 216.  As\nnoted above, the hidden layer 218 may be applied to the outputs of the convolution layer 216.  Each node or set of nodes 226 of the hidden layer 218 receives input from nodes or sets of nodes 224 of the convolution layer via a plurality of second\nconnections.  In an example, at least a subset of the plurality of second connections may have a same weight.  Such weights may be used in computing outputs (e.g., activations) of the hidden layer 218.  Examples of functions that may be used by the\nhidden layer 218 include maximum, sum, and average functions.  In an example, a function used by the hidden layer 218 may be any function that can compute a single value from multiple values.\n It should be appreciated that the convolutional neural network 213 of FIG. 2B is exemplary only.  Thus, although the network 213 includes a single convolutional layer and a single hidden layer, in other examples, there may be a different number\nof convolutional layers and hidden layers.\n FIG. 3 is a flowchart 300 depicting operations of an example method for constructing a model to automatically score a constructed response.  The model may be, for example, a convolutional neural network model.  As described above with reference\nto FIG. 1, a model generation module 106 receives the plurality of human-scored constructed responses 114 and uses the plurality of human-scored constructed responses 114 to determine weights of the model.  The example operations depicted in the\nflowchart 300 of FIG. 3 provide further details on the building of such a model.\n At 302, a model associated with a given item is specified.  The model may be, for example, a convolutional neural network model.  The model includes an input layer configured to receive a plurality of numerical vectors that is representative of\na constructed response, the input layer being connected to a following layer of the model via a first plurality of connections.  Each of the connections has an associated first weight and passes a portion of the plurality of numerical vectors to the\nfollowing layer.  In an example, at least a subset of the connections have a same first weight.  The model also includes a convolution layer including a plurality of nodes, where each node of the convolution layer receives input from an\nimmediately-preceding layer of the convolutional neural network.  The model also includes a first intermediate layer (i.e., a first hidden layer) of nodes configured to receive inputs from the convolution layer of the model via a second plurality of\nconnections.  Each of the second plurality of connections has an associated second weight, and each node of the first intermediate layer generates an output based on a weighted summation of received inputs.  In an example, at least a subset of the second\nplurality of connections have a same second weight.\n The model further includes an output layer connected to the first intermediate layer via a third plurality of connections.  Each of the third plurality of connections has an associated third weight and passes one of the outputs from the first\nintermediate layer to the output layer.  The output layer is configured to generate a score for the constructed response based on the received outputs and the third weights.  In other examples, the model includes additional layers (e.g., additional\nhidden layers, additional convolution layers, etc.) and weights or different layers and weights.  An example model including layers in addition to the input layer, convolution layer, first intermediate layer, and output layer described above is described\nbelow with reference to FIG. 4.\n At 304, a plurality of reference responses for the given item are received, each reference response having been given a reference score.  The reference responses may span a range of reference scores, and the reference responses may have been\naccepted as usable for training the model.  In an example, the reference scores given to the plurality of reference responses are assigned by one or more human graders.  At 305, the multiple reference responses are processed to generate, for each\nreference response, a plurality of numerical vectors that is representative of the reference response.  The processing of a constructed response to generate a plurality of numerical vectors that is representative of the constructed response is described\nbelow with reference to FIG. 6.  At 306, the model is trained with a processing system using the numerical vectors representative of the reference responses and the reference scores to determine values for each of the first, second, and third weights. \nAs explained above, in other examples, the model may include additional layers and weights or different layers and weights.  Thus, the training of the model is used to determine values for the particular weights that are being used with the model, which\nmay include the first, second, and third weights or other weights.\n The training of the convolutional neural network model may include conducting a neural network optimization procedure (i.e., stochastic gradient descent) based on the numerical vectors representative of the reference responses and reference\nscore for each of the plurality of reference responses to determine the first, second, and third weights.  In another example, the training of the convolutional neural network model may include conducting another suitable numerical machine-learning\nanalysis based on the numerical vectors representative of the reference responses and reference score for each of the plurality of reference responses to determine the first, second and third weights.\n In the example of FIG. 3, the convolutional neural network model is trained using the plurality of reference responses that are based on the given item, and the convolutional neural network model is intended to be used thereafter in scoring\nconstructed responses that are based on the given item.  In this manner, the convolutional neural network model trained in the example of FIG. 3 may be item-specific.  In other examples, however, the convolutional neural network model may be trained\nusing data that does not relate to a given item, and the model may thereafter be used to score constructed responses that are based on various different items.  Such a convolutional neural network model is not item-specific.\n At 308, the model is configured with the determined values of the first, second, and third weights.  The model is then ready to be used for scoring, i.e., to receive a plurality of numerical vectors that is representative of an actual\nconstructed response from a user to be scored so as to generate a score for the actual constructed response.  In this manner, the model is thereafter configured to perform automated scoring on new constructed responses that need to be scored.\n FIG. 4 depicts an example convolutional neural network 400 configured to automatically score a constructed response generated by a user.  The example convolutional neural network 400 may be an example of a convolutional neural network model that\nmay be constructed according to the operations described above with reference to FIG. 3.  As shown in FIG. 4, the structure of the example convolutional neural network 400 includes a number of successive layers 406, 408, 410, 412, 414, 416.  In an\nexample, a layer feeds activation forward to successive layers by (i) multiplying activation values of a vector at a layer n of the network 400 by values of a weight matrix, and (ii) transforming the resultant values by a sigmoidal function.  The values\nof the weight matrix comprise the weights that are associated with the connections between the various layers of the network 400.  Such multiplying and transforming (described above) are performed to obtain an activation vector at a layer n+1 of the\nnetwork 400.  As described below, a convolution function used in obtaining a vector stored at a convolution layer 412 is an exception to this method of transforming activations between layers.\n The convolutional neural network 400 is configured to receive a plurality of numerical vectors that is representative of the constructed response.  To generate such numerical vectors, raw text 402 of the constructed response is parsed to\nidentify in the raw text 402 a plurality of multi-character sequences 404.  In an example, each of the multi-character sequences 404 has a fixed length (e.g., a same number of characters or a same number of words).  Each of the multi-character sequences\n404 is transformed into a numerical vector using an encoding algorithm.  Thus, in FIG. 4, a multi-character sequence \"Aaaa bb\" is transformed into a numerical vector, a multi-character sequence \"Jjjj kk\" is transformed into another numerical vector, and\nso on.  The transforming of each multi-character sequence into a numerical vector using an encoding algorithm is described in further detail below with reference to FIG. 6.\n An input layer 406 of the network 400 is configured to receive the numerical vectors generated for each of the multi-character sequences 404.  The input layer 406 comprises multiple sets of nodes 407A, 407B, 407C.  Each set of nodes of the input\nlayer 406 is configured to receive a numerical vector associated with one of the multi-character sequences 404.  In FIG. 4, a set of nodes 407A of the input layer 406 receives the numerical vector for the multi-character sequence \"Aaaa bb,\" a set of\nnodes 407B of the input layer 406 receives the numerical vector for the multi-character sequence \"Jjjj kk,\" and so on.\n A first hidden layer 408 transforms the numerical vectors received at the input layer 406 into hidden-layer representations.  The hidden-layer representations are intended to reflect generalizations implicit in the numerical vectors themselves. \nTo perform this transformation, the first hidden layer 408 includes multiple sets of nodes 409A, 409B, and 409C.  Each set of nodes of the first hidden layer 408 is directly connected to a set of nodes of the input layer 406.  These connections are\nweighted and enable the numerical vectors received at the input layer 406 to be passed to the first hidden layer 408.  Thus, as illustrated in FIG. 4, the set of nodes 409A of the first hidden layer 408 is directly connected to the set of nodes 407A of\nthe input layer 406, enabling the set of nodes 409A to receive the numerical vector received at the set of nodes 407A.  Each set of nodes of the first hidden layer 408 is configured to transform the numerical vector received from a corresponding set of\nnodes of the input layer 406 to generate a second numerical vector.  The transformation of the numerical vector may include (i) multiplying activation values of the numerical vector by appropriate weights associated with the connections between the input\nlayer 406 and the hidden layer 408 (e.g., weights stored in a weight matrix), and (ii) transforming the resultant values by a sigmoidal function.\n The hidden layer representation generated at the first hidden layer 408 may be successively transformed by higher-level filters to obtain a final embedding for each of the multi-character sequences 404 of the constructed response.  In FIG. 4, a\nsecond hidden layer 410 transforms the second numerical vectors generated at the first hidden layer 408.  To perform this transformation, the second hidden layer 410 includes multiple sets of nodes 411A, 411B, and 411C.  Each set of nodes of the second\nhidden layer 410 is directly connected to a set of nodes of the first hidden layer 408.  These connections are weighted and enable the second numerical vectors generated at the first hidden layer 408 to be passed to the second hidden layer 410.  Thus, as\nillustrated in FIG. 4, the set of nodes 411A of the second hidden layer 410 is directly connected to the set of nodes 409A of the first hidden layer 408, enabling the set of nodes 411A to receive the second numerical vector generated by the set of nodes\n409A.  Each set of nodes of the second hidden layer 410 is configured to transform the second numerical vector received from a corresponding set of nodes of the first hidden layer 408 to generate a third numerical vector.  The transformation of the\nsecond numerical vector may include (i) multiplying activation values of the second numerical vector by appropriate weights associated with the connections between the first hidden layer 408 and the second hidden layer 410, and (ii) transforming the\nresultant values by a sigmoidal function.\n In the example of FIG. 4, the sets of nodes 411A, 411B, 411C of the second hidden layer 408 generate the final embedding for each of the multi-character sequences 404 of the constructed response.  In other examples, additional hidden layers are\nused prior to generating the final embedding for each of the multi-character sequences 404 of the constructed response.  In other examples, only a single hidden layer is used in generating the final embedding for each of the multi-character sequences 404\nof the constructed response.\n The convolutional neural network 400 further includes a convolution layer 412 configured to store a fourth numerical vector.  The fourth numerical vector is based on a convolution of the third numerical vectors generated by the second hidden\nlayer 410.  Thus, in the example of FIG. 4, a convolution is applied to aggregate the final embeddings for each of the multi-character sequences 404 into a single vector representation for the constructed response as a whole.  In an example, the\nconvolution process includes the application of an element-wise maximum function to the third numerical vectors generated by the sets of nodes 411A, 411B, 411C of the second hidden layer 410.  In this example, the fourth numerical vector stored at the\nconvolution layer 412 is of the same length as each of the third numerical vectors, with the fourth numerical vector having a value for a given component that is equal to the maximum value of the corresponding components in the third numerical vectors.\n The remainder of the network 400 is constructed as a standard multi-layer perceptron.  As illustrated in FIG. 4, the multi-layer perceptron includes a third hidden layer 414, which may have a non-linear activation function.  The third hidden\nlayer 414 of the network 400 transforms the fourth numerical vector stored at the convolution layer 412.  To perform this transformation, the third hidden layer 414 includes multiple nodes configured to receive inputs from the convolution layer 412 via a\nplurality of weighted connections.  The inputs received at the third hidden layer 414 comprise portions of the fourth numerical vector stored at the convolution layer 412.  The nodes of the third hidden layer 414 are configured to transform the fourth\nnumerical vector.  The transformation of the fourth numerical vector may include (i) multiplying activation values of the fourth numerical vector by appropriate weights associated with the connections between the convolution layer 412 and the third\nhidden layer 414, and (ii) transforming the resultant values by a sigmoidal function.\n The transformed fourth numerical vector is provided as an input to the output layer 416, where the output layer 416 may have a non-linear activation function.  The output layer 416 transforms the numerical vector received from the third hidden\nlayer 414.  To perform this transformation, the output layer 416 includes one or more nodes configured to receive inputs from the third hidden layer 414 via a plurality of weighted connections.  The inputs received at the output layer 416 comprise\nportions of the transformed fourth numerical vector stored at the third hidden layer 414.  The one or more nodes of the output layer 416 are configured to transform the received numerical vector.  The transformation of this numerical vector may include\n(i) multiplying activation values of the numerical vector by appropriate weights associated with the connections between the third hidden layer 414 and the output layer 416, and (ii) transforming the resultant values by a sigmoidal function.\n The output of the network 400 at the output layer 416 is a representation of the network's score prediction for the constructed response.  The output layer 416 may be represented as a single node with a real-valued output (e.g., in instances\nwhere the task is represented as a regression) or as multiple nodes, each of which represents the probability associated with a response being assigned a given score on a scale.  This latter case utilizing the probabilities frames automated response\nscoring as a classification task.\n As explained above, the connections between the various layers of the network 400 are associated with weights.  The weights are primarily determined by training the network 400 based on a plurality of human-scored constructed responses.  In an\nexample, prior to training the network 400 using the plurality of human-scored constructed responses, an unsupervised pre-training step is performed to estimate one or more of the weights.  In the unsupervised pre-training step, values for weights are\nestimated not based on an error function associated with the ultimate discriminative criterion (e.g., scoring accuracy) but rather based on the values' ability to represent or reproduce characteristics of the input space.  The use of the unsupervised\npre-training step prior to the supervised training of the network 400 using the human-scored responses may allow for more efficient convergence of the convolutional neural network model.  Specifically, in an example, prior to training the network 400\nusing scored reference responses, an un-scored response (i.e., a response that has not been given a reference score) is received.  The un-scored response is processed to generate a plurality of numerical vectors that is representative of the un-scored\nresponse and that can be received at the input layer 406 of the network 400.\n The plurality of numerical vectors for the un-scored response are used to train a denoising auto-encoder or restricted Boltzmann machine (RBM) in order to estimate values of the weights for the connections between the input layer 406 and the\nfirst hidden layer 408.  Such weights should be suitable for encoding the regularities in the co-occurrence of directly-observable elements in the un-scored response.  The weights between the input layer 406 and the first hidden layer 408 are shared\nacross all sets of nodes (i.e., weights between the sets of nodes 407A and 409A of the input and first hidden layers 406, 408, respectively, are set to be the same as the weights between the sets of nodes 407B and 409B, and so on).  Multiple un-scored\nresponses can be used in estimating the values of the weights between the input layer 406 and the first hidden layer 408.\n Because the responses used to perform the pre-training step described above are not scored, such training is unsupervised.  Unsupervised pre-training can also be applied to successively-higher layers of the network 400.  Thus, after performing\nthe pre-training to estimate the weights for the connections between the input layer 406 and the first hidden layer 408, embedded representations for the un-scored response are determined at the first hidden layer 408.  Given these embedded\nrepresentations, the same process of unsupervised pre-training can be applied to estimate values of the weights for the connections between the first hidden layer 408 and the second hidden layer 410.  The embedded representations for the un-scored\nresponse determined at the first hidden layer 408 are used as input to a denoising auto-encoder or RBM.  The weights between the first hidden layer 408 and the second hidden layer 410 are shared across all sets of nodes (i.e., weights between the sets of\nnodes 409A and 411A of the first and second hidden layers 408, 410, respectively, are set to be the same as the weights between the sets of nodes 409B and 411B, and so on).  In examples where the network 400 utilizes additional hidden layers prior to\ngenerating the final embedding for each of the multi-character sequences 404, additional unsupervised pre-training steps may be applied.  It should be appreciated that the unsupervised pre-training may be performed on a per-layer basis.  In other words,\nrather than performing the pre-training on the network 400 as a whole, pre-training may instead be performed for one layer at a time.  Performing the pre-training on the single layer may be used to estimate weights for connections between two layers of\nthe network 400.\n Although the unsupervised pre-training described above may be beneficial in helping the network 400 to converge to a good solution, it is not a necessary step and may be omitted in some examples.  Following the optional unsupervised\npre-training, the full convolutional neural network 400 is trained using a standard optimization procedure for neural networks, such as stochastic gradient descent.  The training is a supervised training that uses human-scored reference responses.\n In the training, values for the various weights of the network 400 are iteratively modified in order to reduce a loss function associated with scoring accuracy, such as the root-mean-squared error.  This supervised training may include an\ninitial phase in which weights associated with (i) connections between the input layer 406 and the first hidden layer 408, and (ii) connections between the first hidden layer 408 and the second hidden layer 410 are held constant.  While holding these\nweights constant during the initial phase, only weights associated with connections between the convolution layer 412 and the third hidden layer 414 and connections between the third hidden layer 414 and the output layer 416 are trained.\n After the determining of the weights for the network 400, constructed responses may be scored by applying the network 400 to numerical vectors that are representative of the constructed responses.  It should be appreciated that the scoring of\nconstructed responses using the network 400 applies a \"deep learning\" approach, which reduces or eliminates the need for manual engineering of scoring features by humans.  Specifically, applying the network 400 to predict a score for a constructed\nresponse does not involve the extraction of human-engineered features from the constructed response.  Instead, during the supervised training step, the convolutional neural network 400 itself identifies important characteristics of human-scored reference\nresponses that are related to the classifications or scores assigned to the reference responses by human graders.  Representations of these characteristics are combined to produce an \"embedding\" or representation of the response in a latent space, and\naggregate information from the embedded representations (e.g., as represented by the fourth numerical vector stored by the convolution layer 412) is used to predict the score that the constructed response should receive.\n It should be appreciated that aspects of the convolutional neural network 400 may be modified in other examples.  As described above, hidden units within the network 400 may be stacked into multiple layers that successively transform the\nrepresentations of previous layers.  Stacked hidden layers may be used at the filter level applied to particular multi-character sequences 404 of the response (i.e., as illustrated by the stacked hidden layers 408, 410 of FIG. 4).  Additionally, multiple\nstacked hidden layers may feed into the output layer 416.  Thus, although the example of FIG. 4 depicts the multi-layer perceptron including the single hidden layer 414 feeding into the output layer 416, in other examples, multiple stacked hidden layers\nmay feed into the output layer 416.\n FIG. 5 is a flowchart 500 depicting operations of an example computer-implemented method of automatically scoring a constructed response generated by a user.  At 502, a constructed response generated by a user is received, the constructed\nresponse being based on a given item.  At 504, the constructed response is parsed with a processing system to identify in the constructed response a plurality of multi-character sequences.  At 506, the plurality of multi-character sequences are processed\nwith the processing system to generate a plurality of numerical vectors that is representative of the constructed response.  To illustrate steps 504 and 506 of FIG. 5, FIG. 6 illustrates aspects of processing a constructed response 626 to generate a\nplurality of numerical vectors 619, 648, 649 that is representative of the constructed response 626.  In an example, in both training and scoring phases, a convolutional neural network may require that a constructed response be transformed into a\nplurality of numerical vectors that is representative of the response (e.g., the convolutional neural network may not be able to accept raw text of the response, thus requiring the transformation of the constructed response into a plurality of numerical\nvectors that is representative of the response).  Thus, it should be appreciated that the generation of numerical vectors described below with reference to FIG. 6 may be applied in both training and scoring phases of the convolutional neural network\nmodel.\n In FIG. 6, at 620, the constructed response 626 is received.  The constructed response 626 includes only the raw text \"dogcatbob.\" At 621, the constructed response 626 is parsed to identify in the constructed response 626 a plurality of\nmulti-character sequences 627-629.  Each of the multi-character sequences may have a same number of characters.  In FIG. 6, the constructed response 626 has been parsed to identify a first multi-character sequence 627 (\"dog\"), a second multi-character\nsequence 628 (\"cat\"), and a third multi-character sequence 629 (\"bob\").  Each of these multi-character sequences 627-629 has a same number of characters.  In an example, the multi-character sequences identified in the step 621 are known as \"fixed-length\nwindows\" or \"text windows\" of the constructed response 626.\n At 622, each of the multi-character sequences 627-629 is parsed to identify in the multi-character sequence a plurality of single characters.  In FIG. 6, the first multi-character sequence 627 has been parsed to identify single characters (\"d,\"\n\"o,\" \"g\") 630-632 in the first multi-character sequence 627.  The second and third multi-character sequences 628, 629 are likewise parsed to identify the single characters 633-635 and 636-638, respectively.\n At 623, each of the single characters 630-638 is transformed into a numerical vector using an encoding algorithm.  In an example, the resulting numerical vectors comprise one-dimensional arrays that are configured to store numbers and that have\na fixed length.  In FIG. 6, the single character 630 (i.e., comprising the letter \"d\") is transformed into a numerical vector 639 of length three (i.e., a numerical vector with three elements, a first element storing a value \"0,\" a second element storing\na value \"0,\" and a third element storing a value \"1\").  The other single characters 631-638 are likewise transformed into numerical vectors 640-647 having the same fixed length of three.  Various encoding schemes may be used to generate the numerical\nvectors at step 623.  Thus, the resulting numerical vectors may be sparse (e.g., one-hot) representations of the input characters in one example, and in another example, the resulting numerical vectors may be distributed representations of the input\ncharacters obtained through another process (e.g., including, but not limited to, Latent Semantic Analysis or other dimensionality-reducing techniques).  Although the example numerical vectors 639-647 comprise elements storing binary values, in other\nexamples, the numerical vectors 639-647 may include elements storing natural numbers, integers, real numbers, etc.\n At 624, for each of the multi-character sequences 627-629, the numerical vectors for the associated single characters are joined end-to-end to generate the second numerical vectors 648, 649, 619.  In FIG. 6, the numerical vectors 639-641 for the\nsingle characters 630-632 that are associated with the multi-character sequence 627 are joined end-to-end (e.g., concatenated) to generate the second numerical vector 648 that is representative of the multi-character sequence 627.  Second numerical\nvectors 649, 619 that are representative of the multi-character sequences 628, 629, respectively, are generated in a similar manner.  All of the second numerical vectors 648, 649, 619 have a fixed length of nine in the example of FIG. 6.  Thus, in FIG.\n6, vectors 648, 649, 619 of fixed-length are obtained by concatenating the numerical vectors 639-647 associated with particular multi-character sequences 627-629 of the response 626.  In an example, each of the vectors 648, 649, 619 has a length of M*N,\nwhere M is the number of characters in each of the fixed-length multi-character sequences 627-629, and N is a vector length for each of the vectors 639-647 associated with the single characters 630-638.\n At 625, the numerical vectors 648, 649, 619 are provided as inputs to an input layer of a convolutional neural network.  In FIG. 6, the numerical vectors 648, 649, 619 are received at respective sets of nodes 616, 617, 618 of the input layer of\nthe convolutional neural network.  The sets of nodes 616, 617, 618 may be similar to the sets of nodes 407A, 407B, 407C of the input layer 406 illustrated in FIG. 4.\n It should be understood that the process described above with reference to FIG. 6 may be modified in other examples.  For example, in the example of FIG. 6, the windows 627-629 of the constructed response 626 comprise a fixed number of\ncharacters of the constructed response 626, and single characters 630-638 are the directly-observable elements of the response 626 used in generating the numerical vectors 639-647 that are subsequently joined end-to-end.  In other examples, however, the\nwindows may comprise a fixed number of words of the constructed response.  In these examples, individual words may be used as the direct observables of the constructed response that are used in generating numerical vectors that are subsequently joined\nend-to-end.  For example, a constructed response having nine words may be parsed to identify three windows of the response, each window including three words.  Each of the windows may be further parsed to identify in the window a plurality of individual\nwords.  Each of the individual words may be transformed into a numerical vector using an encoding algorithm.  Then, for each window of the response, the numerical vectors for the associated words may be joined end-to-end to generate a single numerical\nvector that is representative of the window.  These numerical vectors may then be provided as inputs to an input layer of a convolutional neural network.\n Additionally, although the approaches herein are described in terms of textual constructed responses, spoken responses may be scored in a similar manner.  In an example, cepstral features associated with acoustic frames could be used as the\ndirect observables of the response.  For example, a spoken response having a duration of 9 seconds may be parsed to identify three windows of the spoken response, with each window including 3 seconds of audio.  Each of the windows may be further parsed\nto identify in the window a plurality of cepstral features.  Each of the cepstral features may be transformed into a numerical vector using an encoding algorithm.  Then, for each window of the response, the numerical vectors for the associated cepstral\nfeatures may be joined end-to-end to generate a single numerical vector that is representative of the window.  These numerical vectors may then be provided as inputs to an input layer of a convolutional neural network.  Ultimately, any open-ended\nresponse type may be modeled using the deep learning techniques described herein, so long as the response can be segmented into a sequence of directly-observable events that can be represented as numerical vectors.\n Although examples described herein include transforming the constructed response into a plurality of numerical vectors that is representative of the constructed response, in other examples, this transformation is not necessary.  In these other\nexamples, inputs to the convolutional neural network could include, for example, raw character inputs or raw audio from a spoken response.\n With reference again to FIG. 5, after generating the plurality of numerical vectors that is associated with the constructed response, at 508, a convolutional neural network model associated with the given item is applied to the plurality of\nnumerical vectors to determine a score for the constructed response.  The convolutional neural network model includes an input layer configured to receive the plurality of numerical vectors, the input layer being connected to a following layer of the\nconvolutional neural network model via a first plurality of connections.  Each of the connections has an associated first weight and passes a portion of the plurality of numerical vectors to the following layer.  In an example, at least a subset of the\nconnections have a same first weight.  The convolutional neural network may also include a convolution layer including a plurality of nodes (e.g., convolution units).  Each node of the convolution layer may receive input from an immediately-preceding\nlayer of the convolutional neural network.  In an example, the immediately-preceding layer is the input layer, such that each node of the convolution layer receive input from the plurality of numerical vectors received at the input layer.\n The convolutional neural network model also includes an intermediate layer of nodes configured to receive inputs from the convolution layer of the convolutional neural network model via a second plurality of connections, each of the second\nplurality of connections having an associated second weight.  In an example, at least a subset of the second plurality of connections have a same second weight.  Each node of the intermediate layer generates an output based on a weighted summation of\nreceived inputs.  The convolutional neural network model further includes an output layer connected to the intermediate layer via a third plurality of connections, each of the third plurality of connections having an associated third weight and passing\none of the outputs from the intermediate layer to the output layer.  The output layer is configured to generate a score for the constructed response based on the received outputs and the third weights.\n FIGS. 7A, 7B, and 7C depict example systems for automatically scoring a constructed response generated by a user.  For example, FIG. 7A depicts an exemplary system 700 that includes a standalone computer architecture where a processing system\n702 (e.g., one or more computer processors located in a given computer or in multiple computers that may be separate and distinct from one another) includes one or more models 704 being executed on the processing system 702.  The processing system 702\nhas access to a computer-readable memory 706 in addition to one or more data stores 708.  The one or more data stores 708 may include human-scored responses 710 as well as un-scored responses 712.  The processing system 702 may be a distributed parallel\ncomputing environment, which may be used to handle very large-scale data sets.\n FIG. 7B depicts a system 720 that includes a client-server architecture.  One or more user PCs 722 access one or more servers 724 running one or more models 726 on a processing system 727 via one or more networks 728.  The one or more servers\n724 may access a computer-readable memory 730 as well as one or more data stores 732.  The one or more data stores 732 may contain human-scored responses 734 as well as un-scored responses 737.\n FIG. 7C shows a block diagram of exemplary hardware for a standalone computer architecture 750, such as the architecture depicted in FIG. 7A that may be used to contain and/or implement the program instructions of system embodiments of the\npresent disclosure.  A bus 752 may serve as the information highway interconnecting the other illustrated components of the hardware.  A processing system 754 labeled CPU (central processing unit) (e.g., one or more computer processors at a given\ncomputer or at multiple computers), may perform calculations and logic operations required to execute a program.  A non-transitory processor-readable storage medium, such as read only memory (ROM) 756 and random access memory (RAM) 758, may be in\ncommunication with the processing system 754 and may contain one or more programming instructions for performing the method for automatically scoring a constructed response generated by a user.  Optionally, program instructions may be stored on a\nnon-transitory computer-readable storage medium such as a magnetic disk, optical disk, recordable memory device, flash memory, or other physical storage medium.\n In FIGS. 7A, 7B, and 7C, computer readable memories 706, 730, 756, 758 or data stores 708, 732, 762, 764, 766 may include one or more data structures for storing and associating various data used in the example systems for automatically scoring\na constructed response generated by a user.  For example, a data structure may be used to relate connections of a convolutional neural network with associated weights.  Other aspects of the example systems for automatically scoring a constructed response\ngenerated by a user may be stored and associated in the one or more data structures (e.g., numerical measures, scores for human-scored reference responses, etc.).\n A disk controller 760 interfaces one or more optional disk drives to the system bus 752.  These disk drives may be external or internal floppy disk drives such as 762, external or internal CD-ROM, CD-R, CD-RW or DVD drives such as 764, or\nexternal or internal hard drives 766.  As indicated previously, these various disk drives and disk controllers are optional devices.\n Each of the element managers, real-time data buffer, conveyors, file input processor, database index shared access memory loader, reference data buffer and data managers may include a software application stored in one or more of the disk drives\nconnected to the disk controller 760, the ROM 756 and/or the RAM 758.  The processor 754 may access one or more components as required.\n A display interface 768 may permit information from the bus 752 to be displayed on a display 770 in audio, graphic, or alphanumeric format.  Communication with external devices may optionally occur using various communication ports 772.\n In addition to these computer-type components, the hardware may also include data input devices, such as a keyboard 773, or other input device 774, such as a microphone, remote control, pointer, mouse and/or joystick.\n Additionally, the methods and systems described herein may be implemented on many different types of processing devices by program code comprising program instructions that are executable by the device processing subsystem.  The software program\ninstructions may include source code, object code, machine code, or any other stored data that is operable to cause a processing system to perform the methods and operations described herein and may be provided in any suitable language such as C, C++,\nJAVA, for example, or any other suitable programming language.  Other implementations may also be used, however, such as firmware or even appropriately designed hardware configured to carry out the methods and systems described herein.\n The systems' and methods' data (e.g., associations, mappings, data input, data output, intermediate data results, final data results, etc.) may be stored and implemented in one or more different types of computer-implemented data stores, such as\ndifferent types of storage devices and programming constructs (e.g., RAM, ROM, Flash memory, flat files, databases, programming data structures, programming variables, IF-THEN (or similar type) statement constructs, etc.).  It is noted that data\nstructures describe formats for use in organizing and storing data in databases, programs, memory, or other computer-readable media for use by a computer program.\n The computer components, software modules, functions, data stores and data structures described herein may be connected directly or indirectly to each other in order to allow the flow of data needed for their operations.  It is also noted that a\nmodule or processor includes but is not limited to a unit of code that performs a software operation, and can be implemented for example as a subroutine unit of code, or as a software function unit of code, or as an object (as in an object-oriented\nparadigm), or as an applet, or in a computer script language, or as another type of computer code.  The software components and/or functionality may be located on a single computer or distributed across multiple computers depending upon the situation at\nhand.\n While the disclosure has been described in detail and with reference to specific embodiments thereof, it will be apparent to one skilled in the art that various changes and modifications can be made therein without departing from the spirit and\nscope of the embodiments.  Thus, it is intended that the present disclosure cover the modifications and variations of this disclosure provided they come within the scope of the appended claims and their equivalents.", "application_number": "14634203", "abstract": " Systems and methods are provided for automatically scoring a constructed\n     response. The constructed response is processed to generate a plurality\n     of numerical vectors that is representative of the constructed response.\n     A model is applied to the plurality of numerical vectors. The model\n     includes an input layer configured to receive the plurality of numerical\n     vectors, the input layer being connected to a following layer of the\n     model via a first plurality of connections. Each of the connections has a\n     first weight. An intermediate layer of nodes is configured to receive\n     inputs from an immediately-preceding layer of the model via a second\n     plurality of connections, each of the connections having a second weight.\n     An output layer is connected to the intermediate layer via a third\n     plurality of connections, each of the connections having a third weight.\n     The output layer is configured to generate a score for the constructed\n     response.\n", "citations": ["5778152", "20120088219", "20140288928"], "related": ["61945874"]}, {"id": "20160328646", "patent_code": "10373050", "patent_name": "Fixed point neural network based on floating point neural network\n     quantization", "year": "2019", "inventor_and_country_data": " Inventors: \nLin; Dexu (San Diego, CA), Annapureddy; Venkata Sreekanta Reddy (San Diego, CA), Howard; David Edward (San Diego, CA), Julian; David Jonathan (San Diego, CA), Majumdar; Somdeb (Mission Viejo, CA), Bell, II; William Richard (Carlsbad, CA)  ", "description": "BACKGROUND\n Field\n Certain aspects of the present disclosure generally relate to machine learning and, more particularly, to quantizing a floating point neural network to obtain a fixed point neural network.\n Background\n An artificial neural network, which may comprise an interconnected group of artificial neurons (e.g., neuron models), is a computational device or represents a method to be performed by the computational device.  Individual nodes in an\nartificial neural network may emulate biological neurons by taking input data and performing simple operations on the data.  The results of the simple operations performed on the input data are selectively passed on to other neurons.  The output of each\nnode is called its \"activation.\" Weight values are associated with each vector and node in the network, and these values constrain how input data is related to output data.  The weight values associated with individual nodes are also known as biases. \nTheses weight values are determined by the iterative flow of training data through the network (e.g., weight values are established during a training phase in which the network learns how to identify particular classes by their typical input data\ncharacteristics).\n Convolutional neural networks are a type of feed-forward artificial neural network.  Convolutional neural networks may include collections of neurons that each have a receptive field and that collectively tile an input space.  Convolutional\nneural networks (CNNs) have numerous applications.  In particular, CNNs have broadly been used in the areas of pattern recognition and classification.\n Deep learning architectures, such as deep belief networks and deep convolutional networks, are layered neural networks architectures in which the output of a first layer of neurons becomes an input to a second layer of neurons, the output of a\nsecond layer of neurons becomes an input to a third layer of neurons, and so on.  Deep neural networks may be trained to recognize a hierarchy of features and so they have increasingly been used in object recognition applications.  Like convolutional\nneural networks, computation in these deep learning architectures may be distributed over a population of processing nodes, which may be configured into one or more computational chains.  These multi-layered architectures may be trained one layer at a\ntime and may be fine-tuned using back-propagation.\n Other models are also available for object recognition.  For example, support vector machines (SVMs) are learning tools that can be applied for classification.  Support vector machines include a separating hyperplane (e.g., decision boundary)\nthat categorizes data.  The hyperplane is defined by supervised learning.  A desired hyperplane increases the margin of the training data.  In other words, the hyperplane should have the greatest minimum distance to the training examples.\n Although these solutions achieve excellent results on a number of classification benchmarks, their computational complexity can be prohibitively high.  Additionally, training of these models may be challenging.\nSUMMARY\n A method of quantizing a floating point machine learning network to obtain a fixed point machine learning network using a quantizer may include selecting at least one moment of an input distribution of the floating point machine learning\nnetwork.  The method may also include determining quantizer parameters for quantizing values of the floating point machine learning network based at least in part on the at least one selected moment of the input distribution of the floating point machine\nlearning network to obtain corresponding values of the fixed point machine learning network.\n An apparatus for quantizing a floating point machine learning network to obtain a fixed point machine learning network using a quantizer may include means for selecting at least one moment of an input distribution of the floating point machine\nlearning network.  The apparatus may also include means for determining quantizer parameters for quantizing values of the floating point machine learning network based at least in part on the at least one selected moment of the input distribution of the\nfloating point machine learning network to obtain corresponding values of the fixed point machine learning network.\n An apparatus for quantizing a floating point machine learning network to obtain a fixed point machine learning network using a quantizer may include a memory unit and at least one processor coupled to the memory unit.  The at least one processor\nmay be configured to select at least one moment of an input distribution of the floating point machine learning network.  The at least one processor may be further configured to determine quantizer parameters for quantizing values of the floating point\nmachine learning network based at least in part on the at least one selected moment of the input distribution of the floating point machine learning network to obtain corresponding values of the fixed point machine learning network.\n A non-transitory computer-readable medium having program code recorded thereon for quantizing a floating point machine learning network to obtain a fixed point machine learning network using a quantizer when executed by a processor may include\nprogram code to select at least one moment of an input distribution of the floating point machine learning network.  The non-transitory computer-readable medium may further include program code to determine quantizer parameters for quantizing values of\nthe floating point machine learning network based at least in part on the at least one selected moment of the input distribution of the floating point machine learning network to obtain corresponding values of the fixed point machine learning network.\n Additional features and advantages of the disclosure will be described below.  It should be appreciated by those skilled in the art that this disclosure may be readily utilized as a basis for modifying or designing other structures for carrying\nout the same purposes of the present disclosure.  It should also be realized by those skilled in the art that such equivalent constructions do not depart from the teachings of the disclosure as set forth in the appended claims.  The novel features, which\nare believed to be characteristic of the disclosure, both as to its organization and method of operation, together with further objects and advantages, will be better understood from the following description when considered in connection with the\naccompanying figures.  It is to be expressly understood, however, that each of the figures is provided for the purpose of illustration and description only and is not intended as a definition of the limits of the present disclosure. BRIEF\nDESCRIPTION OF THE DRAWINGS\n The features, nature, and advantages of the present disclosure will become more apparent from the detailed description set forth below when taken in conjunction with the drawings in which like reference characters identify correspondingly\nthroughout.\n FIG. 1 illustrates an example implementation of designing a neural network using a system-on-a-chip (SOC), including a general-purpose processor in accordance with certain aspects of the present disclosure.\n FIG. 2 illustrates an example implementation of a system in accordance with aspects of the present disclosure.\n FIG. 3A is a diagram illustrating a neural network in accordance with aspects of the present disclosure.\n FIG. 3B is a block diagram illustrating an exemplary deep convolutional network (DCN) in accordance with aspects of the present disclosure.\n FIG. 4 illustrates an exemplary probability distribution function showing a distribution to define a range of a fixed point representation.\n FIGS. 5A and 5B illustrate distributions of activation values and weights in different layers of an exemplary deep convolutional network.\n FIG. 6A illustrates an input distribution for an exemplary deep convolutional network.\n FIG. 6B illustrates a modified input distribution for an exemplary deep convolutional network according to aspects of the present disclosure.\n FIGS. 7A and 7B illustrate conversion of a first machine learning network into a second machine learning network by incorporating a mean value of a distribution of activation values into a network bias of the second machine learning network\naccording to aspect of the present disclosure.\n FIG. 8 illustrates a method of quantizing a floating point machine learning network to obtain a fixed point machine learning network using a quantizer according to aspects of the present disclosure.\n FIG. 9 illustrates a method of determining a step size for a second machine learning network according to a first machine learning network to reduce a computational complexity for the second machine learning network according to aspects of the\npresent disclosure.\nDETAILED DESCRIPTION\n The detailed description set forth below, in connection with the appended drawings, is intended as a description of various configurations and is not intended to represent the only configurations in which the concepts described herein may be\npracticed.  The detailed description includes specific details for the purpose of providing a thorough understanding of the various concepts.  However, it will be apparent to those skilled in the art that these concepts may be practiced without these\nspecific details.  In some instances, well-known structures and components are shown in block diagram form in order to avoid obscuring such concepts.\n Based on the teachings, one skilled in the art should appreciate that the scope of the disclosure is intended to cover any aspect of the disclosure, whether implemented independently of or combined with any other aspect of the disclosure.  For\nexample, an apparatus may be implemented or a method may be practiced using any number of the aspects set forth.  In addition, the scope of the disclosure is intended to cover such an apparatus or method practiced using other structure, functionality, or\nstructure and functionality in addition to or other than the various aspects of the disclosure set forth.  It should be understood that any aspect of the disclosure disclosed may be embodied by one or more elements of a claim.\n The word \"exemplary\" is used herein to mean \"serving as an example, instance, or illustration.\" Any aspect described herein as \"exemplary\" is not necessarily to be construed as preferred or advantageous over other aspects.\n Although particular aspects are described herein, many variations and permutations of these aspects fall within the scope of the disclosure.  Although some benefits and advantages of the preferred aspects are mentioned, the scope of the\ndisclosure is not intended to be limited to particular benefits, uses or objectives.  Rather, aspects of the disclosure are intended to be broadly applicable to different technologies, system configurations, networks and protocols, some of which are\nillustrated by way of example in the figures and in the following description of the preferred aspects.  The detailed description and drawings are merely illustrative of the disclosure rather than limiting, the scope of the disclosure being defined by\nthe appended claims and equivalents thereof.\n Quantization is a process of mapping a set of input values to a smaller set of values.  For example, the input values may be rounded to a given unit of precision.  Specifically, in one example, the conversion of floating point numbers to fixed\npoint numbers may be a process of quantization.\n In some artificial neural networks (ANNs), such as a deep convolutional network (DCN), quantization may be applied to activations of the normalization layer; weights, biases, and activations of the fully connected layer; and/or weights, biases,\nand activations of the convolution layer.  Furthermore, for DCNs, quantization may not be applied to the pooling layer if maximum pooling is specified; and/or the neuron layer if rectified linear units (ReLU) are specified.\n Aspects of the present disclosure are directed to improving quantization of the weights, biases, and/or activations in ANNs.  That is, aspects of the present disclosure are directed to quantizing weights, biases, and/or activation values in ANNs\nto improve the tradeoff between performance and complexity when implementing an ANN with fixed point numbers.\n FIG. 1 illustrates an example implementation of the aforementioned reduction of computation complexity by quantizing a floating point neural network to obtain a fixed point neural network using a system-on-a-chip (SOC) 100, which may include a\ngeneral-purpose processor (CPU) or multi-core general-purpose processors (CPUs) 102 in accordance with certain aspects of the present disclosure.  Variables (e.g., neural signals and synaptic weights), system parameters associated with a computational\ndevice (e.g., neural network with weights), delays, frequency bin information, and task information may be stored in a memory block associated with a neural processing unit (NPU) 108, in a memory block associated with a CPU 102, in a memory block\nassociated with a graphics processing unit (GPU) 104, in a memory block associated with a digital signal processor (DSP) 106, in a dedicated memory block 118, or may be distributed across multiple blocks.  Instructions executed at the general-purpose\nprocessor 102 may be loaded from a program memory associated with the CPU 102 or may be loaded from a dedicated memory block 118.\n The SOC 100 may also include additional processing blocks tailored to specific functions, such as a GPU 104, a DSP 106, a connectivity block 110, which may include fourth generation long term evolution (4G LTE) connectivity, unlicensed Wi-Fi\nconnectivity, USB connectivity, Bluetooth connectivity, and the like, and a multimedia processor 112 that may, for example, detect and recognize gestures.  In one implementation, the NPU is implemented in the CPU, DSP, and/or GPU.  The SOC 100 may also\ninclude a sensor processor 114, image signal processors (ISPs), and/or navigation 120, which may include a global positioning system.\n The SOC 100 may be based on an ARM instruction set.  In an aspect of the present disclosure, the instructions loaded into the general-purpose processor 102 may comprise code for quantizing a floating point neural network to obtain a fixed point\nneural network.  The instructions loaded into the general-purpose processor 102 may also comprise code for providing a fixed point representation when quantizing weights, biases and activation values in the network.\n FIG. 2 illustrates an example implementation of a system 200 in accordance with certain aspects of the present disclosure.  As illustrated in FIG. 2, the system 200 may have multiple local processing units 202 that may perform various operations\nof methods described herein.  Each local processing unit 202 may comprise a local state memory 204 and a local parameter memory 206 that may store parameters of a neural network.  In addition, the local processing unit 202 may have a local (neuron) model\nprogram (LMP) memory 208 for storing a local model program, a local learning program (LLP) memory 210 for storing a local learning program, and a local connection memory 212.  Furthermore, as illustrated in FIG. 2, each local processing unit 202 may\ninterface with a configuration processor unit 214 for providing configurations for local memories of the local processing unit, and with a routing connection processing unit 216 that provides routing between the local processing units 202.\n Deep learning architectures may perform an object recognition task by learning to represent inputs at successively higher levels of abstraction in each layer, thereby building up a useful feature representation of the input data.  In this way,\ndeep learning addresses a major bottleneck of traditional machine learning.  Prior to the advent of deep learning, a machine learning approach to an object recognition problem may have relied heavily on human engineered features, perhaps in combination\nwith a shallow classifier.  A shallow classifier may be a two-class linear classifier, for example, in which a weighted sum of the feature vector components may be compared with a threshold to predict to which class the input belongs.  Human engineered\nfeatures may be templates or kernels tailored to a specific problem domain by engineers with domain expertise.  Deep learning architectures, in contrast, may learn to represent features that are similar to what a human engineer might design, but through\ntraining.  Furthermore, a deep network may learn to represent and recognize new types of features that a human might not have considered.\n A deep learning architecture may learn a hierarchy of features.  If presented with visual data, for example, the first layer may learn to recognize relatively simple features, such as edges, in the input stream.  In another example, if presented\nwith auditory data, the first layer may learn to recognize spectral power in specific frequencies.  The second layer, taking the output of the first layer as input, may learn to recognize combinations of features, such as simple shapes for visual data or\ncombinations of sounds for auditory data.  For instance, higher layers may learn to represent complex shapes in visual data or words in auditory data.  Still higher layers may learn to recognize common visual objects or spoken phrases.\n Deep learning architectures may perform especially well when applied to problems that have a natural hierarchical structure.  For example, the classification of motorized vehicles may benefit from first learning to recognize wheels, windshields,\nand other features.  These features may be combined at higher layers in different ways to recognize cars, trucks, and airplanes.\n Neural networks may be designed with a variety of connectivity patterns.  In feed-forward networks, information is passed from lower to higher layers, with each neuron in a given layer communicating to neurons in higher layers.  A hierarchical\nrepresentation may be built up in successive layers of a feed-forward network, as described above.  Neural networks may also have recurrent or feedback (also called top-down) connections.  In a recurrent connection, the output from a neuron in a given\nlayer may be communicated to another neuron in the same layer.  A recurrent architecture may be helpful in recognizing patterns that span more than one of the input data chunks that are delivered to the neural network in a sequence.  A connection from a\nneuron in a given layer to a neuron in a lower layer is called a feedback (or top-down) connection.  A network with many feedback connections may be helpful when the recognition of a high-level concept may aid in discriminating the particular low-level\nfeatures of an input.\n Referring to FIG. 3A, the connections between layers of a neural network may be fully connected 302 or locally connected 304.  In a fully connected network 302, a neuron in a first layer may communicate its output to every neuron in a second\nlayer, so that each neuron in the second layer will receive input from every neuron in the first layer.  Alternatively, in a locally connected network 304, a neuron in a first layer may be connected to a limited number of neurons in the second layer.  A\nconvolutional network 306 may be locally connected, and is further configured such that the connection strengths associated with the inputs for each neuron in the second layer are shared (e.g., 308).  More generally, a locally connected layer of a\nnetwork may be configured so that each neuron in a layer will have the same or a similar connectivity pattern, but with connections strengths that may have different values (e.g., 310, 312, 314, and 316).  The locally connected connectivity pattern may\ngive rise to spatially distinct receptive fields in a higher layer, because the higher layer neurons in a given region may receive inputs that are tuned through training to the properties of a restricted portion of the total input to the network.\n Locally connected neural networks may be well suited to problems in which the spatial location of inputs is meaningful.  For instance, a network 300 designed to recognize visual features from a car-mounted camera may develop high layer neurons\nwith different properties depending on their association with the lower versus the upper portion of the image.  Neurons associated with the lower portion of the image may learn to recognize lane markings, for example, while neurons associated with the\nupper portion of the image may learn to recognize traffic lights, traffic signs, and the like.\n A DCN may be trained with supervised learning.  During training, a DCN may be presented with an image, such as a cropped image of a speed limit sign, and a \"forward pass\" may then be computed to produce an output 322.  The output 322 may be a\nvector of values corresponding to features such as \"sign,\" \"60,\" and \"100.\" The network designer may want the DCN to output a high score for some of the neurons in the output feature vector, for example the ones corresponding to \"sign\" and \"60\" as shown\nin the output 322 for a network 300 that has been trained.  Before training, the output produced by the DCN is likely to be incorrect, and so an error may be calculated between the actual output and the target output.  The weights of the DCN may then be\nadjusted so that the output scores of the DCN are more closely aligned with the target.\n To adjust the weights, a learning algorithm may compute a gradient vector for the weights.  The gradient may indicate an amount that an error would increase or decrease if the weight were adjusted slightly.  At the top layer, the gradient may\ncorrespond directly to the value of a weight connecting an activated neuron in the penultimate layer and a neuron in the output layer.  In lower layers, the gradient may depend on the value of the weights and on the computed error gradients of the higher\nlayers.  The weights may then be adjusted so as to reduce the error.  This manner of adjusting the weights may be referred to as \"back-propagation\" as it involves a \"backward pass\" through the neural network.\n In practice, the error gradient of weights may be calculated over a small number of examples, so that the calculated gradient approximates the true error gradient.  This approximation method may be referred to as stochastic gradient descent. \nStochastic gradient descent may be repeated until the achievable error rate of the entire system has stopped decreasing or until the error rate has reached a target level.\n After learning, the DCN may be presented with new images and a forward pass through the network may yield an output 322 that may be considered an inference or a prediction of the DCN.\n Deep belief networks (DBNs) are probabilistic models comprising multiple layers of hidden nodes.  DBNs may be used to extract a hierarchical representation of training data sets.  A DBN may be obtained by stacking up layers of Restricted\nBoltzmann Machines (RBMs).  An RBM is a type of artificial neural network that can learn a probability distribution over a set of inputs.  Because RBMs can learn a probability distribution in the absence of information about the class to which each input\nshould be categorized, RBMs are often used in unsupervised learning.  Using a hybrid unsupervised and supervised paradigm, the bottom RBMs of a DBN may be trained in an unsupervised manner and may serve as feature extractors, and the top RBM may be\ntrained in a supervised manner (on a joint distribution of inputs from the previous layer and target classes) and may serve as a classifier.\n Deep convolutional networks (DCNs) are networks of convolutional networks, configured with additional pooling and normalization layers.  DCNs have achieved state-of-the-art performance on many tasks.  DCNs can be trained using supervised\nlearning in which both the input and output targets are known for many exemplars and are used to modify the weights of the network by use of gradient descent methods.\n DCNs may be feed-forward networks.  In addition, as described above, the connections from a neuron in a first layer of a DCN to a group of neurons in the next higher layer are shared across the neurons in the first layer.  The feed-forward and\nshared connections of DCNs may be exploited for fast processing.  The computational burden of a DCN may be much less, for example, than that of a similarly sized neural network that comprises recurrent or feedback connections.\n The processing of each layer of a convolutional network may be considered a spatially invariant template or basis projection.  If the input is first decomposed into multiple channels, such as the red, green, and blue channels of a color image,\nthen the convolutional network trained on that input may be considered three-dimensional, with two spatial dimensions along the axes of the image and a third dimension capturing color information.  The outputs of the convolutional connections may be\nconsidered to form a feature map in the subsequent layer 318 and 320, with each element of the feature map (e.g., 320) receiving input from a range of neurons in the previous layer (e.g., 318) and from each of the multiple channels.  The values in the\nfeature map may be further processed with a non-linearity, such as a rectification, max(0,x).  Values from adjacent neurons may be further pooled, which corresponds to down sampling, and may provide additional local invariance and dimensionality\nreduction.  Normalization, which corresponds to whitening, may also be applied through lateral inhibition between neurons in the feature map.\n The performance of deep learning architectures may increase as more labeled data points become available or as computational power increases.  Modern deep neural networks are routinely trained with computing resources that are thousands of times\ngreater than what was available to a typical researcher just fifteen years ago.  New architectures and training paradigms may further boost the performance of deep learning.  Rectified linear units may reduce a training issue known as vanishing\ngradients.  New training techniques may reduce over-fitting and thus enable larger models to achieve better generalization.  Encapsulation techniques may abstract data in a given receptive field and further boost overall performance.\n FIG. 3B is a block diagram illustrating a deep convolutional network 350.  The deep convolutional network 350 may include multiple different types of layers based on connectivity and weight sharing.  As shown in FIG. 3B, the deep convolutional\nnetwork 350 includes multiple convolution blocks (e.g., C1 and C2).  Each of the convolution blocks may be configured with a convolution layer, a normalization layer (LNorm), and a pooling layer.  The convolution layers may include one or more\nconvolutional filters, which may be applied to the input data to generate a feature map.  Although only two convolution blocks are shown, the present disclosure is not so limiting, and instead, any number of convolutional blocks may be included in the\ndeep convolutional network 350 according to design preference.  The normalization layer may be used to normalize the output of the convolution filters.  For example, the normalization layer may provide whitening or lateral inhibition.  The pooling layer\nmay provide down sampling aggregation over space for local invariance and dimensionality reduction.\n The parallel filter banks, for example, of a deep convolutional network may be loaded on a CPU 102 or GPU 104 of an SOC 100, optionally based on an ARM instruction set, to achieve high performance and low power consumption.  In alternative\nembodiments, the parallel filter banks may be loaded on the DSP 106 or an ISP 116 of an SOC 100.  In addition, the DCN may access other processing blocks that may be present on the SOC, such as processing blocks dedicated to sensors 114 and navigation\n120.\n The deep convolutional network 350 may also include one or more fully connected layers (e.g., FC1 and FC2).  The deep convolutional network 350 may further include a logistic regression (LR) layer.  Between each layer of the deep convolutional\nnetwork 350 are weights (not shown) that are to be updated.  The output of each layer may serve as an input of a succeeding layer in the deep convolutional network 350 to learn hierarchical feature representations from input data (e.g., images, audio,\nvideo, sensor data and/or other input data) supplied at the first convolution block C1.\n In one configuration, a machine learning model, such as a neural model, is configured for quantizing a floating point neural network to obtain a fixed point neural network.  The model includes a reducing means and/or balancing means.  In one\naspect, the reducing means and/or balancing means may be the general-purpose processor 102, program memory associated with the general-purpose processor 102, memory block 118, local processing units 202, and or the routing connection processing units 216\nconfigured to perform the functions recited.  In another configuration, the aforementioned means may be any module or any apparatus configured to perform the functions recited by the aforementioned means.\n According to certain aspects of the present disclosure, each local processing unit 202 may be configured to determine parameters of the model based upon desired one or more functional features of the model, and develop the one or more functional\nfeatures towards the desired functional features as the determined parameters are further adapted, tuned and updated.\n Fixed Point Neural Network Based on Floating Point Neural Network Quantization\n Floating point representation of weights, biases, and/or activation values in artificial neural networks (ANN) may increase the complexity of hardware and/or software implementations of the network.  In some cases, a fixed point representation\nof a network, such as a deep convolutional network (DCN) or an artificial neural network, may provide an improved performance-complexity tradeoff.  Aspects of the present disclosure are directed to quantizing weights, biases, and activation values in\nartificial neural networks to improve the tradeoff between performance and complexity when implementing the network using fixed point numbers.\n Fixed point numbers may be specified for using less complex software and/or hardware designs at the cost of reduced accuracy because floating point numbers have a greater dynamic range compared to fixed point numbers.  Converting floating point\nnumbers to fixed point numbers through the process of quantization may decrease the complexity of hardware and/or software implementations.  The floating point numbers may assume a single-precision binary format including a sign bit, an 8-bit exponent,\nand a 23-bit fraction component.\n Aspects of the disclosure are directed to using the Q number format to represent fixed point numbers.  Still, other formats may be considered.  The Q number format is represented as Qm.n, where m is a number of bits for an integer part and n is\na number of bits for a fraction part.  In one configuration, m does not include a sign bit.  Each Qm.n format may use an m+n+1 bit signed integer container with n fractional bits.  In one configuration, the range is [-(2.sup.m), 2.sup.m-2.sup.-n)] and\nthe resolution is 2.sup.-n. For example, a Q14.1 format number may use sixteen bits.  In this example, the range is [-2.sup.14, 2.sup.14-2.sup.1] (e.g., [-16384.0, +16383.5]) and the resolution is 2.sup.1 (e.g., 0.5).\n In one configuration, an extension of the Q number format is specified to support instances where the resolution is greater than one or the maximum range is less than one.  In some cases, a negative number of fractional bits may be specified for\na resolution greater than one.  Additionally, a negative number of integer bits may be specified for a maximum range less than one.\n A deep convolutional network (DCN) is one example of an artificial neural network in which quantization may be applied according to aspects of the present disclosure.  Quantization may be applied to activations of the normalization layer;\nweights, biases, and activations of the fully connected layer; and/or weights, biases, and activations of the convolution layer.  Quantization, however, need not be applied to the pooling layer if max pooling is specified, and/or the neuron layer if\nrectified linear units (ReLU) are specified.  Aspects of the present disclosure are directed to improving quantization of the weights, biases, and/or activations in artificial neural networks by applying various optimizations.\n Quantization efficiency in artificial neural networks, according to aspects of the present disclosure, may be better understood by a review of quantization according to the probability distribution function 400 shown in FIG. 4.  For example, an\ninput to the quantizer may be uniformly distributed over [X.sub.min, X.sub.max], where X.sub.min and X.sub.max define the range of a fixed point representation.  When the input to the quantizer is uniformly distributed over [X.sub.min, X.sub.max],\nquantization noise is:\n .sigma..DELTA..times..times..sigma..times..times..times..times..times..ti- mes..times..times..times..DELTA..times..times.  ##EQU00001## signal power is:\n .sigma..times..times..sigma..times..times..times..times..times..times.  ##EQU00002## and a signal to quantization noise ratio (SQNR), assuming M is the number of integer bits is:\n .function..times..times..function..DELTA..times..times..times..times..tim- es..times.  ##EQU00003##\n FIGS. 5A and 5B illustrate distributions of activation values and weights in different layers of an exemplary deep convolutional network.  FIG. 5A shows the activation values for convolution layers zero to five (conv0, .  . . , conv5) and fully\nconnected layers one and two (fc1, fc2).  FIG. 5B shows the weights 550 for convolution layers one to five (conv1, .  . . , conv5) and fully connected layers one and two (fc1, fc2).  Application of quantization to the weights, biases, and activation\nvalues in artificial neural networks includes the determination of a step size.  For example, the step sizes of a symmetric uniform quantizer for Gaussian, Laplacian, and Gamma distributions may be calculated with a deterministic function of the standard\ndeviation of the input distribution, if it is assumed that the distributions have zero mean and unit variance.  Accordingly, aspects of the present disclosure are directed toward modifications of the weight and/or activation value calculations so that\nthe distributions have a zero mean (e.g., an approximately zero mean).  In one configuration, both the weights and activation values are assumed to have Gaussian distributions, however, other distributions are also contemplated.\n In aspects of the present disclosure, modifications of the weight and/or activation value calculations so that the distributions have a zero mean is performed by removing the mean.  In one configuration, quantization may be performed after the\nmean value (.mu.) of the input distribution is removed.  The removal of the mean value may have a greater effect if the distribution has a large mean value.\n FIG. 6A illustrates an input distribution 600 for an exemplary deep convolutional network.  In this example, the input distribution 600 includes a variance (.sigma.) and a mean value (.mu.).  Aspects of the present disclosure are directed\ntowards specifying a zero mean (.mu.=0) for the distributions of weights, biases and activation values, for example, as shown in FIG. 6B.\n FIG. 6B illustrates a modified input distribution 650 for an exemplary deep convolutional network.  In this configuration, the mean value (.mu.) is added to the standard deviation (variance (.sigma.)) to reduce computational overhead when\ndetermining the range of encoding, such that: Input std .sigma.'=|.mu.|+.sigma.  (4)\n The modified input distribution 650 of FIG. 6B, however, involves additional bits to represent the modified input distribution 650 having the zero mean.  That is, in this configuration, the complexity of fixed point computation is reduced at the\nexpense of an increase in a specified bit-width due to the modified input distribution 650.  For example, if |.mu.|&lt;.sigma., the increase in the specified bit-width is less than 1 extra bit for each quantity being quantized.\n FIGS. 7A and 7B illustrate a method of converting a first machine learning network into a second machine learning network by incorporating a mean value of a distribution of activation values of the first machine learning network into a network\nbias of the second machine learning network.  FIG. 7A illustrates an input distribution 700 of activation values for an exemplary deep convolutional network also having a mean (.mu.) and a variance (.sigma.).  In aspects of the present disclosure,\nmodifications of the input distribution 700 of the activation value calculations are performed so that the activation values have a zero mean.  In this aspect of the disclosure, modifications of the weight and/or activation value calculations is\nperformed by absorbing the mean value (.mu.) into a bias of the artificial neural network, for example, as shown in FIG. 7B.\n FIG. 7B illustrates a modified input distribution 750 of activation values for an exemplary deep convolutional network having the variance (.sigma.) and the mean (.mu.).  In this configuration, the mean (.mu.) is absorbed into the bias values of\nthe deep convolutional network model.  Absorbing the mean (.mu.) into the bias values may reduce the computational burden of mean removal.  Absorbing the mean (.mu.) into the bias of the deep convolutional network model essentially removes the mean\nactivations without an extra computational burden.  Calculation of the modified bias values may be performed as follows.\n In some cases, when an ANN has multiple layers, the activation value for the ith neuron in layer l+1 may be computed as follows: a.sub.i.sup.(l+1)=.SIGMA..sub.j=1.sup.Nw.sub.i,j.sup.(l+1)a.sub.j.sup.(l)- +b.sub.l.sup.(l+1) (5) where (l)\nrepresents the lth layer, N represents number of additions, w.sub.i,j represents the weight from neuron j in layer l to neuron i, and b.sub.i represents the bias.\n The activations, a.sub.j.sup.l, may be represented as the sum of the mean component .mu..sup.(l) and a zero-mean portion a.sub.j.sup.(l), then: a.sub.j.sup.(l)=.mu..sup.(l)+a.sub.j.sup.(l) (6)\na.sub.i.sup.(l+1)=.SIGMA..sub.j=1.sup.Nw.sub.i,j.sup.(l+1)a.sub.j.sup.(l)- +b.sub.i.sup.(l+1)+.mu..sup.(l).SIGMA..sub.j=1.sup.Nw.sub.i,j.sup.(l+1) New bias values (7)\n .mu..times..times..mu..times..mu.  ##EQU00004##\n In one configuration, the bias values are modified to specify a zero-mean throughout the network for the distributions of activations.  In addition, a non-linear activation function is modified when the mean value is incorporated into the\nnetwork bias.  In this configuration, the original bias values b.sub.i.sup.(l) are replaced with modified values {circumflex over (b)}.sub.i.sup.(l); {circumflex over (b)}.sub.i.sup.(l)=b.sub.i.sup.(l)+.mu..sup.(l-1).SIGMA..sub.j=1.sup.Nw.s-\nub.i,j.sup.(l)-.mu..sup.(l) (9)\n The resulting network has zero mean activations: a.sub.i.sup.(l+1)=.SIGMA..sub.j=1.sup.Nw.sub.i,j.sup.(l+1)a.sub.j.sup.(l)- +{circumflex over (b)}.sub.i.sup.(l+1) (10)\n For some layers, for example at the output of the ANN, non-zero-mean output activations may be specified, such that: a.sub.i.sup.(l+1)=.SIGMA..sub.j=1.sup.Nw.sub.i,j.sup.(l+1)a.sub.j.sup.(l)- +{tilde over (b)}.sub.i.sup.(l+1) (11) where {tilde\nover (b)}.sub.i.sup.(l)=b.sub.i.sup.(l)+.mu..sup.(l-1).SIGMA..sub.j=1.sup.Nw.s- ub.i,j.sup.(l) (12)\n Application of quantization to the weights, biases, and activation values in artificial neural networks may include a fixed point convertor.  In some cases, w.sub.i,j.sup.(l) and b.sub.i.sup.(l) are known for an ANN model.  Furthermore,\na.sub.j.sup.(l) may be measured from a floating point simulation.  Thus, in one configuration, the floating point to fixed point model convertor may compute and assign a new value to {circumflex over (b)}.sub.i.sup.(l) by measuring the mean activations,\n.mu..sup.(l), at each layer and calculating the new value as follows: {circumflex over (b)}.sub.i.sup.(l)=b.sub.i.sup.(l)+.mu..sup.(l-1).SIGMA..sub.j=1.sup.Nw.s- ub.i,j.sup.(l)-.mu..sup.(l) (13)\n In some cases, it is assumed that weights in the ANN have a substantially zero mean for each observation.  Thus, it is assumed that the aforementioned example is directed towards activation values.  Still, the aforementioned example may also be\napplied to weight values.  Application to weight values may include shifting activation values to create a zero-mean distribution for each layer of a fixed point machine learning network.  When the activations throughout the network are shifted to create\na zero-mean distribution for each layer, if a subsequent non-linear function is applied to the activations, the coordinate of the function is shifted by the same amount such that the output is a shift of the original output without a bias modification. \nThat is, the quantization process includes shifting a coordinate of any non-linear function applied to a shifted activation value by an amount corresponding to the shifted activation value.\n Application of quantization to the weights, biases, and activation values in artificial neural networks includes the determination of a step size.  For fixed point representations, the step sizes may be limited to powers of 2.  Determining the\nstep size that is a power of the 2 may correspond to determining the number of fractional bits in the fixed point number representation.  Equations for determining step size may be specified as: .sigma.'=|.mu.|+.sigma.  or .sigma.'=.sigma.  or .sigma.'=\n{square root over (.mu..sup.2+.sigma..sup.2)} (14) where .mu.  and .sigma.  are mean and standard deviation of the input to compute an effective sigma value .sigma.'.  Next, an effective step size is computed based on the effective sigma value .sigma.'\nas follows: S.sub.float=.sigma.'.times.C.sub.scaling(w).times..alpha.  (15) where S.sub.float is the computed step size in floating point, C.sub.scaling(w) is a scaling constant for bit-width w and .alpha.  is the adjustment factor for the step size. \nFinally, a closest power of 2 is determined for the step size as follows: n=-S.sub.float.left brkt-top.log.sub.2s.sub.float.right brkt-bot.  (16) where n is the number of fractional bits that may be specified to represent the quantizer input and 2.sup.-n\nmay be specified as the step size.  Other rounding functions may be used to obtain an integer n in addition to .left brkt-top.  .right brkt-bot.  (ceiling operation), including round ( ) and .left brkt-bot.  .right brkt-bot.  (floor operation).\n A scaling function and adjustment factor function C.sub.scaling(w) from EQUATION (15) may be specified as a function of the bit-width (W) according to table I.\n TABLE-US-00001 TABLE I Uniform Quantizer for Gaussian Distribution w 1 2 3 4 5 6 7 8 c.sub.scaling (w) 1.596 0.996 0.586 0.335 0.188 0.104 0.057 0.031\n The additional adjustment factor, .alpha., is a value that may be adjusted to improve the classification performance.  For example, .alpha.  may be specified to a value different from 1 in certain scenarios, such as: (1) the input distribution\nis not Gaussian (e.g., potentially longer tails); or (2) the calculated fixed point representation for the DCN does not agree with the representation calculated based on consideration of the signal to quantization noise ratio (SQNR).  In an exemplary DCN\nmodel directed towards scene detection, an a different from 1, such as .alpha.=1.5, improves performance.\n In addition, the step size adjustment factor .alpha.  may be specified differently throughout the model.  For example, .alpha.  may be specified individually for weights and activations of each layer.  In addition, the weights and biases may\nhave very different dynamic ranges.  For example, weights and biases may be specified to have different Q number representations and different bit-widths.  Additionally, the bit-width of weights and biases in the same layer may be the same.  In one\nconfiguration, for a given layer, weights have a format of Q 3.18 and biases have a format of Q 6.9.\n In one configuration, after quantizing the floating point model into a fixed point model, the fixed point network is fine-tuned via additional training to further improve the network performance.  Fine-tuning may include training via\nback-propagation.  Furthermore, the step size and Q number representations identified in the floating point to fixed point conversion may carry over to the fine-tuned network.  In this example, additional step-size optimization is not specified.  A\nquantizer for an ANN may maintain the fidelity of the network while reducing the use of resources.  For an exemplary network, there may be little to no difference in terms of accuracy between a 32-bit floating point network implementation compared with a\n16-bit fixed point network implementation.  A fixed point implementation of an ANN based on the disclosed quantizer design may reduce model size, processing time, memory bandwidth, and power consumption.\n FIG. 8 illustrates a method 800 of quantizing a floating point machine learning network to obtain a fixed point machine learning network using a quantizer according to aspects of the present disclosure.  In block 802, at least one moment of an\ninput distribution of a floating point machine learning network is selected.  The at least one moment of the input distribution of the floating point machine learning network may include a mean, a variance or other like moment of the input distribution. \nIn block 804, quantizer parameters for quantizing values of the floating point machine learning network are determined based on the selected moment of the input distribution of the floating point machine learning network.  At block 806, it is determined\nwhether additional moments are available for the input distribution.  If so, blocks 802 and 804 may be repeated for each moment of the input distribution including, for example, the mean, the variance or other like moments of the input distribution.\n In aspects of the present disclosure, the determining of the quantizer parameters for quantizing values of the floating point machine learning network is performed to obtain corresponding values of the fixed point machine learning network.  The\nquantizer parameters for quantizing values of the floating point machine learning network include a maximum dynamic encoding range, a quantizer step size, a bit width, a signed/unsigned indicator value or other like quantizing parameter of the fixed\npoint machine learning network.  In addition, the corresponding values of the fixed point machine learning network may include, but are not limited to, a bias, a weight, and/or activation values.\n FIG. 9 illustrates a method 900 for determining a step size for a second machine learning network (e.g., a fixed point neural network) according to a first machine learning network (e.g., a floating point neural network) to reduce a\ncomputational complexity for the second machine learning network according to aspects of the present disclosure.  Application of quantization to the weights, biases, and activation values in artificial neural networks includes the determination of a step\nsize.  In block 902, an effective sigma is computed.  For example, as shown in EQUATION (14) are an absolute value of a mean .mu.  of an input distribution is added to a standard deviation (.sigma.) of the input distribution to compute an effective sigma\nvalue (.sigma.').  In block 904, an effective step size (S.sub.float) is computed based on the effective sigma value (.sigma.'), for example, according to EQUATION (15).  In block 906, a closest power of two is determined for the step size, for example,\naccording to EQUATION (16).\n The various operations of methods described above may be performed by any suitable means capable of performing the corresponding functions.  The means may include various hardware and/or software component(s) and/or module(s), including, but not\nlimited to, a circuit, an application specific integrated circuit (ASIC), or processor.  Generally, where there are operations illustrated in the figures, those operations may have corresponding counterpart means-plus-function components with similar\nnumbering.\n As used herein, the term \"determining\" encompasses a wide variety of actions.  For example, \"determining\" may include calculating, computing, processing, deriving, investigating, looking up (e.g., looking up in a table, a database or another\ndata structure), ascertaining and the like.  Additionally, \"determining\" may include receiving (e.g., receiving information), accessing (e.g., accessing data in a memory) and the like.  Furthermore, \"determining\" may include resolving, selecting,\nchoosing, establishing and the like.\n As used herein, a phrase referring to \"at least one of\" a list of items refers to any combination of those items, including single members.  As an example, \"at least one of: a, b, or c\" is intended to cover: a, b, c, a-b, a-c, b-c, and a-b-c.\n The various illustrative logical blocks, modules and circuits described in connection with the present disclosure may be implemented or performed with a general-purpose processor, a digital signal processor (DSP), an application specific\nintegrated circuit (ASIC), a field programmable gate array signal (FPGA) or other programmable logic device (PLD), discrete gate or transistor logic, discrete hardware components or any combination thereof designed to perform the functions described\nherein.  A general-purpose processor may be a microprocessor, but in the alternative, the processor may be any commercially available processor, controller, microcontroller or state machine.  A processor may also be implemented as a combination of\ncomputing devices, e.g., a combination of a DSP and a microprocessor, a plurality of microprocessors, one or more microprocessors in conjunction with a DSP core, or any other such configuration.\n The steps of a method or algorithm described in connection with the present disclosure may be embodied directly in hardware, in a software module executed by a processor, or in a combination of the two.  A software module may reside in any form\nof storage medium that is known in the art.  Some examples of storage media that may be used include random access memory (RAM), read only memory (ROM), flash memory, erasable programmable read-only memory (EPROM), electrically erasable programmable\nread-only memory (EEPROM), registers, a hard disk, a removable disk, a CD-ROM and so forth.  A software module may comprise a single instruction, or many instructions, and may be distributed over several different code segments, among different programs,\nand across multiple storage media.  A storage medium may be coupled to a processor such that the processor can read information from, and write information to, the storage medium.  In the alternative, the storage medium may be integral to the processor.\n The methods disclosed herein comprise one or more steps or actions for achieving the described method.  The method steps and/or actions may be interchanged with one another without departing from the scope of the claims.  In other words, unless\na specific order of steps or actions is specified, the order and/or use of specific steps and/or actions may be modified without departing from the scope of the claims.\n The functions described may be implemented in hardware, software, firmware, or any combination thereof.  If implemented in hardware, an example hardware configuration may comprise a processing system in a device.  The processing system may be\nimplemented with a bus architecture.  The bus may include any number of interconnecting buses and bridges depending on the specific application of the processing system and the overall design constraints.  The bus may link together various circuits\nincluding a processor, machine-readable media, and a bus interface.  The bus interface may be used to connect a network adapter, among other things, to the processing system via the bus.  The network adapter may be used to implement signal processing\nfunctions.  For certain aspects, a user interface (e.g., keypad, display, mouse, joystick, etc.) may also be connected to the bus.  The bus may also link various other circuits such as timing sources, peripherals, voltage regulators, power management\ncircuits, and the like, which are well known in the art, and therefore, will not be described any further.\n The processor may be responsible for managing the bus and general processing, including the execution of software stored on the machine-readable media.  The processor may be implemented with one or more general-purpose and/or special-purpose\nprocessors.  Examples include microprocessors, microcontrollers, DSP processors, and other circuitry that can execute software.  Software shall be construed broadly to mean instructions, data, or any combination thereof, whether referred to as software,\nfirmware, middleware, microcode, hardware description language, or otherwise.  Machine-readable media may include, by way of example, random access memory (RAM), flash memory, read only memory (ROM), programmable read-only memory (PROM), erasable\nprogrammable read-only memory (EPROM), electrically erasable programmable Read-only memory (EEPROM), registers, magnetic disks, optical disks, hard drives, or any other suitable storage medium, or any combination thereof.  The machine-readable media may\nbe embodied in a computer-program product.  The computer-program product may comprise packaging materials.\n In a hardware implementation, the machine-readable media may be part of the processing system separate from the processor.  However, as those skilled in the art will readily appreciate, the machine-readable media, or any portion thereof, may be\nexternal to the processing system.  By way of example, the machine-readable media may include a transmission line, a carrier wave modulated by data, and/or a computer product separate from the device, all which may be accessed by the processor through\nthe bus interface.  Alternatively, or in addition, the machine-readable media, or any portion thereof, may be integrated into the processor, such as the case may be with cache and/or general register files.  Although the various components discussed may\nbe described as having a specific location, such as a local component, they may also be configured in various ways, such as certain components being configured as part of a distributed computing system.\n The processing system may be configured as a general-purpose processing system with one or more microprocessors providing the processor functionality and external memory providing at least a portion of the machine-readable media, all linked\ntogether with other supporting circuitry through an external bus architecture.  Alternatively, the processing system may comprise one or more neuromorphic processors for implementing the neuron models and models of neural systems described herein.  As\nanother alternative, the processing system may be implemented with an application specific integrated circuit (ASIC) with the processor, the bus interface, the user interface, supporting circuitry, and at least a portion of the machine-readable media\nintegrated into a single chip, or with one or more field programmable gate arrays (FPGAs), programmable logic devices (PLDs), controllers, state machines, gated logic, discrete hardware components, or any other suitable circuitry, or any combination of\ncircuits that can perform the various functionality described throughout this disclosure.  Those skilled in the art will recognize how best to implement the described functionality for the processing system depending on the particular application and the\noverall design constraints imposed on the overall system.\n The machine-readable media may comprise a number of software modules.  The software modules include instructions that, when executed by the processor, cause the processing system to perform various functions.  The software modules may include a\ntransmission module and a receiving module.  Each software module may reside in a single storage device or be distributed across multiple storage devices.  By way of example, a software module may be loaded into RAM from a hard drive when a triggering\nevent occurs.  During execution of the software module, the processor may load some of the instructions into cache to increase access speed.  One or more cache lines may then be loaded into a general register file for execution by the processor.  When\nreferring to the functionality of a software module below, it will be understood that such functionality is implemented by the processor when executing instructions from that software module.  Furthermore, it should be appreciated that aspects of the\npresent disclosure result in improvements to the functioning of the processor, computer, machine, or other system implementing such aspects.\n If implemented in software, the functions may be stored or transmitted over as one or more instructions or code on a computer-readable medium.  Computer-readable media include both computer storage media and communication media including any\nmedium that facilitates transfer of a computer program from one place to another.  A storage medium may be any available medium that can be accessed by a computer.  By way of example, and not limitation, such computer-readable media can comprise RAM,\nROM, EEPROM, CD-ROM or other optical disk storage, magnetic disk storage or other magnetic storage devices, or any other medium that can be used to carry or store desired program code in the form of instructions or data structures and that can be\naccessed by a computer.  Additionally, any connection is properly termed a computer-readable medium.  For example, if the software is transmitted from a website, server, or other remote source using a coaxial cable, fiber optic cable, twisted pair,\ndigital subscriber line (DSL), or wireless technologies such as infrared (IR), radio, and microwave, then the coaxial cable, fiber optic cable, twisted pair, DSL, or wireless technologies such as infrared, radio, and microwave are included in the\ndefinition of medium.  Disk and disc, as used herein, include compact disc (CD), laser disc, optical disc, digital versatile disc (DVD), floppy disk, and Blu-ray.RTM.  disc where disks usually reproduce data magnetically, while discs reproduce data\noptically with lasers.  Thus, in some aspects computer-readable media may comprise non-transitory computer-readable media (e.g., tangible media).  In addition, for other aspects computer-readable media may comprise transitory computer-readable media\n(e.g., a signal).  Combinations of the above should also be included within the scope of computer-readable media.\n Thus, certain aspects may comprise a computer program product for performing the operations presented herein.  For example, such a computer program product may comprise a computer-readable medium having instructions stored (and/or encoded)\nthereon, the instructions being executable by one or more processors to perform the operations described herein.  For certain aspects, the computer program product may include packaging material.\n Further, it should be appreciated that modules and/or other appropriate means for performing the methods and techniques described herein can be downloaded and/or otherwise obtained by a user terminal and/or base station as applicable.  For\nexample, such a device can be coupled to a server to facilitate the transfer of means for performing the methods described herein.  Alternatively, various methods described herein can be provided via storage means (e.g., RAM, ROM, a physical storage\nmedium such as a compact disc (CD) or floppy disk, etc.), such that a user terminal and/or base station can obtain the various methods upon coupling or providing the storage means to the device.  Moreover, any other suitable technique for providing the\nmethods and techniques described herein to a device can be utilized.\n It is to be understood that the claims are not limited to the precise configuration and components illustrated above.  Various modifications, changes and variations may be made in the arrangement, operation and details of the methods and\napparatus described above without departing from the scope of the claims.", "application_number": "14920099", "abstract": " A method of quantizing a floating point machine learning network to\n     obtain a fixed point machine learning network using a quantizer may\n     include selecting at least one moment of an input distribution of the\n     floating point machine learning network. The method may also include\n     determining quantizer parameters for quantizing values of the floating\n     point machine learning network based at least in part on the at least one\n     selected moment of the input distribution of the floating point machine\n     learning network to obtain corresponding values of the fixed point\n     machine learning network.\n", "citations": ["5732697", "6144977", "7593804", "7917333", "8442927", "20150170020"], "related": ["62159079"]}, {"id": "20170018004", "patent_code": "10373189", "patent_name": "Systems and methods for data transfer from pop displays with wireless\n     beacons and engaged mobile devices", "year": "2019", "inventor_and_country_data": " Inventors: \nWalden; Charles (Austin, TX)  ", "description": "BACKGROUND OF THE INVENTION\n 1.  Field of the Invention\n Embodiments disclosed herein relate to the use of wireless beacons in point of purchase (\"POP\") displays to facilitate the delivery of consumer oriented content to mobile devices.  Certain embodiments relate to systems and methods for\ntransferring data between wireless beacons, POP displays, mobile devices, wireless network gateways, and remote servers.\n 2.  Description of the Relevant Art\n POP (\"point of purchase\") displays are often used in retail environments to display content for particular products associated with the POP displays.  POP displays typically include signs, graphics, or other marketing materials that communicate\ninformation about associated products and are intended to draw a shopper's (e.g., customer's) attention to the products associated with the displays.  POP displays may be used as integral components for marketing or promotional campaigns.  POP displays\noften contribute to the success of these campaigns.\n Traditional POP display signage, which runs the gamut from a simple plastic holder for a card with product information to illuminated translucent graphic films in an atmospheric light box, are static in nature and are unable to customize the\ninformation conveyed to a potential customer based upon the customer's interest level.  Other conventional POP display signage may include video displays that offer limited interactive options.  Thus, there is a need for POP displays that are capable of\ndynamically interacting with potential customers.  The manufacture, distribution, and/or deployment in retail settings of multiple POP displays, however, poses unique challenges, especially when the ability to dynamically interact with potential\ncustomers is included with the POP displays.  Developments in mobile device technology and mobile communication technology allows for dynamic interaction with potential customers in retail environment.\n Beacons are among the most important new mobile technologies helping merchants engage with consumers via mobile communication while the consumers are in brick and mortar stores.  For many years, near field communication (NFC) was considered to\nbe the technology that would deliver such data to retailers and help them track how customers behave in-store.  NFC, however, has reached certain limits and beacons (and beacon technology) provides increased potential for providing customer engagement to\nshoppers in store environments.\n Beacons may be low-cost devices that communicate with mobile device (e.g., smartphone) apps through a Bluetooth signal.  Beacons are expected to directly influence over $4 billion worth of US retail sales this year at top retailers (0.1% of the\ntotal), and that number may climb tenfold in 2016.  Current beacon implementations are relatively crude and typically broadcast the same, static content (e.g., a coupon, regardless of circumstances or a potential customer's demonstrated intent).  Beacon\ntechnology has enormous potential to enhance the shopping experience.  For example, beacon technology may make it quicker and easier for customers to access the information and products they are looking for or provide special offers or discounts to loyal\nshoppers.  Beacon technology can also provide retailers with invaluable data about their customers' shopping habits as well as the activity of their staff.  Thus, retailers may make improvements to the store layout by identifying store flow, maintaining\nservice standards, and maintaining operations that will benefit both customer and retailer.  Current implementations of beacon technology, however, have failed to develop a more dynamic set of interactions with potential customers, particularly those\nwhich are based on and distinguish between various location-based actions.\n There has been some development in the use of beacon technology in store (customer) environments, however, the implementation of beacon technology remains limited.\n United States Patent Application Publication No. 2015/0287045, filed Apr.  6, 2015 by Brown et al., which is incorporated by reference as if fully set forth herein, describes a \"system for monitoring compliance with a retail display program\nincludes a beacon coupled to a promotional display structure.\" The system includes a \"computing device [that] is configured to compare the location-specific data and time stamp to the specified retail facility and time period to determine whether the\npromotional display structure is displayed in the specified retail facility during the specified time period.\" The system in Brown, however, requires that \"Each promotional display structure 20 is intended to be displayed at a specified retail facility\n50.  Moreover, in the example embodiment, each promotional display structure 20 is intended to be displayed at a specified location 60 within specified retail facility 50.\" Thus, the system of Brown requires that the intended location of each\n\"promotional display structure\" be known before the display structures are sent to their locations so that compliance of the structure (e.g., is it displayed in the correct location) may be determined.  However, as is known in the art of promotional\ndisplays, it can often be very difficult and cumbersome to ensure and know the intended locations of promotional displays.  For example, a large set of identical promotional displays are often sent to a warehouse for storage before being randomly sent\nout to retail locations without any thought being given as to the intended location for each specific promotional display.  Further, multiple locations within a retail location may be intended for a given display once it reaches the retail location.\n United States Patent Application Publication No. 2014/0282620, filed Mar.  15, 2013 by Nuovo et al., which is incorporated by reference as if fully set forth herein, states: \"detecting an advertised device identifier and comparing the detected\ndevice identifier with device identifiers stored on the mobile device.  If there is a match, the match can trigger an event.  The event can be requesting content associated with the matched device identifier, receiving the requested content, and\nrendering the received content.  The requested content can be selected to have additional, corresponding content downloaded and rendered.\" This identification is done by \"an application that operates on a mobile device.  When executed, the application\ncan cause the mobile device to search for device identifiers, e.g., media access controller addresses and/or broadcast identifiers (IDs), which are advertised by wireless beacon units, such as WiFi beacon units and Bluetooth beacon units.\"\n United States Patent Application Publication No. 2002/0176388 filed Mar.  19, 2002, by Rankin and Simons, which is incorporated by reference as if fully set forth herein, describes a centralized system for updating beacons.  The system includes\n\"a modification to the Bluetooth system to enable the connectionless broadcast of short messages from Bluetooth beacons.  This can be achieved by exploiting the Bluetooth Inquiry phase by extending the very short ID packet sent out during this mode and\nusing the extra space thus gained to carry a small amount of information.  This information can be Bluetooth system related data or one-way application data.  This scheme has the potentially useful feature of being backwards-compatible with legacy\nBluetooth devices that are not able to understand this extra field.\"\n United States Patent Application Publication No. 2002/0183004 filed Mar.  15, 2002, by Fulton et al., which is incorporated by reference as if fully set forth herein, describes specialized beacons that are dedicated to either inquiries or\ntransmitting information to a client.\n United States Patent Application Publication No. 2007/0254670, filed May 1, 2006, \"System and method for optimizing throughput in a wireless network,\" by Kawaguchi and Le, which is incorporated by reference as if fully set forth herein,\ndiscusses throttling bandwidth within a mesh network.  For example, \"When the switch 10 determines that a selected mesh node is utilizing a portion of the bandwidth outside of the predetermined threshold range, the switch 10 executes a predetermined\naction (e.g., throttling) on transmissions from the selected node to provide increased bandwidth to mesh nodes further from the switch 10 than the selected node.\"\n WIPO Patent Application WO/2013/054144, \"Method of Estimating the Position of a User Device Using Radio Beacons and Radio Beacons Adapted to Facilitate the Methods of the Invention\" by Usman, et al., which is incorporated by reference as if\nfully set forth herein, discloses methods for \"calculating an estimate of the position of the user device taking into account transmit power data concerning the transmit power level of the one or more said radio beacons .  . . \" Page 2, lines 16-18.\n U.S.  Pat.  No. 6,571,279, issued to Herz et al., which is incorporated by reference as if fully set forth herein, discloses location based services, but more from the perspective of a cellular network.  It states, \"The operation of the location\nenhanced information delivery system as described herein makes use of the fact that each user has a `beacon`, which generally serves as a user identification instrumentality.  The beacons emit identifiers which can be used to associate users with the\ndetected devices.  The beacon can be correlated with location, such as by use of a wireless subscriber station or other systems with known technology.\"\n United States Patent Application Publication No. 2014/0358666, \"Cross-Channel Personalized Promotion Platform,\" by Baghaie and Dempski, which is incorporated by reference as if fully set forth herein, describes a platform for allowing\nadvertisers to purchase promotional opportunities on user's mobile devices.\n United States Patent Application Publication No. 2012/0315839, \"Analyzing Audiences at Public Venues,\" by Mumcuoglu and Engel, which is incorporated by reference as if fully set forth herein, discusses the use of wireless signals to physically\nlocate a user but does not discuss the utilization of that information in real time to transmit pertinent information to that user.\n Despite the previous disclosures described above, there remains many needs related to the concepts of adjusting or \"throttling\" a connection (or a transmission), determination of bumping, or the notions of pushing or pulling content beyond\ngeneric downloading of specific content from a centralized server as discussed herein.  In addition, there is still a need for monitoring surrounding activity and assessing user locations and/or display locations.  In certain applications, transmissions\n(or connections) may need to be throttled with respect to a specific location (e.g., a point of sale).  In some applications, there is a need for the content transmitted over that connection to be varied in relation to either the throttling or determined\nrange.  Thus, there are still improvements needed in the application of beacon technology to engage with customers during their in-store shopping experience and for supporting customers' in-store shopping experiences.\nSUMMARY OF THE INVENTION\n In certain embodiments, context aware solutions are provided for delivering content to potential customers in an efficient manner in association with POP (\"point of purchase\") displays that are used in retail environments.  Embodiments disclosed\nherein include wireless beacon technology associated with the POP displays that can vary the content delivered based upon the relative distance of the potential customer and whether the potential customer has indicated any product interest.  This allows\nfor content to be \"throttled\" to potential customers based on a software configuration that exempts customers who have not signaled interest from being included in messages that might overload and/or annoy the customers and/or trigger privacy concerns\ndue to unrequested content.  Potential customers that have signaled interest, however, may receive content without any throttling.  Furthermore, embodiments disclosed herein may distinguish between \"push\"--use cases where content is provided without an\nintentional request by the customer--and \"pull\"--use cases where content has been intentionally requested by the customer through a physical interaction between the POP display and a customer device (e.g., the customer device being \"bumped\", i.e.,\nintentionally placed in close proximity to an area on the POP display).  The exact information that is pushed or pulled may be located on a remote server that may be configured for each potential use case.  Embodiments disclosed herein may provide\nimplementations that conserve power by allowing devices (e.g., wireless beacons) to be configured to activate at a later date, namely after they have arrived at a certain destination (e.g., a display location).  Embodiments disclosed herein may provide\nfor utilizing context awareness to reduce power consumption when it is unlikely for a potential customer to be around (e.g., when a retail area is dark or no activity is detected).  Furthermore, this context awareness may enable manufacturing and\ndistributions methods to be suited to large-scale production and distribution of POP displays across many locations.  Improved logistical schemes for manufacturing and distributing the embodiments disclosed herein may also be provided since one need not\ndetermine beforehand the exact final location of the POP display and its beacon before distribution to individual retail or advertising venues.\n In certain embodiments, the disclosed systems and methods include a variety of sensors to aid in assessing a proximity of potential customers to the POP display and measuring the surrounding environment.  This information may be recorded and\nanalyzed to gain additional insights about consumer behavior and to gauge the device's performance.  Additionally, information may be inferred from the signal strength of user devices (e.g., mobile devices) carried by potential customers.  This\ninformation may also be retained and analyzed.  In some embodiments, the system may transmit data to a server through various means.  For example, a traditional permanent gateway may be utilized, or user devices with network connectivity that are carried\nby employees or potential customers may be utilized to relay the stored information to the server.\n Embodiments disclosed herein may provide efficient means for communicating with individuals, either to inform or to advertise, and to record information about the disclosed embodiments' performance and its environment.  In some embodiments, the\nrecorded information is harnessed to enable improved logistical schemes to be provided for manufacturing and distributing the disclosed embodiments even when it is unknown where and/or when the disclosed device will be delivered and/or begin operation.\n In certain embodiments, a method includes: recording, using a processor located on a circuit board coupled to a point of purchase (POP) display including a consumer product display, information obtained from one or more sensors coupled to the\nPOP display, wherein the sensors are configured to assess activity around the POP display, the activity including environmental activity around the POP display and user activity around the POP display; recording, using processors located on one or more\nmobile devices configured to interact with a wireless beacon located on the circuit board, information relating to interactions between the mobile devices and the wireless beacon, wherein the information relating to the interactions includes push events,\npull events, signal strength, a unique identifier associated with the wireless beacon, timestamps relating to the interactions, information provided by one or more mobile applications located in software packages on the mobile devices, and sensor data\nfrom the mobile devices at the time of the interactions; storing the information obtained from one or more sensors coupled to the POP display into a memory located on the circuit board and coupled to the processor; transmitting the information, via\nwireless communication, from the memory located on the circuit board to a remote server at a first selected time; storing the information relating to interactions between the mobile devices and the wireless beacon in memory caches located on the mobile\ndevices; and transmitting the information, via wireless communication, from the memory caches of the mobile devices to the remote server at a second selected time.\n In certain embodiments, a non-transient computer-readable medium including instructions that, when executed by one or more processors, causes the one or more processors to perform a method that includes: recording, using a processor located on a\ncircuit board coupled to a point of purchase (POP) display including a consumer product display, information obtained from one or more sensors coupled to the POP display, wherein the sensors are configured to assess activity around the POP display, the\nactivity including environmental activity around the POP display and user activity around the POP display; recording, using processors located on one or more mobile devices configured to interact with a wireless beacon located on the circuit board,\ninformation relating to interactions between the mobile devices and the wireless beacon, wherein the information relating to the interactions includes push events, pull events, signal strength, a unique identifier associated with the wireless beacon,\ntimestamps relating to the interactions, information provided by one or more mobile applications located in software packages on the mobile devices, and sensor data from the mobile devices at the time of the interactions; storing the information obtained\nfrom one or more sensors coupled to the POP display into a memory located on the circuit board and coupled to the processor; transmitting the information, via wireless communication, from the memory located on the circuit board to a remote server at a\nfirst selected time; storing the information relating to interactions between the mobile devices and the wireless beacon in memory caches located on the mobile devices; and transmitting the information, via wireless communication, from the memory caches\nof the mobile devices to the remote server at a second selected time.\n In certain embodiments, a system includes: a processor located on a circuit board, the circuit board being configured to be coupled to a point of purchase (POP) display including a consumer product display; a memory coupled to the processor on\nthe circuit board; a wireless beacon located on the circuit board; one or more sensors coupled to the circuit board, wherein the sensors are configured to assess activity around the POP display, the activity including environmental activity around the\nPOP display and user activity around the POP display; wherein the processor located on the circuit board is configured to record information obtained from the one or more sensors and store the information in the memory coupled to the processor; at least\none mobile device including a processor and a memory cache, wherein the at least one mobile device is configured to interact with the wireless beacon, wherein the at least one mobile device is configured to record information relating to interactions\nbetween the mobile device and the wireless beacon, wherein the information relating to the interactions includes push events, pull events, signal strength, a unique identifier associated with the wireless beacon, timestamps relating to the interactions,\ninformation provided by a mobile application located in a software package on the mobile device, and sensor data from the mobile device at the time of the interactions, and wherein the at least one mobile device is configured to store the information\nrelating to the interactions in the memory cache on the at least one mobile device; wherein the processor on the circuit board is configured to transmit the information from the memory on the circuit board to a remote server at a first selected time via\nwireless communication; and wherein the at least one mobile device is configured to transmit the information from the memory cache of the mobile device to the remote server at a second selected time via wireless communication. BRIEF DESCRIPTION\nOF THE DRAWINGS\n Features and advantages of the methods and apparatus described herein will be more fully appreciated by reference to the following detailed description of presently preferred but nonetheless illustrative embodiments when taken in conjunction\nwith the accompanying drawings in which:\n FIG. 1 depicts a block diagram of an embodiment of a point of purchase display system.\n FIG. 1A depicts an example of an embodiment of a POP display.\n FIG. 2 depicts a block diagram representation of an embodiment of an interaction between a customer device, wireless beacons, and a server.\n FIGS. 3A-3K depict examples of content being displayed on a display of a mobile device.\n FIG. 4 depicts a flowchart of an embodiment of a method to assess a location of a wireless beacon and its POP display.\n FIG. 5 depicts a flowchart of an embodiment of a method used to assess a location of a POP display.\n FIG. 6 depicts a flowchart of a second embodiment of a method used to assess a location of a POP display.\n FIG. 7 depicts a flowchart of a third embodiment of a method used to assess a location of a POP display.\n FIG. 8 depicts a flowchart of a fourth embodiment of a method used to assess a location of a POP display.\n FIG. 9 depicts a flowchart of a manufacturing supply chain associated with POP displays.\n FIG. 10 depicts a block diagram of an embodiment of a plurality of point of purchase displays at a retail location.\n FIG. 11A illustrates an example of a campaign calendar.\n FIG. 11B illustrates an example of an interface displaying various statistics related to POP display deployment and sales.\n FIG. 11C illustrates an information screen related to the product associated with the POP display.\n FIG. 11D illustrates an interface displaying national deployment information for POP displays.\n FIG. 11E illustrates an interface displaying state deployment information for POP displays.\n FIG. 11F illustrates an interface displaying supply chain information.\n FIG. 11G illustrates an interface displaying a sales analysis associated with the POP display.\n FIG. 12 depicts a block diagram of one embodiment of an exemplary computer system.\n FIG. 13 depicts a block diagram of one embodiment of a computer accessible storage medium.\n While the disclosure is susceptible to various modifications and alternative forms, specific embodiments thereof are shown by way of example in the drawings and will herein be described in detail.  It should be understood, however, that the\ndrawings and detailed description thereto are not intended to limit the disclosure to the particular form illustrated, but on the contrary, the intention is to cover all modifications, equivalents and alternatives falling within the spirit and scope of\nthe present disclosure as defined by the appended claims.  The headings used herein are for organizational purposes only and are not meant to be used to limit the scope of the description.  As used throughout this application, the word \"may\" is used in a\npermissive sense (i.e., meaning having the potential to), rather than the mandatory sense (i.e., meaning must).  Similarly, the words \"include,\" \"including,\" and \"includes\" mean including, but not limited to.  Additionally, as used in this specification\nand the appended claims, the singular forms \"a\", \"an\", and \"the\" include singular and plural referents unless the content clearly dictates otherwise.  Furthermore, the word \"may\" is used throughout this application in a permissive sense (i.e., having the\npotential to, being able to), not in a mandatory sense (i.e., must).  The term \"include,\" and derivations thereof, mean \"including, but not limited to.\" The term \"coupled\" means directly or indirectly connected.\n The term \"automatically\" refers to an action or operation performed by a computer system (e.g., software executed by the computer system) or device (e.g., circuitry, programmable hardware elements, ASICs, etc.), without user input directly\nspecifying or performing the action or operation.  Thus the term \"automatically\" is in contrast to an operation being manually performed or specified by the user, where the user provides input to directly perform the operation.  An automatic procedure\nmay be initiated by input provided by the user, but the subsequent actions that are performed \"automatically\" are not specified by the user, i.e., are not performed \"manually\", where the user specifies each action to perform.  For example, a user filling\nout an electronic form by selecting each field and providing input specifying information (e.g., by typing information, selecting check boxes, radio selections, etc.) is filling out the form manually, even though the computer system must update the form\nin response to the user actions.  The form may be automatically filled out by the computer system where the computer system (e.g., software executing on the computer system) analyzes the fields of the form and fills in the form without any user input\nspecifying the answers to the fields.  As indicated above, the user may invoke the automatic filling of the form, but is not involved in the actual filling of the form (e.g., the user is not manually specifying answers to fields but rather they are being\nautomatically completed).  The present specification provides various examples of operations being automatically performed in response to actions the user has taken.\n Various units, circuits, or other components may be described as \"configured to\" perform a task or tasks.  In such contexts, \"configured to\" is a broad recitation of structure generally meaning \"having circuitry that\" performs the task or tasks\nduring operation.  As such, the unit/circuit/component can be configured to perform the task even when the unit/circuit/component is not currently on.  In general, the circuitry that forms the structure corresponding to \"configured to\" may include\nhardware circuits and/or memory storing program instructions executable to implement the operation.  The memory can include volatile memory such as static or dynamic random access memory and/or nonvolatile memory such as optical or magnetic disk storage,\nflash memory, programmable read-only memories, etc. The hardware circuits may include any combination of combinatorial logic circuitry, clocked storage devices such as flops, registers, latches, etc., finite state machines, memory such as static random\naccess memory or embedded dynamic random access memory, custom designed circuitry, programmable logic arrays, etc. Similarly, various units/circuits/components may be described as performing a task or tasks, for convenience in the description.  Such\ndescriptions should be interpreted as including the phrase \"configured to.\" Reciting a unit/circuit/component that is configured to perform one or more tasks is expressly intended not to invoke 35 U.S.C.  .sctn.  112(f) interpretation for that\nunit/circuit/component.\n In an embodiment, hardware circuits in accordance with this disclosure may be implemented by coding the description of the circuit in a hardware description language (HDL) such as Verilog or VHDL.  The HDL description may be synthesized against\na library of cells designed for a given integrated circuit fabrication technology, and may be modified for timing, power, and other reasons to result in a final design database that may be transmitted to a foundry to generate masks and ultimately produce\nthe integrated circuit.  Some hardware circuits or portions thereof may also be custom-designed in a schematic editor and captured into the integrated circuit design along with synthesized circuitry.  The integrated circuits may include transistors and\nmay further include other circuit elements (e.g. passive elements such as capacitors, resistors, inductors, etc.) and interconnect between the transistors and circuit elements.  Some embodiments may implement multiple integrated circuits coupled together\nto implement the hardware circuits, and/or discrete elements may be used in some embodiments.\n The scope of the present disclosure includes any feature or combination of features disclosed herein (either explicitly or implicitly), or any generalization thereof, whether or not it mitigates any or all of the problems addressed herein. \nAccordingly, new claims may be formulated during prosecution of this application (or an application claiming priority thereto) to any such combination of features.  In particular, with reference to the appended claims, features from dependent claims may\nbe combined with those of the independent claims and features from respective independent claims may be combined in any appropriate manner and not merely in the specific combinations enumerated in the appended claims.\nDETAILED DESCRIPTION OF EMBODIMENTS\n The following examples are included to demonstrate preferred embodiments.  It should be appreciated by those of skill in the art that the techniques disclosed in the examples which follow represent techniques discovered by the inventor to\nfunction well in the practice of the disclosed embodiments, and thus can be considered to constitute preferred modes for its practice.  However, those of skill in the art should, in light of the present disclosure, appreciate that many changes can be\nmade in the specific embodiments which are disclosed and still obtain a like or similar result without departing from the spirit and scope of the disclosed embodiments.\n In this patent, certain U.S.  patents, U.S.  patent applications, and other materials (e.g., articles) have been incorporated by reference.  The text of such U.S.  patents, U.S.  patent applications, and other materials is, however, only\nincorporated by reference to the extent that no conflict exists between such text and the other statements and drawings set forth herein.  In the event of such conflict, then any such conflicting text in such incorporated by reference U.S.  patents, U.S. patent applications, and other materials is specifically not incorporated by reference in this patent.\n Further modifications and alternative embodiments of various aspects of the disclosed embodiments will be apparent to those skilled in the art in view of this description.  Accordingly, this description is to be construed as illustrative only\nand is for the purpose of teaching those skilled in the art the general manner of carrying out the disclosed embodiments.  It is to be understood that the forms of the disclosed embodiments shown and described herein are to be taken as examples of\nembodiments.  Elements and materials may be substituted for those illustrated and described herein, parts and processes may be reversed, and certain features of the disclosed embodiments may be utilized independently, all as would be apparent to one\nskilled in the art after having the benefit of this description of the disclosed embodiments.  Changes may be made in the elements described herein without departing from the spirit and scope of the disclosed embodiments as described in the following\nclaims.\n This specification includes references to \"one embodiment\" or \"an embodiment.\" The appearances of the phrases \"in one embodiment\" or \"in an embodiment\" do not necessarily refer to the same embodiment, although embodiments that include any\ncombination of the features are generally contemplated, unless expressly disclaimed herein.  Particular features, structures, or characteristics may be combined in any suitable manner consistent with this disclosure.\n As used herein, the word \"display\" is intended to include an array of merchandising materials and store-based assets such as, but not limited to, signs, test product or samples, permanent or semi-permanent fixtures, coupon dispensers,\naisle-based video screens, mobile coolers, or other movable assets within a retail outlet.\n FIG. 1 depicts a block diagram of an embodiment of point of purchase (\"POP\") display system 100.  In certain embodiments, system 100 includes POP display 102.  In certain embodiments, circuit board 104 is located on POP display 102.  Circuit\nboard 104 may be, for example, a printed circuit board or any other suitable circuit board for connecting and operating multiple electronic components including, but not limited to, integrated circuits.  Circuit board 104 may be placed (installed) on, or\ncoupled to, POP display 102 during or after manufacturing of the POP display.\n In certain embodiments, circuit board 104 includes battery 106, switch 108, memory 110, controller 112, wireless beacon 114, and sensors 116.  In certain embodiments, controller 112 includes circuitry, an integrated circuit, or a processor\noperable to control operation of wireless beacon 114 and/or other components of circuit board 104 and/or POP display 102.  Memory 110 may include many different types of memory known in the art for use on a circuit board.  For example, memory 110 may be\nflash memory, RAM, EEROM, EEPROM, and/or one-time programmable memory.\n In some embodiments, controller 112 is coupled to clock 113.  Clock 113 may be capable of tracking both date and time.  Clock 113 may be associated with wireless beacon 114 to provide time information (e.g., date and time) to the wireless\nbeacon.  In some embodiments, clock 113 is located in a chip on circuit board 104.  In some embodiments, clock 113 is located in a microprocessor in wireless beacon 114.\n In some embodiments, circuit board 104 includes unique label 117.  Unique label 117 may be added during or after manufacturing of circuit board 104.  Unique label 117 may be, for example, a printed label, such as a QR label or barcode, that can\nbe viewed or electronically scanned for identifying information.  Unique label 117 may include a unique identification for circuit board 104 that differentiates the circuit board from other circuit boards that may be used on other POP displays.  For\nexample, each circuit board 104 may have its own identification number that specifically identifies the circuit board.\n POP display 102, as described herein, may be any display that holds products and/or advertises products.  For example, POP display 102 may include signs, graphics, or other marketing materials that communicate information about a product to a\nconsumer.  FIG. 1A depicts an example of an embodiment of POP display 102.  In some embodiments, POP display 102 includes the product itself.  For example, products such as, but not limited to, demo units of electronic items, appliances, and/or rugs may\nbe a POP display.  POP display 102 is typically placed next to or near the merchandise the display is promoting and/or included as part of the merchandise.  In some embodiments, POP display 102 is utilized to hold, support, or display products associated\nwith the POP display.  In certain embodiments, POP display 102 is a corrugated cardboard display.  POP display 102 may also include displays made from materials such as, but not limited to, paper, paperboard, bristol board, foam cored board, plastic, or\nany other material suitable for holding and/or advertising products.\n POP display 102 may be a component of a marketing or promotional campaign.  In certain embodiments, POP display 102 is generally located in a retail environment (e.g., a retail store) or any other location where a customer purchases product or a\ndecision to purchase product is made.  In some embodiments, POP display 102 is placed in other display locations in order to drive potential customers to a specific area.  For example, POP display 102 may be placed in a window display and used to provide\n(e.g., \"beam\") promotional information to people as they pass by outside a retail store.  Regardless of the location of POP display 102, the POP display may be intended to draw the customer's attention to products associated with the display.  These\nproducts may, in some embodiments, be new products, products on sale, and/or products associated with a special offer.  POP display 102 may also be used to promote special events (e.g., seasonal or holiday-time sales).\n In certain embodiments, as shown in FIG. 1, POP system 100 includes the use of customer device 118.  Customer device 118 may be, for example, a mobile device.  Customer device 118 may be a small computing device, typically small enough to be\nhandheld (and hence also commonly known as a handheld computer or simply handheld).  Mobile devices may be any of various types of computer systems devices which are mobile or portable and which perform wireless communications using WLAN communication. \nExamples of mobile devices include mobile telephones or smart phones (e.g., iPhone.TM., Android.TM.-based phones), and tablet computers such as iPad.TM., Samsung Galaxy.TM., etc. Various other types of devices would fall into this category if they\ninclude Wi-Fi or both cellular and Wi-Fi communication capabilities, such as laptop computers (e.g., MacBook.TM.), portable gaming devices (e.g., Nintendo DS.TM.  PlayStation Portable.TM., Gameboy Advance.TM., iPhone.TM.), portable Internet devices, and\nother handheld devices, as well as wearable devices such as smart watches, smart glasses, headphones, pendants, earpieces, etc. In general, the term \"mobile device\" can be broadly defined to encompass any electronic, computing, and/or telecommunications\ndevice (or combination of devices) which is easily transported by a user and capable of wireless communication using WLAN or Wi-Fi.  In certain embodiments, customer device 118 includes any device used by a customer with display 119 (e.g., an LCD screen\nor touchscreen), one or more wireless transceivers (e.g., wireless transceivers 120A, 120B, shown in FIG. 1), software package 122, and memory cache 124.  Display 119, in some embodiments, includes a user interface for customer device 118 (e.g., the\ndisplay allows interactive input for the user).\n In certain embodiments, wireless beacon 114 on POP display 102 interacts with customer devices 118 carried by potential customers.  Wireless beacon 114 may be configured to interact with customer devices 118 through wireless transceiver 120A. \nIn certain embodiments, wireless transceiver 120A is a Bluetooth Low Energy (\"BLE\") transceiver.\n In certain embodiments, wireless beacon 114 includes a unique identifier associated with the wireless beacon.  The unique identifier may be broadcast by wireless beacon 114, received through wireless transceiver 120A, and used to identify the\nwireless beacon (e.g., the unique identifier may be used by a server to identify the wireless beacon as described herein).  Thus, in embodiments with multiple wireless beacons 114, the wireless beacons broadcast their respective unique identifiers and\nthe unique identifiers may be used to identify and/or differentiate the wireless beacons and, by extension, the circuit board and POP display associated with each wireless beacon.\n Wireless beacon 114 may be a transponder sending data via radio signals.  In certain embodiments, wireless beacon 114 is a Bluetooth Low Energy (\"BLE\") beacon.  A Bluetooth LE beacon may operate in either peripheral or central mode, depending on\nthe circumstances, though in certain embodiments, the beacon may default to peripheral mode.  Chipsets implementing beacon functionality may be commercially available.  Two non-limiting examples are the Texas Instruments CC2541 and CC2600.  The disclosed\nembodiments, however, do not depend on the particular choice of Bluetooth chipset.\n Bluetooth low energy (Bluetooth LE, BLE, also marketed as Bluetooth Smart) is a wireless personal area network technology designed and marketed by the Bluetooth Special Interest Group aimed at applications in the healthcare, fitness, beacons,\nsecurity, and home entertainment industries.  Compared to Classic Bluetooth, Bluetooth Smart is intended to provide considerably reduced power consumption and cost while maintaining a similar communication range.\n Bluetooth Smart was originally introduced under the name Wibree by Nokia in 2006.  It was merged into the main Bluetooth standard in 2010 with the adoption of the Bluetooth Core Specification Version 4.0.  In certain embodiments, wireless\nbeacons 114 are Bluetooth LE beacons.  Bluetooth LE beacons may be used, at least in part, because Bluetooth LE has been widely adopted in customer devices 118 (e.g., mobile devices).  Thus, a potential consumer may likely already have the requisite\nhardware to interact with circuit board 104 and POP display 102.  For example, Bluetooth LE has been built into iPhones and iPads since 2010, and many Android devices since 2013.  Bluetooth LE wireless beacons are also, as the name implies, energy\nefficient, which may be an important consideration for technology deployed on mobile devices.  In certain embodiments, the positioning and data transmission capabilities of Bluetooth LE are also of use, though the embodiments disclosed herein may also be\nimplemented using other wireless standards, including the various versions of IEEE 802.11.\n In certain embodiments, POP display system 100 includes server 126.  Server 126 may communicate with customer device 118 through wireless transceiver 120B on the customer device.  In certain embodiments, wireless transceiver 120B is a\nWiFi-enabled or cellular transceiver.  Server 126 may include content 128.  In certain embodiments, content 128 is uploaded to server 126 via an exposed API (Application Programming Interface).  Content 128 may be included as part of a storage structure\nor storage management system (e.g., a database) accessible by server 126.  For example, content 128 may be stored in a database in an accessible memory of server 126.  In certain embodiments, content 128 includes information that corresponds to\nadvertising, marketing, and/or promotional campaigns associated with POP displays 102.  For example, content 128 may include, but not be limited, campaign start times, campaign time periods, campaign locations, coupons associated with the campaign,\nadvertising and/or marketing associated with the campaign, and promotions associated with the campaign.\n As server 126 includes content 128, the server may be referred to as a \"content server\", though the phrase \"content server\" as used in this disclosure should not be considered strictly limiting.  In some embodiments, the physical server(s)\n(e.g., server 126) that stores content 128 may perform other functionality and/or work in conjunction with other servers to enable some or all of its functionality.  For example, server 126 may work with a load balancing server to optimize its\ncommunications load over a network or authentication servers to validate the entities requesting a download of content.  In some embodiments, server 126 may operate in a distributed nature such that content 128 is distributed over more than one physical\nstorage device or logical drive partitions.  The term \"content server\" is intended to encompass all of these scenarios and any other that one of ordinary skill in the art would contemplate in implementing the disclosed functionality.\n In certain embodiments, server 126 includes information 130.  Information 130 may be included as part of a storage structure or storage management system (e.g., a database) accessible by server 126.  Information 130 may include information\nregarding POP display 102 and wireless beacon 114 such as, but not limited to, the unique identifier, location information (if known), and retail location information for the POP display (e.g., store location information for a specific retailer\nassociated with the POP display).  In some embodiments, information 130 includes information recorded from sensors 116 and/or other components on POP displays 102 as well as information recorded on customer devices 118 that is transmitted to server 126.\n In certain embodiments, SDK (\"Software Developer Kit\") 125 is located in software package 122 on customer device 118, as shown in FIG. 1.  SDK 125 may allow programmers to develop applications (e.g., mobile application 127) for customer device\n118 that interface the customer device with server 126 and circuit board 104.  SDK 125 may abstract low level implementation details of POP display system 100 and simplify the development of software applications compatible with the disclosed\nembodiments.  In certain embodiments, SDK 125 includes functionality to facilitate accessing APIs exposed by server 126 (e.g., the content server) as well as wireless (e.g., Bluetooth) mediated interactions with wireless beacons 114.\n In certain embodiments, mobile application 127 is located in software package 122 on customer device 118.  Mobile application 127 may be coupled to SDK to allow the mobile application to interface and utilize functions of the SDK.  In some\nembodiments, SDK 125 may be embedded in mobile application 127 (e.g., the SDK is a software code element of the mobile application).  Mobile application 127 may be, in some embodiments, a retailer \"app\" or other mobile application written for interaction\nbetween a customer and a specific retailer (e.g., the mobile application may be a customer loyalty app specific for a selected retailer).  In certain embodiments, mobile application 127 provides an interactive interface for the customer through customer\ndevice 118.  For example, mobile application 127 may use display 119 as a user interface (the display is a touchscreen) to allow interactive customer input or the mobile application may use the display in combination with another input system (e.g., a\nkeyboard or voice input) to allow interactive customer input.  In certain embodiments, mobile application 127 utilizes SDK 125, when run on customer device 118, to detect that the customer device is in proximity to a compatible Bluetooth LE beacon (e.g.,\nwireless beacon 114), as described herein.\n In certain embodiments, SDK 125 is configured to receive measurements from customer device 118 through built-in features of the customer device.  For example, SDK 125 may receive measurements from accelerometer, gyroscope, compass, audio, light,\nor Near Field Communication measurements on customer device 118.  These measurements may be utilized to increase the accuracy of calculated location information or used to infer additional information about either a user or an environment of POP display\n102.  For example, information from an accelerometer on customer device 118 may be combined with other information to increase the accuracy of detection of \"bumps\" or recognition of gestures as described below.\n In some embodiments, the measurements received by SDK 125 are sent to server 126 and stored in information 130.  Server 126 may integrate the measurement information from customer device 118 to increase accuracy of location information and/or\ninfer additional information, as described below.  In some embodiments, server 126 may integrate the measurement information with information from external data sources, which may be located in information 130 on the server.  For example, server 126 may\nintegrate store specific information from nearby beacons, geolocation information provided by a retail loyalty application on connected mobile devices, or other information received from third party sources.\n In certain embodiments, POP display system 100 utilizes wireless signal strength to infer distance between customer device 118 and POP display 102.  POP display system 100 may utilize this distance information to modulate and/or control the\nparticular information conveyed to the customer through customer device 118.  In certain embodiments, SDK 125 in software 122 on customer device 118 receives information, based on distance, indicating the detection of \"bumps\" or \"pulls\" (e.g., when a\nuser physically touches (or very nearly so) the customer device against a designated area of POP display 102 (e.g., at or near a \"tap device here for more information\" designated area)).  In the disclosed embodiments, the concept of bumping is applied as\na way for a user to express interest in POP display 102 independent of any technical requirements of the underlying wireless communication protocol being used.\n Various techniques may be utilized to estimate distance between customer device 118 and POP display 102.  For example, in certain embodiments, Received Signal Strength Indication (\"RSSI\") values of Bluetooth signals are measured and analyzed to\ninfer distance.  The distance inferred may be relative or absolute in nature (e.g., the technique may only specify a distance from POP display 102 as opposed to exact position).  By means of illustration, the general relationship between RSSI value and\ndistance is approximately RSSI[dbm]=-(10.times.n.times.log.sub.10(d)-A), where d is the distance and A is the offset which is the measured RSSI value 1 meter point away from the Bluetooth LE device.  Again, this is provided simply for illustrative\npurposes and other relationships and formulas may be utilized by the disclosed embodiments to infer location information about the customer device and, by extension the customer.  Other examples of values that may be utilized to determine signal strength\ninclude, but are not limited to, packet loss ratio or rate, header error check, cyclic redundancy check, and forward error correction.  Furthermore, the measurement of these various values, including RSSI, may be implemented in numerous ways in hardware. For example, one may utilize Goertzel algorithms to derive signal strength values from a series of transceiver power measurements.  As shown above, the precise implementation details of the measurement to calculate location information can vary and the\nembodiments disclosed herein may be suited to the usage of any measurement to calculate location information.  Furthermore, location related information (e.g., signal strength measurements, values derived from signal strength measurements, identifiers\nassociated with a particular mobile device, timestamps associated with a signal strength reading) may be saved to a memory (e.g., memory 110 or memory cache 124) for future review and/or analysis.  In some embodiments, the location related information\nincludes information about customer device 118.  For example, the information may include information about chipsets, antennas, and/or an operating system of customer device 118.  The information about customer device 118 may be part of the future review\nand/or analysis to increase accuracy in assessing relative location information of the customer device and POP display 102.\n In certain embodiments, signal strength (e.g., Bluetooth signal strength as measured, for example, via RSSI) between POP display 102's wireless beacon 114 and wireless transceiver 120A on customer device 118 is monitored and, if it surpasses a\npredefined threshold or \"trigger\" level, it is inferred that the customer has \"bumped\" the customer device against the POP display and has made a \"pull\" delivery request (e.g., the user has indicated his/her intention to receive or \"pull\" content\nassociated with the POP display).  In certain embodiments, the predefined threshold is set at a signal strength level that indicates that the user has clearly intended to initiate a \"bump\" or \"pull\" with POP display 102.  For example, the predefined\nthreshold may be set at a signal strength level that clearly defines customer device 118 has intentionally been placed on or near to the designated area of POP display 102 by the customer.  In some embodiments, the predefined threshold is combined with\nother information (e.g., information from an accelerometer on customer device 118) to define intent of the customer in \"pulling\" for content.  For example, accelerometer data may be combined with the predefined threshold (measured via RSSI) to recognize\na gesture (e.g., movement of customer device 118 in an intentional way) made by the user that indicates intent of the customer to receive information.\n In certain embodiments, the predefined threshold improves the reliability of bump detection and the threshold may be dynamic in nature.  For example, the threshold may be specified by a formula that accounts for certain variables rather than a\nset static number.  In some embodiments, the algorithm may not allow a new bump to be registered until the signal is outside of a separate threshold, usually higher in value than the entrance threshold.  This restriction may help to prevent spurious\nbumps.  Additional techniques may be utilized to improve bump detection (such as a filter to smooth RSSI values).  In some embodiments, signal profiles for setting the predefined threshold are associated with a type of customer device 118 (e.g., a type\nof mobile device or a type of antenna used in the mobile device).  Server 126 may receive type data for customer device 118 when the customer device is in contact with the server.  Server 126 then may send RSSI signal profiles associated with the type\ndata to the SDK on customer device 118, which stores the signal profiles in memory cache 124 for accessing in assessment of bump indications.\n Some embodiments may utilize different methods for gauging distance.  For example, other performance measures associated with a Bluetooth signal, RSSI values associated with a 802.11 WiFi signal, information from a Near Field Communication\nsignal, etc. may be used.  Regardless of the origin and type of information used, the associated algorithms may utilize the information to detect bumps.  In some embodiments, the detection of bumps is performed in circuit board 104 rather than on\ncustomer device 118.\n Bumping may be used to signal that the customer is explicitly requesting digital content (e.g., requesting content to be display on display 119 of customer device 118).  In the event that a bump is detected, the SDK may provide content to the\ncustomer on customer device 118 (this may be referred to as \"pull\" delivery).  For example, content may be display on display 119 through mobile application 127.  The content may include content stored in memory cache 124, which includes content 128\npreviously received from server 126 as described herein.  Conversely, \"push\" delivery may occur in the absence of a bump, where content 128 may be delivered by server 126 to customers that have not explicitly requested content.  In certain embodiments,\nunsolicited push content is throttled to prevent from overloading the customer with unrequested content, while pull content (e.g., requested content) is not throttled.  In some embodiments, the exact throttling scheme used is configurable by software and\nmay be specified by various entities.  For example, the exact throttling scheme may be specified by a POP display owner, a retailer, an advertising company, a manufacturer of goods or services associated with the POP display, etc.\n In some embodiments, a throttling scheme is personalized for a particular user.  For example, the throttling scheme may include personalized data based on a persona of the user.  The personalized data may be uploaded to and/or stored in memory\ncache 124 on customer device 118.  The persona of the user may include categories based on one or more user preferences.  The preferences may be for categories that include non-specific information about the user (e.g., anonymous information based only\non the behavior of the user).  Using non-specific information may protect privacy and security of the user of customer device 118.  In some embodiments, the persona of the user is defined by preferences specified by a retailer (e.g., through a retailer\napp in SDK 125 on customer device 118).\n Information relevant to the throttling scheme may be incorporated in several aspects of the disclosed embodiments.  First, content 128 may be uploaded to server 126 via an exposed API (Application Program Interface) designed to work with the\noverall device ecosystem.  This API requires that the uploaded content be associated with information that allows server 126 to associate content 128 with specific beacons (e.g., wireless beacon 114).  The API may also require information associated with\nthe uploaded content that will allow customer device 118, via an API call, to determine if the content should be served up based on push or pull.\n In certain embodiments, as shown in FIG. 1, POP display 102 includes sensors 116.  Sensors 116 may provide monitoring of activity in and/or around the POP display.  In certain embodiments, sensors 116 include proximity sensors that detect\nactivity in the vicinity of POP display 102.  Proximity sensor may detect activity based on, for example, heat, light (reflected infrared and/or visible light), sound, and/or images.  Examples of sensors 116 include, but are not limited to, ambient light\nsensors, passive infrared sensors, active infrared sensors, and image based detection sensors.  Other examples include accelerometers, temperature sensors, weight sensors, cameras, and sensors that detect when a product has been dispensed or when a\ndisplay needs to be restocked.\n Sensors 116 may be used to measure and record (and, in some embodiments, timestamp in combination with clock 113) activity around the display and save these measurements in memory 110.  These measurements and recordings may provide information\nthat can be used for detailed analysis of the level of traffic around POP display 102 by time.  The analysis may include determining information such as, but not limited to, how many people walk past the display, how many people stop to look at the\ndisplay, when a door is opened, how long the door is opened, and whether products are removed.  Such analysis may include measuring the timing of the activity, such as how long a potential customer stood in front of the display, commonly referred to as\ndwell time.  Other potential analyses include, but are not limited to, how many shoppers passed by (divided into buckets of time), the average dwell time per shopper, and/or counts of shoppers that had smartphones (customer devices 118) equipped with\nsoftware package 122.  Measurement data from sensors 116 stored in memory 110 may be transmitted (broadcast) in data packets sent out by wireless beacon 114.  The data packets with the measurement data may be received by, for example, customer devices\n118 and/or network gateway 710 to then be transmitted to a remote server (e.g., server 126).\n In certain embodiments, sensors 116 include a proximity sensor that monitors activity only within a defined range (e.g., a defined distance) from POP display 102.  Sensor data may also be used in a transmission throttling scheme as described\nherein (e.g., a particularly crowded store might dictate the use of a different transmission).  Additionally, as described herein, the connection between wireless beacons 114 and/or customer devices 118 may be utilized to share information between POP\ndisplays 102.\n In certain embodiments, information recorded from sensors 116 and/or other components on POP displays 102 as well as information recorded on customer devices 118 is transmitted and stored in server 126 as information 130.  Information recorded\non customer devices 118 may include any information or data relating to interactions between the customer devices and wireless beacons 114, interactions between the customer devices and server 126, other interactions involving the customer devices, and\ndata obtained by the customer devices such as device sensor data (e.g., position and/or movement measurement data) and/or application data from the software package.  In some embodiments, recorded information may be stored in memory cache 124 on customer\ndevice 118 before being transmitted to server 126.  The recorded information stored in memory cache 124 may include information recorded on customer device and/or information recorded from sensors 116 on POP displays 102 (after the data is transmitted to\nthe customer device via wireless beacon 114).  In some embodiments, proximity sensor data is used by server 126 along with position information obtained through wireless transceiver 120B on customer device 118 to improve the accuracy of determining\nlocation information (e.g., location information related to location of wireless beacons and POP displays).\n In certain embodiments, it may be desirable to only allow wireless beacons to broadcast when there is activity near the wireless beacon.  Allowing wireless beacons to only broadcast with nearby activity may allow a large number (e.g., high\ndensity) of wireless beacons to be located in a single retail location as not all of the wireless beacons will be actively broadcasting at the same time.  In certain embodiments, one or more sensors 116 are used in combination with wireless beacon 114 to\nallow the wireless beacon to operate in a low power (non-broadcasting) mode while located in a retail location and only actively broadcast when nearby activity is detected.  For example, sensor 116 may be a proximity sensor that detects activity within a\nselected distance from wireless beacon 114.  When no activity is detected by sensor 116 (e.g., there is an absence of activity), wireless beacon 114 may enter a low power (sleep or non-broadcasting) mode.  In the low power mode, wireless beacon 114 does\nnot respond or provide push/pull events, described herein, as the wireless beacon is not broadcasting any data packets.  If sensor 116 detects any activity (e.g., via proximity detection of a customer/user), wireless beacon 114 may be switched to an\nactive (broadcasting) mode substantially instantaneously.  Wireless beacon 114 may then be active for any push/pull events or content requests associated with customer device 118.\n In some embodiments, POP display 102 includes other sensors 116 that provide additional measurements.  For example, sensors 116 may include an accelerometer that is used to detect when product is added or removed from POP display 102.  As\nanother example, POP display 102 may be mounted on a door such as a freezer case door found in a grocery store.  The accelerometer on POP display 102 may be used to detect when the door is opened and closed.  This information may be correlated with other\ninformation to determine, for example, how many people walk past the display, how many people stop to look at the display, how long a person looks at items displayed in the freezer before opening the door, how long the door is opened, and whether\nproducts are removed from the freezer.  Yet another example is a light sensor may be used to determine when the display was unpacked and when the store is opened or closed (as described below, this may be used to determine the actual deployment rate for\na set of POP displays).  Many POP displays are never deployed and the use of sensors 116 may allow tracking of POP display deployment and addressing such deployment issues based on the deployment information collected.\n FIG. 2 depicts a block diagram representation of an embodiment of an interaction between customer device 118, wireless beacons 114, and server 126.  In certain embodiments, customer device 118 receives first Bluetooth LE packet 700 from first\nwireless beacon 114A.  First wireless beacon 114A may be, for example, a wireless beacon located at or near a retail entrance (e.g., a store entrance).  In certain embodiments, first wireless beacon 114A is located in an area where customer device 118 is\nable to communicate with server 126 (e.g., the customer device has wireless connectivity (either through WiFi or cellular transmission with the server).  Upon receipt of first Bluetooth LE packet 700, SDK 125 may inspect memory cache 124 and determine if\nthe memory cache contains up-to-date data (content) for first wireless beacon 114A.  If the content is not up-to-date in memory cache 124, then SDK may contact 702 server 126 (e.g., the content server) and retrieve 704 the latest content (e.g., content\n128) associated with first wireless beacon 114A.  The retrieved content may be stored in memory cache 124.\n In some embodiments, server 126 may be aware of the location of first wireless beacon 114A and/or other wireless beacons (identifiable by their unique identifiers) associated with the first wireless beacon.  The other wireless beacons (e.g.,\nsecond wireless beacons 114B, shown in FIG. 2) may be other wireless beacons that are nearby first wireless beacon 114A.  In certain embodiments, second wireless beacons 114B are wireless beacons that are located in the same store as, or in proximity to,\nfirst wireless beacon 114A.  In some embodiments, second wireless beacons 114B are wireless beacons in other stores at other locations that are associated with the particular venue of first wireless beacon 114A (e.g., the beacons are associated with a\nsingle retail chain).\n Knowing the association between first wireless beacon 114A and second wireless beacons 114B, server 126 may, therefore, transmit the latest content for the second wireless beacons in addition to transmitting the latest content for the first\nwireless beacon.  The content for both first wireless beacon 114A and second wireless beacons 114B may be stored in memory cache 124.  Transmitting the latest data for second wireless beacons 114B may improve customer device 118 user's experience as\ninformation for each subsequent wireless beacon encountered may already be on the customer device and accessed immediately as the subsequent beacons are encountered (e.g., when SDK 125 receives second Bluetooth LE packet 706 from the second wireless\nbeacons).  This may be particularly advantageous in areas where there is limited or no data connectivity (e.g., where it would otherwise be impossible to download the content associated with a newly encountered wireless beacon).  For example, when\ncustomer device 118 enters a store and detects first wireless beacon 114A, the customer device may automatically download the latest content associated with all second wireless beacons 114B in the store and store the content in memory cache 124 rather\nthan incrementally downloading content as the customer device encounters each second wireless beacon.  Incremental downloading may be slower and/or may not be possible as one wanders deeper into a physical structure and customer device 118 loses wireless\nnetwork connectivity (e.g., enters cellular deadspots within the structure).  Again, SDK 125 manages this functionality and, from the perspective of mobile application 127, the SDK notifies the mobile application of push and pull events (described\nherein) as well as delivering any associated content from memory cache 124 to the mobile application.  Mobile application 127 may then display content from memory cache 124 to the customer on display 119.  FIGS. 3A-3K depict examples of content being\ndisplayed on display 119.  In some embodiments, display 119 allows the customer to interact with content displayed by mobile application 127 (e.g., the content may include a menu of options for selection by the customer).\n An additional advantage of SDK 125 is that the SDK may transmit to server 126 location information available from customer device 118 about the customer device's location along with the unique identifier received from wireless beacon 114.  In\nsome embodiments, the location information is sent to server 126 when a request for content is made from the server.  In certain embodiments, location information about the location of customer device 118 includes GPS data (such as latitude/longitude\ndata) from the customer device (e.g., using built-in GPS on the customer device).  In some embodiments, location information about the location of customer device 118 includes detected WiFi networks (e.g., WiFi networks accessed by the customer device). \nIn some embodiments, mobile application 127 provides SDK 125 with the location of customer device 118 (e.g., the mobile application may tell the SDK which store associated with the mobile application at which the customer device is located).  The\nlocation information of customer device 118 along with the unique identifier from wireless beacon 114 may allow server 126 to identify the physical or retail location (e.g., a specific store number for a retail chain) of the wireless beacon having the\nunique identifier.\n FIG. 4 depicts a flowchart of an embodiment of method 800.  Method 800 may be used to assess a location of wireless beacon 114 and POP display 102.  In 802, a plurality of POP displays 102 and their wireless beacons 114 may be associated with a\nselected campaign.  As described herein, a \"campaign\" refers to an advertising, a marketing, or a promotional campaign associated with a particular retail product or a grouping of products associated with one campaign.  For example, the campaign may be a\nspecial sale for a limited time for the particular retail product.  In some embodiments, the campaign is associated with specific retailers, specific stores within a retail chain, and/or specific geographic locations.  In some embodiments, the campaign\nhas a selected time period associated with the campaign (e.g., the campaign is active for a selected amount of time).\n In some embodiments, associating wireless beacons 114 with the selected campaign in 802 includes associating the wireless beacons with a selected campaign associated with a specific retailer.  For example, wireless beacons 114 may be designated\nfor a specific advertising campaign intended for a specific retailer.  In 804, the wireless beacons may be randomly distributed to a plurality of retail locations.  Even though the selected campaign may be known for wireless beacons 114, the exact final\nlocation of POP displays 102 with the wireless beacons is typically unknown (as described below for step 314 in FIG. 9).  Thus, each of the retail locations that receive the randomly distributed POP displays 102 may be associated with the same selected\ncampaign.\n After POP displays 102, along with wireless beacons 114, are placed at their intended locations and the wireless beacons are activated (e.g., activated at either their final display location or a temporary storage location such as a store back\n(or storage) area), one or more different methods may be used to assess a retail location of each of the POP displays (e.g., the store at which each POP display is located).  For example, as shown in FIG. 4, method 806A, method 806B, method 806C, and\nmethod 806D may each be used, either alone or in combination, to, in 808, assess the retail location of a selected POP display 102 and wireless beacon 114.  Methods 806A, 806B, 806C, 806D may be used to assess the retail location of multiple POP displays\n102.  In some embodiments, one method may be used to assess the retail location of all the POP displays associated with the selected campaign.  In some embodiments, one method may be used to assess the retail location of a first POP display while another\nmethod is used to assess the retail location of a second POP display, a third POP display, a fourth POP display, etc.\n FIG. 5 depicts a flowchart of an embodiment of method 806A used to assess a location of a POP display.  Once POP display 102 is placed at a retail location (e.g., in 804, shown in FIG. 4), method 806A may be used to assess the retail location of\nthe POP display using interaction with customer device 118 and SDK 125 on the customer device.  In 900, customer device 118 may receive a packet (e.g., a data packet such as packet 700, shown in FIG. 2) from wireless beacon 114.  The packet may include\nthe unique identifier for wireless beacon 114.\n In 902, SDK 125 may combine the received unique identifier along with geographic information on the location of customer device 118.  For example, the geographic information may include the geographic location of customer device 118 such as, but\nnot limited to, latitude and longitude location or GPS location of the customer device.  In 904, SDK 125 may then provide the geographic location of customer device 118 along with the unique identifier to a remote server (e.g., server 126, shown in FIGS.\n1 and 2).  In some embodiments, a time stamp from customer device 118 is also provided to the remote server.  In certain embodiments, SDK 125 provides unique identifiers for a plurality of wireless beacons along with the geographic location of customer\ndevice 118.\n In 906, the remote server may then assess or determine the retail location of wireless beacon 114 with the unique identifier using the geographic location information provided along with the unique identifier.  If multiple unique identifiers are\nsent to the remote server, the remote server may determine the retail location of each of the wireless beacons with the unique identifiers associated with the geographic location information.  The determined retail location may be, for example, a retail\nstore number associated with a retail chain associated with the selected campaign.  In some embodiments, the retail location is determined using the geographic location in combination with other information available to the remote server, including, but\nnot limited to, information from other customer devices and/or information about retail locations associated with the selected campaign.  In some embodiments, the remote server assesses the time stamp received from SDK 125 in combination with the\ngeographic location of customer device 118, the unique identifier, and the retail locations associated with the selected campaign.  Assessing the time stamp may allow the remote server to assess if the POP display is active during a selected time period\nassociated with the selected campaign for the POP display.\n Information from other customer devices may include, but not be limited to, geographic location information from interaction of other customer devices with the wireless beacon.  Thus, in some embodiments, the remote server may use information\nfrom multiple customer devices to determine the retail location of a wireless beacon.  The information about retail locations associated with the selected campaign may be provided to the remote server or obtainable by the remote server using information\ninput earlier about the selected campaign.  In some embodiments, the remote server stores the retail location information along with the unique identifier in a database (e.g., information 130 on server 126, shown in FIG. 1).\n The retail location determined in 906 may be provided to method 800, shown in FIG. 4, to either be used as the assessed retail location in 808 or used in other methods (e.g., 806B or 806C) to determine the retail locations of other POP displays. In some embodiments, the retail location determined in 906 may be used to assess if the location of the POP display and the wireless beacon has changed.  For example, the remote server may look up the unique identifier and assess if a previous location\nfor the unique identifier was recorded to assess if any change in location has occurred.\n FIG. 6 depicts a flowchart of an embodiment of method 806B used to assess a location of a POP display.  Method 806B may include assessing the retail location of POP display 102 and wireless beacon 114 using the presence of other detected\nwireless beacons (POP displays) with known retail locations in proximity to the wireless beacon and interaction with one or more customer devices 118.  In some embodiments, multiple wireless beacons are interacting with a single customer device 118\n(e.g., the single customer device receives packets from multiple wireless beacons at or around the same time).  In some embodiments, the wireless beacons are interacting with multiple customer devices 118 at or around the same time with a remote server\nreceiving information from the multiple customer devices (with knowledge of the customer devices being at the same location).\n As shown in FIG. 6, method 806B includes determining, at the remote server, a retail location of a first POP display in 910.  The retail location of the first POP display may be determined, for example, using method 806A, shown in FIG. 5, method\n806C, shown in FIG. 7, or method 806D, shown in FIG. 8.  Determining the retail location of the first POP display in 910, as shown in FIG. 6, allows the remote server to know the retail location of the first POP display.  In some embodiments, the retail\nlocation of the first POP display may be known and provided to the remote server in 910 (e.g., a separate entity or application provides the retail location of the first POP display or the retail location the first POP display is to be sent to is known\nbefore being sent to the location).  In some embodiments, the same retail location is determined (and then known) for multiple POP displays in 910 (e.g., the same retail location may be determined (and then known) for two or more POP displays).  The\nremote server may associate together the multiple POP displays at the same retail location.\n After the location of the first POP display(s) is determined (or known) in 910, customer device 118 may receive a first data packet (or a set of first data packets for multiple POP displays) with the unique identifier for the first POP\ndisplay(s) in 912.  At or around the same time, customer device 118 may receive a second data packet from a second POP display in 914.  The second POP display may have a retail location that is unknown to the remote server.  The second data packet may\ninclude the unique identifier for the second POP display.\n In 916, SDK 125 on customer device 118 may provide the unique identifier for the first POP display(s) and the unique identifier for the second POP display to the remote server (e.g., server 126, shown in FIGS. 1 and 2).  In 918, the remote\nserver may determine, based on the remote server receiving both the unique identifier for the first POP display(s) and the unique identifier for the second POP display at the same time, that the second POP display is at the same retail location as the\nfirst POP display(s).  Put another way, the remote server determines that the second POP display is at the same retail location as the first POP display(s) because the remote server receives both unique identifiers from the same customer device, which is\nat the retail location.  The retail location of the second POP display determined in 918 may be provided to method 800, shown in FIG. 4, to be used as the assessed retail location in 808.\n In some embodiments, the remote server may receive the unique identifier for the first POP display(s) and the unique identifier for the second POP display from different customer devices in 916 (e.g., two or more different mobile devices).  In\nsuch embodiments, however, the remote server may receive other identifying information (e.g., geographic location information or specific content related information) that allows the remote server to associate the unique identifier for the first POP\ndisplay(s) with the unique identifier for the second POP display and determine that the POP displays are at the same retail location in 918.\n FIG. 7 depicts a flowchart of an embodiment of method 806C used to assess a location of a POP display.  Method 806C may include assessing the retail location of POP display 102 and wireless beacon 114 using communication with a network gateway\nlocated at the retail location.  In certain embodiments, as shown in FIG. 2, network gateway 710 is located at retail location 200.  Multiple network gateways 710 may be installed at known retail locations for interfacing with POP displays after the POP\ndisplays are distributed and reach the retail location.  For example, network gateways 710 may be permanent network gateways installed at each retail location in a retail chain or supply chain with the location of each network gateway being known.  Thus,\nfor POP display distribution, the retail locations of multiple network gateways 710 are known by the remote server.\n Network gateway 710 may be a wireless network gateway.  For example, network gateway 710 may be any hardware (e.g., a processor and one or more wireless antenna) capable of networked communication over one or more wireless communication networks\nand/or interfacing between wireless communication networks (e.g., interfacing between a local area network (LAN) and a wide area network (WAN)).  Communication networks may include, but not be limited to, WANs cellular networks, wireless networks, and\nthe Internet.  In certain embodiments, network gateway 710 is connected to the Internet and is capable of interfacing and communicating using Bluetooth LE, WiFi, sub-gigahertz radio, cellular, and other longer-range radio bands.\n In embodiments with network gateway 710 using sub-gigahertz radio, wireless beacons 114 may be capable of broadcasting over sub-gigahertz (or another longer-range radio band) in addition to Bluetooth LE.  Sub-gigahertz broadcasting may provide\nincreased range of data transmission as compared to Bluetooth LE (e.g., sub-gigahertz may have a transmission range of up to about a mile).  Sub-gigahertz broadcasting may include, for example, broadcasting over the ISM band (UHF).  In some embodiments,\nhowever, other longer broadcast range (and detection range) radio bands may be used instead of sub-gigahertz radio bands.  For example, broadcast radio bands such as, but not limited to, WiFi, LORA, or ZigBee may be used in wireless beacons 114 and/or\nnetwork gateway 710.  In certain embodiments, wireless beacons 114 are equipped with a communication chip (e.g., wireless transceiver 120A) capable of both Bluetooth LE and sub-gigahertz broadcasting (or another longer-range radio band).  While Bluetooth\nLE may be used for broadcasting to mobile devices (or other Bluetooth LE capable devices), a larger LAN may be provided between wireless beacons 114 and network gateway 710 by using the longer broadcast range provided by sub-gigahertz radio (or another\nlonger-range radio band).  In some embodiments, network gateway 710 may provide a data collection network (e.g., a LAN for data collection) for collecting data broadcast 712 by wireless beacons 114 (e.g., unique identifiers, sensor data, etc.) and\ntransmitting the data over a communication network 714 (e.g., a WAN connected to the remote server).\n In certain embodiments, as shown in FIG. 7, method 806C includes a network gateway (e.g., network gateway 710) at a known retail location (e.g., retail location 200) receiving a data packet (e.g., data broadcast 712) from wireless beacon 114 on\nPOP display 102 in 920.  The data packet may include the unique identifier of the wireless beacon and the POP display.  The network gateway may provide the unique identifier to the remote server in response to receiving the data packet in 922.  In some\nembodiments, the network gateway provides the unique identifier to the remote server over communication network 714 (e.g., cellular network, WiFi network, or the Internet).  In some embodiments, the network gateway provides the unique identifier to the\nremote server using customer device 118.  For example, the network gateway provides the unique identifier along with identifying/location information about the network gateway to SDK 125 on customer device 118.  SDK 125 may then provide this information\nto the remote server when customer device 118 communicates with the remote server.\n In 924, the remote server may associate the POP display having the unique identifier with the network gateway providing the unique identifier and the retail location of the network gateway.  Using this association, the remote server may\ndetermine the retail location of the POP display with the unique identifier because the retail location of the associated network gateway is known (e.g., the installation location of the network gateway is known as described above).  In some embodiments,\nthe network gateway associates the POP display having the unique identifier with the retail location of the network gateway and provides data about the association to the remote server, which then stores information about the retail location of the POP\ndisplay.  The retail location of the POP display having the unique identifier determined in 924 may be provided to method 800, shown in FIG. 4, to be used as the assessed retail location in 808.\n In some embodiments, method 806C includes assessing a signal strength between the POP display with the unique identifier and the network gateway in 926.  The assessed signal strength may be provided to the remote server along with the unique\nidentifier in 922.  The remote server may use the assessed signal strength to determine a specific (or relatively specific) location of the POP display within the retail location.  For example, the exact location of the network gateway at the retail\nlocation may be known (e.g., in a server room at the retail location).  The assessed signal strength may provide information that is used to estimate the distance between the POP display with the unique identifier and the network gateway.  From the\nestimated distance, the specific location of the POP display within the retail location may be determined (e.g., estimated or approximated).\n FIG. 8 depicts a flowchart of an embodiment of method 806D used to assess a location of a POP display.  Method 806D may be used to assess the retail location of the POP display using interaction with customer device 118 and SDK 125 on the\ncustomer device.  In 930, customer device 118 may receive a packet (e.g., a data packet such as packet 700, shown in FIG. 2) from wireless beacon 114.  The packet may include the unique identifier for wireless beacon 114.  In some embodiments, customer\ndevice 118 may receive multiple packets from multiple wireless beacons, each packet having the unique identifier for the originating wireless beacon.\n In 932, SDK 125 may combine the received unique identifier along with information about the retail location of customer device 118 from 933.  In certain embodiments, the retail location of customer device 118 in 933 is provided by another\napplication (or entity) located on the customer device.  For example, mobile application 127 (located in software package 122 on customer device 118, as shown in FIG. 1) may provide the retail location of the customer device.  The manner in which mobile\napplication 127 determines the retail location of customer device 118 may be unknown to SDK 125.  For example, mobile application 127 may be a retailer \"app\" that determines the retail location (e.g., store number) of customer device 118 through an\nunknown or proprietary algorithm.  Regardless of the manner in which mobile application 127 determines the retail location of customer device 118, SDK 125 may receive the retail location known by the mobile application in 933 and combine this information\nwith the unique identifiers for the wireless beacons.\n In 934, SDK 125 may then provide the retail location of customer device 118 along with the unique identifier to the remote server.  In some embodiments, a time stamp from customer device 118 is also provided to the remote server.  In certain\nembodiments, SDK 125 provides unique identifiers for a plurality of wireless beacons along with the retail location of customer device 118.\n In 936, the remote server may assess or determine the (selected) retail location of wireless beacon 114 with the unique identifier by associating the wireless beacon with the provided retail location of customer device 118.  If multiple unique\nidentifiers are sent to the remote server, the remote server may determine the retail location of each of the wireless beacons with the unique identifiers by associating the retail location of customer device 118 with each wireless beacon.  The retail\nlocation of the POP display determined in 936 (the POP display having the wireless beacon with the unique identifier) may be provided to method 800, shown in FIG. 4, to be used as the assessed retail location in 808.\n Identification of the retail location of wireless beacon 114 by server 126 through the interaction of the wireless beacon with customer device 118 allows the wireless beacon's location to be dynamically cataloged by the server and potentially\nother portions of the overall system, including the wireless beacon itself.  Identification of the retail location by server 126 may be advantageous in that it does not require that the final destination of wireless beacon 114 and circuit board 104 be\nknown at the time of manufacture or distribution since its location may be determined without any external intervention after POP display 102 containing the circuit board has been set up in its intended final location.\n The above disclosed embodiments may be utilized to optimize embodiments of a manufacturing supply chain associated with POP displays 102 and circuit board 104.  As noted above, the location of a given wireless beacon and the other wireless\nbeacons that it may connect to need not be known before the wireless beacon, or circuit board, is installed in its final location.  Current commercially available products, however, treat the installation of wireless beacons as a network infrastructure\nproject and do not contemplate integrating wireless beacons into other systems (e.g., POP displays).  Thus, for a given store, technicians typically install hard points to supply power and network connectivity for each wireless beacon, which severely\nlimits a store manager's flexibility in placing the beacons.  Additionally, once a wireless beacon is deployed in a location, moving the wireless beacon may require bringing in a technician to disconnect and then rewire each wireless beacon in its new\nlocation.  Location changes must also be accompanied by revisions to the database that describes the placement of each wireless beacon.  This mode of deployment is fundamentally incompatible with conventional POP displays (e.g., non-connected POP\ndisplays), which typically can be moved around at will.  If wireless beacons and their attendant power and networking requirements are installed in POP displays, then the POP displays may become inflexible infrastructure points without a system that\nallows for flexible movement and locating of the POP displays as described herein.\n As described herein, the embodiments of POP display system 100 and POP display 102 with wireless beacon 114 may provide increased flexibility for the placement and movement of the POP displays because of the modular, ad hoc nature of the\ndisclosed embodiments.  POP display system 100 may provide a flexible and easy to deploy system that does not require specialized technicians and/or expensive wiring.  In certain embodiments, a store manager is able to simply assemble POP display 102 and\nplace the POP display in the store anywhere desired as if it was a conventional display.\n Additionally, the disclosed modular POP display system 100 and methods associated with the system simplify the overall supply chain for POP displays.  Traditionally, POP displays are manufactured in bulk at a factory and then sent to a\ndistribution center where the displays may remain in storage for months at a time before distribution to a final location.  When incorporating wireless beacons, this creates a logistical problem as one would ideally like to know where a given wireless\nbeacon will end up (e.g., so the display can be programmed based on the final location).  Current manufacturing supply chains, however, are not structured to accommodate that level of granularity.  Typically, large pallets of displays are manufactured,\nstored, and bulk distributed to stores en masse without regard to a specific display's journey.  This method is not a problem when a display simply contains a static display and perhaps a coupon and/or a product holder.  For POP displays that interact\n(e.g., dynamically interact), via wireless beacons, with a customer device so that location, content, and customer specific information may be communicated between the customer device and a remote server, however, such distribution methods are\nproblematic as they require experienced and expensive technicians to install wireless beacon enabled displays at their known final location.  Once installed, a human may manually configure each wireless beacon enabled display with its location and\nidentification information, as well as record such information for use in a database.  The manual installation, however, may be tedious, time consuming, and difficult to implement on a consistent basis.\n FIG. 9 depicts a flowchart of a manufacturing supply chain associated with POP displays 102.  Supply chain 300 utilizes embodiments of POP display system 100, POP display 102, and circuit board 104 disclosed herein that via their ad hoc nature,\nmay be flexible and readily compatible with existing manufacturing practices.  In certain embodiments, large quantities of POP displays 102 may end up at a final display location without advanced planning and interaction between the POP displays with\ncircuit boards 104, customer devices (e.g., customer devices 118), and a server (e.g., server 126) may provide a configuration to POP system 100 as needed.\n In certain embodiments, as shown in FIG. 1, circuit board 104 includes battery 106.  In some embodiments, battery 106 is a non-removable battery or a permanently affixed battery.  Battery 106 being a non-removable battery may provide power\nsavings in the context of the supply chain 300, shown in FIG. 9.  Non-removable batteries may be desirable in supply chain 300 because they are cheaper and are more reliable.  For example, it may be more difficult for a permanently affixed battery to be\njarred out of its connection as compared to a normal (removable) battery mounted in a typical holder.  Additionally, another downside to removable batteries is that they may have to be installed on site by the personnel who set up POP display 102 with\ncircuit board 104 in its final location.  Such personnel may fail to install the batteries correctly or even install the batteries at all, rendering POP display 102 useless for its intended purpose.\n A consideration in using a non-removable (e.g., permanent) battery as battery 106, as shown in FIG. 1, is that, due to the fact that the battery cannot be replaced, it is important to preserve battery life to maximize the service life of circuit\nboard 104.  To increase battery life, in certain embodiments, as shown in FIG. 1, circuit board 104 includes switch 108.  Switch 108 may be a switch such as a power jumper or functional equivalent (e.g., a dip switch).  Switch 108 may provide a removable\nconnection between battery 106 and the rest of the circuitry in circuit board 104 (e.g., wireless beacon 114).  Thus, no power is drained from battery 106 when switch 108 is not installed or turned on.  Not installing switch 108 allows circuit board 104\nto be stored for extended lengths of time (e.g., approximately one year or greater) without reducing any useable power storage in battery 106.\n As shown in FIG. 9, POP display 102 (shown in FIG. 1) is manufactured in factory 302.  After manufacture in factory 302, POP display 102 may be moved from the factory to distribution center 304.  In certain embodiments, POP display 102 moves\nfrom factory 302 to distribution center 304 along with circuit board 104.  Circuit board 104 may have battery 106 disconnected from any power draining circuitry (e.g., controller 112 and/or wireless beacon 114).  For example, switch 108 may disconnect\npower to power draining circuitry.  At distribution center 304, circuit board 104 (with deactivated circuitry) and POP display 102 are received in 306.  At 308, switch 108 is installed or turned on to provide power from battery 106 to the rest of the\ncircuitry in circuit board 104 and wireless beacon 114 is activated.  In certain embodiments, wireless beacon 114 is configured such that, upon its first activation, the wireless beacon scans for specific connections that are associated with its\nconfiguration mode.\n The use of a power switch (e.g., switch 108) in circuit board 104 and supply chain 300 provides several advantages.  One advantage is that switch 108 ensures that battery 106 is disconnected at the time of manufacture of POP display 102 to\nextend the life of the battery.  Another advantage is that the wireless beacon 114 is deactivated until switch 108 is connected.  Deactivating wireless beacon 114 allows distribution center 304 to activate a small number of POP displays at a time in a\nconfiguration procedure discussed below.  If, for example, the thousands, or even hundreds of thousands, of POP displays in a warehouse (e.g., distribution center 304) were active at the same time and their wireless beacons were actively broadcasting\nand/or scanning, the resulting electronic cacophony could render all communication and configuration difficult or even impossible.  Empirical testing has shown that having more than 50 active beacons in close proximity may cause severe wireless\ninterference problems.  Thus, a distribution center full of active beacons could be rendered completely useless.  Furthermore, transportation rules and requirements often restrict the transmission of electromagnetic signals by shipped items.  Thus, the\nability to disable wireless broadcasts by the included beacons both after initial manufacture and during distribution has the additional benefit of facilitating the transport of embodiments of POP displays 102 with wireless beacons 114.\n In some embodiments, it may be desirable to physically activate a battery at the time a circuit board is removed from its shipping container.  In such embodiments, an insulator may be placed between battery terminals to turn the controller off. \nSuch an insulator may be tethered to the external shipping enclosure in such a way that the battery insulator is pulled from between the terminals as the circuit board is removed from the shipping enclosure.  Removing the battery insulator may then\nactivate the controller and the wireless beacon.\n In certain embodiments, after wireless beacon 114 establishes a connection in 308, variations of the wireless beacon's parameters are configured in controller 112 via software in 310.  Controller 112 may be configured, for example, via the BLE\nconnection made between wireless beacon 114 and a programmer device (e.g., a wireless programming device).  In certain embodiments, wireless beacons 114 are associated with a selected campaign in 310 (e.g., as part of step 802, shown in FIG. 4).  During\nconfiguration in 310, time parameters may be set in controller 112 such that wireless beacon 114 is able to determine and act upon a target start date of a (selected) campaign.  For example, controller 112 and clock 113 may be configured with the current\ndate and time as well as the target date and time for the selected campaign to begin.  In certain embodiments, this process is simplified to absolute amount of time between the time of configuration and the beginning of the selected campaign.  Various\nother parameters may be set during the configuration in 310 as well.\n In certain embodiments, during configuration 310, unique label 117, shown in FIG. 1, is scanned or otherwise identified.  In some embodiments, unique label 117 is added to circuit board 104 before arriving at distribution center 304.  In some\nembodiments, unique label 117 is added at distribution center 304.  Scanning unique label 117 may allow the unique label to be associated with wireless beacon 114 and its unique identifier.  For example, unique label 117 may be scanned and the unique\nlabel may be programmed (via the BLE connection) to be associated with the unique identifier for wireless beacon 114 and/or other associated data such as, but not limited to, an activation date or campaign associated with circuit board 104 and the\nwireless beacon.  Associating unique label 117 with wireless beacon 114 (and its associated data) allows information about the wireless beacon to be accessed while the wireless beacon is asleep by scanning the unique label.  Unique label 117 may be\nscanned while wireless beacon 114 is asleep to obtain programmed information (e.g., campaign information or activation time) and place circuit board 104 into a proper POP display (as described below) before the POP display is shipped to a final display\nlocation.\n After configuration is complete in 310, the configuration application disconnects from controller 112 and wireless beacon 114 (and circuit board 104) may enter sleep mode 312.  In sleep mode 312, wireless beacon 114 may shut down its Bluetooth\nradio, and the circuitry in circuit board 104 may enter a low power mode to conserve the batteries before the circuit board is installed onto POP display 102 and also while the POP display is stored in advance of being shipped to a final location (e.g.,\na retail location).  Thus, even though switch 108 has been activated in distribution center 304, the problem of having too many active Bluetooth radios (e.g., wireless beacons) in close proximity in the distribution center is inhibited as each activated\nradio only remains active for a short period of time as configuration of the wireless beacon is conducted.  Also, it is worth noting that the low power mode referred to above includes any control circuitry, such as a microcontroller, not just the\nBluetooth beacon itself.  It will also be apparent to one of ordinary skill in the art that such circuitry may be implemented in various layouts, such as in several discrete chips or one integrated chip.\n After circuit board 104 and wireless beacon 114 enter sleep mode 312, the circuit board may be installed in POP display 102 in 313.  After circuit board 104 is installed, POP display 102 may be moved (e.g., shipped or transported) in 314.  POP\ndisplay 102 may be moved to final display location 316, as shown in FIG. 9.  Final display location 316 may be, for example, a retail or shopping location for POP display 102 to be located in front of customers to provide an interactive customer\nexperience.  At final display location 316, POP display 102 may be assembled in 318 and placed in an operating location in 320.  After being placed at the operating location in 320, when the configuration circuitry (programmed in 310) determines that the\ntarget date and time for the campaign has been reached, the circuitry awakens from its sleep state at the configured time in 322.  In 322, wireless beacon 114 activates its wireless (e.g., Bluetooth) radio and campaign related operations begin in 324.\n As described above, the embodiment of supply chain 300 may provide a solution to the problem of how to build POP displays with wireless beacons in advance while not running down the batteries while the displays are being stored.  Using supply\nchain 300 may also ensure that the associated radios are inactive while the POP display is being stored and transported.  These aspects may be important when considering the overall supply chain.\n In some embodiments, POP display 102 includes a light sensor (e.g., one of sensors 116, shown in FIG. 1, is a light sensor).  In such embodiments, the light sensor may be used to trigger activation of the radio only after the light sensor\ndetects visible light level above a threshold and the campaign date has started.  Using the light sensor to trigger activation may further conserve power by not turning the radio on if POP display 102 has not been unpacked or the store is closed and the\nlights are off.  The radio and some sensors, however, may not need to operate under the same schedule or conditions.  For example, certain sensors may be active even when POP display 102 is in low power mode.  These sensors may be used to allow POP\ndisplay 102 to determine whether the display is being transported or being set up at a final location.  One skilled in the art will recognize that this permits one to select what a skilled artisan deems to be the optimal tradeoff between battery\nconsumption and situational awareness.  Some embodiments may utilize other sensors to determine whether the store is open or whether potential customers are nearby (e.g., proximity sensors may be used to determine if customers are nearby).  Information\nfrom these sensors may also be combined with the campaign start date to determine whether the radio should be activated.  In some embodiments, one of sensors 116 is an accelerometer.  The accelerometer may be active when POP display 102 is in low power\nmode and may trigger activation of the radio when any movement is registered by the accelerometer.  If, however, a light sensor does not detect a visible light level above the threshold and/or the campaign date has not started, the activation of the\nradio may be temporary (e.g., only for a short, selected amount of time).\n In some embodiments, POP display 102 (and POP display system 100) allows for the tracking of the deployment rate of POP displays at a retail location.  FIG. 10 depicts a block diagram of an embodiment of a plurality of POP displays 102 at retail\nlocation 200.  POP displays 102 at retail location 200 are capable of communicating with each other and/or with server 126.  In certain embodiments, POP displays 102 communicate with each other and provide information to a single POP display (e.g., POP\ndisplay 102').  POP display 102' may then communicate with server 126 to provide the information to the server, as shown in FIG. 10.\n In some embodiments, server 126 detects information about the relative locations of wireless beacons 114.  For example, since each wireless beacon 114 is equipped with a unique identifier, server 126, upon detecting and determining the identity\nof one wireless beacon 114 in communication range with another wireless beacon, may determine that the beacons are associated with a specific store.  In some embodiments, depending on the configured broadcast mode of wireless beacons 114, server 126 may\ncommunicate with wireless beacon 114' to relay information to the wireless beacon and other wireless beacons 114 within communication range of wireless beacon 114'.  In this way, a group of beacons may operate as an ad hoc distributed communication\nnetwork, which is advantageous as this does not require that the network be set up and configured beforehand by a technician.\n It is known in the industry that, on average, only a fraction of POP displays delivered to a store are actually deployed.  In certain embodiments, however, wireless beacon 114 is capable of communicating, whether directly or indirectly, with\nserver 126, as shown in FIGS. 1 and 10.  Thus, wireless beacon 114 may relay information gathered by sensors 116 indicating that its POP display has been deployed.  For example, if POP display 102 reaches its campaign start time but its light sensor\nindicates darkness for a prolonged period of time, it may be inferred that the POP display was not deployed on a timely basis.\n In some embodiments, data collected from sensors 116 and/or customer device 118 (and/or other sources) that is stored in information 130 on server 126, shown in FIG. 1, may be used for historical analysis of the performance of POP display 102. \nIn some embodiments, the historical analysis data is correlated with saved data from wireless beacon 114 to further gauge customer engagement.  For example, combining information about dwell time with the fact of whether the potential customer made a\npull request may be used to gauge customer engagement.  In some embodiments, the historical data is correlated with third party data (e.g., retailer data such as purchase history, etc.).  Correlating the historical data with third party data may provide\nfurther information useful to the retailer to enhance a customer's experience.\n In some embodiments, various statistical analyses are utilized on historical data collected from POP display 102.  Statistical analyses that may be used include, but are not limited to, machine learning and data mining techniques, set theory,\nmultivariate statistics, and time series analyses.  Examples of machine learning include deep learning, neural networks, support vector machines, random forests, decision tree learning, association rule learning, inductive logic, clustering, Bayesian\nnetworks, reinforcement learning, representation learning, similarity and metric learning, sparse dictionary learning, and genetic algorithms.  Examples of data mining, which is often coextensive with machine learning, include spatial data mining,\ntemporal data mining, sensor data mining, pattern mining, and subject-based data mining.  In some embodiments, these techniques are used for aspects besides historical analysis.  For example, smoothing techniques associated with some types of temporal\ndata mining may be used to filter a series of RSSI signal strength values used in bump detection disclosed herein.\n Examples of different types of data that may be collected an analyzed for POP displays 102 are illustrated in FIGS. 11A-11G.  FIG. 11A illustrates an example of a campaign calendar.  FIG. 11B illustrates an example of an interface displaying\nvarious statistics related to POP display deployment and sales.  FIG. 11C illustrates an information screen related to the product associated with the POP display.  FIG. 11D illustrates an interface displaying national deployment information for POP\ndisplays.  FIG. 11E illustrates an interface displaying state deployment information for POP displays.  FIG. 11F illustrates an interface displaying supply chain information.  FIG. 11G illustrates an interface displaying a sales analysis associated with\nthe POP display.\n In some embodiments, POP display 102 includes components or devices that reduce the surrounding infrastructure requirements for supporting the POP display.  Specifically, POP display 102 may be equipped with wireless transmission functionality\nto transmit any recorded measurements or information derived therein as described above.  This information may be transmitted to a network gateway (e.g., network gateway 710, shown in FIG. 2) located within or near the store (e.g., retail location 200,\nshown in FIG. 2).  In certain embodiments, instead of a network gateway, an employee of the retailer or a POP display service provider may be equipped with a mobile device that contains an application adapted to connect to POP display 102 and retrieve\ndata from the point of purchase display to be relayed to a server.  This connection may be accomplished utilizing the same wireless connection that is used for beacon functionality, or may be conducted by other standard wireless transmission protocols as\ndescribed herein, e.g., IEEE 802.11.  This minimizes the need for wireless transmission infrastructure to support the retrieval of information collected by the point of purchase displays.\n In some embodiments, POP display system 100 utilizes the application already installed on a potential customer's mobile device (e.g., software package 122 on customer device 118) to relay the collected information to a server (e.g., server 126). As shown in FIG. 1, POP display 102 may connect with customer device 118 via the same wireless connection that enables beacon functionality (e.g., Bluetooth connection between wireless beacon 114 and wireless transceiver 120A) and deliver the data to the\ncustomer device.  Customer device 118 may then transmit the data to server 126 via wireless transceiver 120B or another wireless transceiver.  Since transmission through customer device 118 may incur a data charge on the customer, the application may be\nconfigured to prompt the user for permission to do so.  In some embodiments, the application may offer the user some form of compensation for the use of their data connection.  For example, a discount on some good or service in the store.  In this way,\ndata can be moved to server 126 by users who are normally just walking around the store and requires no special infrastructure.  Additionally, the data to be transmitted may be compressed using standard techniques to minimize the amount of bandwidth\nconsumed, and the application (e.g., the SDK) may be configured to, under certain cases, delay the transfer of data over the mobile device's wireless network connection to the server if the network connection is not over WiFi and/or the mobile device is\nnot connected to an external power source.  In other words, the mobile device may be directed to save the data and wait until the customer is connected to a wireless network where transmission of the data will not adversely impact the customer's wireless\ndata plan or the mobile device's battery life.\n In certain embodiments, one or more process steps described herein may be performed by one or more processors (e.g., a computer processor) executing instructions stored on a non-transitory computer-readable medium.  For example, communication\nbetween POP display 102, customer device 118, server 126, and/or network gateway 710, shown in FIG. 1, may have one or more steps performed by one or more processors executing instructions stored as program instructions in a computer readable storage\nmedium (e.g., a non-transitory computer readable storage medium).  In certain embodiments, controller 112, on POP display 102, software package 125, on customer device 118, server 126, and/or network gateway 710 include program instructions in the\ncomputer readable storage medium.\n FIG. 12 depicts a block diagram of one embodiment of exemplary computer system 410.  Exemplary computer system 410 may be used to implement one or more embodiments described herein.  In some embodiments, computer system 410 is operable by a user\nto implement one or more embodiments described herein such as communication between POP display 102, customer device 118, server 126, and/or network gateway 710, shown in FIG. 1.  In the embodiment of FIG. 12, computer system 410 includes processor 412,\nmemory 414, and various peripheral devices 416.  Processor 412 is coupled to memory 414 and peripheral devices 416.  Processor 412 is configured to execute instructions, including the instructions for communication between POP display 102, customer\ndevice 118, server 126, and/or network gateway 710, which may be in software.  In various embodiments, processor 412 may implement any desired instruction set (e.g. Intel Architecture-32 (IA-32, also known as x86), IA-32 with 64 bit extensions, x86-64,\nPowerPC, Sparc, MIPS, ARM, IA-64, etc.).  In some embodiments, computer system 410 may include more than one processor.  Moreover, processor 412 may include one or more processors or one or more processor cores.\n Processor 412 may be coupled to memory 414 and peripheral devices 416 in any desired fashion.  For example, in some embodiments, processor 412 may be coupled to memory 414 and/or peripheral devices 416 via various interconnect.  Alternatively or\nin addition, one or more bridge chips may be used to coupled processor 412, memory 414, and peripheral devices 416.\n Memory 414 may comprise any type of memory system.  For example, memory 414 may comprise DRAM, and more particularly double data rate (DDR) SDRAM, RDRAM, etc. A memory controller may be included to interface to memory 414, and/or processor 412\nmay include a memory controller.  Memory 414 may store the instructions to be executed by processor 412 during use, data to be operated upon by the processor during use, etc.\n Peripheral devices 416 may represent any sort of hardware devices that may be included in computer system 410 or coupled thereto (e.g., storage devices, optionally including computer accessible storage medium 500, shown in FIG. 13, other\ninput/output (I/O) devices such as video hardware, audio hardware, user interface devices, networking hardware, etc.).\n Turning now to FIG. 13, a block diagram of one embodiment of computer accessible storage medium 500 including one or more data structures representative of POP display 102 (depicted in FIG. 1) and/or memory cache 124 (depicted in FIG. 1)\nincluded in an integrated circuit design and one or more code sequences representative of communication between POP display 102, customer device 118, server 126, and/or network gateway 710 (shown in FIGS. 1 and 2).  Each code sequence may include one or\nmore instructions, which when executed by a processor in a computer, implement the operations described for the corresponding code sequence.  Generally speaking, a computer accessible storage medium may include any storage media accessible by a computer\nduring use to provide instructions and/or data to the computer.  For example, a computer accessible storage medium may include non-transitory storage media such as magnetic or optical media, e.g., disk (fixed or removable), tape, CD-ROM, DVD-ROM, CD-R,\nCD-RW, DVD-R, DVD-RW, or Blu-Ray.  Storage media may further include volatile or non-volatile memory media such as RAM (e.g. synchronous dynamic RAM (SDRAM), Rambus DRAM (RDRAM), static RAM (SRAM), etc.), ROM, or Flash memory.  The storage media may be\nphysically included within the computer to which the storage media provides instructions/data.  Alternatively, the storage media may be connected to the computer.  For example, the storage media may be connected to the computer over a network or wireless\nlink, such as network attached storage.  The storage media may be connected through a peripheral interface such as the Universal Serial Bus (USB).  Generally, computer accessible storage medium 500 may store data in a non-transitory manner, where\nnon-transitory in this context may refer to not transmitting the instructions/data on a signal.  For example, non-transitory storage may be volatile (and may lose the stored instructions/data in response to a power down) or non-volatile.\n Embodiments of the present disclosure may be realized in any of various forms.  For example some embodiments may be realized as a computer-implemented method, a computer-readable memory medium, or a computer system.  Other embodiments may be\nrealized using one or more custom-designed hardware devices such as ASICs.  Other embodiments may be realized using one or more programmable hardware elements such as FPGAs (field programmable gate arrays).\n In some embodiments, a non-transitory computer-readable memory medium may be configured so that it stores program instructions and/or data, where the program instructions, if executed by a computer system, cause the computer system to perform a\nmethod, e.g., any of a method embodiments described herein, or, any combination of the method embodiments described herein, or, any subset of any of the method embodiments described herein, or, any combination of such subsets.\n In some embodiments, a wireless device (or wireless station) may be configured to include a processor (or a set of processors) and a memory medium, where the memory medium stores program instructions, where the processor is configured to read\nand execute the program instructions from the memory medium, where the program instructions are executable to cause the wireless device to implement any of the various method embodiments described herein (or, any combination of the method embodiments\ndescribed herein, or, any subset of any of the method embodiments described herein, or, any combination of such subsets).  The device may be realized in any of various forms.\n Although specific embodiments have been described above, these embodiments are not intended to limit the scope of the present disclosure, even where only a single embodiment is described with respect to a particular feature.  Examples of\nfeatures provided in the disclosure are intended to be illustrative rather than restrictive unless stated otherwise.  The above description is intended to cover such alternatives, modifications, and equivalents as would be apparent to a person skilled in\nthe art having the benefit of this disclosure.\n The scope of the present disclosure includes any feature or combination of features disclosed herein (either explicitly or implicitly), or any generalization thereof, whether or not it mitigates any or all of the problems addressed herein. \nAccordingly, new claims may be formulated during prosecution of this application (or an application claiming priority thereto) to any such combination of features.  In particular, with reference to the appended claims, features from dependent claims may\nbe combined with those of the independent claims and features from respective independent claims may be combined in any appropriate manner and not merely in the specific combinations enumerated in the appended claims.\n Further modifications and alternative embodiments of various aspects of the embodiments described in this disclosure will be apparent to those skilled in the art in view of this description.  Accordingly, this description is to be construed as\nillustrative only and is for the purpose of teaching those skilled in the art the general manner of carrying out the embodiments.  It is to be understood that the forms of the embodiments shown and described herein are to be taken as the presently\npreferred embodiments.  Elements and materials may be substituted for those illustrated and described herein, parts and processes may be reversed, and certain features of the embodiments may be utilized independently, all as would be apparent to one\nskilled in the art after having the benefit of this description.  Changes may be made in the elements described herein without departing from the spirit and scope of the following claims.", "application_number": "15153180", "abstract": " Systems and methods for using wireless beacons in point of purchase\n     (\"POP\") displays to facilitate the delivery of consumer oriented content\n     to mobile devices is disclosed herein. A processor coupled to a wireless\n     beacon may record information obtained from sensors coupled to a POP\n     display. The sensors may assess environmental activity and/or user\n     activity around the POP display. A mobile device may record information\n     about interactions between the mobile device and the wireless beacon and\n     store the information in a memory cache on the mobile device. The\n     information from the processor and the mobile device may be transmitted\n     to a remote server via wireless communication from both the processor and\n     the mobile device at selected times.\n", "citations": ["6539280", "6571279", "6837427", "6951305", "7021535", "7233241", "7310070", "7374096", "7415426", "7423516", "7510123", "7535337", "7549579", "7614556", "7870019", "8010067", "8070065", "8082177", "8408457", "8531273", "8598988", "8700453", "8798541", "8823521", "8847754", "9107152", "9202245", "9298677", "9363784", "9426627", "9544744", "9629113", "9646328", "9679310", "9898749", "9928536", "9929876", "9953493", "20020176388", "20020183004", "20030167347", "20060080460", "20060087474", "20060109125", "20060114832", "20070067203", "20070114291", "20070254670", "20080021766", "20080045172", "20080243626", "20080256510", "20080284566", "20090030787", "20090288132", "20100121567", "20100131352", "20100201891", "20100235373", "20110178862", "20110178863", "20120142271", "20120171958", "20120191530", "20120228240", "20120239504", "20120306617", "20120310570", "20120315839", "20130013407", "20130073431", "20130210461", "20130217332", "20130268316", "20140206346", "20140244341", "20140249918", "20140249928", "20140254466", "20140269508", "20140282620", "20140316896", "20140324615", "20140324638", "20140337151", "20140358666", "20140358685", "20140359565", "20150025936", "20150026020", "20150079942", "20150081474", "20150082382", "20150100403", "20150140982", "20150142387", "20150142552", "20150161665", "20150215781", "20150237463", "20150248663", "20150249058", "20150278867", "20150278888", "20150281877", "20150287045", "20150317661", "20150371321", "20160034954", "20160042251", "20160050645", "20160092943", "20160092966", "20160094940", "20160095063", "20160110622", "20160134930", "20160148270", "20160171486", "20160171512", "20160178379", "20160217519", "20160225029", "20160227359", "20160227368", "20160232560", "20170124603", "20170169444", "20180027386", "20180047059", "20180108043", "20180158382", "20180165711", "20180189819"], "related": ["62160949", "62256248", "62291828"]}, {"id": "20170068978", "patent_code": "10373190", "patent_name": "System and methods for determining location of pop displays with wireless\n     beacons through engagement with mobile devices", "year": "2019", "inventor_and_country_data": " Inventors: \nWalden; Charles (Austin, TX)  ", "description": "BACKGROUND OF THE INVENTION\n 1.  Field of the Invention\n Embodiments disclosed herein relate to the use of wireless beacons in point of purchase (\"POP\") displays to facilitate the delivery of consumer oriented content to mobile devices.  Certain embodiments relate to systems and methods for\ndetermining locations of wireless beacons and POP displays using interactions with mobile devices.\n 2.  Description of the Relevant Art\n POP (\"point of purchase\") displays are often used in retail environments to display content for particular products associated with the POP displays.  POP displays typically include signs, graphics, or other marketing materials that communicate\ninformation about associated products and are intended to draw a shopper's (e.g., customer's) attention to the products associated with the displays.  POP displays may be used as integral components for marketing or promotional campaigns.  POP displays\noften contribute to the success of these campaigns.\n Traditional POP display signage, which runs the gamut from a simple plastic holder for a card with product information to illuminated translucent graphic films in an atmospheric light box, are static in nature and are unable to customize the\ninformation conveyed to a potential customer based upon the customer's interest level.  Other conventional POP display signage may include video displays that offer limited interactive options.  Thus, there is a need for POP displays that are capable of\ndynamically interacting with potential customers.  The manufacture, distribution, and/or deployment in retail settings of multiple POP displays, however, poses unique challenges, especially when the ability to dynamically interact with potential\ncustomers is included with the POP displays.  Developments in mobile device technology and mobile communication technology allows for dynamic interaction with potential customers in retail environment.\n Beacons are among the most important new mobile technologies helping merchants engage with consumers via mobile communication while the consumers are in brick and mortar stores.  For many years, near field communication (NFC) was considered to\nbe the technology that would deliver such data to retailers and help them track how customers behave in-store.  NFC, however, has reached certain limits and beacons (and beacon technology) provides increased potential for providing customer engagement to\nshoppers in store environments.\n Beacons may be low-cost devices that communicate with mobile device (e.g., smartphone) apps through a Bluetooth signal.  Beacons are expected to directly influence over $4 billion worth of US retail sales this year at top retailers (0.1% of the\ntotal), and that number may climb tenfold in 2016.  Current beacon implementations are relatively crude and typically broadcast the same, static content (e.g., a coupon, regardless of circumstances or a potential customer's demonstrated intent).  Beacon\ntechnology has enormous potential to enhance the shopping experience.  For example, beacon technology may make it quicker and easier for customers to access the information and products they are looking for or provide special offers or discounts to loyal\nshoppers.  Beacon technology can also provide retailers with invaluable data about their customers' shopping habits as well as the activity of their staff.  Thus, retailers may make improvements to the store layout by identifying store flow, maintaining\nservice standards, and maintaining operations that will benefit both customer and retailer.  Current implementations of beacon technology, however, have failed to develop a more dynamic set of interactions with potential customers, particularly those\nwhich are based on and distinguish between various location-based actions.\n There has been some development in the use of beacon technology in store (customer) environments, however, the implementation of beacon technology remains limited.\n United States Patent Application Publication No. 2015/0287045, filed Apr.  6, 2015 by Brown et al., which is incorporated by reference as if fully set forth herein, describes a \"system for monitoring compliance with a retail display program\nincludes a beacon coupled to a promotional display structure.\" The system includes a \"computing device [that] is configured to compare the location-specific data and time stamp to the specified retail facility and time period to determine whether the\npromotional display structure is displayed in the specified retail facility during the specified time period.\" The system in Brown, however, requires that \"Each promotional display structure 20 is intended to be displayed at a specified retail facility\n50.  Moreover, in the example embodiment, each promotional display structure 20 is intended to be displayed at a specified location 60 within specified retail facility 50.\" Thus, the system of Brown requires that the intended location of each\n\"promotional display structure\" be known before the display structures are sent to their locations so that compliance of the structure (e.g., is it displayed in the correct location) may be determined.  However, as is known in the art of promotional\ndisplays, it can often be very difficult and cumbersome to ensure and know the intended locations of promotional displays.  For example, a large set of identical promotional displays are often sent to a warehouse for storage before being randomly sent\nout to retail locations without any thought being given as to the intended location for each specific promotional display.  Further, multiple locations within a retail location may be intended for a given display once it reaches the retail location.\n United States Patent Application Publication No. 2014/0282620, filed Mar.  15, 2013 by Nuovo et al., which is incorporated by reference as if fully set forth herein, states: \"detecting an advertised device identifier and comparing the detected\ndevice identifier with device identifiers stored on the mobile device.  If there is a match, the match can trigger an event.  The event can be requesting content associated with the matched device identifier, receiving the requested content, and\nrendering the received content.  The requested content can be selected to have additional, corresponding content downloaded and rendered.\" This identification is done by \"an application that operates on a mobile device.  When executed, the application\ncan cause the mobile device to search for device identifiers, e.g., media access controller addresses and/or broadcast identifiers (IDs), which are advertised by wireless beacon units, such as WiFi beacon units and Bluetooth beacon units.\"\n United States Patent Application Publication No. 2002/0176388 filed Mar.  19, 2002, by Rankin and Simons, which is incorporated by reference as if fully set forth herein, describes a centralized system for updating beacons.  The system includes\n\"a modification to the Bluetooth system to enable the connectionless broadcast of short messages from Bluetooth beacons.  This can be achieved by exploiting the Bluetooth Inquiry phase by extending the very short ID packet sent out during this mode and\nusing the extra space thus gained to carry a small amount of information.  This information can be Bluetooth system related data or one-way application data.  This scheme has the potentially useful feature of being backwards-compatible with legacy\nBluetooth devices that are not able to understand this extra field.\"\n United States Patent Application Publication No. 2002/0183004 filed Mar.  15, 2002, by Fulton et al., which is incorporated by reference as if fully set forth herein, describes specialized beacons that are dedicated to either inquiries or\ntransmitting information to a client.\n United States Patent Application Publication No. 2007/0254670, filed May 1, 2006, \"System and method for optimizing throughput in a wireless network,\" by Kawaguchi and Le, which is incorporated by reference as if fully set forth herein,\ndiscusses throttling bandwidth within a mesh network.  For example, \"When the switch 10 determines that a selected mesh node is utilizing a portion of the bandwidth outside of the predetermined threshold range, the switch 10 executes a predetermined\naction (e.g., throttling) on transmissions from the selected node to provide increased bandwidth to mesh nodes further from the switch 10 than the selected node.\"\n WIPO Patent Application WO/2013/054144, \"Method of Estimating the Position of a User Device Using Radio Beacons and Radio Beacons Adapted to Facilitate the Methods of the Invention\" by Usman, et al., which is incorporated by reference as if\nfully set forth herein, discloses methods for \"calculating an estimate of the position of the user device taking into account transmit power data concerning the transmit power level of the one or more said radio beacons .  . . .\" Page 2, lines 16-18.\n U.S.  Pat.  No. 6,571,279, issued to Herz et al., which is incorporated by reference as if fully set forth herein, discloses location based services, but more from the perspective of a cellular network.  It states, \"The operation of the location\nenhanced information delivery system as described herein makes use of the fact that each user has a `beacon`, which generally serves as a user identification instrumentality.  The beacons emit identifiers which can be used to associate users with the\ndetected devices.  The beacon can be correlated with location, such as by use of a wireless subscriber station or other systems with known technology.\"\n United States Patent Application Publication No. 2014/0358666, \"Cross-Channel Personalized Promotion Platform,\" by Baghaie and Dempski, which is incorporated by reference as if fully set forth herein, describes a platform for allowing\nadvertisers to purchase promotional opportunities on user's mobile devices.\n United States Patent Application Publication No. 2012/0315839, \"Analyzing Audiences at Public Venues,\" by Mumcuoglu and Engel, which is incorporated by reference as if fully set forth herein, discusses the use of wireless signals to physically\nlocate a user but does not discuss the utilization of that information in real time to transmit pertinent information to that user.\n Despite the previous disclosures described above, there remains many needs related to the concepts of adjusting or \"throttling\" a connection (or a transmission), determination of bumping, or the notions of pushing or pulling content beyond\ngeneric downloading of specific content from a centralized server as discussed herein.  In addition, there is still a need for monitoring surrounding activity and assessing user locations and/or display locations.  In certain applications, transmissions\n(or connections) may need to be throttled with respect to a specific location (e.g., a point of sale).  In some applications, there is a need for the content transmitted over that connection to be varied in relation to either the throttling or determined\nrange.  Thus, there are still improvements needed in the application of beacon technology to engage with customers during their in-store shopping experience and for supporting customers' in-store shopping experiences.\nSUMMARY OF THE INVENTION\n In certain embodiments, context aware solutions are provided for delivering content to potential customers in an efficient manner in association with POP (\"point of purchase\") displays that are used in retail environments.  Embodiments disclosed\nherein include wireless beacon technology associated with the POP displays that can vary the content delivered based upon the relative distance of the potential customer and whether the potential customer has indicated any product interest.  This allows\nfor content to be \"throttled\" to potential customers based on a software configuration that exempts customers who have not signaled interest from being included in messages that might overload and/or annoy the customers and/or trigger privacy concerns\ndue to unrequested content.  Potential customers that have signaled interest, however, may receive content without any throttling.  Furthermore, embodiments disclosed herein may distinguish between \"push\"--use cases where content is provided without an\nintentional request by the customer--and \"pull\"--use cases where content has been intentionally requested by the customer through a physical interaction between the POP display and a customer device (e.g., the customer device being \"bumped\", i.e.,\nintentionally placed in close proximity to an area on the POP display).  The exact information that is pushed or pulled may be located on a remote server that may be configured for each potential use case.  Embodiments disclosed herein may provide\nimplementations that conserve power by allowing devices (e.g., wireless beacons) to be configured to activate at a later date, namely after they have arrived at a certain destination (e.g., a display location).  Embodiments disclosed herein may provide\nfor utilizing context awareness to reduce power consumption when it is unlikely for a potential customer to be around (e.g., when a retail area is dark or no activity is detected).  Furthermore, this context awareness may enable manufacturing and\ndistributions methods to be suited to large-scale production and distribution of POP displays across many locations.  Improved logistical schemes for manufacturing and distributing the embodiments disclosed herein may also be provided since one need not\ndetermine beforehand the exact final location of the POP display and its beacon before distribution to individual retail or advertising venues.\n In certain embodiments, the disclosed systems and methods include a variety of sensors to aid in assessing a proximity of potential customers to the POP display and measuring the surrounding environment.  This information may be recorded and\nanalyzed to gain additional insights about consumer behavior and to gauge the device's performance.  Additionally, information may be inferred from the signal strength of user devices (e.g., mobile devices) carried by potential customers.  This\ninformation may also be retained and analyzed.  In some embodiments, the system may transmit data to a server through various means.  For example, a traditional permanent gateway may be utilized, or user devices with network connectivity that are carried\nby employees or potential customers may be utilized to relay the stored information to the server.\n Embodiments disclosed herein may provide efficient means for communicating with individuals, either to inform or to advertise, and to record information about the disclosed embodiments' performance and its environment.  In some embodiments, the\nrecorded information is harnessed to enable improved logistical schemes to be provided for manufacturing and distributing the disclosed embodiments even when it is unknown where and/or when the disclosed device will be delivered and/or begin operation.\n In certain embodiments, a mobile device includes: a processor; a display; a wireless transceiver; a memory cache; and a software package installed on the mobile device, the software package including a mobile application and a software developer\nkit (SDK) configured to interpret wireless data packets received by the mobile device; wherein the mobile device is configured to: receive a data packet in a wireless signal through the wireless transceiver, the wireless signal being broadcast by a\ncircuit board coupled to a point of purchase (POP) display, the POP display including a consumer product display, wherein the consumer product display is configured to be associated with a selected campaign, wherein the POP display is configured to be\ndistributed at random to a retail location selected from a plurality of retail locations associated with the selected campaign, wherein the circuit board includes a processor and a wireless beacon, the circuit board broadcasting the wireless signal from\nthe wireless beacon, and wherein the wireless signal includes the data packet with a unique identifier for the wireless beacon; and provide a geographic location of the mobile device and the unique identifier for the wireless beacon to a remote server in\nresponse to receiving the data packet with the unique identifier broadcast in the wireless signal, wherein the remote server is configured to determine a selected retail location of the wireless beacon based on the provided geographic location of the\nmobile device, the selected retail location being one of the plurality of retail locations associated with the selected campaign.\n In certain embodiments, a method for assessing a location of a point of purchase (POP display) includes: receiving, in a mobile device at a selected retail location, a wireless signal including a data packet with a unique identifier for a\nwireless beacon, the wireless signal being broadcast by the wireless beacon, wherein the wireless beacon is located on a circuit board coupled to a point of purchase (POP) display, the POP display including a consumer product display, wherein the\nconsumer product display is configured to be associated with a selected campaign, wherein the POP display is configured to be distributed at random to a retail location selected from a plurality of retail locations associated with the selected campaign;\nproviding, from the mobile device to a remote server, a geographic location of the mobile device in response to receiving the data packet with the unique identifier from the wireless beacon at the selected retail location, wherein the geographic location\nof the mobile device is provided by a software package installed on the mobile device in response to the mobile device receiving the data packet, and wherein the unique identifier for the wireless beacon is provided from the mobile device in addition to\nthe geographic location of the mobile device; and determining, using the remote server, the selected retail location of the POP display based on the provided unique identifier for the wireless beacon and the provided geographic location of the mobile\ndevice.\n In certain embodiments, a non-transient computer-readable medium including instructions that, when executed by one or more processors, causes the one or more processors to perform a method that includes: receiving, in a mobile device at a\nselected retail location, a wireless signal including a data packet with a unique identifier for a wireless beacon, the wireless signal being broadcast by the wireless beacon, wherein the wireless beacon is located on a circuit board coupled to a point\nof purchase (POP) display, the POP display including a consumer product display, wherein the consumer product display is configured to be associated with a selected campaign, wherein the POP display is configured to be distributed at random to a retail\nlocation selected from a plurality of retail locations associated with the selected campaign; providing, from the mobile device to a remote server, a geographic location of the mobile device in response to receiving the data packet with the unique\nidentifier from the wireless beacon at the selected retail location, wherein the geographic location of the mobile device is provided by a software package installed on the mobile device in response to the mobile device receiving the data packet, and\nwherein the unique identifier for the wireless beacon is provided from the mobile device in addition to the geographic location of the mobile device; and determining, using the remote server, the selected retail location of the POP display based on the\nprovided unique identifier for the wireless beacon and the provided geographic location of the mobile device. BRIEF DESCRIPTION OF THE DRAWINGS\n Features and advantages of the methods and apparatus described herein will be more fully appreciated by reference to the following detailed description of presently preferred but nonetheless illustrative embodiments when taken in conjunction\nwith the accompanying drawings in which:\n FIG. 1 depicts a block diagram of an embodiment of a point of purchase display system.\n FIG. 1A depicts an example of an embodiment of a POP display.\n FIG. 2 depicts a block diagram representation of an embodiment of an interaction between a customer device, wireless beacons, and a server.\n FIGS. 3A-3K depict examples of content being displayed on a display of a mobile device.\n FIG. 4 depicts a flowchart of an embodiment of a method to assess a location of a wireless beacon and its POP display.\n FIG. 5 depicts a flowchart of an embodiment of a method used to assess a location of a POP display.\n FIG. 6 depicts a flowchart of a second embodiment of a method used to assess a location of a POP display.\n FIG. 7 depicts a flowchart of a third embodiment of a method used to assess a location of a POP display.\n FIG. 8 depicts a flowchart of a fourth embodiment of a method used to assess a location of a POP display.\n FIG. 9 depicts a flowchart of a manufacturing supply chain associated with POP displays.\n FIG. 10 depicts a block diagram of an embodiment of a plurality of point of purchase displays at a retail location.\n FIG. 11A illustrates an example of a campaign calendar.\n FIG. 11B illustrates an example of an interface displaying various statistics related to POP display deployment and sales.\n FIG. 11C illustrates an information screen related to the product associated with the POP display.\n FIG. 11D illustrates an interface displaying national deployment information for POP displays.\n FIG. 11E illustrates an interface displaying state deployment information for POP displays.\n FIG. 11F illustrates an interface displaying supply chain information.\n FIG. 11G illustrates an interface displaying a sales analysis associated with the POP display.\n FIG. 12 depicts a block diagram of one embodiment of an exemplary computer system.\n FIG. 13 depicts a block diagram of one embodiment of a computer accessible storage medium.\n While the disclosure is susceptible to various modifications and alternative forms, specific embodiments thereof are shown by way of example in the drawings and will herein be described in detail.  It should be understood, however, that the\ndrawings and detailed description thereto are not intended to limit the disclosure to the particular form illustrated, but on the contrary, the intention is to cover all modifications, equivalents and alternatives falling within the spirit and scope of\nthe present disclosure as defined by the appended claims.  The headings used herein are for organizational purposes only and are not meant to be used to limit the scope of the description.  As used throughout this application, the word \"may\" is used in a\npermissive sense (i.e., meaning having the potential to), rather than the mandatory sense (i.e., meaning must).  Similarly, the words \"include,\" \"including,\" and \"includes\" mean including, but not limited to.  Additionally, as used in this specification\nand the appended claims, the singular forms \"a\", \"an\", and \"the\" include singular and plural referents unless the content clearly dictates otherwise.  Furthermore, the word \"may\" is used throughout this application in a permissive sense (i.e., having the\npotential to, being able to), not in a mandatory sense (i.e., must).  The term \"include,\" and derivations thereof, mean \"including, but not limited to.\" The term \"coupled\" means directly or indirectly connected.\n The term \"automatically\" refers to an action or operation performed by a computer system (e.g., software executed by the computer system) or device (e.g., circuitry, programmable hardware elements, ASICs, etc.), without user input directly\nspecifying or performing the action or operation.  Thus the term \"automatically\" is in contrast to an operation being manually performed or specified by the user, where the user provides input to directly perform the operation.  An automatic procedure\nmay be initiated by input provided by the user, but the subsequent actions that are performed \"automatically\" are not specified by the user, i.e., are not performed \"manually\", where the user specifies each action to perform.  For example, a user filling\nout an electronic form by selecting each field and providing input specifying information (e.g., by typing information, selecting check boxes, radio selections, etc.) is filling out the form manually, even though the computer system must update the form\nin response to the user actions.  The form may be automatically filled out by the computer system where the computer system (e.g., software executing on the computer system) analyzes the fields of the form and fills in the form without any user input\nspecifying the answers to the fields.  As indicated above, the user may invoke the automatic filling of the form, but is not involved in the actual filling of the form (e.g., the user is not manually specifying answers to fields but rather they are being\nautomatically completed).  The present specification provides various examples of operations being automatically performed in response to actions the user has taken.\n Various units, circuits, or other components may be described as \"configured to\" perform a task or tasks.  In such contexts, \"configured to\" is a broad recitation of structure generally meaning \"having circuitry that\" performs the task or tasks\nduring operation.  As such, the unit/circuit/component can be configured to perform the task even when the unit/circuit/component is not currently on.  In general, the circuitry that forms the structure corresponding to \"configured to\" may include\nhardware circuits and/or memory storing program instructions executable to implement the operation.  The memory can include volatile memory such as static or dynamic random access memory and/or nonvolatile memory such as optical or magnetic disk storage,\nflash memory, programmable read-only memories, etc. The hardware circuits may include any combination of combinatorial logic circuitry, clocked storage devices such as flops, registers, latches, etc., finite state machines, memory such as static random\naccess memory or embedded dynamic random access memory, custom designed circuitry, programmable logic arrays, etc. Similarly, various units/circuits/components may be described as performing a task or tasks, for convenience in the description.  Such\ndescriptions should be interpreted as including the phrase \"configured to.\" Reciting a unit/circuit/component that is configured to perform one or more tasks is expressly intended not to invoke 35 U.S.C.  .sctn.  112(f) interpretation for that\nunit/circuit/component.\n In an embodiment, hardware circuits in accordance with this disclosure may be implemented by coding the description of the circuit in a hardware description language (HDL) such as Verilog or VHDL.  The HDL description may be synthesized against\na library of cells designed for a given integrated circuit fabrication technology, and may be modified for timing, power, and other reasons to result in a final design database that may be transmitted to a foundry to generate masks and ultimately produce\nthe integrated circuit.  Some hardware circuits or portions thereof may also be custom-designed in a schematic editor and captured into the integrated circuit design along with synthesized circuitry.  The integrated circuits may include transistors and\nmay further include other circuit elements (e.g. passive elements such as capacitors, resistors, inductors, etc.) and interconnect between the transistors and circuit elements.  Some embodiments may implement multiple integrated circuits coupled together\nto implement the hardware circuits, and/or discrete elements may be used in some embodiments.\n The scope of the present disclosure includes any feature or combination of features disclosed herein (either explicitly or implicitly), or any generalization thereof, whether or not it mitigates any or all of the problems addressed herein. \nAccordingly, new claims may be formulated during prosecution of this application (or an application claiming priority thereto) to any such combination of features.  In particular, with reference to the appended claims, features from dependent claims may\nbe combined with those of the independent claims and features from respective independent claims may be combined in any appropriate manner and not merely in the specific combinations enumerated in the appended claims.\nDETAILED DESCRIPTION OF EMBODIMENTS\n The following examples are included to demonstrate preferred embodiments.  It should be appreciated by those of skill in the art that the techniques disclosed in the examples which follow represent techniques discovered by the inventor to\nfunction well in the practice of the disclosed embodiments, and thus can be considered to constitute preferred modes for its practice.  However, those of skill in the art should, in light of the present disclosure, appreciate that many changes can be\nmade in the specific embodiments which are disclosed and still obtain a like or similar result without departing from the spirit and scope of the disclosed embodiments.\n In this patent, certain U.S.  patents, U.S.  patent applications, and other materials (e.g., articles) have been incorporated by reference.  The text of such U.S.  patents, U.S.  patent applications, and other materials is, however, only\nincorporated by reference to the extent that no conflict exists between such text and the other statements and drawings set forth herein.  In the event of such conflict, then any such conflicting text in such incorporated by reference U.S.  patents, U.S. patent applications, and other materials is specifically not incorporated by reference in this patent.\n Further modifications and alternative embodiments of various aspects of the disclosed embodiments will be apparent to those skilled in the art in view of this description.  Accordingly, this description is to be construed as illustrative only\nand is for the purpose of teaching those skilled in the art the general manner of carrying out the disclosed embodiments.  It is to be understood that the forms of the disclosed embodiments shown and described herein are to be taken as examples of\nembodiments.  Elements and materials may be substituted for those illustrated and described herein, parts and processes may be reversed, and certain features of the disclosed embodiments may be utilized independently, all as would be apparent to one\nskilled in the art after having the benefit of this description of the disclosed embodiments.  Changes may be made in the elements described herein without departing from the spirit and scope of the disclosed embodiments as described in the following\nclaims.\n This specification includes references to \"one embodiment\" or \"an embodiment.\" The appearances of the phrases \"in one embodiment\" or \"in an embodiment\" do not necessarily refer to the same embodiment, although embodiments that include any\ncombination of the features are generally contemplated, unless expressly disclaimed herein.  Particular features, structures, or characteristics may be combined in any suitable manner consistent with this disclosure.\n As used herein, the word \"display\" is intended to include an array of merchandising materials and store-based assets such as, but not limited to, signs, test product or samples, permanent or semi-permanent fixtures, coupon dispensers,\naisle-based video screens, mobile coolers, or other movable assets within a retail outlet.\n FIG. 1 depicts a block diagram of an embodiment of point of purchase (\"POP\") display system 100.  In certain embodiments, system 100 includes POP display 102.  In certain embodiments, circuit board 104 is located on POP display 102.  Circuit\nboard 104 may be, for example, a printed circuit board or any other suitable circuit board for connecting and operating multiple electronic components including, but not limited to, integrated circuits.  Circuit board 104 may be placed (installed) on, or\ncoupled to, POP display 102 during or after manufacturing of the POP display.\n In certain embodiments, circuit board 104 includes battery 106, switch 108, memory 110, controller 112, wireless beacon 114, and sensors 116.  In certain embodiments, controller 112 includes circuitry, an integrated circuit, or a processor\noperable to control operation of wireless beacon 114 and/or other components of circuit board 104 and/or POP display 102.  Memory 110 may include many different types of memory known in the art for use on a circuit board.  For example, memory 110 may be\nflash memory, RAM, EEROM, EEPROM, and/or one-time programmable memory.\n In some embodiments, controller 112 is coupled to clock 113.  Clock 113 may be capable of tracking both date and time.  Clock 113 may be associated with wireless beacon 114 to provide time information (e.g., date and time) to the wireless\nbeacon.  In some embodiments, clock 113 is located in a chip on circuit board 104.  In some embodiments, clock 113 is located in a microprocessor in wireless beacon 114.\n In some embodiments, circuit board 104 includes unique label 117.  Unique label 117 may be added during or after manufacturing of circuit board 104.  Unique label 117 may be, for example, a printed label, such as a QR label or barcode, that can\nbe viewed or electronically scanned for identifying information.  Unique label 117 may include a unique identification for circuit board 104 that differentiates the circuit board from other circuit boards that may be used on other POP displays.  For\nexample, each circuit board 104 may have its own identification number that specifically identifies the circuit board.\n POP display 102, as described herein, may be any display that holds products and/or advertises products.  For example, POP display 102 may include signs, graphics, or other marketing materials that communicate information about a product to a\nconsumer.  FIG. 1A depicts an example of an embodiment of POP display 102.  In some embodiments, POP display 102 includes the product itself.  For example, products such as, but not limited to, demo units of electronic items, appliances, and/or rugs may\nbe a POP display.  POP display 102 is typically placed next to or near the merchandise the display is promoting and/or included as part of the merchandise.  In some embodiments, POP display 102 is utilized to hold, support, or display products associated\nwith the POP display.  In certain embodiments, POP display 102 is a corrugated cardboard display.  POP display 102 may also include displays made from materials such as, but not limited to, paper, paperboard, bristol board, foam cored board, plastic, or\nany other material suitable for holding and/or advertising products.\n POP display 102 may be a component of a marketing or promotional campaign.  In certain embodiments, POP display 102 is generally located in a retail environment (e.g., a retail store) or any other location where a customer purchases product or a\ndecision to purchase product is made.  In some embodiments, POP display 102 is placed in other display locations in order to drive potential customers to a specific area.  For example, POP display 102 may be placed in a window display and used to provide\n(e.g., \"beam\") promotional information to people as they pass by outside a retail store.  Regardless of the location of POP display 102, the POP display may be intended to draw the customer's attention to products associated with the display.  These\nproducts may, in some embodiments, be new products, products on sale, and/or products associated with a special offer.  POP display 102 may also be used to promote special events (e.g., seasonal or holiday-time sales).\n In certain embodiments, as shown in FIG. 1, POP system 100 includes the use of customer device 118.  Customer device 118 may be, for example, a mobile device.  Customer device 118 may be a small computing device, typically small enough to be\nhandheld (and hence also commonly known as a handheld computer or simply handheld).  Mobile devices may be any of various types of computer systems devices which are mobile or portable and which perform wireless communications using WLAN communication. \nExamples of mobile devices include mobile telephones or smart phones (e.g., iPhone.TM., Android.TM.-based phones), and tablet computers such as iPad.TM., Samsung Galaxy.TM., etc. Various other types of devices would fall into this category if they\ninclude Wi-Fi or both cellular and Wi-Fi communication capabilities, such as laptop computers (e.g., MacBook.TM.), portable gaming devices (e.g., Nintendo DS.TM.  PlayStation Portable.TM., Gameboy Advance.TM., iPhone.TM.), portable Internet devices, and\nother handheld devices, as well as wearable devices such as smart watches, smart glasses, headphones, pendants, earpieces, etc. In general, the term \"mobile device\" can be broadly defined to encompass any electronic, computing, and/or telecommunications\ndevice (or combination of devices) which is easily transported by a user and capable of wireless communication using WLAN or Wi-Fi.  In certain embodiments, customer device 118 includes any device used by a customer with display 119 (e.g., an LCD screen\nor touchscreen), one or more wireless transceivers (e.g., wireless transceivers 120A, 120B, shown in FIG. 1), software package 122, and memory cache 124.  Display 119, in some embodiments, includes a user interface for customer device 118 (e.g., the\ndisplay allows interactive input for the user).\n In certain embodiments, wireless beacon 114 on POP display 102 interacts with customer devices 118 carried by potential customers.  Wireless beacon 114 may be configured to interact with customer devices 118 through wireless transceiver 120A. \nIn certain embodiments, wireless transceiver 120A is a Bluetooth Low Energy (\"BLE\") transceiver.\n In certain embodiments, wireless beacon 114 includes a unique identifier associated with the wireless beacon.  The unique identifier may be broadcast by wireless beacon 114, received through wireless transceiver 120A, and used to identify the\nwireless beacon (e.g., the unique identifier may be used by a server to identify the wireless beacon as described herein).  Thus, in embodiments with multiple wireless beacons 114, the wireless beacons broadcast their respective unique identifiers and\nthe unique identifiers may be used to identify and/or differentiate the wireless beacons and, by extension, the circuit board and POP display associated with each wireless beacon.\n Wireless beacon 114 may be a transponder sending data via radio signals.  In certain embodiments, wireless beacon 114 is a Bluetooth Low Energy (\"BLE\") beacon.  A Bluetooth LE beacon may operate in either peripheral or central mode, depending on\nthe circumstances, though in certain embodiments, the beacon may default to peripheral mode.  Chipsets implementing beacon functionality may be commercially available.  Two non-limiting examples are the Texas Instruments CC2541 and CC2600.  The disclosed\nembodiments, however, do not depend on the particular choice of Bluetooth chipset.\n Bluetooth low energy (Bluetooth LE, BLE, also marketed as Bluetooth Smart) is a wireless personal area network technology designed and marketed by the Bluetooth Special Interest Group aimed at applications in the healthcare, fitness, beacons,\nsecurity, and home entertainment industries.  Compared to Classic Bluetooth, Bluetooth Smart is intended to provide considerably reduced power consumption and cost while maintaining a similar communication range.\n Bluetooth Smart was originally introduced under the name Wibree by Nokia in 2006.  It was merged into the main Bluetooth standard in 2010 with the adoption of the Bluetooth Core Specification Version 4.0.  In certain embodiments, wireless\nbeacons 114 are Bluetooth LE beacons.  Bluetooth LE beacons may be used, at least in part, because Bluetooth LE has been widely adopted in customer devices 118 (e.g., mobile devices).  Thus, a potential consumer may likely already have the requisite\nhardware to interact with circuit board 104 and POP display 102.  For example, Bluetooth LE has been built into iPhones and iPads since 2010, and many Android devices since 2013.  Bluetooth LE wireless beacons are also, as the name implies, energy\nefficient, which may be an important consideration for technology deployed on mobile devices.  In certain embodiments, the positioning and data transmission capabilities of Bluetooth LE are also of use, though the embodiments disclosed herein may also be\nimplemented using other wireless standards, including the various versions of IEEE 802.11.\n In certain embodiments, POP display system 100 includes server 126.  Server 126 may communicate with customer device 118 through wireless transceiver 120B on the customer device.  In certain embodiments, wireless transceiver 120B is a\nWiFi-enabled or cellular transceiver.  Server 126 may include content 128.  In certain embodiments, content 128 is uploaded to server 126 via an exposed API (Application Programming Interface).  Content 128 may be included as part of a storage structure\nor storage management system (e.g., a database) accessible by server 126.  For example, content 128 may be stored in a database in an accessible memory of server 126.  In certain embodiments, content 128 includes information that corresponds to\nadvertising, marketing, and/or promotional campaigns associated with POP displays 102.  For example, content 128 may include, but not be limited, campaign start times, campaign time periods, campaign locations, coupons associated with the campaign,\nadvertising and/or marketing associated with the campaign, and promotions associated with the campaign.\n As server 126 includes content 128, the server may be referred to as a \"content server\", though the phrase \"content server\" as used in this disclosure should not be considered strictly limiting.  In some embodiments, the physical server(s)\n(e.g., server 126) that stores content 128 may perform other functionality and/or work in conjunction with other servers to enable some or all of its functionality.  For example, server 126 may work with a load balancing server to optimize its\ncommunications load over a network or authentication servers to validate the entities requesting a download of content.  In some embodiments, server 126 may operate in a distributed nature such that content 128 is distributed over more than one physical\nstorage device or logical drive partitions.  The term \"content server\" is intended to encompass all of these scenarios and any other that one of ordinary skill in the art would contemplate in implementing the disclosed functionality.\n In certain embodiments, server 126 includes information 130.  Information 130 may be included as part of a storage structure or storage management system (e.g., a database) accessible by server 126.  Information 130 may include information\nregarding POP display 102 and wireless beacon 114 such as, but not limited to, the unique identifier, location information (if known), and retail location information for the POP display (e.g., store location information for a specific retailer\nassociated with the POP display).  In some embodiments, information 130 includes information recorded from sensors 116 and/or other components on POP displays 102 as well as information recorded on customer devices 118 that is transmitted to server 126.\n In certain embodiments, SDK (\"Software Developer Kit\") 125 is located in software package 122 on customer device 118, as shown in FIG. 1.  SDK 125 may allow programmers to develop applications (e.g., mobile application 127) for customer device\n118 that interface the customer device with server 126 and circuit board 104.  SDK 125 may abstract low level implementation details of POP display system 100 and simplify the development of software applications compatible with the disclosed\nembodiments.  In certain embodiments, SDK 125 includes functionality to facilitate accessing APIs exposed by server 126 (e.g., the content server) as well as wireless (e.g., Bluetooth) mediated interactions with wireless beacons 114.\n In certain embodiments, mobile application 127 is located in software package 122 on customer device 118.  Mobile application 127 may be coupled to SDK to allow the mobile application to interface and utilize functions of the SDK.  In some\nembodiments, SDK 125 may be embedded in mobile application 127 (e.g., the SDK is a software code element of the mobile application).  Mobile application 127 may be, in some embodiments, a retailer \"app\" or other mobile application written for interaction\nbetween a customer and a specific retailer (e.g., the mobile application may be a customer loyalty app specific for a selected retailer).  In certain embodiments, mobile application 127 provides an interactive interface for the customer through customer\ndevice 118.  For example, mobile application 127 may use display 119 as a user interface (the display is a touchscreen) to allow interactive customer input or the mobile application may use the display in combination with another input system (e.g., a\nkeyboard or voice input) to allow interactive customer input.  In certain embodiments, mobile application 127 utilizes SDK 125, when run on customer device 118, to detect that the customer device is in proximity to a compatible Bluetooth LE beacon (e.g.,\nwireless beacon 114), as described herein.\n In certain embodiments, SDK 125 is configured to receive measurements from customer device 118 through built-in features of the customer device.  For example, SDK 125 may receive measurements from accelerometer, gyroscope, compass, audio, light,\nor Near Field Communication measurements on customer device 118.  These measurements may be utilized to increase the accuracy of calculated location information or used to infer additional information about either a user or an environment of POP display\n102.  For example, information from an accelerometer on customer device 118 may be combined with other information to increase the accuracy of detection of \"bumps\" or recognition of gestures as described below.\n In some embodiments, the measurements received by SDK 125 are sent to server 126 and stored in information 130.  Server 126 may integrate the measurement information from customer device 118 to increase accuracy of location information and/or\ninfer additional information, as described below.  In some embodiments, server 126 may integrate the measurement information with information from external data sources, which may be located in information 130 on the server.  For example, server 126 may\nintegrate store specific information from nearby beacons, geolocation information provided by a retail loyalty application on connected mobile devices, or other information received from third party sources.\n In certain embodiments, POP display system 100 utilizes wireless signal strength to infer distance between customer device 118 and POP display 102.  POP display system 100 may utilize this distance information to modulate and/or control the\nparticular information conveyed to the customer through customer device 118.  In certain embodiments, SDK 125 in software 122 on customer device 118 receives information, based on distance, indicating the detection of \"bumps\" or \"pulls\" (e.g., when a\nuser physically touches (or very nearly so) the customer device against a designated area of POP display 102 (e.g., at or near a \"tap device here for more information\" designated area)).  In the disclosed embodiments, the concept of bumping is applied as\na way for a user to express interest in POP display 102 independent of any technical requirements of the underlying wireless communication protocol being used.\n Various techniques may be utilized to estimate distance between customer device 118 and POP display 102.  For example, in certain embodiments, Received Signal Strength Indication (\"RSSI\") values of Bluetooth signals are measured and analyzed to\ninfer distance.  The distance inferred may be relative or absolute in nature (e.g., the technique may only specify a distance from POP display 102 as opposed to exact position).  By means of illustration, the general relationship between RSSI value and\ndistance is approximately RSSI[dbm]=-(10.times.n.times.log.sub.10 (d)-A), where d is the distance and A is the offset which is the measured RSSI value 1 meter point away from the Bluetooth LE device.  Again, this is provided simply for illustrative\npurposes and other relationships and formulas may be utilized by the disclosed embodiments to infer location information about the customer device and, by extension the customer.  Other examples of values that may be utilized to determine signal strength\ninclude, but are not limited to, packet loss ratio or rate, header error check, cyclic redundancy check, and forward error correction.  Furthermore, the measurement of these various values, including RSSI, may be implemented in numerous ways in hardware. For example, one may utilize Goertzel algorithms to derive signal strength values from a series of transceiver power measurements.  As shown above, the precise implementation details of the measurement to calculate location information can vary and the\nembodiments disclosed herein may be suited to the usage of any measurement to calculate location information.  Furthermore, location related information (e.g., signal strength measurements, values derived from signal strength measurements, identifiers\nassociated with a particular mobile device, timestamps associated with a signal strength reading) may be saved to a memory (e.g., memory 110 or memory cache 124) for future review and/or analysis.  In some embodiments, the location related information\nincludes information about customer device 118.  For example, the information may include information about chipsets, antennas, and/or an operating system of customer device 118.  The information about customer device 118 may be part of the future review\nand/or analysis to increase accuracy in assessing relative location information of the customer device and POP display 102.\n In certain embodiments, signal strength (e.g., Bluetooth signal strength as measured, for example, via RSSI) between POP display 102's wireless beacon 114 and wireless transceiver 120A on customer device 118 is monitored and, if it surpasses a\npredefined threshold or \"trigger\" level, it is inferred that the customer has \"bumped\" the customer device against the POP display and has made a \"pull\" delivery request (e.g., the user has indicated his/her intention to receive or \"pull\" content\nassociated with the POP display).  In certain embodiments, the predefined threshold is set at a signal strength level that indicates that the user has clearly intended to initiate a \"bump\" or \"pull\" with POP display 102.  For example, the predefined\nthreshold may be set at a signal strength level that clearly defines customer device 118 has intentionally been placed on or near to the designated area of POP display 102 by the customer.  In some embodiments, the predefined threshold is combined with\nother information (e.g., information from an accelerometer on customer device 118) to define intent of the customer in \"pulling\" for content.  For example, accelerometer data may be combined with the predefined threshold (measured via RSSI) to recognize\na gesture (e.g., movement of customer device 118 in an intentional way) made by the user that indicates intent of the customer to receive information.\n In certain embodiments, the predefined threshold improves the reliability of bump detection and the threshold may be dynamic in nature.  For example, the threshold may be specified by a formula that accounts for certain variables rather than a\nset static number.  In some embodiments, the algorithm may not allow a new bump to be registered until the signal is outside of a separate threshold, usually higher in value than the entrance threshold.  This restriction may help to prevent spurious\nbumps.  Additional techniques may be utilized to improve bump detection (such as a filter to smooth RSSI values).  In some embodiments, signal profiles for setting the predefined threshold are associated with a type of customer device 118 (e.g., a type\nof mobile device or a type of antenna used in the mobile device).  Server 126 may receive type data for customer device 118 when the customer device is in contact with the server.  Server 126 then may send RSSI signal profiles associated with the type\ndata to the SDK on customer device 118, which stores the signal profiles in memory cache 124 for accessing in assessment of bump indications.\n Some embodiments may utilize different methods for gauging distance.  For example, other performance measures associated with a Bluetooth signal, RSSI values associated with a 802.11 WiFi signal, information from a Near Field Communication\nsignal, etc. may be used.  Regardless of the origin and type of information used, the associated algorithms may utilize the information to detect bumps.  In some embodiments, the detection of bumps is performed in circuit board 104 rather than on\ncustomer device 118.\n Bumping may be used to signal that the customer is explicitly requesting digital content (e.g., requesting content to be display on display 119 of customer device 118).  In the event that a bump is detected, the SDK may provide content to the\ncustomer on customer device 118 (this may be referred to as \"pull\" delivery).  For example, content may be display on display 119 through mobile application 127.  The content may include content stored in memory cache 124, which includes content 128\npreviously received from server 126 as described herein.  Conversely, \"push\" delivery may occur in the absence of a bump, where content 128 may be delivered by server 126 to customers that have not explicitly requested content.  In certain embodiments,\nunsolicited push content is throttled to prevent from overloading the customer with unrequested content, while pull content (e.g., requested content) is not throttled.  In some embodiments, the exact throttling scheme used is configurable by software and\nmay be specified by various entities.  For example, the exact throttling scheme may be specified by a POP display owner, a retailer, an advertising company, a manufacturer of goods or services associated with the POP display, etc.\n In some embodiments, a throttling scheme is personalized for a particular user.  For example, the throttling scheme may include personalized data based on a persona of the user.  The personalized data may be uploaded to and/or stored in memory\ncache 124 on customer device 118.  The persona of the user may include categories based on one or more user preferences.  The preferences may be for categories that include non-specific information about the user (e.g., anonymous information based only\non the behavior of the user).  Using non-specific information may protect privacy and security of the user of customer device 118.  In some embodiments, the persona of the user is defined by preferences specified by a retailer (e.g., through a retailer\napp in SDK 125 on customer device 118).\n Information relevant to the throttling scheme may be incorporated in several aspects of the disclosed embodiments.  First, content 128 may be uploaded to server 126 via an exposed API (Application Program Interface) designed to work with the\noverall device ecosystem.  This API requires that the uploaded content be associated with information that allows server 126 to associate content 128 with specific beacons (e.g., wireless beacon 114).  The API may also require information associated with\nthe uploaded content that will allow customer device 118, via an API call, to determine if the content should be served up based on push or pull.\n In certain embodiments, as shown in FIG. 1, POP display 102 includes sensors 116.  Sensors 116 may provide monitoring of activity in and/or around the POP display.  In certain embodiments, sensors 116 include proximity sensors that detect\nactivity in the vicinity of POP display 102.  Proximity sensor may detect activity based on, for example, heat, light (reflected infrared and/or visible light), sound, and/or images.  Examples of sensors 116 include, but are not limited to, ambient light\nsensors, passive infrared sensors, active infrared sensors, and image based detection sensors.  Other examples include accelerometers, temperature sensors, weight sensors, cameras, and sensors that detect when a product has been dispensed or when a\ndisplay needs to be restocked.\n Sensors 116 may be used to measure and record (and, in some embodiments, timestamp in combination with clock 113) activity around the display and save these measurements in memory 110.  These measurements and recordings may provide information\nthat can be used for detailed analysis of the level of traffic around POP display 102 by time.  The analysis may include determining information such as, but not limited to, how many people walk past the display, how many people stop to look at the\ndisplay, when a door is opened, how long the door is opened, and whether products are removed.  Such analysis may include measuring the timing of the activity, such as how long a potential customer stood in front of the display, commonly referred to as\ndwell time.  Other potential analyses include, but are not limited to, how many shoppers passed by (divided into buckets of time), the average dwell time per shopper, and/or counts of shoppers that had smartphones (customer devices 118) equipped with\nsoftware package 122.  Measurement data from sensors 116 stored in memory 110 may be transmitted (broadcast) in data packets sent out by wireless beacon 114.  The data packets with the measurement data may be received by, for example, customer devices\n118 and/or network gateway 710 to then be transmitted to a remote server (e.g., server 126).\n In certain embodiments, sensors 116 include a proximity sensor that monitors activity only within a defined range (e.g., a defined distance) from POP display 102.  Sensor data may also be used in a transmission throttling scheme as described\nherein (e.g., a particularly crowded store might dictate the use of a different transmission).  Additionally, as described herein, the connection between wireless beacons 114 and/or customer devices 118 may be utilized to share information between POP\ndisplays 102.\n In certain embodiments, information recorded from sensors 116 and/or other components on POP displays 102 as well as information recorded on customer devices 118 is transmitted and stored in server 126 as information 130.  Information recorded\non customer devices 118 may include any information or data relating to interactions between the customer devices and wireless beacons 114, interactions between the customer devices and server 126, other interactions involving the customer devices, and\ndata obtained by the customer devices such as device sensor data (e.g., position and/or movement measurement data) and/or application data from the software package.  In some embodiments, recorded information may be stored in memory cache 124 on customer\ndevice 118 before being transmitted to server 126.  The recorded information stored in memory cache 124 may include information recorded on customer device and/or information recorded from sensors 116 on POP displays 102 (after the data is transmitted to\nthe customer device via wireless beacon 114).  In some embodiments, proximity sensor data is used by server 126 along with position information obtained through wireless transceiver 120B on customer device 118 to improve the accuracy of determining\nlocation information (e.g., location information related to location of wireless beacons and POP displays).\n In certain embodiments, it may be desirable to only allow wireless beacons to broadcast when there is activity near the wireless beacon.  Allowing wireless beacons to only broadcast with nearby activity may allow a large number (e.g., high\ndensity) of wireless beacons to be located in a single retail location as not all of the wireless beacons will be actively broadcasting at the same time.  In certain embodiments, one or more sensors 116 are used in combination with wireless beacon 114 to\nallow the wireless beacon to operate in a low power (non-broadcasting) mode while located in a retail location and only actively broadcast when nearby activity is detected.  For example, sensor 116 may be a proximity sensor that detects activity within a\nselected distance from wireless beacon 114.  When no activity is detected by sensor 116 (e.g., there is an absence of activity), wireless beacon 114 may enter a low power (sleep or non-broadcasting) mode.  In the low power mode, wireless beacon 114 does\nnot respond or provide push/pull events, described herein, as the wireless beacon is not broadcasting any data packets.  If sensor 116 detects any activity (e.g., via proximity detection of a customer/user), wireless beacon 114 may be switched to an\nactive (broadcasting) mode substantially instantaneously.  Wireless beacon 114 may then be active for any push/pull events or content requests associated with customer device 118.\n In some embodiments, POP display 102 includes other sensors 116 that provide additional measurements.  For example, sensors 116 may include an accelerometer that is used to detect when product is added or removed from POP display 102.  As\nanother example, POP display 102 may be mounted on a door such as a freezer case door found in a grocery store.  The accelerometer on POP display 102 may be used to detect when the door is opened and closed.  This information may be correlated with other\ninformation to determine, for example, how many people walk past the display, how many people stop to look at the display, how long a person looks at items displayed in the freezer before opening the door, how long the door is opened, and whether\nproducts are removed from the freezer.  Yet another example is a light sensor may be used to determine when the display was unpacked and when the store is opened or closed (as described below, this may be used to determine the actual deployment rate for\na set of POP displays).  Many POP displays are never deployed and the use of sensors 116 may allow tracking of POP display deployment and addressing such deployment issues based on the deployment information collected.\n FIG. 2 depicts a block diagram representation of an embodiment of an interaction between customer device 118, wireless beacons 114, and server 126.  In certain embodiments, customer device 118 receives first Bluetooth LE packet 700 from first\nwireless beacon 114A.  First wireless beacon 114A may be, for example, a wireless beacon located at or near a retail entrance (e.g., a store entrance).  In certain embodiments, first wireless beacon 114A is located in an area where customer device 118 is\nable to communicate with server 126 (e.g., the customer device has wireless connectivity (either through WiFi or cellular transmission with the server).  Upon receipt of first Bluetooth LE packet 700, SDK 125 may inspect memory cache 124 and determine if\nthe memory cache contains up-to-date data (content) for first wireless beacon 114A.  If the content is not up-to-date in memory cache 124, then SDK may contact 702 server 126 (e.g., the content server) and retrieve 704 the latest content (e.g., content\n128) associated with first wireless beacon 114A.  The retrieved content may be stored in memory cache 124.\n In some embodiments, server 126 may be aware of the location of first wireless beacon 114A and/or other wireless beacons (identifiable by their unique identifiers) associated with the first wireless beacon.  The other wireless beacons (e.g.,\nsecond wireless beacons 114B, shown in FIG. 2) may be other wireless beacons that are nearby first wireless beacon 114A.  In certain embodiments, second wireless beacons 114B are wireless beacons that are located in the same store as, or in proximity to,\nfirst wireless beacon 114A.  In some embodiments, second wireless beacons 114B are wireless beacons in other stores at other locations that are associated with the particular venue of first wireless beacon 114A (e.g., the beacons are associated with a\nsingle retail chain).\n Knowing the association between first wireless beacon 114A and second wireless beacons 114B, server 126 may, therefore, transmit the latest content for the second wireless beacons in addition to transmitting the latest content for the first\nwireless beacon.  The content for both first wireless beacon 114A and second wireless beacons 114B may be stored in memory cache 124.  Transmitting the latest data for second wireless beacons 114B may improve customer device 118 user's experience as\ninformation for each subsequent wireless beacon encountered may already be on the customer device and accessed immediately as the subsequent beacons are encountered (e.g., when SDK 125 receives second Bluetooth LE packet 706 from the second wireless\nbeacons).  This may be particularly advantageous in areas where there is limited or no data connectivity (e.g., where it would otherwise be impossible to download the content associated with a newly encountered wireless beacon).  For example, when\ncustomer device 118 enters a store and detects first wireless beacon 114A, the customer device may automatically download the latest content associated with all second wireless beacons 114B in the store and store the content in memory cache 124 rather\nthan incrementally downloading content as the customer device encounters each second wireless beacon.  Incremental downloading may be slower and/or may not be possible as one wanders deeper into a physical structure and customer device 118 loses wireless\nnetwork connectivity (e.g., enters cellular deadspots within the structure).  Again, SDK 125 manages this functionality and, from the perspective of mobile application 127, the SDK notifies the mobile application of push and pull events (described\nherein) as well as delivering any associated content from memory cache 124 to the mobile application.  Mobile application 127 may then display content from memory cache 124 to the customer on display 119.  FIGS. 3A-3K depict examples of content being\ndisplayed on display 119.  In some embodiments, display 119 allows the customer to interact with content displayed by mobile application 127 (e.g., the content may include a menu of options for selection by the customer).\n An additional advantage of SDK 125 is that the SDK may transmit to server 126 location information available from customer device 118 about the customer device's location along with the unique identifier received from wireless beacon 114.  In\nsome embodiments, the location information is sent to server 126 when a request for content is made from the server.  In certain embodiments, location information about the location of customer device 118 includes GPS data (such as latitude/longitude\ndata) from the customer device (e.g., using built-in GPS on the customer device).  In some embodiments, location information about the location of customer device 118 includes detected WiFi networks (e.g., WiFi networks accessed by the customer device). \nIn some embodiments, mobile application 127 provides SDK 125 with the location of customer device 118 (e.g., the mobile application may tell the SDK which store associated with the mobile application at which the customer device is located).  The\nlocation information of customer device 118 along with the unique identifier from wireless beacon 114 may allow server 126 to identify the physical or retail location (e.g., a specific store number for a retail chain) of the wireless beacon having the\nunique identifier.\n FIG. 4 depicts a flowchart of an embodiment of method 800.  Method 800 may be used to assess a location of wireless beacon 114 and POP display 102.  In 802, a plurality of POP displays 102 and their wireless beacons 114 may be associated with a\nselected campaign.  As described herein, a \"campaign\" refers to an advertising, a marketing, or a promotional campaign associated with a particular retail product or a grouping of products associated with one campaign.  For example, the campaign may be a\nspecial sale for a limited time for the particular retail product.  In some embodiments, the campaign is associated with specific retailers, specific stores within a retail chain, and/or specific geographic locations.  In some embodiments, the campaign\nhas a selected time period associated with the campaign (e.g., the campaign is active for a selected amount of time).\n In some embodiments, associating wireless beacons 114 with the selected campaign in 802 includes associating the wireless beacons with a selected campaign associated with a specific retailer.  For example, wireless beacons 114 may be designated\nfor a specific advertising campaign intended for a specific retailer.  In 804, the wireless beacons may be randomly distributed to a plurality of retail locations.  Even though the selected campaign may be known for wireless beacons 114, the exact final\nlocation of POP displays 102 with the wireless beacons is typically unknown (as described below for step 314 in FIG. 9).  Thus, each of the retail locations that receive the randomly distributed POP displays 102 may be associated with the same selected\ncampaign.\n After POP displays 102, along with wireless beacons 114, are placed at their intended locations and the wireless beacons are activated (e.g., activated at either their final display location or a temporary storage location such as a store back\n(or storage) area), one or more different methods may be used to assess a retail location of each of the POP displays (e.g., the store at which each POP display is located).  For example, as shown in FIG. 4, method 806A, method 806B, method 806C, and\nmethod 806D may each be used, either alone or in combination, to, in 808, assess the retail location of a selected POP display 102 and wireless beacon 114.  Methods 806A, 806B, 806C, 806D may be used to assess the retail location of multiple POP displays\n102.  In some embodiments, one method may be used to assess the retail location of all the POP displays associated with the selected campaign.  In some embodiments, one method may be used to assess the retail location of a first POP display while another\nmethod is used to assess the retail location of a second POP display, a third POP display, a fourth POP display, etc.\n FIG. 5 depicts a flowchart of an embodiment of method 806A used to assess a location of a POP display.  Once POP display 102 is placed at a retail location (e.g., in 804, shown in FIG. 4), method 806A may be used to assess the retail location of\nthe POP display using interaction with customer device 118 and SDK 125 on the customer device.  In 900, customer device 118 may receive a packet (e.g., a data packet such as packet 700, shown in FIG. 2) from wireless beacon 114.  The packet may include\nthe unique identifier for wireless beacon 114.\n In 902, SDK 125 may combine the received unique identifier along with geographic information on the location of customer device 118.  For example, the geographic information may include the geographic location of customer device 118 such as, but\nnot limited to, latitude and longitude location or GPS location of the customer device.  In 904, SDK 125 may then provide the geographic location of customer device 118 along with the unique identifier to a remote server (e.g., server 126, shown in FIGS.\n1 and 2).  In some embodiments, a time stamp from customer device 118 is also provided to the remote server.  In certain embodiments, SDK 125 provides unique identifiers for a plurality of wireless beacons along with the geographic location of customer\ndevice 118.\n In 906, the remote server may then assess or determine the retail location of wireless beacon 114 with the unique identifier using the geographic location information provided along with the unique identifier.  If multiple unique identifiers are\nsent to the remote server, the remote server may determine the retail location of each of the wireless beacons with the unique identifiers associated with the geographic location information.  The determined retail location may be, for example, a retail\nstore number associated with a retail chain associated with the selected campaign.  In some embodiments, the retail location is determined using the geographic location in combination with other information available to the remote server, including, but\nnot limited to, information from other customer devices and/or information about retail locations associated with the selected campaign.  In some embodiments, the remote server assesses the time stamp received from SDK 125 in combination with the\ngeographic location of customer device 118, the unique identifier, and the retail locations associated with the selected campaign.  Assessing the time stamp may allow the remote server to assess if the POP display is active during a selected time period\nassociated with the selected campaign for the POP display.\n Information from other customer devices may include, but not be limited to, geographic location information from interaction of other customer devices with the wireless beacon.  Thus, in some embodiments, the remote server may use information\nfrom multiple customer devices to determine the retail location of a wireless beacon.  The information about retail locations associated with the selected campaign may be provided to the remote server or obtainable by the remote server using information\ninput earlier about the selected campaign.  In some embodiments, the remote server stores the retail location information along with the unique identifier in a database (e.g., information 130 on server 126, shown in FIG. 1).\n The retail location determined in 906 may be provided to method 800, shown in FIG. 4, to either be used as the assessed retail location in 808 or used in other methods (e.g., 806B or 806C) to determine the retail locations of other POP displays. In some embodiments, the retail location determined in 906 may be used to assess if the location of the POP display and the wireless beacon has changed.  For example, the remote server may look up the unique identifier and assess if a previous location\nfor the unique identifier was recorded to assess if any change in location has occurred.\n FIG. 6 depicts a flowchart of an embodiment of method 806B used to assess a location of a POP display.  Method 806B may include assessing the retail location of POP display 102 and wireless beacon 114 using the presence of other detected\nwireless beacons (POP displays) with known retail locations in proximity to the wireless beacon and interaction with one or more customer devices 118.  In some embodiments, multiple wireless beacons are interacting with a single customer device 118\n(e.g., the single customer device receives packets from multiple wireless beacons at or around the same time).  In some embodiments, the wireless beacons are interacting with multiple customer devices 118 at or around the same time with a remote server\nreceiving information from the multiple customer devices (with knowledge of the customer devices being at the same location).\n As shown in FIG. 6, method 806B includes determining, at the remote server, a retail location of a first POP display in 910.  The retail location of the first POP display may be determined, for example, using method 806A, shown in FIG. 5, method\n806C, shown in FIG. 7, or method 806D, shown in FIG. 8.  Determining the retail location of the first POP display in 910, as shown in FIG. 6, allows the remote server to know the retail location of the first POP display.  In some embodiments, the retail\nlocation of the first POP display may be known and provided to the remote server in 910 (e.g., a separate entity or application provides the retail location of the first POP display or the retail location the first POP display is to be sent to is known\nbefore being sent to the location).  In some embodiments, the same retail location is determined (and then known) for multiple POP displays in 910 (e.g., the same retail location may be determined (and then known) for two or more POP displays).  The\nremote server may associate together the multiple POP displays at the same retail location.\n After the location of the first POP display(s) is determined (or known) in 910, customer device 118 may receive a first data packet (or a set of first data packets for multiple POP displays) with the unique identifier for the first POP\ndisplay(s) in 912.  At or around the same time, customer device 118 may receive a second data packet from a second POP display in 914.  The second POP display may have a retail location that is unknown to the remote server.  The second data packet may\ninclude the unique identifier for the second POP display.\n In 916, SDK 125 on customer device 118 may provide the unique identifier for the first POP display(s) and the unique identifier for the second POP display to the remote server (e.g., server 126, shown in FIGS. 1 and 2).  In 918, the remote\nserver may determine, based on the remote server receiving both the unique identifier for the first POP display(s) and the unique identifier for the second POP display at the same time, that the second POP display is at the same retail location as the\nfirst POP display(s).  Put another way, the remote server determines that the second POP display is at the same retail location as the first POP display(s) because the remote server receives both unique identifiers from the same customer device, which is\nat the retail location.  The retail location of the second POP display determined in 918 may be provided to method 800, shown in FIG. 4, to be used as the assessed retail location in 808.\n In some embodiments, the remote server may receive the unique identifier for the first POP display(s) and the unique identifier for the second POP display from different customer devices in 916 (e.g., two or more different mobile devices).  In\nsuch embodiments, however, the remote server may receive other identifying information (e.g., geographic location information or specific content related information) that allows the remote server to associate the unique identifier for the first POP\ndisplay(s) with the unique identifier for the second POP display and determine that the POP displays are at the same retail location in 918.\n FIG. 7 depicts a flowchart of an embodiment of method 806C used to assess a location of a POP display.  Method 806C may include assessing the retail location of POP display 102 and wireless beacon 114 using communication with a network gateway\nlocated at the retail location.  In certain embodiments, as shown in FIG. 2, network gateway 710 is located at retail location 200.  Multiple network gateways 710 may be installed at known retail locations for interfacing with POP displays after the POP\ndisplays are distributed and reach the retail location.  For example, network gateways 710 may be permanent network gateways installed at each retail location in a retail chain or supply chain with the location of each network gateway being known.  Thus,\nfor POP display distribution, the retail locations of multiple network gateways 710 are known by the remote server.\n Network gateway 710 may be a wireless network gateway.  For example, network gateway 710 may be any hardware (e.g., a processor and one or more wireless antenna) capable of networked communication over one or more wireless communication networks\nand/or interfacing between wireless communication networks (e.g., interfacing between a local area network (LAN) and a wide area network (WAN)).  Communication networks may include, but not be limited to, WANs cellular networks, wireless networks, and\nthe Internet.  In certain embodiments, network gateway 710 is connected to the Internet and is capable of interfacing and communicating using Bluetooth LE, WiFi, sub-gigahertz radio, cellular, and other longer-range radio bands.\n In embodiments with network gateway 710 using sub-gigahertz radio, wireless beacons 114 may be capable of broadcasting over sub-gigahertz (or another longer-range radio band) in addition to Bluetooth LE.  Sub-gigahertz broadcasting may provide\nincreased range of data transmission as compared to Bluetooth LE (e.g., sub-gigahertz may have a transmission range of up to about a mile).  Sub-gigahertz broadcasting may include, for example, broadcasting over the ISM band (UHF).  In some embodiments,\nhowever, other longer broadcast range (and detection range) radio bands may be used instead of sub-gigahertz radio bands.  For example, broadcast radio bands such as, but not limited to, WiFi, LORA, or ZigBee may be used in wireless beacons 114 and/or\nnetwork gateway 710.  In certain embodiments, wireless beacons 114 are equipped with a communication chip (e.g., wireless transceiver 120A) capable of both Bluetooth LE and sub-gigahertz broadcasting (or another longer-range radio band).  While Bluetooth\nLE may be used for broadcasting to mobile devices (or other Bluetooth LE capable devices), a larger LAN may be provided between wireless beacons 114 and network gateway 710 by using the longer broadcast range provided by sub-gigahertz radio (or another\nlonger-range radio band).  In some embodiments, network gateway 710 may provide a data collection network (e.g., a LAN for data collection) for collecting data broadcast 712 by wireless beacons 114 (e.g., unique identifiers, sensor data, etc.) and\ntransmitting the data over a communication network 714 (e.g., a WAN connected to the remote server).\n In certain embodiments, as shown in FIG. 7, method 806C includes a network gateway (e.g., network gateway 710) at a known retail location (e.g., retail location 200) receiving a data packet (e.g., data broadcast 712) from wireless beacon 114 on\nPOP display 102 in 920.  The data packet may include the unique identifier of the wireless beacon and the POP display.  The network gateway may provide the unique identifier to the remote server in response to receiving the data packet in 922.  In some\nembodiments, the network gateway provides the unique identifier to the remote server over communication network 714 (e.g., cellular network, WiFi network, or the Internet).  In some embodiments, the network gateway provides the unique identifier to the\nremote server using customer device 118.  For example, the network gateway provides the unique identifier along with identifying/location information about the network gateway to SDK 125 on customer device 118.  SDK 125 may then provide this information\nto the remote server when customer device 118 communicates with the remote server.\n In 924, the remote server may associate the POP display having the unique identifier with the network gateway providing the unique identifier and the retail location of the network gateway.  Using this association, the remote server may\ndetermine the retail location of the POP display with the unique identifier because the retail location of the associated network gateway is known (e.g., the installation location of the network gateway is known as described above).  In some embodiments,\nthe network gateway associates the POP display having the unique identifier with the retail location of the network gateway and provides data about the association to the remote server, which then stores information about the retail location of the POP\ndisplay.  The retail location of the POP display having the unique identifier determined in 924 may be provided to method 800, shown in FIG. 4, to be used as the assessed retail location in 808.\n In some embodiments, method 806C includes assessing a signal strength between the POP display with the unique identifier and the network gateway in 926.  The assessed signal strength may be provided to the remote server along with the unique\nidentifier in 922.  The remote server may use the assessed signal strength to determine a specific (or relatively specific) location of the POP display within the retail location.  For example, the exact location of the network gateway at the retail\nlocation may be known (e.g., in a server room at the retail location).  The assessed signal strength may provide information that is used to estimate the distance between the POP display with the unique identifier and the network gateway.  From the\nestimated distance, the specific location of the POP display within the retail location may be determined (e.g., estimated or approximated).\n FIG. 8 depicts a flowchart of an embodiment of method 806D used to assess a location of a POP display.  Method 806D may be used to assess the retail location of the POP display using interaction with customer device 118 and SDK 125 on the\ncustomer device.  In 930, customer device 118 may receive a packet (e.g., a data packet such as packet 700, shown in FIG. 2) from wireless beacon 114.  The packet may include the unique identifier for wireless beacon 114.  In some embodiments, customer\ndevice 118 may receive multiple packets from multiple wireless beacons, each packet having the unique identifier for the originating wireless beacon.\n In 932, SDK 125 may combine the received unique identifier along with information about the retail location of customer device 118 from 933.  In certain embodiments, the retail location of customer device 118 in 933 is provided by another\napplication (or entity) located on the customer device.  For example, mobile application 127 (located in software package 122 on customer device 118, as shown in FIG. 1) may provide the retail location of the customer device.  The manner in which mobile\napplication 127 determines the retail location of customer device 118 may be unknown to SDK 125.  For example, mobile application 127 may be a retailer \"app\" that determines the retail location (e.g., store number) of customer device 118 through an\nunknown or proprietary algorithm.  Regardless of the manner in which mobile application 127 determines the retail location of customer device 118, SDK 125 may receive the retail location known by the mobile application in 933 and combine this information\nwith the unique identifiers for the wireless beacons.\n In 934, SDK 125 may then provide the retail location of customer device 118 along with the unique identifier to the remote server.  In some embodiments, a time stamp from customer device 118 is also provided to the remote server.  In certain\nembodiments, SDK 125 provides unique identifiers for a plurality of wireless beacons along with the retail location of customer device 118.\n In 936, the remote server may assess or determine the (selected) retail location of wireless beacon 114 with the unique identifier by associating the wireless beacon with the provided retail location of customer device 118.  If multiple unique\nidentifiers are sent to the remote server, the remote server may determine the retail location of each of the wireless beacons with the unique identifiers by associating the retail location of customer device 118 with each wireless beacon.  The retail\nlocation of the POP display determined in 936 (the POP display having the wireless beacon with the unique identifier) may be provided to method 800, shown in FIG. 4, to be used as the assessed retail location in 808.\n Identification of the retail location of wireless beacon 114 by server 126 through the interaction of the wireless beacon with customer device 118 allows the wireless beacon's location to be dynamically cataloged by the server and potentially\nother portions of the overall system, including the wireless beacon itself.  Identification of the retail location by server 126 may be advantageous in that it does not require that the final destination of wireless beacon 114 and circuit board 104 be\nknown at the time of manufacture or distribution since its location may be determined without any external intervention after POP display 102 containing the circuit board has been set up in its intended final location.\n The above disclosed embodiments may be utilized to optimize embodiments of a manufacturing supply chain associated with POP displays 102 and circuit board 104.  As noted above, the location of a given wireless beacon and the other wireless\nbeacons that it may connect to need not be known before the wireless beacon, or circuit board, is installed in its final location.  Current commercially available products, however, treat the installation of wireless beacons as a network infrastructure\nproject and do not contemplate integrating wireless beacons into other systems (e.g., POP displays).  Thus, for a given store, technicians typically install hard points to supply power and network connectivity for each wireless beacon, which severely\nlimits a store manager's flexibility in placing the beacons.  Additionally, once a wireless beacon is deployed in a location, moving the wireless beacon may require bringing in a technician to disconnect and then rewire each wireless beacon in its new\nlocation.  Location changes must also be accompanied by revisions to the database that describes the placement of each wireless beacon.  This mode of deployment is fundamentally incompatible with conventional POP displays (e.g., non-connected POP\ndisplays), which typically can be moved around at will.  If wireless beacons and their attendant power and networking requirements are installed in POP displays, then the POP displays may become inflexible infrastructure points without a system that\nallows for flexible movement and locating of the POP displays as described herein.\n As described herein, the embodiments of POP display system 100 and POP display 102 with wireless beacon 114 may provide increased flexibility for the placement and movement of the POP displays because of the modular, ad hoc nature of the\ndisclosed embodiments.  POP display system 100 may provide a flexible and easy to deploy system that does not require specialized technicians and/or expensive wiring.  In certain embodiments, a store manager is able to simply assemble POP display 102 and\nplace the POP display in the store anywhere desired as if it was a conventional display.\n Additionally, the disclosed modular POP display system 100 and methods associated with the system simplify the overall supply chain for POP displays.  Traditionally, POP displays are manufactured in bulk at a factory and then sent to a\ndistribution center where the displays may remain in storage for months at a time before distribution to a final location.  When incorporating wireless beacons, this creates a logistical problem as one would ideally like to know where a given wireless\nbeacon will end up (e.g., so the display can be programmed based on the final location).  Current manufacturing supply chains, however, are not structured to accommodate that level of granularity.  Typically, large pallets of displays are manufactured,\nstored, and bulk distributed to stores en masse without regard to a specific display's journey.  This method is not a problem when a display simply contains a static display and perhaps a coupon and/or a product holder.  For POP displays that interact\n(e.g., dynamically interact), via wireless beacons, with a customer device so that location, content, and customer specific information may be communicated between the customer device and a remote server, however, such distribution methods are\nproblematic as they require experienced and expensive technicians to install wireless beacon enabled displays at their known final location.  Once installed, a human may manually configure each wireless beacon enabled display with its location and\nidentification information, as well as record such information for use in a database.  The manual installation, however, may be tedious, time consuming, and difficult to implement on a consistent basis.\n FIG. 9 depicts a flowchart of a manufacturing supply chain associated with POP displays 102.  Supply chain 300 utilizes embodiments of POP display system 100, POP display 102, and circuit board 104 disclosed herein that via their ad hoc nature,\nmay be flexible and readily compatible with existing manufacturing practices.  In certain embodiments, large quantities of POP displays 102 may end up at a final display location without advanced planning and interaction between the POP displays with\ncircuit boards 104, customer devices (e.g., customer devices 118), and a server (e.g., server 126) may provide a configuration to POP system 100 as needed.\n In certain embodiments, as shown in FIG. 1, circuit board 104 includes battery 106.  In some embodiments, battery 106 is a non-removable battery or a permanently affixed battery.  Battery 106 being a non-removable battery may provide power\nsavings in the context of the supply chain 300, shown in FIG. 9.  Non-removable batteries may be desirable in supply chain 300 because they are cheaper and are more reliable.  For example, it may be more difficult for a permanently affixed battery to be\njarred out of its connection as compared to a normal (removable) battery mounted in a typical holder.  Additionally, another downside to removable batteries is that they may have to be installed on site by the personnel who set up POP display 102 with\ncircuit board 104 in its final location.  Such personnel may fail to install the batteries correctly or even install the batteries at all, rendering POP display 102 useless for its intended purpose.\n A consideration in using a non-removable (e.g., permanent) battery as battery 106, as shown in FIG. 1, is that, due to the fact that the battery cannot be replaced, it is important to preserve battery life to maximize the service life of circuit\nboard 104.  To increase battery life, in certain embodiments, as shown in FIG. 1, circuit board 104 includes switch 108.  Switch 108 may be a switch such as a power jumper or functional equivalent (e.g., a dip switch).  Switch 108 may provide a removable\nconnection between battery 106 and the rest of the circuitry in circuit board 104 (e.g., wireless beacon 114).  Thus, no power is drained from battery 106 when switch 108 is not installed or turned on.  Not installing switch 108 allows circuit board 104\nto be stored for extended lengths of time (e.g., approximately one year or greater) without reducing any useable power storage in battery 106.\n As shown in FIG. 9, POP display 102 (shown in FIG. 1) is manufactured in factory 302.  After manufacture in factory 302, POP display 102 may be moved from the factory to distribution center 304.  In certain embodiments, POP display 102 moves\nfrom factory 302 to distribution center 304 along with circuit board 104.  Circuit board 104 may have battery 106 disconnected from any power draining circuitry (e.g., controller 112 and/or wireless beacon 114).  For example, switch 108 may disconnect\npower to power draining circuitry.  At distribution center 304, circuit board 104 (with deactivated circuitry) and POP display 102 are received in 306.  At 308, switch 108 is installed or turned on to provide power from battery 106 to the rest of the\ncircuitry in circuit board 104 and wireless beacon 114 is activated.  In certain embodiments, wireless beacon 114 is configured such that, upon its first activation, the wireless beacon scans for specific connections that are associated with its\nconfiguration mode.\n The use of a power switch (e.g., switch 108) in circuit board 104 and supply chain 300 provides several advantages.  One advantage is that switch 108 ensures that battery 106 is disconnected at the time of manufacture of POP display 102 to\nextend the life of the battery.  Another advantage is that the wireless beacon 114 is deactivated until switch 108 is connected.  Deactivating wireless beacon 114 allows distribution center 304 to activate a small number of POP displays at a time in a\nconfiguration procedure discussed below.  If, for example, the thousands, or even hundreds of thousands, of POP displays in a warehouse (e.g., distribution center 304) were active at the same time and their wireless beacons were actively broadcasting\nand/or scanning, the resulting electronic cacophony could render all communication and configuration difficult or even impossible.  Empirical testing has shown that having more than 50 active beacons in close proximity may cause severe wireless\ninterference problems.  Thus, a distribution center full of active beacons could be rendered completely useless.  Furthermore, transportation rules and requirements often restrict the transmission of electromagnetic signals by shipped items.  Thus, the\nability to disable wireless broadcasts by the included beacons both after initial manufacture and during distribution has the additional benefit of facilitating the transport of embodiments of POP displays 102 with wireless beacons 114.\n In some embodiments, it may be desirable to physically activate a battery at the time a circuit board is removed from its shipping container.  In such embodiments, an insulator may be placed between battery terminals to turn the controller off. \nSuch an insulator may be tethered to the external shipping enclosure in such a way that the battery insulator is pulled from between the terminals as the circuit board is removed from the shipping enclosure.  Removing the battery insulator may then\nactivate the controller and the wireless beacon.\n In certain embodiments, after wireless beacon 114 establishes a connection in 308, variations of the wireless beacon's parameters are configured in controller 112 via software in 310.  Controller 112 may be configured, for example, via the BLE\nconnection made between wireless beacon 114 and a programmer device (e.g., a wireless programming device).  In certain embodiments, wireless beacons 114 are associated with a selected campaign in 310 (e.g., as part of step 802, shown in FIG. 4).  During\nconfiguration in 310, time parameters may be set in controller 112 such that wireless beacon 114 is able to determine and act upon a target start date of a (selected) campaign.  For example, controller 112 and clock 113 may be configured with the current\ndate and time as well as the target date and time for the selected campaign to begin.  In certain embodiments, this process is simplified to absolute amount of time between the time of configuration and the beginning of the selected campaign.  Various\nother parameters may be set during the configuration in 310 as well.\n In certain embodiments, during configuration 310, unique label 117, shown in FIG. 1, is scanned or otherwise identified.  In some embodiments, unique label 117 is added to circuit board 104 before arriving at distribution center 304.  In some\nembodiments, unique label 117 is added at distribution center 304.  Scanning unique label 117 may allow the unique label to be associated with wireless beacon 114 and its unique identifier.  For example, unique label 117 may be scanned and the unique\nlabel may be programmed (via the BLE connection) to be associated with the unique identifier for wireless beacon 114 and/or other associated data such as, but not limited to, an activation date or campaign associated with circuit board 104 and the\nwireless beacon.  Associating unique label 117 with wireless beacon 114 (and its associated data) allows information about the wireless beacon to be accessed while the wireless beacon is asleep by scanning the unique label.  Unique label 117 may be\nscanned while wireless beacon 114 is asleep to obtain programmed information (e.g., campaign information or activation time) and place circuit board 104 into a proper POP display (as described below) before the POP display is shipped to a final display\nlocation.\n After configuration is complete in 310, the configuration application disconnects from controller 112 and wireless beacon 114 (and circuit board 104) may enter sleep mode 312.  In sleep mode 312, wireless beacon 114 may shut down its Bluetooth\nradio, and the circuitry in circuit board 104 may enter a low power mode to conserve the batteries before the circuit board is installed onto POP display 102 and also while the POP display is stored in advance of being shipped to a final location (e.g.,\na retail location).  Thus, even though switch 108 has been activated in distribution center 304, the problem of having too many active Bluetooth radios (e.g., wireless beacons) in close proximity in the distribution center is inhibited as each activated\nradio only remains active for a short period of time as configuration of the wireless beacon is conducted.  Also, it is worth noting that the low power mode referred to above includes any control circuitry, such as a microcontroller, not just the\nBluetooth beacon itself.  It will also be apparent to one of ordinary skill in the art that such circuitry may be implemented in various layouts, such as in several discrete chips or one integrated chip.\n After circuit board 104 and wireless beacon 114 enter sleep mode 312, the circuit board may be installed in POP display 102 in 313.  After circuit board 104 is installed, POP display 102 may be moved (e.g., shipped or transported) in 314.  POP\ndisplay 102 may be moved to final display location 316, as shown in FIG. 9.  Final display location 316 may be, for example, a retail or shopping location for POP display 102 to be located in front of customers to provide an interactive customer\nexperience.  At final display location 316, POP display 102 may be assembled in 318 and placed in an operating location in 320.  After being placed at the operating location in 320, when the configuration circuitry (programmed in 310) determines that the\ntarget date and time for the campaign has been reached, the circuitry awakens from its sleep state at the configured time in 322.  In 322, wireless beacon 114 activates its wireless (e.g., Bluetooth) radio and campaign related operations begin in 324.\n As described above, the embodiment of supply chain 300 may provide a solution to the problem of how to build POP displays with wireless beacons in advance while not running down the batteries while the displays are being stored.  Using supply\nchain 300 may also ensure that the associated radios are inactive while the POP display is being stored and transported.  These aspects may be important when considering the overall supply chain.\n In some embodiments, POP display 102 includes a light sensor (e.g., one of sensors 116, shown in FIG. 1, is a light sensor).  In such embodiments, the light sensor may be used to trigger activation of the radio only after the light sensor\ndetects visible light level above a threshold and the campaign date has started.  Using the light sensor to trigger activation may further conserve power by not turning the radio on if POP display 102 has not been unpacked or the store is closed and the\nlights are off.  The radio and some sensors, however, may not need to operate under the same schedule or conditions.  For example, certain sensors may be active even when POP display 102 is in low power mode.  These sensors may be used to allow POP\ndisplay 102 to determine whether the display is being transported or being set up at a final location.  One skilled in the art will recognize that this permits one to select what a skilled artisan deems to be the optimal tradeoff between battery\nconsumption and situational awareness.  Some embodiments may utilize other sensors to determine whether the store is open or whether potential customers are nearby (e.g., proximity sensors may be used to determine if customers are nearby).  Information\nfrom these sensors may also be combined with the campaign start date to determine whether the radio should be activated.  In some embodiments, one of sensors 116 is an accelerometer.  The accelerometer may be active when POP display 102 is in low power\nmode and may trigger activation of the radio when any movement is registered by the accelerometer.  If, however, a light sensor does not detect a visible light level above the threshold and/or the campaign date has not started, the activation of the\nradio may be temporary (e.g., only for a short, selected amount of time).\n In some embodiments, POP display 102 (and POP display system 100) allows for the tracking of the deployment rate of POP displays at a retail location.  FIG. 10 depicts a block diagram of an embodiment of a plurality of POP displays 102 at retail\nlocation 200.  POP displays 102 at retail location 200 are capable of communicating with each other and/or with server 126.  In certain embodiments, POP displays 102 communicate with each other and provide information to a single POP display (e.g., POP\ndisplay 102').  POP display 102' may then communicate with server 126 to provide the information to the server, as shown in FIG. 10.\n In some embodiments, server 126 detects information about the relative locations of wireless beacons 114.  For example, since each wireless beacon 114 is equipped with a unique identifier, server 126, upon detecting and determining the identity\nof one wireless beacon 114 in communication range with another wireless beacon, may determine that the beacons are associated with a specific store.  In some embodiments, depending on the configured broadcast mode of wireless beacons 114, server 126 may\ncommunicate with wireless beacon 114' to relay information to the wireless beacon and other wireless beacons 114 within communication range of wireless beacon 114'.  In this way, a group of beacons may operate as an ad hoc distributed communication\nnetwork, which is advantageous as this does not require that the network be set up and configured beforehand by a technician.\n It is known in the industry that, on average, only a fraction of POP displays delivered to a store are actually deployed.  In certain embodiments, however, wireless beacon 114 is capable of communicating, whether directly or indirectly, with\nserver 126, as shown in FIGS. 1 and 10.  Thus, wireless beacon 114 may relay information gathered by sensors 116 indicating that its POP display has been deployed.  For example, if POP display 102 reaches its campaign start time but its light sensor\nindicates darkness for a prolonged period of time, it may be inferred that the POP display was not deployed on a timely basis.\n In some embodiments, data collected from sensors 116 and/or customer device 118 (and/or other sources) that is stored in information 130 on server 126, shown in FIG. 1, may be used for historical analysis of the performance of POP display 102. \nIn some embodiments, the historical analysis data is correlated with saved data from wireless beacon 114 to further gauge customer engagement.  For example, combining information about dwell time with the fact of whether the potential customer made a\npull request may be used to gauge customer engagement.  In some embodiments, the historical data is correlated with third party data (e.g., retailer data such as purchase history, etc.).  Correlating the historical data with third party data may provide\nfurther information useful to the retailer to enhance a customer's experience.\n In some embodiments, various statistical analyses are utilized on historical data collected from POP display 102.  Statistical analyses that may be used include, but are not limited to, machine learning and data mining techniques, set theory,\nmultivariate statistics, and time series analyses.  Examples of machine learning include deep learning, neural networks, support vector machines, random forests, decision tree learning, association rule learning, inductive logic, clustering, Bayesian\nnetworks, reinforcement learning, representation learning, similarity and metric learning, sparse dictionary learning, and genetic algorithms.  Examples of data mining, which is often coextensive with machine learning, include spatial data mining,\ntemporal data mining, sensor data mining, pattern mining, and subject-based data mining.  In some embodiments, these techniques are used for aspects besides historical analysis.  For example, smoothing techniques associated with some types of temporal\ndata mining may be used to filter a series of RSSI signal strength values used in bump detection disclosed herein.\n Examples of different types of data that may be collected an analyzed for POP displays 102 are illustrated in FIGS. 11A-11G.  FIG. 11A illustrates an example of a campaign calendar.  FIG. 11B illustrates an example of an interface displaying\nvarious statistics related to POP display deployment and sales.  FIG. 11C illustrates an information screen related to the product associated with the POP display.  FIG. 11D illustrates an interface displaying national deployment information for POP\ndisplays.  FIG. 11E illustrates an interface displaying state deployment information for POP displays.  FIG. 11F illustrates an interface displaying supply chain information.  FIG. 11G illustrates an interface displaying a sales analysis associated with\nthe POP display.\n In some embodiments, POP display 102 includes components or devices that reduce the surrounding infrastructure requirements for supporting the POP display.  Specifically, POP display 102 may be equipped with wireless transmission functionality\nto transmit any recorded measurements or information derived therein as described above.  This information may be transmitted to a network gateway (e.g., network gateway 710, shown in FIG. 2) located within or near the store (e.g., retail location 200,\nshown in FIG. 2).  In certain embodiments, instead of a network gateway, an employee of the retailer or a POP display service provider may be equipped with a mobile device that contains an application adapted to connect to POP display 102 and retrieve\ndata from the point of purchase display to be relayed to a server.  This connection may be accomplished utilizing the same wireless connection that is used for beacon functionality, or may be conducted by other standard wireless transmission protocols as\ndescribed herein, e.g., IEEE 802.11.  This minimizes the need for wireless transmission infrastructure to support the retrieval of information collected by the point of purchase displays.\n In some embodiments, POP display system 100 utilizes the application already installed on a potential customer's mobile device (e.g., software package 122 on customer device 118) to relay the collected information to a server (e.g., server 126). As shown in FIG. 1, POP display 102 may connect with customer device 118 via the same wireless connection that enables beacon functionality (e.g., Bluetooth connection between wireless beacon 114 and wireless transceiver 120A) and deliver the data to the\ncustomer device.  Customer device 118 may then transmit the data to server 126 via wireless transceiver 120B or another wireless transceiver.  Since transmission through customer device 118 may incur a data charge on the customer, the application may be\nconfigured to prompt the user for permission to do so.  In some embodiments, the application may offer the user some form of compensation for the use of their data connection.  For example, a discount on some good or service in the store.  In this way,\ndata can be moved to server 126 by users who are normally just walking around the store and requires no special infrastructure.  Additionally, the data to be transmitted may be compressed using standard techniques to minimize the amount of bandwidth\nconsumed, and the application (e.g., the SDK) may be configured to, under certain cases, delay the transfer of data over the mobile device's wireless network connection to the server if the network connection is not over WiFi and/or the mobile device is\nnot connected to an external power source.  In other words, the mobile device may be directed to save the data and wait until the customer is connected to a wireless network where transmission of the data will not adversely impact the customer's wireless\ndata plan or the mobile device's battery life.\n In certain embodiments, one or more process steps described herein may be performed by one or more processors (e.g., a computer processor) executing instructions stored on a non-transitory computer-readable medium.  For example, communication\nbetween POP display 102, customer device 118, server 126, and/or network gateway 710, shown in FIG. 1, may have one or more steps performed by one or more processors executing instructions stored as program instructions in a computer readable storage\nmedium (e.g., a non-transitory computer readable storage medium).  In certain embodiments, controller 112, on POP display 102, software package 125, on customer device 118, server 126, and/or network gateway 710 include program instructions in the\ncomputer readable storage medium.\n FIG. 12 depicts a block diagram of one embodiment of exemplary computer system 410.  Exemplary computer system 410 may be used to implement one or more embodiments described herein.  In some embodiments, computer system 410 is operable by a user\nto implement one or more embodiments described herein such as communication between POP display 102, customer device 118, server 126, and/or network gateway 710, shown in FIG. 1.  In the embodiment of FIG. 12, computer system 410 includes processor 412,\nmemory 414, and various peripheral devices 416.  Processor 412 is coupled to memory 414 and peripheral devices 416.  Processor 412 is configured to execute instructions, including the instructions for communication between POP display 102, customer\ndevice 118, server 126, and/or network gateway 710, which may be in software.  In various embodiments, processor 412 may implement any desired instruction set (e.g. Intel Architecture-32 (IA-32, also known as x86), IA-32 with 64 bit extensions, x86-64,\nPowerPC, Sparc, MIPS, ARM, IA-64, etc.).  In some embodiments, computer system 410 may include more than one processor.  Moreover, processor 412 may include one or more processors or one or more processor cores.\n Processor 412 may be coupled to memory 414 and peripheral devices 416 in any desired fashion.  For example, in some embodiments, processor 412 may be coupled to memory 414 and/or peripheral devices 416 via various interconnect.  Alternatively or\nin addition, one or more bridge chips may be used to coupled processor 412, memory 414, and peripheral devices 416.\n Memory 414 may comprise any type of memory system.  For example, memory 414 may comprise DRAM, and more particularly double data rate (DDR) SDRAM, RDRAM, etc. A memory controller may be included to interface to memory 414, and/or processor 412\nmay include a memory controller.  Memory 414 may store the instructions to be executed by processor 412 during use, data to be operated upon by the processor during use, etc.\n Peripheral devices 416 may represent any sort of hardware devices that may be included in computer system 410 or coupled thereto (e.g., storage devices, optionally including computer accessible storage medium 500, shown in FIG. 13, other\ninput/output (I/O) devices such as video hardware, audio hardware, user interface devices, networking hardware, etc.).\n Turning now to FIG. 13, a block diagram of one embodiment of computer accessible storage medium 500 including one or more data structures representative of POP display 102 (depicted in FIG. 1) and/or memory cache 124 (depicted in FIG. 1)\nincluded in an integrated circuit design and one or more code sequences representative of communication between POP display 102, customer device 118, server 126, and/or network gateway 710 (shown in FIGS. 1 and 2).  Each code sequence may include one or\nmore instructions, which when executed by a processor in a computer, implement the operations described for the corresponding code sequence.  Generally speaking, a computer accessible storage medium may include any storage media accessible by a computer\nduring use to provide instructions and/or data to the computer.  For example, a computer accessible storage medium may include non-transitory storage media such as magnetic or optical media, e.g., disk (fixed or removable), tape, CD-ROM, DVD-ROM, CD-R,\nCD-RW, DVD-R, DVD-RW, or Blu-Ray.  Storage media may further include volatile or non-volatile memory media such as RAM (e.g. synchronous dynamic RAM (SDRAM), Rambus DRAM (RDRAM), static RAM (SRAM), etc.), ROM, or Flash memory.  The storage media may be\nphysically included within the computer to which the storage media provides instructions/data.  Alternatively, the storage media may be connected to the computer.  For example, the storage media may be connected to the computer over a network or wireless\nlink, such as network attached storage.  The storage media may be connected through a peripheral interface such as the Universal Serial Bus (USB).  Generally, computer accessible storage medium 500 may store data in a non-transitory manner, where\nnon-transitory in this context may refer to not transmitting the instructions/data on a signal.  For example, non-transitory storage may be volatile (and may lose the stored instructions/data in response to a power down) or non-volatile.\n Embodiments of the present disclosure may be realized in any of various forms.  For example some embodiments may be realized as a computer-implemented method, a computer-readable memory medium, or a computer system.  Other embodiments may be\nrealized using one or more custom-designed hardware devices such as ASICs.  Other embodiments may be realized using one or more programmable hardware elements such as FPGAs (field programmable gate arrays).\n In some embodiments, a non-transitory computer-readable memory medium may be configured so that it stores program instructions and/or data, where the program instructions, if executed by a computer system, cause the computer system to perform a\nmethod, e.g., any of a method embodiments described herein, or, any combination of the method embodiments described herein, or, any subset of any of the method embodiments described herein, or, any combination of such subsets.\n In some embodiments, a wireless device (or wireless station) may be configured to include a processor (or a set of processors) and a memory medium, where the memory medium stores program instructions, where the processor is configured to read\nand execute the program instructions from the memory medium, where the program instructions are executable to cause the wireless device to implement any of the various method embodiments described herein (or, any combination of the method embodiments\ndescribed herein, or, any subset of any of the method embodiments described herein, or, any combination of such subsets).  The device may be realized in any of various forms.\n Although specific embodiments have been described above, these embodiments are not intended to limit the scope of the present disclosure, even where only a single embodiment is described with respect to a particular feature.  Examples of\nfeatures provided in the disclosure are intended to be illustrative rather than restrictive unless stated otherwise.  The above description is intended to cover such alternatives, modifications, and equivalents as would be apparent to a person skilled in\nthe art having the benefit of this disclosure.\n The scope of the present disclosure includes any feature or combination of features disclosed herein (either explicitly or implicitly), or any generalization thereof, whether or not it mitigates any or all of the problems addressed herein. \nAccordingly, new claims may be formulated during prosecution of this application (or an application claiming priority thereto) to any such combination of features.  In particular, with reference to the appended claims, features from dependent claims may\nbe combined with those of the independent claims and features from respective independent claims may be combined in any appropriate manner and not merely in the specific combinations enumerated in the appended claims.\n Further modifications and alternative embodiments of various aspects of the embodiments described in this disclosure will be apparent to those skilled in the art in view of this description.  Accordingly, this description is to be construed as\nillustrative only and is for the purpose of teaching those skilled in the art the general manner of carrying out the embodiments.  It is to be understood that the forms of the embodiments shown and described herein are to be taken as the presently\npreferred embodiments.  Elements and materials may be substituted for those illustrated and described herein, parts and processes may be reversed, and certain features of the embodiments may be utilized independently, all as would be apparent to one\nskilled in the art after having the benefit of this description.  Changes may be made in the elements described herein without departing from the spirit and scope of the following claims.", "application_number": "15153200", "abstract": " Systems and methods for using wireless beacons in point of purchase\n     (\"POP\") displays to facilitate the delivery of consumer oriented content\n     to mobile devices is disclosed herein. Wireless beacons may be used to\n     broadcast wireless signals from POP displays, where the wireless signals\n     include data packets with unique identifiers for the wireless beacons. A\n     wireless signal from a POP display may be received by a mobile device.\n     The mobile device may provide the unique identifier in the wireless\n     signal and a geographic location of the mobile device to a remote server.\n     The remote server may assess the location of the POP display based on the\n     unique identifier for the wireless beacon and the provided geographic\n     location of the mobile device.\n", "citations": ["6539280", "6571279", "6837427", "6951305", "7021535", "7233241", "7310070", "7374096", "7415426", "7423516", "7510123", "7535337", "7549579", "7614556", "7870019", "8010067", "8070065", "8082177", "8408457", "8531273", "8598988", "8700453", "8798541", "8823521", "9107152", "9202245", "9298677", "9363784", "9426627", "9544744", "9629113", "9646328", "9898749", "9928536", "9953493", "20020176388", "20020183004", "20030167347", "20060080460", "20060087474", "20060109125", "20060114832", "20070067203", "20070114291", "20070254670", "20080021766", "20080045172", "20080243626", "20080256510", "20080284566", "20090030787", "20090288132", "20100121567", "20100131352", "20100201891", "20100235373", "20110178862", "20110178863", "20120142271", "20120171958", "20120191530", "20120228240", "20120239504", "20120306617", "20120310570", "20120315839", "20130013407", "20130073431", "20130210461", "20130217332", "20130268316", "20140206346", "20140244341", "20140249918", "20140249928", "20140254466", "20140269508", "20140282620", "20140316896", "20140324615", "20140324638", "20140337151", "20140358666", "20140359565", "20150025936", "20150026020", "20150079942", "20150081474", "20150082382", "20150100403", "20150140982", "20150142387", "20150142552", "20150161665", "20150215781", "20150237463", "20150248663", "20150249058", "20150278867", "20150278888", "20150281877", "20150287045", "20150317661", "20150371321", "20160034954", "20160042251", "20160050645", "20160092943", "20160092966", "20160094940", "20160095063", "20160110622", "20160134930", "20160148270", "20160171486", "20160171512", "20160178379", "20160217519", "20160225029", "20160227359", "20160227368", "20160232560", "20170124603", "20170169444", "20180027386", "20180047059", "20180108043", "20180158382", "20180165711", "20180189819"], "related": ["62160949", "62256248", "62291828"]}, {"id": "20170200063", "patent_code": "10373019", "patent_name": "Low- and high-fidelity classifiers applied to road-scene images", "year": "2019", "inventor_and_country_data": " Inventors: \nNariyambut Murali; Vidya (Sunnyvale, CA), Schrier; Madeline Jane (Palo Alto, CA)  ", "description": "FIELD OF THE INVENTION\n This invention relates to image processing systems and, more particularly, to object classification, detection, and/or locating systems for images of road scenes.\nBACKGROUND OF THE INVENTION\n The promises and potential of assisted driving and autonomous driving technologies rely on an ability to quickly and accurately classify, detect, and/or locate oncoming and/or surrounding objects.  Various technologies that have been brought to\nbear to provide quick and accurate classification, detection, and/or location information include Radio Detection And Ranging (RADAR) subsystems and Light Detection And Ranging (LIDAR) subsystems, Sound Navigation and Ranging (SONAR) subsystems, and\nimage analysis techniques.  With respect to image analysis techniques, a camera mounted on a vehicle may capture images of oncoming and/or surrounding road and/or environment scenes for digital processing.\n Over the years, improvements in the techniques applied for image analysis, such as the use of Convolutional Neural Networks (CNNs), have made impressive gains in the accuracy of image analysis.  However, techniques, such as the use of CNNs, can\nbe computationally intense both to develop and to deploy, raising problems for the requirements to quickly and accurately provide classification, detection, and/or location information and/or to flexibly and responsively improve analysis techniques. \nInnovations able to maintain gains in accuracy while successfully managing computation needs could be capitalized upon to improve analyses development.  Additionally, such innovations could better bring gains in accuracy to assisted and autonomous\ndriving for purposes of providing detection, classification, and/or location information and/or providing redundancy to and/or filling in gaps in similar information provided by other technologies. BRIEF DESCRIPTION OF THE DRAWINGS\n In order that the advantages of the disclosures will be readily understood, a more particular description will be rendered by reference to specific embodiments illustrated in the appended drawings.  Understanding that these drawings depict only\ntypical examples and are not, therefore, to be considered limiting in scope, the invention will be described and explained with additional specificity and detail through use of the accompanying drawings, in which:\n FIG. 1 is a depiction of an automobile equipped with various technologies for capturing information used in assisted and/or autonomous driving technologies, including a camera for capturing images of oncoming road scenes, in accordance with\nexamples;\n FIG. 2 is a schematic block diagram depicting an example of forward-feed in a Convolution Neural Network (CNN), in accordance with examples;\n FIG. 3A is a schematic block diagram depicting a lengthy, exhaustive, sliding-window approach to applying a CNN to an image, together with a segmenting-grid approach used to reduce implementation time, in accordance with the prior art;\n FIG. 3B is a schematic block diagram depicting a selective approach using context information in applying a CNN that skips over portions of the image to which the CNN is applied to reduce computations and time, in accordance with the prior art;\n FIG. 4 is a schematic block diagram depicting the creation of a low-fidelity dataset of cropped, labeled, images of classified objects with which to train a low-fidelity classifier, implementing a CNN, as a first stage in a multi-stage image\nanalysis system, in accordance with examples;\n FIG. 5 is a schematic block diagram depicting the training of the low-fidelity classifier with the low-fidelity dataset of images of differing scales and resolutions, in accordance with examples;\n FIG. 6 is a schematic block diagram depicting the training of a high-fidelity classifier serving as the second stage in the multi-stage image-analysis system, the high-fidelity classifier being trained with a high-fidelity dataset of cropped,\nlabeled, high-fidelity images of classified objects, in accordance with examples;\n FIG. 7 is a schematic block diagram depicting a down-sample module down-sampling test images of a road-scene for analysis by the image analysis system, in accordance with examples;\n FIG. 8 is a schematic block diagram of the application of sliding windows of various scales to a down-sampled portion of a road scene to abstract segments of the portion of the image for application to a low-fidelity classifier, in accordance\nwith examples;\n FIG. 9 is a schematic block diagram depicting a multi-stage image analysis system in which a low-fidelity classifier implementing a first CNN identifies candidate sections of a down-sampled image of a road scene as potentially depicting\nclassified objects for confirmation by a high-fidelity classifier implementing a second CNN applied to high-fidelity versions of the candidate sections, resulting in the classification, detection, and locating of the depiction of the classified object in\nthe road-scene image, in accordance with examples;\n FIG. 10 is a schematic block diagram depicting a map module operable to map a section from a down-sampled image of a road-scene to a corresponding sector in a high-fidelity version of the image; in accordance with examples; and\n FIG. 11 is a flow chart of steps for quick, accurate, and comprehensive classification, detection, and locating of objects in images by a multi-stage image analysis system built on a low-fidelity classifier and a high-fidelity classifier,\nimplementing a pair of CNNs, in accordance with examples.\nDETAILED DESCRIPTION\n It will be readily understood that the components of the present invention, as generally described and illustrated in the figures herein, can be arranged and designed in a wide variety of different configurations.  Thus, the following more\ndetailed description, as represented in the figures, is not intended to be limiting in scope, as claimed, but is merely representative of certain examples.  The presently described examples will be best understood by reference to the drawings, wherein\nlike parts are designated by like numerals throughout.  In some cases, particular instances of an element in a figure may be identified with an identification number followed by a letter, where the letter may change from figure to figure for the same\nidentification number, indicating differing instances of the element with the same or varying attributes.  References to such elements by number only in the specification may refer more generally to a class of such elements and/or a representative\ninstance of the class.\n Referring to FIG. 1, an automobile 10 is depicted.  However, the disclosures herein may also be applied to other vehicles 10.  The vehicle 10 may be equipped with various types of instruments capable of sensing and recording signals reflecting\nvarious objects, signs, signals and/or the like that may be relevant to driving the vehicle 10.  Non-limiting examples of such equipment may include a RAdio Detection And Ranging (RADAR) subsystem 12 and a LIght Detection And Ranging (LIDAR) subsystem\n14.  Either the RADAR subsystem 12, the LIDAR subsystem 14, or both, potentially with additional subsystems, may be part of a broader imaging subsystem.  As can be appreciated, the vehicle 10 may be outfitted with other such equipment, such as a SOund\nNAvigation and Ranging (SONAR) subsystem.  Additionally, the vehicle 10 is equipped with a camera 16.\n Although a single camera 16 is depicted near the leading edge of the roof of the vehicle 10, any number of cameras 16 may be mounted at any of a number of locations--for example and without limitation, on the dashboard of the vehicle's\ninterior--to capture images of oncoming road-scenes and/or the surrounding environment.  Such a camera 16 may be a digital camera with a digital image sensor, such as, without limitation, a Charge-Coupled Device (CCD) and/or Complementary\nMetal-Oxide-Semiconductor (CMOS).  Furthermore, the camera 16 may be a video camera capable of capturing images at a frame rate.\n In some examples, the frame rate may exceed fourteen frames per second, potentially by many frames per second, to provide images of sufficient recency to allow for responses to information in the images characteristic of assisted and/or\nautonomous driving technologies.  Additionally, the camera 16 may be responsive to a variety of wavelengths both inside and outside of the visible range and/or may capture color information together with the images it captures.  As depicted in the\nexploded view of the interior of the mounted camera device 16, the camera may be provided with an optical system 18, such as, without limitation, a monocular system of lenses controlling focus, field of view, magnification, and/or similar parameters.\n Images captured by such cameras 16 may be analyzed to render information, such as information classifying, detecting, and/or locating objects depicted in such images that are relevant to assisted and autonomous driving systems.  In some\nexamples, this information may be combined with information from other subsystems, such as, without limitation, a RADAR subsystem 12 and/or a LIDAR subsystem 14, to provided redundancy, fill in gaps, and/or improve statistical accuracy.  Additionally, or\nin the alternative, such information from a camera 16 may be used on its own for purposes of informing assisted and/or automated driving systems.  However, before such information is applied in driving systems, it may be extracted from the images by\nprocessing.\n Referring to FIG. 2, the use of a Convolution Neural Network (CNN) 20 to process images 22a from the camera 16 is depicted.  Although several different approaches, from rule-based approaches to different types of machine-learning approaches,\nhave been applied to image analysis, the most accurate to immerge in recent years have involved the use of CCNs 20.  An example CNN 20 is depicted in FIG. 2 at various stages of processing a digital, or digitized, image 22a from a camera 16.\n The CNN 20 may be subdivided between a convolution portion 24, with one or more convolution layers 26a-n, and a neural-network portion 28.  A convolution layer 26n in the convolution portion 24 may include one or more sublayers.  Examples of\nsuch sublayers may include, without limitation, a convolution sublayer 30n, a subsample sublayer 32n, and/or a non-linear sublayer 34n, not necessarily in the proceeding order.  Normalization layers provide an additional non-limiting example of such\nsublayers.  Each of a convolution sublayer 30n, a subsampling sublayer 32n, and a non-linear sublayer 34n, may, respectively include one, but typically more than one, convolution neuron/node 36a-n, non-linear neuron/node 38a-n, and/or subsampling\nneuron/node 40a-n.\n By way of explaining a forward-feed in the convolution portion 24, an exemplary image 22a is depicted.  The image 22a processed by the CNN 20 provides a low-resolution depiction of a stop sign, an important object to be classified, detected,\nand/or located for purposes of assisted and/or autonomous driving.  The image 22a may be provided to an input layer 42a of the CNN 20.\n For ease of explanation, a simple gray-scale image 22a is depicted with black and white cross hatching.  In additional examples, an image 22 may have multiple channels, as would be the case with colored images.  For example, and without\nlimitation, the image 22 may be a Red-Green-Blue (RGB) image 22, or an image 22 encoded for a YUV color space.  In such examples, an individual image 22 may be considered as a three dimensional matrix, with the first two dimensions assigned to the rows\nand columns of pixels and a third dimension assigned to the color channels.  As can be appreciated, a third dimension could also be used for other types of information, such as stereoscopic information where multiple cameras 16 are used to capture a road\nscene from multiple angles.\n With respect to the first convolution layer 26a, at the first convolution sublayer 30a, a different, individual, first-layer filter, or kernel, 46a-n for each of convolution neurons/nodes 36a-n in the first sublayer 30a, with dimensions for its\ntwo-dimensional weights/values smaller than those of the dimensions of the image 22a, may be convolved with the image 22a-n. Example numbers of neurons/nodes 36a-n may include, without limitation, values from four to forty.  During the training process\nfor the CNN 20, each filter 46a-m may come to approximate one or more features of objects in the set of classified objects for which the CNN 20 is being trained.  Since convolution can be used to determine the similarity between functions, by convolving\nthe image 22a with a first-layer filter 46, a feature map may be produced indicating the various degrees to which features represented in the first-layer filter 46 are present in different regions of the image 22a.\n Although different forms of convolution may be applied at convolution neurons/nodes 36a-n, in some examples consistent with the gray-scale image 22a depicted in FIG. 2, each weight/value in a first-layer filter 46a-n may be multiplied by the\nunderlying values used to render the image 22a on which it is overlaid and the products may be summed in a single cell in a resultant feature map.  Hence, each pixel in the resultant feature map may be viewed as the dot product of a portion of the image\n22 of dimensions equal to those of the first-layer filter 46a-n applied thereto.\n After the dot product of the first-layer filter 46a-n and the first portion of the image 22a is calculated, convolution may proceed by sliding, either horizontally, vertically, or both by one or more pixels, the first-layer filter 46 relative to\nthe underlying image 22a and computing another dot product with respect to the new corresponding portion of the image 22a, which will overlap the first portion.  A corresponding convolution neuron/node 36 may continue to calculate dot products, ordering\nthem in the rows and columns of the resultant feature map until the first-layer filter 46 has been applied across the entire underlying image 22a, or matrix 22a where the image 22 includes multiple information channels.  Consequently, without padding,\nand if the first-layer filter 46a-n is slid only one pixel for each dot product, the resultant matrix will have its first two dimensions equal to those of the underlying image/matrix less the dimensions of the first-layer filter 46 plus one in each\ndimension.\n Consequently, the small first-layer filter 46, relative to the dimensions of the image 22a, can capitalize on the relatively high local signal correlation in images 22, without being burdened with the low correlation over large distances. \nAdditionally, since the small first-layer filter 46 is slid over the image 22a, the ability to recognize features is position invariant.  By providing multiple first-layer filters 46a-n, feature maps may be produced for several different features.  In\nexamples in which the image 22 has multiple image channels, resulting in a three, or more, dimensional matrix, first-layer filters 46a-n, as well as subsequent layer filters 46na-nn may also be provided with a third dimension, or more.  Such\nthree-dimensional filters 46 may provide an individual two-dimension filter for each image channel of the image 22.\n In some examples, a non-linear sublayer 32a may be included to apply a non-linear function to values in the matrix produced by the convolution sublayer 30a.  The resultant non-linearities allow additional convolution layers 26b-n to make\ncontributions, which could not be made if the feed-forward process were purely linear.  Non-limiting examples of such a function that may be applied include a sigmoid function and/or a hyperbolic tangent function.\n Subsampling neurons/nodes 40a-n in the first subsampling sublayer 34a may also be applied.  In some examples, such neurons/nodes 40a-n may be operable to implement max-pooling, whereby a set of cells from a resultant matrix is replaced by a\nsingle cell having a value equal to the maximum value for cells in the set of cells.  However, other forms of pooling, such as, without limitation, average pooling may be implemented.  The number of cells pooled into a single cell may include, without\nlimitation, four to one, nine to one, sixteen to one, and so on.  Not only may a subsampling sublayer 34 be used to reduce storage, but it may also provide local invariance and resilience to perturbation.\n A set of convolved, non-linearly mapped, and sub-sampled feature maps 48a-n downstream from the first convolution layer 26a, with smaller dimensions than those of the original image 22a are depicted in FIG. 2.  At this point, the CNN 20 may\nprovide the advantages of deep learning by including multiple convolution layers 26a-n. These convolution layers 26a-n may be arranged hierarchically such that successive filter 46 at convolution sublayers 30 may be determined by the presence of higher\nand higher level, or more abstract, features.\n With respect to the activation of neurons/nodes between convolution layers 26, feature maps 48 resulting from a previous convolution layer 26 may activate convolution neurons/nodes 36 in a subsequent convolution layer 26.  In some examples, each\nfeature map 48 from a previous convolution layer 26 may activate an individual convolution neuron/node 36 at a subsequent convolution layer 26.  Additionally, or in the alternative, after the first convolution layer 26a, resultant feature maps 48 may be\ncombined in a subsequent convolution layer 26.\n Such combinations may be performed at single convolution neurons/nodes 30, wherein the corresponding filter 46, which may have at least three dimensions, one of which describing a number of feature maps 48 being combined.  Such filters 46 may be\nthought of as having a volume.  Dimensions beyond three, may be used, for example and without limitation, for attributes such as image channels.  Resulting feature maps 48 combining previous feature maps, may depict the degree to which features from\nvarious previous filters 46 may be conjoined within different regions of the image 22.  Where resultant feature maps 48 from a previous convolution layer 26 are combined, all of the feature maps 48 from a previous layer 26 may be combined in a subsequent\nfeature map 48 and/or one or more subsets of the previous feature maps 48 may be combined in one or more subsequent feature maps 48.  In many examples, it may be helpful to expand the number of convolution neurons/nodes 36 in a subsequent convolution\nlayer 26.  However, for some subsequent convolution layers 26 it may also be helpful to reduce the number.\n The number of convolution layers 26 may vary, with non-limiting example numbers including numbers two through twenty, potentially making the convolution portion 24 a deep network.  As processing progresses through convolution layers 26, the\nresulting feature maps 48 may become smaller and smaller, approaching the size of the filters 46 with which they are convolved.  Eventually, in some examples, either the height, width, or both dimensions of an iteration of feature maps 48 may equal those\nof the filters 46 to which they are applied, resulting in scalars and/or vectors.  Also, in some examples, a subsampling sublayer 32 may result in scalars or vectors.  Such scalars and/or vectors may be the output of the convolution portion 24 and the\ninput to the classifier/neural-network portion 28\n As with the convolution portion 24, the number of layers 54a-n, which may be referred to as hidden layers 54a-n, may vary, with non-limiting example numbers including numbers two through twelve.  Also, as with the non-linear sublayers 34 in the\nconvolution portion 24, the hidden layers 54a-n in the neural-network portion 28 may apply a non-linear function.  Within the neural-network portion 28, two adjacent sublayers 54 may be fully connected, such that the output of each neural/node 52 in the\nfirst sublayer 54a may be duly weighted and communicated to activate each neuron/node 52 in the second sublayer 54b.\n In such examples, the weight/value applied to the input of each neuron/node 52 in the second sublayer 54b may depend on the originating neuron/node 52 in the first sublayer 54a, the neuron/node 52 in the second sublayer 54b being activated,\nand/or both.  Additionally, a bias weight/value 58 may be applied to one or more sublayers 54.  In some examples, one or more bias values 58 may also be applied in one or more convolution layers 26.  Consequently, a progression from one layer 54 to\nanother layer 54 in the classifier portion 28, may result in a non-linear, weighted sum, to which a bias value 58 may be added, at each neuron/node 52 in the subsequent layer 54.  In addition, or in alternative to fully connected layers 54, layers 54 may\nalso be connected in other ways, such as, without limitation, with a Gaussian style connection.\n The result of processing by the neural-network portion 28 may be recorded in an output layer 58.  The output layer 58, may provide a number of output nodes 60a-n, where each output node 60 provides a probability value indicating a probability\nthat the image 22a depicts an object, sign, signal, or the like classifiable within a corresponding category/class from a set of classes/categories for which the CNN 20 is trained, or for which the CNN 20 is being trained.  For example, the output\nneuron/node 60a assigned to the class/category for stop signs displays a check symbol, symbolic of a probability value indicative of the image 22a depicting a stop sign, whereas, the `x` in the other output neuron/node 60n indicates that an object\npertaining to that output neuron/node 60n is probably not depicted.\n As can be appreciated, the number of calculations involved in the forward feed on a CNN 20 is quite large.  Fortunately, most of the calculations are simple multiplication and summation operations performed on floating point values. \nAdditionally, many of these operations can be performed in parallel.  Consequently, in some examples, a CNN 20, with its large number of floating point operations, may be favorably implemented on one or more Graphic Processing Units (GPUs) 62, which may\nhave one or more cores, to take advantage of the parallel processing and high FLoating-point Operations Per Second (FLOPS) capabilities of such processors.  However, the computational intensity of a CNN 20 may increase greatly when, in addition to\nproviding classification functionality, a CNN 20 is used to provide detection and/or locating functionalities, as discussed in greater detail with respect to the following figure.\n Referring to FIG. 3A, a portion 66 of an image of a road scene is depicted.  However, unlike the image 22a being processed in FIG. 2, in which the classifiable object, i.e., the stop sign, dominates the image, classifiable objects of interest in\nthe portion of the image 66 include traffic lights 68a-b and the pedestrian 70a, which occupy a much smaller sectors of the image portion 66.  Consequently, the signal from such classifiable objects may be overcome by other elements in the image, when\nprocessed as a whole, and/or not otherwise detected.\n Furthermore, for the purposes of an assisted and/or automated driving system, not only the detection and/or classification of an object are important, but also the location of the object within an image.  For example, whether a pedestrian 70 is\nwithin and/or approaching an oncoming portion of the pathway of the vehicle 10 for which the image is captured, or remains safely on the periphery, may be of paramount importance.  However, with respect to a single input image, the architecture of the\nCNN 20 has a track record of accuracy for detection and classification of objects dominating an image, but the design of the CNN 20 is ill suited to locate objects.  The very positional invariance of the filters 46 tiled in an overlapping manner across\nan impute image, which are so helpful to classification, obscure an objects relative location in an image.\n A first approach, denoted by the circled number 1, that has been applied to overcome such limitations in locating objects is to apply a sliding window 72a over an image, as depicted with respect to the portion 66 of the image depicted in FIG.\n3A.  Such a window 72 extracts and/or abstracts the sector 74 of the image over which the sliding window 72 is overlaid.  The extracted and/or abstracted sector 74 may then be applied to a CNN 20 for purposes of object detection and/or classification,\nalong the lines of the forward feed described above with respect to FIG. 2.\n The sliding window 72a then slides horizontally or vertically, one pixel at a time and then extracts and/or abstracts another sector 74 of the image for processing, such that the subsequent sector 74 overlaps the previous sector 74, as\ndemonstrated by the residual outlines of the sectors 74 on the image.  The sliding window 72a continues this process until it has exhaustively covered the image.  In this way, regardless of the where in the image an object may be located, it may be\ndetected and classified.  Furthermore, by sliding pixel by pixel, as opposed to placing the window 72a at the opposite side of its previous boundary, the depiction of an object sliced by such a boundary will not be missed.  Also, by keeping track of the\ncoordinates of the abstracted sectors 74 applied to a CNN 20, the relative location in an image of an object detected and classified can be obtained from the coordinates of the sector 74 in which the object is detected.\n As can be appreciated, applying each overlapping sector 74 individually to a CNN 20 can increase the already computationally intense operations of processing an image with a CNN 20 by orders of magnitude.  The large computational increases can\nfurther be highlighted upon consideration that the portion 66 of the image depicted in FIG. 3A likely does not make up the entire image of the road scene.  Furthermore, since the scale at which classifiable objects may be detected will diminish or\nincrease based on the distance of the object relative to the camera 16, according to the laws of perspective, it is often necessary to apply multiple windows 72a-n of different sizes, each of which extracting its own set of sections 74 to be processed\nindividually.\n For, example, while the first window 72a depicted in FIG. 3A may be large enough to detect and classify the pedestrian 70a, it may be too large to detect and classify the traffic lights 68a,b.  A second window 72b may be applied to detect the\ntraffic lights 68a,b, but would be too small to detect the pedestrian 70a.  Hence, multiple windows 72a-n may need to be applied.\n Therefore, this first approach, albeit thorough, is not only computationally intense, but very time consuming, as indicated by the clock icon.  Conversely, the human eye and brain can process the gist of a road scene in under a tenth of a\nsecond, giving a human driver time to react and/or to respond to objects in a road scene.  Assisted and/or automated driving systems also need to acquire information on such time scales in order to respond to rapidly approaching objects of significance\nin an oncoming road scene.  Processing fifteen to thirty images per second, of which only a portion 66 is depicted in FIG. 3A, according to the first approach is not feasible for meeting the needs of assisted and/or automated driving systems.\n Referring to FIG. 3B, a second approach, indicated by the circled number two, is depicted.  According to this second approach, contextual information is relied upon to reduce the locations from which, the densities at which, and/or the scales\nfor which sections 74a-n are extracted and/or abstracted from an image for application to a CNN 20.  For example, the number and/or different scales of sectors 74 extracted and/or abstracted from an image may be greatly reduced in an upper region of an\nimage, which would often be occupied by the sky.  Additionally, such an approach may focus on the region directly in front of the vehicle, or just off to the side, giving reduced attention to regions more to the periphery.\n However, although this second approach may greatly speed up image processing, it does so at an unacceptable cost of leaving many gaps in the field of view of an image for false negatives, i.e., incorrect determinations that objects are not\npresent when they in fact are.  For example, the upper portion of a road scene may be important where a car is headed downhill, resulting in portions of the image that would normally be devoted to the sky depicting the road ahead.  By way of another\nexample, portions of the periphery that may not be important for objects approaching slowly from the side may be very relevant where the objects are approaching rapidly from the side.  Several other examples may arise upon further reflection.\n Also, even where such a contextual approach abstracts and/or extracts a section 74 for application to a CNN 20 that depicts a classifiable object, the size of the extracted portion may be insufficient and/or may not be correctly positioned to\nabstract a sufficient amount of the depicted region.  For example, FIG. 3B depicts a series of sections 74c-f that may be abstracted from the image portion 66 for which contextual information, derived, for example and without limitation, from previous\nimage processing and/or relative positions in the image portion 66, indicates correspond to an adjacent lane or an abutting curb and sidewalk.  Processing of the first three sections 74c-e correctly indicate the absence of pedestrians, as indicated by\nthe check-mark symbols and minus signs.\n However, the right-most section 74f results in a false negative, as indicated by the barred circle.  The right-most section 74f not only appears to be a too small to capture the region of the image portion 66 depicting the pedestrian, but is\nalso ill positioned so as not to capture the head, chest, back arm, and back leg of the pedestrian.  Classification within the right-most section 74f is further complicated by the presence of the traffic light post 76.  Whereas a correctly sized and\npositioned image would result in a positive detection and classification of the pedestrian 70a in the correct location, the second approach results in a dangerous false negative.  Even if the second approach were able to detect and classify the\npedestrian 70, the ill positioned sector 74f would have provided misleading location information.\n Although the second approach in FIG. 3B may speed up the processing rate of images to provide classification, detection, and/or locating information quickly enough to allow for response, it does so at with an unacceptable cost to accuracy.  The\npedestrian 70a may have been detected by, for example, a LIDAR subsystem 14.  However the camera 16 and imaging system would fail to provide the requisite redundancy and or ability to provide completeness and to fill in gaps of other subsystems 12, 14\nrequisite to achieve the levels of statistical certainty required by assisted and/or automated driving systems.\n Also with respect to FIG. 3B, a third approach is depicted in which an image, or portion 66 thereof, is divided into different grid cells 78 by applying a grid 80 thereto.  The grid 80 applied to the image portion 66 in FIG. 3A includes four\nrows and four columns, resulting in sixteen grid cells 78.  In the third approach, grid cells 78 may completely span an image so that the image is considered in its entirety.  Also, in this third approach, the outputs 60 in the output layer 58 of a CNN\n20 may be equal to the number of grid cells 78, with each output 60 indicating whether a pedestrian 70 is present in the corresponding grid cell 78.  During image processing, grid cells 78 can be processed simultaneously to speed up processing, with\ndifferent filters 46 applied to different grid cells 78.  By applying a fully-connected layer in a neural-network portion 28 of the classifier, the output 60 for one grid cell 78 may be informed by contextual information for other grid cells 78.\n During training, as opposed to using a cropped image of an object that the network may be trained to classify, detect, and/or locate, in approaches similar to the third approach, training occurs on images of larger environments in which one or\nmore classifiable objects, such as pedestrians, may occupy only a fraction of the training images, fitting within one or more grid cells 78.  Such images are labeled both in terms of the classifiable objects that they may depict and which grid cell(s) 78\nin which they are depicted.  Additionally, overhead may be involved in generating differently labelled datasets by shifting and/or reconfiguring images in the original training dataset so that the classifiable objects therein may appear in different grid\ncells 78.\n When applied to test images, the grid cells 78 for which the corresponding outputs 60 indicate the presence of classifiable objects may be used as candidates to be applied to one or more additional classifiers.  A secondary classifier, applied\nto a single grid cell 78, may be trained with images, including cropped images dominated by classifiable objects.  Such training images may be labeled in accordance with the classifiable object they depict, requiring a different system of classification\nlabels.\n Also, the third approach is problematic where objects, such as the pedestrian 70 lie at the intersection of grid cells 78.  For example, while the eighth grid cell 78f covers much of the depicted pedestrian 70a, it also truncates the pedestrian\n70a by excluding the legs of the pedestrian 70a.  Such incongruities between grid cells 78 and depicted objects cause problems for the classification, detection, and/or the locating of objects.  As another example of a problem depicted in FIG. 3B, if the\nclassifiable object depicted, such as the pedestrian 70a, is of a different scale, such as the pedestrian 70a that extends outside of the eighth grid cell 78f, the classifier may not be well trained for classifying, detecting, and locating the depicted\nobject.\n Therefore, new approaches are needed to leverage the newly achieved accuracy of CNNs 20 to improve assisted and/or autonomous driving.  Such approaches need to be simultaneously accurate, exhaustive, and provide classification, detection, and\nlocation information at speeds allowing for timely responses for assisted and/or autonomous driving.  Such approaches, should perform with equal reliability regardless of the relative position and/or scale of the depiction of a classifiable object.  The\napproaches should be trainable with low overhead and training times susceptible to fine tuning, despite the large numbers of training images used train to acceptable levels of accuracy.\n By way of providing a brief overview of exemplary approaches to addressing these requirements, hierarchical, multi-stage approaches may be implemented.  Stages in such approaches may include both a low-fidelity classifier and a high-fidelity\nclassifier.  The low-fidelity classifier may be implemented on a set of processors, also referred to herein as a processor set.  As used herein, the terms set and subset may include any number of elements, including a single element.  The low-fidelity\nclassifier may be operable to select one or more candidate regions, from a set of overlapping regions, also referred to herein as a region set, spanning a down-sampled version of an image.\n The image may be provisioned from an automobile-affixed camera 16 capturing road-scenes.  The low-fidelity classifier may select one or more candidate regions upon determining the candidate regions(s) depict a classified object, such as a\npedestrian 70 or traffic sign, for which the low-fidelity classifier is trained.  Such multistage approaches may also include a high-fidelity classifier, which also may be implemented on the processor set.\n The high-fidelity classifier may be operable to verify classified-object depiction in one or more patches.  The patch(es) may be mapped from the candidate region(s), selected by the low-fidelity classifier, to a high-fidelity version of the\nimage.  The high-fidelity classifier may also be trained to classify, detect, and/or locate the classified object, but with higher fidelity.  The high-fidelity classifier may, therefore, verify a classified depiction in a patch mapped from a candidate\nregion, where the high-fidelity classifier indicates the depiction.\n In such approaches, the low-fidelity classifier may include a first CNN 20 that is trained with a down-sampled training set.  The down-sampled training set may include multiple, labeled, down-sampled versions of images of objects in a class\ncharacterizing the classified object.  The labeled, down-sampled versions may have dimensions commensurate to dimensions of regions in the region set.  Similarly, the high-fidelity classifier may include a second CNN 20.  This second CNN 20 is trained\nwith a high-resolution training set comprising multiple, labeled, high-fidelity versions of images of objects in the class.\n In some examples, at least some of the down-sampled versions in the down-sampled training set may be down-sampled to a lowest resolution at which entropies in the down-sampled versions remain above a threshold defined relative to entropies in\nthe original images of objects in the class.  In some, but not necessarily all such examples, the predetermined percent of entropy may come from a range centered on eighty percent and extending above and below eighty percent by five percent.\n With respect to test images, as opposed to the images used to train the first and second CNNs 20, in some examples, a down-sample module may also be implemented on the processor set.  The down-sample module may be operable to produce the\ndown-sampled version of the image from the automobile-affixed camera 16 at a down-sample factor.  In such examples, the down-sample factor may be determined to preserve, in the down-sampled version, a predetermined percent of entropy in the image from\ncamera.\n Additionally, or in the alternative, some examples may include a window module, which may be implemented on the processor set.  The window module may be operable to abstract overlapping regions from the down-sampled version of a test image. \nSuch overlapping regions may have dimensions as can be framed by a window slid fully across the down-sampled version, for purposes of generating the region set.  The window module and/or the low-fidelity classifier may then apply the overlapping regions\nof the region set to the low-fidelity classifier.  With respect to the high-fidelity classifier, some examples may include a mapping module operable to map one or more candidate regions from the down-sampled version of the image to one or more patches of\nthe high-fidelity version of the image.  As a result, the candidate region and the patch may cover a common sector of the image in the down-sampled version and the high-fidelity version respectively.\n As can be appreciated, much of the structure and functionalities discussed with respect to elements disclosed herein, may be provided by modules.  Modules may take the form of an entirely hardware embodiment, an entirely software embodiment\n(including firmware, resident software, micro-code, etc.), or an embodiment combining software and hardware aspects.  Furthermore, aspects of the presently discussed subject matter may take the form of a computer program product embodied in any tangible\nmedium of expression having computer-usable program code.\n With respect to software aspects, any combination of one or more computer-usable or computer-readable media may be utilized.  For example, a computer-readable medium may include one or more of a portable computer diskette, a hard disk, a random\naccess memory (RAM) device, a read-only memory (ROM) device, an erasable programmable read-only memory (EPROM or Flash memory) device, a portable compact disc read-only memory (CDROM), an optical storage device, and a magnetic storage device.  In\nselected embodiments, a computer-readable medium may comprise any non-transitory medium that may contain, store, communicate, propagate, or transport the program for use by, or in connection with, the instruction execution system, apparatus, or device.\n Computer program code for carrying out operations of the present invention may be written in any combination of one or more programming languages, including an object-oriented programming language such as C++, and conventional procedural\nprogramming languages, such as the \"C\" programming language, or similar programming languages.  Aspects of a module that are implemented with software may be executed on a micro-processor, Central Processing Unit (CPU) and/or the like.  Any hardware\naspects of the module may be implemented to interact with software aspects.\n As can be appreciated, the foregoing overview is not exhaustive of innovations involved in such approaches.  Several additional aspects of such approaches are discussed below.  A dataset used in such new approaches for purposes of training a\nlow-fidelity classifier is discussed in further detail with respect to the following figure.\n Referring to FIG. 4, aspects of a first dataset 82a, stored at one or more databases 84a, are depicted for the training of a first stage in multi-stage, hierarchical approaches that satisfy the requirements discussed above.  A database 84 may be\nstored on one or more physical storage mediums, such as, but without limitation, those discussed herein below.  This first stage may be a low-fidelity classifier implemented as a CNN 20.  The first dataset 82a, also referred to herein as a down-sampled\ntraining set 82a and/or a down-sampled set 82a, may be made up of versions 86 of images objects.\n The down-sampled training set 82a may include multiple, cropped, labeled, down-sampled images/versions 86a-n of images 88a-n depicting objects in a set of classes for which the low-fidelity classifier is being trained to perform classification,\ndetection, and/or location functions.  In FIG. 4, relative dimensions of a field of view of a training image 90a relative to a section 92a, region 92a, or zone 92a, of a cropped portion 88a, which is expanded for purposes of illustration, of the\nroad-scene image 90a depicting a classified object 94a, namely, a stop sign 94a.  In some examples, cropping may be achieved by framing the classified object 94 within a training image 90 to dominate the image 90.\n Also depicted in FIG. 4, is a resolution module 96 operable to down-sample 98a the training image 90a, and/or down-sample 98b the cropped portion 88a.  In the example depicted in FIG. 4, the resolution module 96 may down-sample 98 by a factor of\nfour with respect to each dimension.  However, as can be appreciated, the factor with respect to a given dimension, or both dimensions, may vary.\n By way of providing an example of guidance for determining such factors, the down-sampled images 86a-n in the first dataset 82a may include fully down-sampled images 86 that are down-sampled 98 to a limit resolution.  The resolution module 96\nmay apply and/or calculate the limit resolution as a lower limit on resolution capable of maintaining at least a predetermined percentage of entropy relative to an original, cropped image 88a from which a corresponding down-sampled image 86a/n is\ngenerated.  In some examples, this limit resolution may depend on the size, or scale, for which a cropped image 88 is being down sampled.\n In other words, methods applying the disclosures herein may involve calculating a maximum factor by which the image 88 can be down-sampled to generate a down-sampled image/version 86 while maintaining a ratio of entropy in the down-sampled\nimage/version 86 to entropy in the original image 88 above a predetermined threshold level and/or maintaining an absolute value of entropy, which may be scale dependent.  As indicated by the down-sampling icon, with arrows radiating inward from a\ncircular perimeter, in some, but not necessarily all such examples, the predetermined percent of entropy may come from a range centered on eighty percent and extending above and below eighty percent by five percent.\n Additionally, for purposes of training, each cropped, down-sampled image 86a-n may be labeled with a corresponding label 100a-n classifying the depicted object 94.  Although all of the labels 100a in FIG. 4 identify the same class, i.e., stop\nsigns, the labels 100a may come from a broader, label set 102a.  This broader label set 102a may include labels 100 for several different classes of objects 94 in a broader set of detection classes.  Examples of such classes may include pedestrians 70,\ndifferent types of pedestrians 70 (such as children), animals, vehicles, traffic signs, road markings, curb boundaries, and/or any other objects that may be pertinent to the decision processes of assisted and/or automated driving systems.\n Hence, collecting a training set of images 82a, may involve collecting a set of images 88 depicting pedestrians 70 in various positions and contexts for inclusion within the set of images 82a.  In such examples, images labeling the training set\n82a may be done according to a common class in the set of detection classes.  In as much as classified objects 94, which may be depicted at different relative distances, may be depicted at different sizes, the low-fidelity classifier, and/or high\nfidelity classifier may be trained with different image, or version, sizes, or scales, for one or more classes of objects in the set of detection objects.  Collecting a training set of images 82 may include cropping a set of images 88 of classified\nobjects 94 at a set of one or more image sizes for different classes and/or within the same class of the set of detection classes.  Hence, FIG. 4 depicts the cropping of down-sampled images/versions 86a/n at different image sizes, or scales.\n Different image sizes, or scales may correspond to different resolutions.  Consequently, in some examples, different image/versions 86 may be generated for different version/image sizes, or scales, for a common detection class by further down\nsampling 104 to further decrease resolution.  FIG. 4 depicts a first down-sampled version 86a, which is further down sampled 104 to produce a further down-sampled version 86n of the cropped image 88a.  The further down-sampled version 86n may stand in\nfor a depiction of the same stop sign 94a depicted at a new portion 106a of the down-sampled image 100a, corresponding to a greater relative distance from the camera 16 and correspondingly diminished by the laws of perspective.\n Referring to FIG. 5, the use of the low-fidelity dataset 82a to train a low-fidelity classifier 108a is further explained.  The low-fidelity classifier 108a may comprise a CNN 20a.  The low-fidelity classifier 108a is simply symbolic of a\nlow-fidelity classifier 108 implemented with a CNN 20 and is depicted with only a single convolution layer 26 with a filter depth of only three for ease of illustration.  However, in an actual implementation of the low-fidelity classifier 108, all of the\npossibilities for the CNN 20 discussed with respect to FIG. 2, including the neural-network portion 28, are open to the low-fidelity classifier 108.\n The training of the low-fidelity classifier 108a may be accomplished with the down-sampled set of labeled images 82a and the many cropped, down-sampled, labeled images/versions 86a-n therein.  The number of cropped, down-sampled, labeled\nimages/versions 86a-n may vary from the thousands to the millions.  Consequently, previous applications of CNNs 20 for image classification, detection, and/or locating have entailed training times measured in days to weeks, and even many hours on super\ncomputers.\n The speed with which an image/version 86 may be applied to the low-fidelity classifier 108a determines whether different configurations for the low-fidelity classifier 108a are feasible and/or whether it is feasible to reconfigure, retrain\nand/or fine tune the low-fidelity classifier 108a.  By using down-sampled images/versions 86, the number of computations required to forward feed 110 an image/version 86 to the low-fidelity classifier 108a may be decreased by orders of magnitude, from,\nfor example and without limitation, millions to thousands.  As a result, the time to apply the dataset 82a to the low-fidelity classifier 108a can be reduced from weeks to hours, allowing the low-fidelity classifier 108a to be implemented with a wider\nrange of possible configurations, retrained and/or fine-tuned to produce more accurate results.\n By way of explanation, a first down-sampled image/version 86a from the down-sampled set 82a may be forward fed 110a through the low-fidelity classifier 108a, from the input layer 42b to the output layer 58b.  Although only a single output 60aa\nis depicted in FIG. 5, as discussed with respect to FIG. 2, the output layer 58b may include any number of outputs 60aa-an.  Each output 60 may correspond to a different class from a set of classes for which the low-fidelity classifier 108 is being\ntrained.  Furthermore, each output 60 may indicate a probability as to whether or not the image/version 86 being forward fed 110 to the low-fidelity classifier 108 depicts a classified object 94 in the class assigned to the output 60.\n In the depiction provided in FIG. 5, the first image/version 86a has been forward fed 110a through the low-fidelity classifier 108a.  Incorrectly, however, the output 60aa assigned to the class of depicted objects 94a that are traffic signals,\nor more particularly, stop signs 94a, does not indicate that the first down-sampled image/version 86a depicts a stop sign 94a.  As discussed above, images/versions 86 in the set of images 82a may be classified according to a set of detection classes by\nlabels 100 assigned to the images/versions 86.\n Consequently, the labels 100 may be leveraged to implement a supervised learning approach to machine learning to train the low-fidelity classifier 108a.  For example, the incorrect result may be discovered and utilized to further train the\nlow-fidelity classifier 108a by comparing 112a the output 60aa, indicated as not finding a probability of a depiction by the `x` character, to the label 100a corresponding to the first version/image 86a.  The incorrect result 60aa indicates that elements\nof the low-fidelity classifier 108a, such as, without limitation, the filters 46-1a to 46-1n, the hidden layers 54, and biases 56, require further additional adjustments.  Such adjustments can be determined by applying a backward propagation of errors\nmethod 114, referred herein as backpropagation 114.\n One or more processors, which may include one or more GPUs 62, may implement the backpropagation 114a by applying an optimization method across the low-fidelity classifier 108a from the output layer 58b to the input layer 42b.  The optimization\nmethod may involve a performance function.  In some examples, the low-fidelity classifier 108a may be trained by minimizing, or maximizing, the performance function, which may be, by way of example and not limitation, one-half the squared difference\nbetween the result indicated by the label 100a and the actual probability of the output 60aa, as in a least-squares approach.  In some examples, binary values of one and zero can be assigned to the depiction of the relevant object and the lack of such a\ndepiction, respectively, or vice versa, as indicated by the corresponding label 100.\n Since, as discussed with respect to FIG. 2, a smooth, non-linear function 40a-n, 52a-n may be applied to all weights/values throughout the low-fidelity classifier 108a, a derivative of the performance function may be used to indicate the\ndirection and relative amounts by which to adjust the weights/values to minimize the performance function.  Hence the optimization method may be implemented as a gradient descent/ascent method, such as a stochastic gradient descent/ascent method. \nHowever, the large numbers of adjustable weights/values in the low-fidelity classifier 108a, complicate the problem of differentiation due to the high dimensionality of the space for the performance function.\n Backpropagation 114 provides an approach for differentiating the performance function in terms of the partial derivatives of the various adjustable weights/values in the low-fidelity classifier 108a.  In accordance with backpropagation 114, to\nfind the partial derivative of the performance function with respect to a given adjustable weight/value, the chain rule may be applied.  In applying the chain rule to find the partial derivative of a given adjustable weight/value, the partial derivative\nwith respect to a given adjustable weight/value is multiplied by the partial derivatives of any inputs leading into the performance function between the given adjustable weight/value and the output 60aa.\n By working backwards, layer by layer, from the output layer 58b toward the input layer 42b in calculating the partial derivatives of adjustable weights/values, therefore, backpropagation 114 can reuse the calculations of partial derivatives from\nthe previous layer(s).  More specifically, backpropagation 114 can use these calculations of partial derivatives when applying the chain rule and finding the product of all the intervening partial derivatives between the subject adjustable weight/value\nand the output(s) 60.  In other words, backpropagation 114 proceeds by calculating an adjustment, often scaled by a rate factor to prevent significantly overshooting a final answer, for the adjustable weights/values for each neuron/node in a layer.\n Backpropagation 114 may begin at a layer closest to the output layer 58b, such as a hidden layer 54.  Backpropagation 114 may continue by calculating the weights/values for each neuron/node in the next closest layer until the input layer 42b is\nreached.  The calculated adjustments may then be applied to their corresponding adjustable weights/values and the performance function recalculated.  Backpropagation 114 then iterates through this process until the performance function is sufficiently\nminimized and/or maximized.  Because of the iterative nature of this process, the savings in calculations and time achieved by utilizing down-sampled images/version 86 for training are multiplied for each image/version 86 used in training by the number\nof iterations required and the number of images/versions 86 in the training set 82a.\n Whereas each neuron/node in the neural-network portion 28 may involve taking the partial derivative of an action function with respect to a single adjustable weight/value, within convolution sublayers 30 of the convolution portion 24, each\nneuron/node 36 presents an action function, in terms of its corresponding filter 46, with a multidimensional matrix of variables.  When applying backpropagation 114 to a convolution sublayer 30, the partial derivative may be taken of the convolution\nfunction with respect to each indexed, adjustable weight/value of the corresponding filter 46.\n Once the performance function is minimized, or maximized, another image/version 86n may be forward fed 110b through the low-fidelity classifier 108a, to provide further training.  As depicted, another image/version 86n may have a different\nscale.  As depicted by the two enlarged instances of the first and second images/versions 86a/n, a difference in scale is also a difference of resolution with respect to the depicted, classified object 94a.\n At first, the various adjustable weights/values in the low-fidelity classifier 108a may be selected at random, they may be initialized based on the training of other networks, and/or they may be initialized based on various image processing\ntechniques.  Also, in some examples, one or more layers may be determined by an unsupervised learning process.  The high-fidelity classifier may be trained in a similar manner, as discussed with respect to the following figure.\n Referring to FIG. 6, a high-fidelity classifier 116a, which may serve as the second stage in the multi-stage image analysis system, is depicted in the context of its training.  Whereas, the down-sampled images 86a-n for which a low-fidelity\nclassifier 108 may be trained, as discussed above, to provide speed and flexibility during the training process and speed and extensive coverage during classifying, detecting, and/or locating objects 94 in a road-scene image in a first stage, a\nhigh-fidelity classifier 116 may be trained to provide improved accuracy in a second stage, to achieve an accuracy goal for no, or substantially no, misses and few false alarms.  Also, because of the additional accuracy provided by a high-fidelity\nclassifier 116, a low-fidelity classifier 108 may be trained for a high recall, and/or sensitivity, with respect to the candidate region(s) it selects, trusting the high-fidelity classifier 116 to remove irrelevant candidates.\n As with the low-fidelity classifier 108a, the high-fidelity classifier 116a may implement a CNN 20.  Also, the high-fidelity classifier 116a in FIG. 6 is simply symbolic and is, therefore, only depicted with a single convolution layer 26 with\nonly three filters 46-2a to 46-2n for ease of illustration.  Again, an actual high-fidelity classifier 116 may implement any combination of the possibilities for a CNN 20 discussed with respect to FIG. 2, including a neural-network portion 28.  Because\nof distinct roles played by the high-fidelity classifier 116, details of the architectures for the high-fidelity classifier 116 and the low-fidelity classifier 108 may, or may not, vary.\n Additionally, because of its different roles, the high-fidelity classifier 116a, with its second CNN 20, may be operable to be trained on a second dataset 82b, which may differ from the first dataset 82a, used to train the low-fidelity\nclassifier 108a.  The second dataset 82b, also referred to herein as a high-resolution training set 82b, or simply as an image set 82b, may include images 88a-n. These images 88a-n may also be cropped.  However, these images 88a-n may be maintained at\nhigher resolutions, and/or fidelity, relative to the images/versions 86a-n in the first dataset used 82a to train the low-fidelity classifier 108a.  In some examples, for which the high-fidelity classifier 116a is trained for multiple different sizes\nand/or scales, the resolution for an image 88 at a smaller size/scale may be less than the resolution of an image/version 86 at a larger scale in the first dataset 82a.  However, at a common size/scale, images in the second dataset 82b may be provided at\nhigher resolutions than those in the first dataset 82a.\n In some examples, the images 88a-n in the second dataset 82b may include the same images 88a-n used to create the first dataset 82a, but without down sampling 98, and/or with less down sampling 98.  Additionally, or in the alternative, the\nsecond dataset 82b may include new and different, albeit comparable, images 88 selected for purposes of training the high-fidelity classifier 116a.  For purposes of continuity and to enable the high-fidelity classifier to better check the candidate\nregion(s) selected by the low-fidelity classifier 108a, the images 88 in the second dataset 82b may be labeled with labels 100 from the same label set 102a as those used with respect to the first dataset 82a.\n Once trained on the second dataset 82b, the high-fidelity classifier 116a may re-classify, relative to the label set 102a, an area of a road-scene image, at high fidelity, covering the section(s), probable zone(s), and/or candidate region(s),\nselected by the low-fidelity classifier 108a.  In this way, the high-fidelity classifier 116a may be used to confirm the depiction of an object 94 classified according to the label set 102a.  Because of the increased resolutions, training times for the\nhigh-fidelity classifier 116a may increase, but adjustments and fine tuning may still be made with relative ease through the low-fidelity classifier 108a.\n For purposes of explaining the training process, an exemplary image 88a from the second dataset 82b is depicted.  The second dataset 82b may be stored on the same set of databases 84a, or different set of databases 84b, as the first dataset 82a. The image 88a, as with the images/versions 86a/n discussed in the explanation of the low-fidelity classifier 108a, is labeled with the label 100a for the classifiable object 94 of a stop sign 94a.\n The image 88a may be forward fed 110c through the high-fidelity classifier 116a, from the input layer 42c to the output layer 58c, which may provide a probability that the image 88a depicts one or more classifiable object 94 assigned to the one\nor more nodes 60ba-bn of the output layer 58c As with the low-fidelity classifier 108a, if the difference between the probability and the value indicated by the label 100a is above an acceptable amount, the backpropagation 114b process, as discussed\nabove, may be applied.  Backpropagation 114 may be implemented and/or iterated through until any significant difference is resolved.  At which point, a next image 88 from the dataset 82b may be forward fed 110 through the high-fidelity classifier 116a,\nand the process repeated.  Again, the number of cropped, labeled images 88a-n may vary from the thousands to the millions.\n The higher resolutions in the second dataset 82b, and/or in a candidate sector(s)/zone(s)/region(s), may provide additional information with which to improve the accuracy with which detecting, classifying, and/or locating may be performed on\nroad-scene images.  However, since the number of candidate regions from a road-scene image are limited by the low-fidelity classifier 108a, the high-fidelity classifier 116a can be included in a hierarchical multi-stage system without contributing\nprohibitive amounts of computation and/or time for purposes of assisted and/or autonomous driving applications.  Once the low-fidelity classifier 108a and the high-fidelity classifier 116a have been trained, therefore, they may be applied to classifying,\ndetecting, and/or locating classified objects 94 in images for purposes of assisted and/or autonomous driving applications.\n Referring to FIG. 7, a down-sample module 118 is depicted.  The down sample module 118 may be implemented on the processor set and/or a different processor set.  The down-sample module 118 may generate a down-sampled version 120 of an image 122\nfrom an automobile-affixed camera 116 capturing oncoming road-scenes by down-sampling 98 the image 122 to a down-sampled image/version 120 of the image 122.  Once down-sampled, the down-sampled image/version 120 may be analyzed by the low-fidelity\nclassifier 108a of the image analysis system.\n As with the down-sampled training set 82a, the factor, with respect to any or all dimensions, by which the down-sample module 118 may down sample 98c a road-scene image 122a may, may be determined, is some examples, to maintain a predetermined\npercent, ratio, or absolute value of entropy in the down-sampled version 120a relative to the entropy in the image 122a from the camera 16.  Again, the predetermined percent of entropy may come from a range centered on eighty percent and extend above and\nbelow eighty percent by five percent.  In some examples, the factor(s) may be determined to be as high as possible while maintaining the entropy in the down-sampled version 120a above a predetermined threshold value for a ratio or absolute value.\n In the example depicted in FIG. 7, the original road-scene image 122a has resolution dimensions of 1280 by 960, but any number of different resolutions are possible.  Because the road-scene image 122a is down sampled 98c by a factor of four, the\ndown-sampled version 120a has a resolution of 320 by 240.  Again, however, any number of different down-sampling factors and/or resolutions are possible for the down-sampled version 120a.  The down-sampled version 120a may be fed to the input layer 42b\nof the low-fidelity classifier 108a for the speedy and extensive, and/or exhaustive, preliminary analysis of the field of view captured in both the road-scene image 122a and the down-sampled version 120a.\n As indicated, the field of view captured may vary.  Possible fields of view defined with angles of view with respect to azimuth of seventy degrees and eighty degrees are indicated in FIG. 7 by way of example and not limitation.  Other\nnon-limiting examples can include fifteen degrees and three-hundred-and-sixty degrees.  Possible fields of view may also be defined with a similar variety angles of view with respect to a vertical dimension.\n Also shown in FIG. 7, is a portion 124a of the down-sampled version 120a depicting some of the oncoming road scene.  In some examples, the entire down-sampled version 120a may be fed to the input layer 42b of the low-fidelity classifier 108a as\na series of overlapping sub-regions.  In other examples, any number of different overlapping portions 124 of the down-sampled version 120a may be fed to different instances of the low-fidelity classifier 108a for simultaneous processing.  The different\noverlapping portions 124 may be overlapped by a number of pixels in each dimension corresponding to the number of cells, in the corresponding dimensions, of the largest filters 46 in the low-fidelity classifier 108a so that the entire down-sampled\nversion 120a may be processed as though it were processed on a single instance of the low-fidelity classifier 108a and without segmentation, such as the segmentation in prior art approaches discussed with respect to FIG. 3B.  The portion 124a depicted is\nof dimensions ninety-six pixels by ninety-six pixels, but any number of different dimensions are possible.\n Referring to FIG. 8, the extraction of a set of overlapping zones 126-1a to 126-1n, 126-2a to 126-2n, 126-3a to 126-3n, 126-4a to 126-4n, also referred to herein as sections and/or regions, covering the down-sampled version 120a is depicted. \nFor ease of illustration, the portion 124a of the down-sampled version 120a is depicted instead of the entire down-sampled version.  Nevertheless, the portion 124a of the down-sampled version 120a can be viewed as representative of the entire\ndown-sampled version 120a for purposes of discussion.\n The overlapping zones 126-1a to 126-1n, 126-2a to 126-2n, 126-3a to 126-3n, 126-4a to 126-4n may be definable by one or more sliding windows 128a-n with dimensions equal to dimensions of different groups of the overlapping zones 126-1a to\n126-1n, 126-2a to 126-2n, 126-3a to 126-3n, 126-4a to 126-4n.  Some examples may include a window module 130 and/or an application module 132 either on the same processor set as the low-fidelity classifier 108a or another processor set.  The window\nmodule 130 may be operable to abstract overlapping regions 126-1a to 126-1n, 126-2a to 126-2n, 126-3a to 126-3n, 126-4a to 126-4n, from the down-sampled version 120a, as can be framed by at least one window 128a-d slid 134 fully across the down-sampled\nversion 120a, for a region set, also referred to herein as a set of overlapping zones, a set of overlapping sections, and/or a set of scaled zones.\n The window module 130 may apply 136 the overlapping regions 126-1a to 126-1n, 126-2a to 126-2n, 126-3a to 126-3n, 126-4a to 126-4n to the low-fidelity classifier 108a.  Similarly, in alternative examples, the application module 132 may canvass\n134 the full field of view captured by the low-resolution image 120a by applying 136 overlapping sections 126-1a to 126-1n, 126-2a to 126-2n, 126-3a to 126-3n, 126-4a to 126-4n of the low-resolution image 120a to the low-fidelity classifier 108a.  As\neach region 126 of the region set is applied 136 to the low-fidelity classifier 108a, it is convolved 138 with filters 46-3a to 46-3n as it is forward fed 110 through the low-fidelity classifier 108a.\n To generate a region set to apply 136 to the low-fidelity classifier 108a, the window module 130 and/or application module 132 may slide 134 a window 128 from a first region 126 across the down-sampled version 120a, and/or portion 124a thereof,\nby a predetermined stride.  The stride may be one pixel horizontally, vertically, or both.  However, different numbers of pixels per stride are also possible as long as there is sufficient overlap between strides to center a classifiable object 94 in an\nabstracted region 126 and accurately locate the object 94.  In some examples, contextual information may be used to vary the stride in different regions of the down-sampled version 120a and/or portion 124a thereof.  A region 126, zone 126, and/or section\n126 may be abstracted and/or included in the region set from each position at which the sliding window 128 resides.  Hence, the number of sections 126 in the region set for a given window size may be equal to the product of the dimensions of the\ndown-sampled version 120a and/or portion 124a thereof less the corresponding dimensions of the window 128, minus one, in each dimension.\n Therefore, down sampling 98 by a factor of four can reduce the number of regions to be applied to the low-fidelity classifier 108a from a road scene image 122a of dimensions 1,280 by 960, where zero padding is applied at image boundaries, from\n1,228,800 to 76,000, or two orders of magnitude.  If portions 124 of dimensions of ninety-six by ninety-six are applied to multiple instances of the low-fidelity classifier 108a, the number of regions 126 to be applied 136 may be reduced to 9,216.  As\ncan be appreciated, such reductions greatly speed the process of classifying, detecting, and/or locating classifiable objects 94.  This is particularly true when sliding windows 128a-n of multiple different sizes, or scales, are applied to the\ndown-sampled version 120a and/or portion 124a.\n In such examples, a first window 128a may have a first set of dimensions differing from a second set of dimensions for a second window 128b.  The first window 128a, second window 128b, and/or additional windows 128c,n, may each be slid 134\nacross the down-sampled version 120a and/or portion 124a.  In such examples, the first dimensions and the second dimensions, and/or the dimensions of any additional sliding windows 128c,n may correspond to different scales at which objects in the set of\nclasses can potentially be depicted and detected in the down-sampled version 120a of the image 122a.  In other words, the window module 130 and/or an abstraction module 132 may abstract a set of scaled zones from the down-sampled image 120a, or portion\n124a thereof, with scaled zones 126 in the set of scaled zones having differing dimensions from the dimensions of a first sliding window 128a and commensurate with scaled dimensions of a scaled sliding window 128b.\n In such examples, the region set may include multiple subsets for differing sizes/scales, such as a first region subset of first overlapping regions 126-1a to 126-1n with dimensions commensurate to the first dimensions and a second region subset\nof second overlapping regions 126-2a to 126-2n with dimensions commensurate to the second dimensions.  Also, in some, but not all of such examples, the down-sampled training set 82a may include a first down-sampled subset of first down-sampled versions\n86 having dimensions commensurate to the first dimensions.  The down-sampled training set 82a may also include a second down-sampled subset with second down-sampled versions 86 having dimensions commensurate to the second dimensions.  As discussed with\nrespect to the following figure, the low-fidelity classifier 108a may select one or more candidate regions, from a region set as potentially depicting a classifiable object 94.\n Referring to FIG. 9, a hierarchical, multi-stage, image-analysis system 140 is depicted.  The image-analysis system 140 may include a low-fidelity classifier 108a followed by a high-fidelity classifier 116a.  In some examples, the\nmulti-stage-image-classification system 140 may include, together with both the low-fidelity classifier 108a and the high-fidelity classifier 116a on a processor set 142, a camera 16.  In some examples, one or more GPUs 62b, may be included within the\nprocessor set 142, implementing the low-fidelity classifier 108a and/or the high-fidelity classifier 116a.\n As discussed with respect to the first figure, a camera 16 operable to be mounted on an automobile 10 may be so mounted to capture a series road-scene images 122.  Such a camera 16 may capture the series of images 122 of oncoming road-scenes at\na frame rate satisfying a predefined threshold.  The predefined threshold may include any of the examples discussed above with respect to FIG. 1.\n Additionally, the system 140 may include an image queue 144, which may be implemented, without limitation, as a set of buffers, operable to sequentially queue a series of images 122 of oncoming road-scenes captured by the camera 16.  Also, in\nsome examples, the system may include a down-sample module 118 operable to down sample 98d road-scene images 122 to low-resolution images/versions 120.  In some examples, the image queue 144 may queue images 122 direct from the camera 16.  In other\nexamples, such as the example depicted in FIG. 9, the image queue 144 may store down-sampled versions 120 of the images 122.  Additionally, some examples may provide a portion of the image queue 144 for images 122 direct from the camera 16 and another\nportion for down sampled versions 120.\n Once a down-sampled version 120b of a road-scene image 122a, or portion 124a thereof, is forward fed 110d through the low-fidelity classifier 108a, the low-fidelity classifier 108a may select one or more probable zones 146a-n from the set of\noverlapping zones, abstracted/extracted by the window module 130 and/or aggregation module 132.\n The low-fidelity classifier 108a, which may be implemented with a first Convolution Neural Network (CNN) 20a, may be trained, or operable to be trained on the first dataset 82a, to classify, relative to the label set 102a, one or more sections\n144 from the set of overlapping sections 126a-n spanning a down-sampled version 120, or portion thereof 124, of a road-scene image 122.  Consequently, low-fidelity classifier 108a may select the one or more probable zones 146a-n for which the\nlow-fidelity classifier 108a indicates a probability of a presence of an object 94 pertaining to a class of objects classifiable by the low-fidelity classifier 108b.  In some examples, the application module 132 may be operable to note a set of potential\nsections 146a-n in which the low-fidelity classifier 108a identifies potential depictions of objects 94 classifiable according to the label set 102a.  In examples for which the low-fidelity classifier 108a is trained for multiple sizes, or scales, the\nlow-fidelity classifier 108a may select a scaled zone 146n from the set of scaled zones for which the low-fidelity classifier 108a indicates a probability of an existence of a scaled object 94 classifiable by the low-fidelity classifier 108a.\n The multi-stage system 140 may then forward feed 110e the one or more probable zones 1446a-n, or candidate regions 146a-n selected by the low-fidelity classifier 108a through the high-fidelity classifier 116a.  Where applying the sector\ncandidates 146a-n to the high-fidelity classifier 116a, which may be implemented with a second CNN 20, indicates the presence of one or more classifiable objects 94 in one or more of the sector candidates 146a-n, the high-fidelity classifier 116a may\nconfirm the presence of the one or more objects 94.  In examples where the candidate regions 146a-n include multiple sizes/scales and/or the high-fidelity classifier 116a is trained for multiple sizes/scales, the high-fidelity classifier 116a may confirm\nthe existence of a scaled object 94, where applying 110e the candidate scaled sector 146n results in a probability of the existence of the scaled object 94.  A determination module 148 may be included in some examples, which may be operable to determine\na confirmed set of areas by applying the high-fidelity classifier 116a to the set of candidate areas 146a-n.\n The multi-stage system 140 may be hierarchical, such that the low-fidelity classifier 108a first selects candidate regions 146a-n before the candidate regions 146a-n are applied to the high-fidelity classifier 116a.  However, in some examples,\nwhere the low-fidelity classifier 108a has found at least one candidate zone 146, the low-fidelity classifier 108a may continue searching zones 126 in the set of overlapping zones to which the low-fidelity classifier 108a has yet to be applied for one or\nmore additional candidate zones 146.  At the same time, the high-fidelity classifier 116a may simultaneously confirm the presence, or depiction, of one or more objects 94 by applying one or more sectors corresponding to the at least one candidate zone\n146 found previously to the high-fidelity classifier 116a.\n Additionally, the image-analysis system 140, may be operable for processing the series of images, by applying the foregoing teachings on individual images 122 in the series of images, at a processing-rate also satisfying the predefined\nthreshold.  As discussed, the predefined threshold may be set to provide sufficient time for a pre-determined assisted and/or autonomous response by the automobile 10 to classification information in the series of images.  Parameters of one of or both of\nthe low-fidelity classifier 108a and the high-fidelity classifier 116a may be set to limit computation requirements of the low-fidelity classifier 108a and/or the high-fidelity classifier 116a, relative to computing capabilities of the at least one GPU\n62b and/or processor set 142.\n Such parameters may include, without limitation, a number of convolution layers 26, the depth of one or more convolution layers 26, the dimensions of filters 46 applied at individual convolution neurons/nodes 36, the number of hidden layers 54,\nthe depth of one or more hidden layers 54, the depth of the output layer 58, and/or other such parameters.  These parameters may be controlled to enable processing the series of images at a predetermined rate providing real-time access to classification\ninformation in the series of images.  In such examples, the GPU set 62b, in the set of processors 142, may implement the first and/or second CNNs 20 to capitalize on parallel processing capabilities of the GPU 62b.  Again such capabilities may enable the\nfirst and/or second CNNs 20 to process the series of road-scene images at a rate providing time for a predetermined, assisted and/or autonomous-vehicle response to classification information in the series of road-scene images as processed.\n In some examples, the classification, detection, and/or location information in the series of road-scene images 122 provided by the multi-stage system 140 may be utilized by itself for assisted and/or autonomous driving applications.  In other\nexamples, the classification, detection, and/or location information provided by the multi-stage system 140 may be combined with information from one or more additional instruments, discussed above in relation to FIG. 1, for assisted and/or autonomous\ndriving applications.  In such examples, an aggregation module 150, implemented on the processor set 142, or some other processor set, may be utilized.  The aggregation module 150 may be operable to apply the low-fidelity classifier 108a with an\nexhaustive coverage of the down-sampled version(s) 120 of the image 122 from the camera 16, as applied to the region set, to provide redundancy to information from another imaging subsystem, such as a RADAR subsystem 12 and/or a LIDAR subsystem 14. \nAdditionally, and/or in the alternative, the information from the multi-stage system 140 may supply missing classification, detection, and/or location information absent from classification information provided by another imaging subsystem, and/or\nimprove the statistical reliability of the same.\n Referring to FIG. 10, additional infrastructure that may be relied upon to interface the low-fidelity classifier 108a and the high-fidelity classifier 116a in the multi-stage system 140 is depicted.  Such infrastructure may be utilized to\novercome the disparate resolutions for which the low-fidelity classifier 108a and the high-fidelity classifier 116a may be trained.  Such infrastructure may include a map module 152.  The map module 152 may be operable to map 154 a probable zone 146\nselected from the down-sampled version 120, or portion 124a thereof, to a sector 156a, also referred to herein as a patch 156a and/or area 156a, of a higher-resolution version of the image 122, or a portion 158 thereof.  Many different approaches to\nmapping 154 may be employed, such as, without limitation, taking into account the position of a sliding window 128 when it extracted the corresponding candidate region 146 and multiplying its position in multiple dimension by the down sampling factor.\n Where the multi-stage system 140 is trained for different sizes/scales, the mapping module 152 may map 154 a scaled zone 126 in the down-sampled version 120, or portion 124 thereof, to a scaled sector 156 of the higher-resolution version 122. \nIn some examples, the determination module 148, introduced with respect to the previous figure, may be operable to project the set of potential sections 146 on a high-fidelity version of the road-scene image 122 to create a set of candidate areas that\nmay be forward fed 110 through the high-fidelity classifier 116a.\n By way of example, a particular sector 156a depicting a classifiable object 94b of a pedestrian 70 may be forward fed 110f through the high-fidelity classifier 116a, with the corresponding convolutions 138 with filters 46 and other operations. \nAt the output layer 58d, a node 60ca trained to provide a probability that a sector 156a depicts a classifiable object 94b may cross a threshold value indicating the detection, indicated by the check symbol.  The particular node 60ca in the output layer\n58d in which the detection is made may be used to provide classification information, i.e., that the detected object 94b is pedestrian 70, which may be the class assigned to the particular output node 60ac.  Furthermore, the same information used to map\n154 the section 126 to the particular sector 154a may be used to determine the location 160 of the depicted object 94b, or pedestrian 70, in the original image 122.\n Referring to FIG. 11, a flow chart 200 depicts steps for quick, accurate, and comprehensive classification, detection, and/or locating of classifiable objects 94 in images 122 by a multi-stage image analysis system 140.  Such a system 140 may be\nbuilt with a low-fidelity classifier 108a and a high-fidelity classifier 116a that may be built around a pair of CNNs 20.  The flowchart illustrates the architecture, functionality, and/or operation of possible implementations of systems, methods, and\ncomputer program products according to examples.  In this regard, each block in the flowchart may represent a module, segment, or portion of code, which comprises one or more executable instructions for implementing the specified logical function(s).  It\nwill also be noted that each block of the flowchart illustrations, and combinations of blocks in the flowchart illustrations, may be implemented by special-purpose, hardware-based systems that perform the specified functions or acts, or combinations of\nspecial-purpose hardware and computer instructions.\n Where computer program instructions are involved, these instructions may be provided to a processor of a general purpose computer, special purpose computer, or other programmable data processing apparatus to produce a machine, such that the\ninstructions, which execute via the processor of the computer or other programmable data processing apparatus, create means for implementing the functions/acts specified in the flowchart and/or block or blocks.  These computer program instructions may\nalso be stored in a computer readable medium that may direct a computer to function in a particular manner, such that the instructions stored in the computer-readable medium produce an article of manufacture including instruction means which implement\nthe function/act specified in the flowchart and/or block or blocks.\n It should also be noted that, in some alternative implementations, the functions noted in the blocks may occur out of the order noted.  In certain embodiments, two blocks shown in succession may, in fact, be executed substantially concurrently,\nor the blocks may sometimes be executed in the reverse order, depending upon the functionality involved.  Alternatively, certain steps or functions may be omitted.\n Operations in methods 200 consistent with FIG. 11, may be grouped as training operations 202 and testing operations 204.  Some examples may simply involve the training operations 202, others just the testing operations 204, while still other\nexamples may involve both 202, 204.  In examples involving both 202, 204, such methods 200 may begin 206 by cropping 208 images 88 of one or more different objects 94 at one or more different scales.  The cropped images 88 may then be down sampled 210\nwith corresponding labels 100 attached.\n A determination 212 may be made as to whether or not the entropies of the down sampled versions 86 have been reduced to a predetermined value and/or ratio relative to the original images 88.  If the answer is no, methods 200 may return to\nfurther down-sample 210 one or more of the previously down-sampled versions 86.  If the answer is yes, methods 200 may proceed by training 214 a low-fidelity classifier 108a on the resultant down-sampled dataset 82a of down sampled images/versions 86. \nAdditionally, a high-fidelity classifier 116a may be trained 216 on a high resolution dataset 82b, which may be made up of the original, cropped images 88.\n Methods 200 may proceed to the testing operations 204 by down sampling 218 a test image 122.  Each region 126 of the down sampled test image 120 may be tested on the low-fidelity classifier 108a as a window 128 with dimensions commensurate to\nthe regions 126 is slid 134 across the down sampled test image 120.  A determination 222 may be made as to whether the low-fidelity classifier 108a has selected a candidate region 126 as potentially depicting a classifiable object 94.  If the answer is\nno, methods 200 may continue to test additional regions 126 of the down-sampled version 102.  However, if the answer is yes, methods may retrieve 224 one or more sectors 156 of the high resolution image 122 corresponding to the candidate regions 146. \nEach high-resolution sector 156 corresponding to a candidate region 146 may be tested 226 on the high-fidelity classifier 116a, which may or may not confirm the presence of depicted, classified objects 94, and the methods may end 228.\n The present disclosures may be embodied in other specific forms without departing from their spirit or essential characteristics.  The described examples are to be considered in all respects only as illustrative, not restrictive.  The scope of\nthe invention is, therefore, indicated by the appended claims, rather than by the foregoing description.  All changes within the meaning and range of equivalency of the claims are to be embraced within their scope.", "application_number": "14995134", "abstract": " Disclosures herein teach applying a set of sections spanning a\n     down-sampled version of an image of a road-scene to a low-fidelity\n     classifier to determine a set of candidate sections for depicting one or\n     more objects in a set of classes. The set of candidate sections of the\n     down-sampled version may be mapped to a set of potential sectors in a\n     high-fidelity version of the image. A high-fidelity classifier may be\n     used to vet the set of potential sectors, determining the presence of one\n     or more objects from the set of classes. The low-fidelity classifier may\n     include a first Convolution Neural Network (CNN) trained on a first\n     training set of down-sampled versions of cropped images of objects in the\n     set of classes. Similarly, the high-fidelity classifier may include a\n     second CNN trained on a second training set of high-fidelity versions of\n     cropped images of objects in the set of classes.\n", "citations": ["6233365", "7545965", "9129190", "20030076992", "20080285849", "20150139485", "20160140436", "20170083752"], "related": []}, {"id": "20170200092", "patent_code": "10373073", "patent_name": "Creating deep learning models using feature augmentation", "year": "2019", "inventor_and_country_data": " Inventors: \nKisilev; Pavel (Maalot, IL)  ", "description": "BACKGROUND\n The present invention, in some embodiments thereof, relates to creating a classification function for classifying media objects, and, more specifically, but not exclusively, to creating a classification function for classifying media objects by\na statistical analysis of features extracted from a plurality of sample media objects.\n With the increasing need for recognition and/or detection of captured media objects such as, for example, video streams, images, speech, audio, music and/or hand writing, learning models have become a common practice for classifying the captured\nmedia objects.  The learning models, such as for example, artificial neural networks (ANN) and/or convolutional neural networks (CNN) are trained with sample data, i.e. sample media objects and continuously evolve (learn) during the process of\nclassifying new (previously unseen) media objects.\n To improve generalization and/or avoid overfitting of the learning models the sample data used during the training process may often be subject to data augmentation and/or transformation.  Data augmentation aims to increase the sample data base\nin order to enhance the ability of the learning models to identify, recognize and/or classify invariances of the sample data which is presented in different and/or transformed representations.  Data augmentation may also serve to compensate for the\nlimited sample data base available for training the learning models.  Data augmentation may present additional major challenges as it may require excessive resources, for example, computation time, storage space and/or computation load.\nSUMMARY\n According to some embodiments of the present invention there are provided methods for automatically creating a classification function trained with augmented representation of features extracted from a plurality of sample media objects using one\nor more hardware processor for executing a code.  the code comprises code instructions for extracting a plurality of features from a plurality of sample media objects, generating a plurality of feature samples for each one of the plurality of features by\naugmenting the plurality of features, training a classification function with the plurality of features samples and outputting the classification function for classifying one or more new media objects.\n The classification function may perform as one or more regression functions and/or clustering functions.\n The classification function is a CNN.\n The plurality of features is extracted by submitting the plurality of sample media objects to a CNN during a learning process of the CNN.\n Optionally, topology of the classification function is optimized during the extraction of the plurality of features according to a size of each of the plurality of sample media objects.\n The augmentation includes applying one or more transformation to each of the plurality of features.\n Optionally, the one or more transformations include, a rotation, a shift, a translation, a scaling, a reflection, a geometric transformation, a pixel flip, a noise injection, a speed perturbation, a pitch shifting, a time stretching, a gain\ninduction and/or a frequency spectrum warpage.\n According to some embodiments of the present invention there are provided systems for creating a classification function trained with augmented representations of features extracted from a plurality of sample media objects.  The system comprises\nan interface module, a program store storing a code and one or more processors coupled to the interface and the program store for executing the stored code.  The code comprises code instructions to extract a plurality of features from a plurality of\nsample media objects, code instructions to generate a plurality of features samples for each one of the plurality of features by augmenting the plurality of features, code instructions to train a classification function with the plurality of features\nsamples and code instructions to output the classification function for classifying one or more new media objects.\n According to some embodiments of the present invention there are provided computer program products for creating a classification function trained with augmented representations of features extracted from a plurality of sample media objects. \nThe computer program product comprises a non-transitory computer readable storage medium and a plurality of program instructions executed by one or more processors from the non-transitory computer readable storage medium.  The program instructions\ncomprising first program instructions to extract a plurality of features from a plurality of sample media objects, second program instructions to generate a plurality of features samples for each of the plurality of features by augmenting the plurality\nof features, third program instructions to train a classification function with the plurality of features samples and fourth program instructions to output the classification function for classifying one or more new media objects. BRIEF\nDESCRIPTION OF THE SEVERAL VIEWS OF THE DRAWINGS\n Some embodiments of the invention are herein described, by way of example only, with reference to the accompanying drawings.  With specific reference now to the drawings in detail, it is stressed that the particulars shown are by way of example\nand for purposes of illustrative discussion of embodiments of the invention.  In this regard, the description taken with the drawings makes apparent to those skilled in the art how embodiments of the invention may be practiced.\n In the drawings:\n FIG. 1 is a flowchart of an exemplary process for creating a classification function through statistical analysis of augmented representations of features extracted from sample media objects, according to some embodiments of the present\ninvention;\n FIG. 2 is a schematic illustration of an exemplary CNN for extracting features from sample media objects, according to some embodiments of the present invention;\n FIG. 3 is a schematic illustration of an exemplary CNN trained with features samples, according to some embodiments of the present invention; and\n FIG. 4 is a schematic illustration of an exemplary system for creating a classification function through statistical analysis of augmented representations of features extracted from sample media objects, according to some embodiments of the\npresent invention.\nDETAILED DESCRIPTION\n The present invention, in some embodiments thereof, relates to creating a classification function for classifying media objects, and, more specifically, but not exclusively, to creating a classification function for classifying media objects by\na statistical analysis of features extracted from a plurality of sample media objects.\n According to some embodiments of the present invention, there are provided methods, systems and computer program products for creating one or more trained classification functions (models) for classifying one or more media objects, for example,\nan image, a speech record, hand writing and the likes.  The method presented in the present invention may be utilized using one or more of a plurality of classification function models, for example, an ANN and/or a CNN, however for brevity, the CNN model\nis presented herein without construing any limitation to using other learning models with the present invention.  The classification function may be used as a regression function and/or a clustering function for statistical analysis of the media objects.\n The process for creating the classification function(s) (model) involves a deep learning process during which a plurality of features are extracted from the sample media objects, augmenting the features to create one or more features maps and\ntraining the classification function(s) with the features map(s).  Creating the classification function(s) by training the CNN model using the augmented features extracted from the sample media objects may provide significant benefits over current\nexisting methods for media objects classification and more specifically classification models using CNN models.  As opposed to the augmented features classification function(s) presented in the present invention the current existing methods for media\nobjects classification consist of, for example, CNN models which are trained with a sample dataset that includes augmenting the sample media objects themselves.  It may be demonstrated that the augmented features classification function(s) present the\nsame classification, recognition and/or regression results as the current existing methods for media objects classification.\n The main advantages of the augmented features classification function(s) compared to the current existing CNN models may include reduced size of a sample dataset used for training the CNN model, avoiding overfitting of the CNN model and/or\nproviding fast convergence both during the feature extraction phase and/or during the training phase.  The size of sample dataset, i.e. the features map(s) may be significantly lower than the size of the original sample media objects and may therefore\ngreatly reduce the time and/or the resources required for training the CNN, for example, training time, storage space and/or computation load.  Moreover, traditional methods for media objects classification which use CNN trained using the sample media\nobjects themselves may often be subject to overfitting.  Overfitting relates to a situation in which the classification model, i.e. the CNN model focuses on noise and/or random errors present in the sample data rather than on the relationships and/or\ncommon characteristics of the samples of the sample dataset.  When over-fitted, the CNN model may present poor predictive performance, as it may exaggerate minor fluctuations and/or noise in the data.  Overfitting may occur as result of training a\ncomplex CNN model having multiple parameters with sample dataset which is too small.  By extracting the common features of the sample media objects (sample dataset) noise and/or random errors may be rejected from the training process making the CNN model\nmuch more robust.  By applying major pooling layers, i.e. sub-sampling layers, along the CNN model the CNN model may fast converge to produce an optimal size for providing best classification, prediction and/or recognition results.\n The first step in creating the classification function(s) is feature extraction.  Each of the extracted features may be an individual measurable property of the classified media objects.  The feature may be an informative, discriminating and\nindependent element that may be used for effective classification, recognition and/or regression of the media object(s).  The plurality of features is extracted during a learning process of a CNN model processing the incoming sample media objects to\ncreate one or more feature maps.  The CNN model may include one or more convolutional layers with pooling layers inserted in between them.  The pooling layers are inserted to reduce the amount of data that is processed by the successive convolution layer\nthus reducing processing load and/or processing time.  Each of the convolutional layers may include a plurality of learnable filters (kernels) each having a weight and a bias which are small (smaller than the input sample media object).  Each of the\nfilters is applied to sub-regions of each of the sample media objects and is replicated over the entire contents of each of the input sample media objects.  Each of the filters may be learned to activate at the detection of one or more features within\nthe sample media objects to create a dot product.  The filters may be incorporated with a non-linearity module, for example, a rectified linear unit (ReLU) to apply non-linearity to each bit in convolved data of the filter.  Stacking together the\nactivation points of the filters may form the complete activation map and/or feature map of the convolutional layer of the CNN model for each of the features.  Following the entire path of CNN model the feature map(s) may be further extended for mapping\neach of the plurality of features detected in the sample media objects.\n The second phase for creating the classification function is training the CNN model with the features map(s) after augmenting them.  The feature map(s) may be augmented through an augmentation layer using one or more of a plurality of\ntransformations, for example, rotation, shift, translation, scaling, reflection, geometric transformation, pixel flip and/or noise injection.  The augmented features are driven into a fully connected layer of the CNN model to connect the activations from\nall neurons of the convolutional layers.  The outcome of the fully-connected layer may be normalized through a softmax function to reduce the impact of extreme values in the sample media objects without removing them from the sample dataset.  The\nnormalized output may be trained by a log loss layer to identify the classification criteria for classifying the sample media objects and naturally new sample media objects.\n Before explaining at least one embodiment of the invention in detail, it is to be understood that the invention is not necessarily limited in its application to the details of construction and the arrangement of the components and/or methods set\nforth in the following description and/or illustrated in the drawings and/or the Examples.  The invention is capable of other embodiments or of being practiced or carried out in various ways.\n As will be appreciated by one skilled in the art, aspects of the present invention may be embodied as a system, method or computer program product.  Accordingly, aspects of the present invention may take the form of an entirely hardware\nembodiment, an entirely software embodiment (including firmware, resident software, micro-code, etc.) or an embodiment combining software and hardware aspects that may all generally be referred to herein as a \"circuit,\" \"module\" or \"system.\" Furthermore,\naspects of the present invention may take the form of a computer program product embodied in one or more computer readable medium(s) having computer readable program code embodied thereon.\n Any combination of one or more computer readable medium(s) may be utilized.  The computer readable medium may be a computer readable signal medium or a computer readable storage medium.  A computer readable storage medium may be, for example,\nbut not limited to, an electronic, magnetic, optical, electromagnetic, infrared, or semiconductor system, apparatus, or device, or any suitable combination of the foregoing.  More specific examples (a non-exhaustive list) of the computer readable storage\nmedium would include the following: an electrical connection having one or more wires, a portable computer diskette, a hard disk, a random access memory (RAM), a read-only memory (ROM), an erasable programmable read-only memory (EPROM or Flash memory),\nan optical fiber, a portable compact disc read-only memory (CD-ROM), an optical storage device, a magnetic storage device, or any suitable combination of the foregoing.  In the context of this document, a computer readable storage medium may be any\ntangible medium that can contain, or store a program for use by or in connection with an instruction execution system, apparatus, or device.\n A computer readable signal medium may include a propagated data signal with computer readable program code embodied therein, for example, in baseband or as part of a carrier wave.  Such a propagated signal may take any of a variety of forms,\nincluding, but not limited to, electro-magnetic, optical, or any suitable combination thereof.  A computer readable signal medium may be any computer readable medium that is not a computer readable storage medium and that can communicate, propagate, or\ntransport a program for use by or in connection with an instruction execution system, apparatus, or device.\n Program code embodied on a computer readable medium may be transmitted using any appropriate medium, including but not limited to wireless, wire line, optical fiber cable, RF, etc., or any suitable combination of the foregoing.\n Computer program code for carrying out operations for aspects of the present invention may be written in any combination of one or more programming languages, including an object oriented programming language such as Java, Smalltalk, C++ or the\nlike and conventional procedural programming languages, such as the \"C\" programming language or similar programming languages.  The program code may execute entirely on the user's computer, partly on the user's computer, as a stand-alone software\npackage, partly on the user's computer and partly on a remote computer or entirely on the remote computer or server.  In the latter scenario, the remote computer may be connected to the user's computer through any type of network, including a local area\nnetwork (LAN) or a wide area network (WAN), or the connection may be made to an external computer (for example, through the Internet using an Internet Service Provider).\n Aspects of the present invention are described below with reference to flowchart illustrations and/or block diagrams of methods, apparatus (systems) and computer program products according to embodiments of the invention.  It will be understood\nthat each block of the flowchart illustrations and/or block diagrams, and combinations of blocks in the flowchart illustrations and/or block diagrams, can be implemented by computer program instructions.  These computer program instructions may be\nprovided to a processor of a general purpose computer, special purpose computer, or other programmable data processing apparatus to produce a machine, such that the instructions, which execute via the processor of the computer or other programmable data\nprocessing apparatus, create means for implementing the functions/acts specified in the flowchart and/or block diagram block or blocks.\n These computer program instructions may also be stored in a computer readable medium that can direct a computer, other programmable data processing apparatus, or other devices to function in a particular manner, such that the instructions stored\nin the computer readable medium produce an article of manufacture including instructions which implement the function/act specified in the flowchart and/or block diagram block or blocks.\n The computer program instructions may also be loaded onto a computer, other programmable data processing apparatus, or other devices to cause a series of operational steps to be performed on the computer, other programmable apparatus or other\ndevices to produce a computer implemented process such that the instructions which execute on the computer or other programmable apparatus provide processes for implementing the functions/acts specified in the flowchart and/or block diagram block or\nblocks.\n Reference is now made to FIG. 1 which is a flowchart of an exemplary process for creating a classification function through statistical analysis of augmented representations of features extracted from sample media objects, according to some\nembodiments of the present invention.\n As shown at 110, a process 100 for creating one or more classification functions starts with extracting a plurality of features from a sample dataset which may include a plurality of media objects, for example, video streams, images, speech,\naudio, music and/or hand writing.  The plurality of features is extracted during a learning process of a learning model, for example, a CNN model which receives and processes the incoming sample media objects to create one or more feature maps each\nassociated with one of the plurality of features.  The CNN model may include one or more convolutional layers each constituted of a plurality of learnable filters (kernels).  Each of the filters may have a weight and/or a bias which may be learned during\nthe training process and/or during classification of new and/or previously unseen media objects in order to improve the classification accuracy.  The classification function(s) may be used as regression function(s) and/or a clustering function(s) for\nstatistical analysis of the media objects.\n Each of the plurality of filters is small (smaller than the input sample media object) and is applied to sub-regions of each of the sample media objects and is replicated over the entire contents of each of the input sample media objects.  Each\nof the filters is convolved (moved across) the patches of each of the incoming sample media objects and is learned to detect one or more features within the sample media objects to generate an activation point dot product.  The feature may be an\ninformative, discriminating, non-redundant and/or independent element.  The activation points may be stacked together to create a complete activation map and/or feature map of the convolutional layer of the CNN model for one or more of the features.  The\nfilters may be incorporated with a non-linearity module, for example, a ReLU to apply non-linearity to each bit in the convolved data of the filter.  One or more pooling layers, i.e. sub-sampling layers may be inserted in between the convolutional layers\nof the CNN model.  Sub-sampling may significantly reduce the amount of data within the processed sample media objects which are processed by successive one or more convolution layers, reducing the processed data may significantly reduce processing\nresources, for example, processing time, computation load and/or storage space consumed by the convolutional layer(s).  Following the entire path of CNN model the feature map(s) may be further extended for mapping each of the plurality of features\ndetected in the sample media objects.\n Reference is now made to FIG. 2 which is a schematic illustration of an exemplary CNN for extracting features from sample media objects, according to some embodiments of the present invention.  A CNN model 201 receives sample dataset 210\ncomprising a plurality of sample media objects, for example images.  The CNN model 201 operates on extracted 36.times.36 patches from the incoming images of the sample dataset 210.  The patch size of 36.times.36 pixels is a common patch size used for\nclassification, recognition and/or detection of images.  Each of the patches of each of the sample images 210 undergoes sub-sampling in the pooling layer 220A to produce 16.times.16 pixels data segments which are fed into a first convolutional layer\n230A.  The filters of the convolutional layer 230A process each of the incoming patches to generate a dot product which may be applied with a non-linearity operation, for example, a ReLU function to apply non-linearity to each bit in the convolved data\nof the filter.  Each of the patches moves through the additional layers of the CNN model 201--convolutional layers 230B, 230C and 230D which are each proceeded with a pooling layer 220B, 220C and 220D respectively which further reduce the size of the\nprocessed data to further reduce the required processing resources.  The outcome of the final convolutional layer 230D is a plurality of feature maps 250, each associated with a respective one of the patches extracted from the incoming sample images 210. The feature map identifies one or more features detected within the respective patch.\n Reference is now made once again to FIG. 1.  As shown at 120, the plurality of feature maps 250 is augmented, i.e. subjected to one or more of a plurality of transformations in order to increase the sample dataset to increase the generalization\nlevel and/or avoid overfitting of the classification function(s).  In the event the classification function is created to classify visual objects such as, for example, video streams and/or images the transformations may include, for example, rotation,\nshift, translation, scaling, reflection, geometric transformation, pixel flip and/or noise injection.  For audio and/or acoustic objects classification, the feature maps may be augmented using other transformations, for example, speed perturbation, pitch\nshifting, time stretching, inducing gain over a frequency spectrum and/or part thereof, warping a frequency spectrum and/or part thereof and/or injecting noise to a frequency spectrum and/or part thereof.  The augmentation of the features maps 250 may be\ndone using a CNN augmentation layer which produces a plurality of features samples.\n As shown at 130, the plurality of features samples is used to train the one or more classification functions, for example a CNN model to classify new and/or unseen media objects.  The variance of the augmented features maps 250 serves to both\nincrease the sample dataset of the features maps 250 and to achieve a higher level of generalization of the classification function thus avoiding overfitting of the classification function to the features maps 250.  The classification function CNN model\nmay include a fully connected layer connecting to all neurons of the previous augmentation layer to create a full activation map and/or feature map for each of the features for the classification function CNN model.  The produced activation map may be\nnormalized through a softmax function to reduce the impact of extreme values in the features map 250 without removing them from the sample dataset.  The normalized output may be trained by a log loss layer to identify the classification criteria for\nclassifying the features map 250 (extracted from the sample media objects 210) and in the same manner classify new and/or unseen sample media objects.\n Reference is now made to FIG. 3 which is a schematic illustration of an exemplary CNN trained with feature samples, according to some embodiments of the present invention.  An exemplary CNN model 301 describes one possible implementation of A\nclassification function CNN model trained with one or more of a plurality of feature maps such as the feature maps 250 produced by the CNN model 201 using the sample images 210.  The CNN model 301 includes an augmentation layer 310 which receives the one\nor more feature maps 250 and augments the feature map(s) 250 to create a plurality of features samples 350.  The one or more transformation may include, for example, rotation, shift, translation, scaling, reflection, geometric transformation, pixel flip\nand/or noise injection.  The features samples are driven into a fully connected layer 320 connecting to all neurons of the augmentation layer 310 to create a full activation map and/or feature map for each of the features for the classification function\nCNN model 301.  The fully connected layer may include a softmax function to normalize the activation maps and/or features maps to reduce the impact of extreme values in the activation map and/or features map 250 without removing them from the sample\ndataset.  The normalized output may be trained by a log loss layer 330 to identify the classification criteria for classifying the features map 250 (extracted from the sample media objects 210) and in the same manner classify new and/or unseen sample\nmedia objects.\n Reference is now made once again to FIG. 1.  As shown at 140, the classification function, for example, the CNN model is outputted to, for example, a user for classifying new and/or unseen media objects.\n It may be demonstrated that the classification performance of the classification function CNN model trained with the features sample (data augmentation of the extracted features) is similar to the classification performance of a CNN model\ntrained with data augmentation of the media objects themselves.  The following set of equations presents the convolution properties as implemented in the classification function CNN model which imply the same performance is achieved with both\naugmentation approaches.  The following equations set show that in a particular case of augmentation both augmentation methods, namely the data (media samples) based augmentation and the feature based augmentation yield the same result which is the basis\nfor the present invention.\n Equation 1 below presents the definition of convolution as applied by each of the convolutional layers of the CNN model.  f.sub.1(t).times.f.sub.2(t).ident..intg..sub.-.infin..sup..infin.f.sub.1(- .tau.).times.f.sub.2(t-.tau.)d.tau.  Equation 1:\n Where f.sub.1 is a media object and f.sub.2 is a learned feature map (or filter/kernel).  By the nature of the definition of convolution the elements of the convolution function may be interchanged as presented in equation 2 below. \nf.sub.1(t).times.f.sub.2(t).ident..intg..sub.-.infin..sup..infin.f.sub.1(- t-.tau.).times.f.sub.2(t)d.tau.  Equation 2: c.sub.1(t)=f.sub.1(t).times.f.sub.2(t).ident..intg..sub.-.infin..sup..inf-\nin.f.sub.1(.tau.).times.f.sub.2(t-.tau.)d.tau..ident..intg..sub.-.infin..s- up..infin.f.sub.1(t-.tau.).times.f.sub.2(t)d.tau.  f.sub.1(t).times.f.sub.2(t)=c.sub.1(t) Equation 3:\n Where c.sub.1 is the product of the convolution operation.\n Using standard convolution rules, produces equations 4 and 5 below.  Equation 4 expresses augmentation performed over the sample media objects themselves while equation 5 expresses augmentation of the features extracted from the sample media\nobjects.  f.sub.1(t).times.f.sub.2(t-T)=c.sub.1(t-T) Equation 4: f.sub.1(t-T).times.f.sub.2(t)=c.sub.1(t-T) Equation 5: As reflected from the equations 4 and 5 above, the result of the two augmentation approaches is the same.  Using the augmented\nfeatures maps for training deep learning models such as may present some major advantages while maintaining the same classification performance as the legacy CNN model (trained with augmentation of the sample media objects themselves): Reducing the\nsample data size which translates to reduced processing resources, for example, computation time, computation load and/or storage space during the training and/or classification processes.  Avoid overfitting by increasing the sample base using the\naugmented feature maps and avoiding noise and or random errors which may be present in the sample media objects.\n Reference is now made to FIG. 4 which is a schematic illustration of an exemplary system for creating a classification function through statistical analysis of augmented representations of features extracted from sample media objects, according\nto some embodiments of the present invention.  A classification function generation 400 includes an input module 410 which receives a plurality of sample media objects, an analysis module 420 which extracts the plurality of features, a features\naugmentation module 430 which applies transformation(s) to create features samples, a training module 440 which trains a classification function with the features samples and an output module 450 which outputs a classification function 460 to be used for\nclassification of one or more new and/or previously unseen media objects.  Wherein a module refers to a plurality of program instructions stored in a non-transitory medium and executed by one or more processors and/or one or more cores of one or more\nprocessors.  The analysis module 420 may extract one or more features from the plurality of input sample media objects 210 as described in step 110 of the process 100 and as presented by the exemplary CNN model 201.  The features augmentation module 430\nmay apply one or more of the plurality of transformations to create the features samples as described in step 120 of the process 100 and as presented by the exemplary CNN model 301.  The training module 440 may perform training of the classification\nfunction 460 as described in step 130 of the process 100 and as presented by the exemplary CNN model 301.\n The flowchart and block diagrams in the Figures illustrate the architecture, functionality, and operation of possible implementations of systems, methods and computer program products according to various embodiments of the present invention. \nIn this regard, each block in the flowchart or block diagrams may represent a module, segment, or portion of code, which comprises one or more executable instructions for implementing the specified logical function(s).  It should also be noted that, in\nsome alternative implementations, the functions noted in the block may occur out of the order noted in the figures.  For example, two blocks shown in succession may, in fact, be executed substantially concurrently, or the blocks may sometimes be executed\nin the reverse order, depending upon the functionality involved.  It will also be noted that each block of the block diagrams and/or flowchart illustration, and combinations of blocks in the block diagrams and/or flowchart illustration, can be\nimplemented by special purpose hardware-based systems that perform the specified functions or acts, or combinations of special purpose hardware and computer instructions.\n The descriptions of the various embodiments of the present invention have been presented for purposes of illustration, but are not intended to be exhaustive or limited to the embodiments disclosed.  Many modifications and variations will be\napparent to those of ordinary skill in the art without departing from the scope and spirit of the described embodiments.  The terminology used herein was chosen to best explain the principles of the embodiments, the practical application or technical\nimprovement over technologies found in the marketplace, or to enable others of ordinary skill in the art to understand the embodiments disclosed herein.\n It is expected that during the life of a patent maturing from this application many relevant systems, methods and computer programs will be developed and the scope of the term commerce information and price is intended to include all such new\ntechnologies a priori.\n As used herein the term \"about\" refers to .+-.10%.\n The terms \"comprises\", \"comprising\", \"includes\", \"including\", \"having\" and their conjugates mean \"including but not limited to\".  This term encompasses the terms \"consisting of\" and \"consisting essentially of\".\n The phrase \"consisting essentially of\" means that the composition or method may include additional ingredients and/or steps, but only if the additional ingredients and/or steps do not materially alter the basic and novel characteristics of the\nclaimed composition or method.\n As used herein, the singular form \"a\", \"an\" and \"the\" include plural references unless the context clearly dictates otherwise.  For example, the term \"a compound\" or \"at least one compound\" may include a plurality of compounds, including\nmixtures thereof.\n The word \"exemplary\" is used herein to mean \"serving as an example, instance or illustration\".  Any embodiment described as \"exemplary\" is not necessarily to be construed as preferred or advantageous over other embodiments and/or to exclude the\nincorporation of features from other embodiments.\n The word \"optionally\" is used herein to mean \"is provided in some embodiments and not provided in other embodiments\".  Any particular embodiment of the invention may include a plurality of \"optional\" features unless such features conflict.\n Throughout this application, various embodiments of this invention may be presented in a range format.  It should be understood that the description in range format is merely for convenience and brevity and should not be construed as an\ninflexible limitation on the scope of the invention.  Accordingly, the description of a range should be considered to have specifically disclosed all the possible subranges as well as individual numerical values within that range.  For example,\ndescription of a range such as from 1 to 6 should be considered to have specifically disclosed subranges such as from 1 to 3, from 1 to 4, from 1 to 5, from 2 to 4, from 2 to 6, from 3 to 6 etc., as well as individual numbers within that range, for\nexample, 1, 2, 3, 4, 5, and 6.  This applies regardless of the breadth of the range.\n Whenever a numerical range is indicated herein, it is meant to include any cited numeral (fractional or integral) within the indicated range.  The phrases \"ranging/ranges between\" a first indicate number and a second indicate number and\n\"ranging/ranges from\" a first indicate number \"to\" a second indicate number are used herein interchangeably and are meant to include the first and second indicated numbers and all the fractional and integral numerals there between.\n It is appreciated that certain features of the invention, which are, for clarity, described in the context of separate embodiments, may also be provided in combination in a single embodiment.  Conversely, various features of the invention, which\nare, for brevity, described in the context of a single embodiment, may also be provided separately or in any suitable subcombination or as suitable in any other described embodiment of the invention.  Certain features described in the context of various\nembodiments are not to be considered essential features of those embodiments, unless the embodiment is inoperative without those elements.\n Although the invention has been described in conjunction with specific embodiments thereof, it is evident that many alternatives, modifications and variations will be apparent to those skilled in the art.  Accordingly, it is intended to embrace\nall such alternatives, modifications and variations that fall within the spirit and broad scope of the appended claims.\n All publications, patents and patent applications mentioned in this specification are herein incorporated in their entirety by reference into the specification, to the same extent as if each individual publication, patent or patent application\nwas specifically and individually indicated to be incorporated herein by reference.  In addition, citation or identification of any reference in this application shall not be construed as an admission that such reference is available as prior art to the\npresent invention.  To the extent that section headings are used, they should not be construed as necessarily limiting.", "application_number": "14992047", "abstract": " A computer implemented method of automatically creating a classification\n     function trained with augmented representation of features extracted from\n     a plurality of sample media objects using one or more hardware processors\n     for executing a code. The code comprises code instructions for extracting\n     a plurality of features from a plurality of sample media objects,\n     generating a plurality of feature samples for each of the plurality of\n     features by augmenting the plurality of features, training a\n     classification function with the plurality of features samples and\n     outputting the classification function for classifying one or more new\n     media objects.\n", "citations": ["9368086", "9600628", "9668699", "9704059", "9730643", "9811905", "9940731", "9999402", "10115316", "10140709", "20110071956", "20150071539", "20150139485", "20150227809", "20150238148", "20150332111", "20160174902"], "related": []}, {"id": "20170213070", "patent_code": "10372968", "patent_name": "Object-focused active three-dimensional reconstruction", "year": "2019", "inventor_and_country_data": " Inventors: \nAghamohammadi; Aliakbar (San Diego, CA), Najafi Shoushtari; Seyed Hesameddin (San Diego, CA), Towal; Regan Blythe (La Jolla, CA)  ", "description": "BACKGROUND\n Field\n Certain aspects of the present disclosure generally relate to machine learning and, more particularly, to improving systems and methods of object-focused three-dimensional reconstruction and motion planning.\n Background\n It is desirable for autonomous systems, such as robots, to have the ability to make decisions in view of uncertainty.  For example, when operating in an unknown environment, it is also desirable, in some cases, to locate and identify certain\nobjects within the environment.  Furthermore, it may desirable to determine a plan for controlling the robot to interact with certain objects in the environment.  However, determining such a plan is computationally intensive and expensive.\nSUMMARY\n In an aspect of the present disclosure, a method for guiding a robot equipped with a camera to facilitate three-dimensional (3D) reconstruction through sampling based planning is presented.  The method includes recognizing and localizing an\nobject in a two-dimensional (2D) image.  The method also includes computing a plurality of 3D depth maps for the localized object and constructing a 3D object map from the depth maps.  The method further includes growing a sampling based structure around\nthe 3D object map and assigning a cost to each edge of the sampling based structure.  Additionally, the method includes searching the sampling based structure to determine a lowest cost sequence of edges and guiding the robot based on the searching.\n In another aspect of the present disclosure, an apparatus for guiding a robot equipped with a camera to facilitate three-dimensional (3D) reconstruction through sampling based planning is presented.  The apparatus includes a memory and at least\none processor.  The one or more processors are coupled to the memory and configured to recognize and localize an object in a two-dimensional (2D) image.  The processor(s) is(are) also configured to compute 3D depth maps for the localized object and to\nconstruct a 3D object map from the depth maps.  The processor(s) is(are) further configured to grow a sampling based structure around the 3D object map and to assign a cost to each edge of the sampling based structure.  Additionally, the processor(s)\nis(are) configured to search the sampling based structure to determine a lowest cost sequence of edges and to guide the robot based on the search.\n In yet another aspect of the present disclosure, an apparatus for guiding a robot equipped with a camera to facilitate three-dimensional (3D) reconstruction through sampling based planning is presented.  The apparatus includes means for\nrecognizing and localizing an object in a two-dimensional (2D) image.  The apparatus also includes means for computing 3D depth maps for the localized object and means for constructing a 3D object map from the depth maps.  The apparatus further includes\nmeans for growing a sampling based structure around the 3D object map and means for assigning a cost to each edge of the sampling based structure.  Additionally, the apparatus includes means for searching the sampling based structure to determine a\nlowest cost sequence of edges and means for guiding the robot based on the search.\n In still another aspect of the present disclosure, a non-transitory computer readable medium is presented.  The non-transitory computer readable medium has encoded thereon program code for guiding a robot equipped with a camera to facilitate\nthree-dimensional (3D) reconstruction through sampling based planning.  The program code is executed by a processor and includes program code to recognize and localize an object in a two-dimensional (2D) image.  The program code also includes program\ncode to compute 3D depth maps for the localized object and to construct a 3D object map from the depth maps.  The program code further includes program code to grow a sampling based structure around the 3D object map and to assign a cost to each edge of\nthe sampling based structure.  Additionally, the program code includes program code to search the sampling based structure to determine a lowest cost sequence of edges and to guide the robot based on the search.\n Additional features and advantages of the disclosure will be described below.  It should be appreciated by those skilled in the art that this disclosure may be readily utilized as a basis for modifying or designing other structures for carrying\nout the same purposes of the present disclosure.  It should also be realized by those skilled in the art that such equivalent constructions do not depart from the teachings of the disclosure as set forth in the appended claims.  The novel features, which\nare believed to be characteristic of the disclosure, both as to its organization and method of operation, together with further objects and advantages, will be better understood from the following description when considered in connection with the\naccompanying figures.  It is to be expressly understood, however, that each of the figures is provided for the purpose of illustration and description only and is not intended as a definition of the limits of the present disclosure. BRIEF\nDESCRIPTION OF THE DRAWINGS\n The features, nature, and advantages of the present disclosure will become more apparent from the detailed description set forth below when taken in conjunction with the drawings in which like reference characters identify correspondingly\nthroughout.\n FIG. 1 illustrates an example implementation of designing a neural network using a system-on-a-chip (SOC), including a general-purpose processor in accordance with certain aspects of the present disclosure.\n FIG. 2 illustrates an example implementation of a system in accordance with aspects of the present disclosure.\n FIG. 3A is a diagram illustrating a neural network in accordance with aspects of the present disclosure.\n FIG. 3B is a block diagram illustrating an exemplary deep convolutional network (DCN) in accordance with aspects of the present disclosure.\n FIG. 4 is a block diagram illustrating an exemplary software architecture that may modularize artificial intelligence (AI) functions in accordance with aspects of the present disclosure.\n FIG. 5 is a block diagram illustrating the run-time operation of an artificial intelligence (AI) application on a smartphone in accordance with aspects of the present disclosure.\n FIG. 6 is a block diagram illustrating a framework for 3D reconstruction in accordance with aspects of the present disclosure.\n FIG. 7A is an exemplary diagram illustrating a pixel depth determination in accordance with aspects of the present disclosure.\n FIG. 7B is an exemplary diagram illustrating motion-dependent depth variance in accordance with aspects of the present disclosure.\n FIG. 7C illustrates an exemplary manipulator in accordance with aspects of the present disclosure.\n FIG. 8 illustrates a method for guiding a robot equipped with a camera to facilitate 3D reconstruction according to aspects of the present disclosure.\nDETAILED DESCRIPTION\n The detailed description set forth below, in connection with the appended drawings, is intended as a description of various configurations and is not intended to represent the only configurations in which the concepts described herein may be\npracticed.  The detailed description includes specific details for the purpose of providing a thorough understanding of the various concepts.  However, it will be apparent to those skilled in the art that these concepts may be practiced without these\nspecific details.  In some instances, well-known structures and components are shown in block diagram form in order to avoid obscuring such concepts.\n Based on the teachings, one skilled in the art should appreciate that the scope of the disclosure is intended to cover any aspect of the disclosure, whether implemented independently of or combined with any other aspect of the disclosure.  For\nexample, an apparatus may be implemented or a method may be practiced using any number of the aspects set forth.  In addition, the scope of the disclosure is intended to cover such an apparatus or method practiced using other structure, functionality, or\nstructure and functionality in addition to or other than the various aspects of the disclosure set forth.  It should be understood that any aspect of the disclosure disclosed may be embodied by one or more elements of a claim.\n The word \"exemplary\" is used herein to mean \"serving as an example, instance, or illustration.\" Any aspect described herein as \"exemplary\" is not necessarily to be construed as preferred or advantageous over other aspects.\n Although particular aspects are described herein, many variations and permutations of these aspects fall within the scope of the disclosure.  Although some benefits and advantages of the preferred aspects are mentioned, the scope of the\ndisclosure is not intended to be limited to particular benefits, uses or objectives.  Rather, aspects of the disclosure are intended to be broadly applicable to different technologies, system configurations, networks and protocols, some of which are\nillustrated by way of example in the figures and in the following description of the preferred aspects.  The detailed description and drawings are merely illustrative of the disclosure rather than limiting, the scope of the disclosure being defined by\nthe appended claims and equivalents thereof.\n 3D Model Reconstruction\n Aspects of the present disclosure are directed to systems and methods for improved 3D model reconstruction.  In one exemplary aspect, 3D model reconstruction may be employed in the context of motion planning for an autonomous robot or other\nagent (e.g., manipulators, drones, ground mobile robots, surface vehicles (e.g., boats), underwater vehicles, autonomous cars, and the like).  In this context, it may be desirable to determine how to move a robot to interact with or contact an object in\nan environment.  For instance, a robot may be configured with a camera.  The camera may be positioned within or about the grasper or hand of the robot.  The location and number of cameras is merely exemplary and the robot or other agent may also be\nconfigured with multiple cameras at various locations.  In this configuration, the accuracy of a reconstruction mechanism may be characterized with respect to the motion of the camera.  This information may be incorporated into a planning framework to\ncalculate a camera trajectory that may produce improved or highly accurate surface reconstruction of an object of interest.\n The desired objective may be to grasp an object (e.g., a cup) with a robot arm.  The scene or current view of the environment via the camera may be explored to locate the object of interest.  The goal of the exploration process is to move the\nmanipulator and/or camera so as to find the object in the environment or scene (e.g., the object of interest in an image or within the field of view of the camera).  In some aspects, the scene exploration may be conducted using random search techniques,\ncoverage techniques, frontier-based exploration techniques and the like.  When the object is recognized, a depth map may be computed based on camera images of the object.  For example, the depth of the pixel in each of the images may be determined.  The\ndepth information or depth maps may in turn be used to determine an object map, which is a 3D reconstruction of the localized object.\n The object map may be used to generate a planning graph.  The planning graph may comprise a graph of candidate motions around the object to be grasped.  A cost for each of the candidate motions may be determined.  The candidate motion having the\nlowest cost may be selected and used to move the robot arm.  As the robot arm is moved, additional images of the object may be captured and used to determine a subsequent movement or sequence of movements.  Accordingly, a best or most efficient\ntrajectory for grasping the object with the robotic arm may be determined based on the generated 3D object reconstruction.\n FIG. 1 illustrates an example implementation for guiding a robot equipped with a camera to facilitate 3D reconstruction through sampling based planning using a system-on-a-chip (SOC) 100, which may include a general-purpose processor (CPU) or\nmulti-core general-purpose processors (CPUs) 102 in accordance with certain aspects of the present disclosure.  Variables (e.g., neural signals and synaptic weights), system parameters associated with a computational device (e.g., neural network with\nweights), delays, frequency bin information, and task information may be stored in a memory block associated with a neural processing unit (NPU) 108, in a memory block associated with a CPU 102, in a memory block associated with a graphics processing\nunit (GPU) 104, in a memory block associated with a digital signal processor (DSP) 106, in a dedicated memory block 118, or may be distributed across multiple blocks.  Instructions executed at the general-purpose processor 102 may be loaded from a\nprogram memory associated with the CPU 102 or may be loaded from a dedicated memory block 118.\n The SOC 100 may also include additional processing blocks tailored to specific functions, such as a GPU 104, a DSP 106, a connectivity block 110, which may include fourth generation long term evolution (4G LTE) connectivity, unlicensed Wi-Fi\nconnectivity, USB connectivity, Bluetooth connectivity, and the like, and a multimedia processor 112 that may, for example, detect and recognize gestures.  In one implementation, the NPU is implemented in the CPU, DSP, and/or GPU.  The SOC 100 may also\ninclude a sensor processor 114, image signal processors (ISPs), and/or navigation 120, which may include a global positioning system.\n The SOC 100 may be based on an ARM instruction set.  In an aspect of the present disclosure, the instructions loaded into the general-purpose processor 102 may comprise code for recognizing and localizing an object in a two-dimensional (2D)\nimage.  The instructions loaded into the general-purpose processor 102 may also comprise code for computing three dimensional (3D) depth maps for the localized object and constructing a 3D object map from the depth maps.  Additionally, instructions\nloaded into the general-purpose processor 102 may comprise code for growing a sampling based structure around the 3D object map and assigning a cost to each edge of the sampling based structure.  Furthermore, the instructions loaded into the\ngeneral-purpose processor 102 may comprise code for searching the sampling based structure to determine a lowest cost sequence of edges and guiding the robot based on the search.\n FIG. 2 illustrates an example implementation of a system 200 in accordance with certain aspects of the present disclosure.  As illustrated in FIG. 2, the system 200 may have multiple local processing units 202 that may perform various operations\nof methods described herein.  Each local processing unit 202 may comprise a local state memory 204 and a local parameter memory 206 that may store parameters of a neural network.  In addition, the local processing unit 202 may have a local (neuron) model\nprogram (LMP) memory 208 for storing a local model program, a local learning program (LLP) memory 210 for storing a local learning program, and a local connection memory 212.  Furthermore, as illustrated in FIG. 2, each local processing unit 202 may\ninterface with a configuration processor unit 214 for providing configurations for local memories of the local processing unit, and with a routing connection processing unit 216 that provides routing between the local processing units 202.\n Deep learning architectures may perform an object recognition task by learning to represent inputs at successively higher levels of abstraction in each layer, thereby building up a useful feature representation of the input data.  In this way,\ndeep learning addresses a major bottleneck of traditional machine learning.  Prior to the advent of deep learning, a machine learning approach to an object recognition problem may have relied heavily on human engineered features, perhaps in combination\nwith a shallow classifier.  A shallow classifier may be a two-class linear classifier, for example, in which a weighted sum of the feature vector components may be compared with a threshold to predict to which class the input belongs.  Human engineered\nfeatures may be templates or kernels tailored to a specific problem domain by engineers with domain expertise.  Deep learning architectures, in contrast, may learn to represent features that are similar to what a human engineer might design, but through\ntraining.  Furthermore, a deep network may learn to represent and recognize new types of features that a human might not have considered.\n A deep learning architecture may learn a hierarchy of features.  If presented with visual data, for example, the first layer may learn to recognize relatively simple features, such as edges, in the input stream.  In another example, if presented\nwith auditory data, the first layer may learn to recognize spectral power in specific frequencies.  The second layer, taking the output of the first layer as input, may learn to recognize combinations of features, such as simple shapes for visual data or\ncombinations of sounds for auditory data.  For instance, higher layers may learn to represent complex shapes in visual data or words in auditory data.  Still higher layers may learn to recognize common visual objects or spoken phrases.\n Deep learning architectures may perform especially well when applied to problems that have a natural hierarchical structure.  For example, the classification of motorized vehicles may benefit from first learning to recognize wheels, windshields,\nand other features.  These features may be combined at higher layers in different ways to recognize cars, trucks, and airplanes.\n Neural networks may be designed with a variety of connectivity patterns.  In feed-forward networks, information is passed from lower to higher layers, with each neuron in a given layer communicating to neurons in higher layers.  A hierarchical\nrepresentation may be built up in successive layers of a feed-forward network, as described above.  Neural networks may also have recurrent or feedback (also called top-down) connections.  In a recurrent connection, the output from a neuron in a given\nlayer may be communicated to another neuron in the same layer.  A recurrent architecture may be helpful in recognizing patterns that span more than one of the input data chunks that are delivered to the neural network in a sequence.  A connection from a\nneuron in a given layer to a neuron in a lower layer is called a feedback (or top-down) connection.  A network with many feedback connections may be helpful when the recognition of a high-level concept may aid in discriminating the particular low-level\nfeatures of an input.\n Referring to FIG. 3A, the connections between layers of a neural network may be fully connected 302 or locally connected 304.  In a fully connected network 302, a neuron in a first layer may communicate its output to every neuron in a second\nlayer, so that each neuron in the second layer will receive input from every neuron in the first layer.  Alternatively, in a locally connected network 304, a neuron in a first layer may be connected to a limited number of neurons in the second layer.  A\nconvolutional network 306 may be locally connected, and is further configured such that the connection strengths associated with the inputs for each neuron in the second layer are shared (e.g., 308).  More generally, a locally connected layer of a\nnetwork may be configured so that each neuron in a layer will have the same or a similar connectivity pattern, but with connections strengths that may have different values (e.g., 310, 312, 314, and 316).  The locally connected connectivity pattern may\ngive rise to spatially distinct receptive fields in a higher layer, because the higher layer neurons in a given region may receive inputs that are tuned through training to the properties of a restricted portion of the total input to the network.\n Locally connected neural networks may be well suited to problems in which the spatial location of inputs is meaningful.  For instance, a network 300 designed to recognize visual features from a car-mounted camera may develop high layer neurons\nwith different properties depending on their association with the lower versus the upper portion of the image.  Neurons associated with the lower portion of the image may learn to recognize lane markings, for example, while neurons associated with the\nupper portion of the image may learn to recognize traffic lights, traffic signs, and the like.\n A deep convolutional network (DCN) may be trained with supervised learning.  During training, a DCN may be presented with an image, such as a cropped image of a speed limit sign 326, and a \"forward pass\" may then be computed to produce an output\n322.  The output 322 may be a vector of values corresponding to features such as \"sign,\" \"60,\" and \"100.\" The network designer may want the DCN to output a high score for some of the neurons in the output feature vector, for example the ones\ncorresponding to \"sign\" and \"60\" as shown in the output 322 for a network 300 that has been trained.  Before training, the output produced by the DCN is likely to be incorrect, and so an error may be calculated between the actual output and the target\noutput.  The weights of the DCN may then be adjusted so that the output scores of the DCN are more closely aligned with the target.\n To adjust the weights, a learning algorithm may compute a gradient vector for the weights.  The gradient may indicate an amount that an error would increase or decrease if the weight were adjusted slightly.  At the top layer, the gradient may\ncorrespond directly to the value of a weight connecting an activated neuron in the penultimate layer and a neuron in the output layer.  In lower layers, the gradient may depend on the value of the weights and on the computed error gradients of the higher\nlayers.  The weights may then be adjusted so as to reduce the error.  This manner of adjusting the weights may be referred to as \"back propagation\" as it involves a \"backward pass\" through the neural network.\n In practice, the error gradient of weights may be calculated over a small number of examples, so that the calculated gradient approximates the true error gradient.  This approximation method may be referred to as stochastic gradient descent. \nStochastic gradient descent may be repeated until the achievable error rate of the entire system has stopped decreasing or until the error rate has reached a target level.\n After learning, the DCN may be presented with new images 326 and a forward pass through the network may yield an output 322 that may be considered an inference or a prediction of the DCN.\n Deep belief networks (DBNs) are probabilistic models comprising multiple layers of hidden nodes.  DBNs may be used to extract a hierarchical representation of training data sets.  A DBN may be obtained by stacking up layers of Restricted\nBoltzmann Machines (RBMs).  An RBM is a type of artificial neural network that can learn a probability distribution over a set of inputs.  Because RBMs can learn a probability distribution in the absence of information about the class to which each input\nshould be categorized, RBMs are often used in unsupervised learning.  Using a hybrid unsupervised and supervised paradigm, the bottom RBMs of a DBN may be trained in an unsupervised manner and may serve as feature extractors, and the top RBM may be\ntrained in a supervised manner (on a joint distribution of inputs from the previous layer and target classes) and may serve as a classifier.\n Deep convolutional networks (DCNs) are networks of convolutional networks, configured with additional pooling and normalization layers.  DCNs have achieved state-of-the-art performance on many tasks.  DCNs can be trained using supervised\nlearning in which both the input and output targets are known for many exemplars and are used to modify the weights of the network by use of gradient descent methods.\n DCNs may be feed-forward networks.  In addition, as described above, the connections from a neuron in a first layer of a DCN to a group of neurons in the next higher layer are shared across the neurons in the first layer.  The feed-forward and\nshared connections of DCNs may be exploited for fast processing.  The computational burden of a DCN may be much less, for example, than that of a similarly sized neural network that comprises recurrent or feedback connections.\n The processing of each layer of a convolutional network may be considered a spatially invariant template or basis projection.  If the input is first decomposed into multiple channels, such as the red, green, and blue channels of a color image,\nthen the convolutional network trained on that input may be considered three-dimensional, with two spatial dimensions along the axes of the image and a third dimension capturing color information.  The outputs of the convolutional connections may be\nconsidered to form a feature map in the subsequent layer 318 and 320, with each element of the feature map (e.g., 320) receiving input from a range of neurons in the previous layer (e.g., 318) and from each of the multiple channels.  The values in the\nfeature map may be further processed with a non-linearity, such as a rectification, max(0,x).  Values from adjacent neurons may be further pooled, which corresponds to down sampling, and may provide additional local invariance and dimensionality\nreduction.  Normalization, which corresponds to whitening, may also be applied through lateral inhibition between neurons in the feature map.\n The performance of deep learning architectures may increase as more labeled data points become available or as computational power increases.  Modern deep neural networks are routinely trained with computing resources that are thousands of times\ngreater than what was available to a typical researcher just fifteen years ago.  New architectures and training paradigms may further boost the performance of deep learning.  Rectified linear units may reduce a training issue known as vanishing\ngradients.  New training techniques may reduce over-fitting and thus enable larger models to achieve better generalization.  Encapsulation techniques may abstract data in a given receptive field and further boost overall performance.\n FIG. 3B is a block diagram illustrating an exemplary deep convolutional network 350.  The deep convolutional network 350 may include multiple different types of layers based on connectivity and weight sharing.  As shown in FIG. 3B, the exemplary\ndeep convolutional network 350 includes multiple convolution blocks (e.g., C1 and C2).  Each of the convolution blocks may be configured with a convolution layer, a normalization layer (LNorm), and a pooling layer.  The convolution layers may include one\nor more convolutional filters, which may be applied to the input data to generate a feature map.  Although only two convolution blocks are shown, the present disclosure is not so limiting, and instead, any number of convolutional blocks may be included\nin the deep convolutional network 350 according to design preference.  The normalization layer may be used to normalize the output of the convolution filters.  For example, the normalization layer may provide whitening or lateral inhibition.  The pooling\nlayer may provide down sampling aggregation over space for local invariance and dimensionality reduction.\n The parallel filter banks, for example, of a deep convolutional network may be loaded on a CPU 102 or GPU 104 of an SOC 100, optionally based on an ARM instruction set, to achieve high performance and low power consumption.  In alternative\nembodiments, the parallel filter banks may be loaded on the DSP 106 or an ISP 116 of an SOC 100.  In addition, the DCN may access other processing blocks that may be present on the SOC, such as processing blocks dedicated to sensors 114 and navigation\n120.\n The deep convolutional network 350 may also include one or more fully connected layers (e.g., FC1 and FC2).  The deep convolutional network 350 may further include a logistic regression (LR) layer.  Between each layer of the deep convolutional\nnetwork 350 are weights (not shown) that are to be updated.  The output of each layer may serve as an input of a succeeding layer in the deep convolutional network 350 to learn hierarchical feature representations from input data (e.g., images, audio,\nvideo, sensor data and/or other input data) supplied at the first convolution block C1.\n FIG. 4 is a block diagram illustrating an exemplary software architecture 400 that may modularize artificial intelligence (AI) functions.  Using the architecture, applications 402 may be designed that may cause various processing blocks of an\nSOC 420 (for example a CPU 422, a DSP 424, a GPU 426 and/or an NPU 428) to perform supporting computations during run-time operation of the application 402.\n The AI application 402 may be configured to call functions defined in a user space 404 that may, for example, provide for the detection and recognition of a scene indicative of the location in which the device currently operates.  The AI\napplication 402 may, for example, configure a microphone and a camera differently depending on whether the recognized scene is an office, a lecture hall, a restaurant, or an outdoor setting such as a lake.  The AI application 402 may make a request to\ncompiled program code associated with a library defined in a SceneDetect application programming interface (API) 406 to provide an estimate of the current scene.  This request may ultimately rely on the output of a deep neural network configured to\nprovide scene estimates based on video and positioning data, for example.\n A run-time engine 408, which may be compiled code of a Runtime Framework, may be further accessible to the AI application 402.  The AI application 402 may cause the run-time engine, for example, to request a scene estimate at a particular time\ninterval or triggered by an event detected by the user interface of the application.  When caused to estimate the scene, the run-time engine may in turn send a signal to an operating system 410, such as a Linux Kernel 412, running on the SOC 420.  The\noperating system 410, in turn, may cause a computation to be performed on the CPU 422, the DSP 424, the GPU 426, the NPU 428, or some combination thereof.  The CPU 422 may be accessed directly by the operating system, and other processing blocks may be\naccessed through a driver, such as a driver 414-418 for a DSP 424, for a GPU 426, or for an NPU 428.  In the exemplary example, the deep neural network may be configured to run on a combination of processing blocks, such as a CPU 422 and a GPU 426, or\nmay be run on an NPU 428, if present.\n FIG. 5 is a block diagram illustrating the run-time operation 500 of an AI application on a smartphone 502.  The AI application may include a pre-process module 504 that may be configured (using for example, the JAVA programming language) to\nconvert the format of an image 506 and then crop and/or resize the image 508.  The pre-processed image may then be communicated to a classify application 510 that contains a SceneDetect Backend Engine 512 that may be configured (using for example, the C\nprogramming language) to detect and classify scenes based on visual input.  The SceneDetect Backend Engine 512 may be configured to further preprocess 514 the image by scaling 516 and cropping 518.  For example, the image may be scaled and cropped so\nthat the resulting image is 224 pixels by 224 pixels.  These dimensions may map to the input dimensions of a neural network.  The neural network may be configured by a deep neural network block 520 to cause various processing blocks of the SOC 100 to\nfurther process the image pixels with a deep neural network.  The results of the deep neural network may then be thresholded 522 and passed through an exponential smoothing block 524 in the classify application 510.  The smoothed results may then cause a\nchange of the settings and/or the display of the smartphone 502.\n In one configuration, a machine learning model is configured for recognizing and localizing an object.  The model is also configured for computing a plurality of depth maps for the localized object and for constructing an object map (3D\nconstruction of the localized object) from the depth maps.  The model is further configured for growing a sampling based structure around the object map and assigning a cost to each edge of the sampling based structure.  Furthermore, the model is\nconfigured for searching the sampling based structure to determine a lowest cost sequence of edges and for guiding the robot based on the search.  The model includes means for recognizing and localizing, computing means, constructing means, growing\nmeans, assigning means, searching means and/or guiding means.  In one aspect, the means for recognizing and localizing, computing means, constructing means, growing means, assigning means, searching means and/or guiding means may be the general-purpose\nprocessor 102, program memory associated with the general-purpose processor 102, memory block 118, local processing units 202, and or the routing connection processing units 216 configured to perform the functions recited.  In another configuration, the\naforementioned means may be any module or any apparatus configured to perform the functions recited by the aforementioned means.\n According to certain aspects of the present disclosure, each local processing unit 202 may be configured to determine parameters of the model based upon desired one or more functional features of the model, and develop the one or more functional\nfeatures towards the desired functional features as the determined parameters are further adapted, tuned and updated.\n FIG. 6 is a block diagram illustrating a framework 600 for 3D reconstruction in accordance with aspects of the present disclosure.  The framework may be used to produce a motion plan that facilitates 3D reconstruction of an object observed in a\n2D image.  The framework 600 includes an object recognition and localization unit 602, a depth mapping unit 604, a planning graph unit 606, a motion planning unit 610 and an execution unit 612.  In some aspects, the framework may also include an accuracy\nevaluation unit 608, which may evaluate the accuracy of the object reconstruction.\n The object recognition and localization unit 602 performs object localization in an image, for example, using deep learning techniques, to determine a region of interest in the image.  As such, the framework 600 may focus on the determined\nregion of interest to achieve a focused and efficient 3D reconstruction.\n The object recognition and localization unit 602 may be configured to localize and recognize or identify an object in an image (e.g., the field of view of a camera).  In some aspects, scene exploration may also be performed, for example, when\nthe object of interest is not in the field of view.  The scene exploration techniques may be employed to move the camera and/or agent to find the object of interest in the environment or scene.  For instance, a scene may be explored using coverage or\nrandom techniques, frontier-based exploration or other exploration techniques.  In one example, where the agent is a drone, the terrain of a region may be explored.  Scene exploration may be performed to locate a landing area by controlling the camera to\nsweep the area below as the drone flies over the terrain.\n In some aspects, an object-relation graph may also be used to enhance the scene exploration performance.  The object-relation graph may incorporate knowledge regarding the object of interest to limit the region to be searched.  For example,\nwhere the object being searched for is a cup, there is a higher probability that the cup is on a table, as opposed to on the floor.  Accordingly, if a table is included in the image (or partially included), the object-relation graph may be used to adjust\nthe scene exploration such that the top of the table is searched with a higher priority than under the table.\n In some aspects, the object recognition and localization unit 602 may also be trained to recognize objects based on audible input.  For example, upon receiving an audible input for the object of interest (e.g., a cup), the object recognition and\nlocalization unit 602 may retrieve images from an image repository corresponding to the word \"cup\".\n When a candidate object is detected, object recognition techniques may be used to identify the candidate object.  If the candidate object is not the object of interest for the scene exploration, the scene exploration may continue.\n If the candidate object is the object of interest for the scene exploration (e.g., the object of interest is recognized in the field of view (or image)), object localization may be performed to determine the location of the object or part of the\nobject in the image (e.g., a 2D image).  Object localization techniques may be used to determine an estimate of the object location.  In some aspects, a bounding box may be formed around the object.  In doing so, the scale and location of the object may\nbe determined.  Based on this information and the location of the camera, control input may be determined to move the camera to better center the object within the bounding box.\n In some aspects, lightweight localization may be achieved by finding the residuals in the power spectrum of an image.  On the other hand, localization that is more robust may be achieved using deep learning techniques.  For example, a DCN 350\n(FIG. 3B) may learn features of image patches likely to include the object of interest.  Using the more robust methods, the object may be located and then tracked rather than repeating localization procedures.\n The framework may also include a depth mapping unit 604.  The depth mapping unit 604 computes a dense depth map for the localized object.  Having localized the object, depth information such as a depth estimate may be determined for each pixel\ncorresponding to the object.  Because the object has been localized, the depth estimates may be limited to relevant portions of the image (e.g., pixels within the bounding box area) rather than computing depth estimates for every pixel in the image.  By\nfocusing the depth computations in this manner, the framework 600 may enable reduction in power and memory consumption, as well as increased processing efficiency.\n The depth estimate for each pixel corresponding to the object of interest may be used to generate a depth map for the object.  The depth map may comprise a grid such as a three-dimensional grid, for example.  The grid may be arranged based on\nthe position of the pixels in the image and the corresponding depths or depth estimates.  In some aspects, the position of the pixels and the corresponding depth information may be used to find a corresponding cell (or voxel) in the grid for each pixel\nin the image or identified portion.  The pixel and its depth information may be stored in the corresponding cell of the grid.  This process of finding a corresponding cell or voxel in the grid may be repeated for each of the cells over time to generate\nthe depth map.\n In one exemplary configuration, the camera may be positioned and/or coupled on or about the hand (e.g., palm) of the agent (e.g., robot).  Of course, the number of cameras and placement of the camera with respect to the agent is merely exemplary\nand not limiting.  Positioning the camera in the hand may improve depth inference.  This is because the depth of a point is determined by observing the point from two different positions.  The greater the distance between the two positions, the better\nthe inference of the point depth.  Accordingly, as compared to conventional approaches of using a humanoid robot in which the camera is placed on or about the head of the robot, a greater amount of displacement is possible with the camera positioned on\nor about the hand.\n Additionally, scene exploration tasks may also be enhanced by positioning or coupling the camera on or about the hand of the agent (e.g., robot).  That is, by moving the hand of the agent, the camera position may be changed to provide an\nincreased range of vantage points from which to observe an environment or region.  For instance, the hand of an agent may be raised to view a region from a position above the agent's head.  In another example, the hand of an agent may be lowered such\nthat areas underneath structures (e.g., a table) may be observed.\n FIG. 7A is an exemplary diagram illustrating a pixel depth determination in accordance with aspects of the present disclosure.  The point rP (real location of point p) is observed from two locations (r, k) indicated by the center of the camera\nat the respective locations and denoted C.sub.r and C.sub.k.  A pixel u corresponding to the point p is shown on image planes (I.sub.r and I.sub.k, respectively) for the camera at each location.  An estimate of the pixel depth, which may correspond to\nthe distance between the camera center C.sub.r and the point location (rP), may be determined.\n In one example, an estimate of the pixel depth may be determined using a Kalman filter.  The filter output may be in the form of a probability distribution function (PDF) (see element number 702) for the actual location of point p (rP) based on\nan estimated location (shown as rP.sup.+).  The variance of point p may be computed by back-projecting a constant variance (e.g., for one pixel).  Using the peak of the PDF at the most likely location of point p, the distance camera center C.sub.r and\nthe point location (rP).\n In addition, the breadth or narrowness of the distribution may provide an indication of the confidence in the estimated pixel depth rP.sup.+.  That is, the wider the probability distribution, the greater the number of possible locations for\npoint p. Thus, a relationship between pixel depth variance .sigma..sub.d.sup.u=.parallel.rP.sup.+.parallel.-.parallel.rP.pa- rallel.  and the trajectory between locations k and r (T.sub.k,r) may be inferred.  In the example of FIG. 7A, the pixel depth\nvariance .sigma..sub.d.sup.u may be computed in view of the following:\n .times..times..alpha..function..beta..function..beta..beta..times..times.- .function..sigma..times..gamma..pi..alpha..beta..times..times..times..beta- ..times..times..gamma.  ##EQU00001## where f (bolded) is a unit vector, f (unbolded) is a\nfocal length, and .sigma..sub.p is the pixel matching uncertainty.  The pixel matching uncertainty .sigma..sub.p may directly affect the pixel depth uncertainty .sigma..sub.d.sup.u.  As illustrated in the example of FIG. 7A, a smaller pixel matching\nuncertainty .sigma..sub.p may result in a more narrow pixel depth uncertainty .sigma..sub.d.sup.u and conversely, a larger pixel matching uncertainty .sigma..sub.p may result in a broader pixel depth uncertainty .sigma..sub.d.sup.u.  Accordingly,\nlocations for viewing or observing the point p may be selected such that the PDF is narrow, and in some cases, the most narrow.\n In some aspects, the determined pixel depth and variance information may be supplied as feedback to the object recognition and localization unit 602 to improve object localization.  For instance, the pixel depth and variance information may be\nused to reduce uncertainty with respect to and/or adjust the location of the bounding box enclosing the object of interest.\n FIG. 7B is an exemplary diagram illustrating motion-dependent depth variance in accordance with aspects of the present disclosure.  As shown in FIG. 7B, three images are taken of a point in region S. Region S has a surface divided into two\nareas.  The number of areas within the region is merely exemplary, for ease of illustration.  The present disclosure is not so limiting and any number of areas may be included in the region.\n The areas may comprise surfaces having different characteristics (e.g., color, texture, and/or topology).  In one example, the areas may have a different color (e.g., black carpet and white carpet).  In another example, the areas may have\ndifferent textures (e.g., grass and concrete).  As shown, in FIG. 7B, the motion of the camera from one position to the next may significantly affect the pixel depth variance.  Here, moving the camera from a location producing the image plane I.sub.r to\na location producing an image plane positioned at\n .theta..pi.  ##EQU00002## results in a smaller pixel depth variance than moving the camera to a location producing an image plane positioned at .theta.=0 (shown via more narrow PDF (.tau.)).  Notably, FIG. 7B illustrates that moving the camera\nin two different directions may result in two different pixel depth variances and thus, two different amounts of information depending on the available texture in the environment.\n Referring again to FIG. 6, the framework 600 may also include a planning graph unit 606.  The planning graph unit 606 may be used to construct an object map or reconstruction based on the depth map.  In some aspects, a 3D object map or 3D\nreconstruction of the 2D image may be generated.\n The planning graph unit 606 may also construct and/or update a motion planning graph.  The motion planning graph may be used to determine control inputs for controlling the agent to move about the object of interest to facilitate a 3D\nreconstruction.  The planning graph may be grown incrementally around the object of interest.  For example, points may be sampled in a given radius r around the current position of the camera.  Each of the sampling points, which may be referred to as\nnodes, may be connected to its k-nearest neighbors on the graph.  The connections may comprise one or more edges.  An edge is a motion primitive that may denote a short trajectory or a small segment of motion (e.g., a few centimeters) for the camera. \nThe edges may be concatenated to form the graph, which may be used for motion planning purposes.  In this way, a sampling based motion planning framework may be incrementally created.\n In some aspects, shape priors may also be used to aid the 3D reconstruction of the object of interest.  That is, if there is some knowledge of the shape of the object of interest, the prior knowledge may be used as a starting point for\nconstructing the planning graph.  For example, sampling and connection of points in a motion library may be determined based on the prior knowledge of the object's shape.  Similarly, the 3D reconstruction (e.g., object map) may also be determined based\non the prior knowledge of the object's shape.\n The motion planning unit 610 may determine a sequence of edges or connected nodes to form a potential plan for moving the camera and/or agent along a trajectory to positions from which to observe the object of interest and to facilitate a 3D\nreconstruction of the object.  In some aspects, multiple potential motion plans may be generated.  A potential motion plan may be selected based on a selection criteria.  For instance, a potential plan may be selected based on the distance to the desired\nobject (e.g., distance to grasp position of a teacup) or other metrics.\n In some aspects, a potential plan may be selected according to a reconstruction metric.  For example, the reconstruction may comprise an edge cost.  The edge cost may be defined as the cost of moving the camera and/or agent along a particular\nedge of a potential motion plan.  In one exemplary aspect, the edge cost or reconstruction reward may be determined based on the variance of pixel depth for each of the pixels in an image corresponding to the object of interest.\n In this exemplary aspect, the standard deviation of the depth estimate corresponding to a pixel u of a reference image may be given by .sigma..sub.k.sup.z at the k-th time step.  A filter may be used to estimate an unknown (e.g., depth).  In one\nexemplary aspect, the filter (e.g., Kalman filter) may filter along the edge to recursively compute the depth estimate.  Accordingly, the covariance may evolve as: P.sub.k+1.sup.-=AP.sub.k.sup.+A.sup.T-GQG.sup.T (7)\nP.sub.k+1.sup.+=P.sub.k+1.sup.--P.sub.k+1.sup.-H.sup.T(HP.sub.k+1.sup.-+H- .sup.T+R)HP.sub.k+1.sup.- (8) where P.sub.k+1.sup.- is the prediction, P.sub.k+1.sup.+ is the update of the variance at time step k+1, Q is the process noise, R is the measurement\nnoise, A is the Jacobian of system kinematics (e.g., obtained from linearization) and H is the Jacobian of the sensor model (e.g., obtained from linearization).  The filter output comprises a probability distribution of the mean and variance.\n The filtering equations (7) and (8) may be rewritten to define an information matrix given by: .OMEGA.k=(P.sub.k).sup.-1 (9)\n The information may be added up along an edge as: .OMEGA..sub.k+1.sup.+=.OMEGA..sub.k+1.sup.-+.OMEGA..sub.k+1.sup.z (10) where .OMEGA..sub.k+1.sup.z is information corresponding to a measurement z (e.g., pixel depth).  Because information\n(.OMEGA..sub.k) is inversely proportional to the variance, the smaller the variance the more information that is provided.  As such, each pixel of the object of interest may add to the information regarding the object of interest.  Furthermore, each\nobservation (e.g., image) via the camera may add to the information regarding the object of interest.\n Accordingly, the cost of the (i,j)th edge may be defined as the sum of information gains along the edge as expressed by:\n .times..di-elect cons..times..OMEGA.  ##EQU00003## where BB is the bounding box around the object in the reference frame and N is the length of the edge.  According to equation (11), the cost function may be focused to consider the information\nfor pixels along an edge that lies within the bounding box around the object of interest in the reference frame.\n Using the cost metric, it may be more desirable to select a motion path along an edge that produces the greater reward (e.g., the most information).  That is, by moving the camera along a trajectory that leads to increased information (and lower\npixel depth variance), more accurate 3D reconstructions of the 2D image of the object of interest may be achieved.  In addition, the 3D reconstructions may be performed in a more efficient manner.  As such, the approaches of the present disclosure may\nbeneficially reduce power consumption and improve processing efficiency.\n In some aspects, a weighted reward or cost may be used.  The weighted cost may be given by:\n .di-elect cons..times..times..times..times..OMEGA.  ##EQU00004## where w.sub.t.sup.z is a weight for the information of measurement z (e.g., pixel depth).  For example, in a grasping application, where the agent is tasked with grasping a cup,\nedges along the handle of the cup may be weighted less than edges along the bowl-shaped reservoir.\n In some aspects, the cost (reward) may vary in relation to the pixel depth variance.  Where the measurement is modeled as pixel depth, the weighted edge cost may be expressed as:\n .di-elect cons..times..times..times..sigma.  ##EQU00005## where .sigma..sub.d,t.sup.u is the pixel depth variance as a function of the distance between camera locations.\n In some aspects, a keyframe or reference frame may be fixed at each node of the planning graph.  Keyframes may also be fixed at each edge.  In this case, the keyframes may serve as or play the role of the reference frames for the edge extending\nout of (e.g., outgoing from) that keyframe's node.  In this case, when an edge is determined to be too long, the edge may be broken into two edges.  If the keyframes are limited to nodes, the image overlap may be considered when sampling nodes and\nconnecting edges.  For example, if the image overlap at the start and end of an edge is not sufficient for an accurate 3D reconstruction of the object, the edge may be discarded.  Alternatively, the edge may be broken again.  In some aspects, the graph\nnodes may be adjusted or updated based on the suitability of the keyframes (e.g., based on motion blur, percentage of available features).\n The information gain and reconstruction uncertainty along each edge may be determined and evaluated.  Using the cost function (e.g., C.sup.ij) as a planning metric, the planning graph may be searched to determine the best sequence of edges along\nwhich to move the camera.  The motion planning unit 610 may, in turn generate a control input, which may be executed by the execution unit 612 to move the agent and/or camera according to the determined sequence of edges.  In some aspects, the motion\nplanning unit 610 may generate a control input to move the agent and/or camera only along the first edge in the sequence of edges.  As the camera is moved along the trajectory of the edges, the procedure may be repeated.  For example, the depth map and\nobject map may be updated.  The planning graph and motion plan may also be updated.\n Referring again to FIG. 6, in some aspects, the framework 600 may also include an accuracy evaluation unit 608.  The accuracy evaluation unit 608 may evaluate the accuracy of the 3D reconstruction.  For example, given a ground truth for the\npixel depth, a reconstruction error may be determined.  In some aspects, the reconstruction error may be used to determine an updated motion plan for moving the camera and/or agent.\n The framework 600 may further include a planning graph unit 606 to construct and/or update a motion planning graph.  The graph may be grown incrementally around the object of interest.  For example, points may be sampled in a given radius r\naround the current position of the camera.  Each of the sampling points, which may be referred to as nodes, may be connected to it k-nearest neighbors on the graph.  The connections may comprise an edge or motion primitive.  A sequence of the connected\nnodes may form a potential plan for moving the camera or a trajectory to positions from which to observe the object of interest to facilitate a 3D reconstruction of the object.\n In one illustrative example, the camera may be provided with a manipulator (shown as element 720 in FIG. 7C).  The manipulator 720 comprises a set of joints (revolute or prismatic) and a camera (not shown), which may be positioned or coupled on\nor about the end effector 722.  In this configuration, an inverse kinematics model (IK) for the robotic manipulator may be computed to determine the joint parameters that provide a desired position of the end-effector.  That is, the inverse kinematics\nmay transform the motion plan into joint actuator trajectories for the robot (e.g., mapping 3D space (camera position) into joint angle space) as follows:\n .theta..theta..theta..function.  ##EQU00006##\n A library of motions (e.g., camera trajectories) may be generated by sampling points around the end-effector and connecting the points by open-loop trajectories (e.g., straight lines).  The corresponding control action (e.g., actuator commands)\nmay be computed by transforming the camera position to the joint space using inverse kinematics.  As the camera moves according to the computed control action, a planning graph may be grown to represent the manipulator's workspace around the object of\ninterest.\n In some aspects, multiple potential motion plans may be generated.  A potential motion plan may be selected based on a selection criteria.  For instance, a potential plan may be selected based on the distance to the desired object (e.g.,\ndistance to grasp position of a teacup) or other metrics.\n In some aspects, a potential plan may be selected according to a reconstruction metric.  A keyframe or reference frame may be at each node on the graph.  The information gain and reconstruction uncertainty along each edge may be determined and\nevaluated.\n FIG. 8 illustrates a method 800 for guiding a robot equipped with a camera to facilitate 3D reconstruction.  In some aspects, multiple cameras may be used to provide multi-view stereo vision.  Additionally, in some exemplary configurations, the\ncamera may be placed in an end of an extremity closest to the object.\n In block 802, the process recognizes and localizes an object in a 2D image (2D localizing).  In some aspects, the recognizing and localizing may be object focused.  In other aspects, the recognizing and localizing may be limited according to a\nbounding box around the object.  Furthermore, the 2D localizing may be based on deep learning techniques (e.g., the DCN 350 may learn features of image patches likely to include the object of interest).\n In block 804, the process computes 3D depth maps for the localized object.  The depth maps may be computed based on the depth of the pixel in each image of the object of interest.  In block 806, the process constructs a 3D object map from the\ndepth maps.\n In block 808, the process grows a sampling based structure around the 3D object map.  The sampling based structure may comprise edges or motion primitives that correspond to a short trajectory for the camera (and/or robot arm).  In block 810,\nthe process assigns a cost to each edge of the sampling based structure.  In block 812, the process searches the sampling based structure to determine a lowest cost sequence of edges (or sequence with the greatest reward).  Furthermore, in block 814, the\nprocess guides the robot based on the search.\n In some aspects, the process may optionally guide the robot based on texture information about the object, in block 816.  In one example, the texture information may comprise information regarding the terrain or topology of a region, which may\nbe used to determine a landing area for a drone.  In another example, the texture information may comprise information regarding the presence of a floor covering such as carpet.\n In some aspects, the process may optionally guide the robot based on importance weights assigned to different portions of the object, in block 818.  For example, where the object is to grasp a teacup, the handle may be assigned a greater weight\nthan that of the bowl/reservoir of the cup.\n In some aspects, the process may optionally guide the robot by incrementally creating a sampling based motion planning framework, in block 820.\n In some aspects, the process may optionally refine the object map from the depth maps, in block 822.  Additional depth maps may also be computed using further or additional images of the object.  the additional depth maps may in turn be used to\nfurther refine the object maps.\n In some aspects, the process may quantify obtained information about 3D structure for use as a cost in motion planning.\n The various operations of methods described above may be performed by any suitable means capable of performing the corresponding functions.  The means may include various hardware and/or software component(s) and/or module(s), including, but not\nlimited to, a circuit, an application specific integrated circuit (ASIC), or processor.  Generally, where there are operations illustrated in the figures, those operations may have corresponding counterpart means-plus-function components with similar\nnumbering.\n In some aspects, method 800 may be performed by the SOC 100 (FIG. 1) or the system 200 (FIG. 2).  That is, each of the elements of method 800 may, for example, but without limitation, be performed by the SOC 100 or the system 200 or one or more\nprocessors (e.g., CPU 102 and local processing unit 202) and/or other components included therein.\n As used herein, the term \"determining\" encompasses a wide variety of actions.  For example, \"determining\" may include calculating, computing, processing, deriving, investigating, looking up (e.g., looking up in a table, a database or another\ndata structure), ascertaining and the like.  Additionally, \"determining\" may include receiving (e.g., receiving information), accessing (e.g., accessing data in a memory) and the like.  Furthermore, \"determining\" may include resolving, selecting,\nchoosing, establishing and the like.\n As used herein, a phrase referring to \"at least one of\" a list of items refers to any combination of those items, including single members.  As an example, \"at least one of: a, b, or c\" is intended to cover: a, b, c, a-b, a-c, b-c, and a-b-c.\n The various illustrative logical blocks, modules and circuits described in connection with the present disclosure may be implemented or performed with a general-purpose processor, a digital signal processor (DSP), an application specific\nintegrated circuit (ASIC), a field programmable gate array signal (FPGA) or other programmable logic device (PLD), discrete gate or transistor logic, discrete hardware components or any combination thereof designed to perform the functions described\nherein.  A general-purpose processor may be a microprocessor, but in the alternative, the processor may be any commercially available processor, controller, microcontroller or state machine.  A processor may also be implemented as a combination of\ncomputing devices, e.g., a combination of a DSP and a microprocessor, a plurality of microprocessors, one or more microprocessors in conjunction with a DSP core, or any other such configuration.\n The steps of a method or algorithm described in connection with the present disclosure may be embodied directly in hardware, in a software module executed by a processor, or in a combination of the two.  A software module may reside in any form\nof storage medium that is known in the art.  Some examples of storage media that may be used include random access memory (RAM), read only memory (ROM), flash memory, erasable programmable read-only memory (EPROM), electrically erasable programmable\nread-only memory (EEPROM), registers, a hard disk, a removable disk, a CD-ROM and so forth.  A software module may comprise a single instruction, or many instructions, and may be distributed over several different code segments, among different programs,\nand across multiple storage media.  A storage medium may be coupled to a processor such that the processor can read information from, and write information to, the storage medium.  In the alternative, the storage medium may be integral to the processor.\n The methods disclosed herein comprise one or more steps or actions for achieving the described method.  The method steps and/or actions may be interchanged with one another without departing from the scope of the claims.  In other words, unless\na specific order of steps or actions is specified, the order and/or use of specific steps and/or actions may be modified without departing from the scope of the claims.\n The functions described may be implemented in hardware, software, firmware, or any combination thereof.  If implemented in hardware, an example hardware configuration may comprise a processing system in a device.  The processing system may be\nimplemented with a bus architecture.  The bus may include any number of interconnecting buses and bridges depending on the specific application of the processing system and the overall design constraints.  The bus may link together various circuits\nincluding a processor, machine-readable media, and a bus interface.  The bus interface may be used to connect a network adapter, among other things, to the processing system via the bus.  The network adapter may be used to implement signal processing\nfunctions.  For certain aspects, a user interface (e.g., keypad, display, mouse, joystick, etc.) may also be connected to the bus.  The bus may also link various other circuits such as timing sources, peripherals, voltage regulators, power management\ncircuits, and the like, which are well known in the art, and therefore, will not be described any further.\n The processor may be responsible for managing the bus and general processing, including the execution of software stored on the machine-readable media.  The processor may be implemented with one or more general-purpose and/or special-purpose\nprocessors.  Examples include microprocessors, microcontrollers, DSP processors, and other circuitry that can execute software.  Software shall be construed broadly to mean instructions, data, or any combination thereof, whether referred to as software,\nfirmware, middleware, microcode, hardware description language, or otherwise.  Machine-readable media may include, by way of example, random access memory (RAM), flash memory, read only memory (ROM), programmable read-only memory (PROM), erasable\nprogrammable read-only memory (EPROM), electrically erasable programmable Read-only memory (EEPROM), registers, magnetic disks, optical disks, hard drives, or any other suitable storage medium, or any combination thereof.  The machine-readable media may\nbe embodied in a computer-program product.  The computer-program product may comprise packaging materials.\n In a hardware implementation, the machine-readable media may be part of the processing system separate from the processor.  However, as those skilled in the art will readily appreciate, the machine-readable media, or any portion thereof, may be\nexternal to the processing system.  By way of example, the machine-readable media may include a transmission line, a carrier wave modulated by data, and/or a computer product separate from the device, all which may be accessed by the processor through\nthe bus interface.  Alternatively, or in addition, the machine-readable media, or any portion thereof, may be integrated into the processor, such as the case may be with cache and/or general register files.  Although the various components discussed may\nbe described as having a specific location, such as a local component, they may also be configured in various ways, such as certain components being configured as part of a distributed computing system.\n The processing system may be configured as a general-purpose processing system with one or more microprocessors providing the processor functionality and external memory providing at least a portion of the machine-readable media, all linked\ntogether with other supporting circuitry through an external bus architecture.  Alternatively, the processing system may comprise one or more neuromorphic processors for implementing the neuron models and models of neural systems described herein.  As\nanother alternative, the processing system may be implemented with an application specific integrated circuit (ASIC) with the processor, the bus interface, the user interface, supporting circuitry, and at least a portion of the machine-readable media\nintegrated into a single chip, or with one or more field programmable gate arrays (FPGAs), programmable logic devices (PLDs), controllers, state machines, gated logic, discrete hardware components, or any other suitable circuitry, or any combination of\ncircuits that can perform the various functionality described throughout this disclosure.  Those skilled in the art will recognize how best to implement the described functionality for the processing system depending on the particular application and the\noverall design constraints imposed on the overall system.\n The machine-readable media may comprise a number of software modules.  The software modules include instructions that, when executed by the processor, cause the processing system to perform various functions.  The software modules may include a\ntransmission module and a receiving module.  Each software module may reside in a single storage device or be distributed across multiple storage devices.  By way of example, a software module may be loaded into RAM from a hard drive when a triggering\nevent occurs.  During execution of the software module, the processor may load some of the instructions into cache to increase access speed.  One or more cache lines may then be loaded into a general register file for execution by the processor.  When\nreferring to the functionality of a software module below, it will be understood that such functionality is implemented by the processor when executing instructions from that software module.  Furthermore, it should be appreciated that aspects of the\npresent disclosure result in improvements to the functioning of the processor, computer, machine, or other system implementing such aspects.\n If implemented in software, the functions may be stored or transmitted over as one or more instructions or code on a computer-readable medium.  Computer-readable media include both computer storage media and communication media including any\nmedium that facilitates transfer of a computer program from one place to another.  A storage medium may be any available medium that can be accessed by a computer.  By way of example, and not limitation, such computer-readable media can comprise RAM,\nROM, EEPROM, CD-ROM or other optical disk storage, magnetic disk storage or other magnetic storage devices, or any other medium that can be used to carry or store desired program code in the form of instructions or data structures and that can be\naccessed by a computer.  Additionally, any connection is properly termed a computer-readable medium.  For example, if the software is transmitted from a website, server, or other remote source using a coaxial cable, fiber optic cable, twisted pair,\ndigital subscriber line (DSL), or wireless technologies such as infrared (IR), radio, and microwave, then the coaxial cable, fiber optic cable, twisted pair, DSL, or wireless technologies such as infrared, radio, and microwave are included in the\ndefinition of medium.  Disk and disc, as used herein, include compact disc (CD), laser disc, optical disc, digital versatile disc (DVD), floppy disk, and Blu-ray.RTM.  disc where disks usually reproduce data magnetically, while discs reproduce data\noptically with lasers.  Thus, in some aspects computer-readable media may comprise non-transitory computer-readable media (e.g., tangible media).  In addition, for other aspects computer-readable media may comprise transitory computer-readable media\n(e.g., a signal).  Combinations of the above should also be included within the scope of computer-readable media.\n Thus, certain aspects may comprise a computer program product for performing the operations presented herein.  For example, such a computer program product may comprise a computer-readable medium having instructions stored (and/or encoded)\nthereon, the instructions being executable by one or more processors to perform the operations described herein.  For certain aspects, the computer program product may include packaging material.\n Further, it should be appreciated that modules and/or other appropriate means for performing the methods and techniques described herein can be downloaded and/or otherwise obtained by a user terminal and/or base station as applicable.  For\nexample, such a device can be coupled to a server to facilitate the transfer of means for performing the methods described herein.  Alternatively, various methods described herein can be provided via storage means (e.g., RAM, ROM, a physical storage\nmedium such as a compact disc (CD) or floppy disk, etc.), such that a user terminal and/or base station can obtain the various methods upon coupling or providing the storage means to the device.  Moreover, any other suitable technique for providing the\nmethods and techniques described herein to a device can be utilized.\n It is to be understood that the claims are not limited to the precise configuration and components illustrated above.  Various modifications, changes and variations may be made in the arrangement, operation and details of the methods and\napparatus described above without departing from the scope of the claims.", "application_number": "15192857", "abstract": " A method for guiding a robot equipped with a camera to facilitate\n     three-dimensional (3D) reconstruction through sampling based planning\n     includes recognizing and localizing an object in a two-dimensional (2D)\n     image. The method also includes computing 3D depth maps for the localized\n     object. A 3D object map is constructed from the depth maps. A sampling\n     based structure is grown around the 3D object map and a cost is assigned\n     to each edge of the sampling based structure. The sampling based\n     structure may be searched to determine a lowest cost sequence of edges\n     that may, in turn be used to guide the robot.\n", "citations": ["8711206", "9102055", "9880553", "10003787", "20040073337", "20090290758", "20100315412", "20120256906", "20120287247", "20130016098", "20130141433", "20130325244", "20140118494", "20140146045", "20150091899", "20150294473", "20160005213", "20160232706", "20170004406", "20170157769", "20170160747", "20170161946", "20170165835", "20170168488", "20170193830", "20180012370", "20180074505", "20180161986", "20180189565", "20180217614", "20180247451", "20180302614", "20180322646"], "related": ["62286032"]}, {"id": "20170243352", "patent_code": "10373380", "patent_name": "3-dimensional scene analysis for augmented reality operations", "year": "2019", "inventor_and_country_data": " Inventors: \nKutliroff; Gershom (Alon Shvut, IL), Yanai; Yaron (Modi'in, IL), Fleishman; Shahar (Hod Hasharon, IL), Kliger; Mark (Modi'in, IL)  ", "description": "BACKGROUND\n Augmented reality (AR) applications attempt to blend virtual objects into real world images in a manner that appears seamless to the user in order to create a compelling AR experience.  Existing AR systems typically rely on a combination of\nstandard camera pose calculations and object recognition techniques that tend to be insufficient on their own to provide a convincing fusion of the real and the virtual.  Furthermore, existing techniques that merge homogeneous regions together often have\ndifficulty in joining heterogeneous regions of the same object into a single segment.  As a result, virtual objects rendered into the camera's video stream can appear at odd, unnatural angles, and contain visible artifacts.  The overall effect is\ngenerally awkward and displeasing to the user. BRIEF DESCRIPTION OF THE DRAWINGS\n The patent or application file contains at least one drawing executed in color.  Copies of this patent or patent application publication with color drawing(s) will be provided by the Office upon request and payment of the necessary fee.\n Features and advantages of embodiments of the claimed subject matter will become apparent as the following Detailed Description proceeds, and upon reference to the Drawings, wherein like numerals depict like parts.\n FIG. 1 is a top level block diagram of a system for 3-Dimensional (3D) scene analysis, in accordance with certain of the embodiments disclosed herein.\n FIG. 2 illustrates an example color image and depth map, in accordance with certain of the embodiments disclosed herein.\n FIG. 3 is a more detailed block diagram of a 3D reconstruction circuit, configured in accordance with certain of the embodiments disclosed herein.\n FIG. 4 illustrates an example of a 3D scene reconstruction, in accordance with certain of the embodiments disclosed herein.\n FIG. 5 is a more detailed block diagram of an object detection circuit, configured in accordance with certain of the embodiments disclosed herein.\n FIG. 6 illustrates an example of detected objects in a 3D image, in accordance with certain of the embodiments disclosed herein.\n FIG. 7 is a more detailed block diagram of a 3D segmentation circuit, configured in accordance with certain of the embodiments disclosed herein.\n FIG. 8 illustrates an example of segmented objects in a 3D image, in accordance with certain of the embodiments disclosed herein.\n FIG. 9 is a more detailed block diagram of a 3D registration circuit, configured in accordance with certain of the embodiments disclosed herein.\n FIG. 10 illustrates an example of a 3D registration of an object, in accordance with certain of the embodiments disclosed herein.\n FIG. 11 is a more detailed block diagram of an augmented reality manipulation circuit, configured in accordance with certain of the embodiments disclosed herein.\n FIG. 12 illustrates an example of a generated blueprint of a scene, in accordance with certain of the embodiments disclosed herein.\n FIG. 13 is a flowchart illustrating a methodology for 3D scene analysis, in accordance with certain of the embodiments disclosed herein.\n FIG. 14 is another detailed block diagram of a 3D segmentation circuit, configured in accordance with certain of the embodiments disclosed herein.\n FIG. 15 illustrates an example position and ray associated with pixels in an object boundary set, in accordance with certain of the embodiments disclosed herein.\n FIG. 16 is a flowchart illustrating a methodology for boundary adjustment, in accordance with certain of the embodiments disclosed herein.\n FIGS. 17A, 17B, 17C and 17D illustrate examples of rays used for boundary adjustments, in accordance with certain of the embodiments disclosed herein.\n FIG. 18 is a flowchart illustrating a methodology for surface plane removal, in accordance with certain of the embodiments disclosed herein.\n FIG. 19 is a block diagram schematically illustrating a system platform for 3D scene analysis, configured in accordance with certain of the embodiments disclosed herein.\n Although the following Detailed Description will proceed with reference being made to illustrative embodiments, many alternatives, modifications, and variations thereof will be apparent to those skilled in the art.\nDETAILED DESCRIPTION\n Generally, this disclosure provides techniques for 3-Dimensional (3D) scene analysis, the results of which may facilitate augmented reality (AR) operations to be performed on the scene.  In a more general sense, the techniques can be used to\nmodify an image of a scene by adding one or more objects to that scene, so as to provide an augmented image.  In accordance with an embodiment, the 3D scene analysis techniques generate data that describes a real-world scene as imaged by a depth camera\nfrom various perspectives.  In particular, the objects in the scene are detected and recognized, their respective orientations and 3D poses are calculated, and each object is segmented and mapped to an associated group of points in a 3D reconstruction of\nthe scene.  Additionally, the objects may be registered or aligned to preexisting 3D models of the objects, as generated for example by computer aided design (CAD) systems or scanning techniques.  The analysis results provide the capability to perform a\nnumber of manipulations of the scene in a more realistic manner as compared to conventional AR systems.  For example, selected objects may be deleted from the scene and new virtual objects can be rendered into the scene, as additions or replacements, in\na relatively realistic manner.  In some embodiments, the new virtual objects may be selected by a user from a catalog.  Similarly, the understanding of the scene provided by the analysis can serve as a basis for insertion of annotations into the scene,\nsuch as, for example, object size measurements, distance measurements, user assistance prompts, etc. Additionally, blueprints of the scene may be generated.\n A typical real-world scene may include a complex distribution of various objects, some of which may occlude others, as well as structural elements such as the walls, floor and ceiling.  Placing virtual objects convincingly into the scene, and\naccurately segmenting out real objects requires knowledge of the positions of the real objects relative to each other, and identification of the groups of pixels associated with the objects.  The analysis techniques described herein provide such an\nunderstanding of the scene including its composition, the objects it contains, and the relationships between them.  As will be appreciated in light of this disclosure, the scene analysis and image modification techniques provided herein can be\nimplemented in hardware or software or a combination thereof, and may be adapted into any number of applications where scene analysis and AR operations are desired.\n FIG. 1 is a top level block diagram 100 of a system for 3D scene analysis and AR manipulation, in accordance with certain of the embodiments disclosed herein.  A scene analysis system 130 is shown to include a 3D reconstruction circuit 132, an\nobject detection circuit 134, a 3D segmentation circuit 136 and a 3D registration circuit 138.  In some embodiments, the scene analysis system 130, along with an AR manipulation circuit 140, may be integrated into a mobile platform 102 along with a depth\ncamera 110 and a display element 112.  The mobile platform 102 may be a tablet or smartphone or other such similar device.  The depth camera may be configured as a rear mounted camera on the platform to facilitate scanning of the scene 120 as the user\nmoves with respect to the scene to capture 3D image frames from multiple perspectives or camera poses.  In some embodiments, the depth camera may be configured to capture a stream of images at a relatively high frame rate, for example 30 frames per\nsecond or more.  Each 3D image frame may comprise a color image frame that provides color (e.g., red, green and blue or RGB) pixels, and a depth map frame that provides depth pixels.  The scene 120 may include one or more objects of interest, some of\nwhich may be positioned or otherwise resting on surface planes, such as tables, shelves or the floor.\n At a top level, the 3D reconstruction circuit 132 may be configured to generate a 3D reconstruction of the scene comprising points in 3D space corresponding to structures within the scene.  The object detection circuit 134 may be configured to\ndetect, recognize and locate objects of interest within the scene.  The 3D segmentation circuit 136 may be configured to generate an estimate of 3D boundaries of objects of interest in the scene based on 3D images obtained from a number of poses of a\ndepth camera.  The estimated boundaries may be expressed as a set of 3D pixels associated with the boundary.  The 3D registration circuit 138 may be configured to operate on each of the detected objects in the scene, along with the associated segmented\nregion, to obtain a 3D alignment of the object in the scene.  The AR manipulation circuit 140 may be configured to allow the user to delete, insert and/or replace objects within the scene based on information obtained from the scene analysis system 130,\nas will be explained in greater detail below.\n FIG. 2 illustrates an example color image and depth map, in accordance with certain of the embodiments disclosed herein.  An RGB image frame 202 is shown alongside a corresponding depth map frame 204 of a scene of a furnished room provided by a\ndepth camera.  The scene includes a variety of objects such as a table, lamp, sofa, etc. The RGB frame 202 captures color data represented by RGB pixels of the image, although other color schemes are possible.  In the depth map frame 204, each pixel\nrepresents the distance between that region of the scene and the camera.  This can be done through the intensity value of each pixel.  In the depth map frame 204, for example, the pixel indicates proximity to the camera.  In particular, regions with\ndarker (more intense) pixels 210 are relatively far from the camera, while lighter (less intense) pixels 212 are closer.  In addition, the operational parameters of the camera, such as focal length and principal axis, are known or provided so that each\ndepth pixel can be projected into a 3D position in the scene relative to the camera.  Furthermore the RGB and depth components of the camera may be calibrated, both spatially and temporally, to improve the accuracy of the RGB values and the depth values\nof each individual pixel.\n FIG. 3 is a more detailed block diagram of a 3D reconstruction circuit 132, configured in accordance with certain of the embodiments disclosed herein.  The reconstruction circuit is shown to include a camera pose calculation circuit 302, a depth\npixel accumulation circuit 306 and, in some embodiments, inertial sensors 304 such as, for example, a gyroscope and/or an accelerometer.  An example rendering 400 of a 3D reconstruction of the scene shown in FIG. 2 is illustrated in FIG. 4.  This 3D\nreconstruction is composed of a relatively large number of points in 3D space, corresponding to structures within the scene, and may be represented in one of several ways including, for example, a signed distance function in a volumetric structure, or,\nequivalently, a polygonal mesh.  A 3D reconstruction of a scene generally includes at least two operations: calculation of the camera pose, and accumulation of the depth pixels into a global coordinate system.\n As new RGB and depth frames of the scene 120 are captured by depth camera 110 they are provided to the camera pose calculation circuit 302.  The camera pose calculation circuit 302 may be configured to compute and update the position and\norientation (or pose) of the camera.  The calculated pose of the camera is the 3D transformation from the position and orientation of the camera in a previous frame, to its position and orientation in the current frame.  Three parameters may describe the\ntranslation of the camera between consecutive frames (e.g., x, y and z).  Three additional parameters may describe the change in orientation (e.g., yaw, pitch and roll angle) for a total of six degrees of freedom (6DOF) that are computed to determine the\nupdated pose of the camera relative to its pose in the previous frame.  Determination of the camera pose for each frame can be used to establish a global coordinate system, consistent across all captured frames, in which the 3D points extracted from the\ndepth map can be projected and accumulated by depth pixel accumulation circuit 306.  The calculation of the camera pose may be performed in real-time.\n In some embodiments, the camera pose may be calculated using an RGB-based Simultaneous Localization and Mapping (SLAM) algorithm which is configured to extract feature descriptors from each RGB frame, match corresponding features across multiple\nframes and calculate the 6DOF camera pose for each frame through triangulation.  Alternatively, data from inertial sensors 304, such as gyroscopes and accelerometers, may be used, either independently, or in combination with the results of the RGB SLAM\ntechnique to obtain a more robust estimate of the camera pose.\n In some embodiments, the calculation of the camera pose may be based on the depth maps.  An Iterative Closest Point algorithm (ICP) can be applied to successive depth maps captured by the camera, to align two sets of point clouds and compute the\ntransformation between them.  Assuming the two point cloud sets represent static scenes, the computed transformation describes the movement of the camera that captured the two point cloud sets.  By computing the camera transformation describing\nsuccessive point cloud frames, the global camera pose for each frame can be computed, and the associated point clouds can be accumulated (by depth pixel accumulation circuit 306) into a single data structure representing the 3D reconstruction of the\nentire scene.  Moreover, each point in the scene is sampled multiple times, and consequently, a weighted average of their 3D positions can be computed, generating a higher quality 3D reconstruction of the scene.  Furthermore, the segmentation techniques\ndescribed below, which rely on depth data, may operate directly on the depth maps provided by the camera or on the 3D reconstruction based on aligning successive depth frames and averaging the 3D positions of the accumulated points.  In some embodiments,\nthe camera pose may be estimated using other known techniques in light of the present disclosure.\n The camera pose calculation circuit 302 determines the 3D position of the camera at each frame, in a global coordinate system.  Consequently, 3D points extracted from the associated depth maps can also be transformed or projected to this\ncoordinate system.  Thus, computation of the camera pose for each frame allows for integration of the depth maps obtained at different times into a single 3D space.  Each camera frame also includes an RGB image, which may similarly be associated with the\n6DOF camera pose\n FIG. 5 is a more detailed block diagram of an object detection circuit 134, configured in accordance with certain of the embodiments disclosed herein.  The object detection circuit is shown to include an object detection/recognition circuit 504\nand an object location circuit 508.\n The object detection/recognition circuit 504 may be configured to process the RGB image, and in some embodiments the associated depth map as well, along with the 3D reconstruction, to generate a list of any objects of interest recognized in the\nimage.  The object location circuit 508 may be configured to determine an associated location of each object in the scene.  Any suitable object detection technique may be used in to recognize the objects in the scene, and compute their locations in the\nimage including, for example, template matching 504 or classification using a bag-of-words vision model.  In some embodiments, deep learning methods, and, in particular, convolutional neural networks 506 are employed by the detection circuit 502.  Some\nneural network methods process an image as input and calculate a probability that a given object is present in the image.  The 3D Reconstruction or the depth maps may also be used as additional channels in a deep convolutional network for the purpose of\nthe object recognition and detection.  Determination of the location of the object in the image may be accomplished using sliding windows 510 that can be applied progressively over the image, cropping smaller regions of the image and applying the network\nto each window.  Other techniques for object location first filter out and reject those windows that are unlikely to contain objects.  Importantly, these methods are generally invariant to viewing angles so that the same object, and its corresponding 2D\nlocation, can be detected from multiple camera poses\n FIG. 6 illustrates an example of detected objects in a 3D image, in accordance with certain of the embodiments disclosed herein.  The detected and recognized objects in an RGB image of the scene, associated with one camera pose, are shown\nincluding for example the lamp 610.\n FIG. 7 is a more detailed block diagram of a 3D segmentation circuit 136, configured in accordance with certain of the embodiments disclosed herein.  The segmentation circuit is shown to include a plane fitting circuit 702, a floor/wall/ceiling\ndetermination circuit 704, a floor/wall/ceiling removal circuit 706, and a connected components clustering circuit 708.  The 3D segmentation circuit may be configured to segment out each of the objects detected, finding the points of the 3D\nreconstruction that correspond to the contours of the objects.\n The plane fitting circuit 702 may be configured to scan the 3D reconstruction of the scene for planar surfaces.  One method to accomplish this is to calculate normal vectors to the surfaces by scanning the depth maps and calculating the cross\nproduct of differences of neighboring depth pixels.  The normal vectors are then clustered into groups based on spatial proximity and the values of the vectors.  Next, a plane is fitted to each cluster.  Specifically, the equation for the plane, in an x,\ny, z coordinate system, may be expressed as: ax+by+cz+d=0 where the constants a, b, c, d which define the plane may be calculated by a least-squares fit or other known techniques in light of the present disclosure.\n Floor/wall/ceiling determination circuit 704 may be configured to identify floors, walls and ceilings from the list of planes detected in the scene.  In some embodiments, a gyroscope inertial sensor 304 is employed to detect the direction of\ngravity during scanning, and this data is stored during the scanning process.  The direction of gravity may then be used to find the plane corresponding to the floor.  Next, planar surfaces that are perpendicular to the floor plane (within a margin of\nerror) and cover a sufficiently large region of the scene are identified as walls.  Planar surfaces that are perpendicular to the walls and/or parallel to the floor and cover a sufficiently large region of the scene are identified as the ceiling. \nFloor/wall/ceiling removal circuit 706 may be configured to remove points that are located within a relatively small distance from one of these planar surfaces from the scene.  Connected components clustering circuit 708 may be configured to generate\nclusters of pixels that are not connected to other regions and classify them as individual, segmented objects of the scene.\n FIG. 8 illustrates an example of segmented objects in a 3D image, in accordance with certain of the embodiments disclosed herein.  Here, the same furnished room is shown as in FIG. 2, but the detected objects (e.g., lamp 810, table 820, sofa\n830, etc.) have been marked with boundaries (in blue) to represent the result of the segmentation process described herein.  More specifically, the segmentation process generates, for each object of interest, a collection of 3D points, referred to as an\nobject boundary set, representing the 3D boundary or contour of the object of interest.\n FIG. 9 is a more detailed block diagram of a 3D registration circuit 138, configured in accordance with certain of the embodiments disclosed herein.  The registration circuit is shown to include a feature detection circuit 902, a feature\nmatching circuit 904, a Random Sample Consensus (RANSAC) circuit 906 and an Iterative Closest Point (ICP) matching circuit 908.  The 3D registration circuit 138 may be configured to operate on each of the detected objects in the scene, along with the\nassociated segmented region, to obtain a 3D alignment of the object in the scene.  In particular, a 3D transformation is computed to align the detected object to a source object, such as, for example a CAD model of the object.  In some embodiments, the\nsource object may be created by scanning the object with a depth camera and applying 3D scanning software to produce a 3D reconstruction of the object.\n Feature detection circuit 902 may be configured to detect features in both the detected/segmented objects and the source objects (models).  These features may include, for example, 3D corners or any other suitable distinctive features of the\nobject.  In some embodiments, the RGB image frames are stored and mapped to the 3D reconstruction, enabling the use of 2D feature detection techniques such as Scale Invariant Feature Transform (SIFT) detection and Speeded-Up Robust Feature (SURF)\ndetection.  Feature matching circuit 904 may be configured to match a subset of at least 3 pairs of features from the detected and source objects to each other.  These 3 matches are used to generate a 3D transformation that aligns the detected and source\nobjects.  Because some of the matches may be incorrect, and the object data set may be noisy and/or missing some points, the RANSAC circuit 906 may be configured to iteratively improve the 3D transformation alignment.  An approximate 3D transformation,\ngenerated by the RANSAC circuit 906 is applied to the source object, and the Iterative Closest Point (ICP) circuit 908 may be configured to further improve or refine the computation of the 3D transformation.\n FIG. 10 illustrates an example 1000 of a 3D registration of an object, in accordance with certain of the embodiments disclosed herein.  A model of a coffee table 1010 is shown at a point in progress during alignment with the coffee table 1010 of\nthe scene in FIG. 2.\n FIG. 11 is a more detailed block diagram of an augmented reality (AR) manipulation circuit 140, configured in accordance with certain of the embodiments disclosed herein.  AR manipulation circuit 140 is shown to include an object deletion\ncircuit 1102, an object insertion circuit 1104 and a blueprint generation circuit 1106, the operations of which will be explained in detail below.\n As the 3D Registration circuit 138 is applied to each pair of segmented cluster of 3D points and associated detected object, an improved analytical \"understanding\" of the scene becomes known.  This scene analysis includes knowledge of the 3D\norientation and position of each object in the scene, and knowledge of the mapping of the 3D points associated with each object to the appropriate source model.  Such understanding of the scene enables the implementation of various end-user applications. For example, in some embodiments, blueprint generation circuit 1106 may be configured to generate a blueprint 1210, as illustrated in FIG. 12, of the scene shown in FIG. 2, based on the scene analysis.  Additionally, an inventory 1220 of the objects\ndetected in the scene, as well as measurements of the dimensions of the rooms (and/or objects) 1230, may also be generated.\n In some embodiments, the AR manipulation circuit 140 may be configured to allow a user to select one or more objects of interest in the scene to be deleted or to be replaced by a virtual object.  For example, the user may be presented with a\nlist of the detected objects and allowed to make selections including deletion, replacement or insertion of new objects into the scene.  In some embodiments, the user may be given the option to select objects from a catalog of virtual objects for\ninsertion or replacement into the scene.  In the case of replacement, the user may be prompted to place the selected virtual object at the center of the 3D position which the deleted object previously occupied.  Alternatively, if the user selects a\nvirtual object to be inserted, manipulation controls may be provided to enable the user to place the virtual object in the scene at the desired location.  The process may be repeated as many times as the user desires.\n Object deletion circuit 1102 may be configured to delete user selected objects from the scene.  For each object to be deleted from the scene, a rendering engine generates a 2D mask 1110 based on the camera pose and the registration of the 3D\nmodel of the object to be deleted.  The 2D mask is transparent, except for the pixels corresponding to the object of interest.  The values of these pixels may be sampled from the surrounding texture, or an in-filling algorithm may be used to assign\nvalues to the pixels consistent with the surrounding pixels in the RGB image of the scene.  The mask is overlaid (pixel replacement 1112) onto the RGB image captured at the current image frame of the camera and displayed to the user, effectively deleting\nthe selected object from the displayed image.\n Object insertion circuit 1104 may be configured to insert virtual objects into the scene, super-imposed onto the user's view, in a relatively seamless manner.  A rendering engine 114 takes as input the calculated camera pose of the current\nscene, a 3D model of the virtual object to be inserted, a lighting model, the plane of the floor, and the camera operational parameters (e.g., focal length, principal axis) and renders a 2D RGB image of the virtual object to be super-imposed onto the RGB\nimage of the current frame and displayed to the user.  In some embodiments, the depth map may also be used to further augment the scene, by considering occlusions between objects, or by rendering shadows, color bleed and reflections.\n The overall effect, as presented to the user, for example on display element 112, is that of the selected deleted object(s) being removed from the scene and the selected virtual object(s) being inserted into the scene.  The described process may\nbe repeated for each frame generated by the depth camera, so that as the user moves the depth camera, which may be integrated in a tablet, around the scene, the camera pose and the masks generated for each frame are continuously recomputed and\nsuper-imposed on the current frame from the camera, in order to maintain the realistic effect.  In a similar manner, walls in the scene can be treated as objects, to be deleted and replaced by a wall at a different location in the scene (e.g., further\naway), or of a different color.  Additionally, in some embodiments, virtual characters, for example as in a game, can be rendered and inserted into each frame, in real-time.  Furthermore, text and graphic displays may also be overlaid onto the scene, for\nexample, to guide the user, measure object sizes, etc.\n In general, RGB and depth images may be captured in real-time from the camera and the scene analysis techniques described herein may be applied successively on each capture or on combinations of previous captures.  The system may be configured\nto execute on a tablet platform in real-time, so as to provide the user with live feedback on how well the system has been able to analyze or understand the scene.  This feedback may be provided, for example, through display element 112.  If there is\ninsufficient data, some components of the system may be unable to perform their tasks in an adequate manner.  For example, if a clear view of an object is unavailable, the object detection circuit 134 may fail to detect an object in the scene. \nSimilarly, if only partial views of an object have been captured by the camera, the 3D registration circuit 138 may be unable to align the source model to the detected object.  In some embodiments, therefore, a user assistance application may execute on\nthe platform to aid the user and the process, by indicating which objects have been successfully analyzed.  In this way, the user is directed to provide additional information to the system, by changing the view of the camera to provide missing data.\n In some embodiments, the scene may first be scanned and frame images stored for subsequent processing by the scene analysis system.  Alternatively, some operations of the scene analysis system may be performed in real-time and others as a\npost-process.  For example, the 3D reconstruction and the object detection may be performed in real-time, while the remaining 3D segmentation, and 3D registration may be performed as a post-processing operation.\n Methodology\n FIG. 13 is a flowchart illustrating an example method 1300 for 3D scene analysis and augmented reality operations, in accordance with an embodiment of the present disclosure.  As can be seen, example method 1000 includes a number of phases and\nsub-processes, the sequence of which may vary from one embodiment to another.  However, when considered in the aggregate, these phases and sub-processes form a process for 3D scene analysis and AR operations in accordance with certain of the embodiments\ndisclosed herein.  These embodiments can be implemented, for example using the system architecture illustrated in the Figures, as described herein.  However other system architectures can be used in other embodiments, as will be apparent in light of this\ndisclosure.  To this end, the correlation of the various functions shown in FIG. 13 to the specific components illustrated in the other Figures is not intended to imply any structural and/or use limitations.  Rather other embodiments may include, for\nexample, varying degrees of integration wherein multiple functionalities are effectively performed by one system.  For example, in an alternative embodiment a single module can be used to perform all of the functions of method 1300.  Thus other\nembodiments may have fewer or more modules and/or sub-modules depending on the granularity of implementation.  Numerous variations and alternative configurations will be apparent in light of this disclosure.\n As illustrated in FIG. 13, in one embodiment, method 1300 for 3D scene analysis and AR operations commences by receiving, at operation 1310, a series of 3D image frames, for example from a depth camera, of a scene containing one or more objects\nas the camera scans the scene.  Each frame may thus provide a new view of the scene from a different perspective or camera pose.  Each frame provided by the depth camera may include a color image frame comprising color (RGB) pixels and a depth map frame\ncomprising depth pixels.  Next, at operation 1320, the depth pixels are projected into points in a global coordinate system based on the camera pose, and at operation 1330, the projected points are accumulated into a 3D reconstruction of the scene.\n At operation 1340, objects within the scene are detected and located for each 3D image frame, based on the camera pose, the 3D reconstruction, the RGB image frame and the depth map frame.  At operation 1350, each of the detected objects in the\nscene is segmented.  The segments comprise the points of the 3D reconstruction corresponding to contours of the associated detected object.  At operation 1360, the segmented objects are registered to a 3D model of the object to determine an alignment of\nthe object in the scene.\n Of course, in some embodiments, additional operations may be performed, as previously described in connection with the system.  These additional operations may include, for example, deleting a selected object from the scene and replacing\nbackground pixels for the deleted object and/or adding a new object to the scene, wherein both augmented reality operations are based on the scene analysis.  Further additional operations may include, for example, generating a blueprint of the scene\nbased on the registered objects and the associated locations of the objects.\n Alternative Segmentation Methodology\n FIG. 14 is a detailed block diagram of another embodiment of 3D segmentation circuit 136.  The segmentation circuit 136 is shown to include the camera pose calculation circuit 302 and object detection circuit 134 as previously described, as well\nas an object boundary set matching circuit 1410 and an object boundary set creation circuit 1420.\n As new RGB and depth frames of the scene 120 are captured by depth camera 110, at operation 1402, they are provided to the camera pose calculation circuit 302.  The camera pose calculation circuit 302 may be configured to compute and update the\nposition and orientation (or pose) of the camera as previously described.\n Camera motion, as the scene is scanned, may be relatively slow such that there could be significant overlap between successive RGB images.  At operation 1406, if the change in camera pose does not exceed a threshold value, processing of the RGB\nframe may not be justified and the system will wait for a subsequent frame capture.  The threshold value may be set based on the processing capabilities of the system and/or the expected speed of the camera scanning motion.\n The object detection circuit 134 may be configured to process the RGB image, and in some embodiments the associated depth map as well, to generate a list of any objects of interest recognized in the image.  A label may be attached to each of the\nrecognized objects and a 2D bounding box is generated which contains the object.  Additionally, a 3D location of the center of the 2D bounding box is computed.  Referring back to FIG. 6 examples are illustrated of 2D bounding boxes applied to recognized\nobjects in an RGB image of the scene associated with one camera pose.  For example, the object recognized and labeled as a lamp 610 is contained within a boundary box 620.  Any suitable object detection technique may be used in to recognize the objects\nin the scene, and compute their locations in the image as previously described.\n The object boundary set matching circuit 1410 may be configured to find an appropriate existing object boundary set that matches each of the detected objects, if possible.  The matching is based on a comparison of the object label and/or the 3D\nlocation of the center of the 2D bounding box, between the detected object and each of the existing object boundary sets, if any.  A comparison threshold may be used to account for the fact that the estimated center of the 2D bounding boxes corresponding\nto the same object may vary somewhat when captured from different camera perspectives.\n The object boundary set creation circuit 1420 may be configured to create a new object boundary set if a suitable match for the detected object is not found by the object boundary set matching circuit 1410.  For each unmatched detected object of\ninterest, the 2D bounding box containing the object is scanned to analyze each pixel within the bounding box.  For each pixel, the associated 3D position of the 2D pixel is computed, by sampling the associated depth map to obtain the associated depth\npixel and projecting that depth pixel to a point in 3D space, at operation 1412.  A ray is then generated which extends from the camera to the location of the projected point in 3D space at operation 1414.  The point at this 3D position is included in\nthe object boundary set at operation 1416.  In order to represent this point in the object boundary set, two 3-element vectors are stored: the 3D (x,y,z) position of the point in the global coordinate system, and the vector representing the ray extending\nfrom the camera's position to that point (which is referred to herein as the \"camera ray\").  FIG. 15 illustrates this in a top-down view, showing the position of the camera 110 in a given pose along with the object 1510, which is contained in the 2D\nbounding box 1550.  Two example pixel points are also shown, 1530 and 1540, along with the ray 1520 that extends from the camera to the intersection with each pixel point on the boundary of the object.  After processing each pixel in the 2D bounding box,\nthe image capture continues, at a new camera pose, until the scan of the scene is complete, operation 1418.\n When the scanning of the scene is completed, there is a single object boundary set for each object detected in the scene.  Each object boundary set contains the aggregate of all points projected from pixels in the 2D bounding box of the object,\nas captured from multiple camera perspectives.  The object boundary set may then be further processed to remove multiple samples of the same point (as seen from different camera perspectives), and to remove incorrect points (not belonging to the object\nof interest) that were included in the bounding box due, for example to the presence of other objects occluding the view of the object of interest.\n FIG. 16 is a flowchart 1600 illustrating a methodology for boundary adjustment, in accordance with certain of the embodiments disclosed herein.  Boundary adjustment circuit 1600 may be configured to implement this methodology to improve the\nestimated boundary of the detected objects by removing, from each object boundary set, duplicate pixels and pixels associated with other objects occluding the camera's view of the object of interest.  At operation 1602, each point P in the object\nboundary set is considered, and a ray is projected, at operation 1604, from the point P in the direction of the camera ray that was previously stored for that point.  This ray is referred to herein as the \"point ray.\" Next, at operation 1606, all points\nin the object boundary set are analyzed with respect to the current point P, and any point lying close enough to the point ray (within a given threshold distance) is selected.  This set of selected points is referred to as \"set A,\" and the points in set\nA are considered to be lying on the point ray.\n For each of the points in set A, the associated camera ray is extracted and compared to the point ray of the current point P. If the camera ray is opposite in direction to that of the current point ray, then that point, associated with the\ncamera ray, is discarded from set A, at operation 1608.  To determine whether the camera ray and point ray are in opposite directions, a dot product of the two rays may be computed.  The value of this dot product will be negative if the directions of the\ntwo rays differ by more than 90 degrees.  Thus, if the dot product is negative, the point from set A was likely captured by the camera when viewing the opposite side of the object, and it is ignored for further consideration with respect to point P. In\nother words, the point discarded from set A is no longer a candidate for removal from the object boundary set since, being on the other side of the object it is not a duplicate of point P.\n After each point in set A has been similarly processed, to eliminate points on the opposite side of the object from consideration for removal, the remaining points of set A are assumed to have been captured from the same general view of the\nobject, and only one will be kept in the object boundary set.  To decide which point to keep, at operation 1610 all remaining points of set A are projected onto the point ray.  The point in set A that is furthest away from point P, in the direction of\nthe point ray, is selected, at operation 1612, and the rest of the points are discarded from the object boundary set.  Any point in set A with a negative position on the point ray (that is, lying in the direction opposite that of the point ray) is also\ndiscarded.  Note point P may also be discarded from the object boundary set in this stage.  By retaining the point which is furthest away from point P (along the direction of P's point ray), point associated with occluding objects will be discarded,\nsince, by definition, the occluding object will be closer to the camera than the object of interest.\n FIGS. 17A, 17B, 17C and 17D illustrate examples of rays used for boundary adjustments, in accordance with certain of the embodiments disclosed herein.  In FIG. 17A, the camera 110 is shown viewing the object of interest 1510 from a first\nperspective or pose described by camera ray 1730a.  An occluding object 1710 blocks the camera's view from this perspective and point number 1 may thus be included in the object boundary set.  In FIG. 17B, the camera's perspective has changed as the\nscene is scanned and a new camera ray 1730b avoids the occluding object 1710.  Point number 2 may thus be added to the object boundary set.  In FIG. 17C, the camera's perspective has changed again and a new camera ray 1730c provides a view of the\nopposite side of object 1510.  Point number 3 may thus be added to the object boundary set.  In FIG. 17D, the object boundary set adjustment is illustrated with respect to point 1 being chosen as point P, using the terminology established above, with a\ncorresponding point ray 1740.  Points 2 and 3 are selected form the object boundary set for inclusion in set A because they lie close enough to the point ray 1740.  Point 3 is eliminated from consideration for removal because its camera ray 1520b is in\nthe opposite direction from point ray 1740 (because it is on the opposite side of the object with respect to point P).  Point 2 is retained in the object boundary set because it is furthest away from point P (and is correctly associated with the object\nof interest rather than the occluding object).  Point 1 is thus removed from the object boundary set since point 2 was retained.\n FIG. 18 is a flowchart illustrating a methodology for surface plane removal, in accordance with certain of the embodiments disclosed herein.  Surface plane removal circuit 1800 may be configured to implement this methodology to improve the\nestimated boundary of the detected objects by removing surface planes upon which the objects may be positioned.  It may be common for objects of interest to be resting on a planar surface, such as the floor, table shelves, etc. As a result, it is likely\nthat points from the surface might be erroneously included in the boundary shape.  In some embodiments, therefore, a post-processing operation is implemented in which such surface planes are detected, and removed from the 3D segmentation results.\n At operation 1802, a plane fitting algorithm is applied to the entire scene, in order to detect the planes contained within the scene, as previously described in connection with plane fitting circuit 702.  Next, at operation 1804, the object\nboundary sets are scanned against the list of planar surfaces, to check for intersections.  For each object boundary set, the subset of intersecting points is computed.  The size of this intersection relative to the size of the entire planar surface is\nevaluated at operation 1806 and may be expressed as a ratio.  If the planar surface extends beyond the boundaries of the object, it is reasonable to assume that it represents a surface supporting the object of interest, and its pixels should be excluded\nfrom the object boundary set.  In some embodiments, a value of 90% may be used as a threshold for the ratio (operation 1808).  That is, if the intersection set contains less than 90% of the pixels that are contained by the planar surface, then all the\npixels of the planar surface are excluded from the object boundary set, at operation 1810.  Otherwise, the object boundary set is preserved as is, and the next detected object boundary set is evaluated.\nExample System\n FIG. 19 illustrates an example system 1900 that may be configured to segment objects from a 3D image of a scene, for example based on object recognition, as described herein.  In some embodiments, system 1900 comprises a platform 102 which may\nhost, or otherwise be incorporated into a personal computer, workstation, laptop computer, ultra-laptop computer, tablet, touchpad, portable computer, handheld computer, palmtop computer, personal digital assistant (PDA), cellular telephone, combination\ncellular telephone and PDA, smart device (for example, smartphone or smart tablet), mobile internet device (MID), and so forth.  Any combination of different devices may be used in certain embodiments.\n In some embodiments, platform 102 may comprise any combination of a processor 1920, a memory 1930, a scene analysis system 130, an AR manipulation circuit 140, a depth camera 110, a network interface 1940, an input/output (I/O) system 1950, a\ndisplay element 112, and a storage system 1970.  As can be further seen, a bus and/or interconnect 1992 is also provided to allow for communication between the various components listed above and/or other components not shown.  Platform 102 can be\ncoupled to a network 1994 through network interface 1940 to allow for communications with other computing devices, platforms or resources.  Other componentry and functionality not reflected in the block diagram of FIG. 19 will be apparent in light of\nthis disclosure, and it will be appreciated that other embodiments are not limited to any particular hardware configuration.\n Processor 1920 can be any suitable processor, and may include one or more coprocessors or controllers, such as an audio processor or a graphics processing unit, to assist in control and processing operations associated with system 1900.  In some\nembodiments, the processor 1920 may be implemented as any number of processor cores.  The processor (or processor cores) may be any type or combination of processor, such as, for example, a micro-processor, an embedded processor, a digital signal\nprocessor (DSP), a graphics processor (GPU), a network processor, a field programmable gate array or other device configured to execute code.  The processors may be multithreaded cores in that they may include more than one hardware thread context (or\n\"logical processor\") per core.  Processor 1920 may be implemented as a complex instruction set computer (CISC) or a reduced instruction set computer (RISC) processor.  In some embodiments, processor 1920 may be configured as an x86 instruction set\ncompatible processor.\n Memory 1930 can be implemented using any suitable type of digital storage including, for example, flash memory and/or random access memory (RAM).  In some embodiments, the memory 1930 may include various layers of memory hierarchy and/or memory\ncaches as are known to those of skill in the art.  Memory 1930 may be implemented as a volatile memory device such as, but not limited to, a RAM, dynamic RAM (DRAM), or static RAM (SRAM) device.  Storage system 1970 may be implemented as a non-volatile\nstorage device such as, but not limited to, one or more of a hard disk drive (HDD), a solid state drive (SSD), a universal serial bus (USB) drive, an optical disk drive, tape drive, an internal storage device, an attached storage device, flash memory,\nbattery backed-up synchronous DRAM (SDRAM), and/or a network accessible storage device.  In some embodiments, storage 1970 may comprise technology to increase the storage performance enhanced protection for valuable digital media when multiple hard\ndrives are included.\n Processor 1920 may be configured to execute an Operating System (OS) 1980 which may comprise any suitable operating system, such as Google Android (Google Inc., Mountain View, Calif.), Microsoft Windows (Microsoft Corp., Redmond, Wash.), Linux,\nor Apple OS X (Apple Inc., Cupertino, Calif.) and/or various real-time operating systems.  As will be appreciated in light of this disclosure, the techniques provided herein can be implemented without regard to the particular operating system provided in\nconjunction with system 1900, and therefore may also be implemented using any suitable existing or subsequently-developed platform.\n Network interface module 1940 can be any appropriate network chip or chipset which allows for wired and/or wireless connection between other components of computer system 1900 and/or network 1994, thereby enabling system 1900 to communicate with\nother local and/or remote computing systems, servers, and/or resources.  Wired communication may conform to existing (or yet to developed) standards, such as, for example, Ethernet.  Wireless communication may conform to existing (or yet to developed)\nstandards, such as, for example, cellular communications including LTE (Long Term Evolution), Wireless Fidelity (Wi-Fi), Bluetooth, and/or Near Field Communication (NFC).  Exemplary wireless networks include, but are not limited to, wireless local area\nnetworks, wireless personal area networks, wireless metropolitan area networks, cellular networks, and satellite networks.\n I/O system 1950 may be configured to interface between various I/O devices and other components of computer system 1900.  I/O devices may include, but not be limited to, a display element 112, depth camera 106, and other devices not shown such\nas a keyboard, mouse, speaker, microphone, etc.\n I/O system 1950 may include a graphics subsystem configured to perform processing of images for display element 112.  Graphics subsystem may be a graphics processing unit or a visual processing unit (VPU), for example.  An analog or digital\ninterface may be used to communicatively couple graphics subsystem and display element 112.  For example, the interface may be any of a high definition multimedia interface (HDMI), DisplayPort, wireless HDMI, and/or any other suitable interface using\nwireless high definition compliant techniques.  In some embodiment, the graphics subsystem could be integrated into processor 1920 or any chipset of platform 102.  In some embodiments, display element 112 may comprise any television type monitor or\ndisplay, including liquid crystal displays (LCDs) and light emitting diode displays (LEDs).  Display element 112 may comprise, for example, a computer display screen, touchscreen display, video monitor, television-like device, and/or a television. \nDisplay element 112 may be digital and/or analog.  Under the control of the OS 1980 (or one or more software applications), platform 102 may display processed images on display element 112.  The images may be provided by scene analysis system 130, AR\nmanipulation circuit 140, depth camera 106, or other sources.  Camera 106 may be configured to provide color (RGB) and depth images or scans of the scene from which a 3D image segmentation of the object may be generated.\n It will be appreciated that in some embodiments, the various components of the system 1900 may be combined or integrated in a system-on-a-chip (SoC) architecture.  In some embodiments, the components may be hardware components, firmware\ncomponents, software components or any suitable combination of hardware, firmware or software.\n Scene analysis system 130 is configured to provide 3D analysis of a scene including detection, segmentation and registration of objects within the scene.  The segmentation may employ object recognition techniques and may include scanning of the\nscene by a depth camera to collect 3D images from a number of perspectives or camera poses.  AR manipulation circuit 140 is configured to implement augmented reality operations including removal and insertion of objects and the generation of blueprints\nbased on the scene analysis.  Scene analysis system 130 and AR manipulation circuit 140 may include any or all of the components illustrated in the Figures and described above.  Scene analysis system 130 and AR manipulation circuit 140 can be implemented\nor otherwise used in conjunction with a variety of suitable software and/or hardware that is coupled to or that otherwise forms a part of system 1900.  Scene analysis system 130 and AR manipulation circuit 190 can additionally or alternatively be\nimplemented or otherwise used in conjunction with user I/O devices that are capable of providing information to, and receiving information and commands from, a user.  These I/O devices may include display element 112, a textual input device such as a\nkeyboard, and a pointer-based input device such as a mouse.  Other input/output devices that may be used in other embodiments include a touchscreen, a touchpad, a speaker, and/or a microphone.  Still other input/output devices can be used in other\nembodiments.\n In some embodiments, scene analysis system 130 and AR manipulation circuit 140 may be installed local to system 1900, as shown in the example embodiment of FIG. 19.  Alternatively, system 1900 can be implemented in a client-server arrangement\n(or local and cloud based arrangement) wherein at least some functionality associated with scene analysis system 130 and AR manipulation circuit 140 is provided to system 1900 using an applet, such as a JavaScript applet, or other downloadable module. \nSuch a remotely accessible module or sub-module can be provisioned in real-time in response to a request from a client computing system for access to a given server having resources that are of interest to the user of the client computing system.  In\nsuch embodiments the server can be local to network 1994 or remotely coupled to network 1994 by one or more other networks and/or communication channels.  In some cases access to resources on a given network or computing system may require credentials\nsuch as usernames, passwords, and/or compliance with any other suitable security mechanism.\n In various embodiments, system 1900 may be implemented as a wireless system, a wired system, or a combination of both.  When implemented as a wireless system, system 1900 may include components and interfaces suitable for communicating over a\nwireless shared media, such as one or more antennae, transmitters, receivers, transceivers, amplifiers, filters, control logic, and so forth.  An example of wireless shared media may include portions of a wireless spectrum, such as the radio frequency\nspectrum and so forth.  When implemented as a wired system, system 1900 may include components and interfaces suitable for communicating over wired communications media, such as input/output adapters, physical connectors to connect the input/output\nadaptor with a corresponding wired communications medium, a network interface card (NIC), disc controller, video controller, audio controller, and so forth.  Examples of wired communications media may include a wire, cable metal leads, printed circuit\nboard (PCB), backplane, switch fabric, semiconductor material, twisted pair wire, coaxial cable, fiber optics, and so forth.\n Various embodiments may be implemented using hardware elements, software elements, or a combination of both.  Examples of hardware elements may include processors, microprocessors, circuits, circuit elements (for example, transistors, resistors,\ncapacitors, inductors, and so forth), integrated circuits, ASICs, programmable logic devices, digital signal processors, FPGAs, logic gates, registers, semiconductor devices, chips, microchips, chipsets, and so forth.  Examples of software may include\nsoftware components, programs, applications, computer programs, application programs, system programs, machine programs, operating system software, middleware, firmware, software modules, routines, subroutines, functions, methods, procedures, software\ninterfaces, application program interfaces, instruction sets, computing code, computer code, code segments, computer code segments, words, values, symbols, or any combination thereof.  Determining whether an embodiment is implemented using hardware\nelements and/or software elements may vary in accordance with any number of factors, such as desired computational rate, power level, heat tolerances, processing cycle budget, input data rates, output data rates, memory resources, data bus speeds, and\nother design or performance constraints.\n Some embodiments may be described using the expression \"coupled\" and \"connected\" along with their derivatives.  These terms are not intended as synonyms for each other.  For example, some embodiments may be described using the terms \"connected\"\nand/or \"coupled\" to indicate that two or more elements are in direct physical or electrical contact with each other.  The term \"coupled,\" however, may also mean that two or more elements are not in direct contact with each other, but yet still cooperate\nor interact with each other.\n The various embodiments disclosed herein can be implemented in various forms of hardware, software, firmware, and/or special purpose processors.  For example in one embodiment at least one non-transitory computer readable storage medium has\ninstructions encoded thereon that, when executed by one or more processors, cause one or more of the methodologies for generating 3D object image variations, disclosed herein, to be implemented.  The instructions can be encoded using a suitable\nprogramming language, such as C, C++, object oriented C, JavaScript, Visual Basic .NET, Beginner's All-Purpose Symbolic Instruction Code (BASIC), or alternatively, using custom or proprietary instruction sets.  The instructions can be provided in the\nform of one or more computer software applications and/or applets that are tangibly embodied on a memory device, and that can be executed by a computer having any suitable architecture.  In one embodiment, the system can be hosted on a given website and\nimplemented, for example, using JavaScript or another suitable browser-based technology.  For instance, in certain embodiments, scene analysis system 130 and AR manipulation circuit 140 may operate by leveraging processing resources provided by a remote\ncomputer system accessible via network 1994.  In other embodiments the functionalities disclosed herein can be incorporated into other software applications, such as image management applications.  The computer software applications disclosed herein may\ninclude any number of different modules, sub-modules, or other components of distinct functionality, and can provide information to, or receive information from, still other components.  These modules can be used, for example, to communicate with input\nand/or output devices such as a display screen, a touch sensitive surface, a printer, and/or any other suitable device.  Other componentry and functionality not reflected in the illustrations will be apparent in light of this disclosure, and it will be\nappreciated that other embodiments are not limited to any particular hardware or software configuration.  Thus in other embodiments system 1900 may comprise additional, fewer, or alternative subcomponents as compared to those included in the example\nembodiment of FIG. 19.\n The aforementioned non-transitory computer readable medium may be any suitable medium for storing digital information, such as a hard drive, a server, a flash memory, and/or random access memory (RAM), or a combination of memories.  In\nalternative embodiments, the components and/or modules disclosed herein can be implemented with hardware, including gate level logic such as a field-programmable gate array (FPGA), or alternatively, a purpose-built semiconductor such as an\napplication-specific integrated circuit (ASIC).  Still other embodiments may be implemented with a microcontroller having a number of input/output ports for receiving and outputting data, and a number of embedded routines for carrying out the various\nfunctionalities disclosed herein.  It will be apparent that any suitable combination of hardware, software, and firmware can be used, and that other embodiments are not limited to any particular system architecture.\n Some embodiments may be implemented, for example, using a machine readable medium or article which may store an instruction or a set of instructions that, if executed by a machine, may cause the machine to perform a method and/or operations in\naccordance with the embodiments.  Such a machine may include, for example, any suitable processing platform, computing platform, computing device, processing device, computing system, processing system, computer, process, or the like, and may be\nimplemented using any suitable combination of hardware and/or software.  The machine readable medium or article may include, for example, any suitable type of memory unit, memory device, memory article, memory medium, storage device, storage article,\nstorage medium, and/or storage unit, such as memory, removable or non-removable media, erasable or non-erasable media, writeable or rewriteable media, digital or analog media, hard disk, floppy disk, compact disk read only memory (CD-ROM), compact disk\nrecordable (CD-R) memory, compact disk rewriteable (CR-RW) memory, optical disk, magnetic media, magneto-optical media, removable memory cards or disks, various types of digital versatile disk (DVD), a tape, a cassette, or the like.  The instructions may\ninclude any suitable type of code, such as source code, compiled code, interpreted code, executable code, static code, dynamic code, encrypted code, and the like, implemented using any suitable high level, low level, object oriented, visual, compiled,\nand/or interpreted programming language.\n Unless specifically stated otherwise, it may be appreciated that terms such as \"processing,\" \"computing,\" \"calculating,\" \"determining,\" or the like refer to the action and/or process of a computer or computing system, or similar electronic\ncomputing device, that manipulates and/or transforms data represented as physical quantities (for example, electronic) within the registers and/or memory units of the computer system into other data similarly represented as physical quantities within the\nregisters, memory units, or other such information storage transmission or displays of the computer system.  The embodiments are not limited in this context.\n The terms \"circuit\" or \"circuitry,\" as used in any embodiment herein, may comprise, for example, singly or in any combination, hardwired or purpose-built circuitry, programmable circuitry such as computer processors comprising one or more\nindividual instruction processing cores, state machine circuitry, and/or firmware that stores instructions executed by programmable circuitry.  The circuitry may include a processor and/or controller configured to execute one or more instructions to\nperform one or more operations described herein.  The instructions may be embodied as, for example, an application, software, firmware, etc. configured to cause the circuitry to perform any of the aforementioned operations.  Software may be embodied as a\nsoftware package, code, instructions, instruction sets and/or data recorded on a computer-readable storage device.  Software may be embodied or implemented to include any number of processes, and processes, in turn, may be embodied or implemented to\ninclude any number of threads, in a hierarchical fashion.  Firmware may be embodied as code, instructions or instruction sets and/or data that are hard-coded (e.g., nonvolatile) in memory devices.  The circuitry may, collectively or individually, be\nembodied as circuitry that forms part of a larger system, for example, an integrated circuit (IC), an application-specific integrated circuit (ASIC), a system on-chip (SoC), desktop computers, laptop computers, tablet computers, servers, smart phones,\netc. Other embodiments may be implemented as software executed by a programmable control device.  As described herein, various embodiments may be implemented using hardware elements, software elements, or any combination thereof.  Examples of hardware\nelements may include processors, microprocessors, circuits, circuit elements (e.g., transistors, resistors, capacitors, inductors, and so forth), integrated circuits, application specific integrated circuits (ASIC), programmable logic devices (PLD),\ndigital signal processors (DSP), field programmable gate array (FPGA), logic gates, registers, semiconductor device, chips, microchips, chip sets, and so forth.\n Numerous specific details have been set forth herein to provide a thorough understanding of the embodiments.  It will be understood by an ordinarily-skilled artisan, however, that the embodiments may be practiced without these specific details. \nIn other instances, well known operations, components and circuits have not been described in detail so as not to obscure the embodiments.  It can be appreciated that the specific structural and functional details disclosed herein may be representative\nand do not necessarily limit the scope of the embodiments.  In addition, although the subject matter has been described in language specific to structural features and/or methodological acts, it is to be understood that the subject matter defined in the\nappended claims is not necessarily limited to the specific features or acts described herein.  Rather, the specific features and acts described herein are disclosed as example forms of implementing the claims.\nFurther Example Embodiments\n The following examples pertain to further embodiments, from which numerous permutations and configurations will be apparent.\n Example 1 is a method for 3-Dimensional (3D) scene analysis.  The method comprises: receiving a plurality of 3D image frames of a scene, each frame comprising a red-green-blue (RGB) image frame comprising color pixels and a depth map frame\ncomprising depth pixels, wherein each of the 3D image frames is associated with a pose of a depth camera that generated the 3D image frames; projecting the depth pixels into points in a global coordinate system based on the camera pose; accumulating the\nprojected points into a 3D reconstruction of the scene; detecting objects and associated locations in the scene, for each 3D image frame, based on the camera pose, the 3D reconstruction, the RGB image frame and the depth map frame; segmenting, each of\nthe detected objects in the scene, the segmented objects comprising the points of the 3D reconstruction corresponding to contours of the associated detected object; and registering the segmented objects to a 3D model of the associated detected object to\ndetermine an alignment of the detected object in the scene.\n Example 2 includes the subject matter of Example 1, further comprising deleting a selected object from the scene by: capturing a new RGB image frame that includes the selected object; generating a 2D mask based on the camera pose associated with\nthe new RGB image frame and the registration corresponding to the selected object; replacing pixels associated with the selected object within the 2D mask, with values based on pixels associated with neighboring regions in the new RGB image frame; and\napplying the mask to the new RGB image frame.\n Example 3 includes the subject matter of Examples 1 and 2, further comprising adding a selected object to the scene by: capturing a new RGB image frame that includes a region where the selected object is to be added; generating a 2D RGB image of\nthe selected object based on the camera pose associated with the new RGB image frame and a 3D model of the selected object; and rendering the 2D RGB image of the selected object onto the new RGB image frame.\n Example 4 includes the subject matter of Examples 1-3, further comprising generating a blueprint of the scene based on the registered objects and the associated locations of the detected objects.\n Example 5 includes the subject matter of Examples 1-4, wherein each pose of the depth camera is calculated by one of: using a transformation of the camera based on an Iterative Closest Point (ICP) matching operation performed on the depth pixels\nof the depth map frame; or using a Simultaneous Localization and Mapping (SLAM) operation performed on the color pixels of the RGB image frame; or based on data provided by inertial sensors of the depth camera.\n Example 6 includes the subject matter of Examples 1-5, wherein the object detection is based on at least one of template matching, classification using a bag-of-words vision model, and classification using a convolutional neural network.\n Example 7 includes the subject matter of Examples 1-6, wherein the object segmentation is based on detecting and removing surface planes from the scene to generate a processed scene; and performing a connected component clustering operation on\nthe processed scene to generate the segmented objects.\n Example 8 includes the subject matter of Examples 1-7, wherein the object segmentation further comprises: associating a label with the detected object; calculating a 2-Dimensional (2D) bounding box containing the detected object, and a 3D\nlocation of the center of the 2D bounding box; matching the detected object to an existing object boundary set created from a previously received 3D image frame, the matching based on the label and the 3D location of the center of the 2D bounding box; in\nresponse to a failure of the matching, creating a new object boundary set associated with the detected object, wherein the object boundary set comprises 3D positions of pixels in the 2D bounding box corresponding to the boundary of the object, and\nfurther comprises vectors associated with the pixels, the vectors specifying a ray from the position of the depth camera associated with the corresponding pose, to each of the pixels; and adjusting the object boundary set to remove duplicate pixels\ngenerated from different poses of the depth camera, the removal based on the distance of the pixels from the camera and further based on the direction of the associated vectors.\n Example 9 includes the subject matter of Examples 1-8, wherein the registration further comprises performing feature matching between the segmented objects and the associated 3D model; and generating an alignment transformation based on the\nmatched features using an Iterative Closest Point (ICP) matching operation and a Random Sample Consensus (RANSAC) operation.\n Example 10 is a system for 3-Dimensional (3D) scene analysis.  The system comprises: a 3D reconstruction circuit to receive a plurality of 3D image frames of a scene, each frame comprising a red-green-blue (RGB) image frame comprising color\npixels and a depth map frame comprising depth pixels, wherein each of the 3D image frames is associated with a pose of a depth camera that generated the 3D image frames, the 3D reconstruction circuit further to project the depth pixels into points in a\nglobal coordinate system based on the camera pose and accumulate the projected points into a 3D reconstruction of the scene; an object detection circuit to detect objects and associated locations in the scene, for each 3D image frame, based on the camera\npose, the 3D reconstruction, the RGB image frame and the depth map frame; a 3D segmentation circuit to segment each of the detected objects in the scene, the segmented objects comprising the points of the 3D reconstruction corresponding to contours of\nthe associated detected object; and a 3D registration circuit to register the segmented objects to a 3D model of the associated detected object to determine an alignment of the detected object in the scene.\n Example 11 includes the subject matter of Example 10, further comprising an augmented reality (AR) manipulation circuit to delete a selected object from the scene by: capturing a new RGB image frame that includes the selected object; generating\na 2D mask based on the camera pose associated with the new RGB image frame and the registration corresponding to the selected object; replacing pixels associated with the selected object within the 2D mask, with values based on pixels associated with\nneighboring regions in the new RGB image frame; and applying the mask to the new RGB image frame.\n Example 12 includes the subject matter of Examples 10 and 11, further comprising an AR manipulation circuit to add a selected object to the scene by: capturing a new RGB image frame that includes a region where the selected object is to be\nadded; generating a 2D RGB image of the selected object based on the camera pose associated with the new RGB image frame and a 3D model of the selected object; and rendering the 2D RGB image of the selected object onto the new RGB image frame.\n Example 13 includes the subject matter of Examples 10-12, further comprising an AR manipulation circuit to generate a blueprint of the scene based on the registered objects and the associated locations of the detected objects.\n Example 14 includes the subject matter of Examples 10-13, wherein each pose of the depth camera is calculated by one of: using a transformation of the camera based on an Iterative Closest Point (ICP) matching operation performed on the depth\npixels of the depth map frame; or using a Simultaneous Localization and Mapping (SLAM) operation performed on the color pixels of the RGB image frame; or based on data provided by inertial sensors of the depth camera, and wherein the object detection is\nbased on at least one of template matching, classification using a bag-of-words vision model, and classification using a convolutional neural network.\n Example 15 includes the subject matter of Examples 10-14, wherein the object segmentation is based on detecting and removing surface planes from the scene to generate a processed scene; and performing a connected component clustering operation\non the processed scene to generate the segmented objects.\n Example 16 includes the subject matter of Examples 10-15, wherein the object segmentation circuit is further to: associate a label with the detected object; calculate a 2-Dimensional (2D) bounding box containing the detected object, and a 3D\nlocation of the center of the 2D bounding box; match the detected object to an existing object boundary set created from a previously received 3D image frame, the match based on the label and the 3D location of the center of the 2D bounding box; in\nresponse to a failure of the match, create a new object boundary set associated with the detected object, wherein the object boundary set comprises 3D positions of pixels in the 2D bounding box corresponding to the boundary of the object, and further\ncomprises vectors associated with the pixels, the vectors specifying a ray from the position of the depth camera associated with the corresponding pose, to each of the pixels; and adjust the object boundary set to remove duplicate pixels generated from\ndifferent poses of the depth camera, the removal based on the distance of the pixels from the camera and further based on the direction of the associated vectors.\n Example 17 includes the subject matter of Examples 10-16, wherein the registration circuit is further to perform feature matching between the segmented objects and the associated 3D model; and generate an alignment transformation based on the\nmatched features using an Iterative Closest Point (ICP) matching operation and a Random Sample Consensus (RANSAC) operation.\n Example 18 is at least one non-transitory computer readable storage medium having instructions encoded thereon that, when executed by one or more processors, result in the following operations for 3-Dimensional (3D) scene analysis, the\noperations comprising: receiving a plurality of 3D image frames of a scene, each frame comprising a red-green-blue (RGB) image frame comprising color pixels and a depth map frame comprising depth pixels, wherein each of the 3D image frames is associated\nwith a pose of a depth camera that generated the 3D image frames; projecting the depth pixels into points in a global coordinate system based on the camera pose; accumulating the projected points into a 3D reconstruction of the scene; detecting objects\nand associated locations in the scene, for each 3D image frame, based on the camera pose, the 3D reconstruction, the RGB image frame and the depth map frame; segmenting each of the detected objects in the scene, the segmented objects comprising the\npoints of the 3D reconstruction corresponding to contours of the associated detected object; and registering the segmented objects to a 3D model of the associated detected object to determine an alignment of the detected object in the scene.\n Example 19 includes the subject matter of Examples 18, further comprising deleting a selected object from the scene by: capturing a new RGB image frame that includes the selected object; generating a 2D mask based on the camera pose associated\nwith the new RGB image frame and the registration corresponding to the selected object; replacing pixels associated with the selected object within the 2D mask, with values based on pixels associated with neighboring regions in the new RGB image frame;\nand applying the mask to the new RGB image frame.\n Example 20 includes the subject matter of Examples 18 and 19, further comprising adding a selected object to the scene by: capturing a new RGB image frame that includes a region where the selected object is to be added; generating a 2D RGB image\nof the selected object based on the camera pose associated with the new RGB image frame and a 3D model of the selected object; and rendering the 2D RGB image of the selected object onto the new RGB image frame.\n Example 21 includes the subject matter of Examples 18-20, further comprising generating a blueprint of the scene based on the registered objects and the associated locations of the detected objects.\n Example 22 includes the subject matter of Examples 18-21, wherein each pose of the depth camera is calculated by one of: using a transformation of the camera based on an Iterative Closest Point (ICP) matching operation performed on the depth\npixels of the depth map frame; or using a Simultaneous Localization and Mapping (SLAM) operation performed on the color pixels of the RGB image frame; or based on data provided by inertial sensors of the depth camera, and wherein the object detection is\nbased on at least one of template matching, classification using a bag-of-words vision model, and classification using a convolutional neural network.\n Example 23 includes the subject matter of Examples 18-22, wherein the object segmentation is based on detecting and removing surface planes from the scene to generate a processed scene; and performing a connected component clustering operation\non the processed scene to generate the segmented objects.\n Example 24 includes the subject matter of Examples 18-23, wherein the object segmentation further comprises: associating a label with the detected object; calculating a 2-Dimensional (2D) bounding box containing the detected object, and a 3D\nlocation of the center of the 2D bounding box; matching the detected object to an existing object boundary set created from a previously received 3D image frame, the matching based on the label and the 3D location of the center of the 2D bounding box; in\nresponse to a failure of the matching, creating a new object boundary set associated with the detected object, wherein the object boundary set comprises 3D positions of pixels in the 2D bounding box corresponding to the boundary of the object, and\nfurther comprises vectors associated with the pixels, the vectors specifying a ray from the position of the depth camera associated with the corresponding pose, to each of the pixels; and adjusting the object boundary set to remove duplicate pixels\ngenerated from different poses of the depth camera, the removal based on the distance of the pixels from the camera and further based on the direction of the associated vectors.\n Example 25 includes the subject matter of Examples 18-24, wherein the registration further comprises performing feature matching between the segmented objects and the associated 3D model; and generating an alignment transformation based on the\nmatched features using an Iterative Closest Point (ICP) matching operation and a Random Sample Consensus (RANSAC) operation.\n Example 26 is a system for 3-Dimensional (3D) scene analysis.  The system comprises: means for receiving a plurality of 3D image frames of a scene, each frame comprising a red-green-blue (RGB) image frame comprising color pixels and a depth map\nframe comprising depth pixels, wherein each of the 3D image frames is associated with a pose of a depth camera that generated the 3D image frames; means for projecting the depth pixels into points in a global coordinate system based on the camera pose;\nmeans for accumulating the projected points into a 3D reconstruction of the scene; means for detecting objects and associated locations in the scene, for each 3D image frame, based on the camera pose, the 3D reconstruction, the RGB image frame and the\ndepth map frame; means for segmenting each of the detected objects in the scene, the segmented objects comprising the points of the 3D reconstruction corresponding to contours of the associated detected object; and means for registering the segmented\nobjects to a 3D model of the associated detected object to determine an alignment of the detected object in the scene.\n Example 27 includes the subject matter of Example 26, further comprising means for deleting a selected object from the scene by: capturing a new RGB image frame that includes the selected object; generating a 2D mask based on the camera pose\nassociated with the new RGB image frame and the registration corresponding to the selected object; replacing pixels associated with the selected object within the 2D mask, with values based on pixels associated with neighboring regions in the new RGB\nimage frame; and applying the mask to the new RGB image frame.\n Example 28 includes the subject matter of Examples 26 and 27, further comprising means for adding a selected object to the scene by: capturing a new RGB image frame that includes a region where the selected object is to be added; generating a 2D\nRGB image of the selected object based on the camera pose associated with the new RGB image frame and a 3D model of the selected object; and rendering the 2D RGB image of the selected object onto the new RGB image frame.\n Example 29 includes the subject matter of Examples 26-28, further comprising means for generating a blueprint of the scene based on the registered objects and the associated locations of the detected objects.\n Example 30 includes the subject matter of Examples 26-29, wherein each pose of the depth camera is calculated by one of: using a transformation of the camera based on an Iterative Closest Point (ICP) matching operation performed on the depth\npixels of the depth map frame; or using a Simultaneous Localization and Mapping (SLAM) operation performed on the color pixels of the RGB image frame; or based on data provided by inertial sensors of the depth camera, and wherein the object detection is\nbased on at least one of template matching, classification using a bag-of-words vision model, and classification using a convolutional neural network.\n Example 31 includes the subject matter of Examples 26-30, wherein the object segmentation is based on detecting and removing surface planes from the scene to generate a processed scene; and performing a connected component clustering operation\non the processed scene to generate the segmented objects.\n Example 32 includes the subject matter of Examples 26-31, wherein the object segmentation further comprises: means for associating a label with the detected object; means for calculating a 2-Dimensional (2D) bounding box containing the detected\nobject, and a 3D location of the center of the 2D bounding box; means for matching the detected object to an existing object boundary set created from a previously received 3D image frame, the matching based on the label and the 3D location of the center\nof the 2D bounding box; means for, in response to a failure of the matching, creating a new object boundary set associated with the detected object, wherein the object boundary set comprises 3D positions of pixels in the 2D bounding box corresponding to\nthe boundary of the object, and further comprises vectors associated with the pixels, the vectors specifying a ray from the position of the depth camera associated with the corresponding pose, to each of the pixels; and means for adjusting the object\nboundary set to remove duplicate pixels generated from different poses of the depth camera, the removal based on the distance of the pixels from the camera and further based on the direction of the associated vectors.\n Example 33 includes the subject matter of Examples 26-32, wherein the registration further comprises means for performing feature matching between the segmented objects and the associated 3D model; and means for generating an alignment\ntransformation based on the matched features using an Iterative Closest Point (ICP) matching operation and a Random Sample Consensus (RANSAC) operation.\n The terms and expressions which have been employed herein are used as terms of description and not of limitation, and there is no intention, in the use of such terms and expressions, of excluding any equivalents of the features shown and\ndescribed (or portions thereof), and it is recognized that various modifications are possible within the scope of the claims.  Accordingly, the claims are intended to cover all such equivalents.  Various features, aspects, and embodiments have been\ndescribed herein.  The features, aspects, and embodiments are susceptible to combination with one another as well as to variation and modification, as will be understood by those having skill in the art.  The present disclosure should, therefore, be\nconsidered to encompass such combinations, variations, and modifications.  It is intended that the scope of the present disclosure be limited not be this detailed description, but rather by the claims appended hereto.  Future filed applications claiming\npriority to this application may claim the disclosed subject matter in a different manner, and may generally include any set of one or more elements as variously disclosed or otherwise demonstrated herein.", "application_number": "15046614", "abstract": " Techniques are provided for 3D analysis of a scene including detection,\n     segmentation and registration of objects within the scene. The analysis\n     results may be used to implement augmented reality operations including\n     removal and insertion of objects and the generation of blueprints. An\n     example method may include receiving 3D image frames of the scene, each\n     frame associated with a pose of a depth camera, and creating a 3D\n     reconstruction of the scene based on depth pixels that are projected and\n     accumulated into a global coordinate system. The method may also include\n     detecting objects, and associated locations within the scene, based on\n     the 3D reconstruction, the camera pose and the image frames. The method\n     may further include segmenting the detected objects into points of the 3D\n     reconstruction corresponding to contours of the object and registering\n     the segmented objects to 3D models of the objects to determine their\n     alignment.\n", "citations": ["8488877", "8571328", "8849039", "8861840", "8942917", "9083960", "9158971", "9171403", "9201499", "9378431", "9436987", "9438891", "9525862", "9626737", "9639943", "9665937", "9699380", "9779508", "20070031064", "20080225044", "20090128577", "20090232355", "20090232388", "20100079456", "20100207936", "20100289817", "20110116698", "20110194732", "20120194644", "20120195471", "20120306876", "20130170696", "20130181983", "20140003705", "20140037189", "20140105486", "20140206443", "20140340489", "20150006117", "20150030236", "20150231490", "20150262412", "20150347872", "20160012646", "20160171755", "20160189381", "20160189419", "20160196659", "20160328856", "20160343152", "20170046868", "20170084025", "20170228940", "20170243352", "20170278231", "20170341237", "20170345181", "20170372489", "20180005015"], "related": []}, {"id": "20180063163", "patent_code": "10375143", "patent_name": "Learning indicators of compromise with hierarchical models", "year": "2019", "inventor_and_country_data": " Inventors: \nPevny; Tomas (Praha-Modrany, CZ), Somol; Petr (Marianske Lazne, CZ)  ", "description": "TECHNICAL FIELD\n The present disclosure relates to network security, and more particularly to the use of neural networks to identify indicators of compromise (IOCs) in connection with discovery of malware.\nBACKGROUND\n Enterprise networks can easily become infected with viruses and malware, particularly as the types and number of applications proliferate over the Internet.  Keeping track of and preventing viruses and malware has, accordingly, become\nincreasingly difficult.\n Traditionally, signature-based security devices, firewalls, or anti-viruses are deployed to detect such threats.  However, signature-based algorithms simply compare a byte sequence that has been detected to stored byte-sequences corresponding to\nknown threats, which may be in a database.  Thus, if a new threat has not yet been analyzed and recorded into the database, the signature based algorithm may not identify the new threat.  Furthermore, if a threat has the ability to change, the\nsignature-based algorithms may again fail to identify the threat because a current signature of the threat may be different from a stored signature of the same threat that was recorded earlier.  Thus, polymorphic malware, zero-day attacks by threats that\nare novel or previously unseen, or other types of advanced persistent network threats are usually not detected or blocked by signature-based security algorithms. BRIEF DESCRIPTION OF THE DRAWINGS\n FIG. 1 illustrates an example operating environment in which an embodiment may be implemented.\n FIG. 2 is a representation of traffic flowing between computers within an enterprise and multiple external domains, with individual flows organized as bags in accordance with an example embodiment.\n FIG. 3 depicts the aggregation of traffic through a neural network in accordance with an example embodiment.\n FIG. 4 is a flowchart depicting a series of operations in accordance with an example embodiment.\n FIG. 5 is a block diagram that illustrates a computer system or apparatus upon which an embodiment of the disclosed malware detection system may be implemented.\nDESCRIPTION OF EXAMPLE EMBODIMENTS\n Overview\n Presented herein are techniques for classifying devices as being infected with malware based on learned indicators of compromise.  A method includes receiving at a security analysis device, traffic flows from a plurality of entities destined for\na plurality of users, aggregating the traffic flows into discrete bags of traffic, wherein the bags of traffic comprise a plurality of flows of traffic for a given user over a predetermined period of time, extracting features from the bags of traffic and\naggregating the features into per-flow feature vectors, aggregating the per-flow feature vectors into per-destination domain aggregated vectors, combining the per-destination-domain aggregated vectors into a per-user aggregated vector, and classifying a\ncomputing device used by a given user as infected with malware when indicators of compromise detected in the bags of traffic indicate that the per-user aggregated vector for the given user includes suspicious features among the extracted features.\n An apparatus is also presented and includes a network interface unit configured to enable communications via a network, a memory configured to store logic instructions, and a processor, when executing the logic instructions, configured to\nreceive traffic flows from a plurality of entities destined for a plurality of users, aggregate the traffic flows into discrete bags of traffic, wherein the bags of traffic comprise a plurality of flows of traffic for a given user over a predetermined\nperiod of time, extract features from the bags of traffic and aggregating the features into per-flow feature vectors, aggregate the per-flow feature vectors into per-destination domain aggregated vectors, combine the per-destination-domain aggregated\nvectors into a per-user aggregated vector, and classify a computing device used by a given user as infected with malware when indicators of compromise detected in the bags of traffic indicate that the per-user aggregated vector for the given user\nincludes suspicious features among the extracted features.\n Example Embodiments\n A problem preventing a wider use of machine learning in network intrusion detection is the difficulty of obtaining accurate labels on the level of individual network connections (Terminal Control Protocol (TCP) flow, Hypertext Transfer Protocol\n(HTTP) request, etc.).  Even for an experienced security officer it is almost impossible to determine which network connections are caused by malware and which by a benign user or an application.  Moreover, malware often exhibits itself by performing\nseemingly innocent connections.  For example, it might connect to google.com to verify if the computer is connected to the network (connection check), or it may display advertisements to render money for malware authors, etc. These problems in obtaining\ntrue labels on level of individual connections makes automatic and large-scale training of accurate classifiers for a given type of network traffic very difficult.\n To address the foregoing, described herein is a hierarchical classifier where lower layers detect a type of traffic typical for the malware, and upper layers learn that certain combinations of the traffic are very typical for infected hosts. \nThe advantage of this hierarchy is that accuracy of classifiers on lower layers can be relatively low, but their combination performed in upper layers tends to make the overall classifier (detector) very accurate.  Further, in an embodiment, the\nclassifier may be trained with labels provided on the level of user, i.e., it can be determined that a given computer is clean, or another one is infected, all while also determining which connections were more likely caused by malware and which ones\nwere more likely caused by a user or a legitimate system process.\n More specifically, the embodiments described herein train a Machine Learning (ML) system (e.g., a neural network) based on collectable data from an arbitrarily large computer network where the human analyst input merely includes verdicts on a\nnetwork node level--i.e., an analyst identifies nodes (corresponding to users) that are likely infected, without providing any further detail.  The instant embodiments analyze traffic logs, sys logs and possibly other information, and discover patterns\nthat are significant in distinguishing the suspicious nodes from the presumed benign ones.  Subsequent to this training stage, the ML system uses the discovered patterns to make verdicts about future infections in network nodes.\n FIG. 1 illustrates an example operating environment in which an embodiment may be implemented.  In FIG. 1, network traffic 152 between an enterprise 110 and external domains 120, 130 is depicted.  Domains 120, 130, may be devices such as, as\nservers (e.g., web servers, and the like) with which computers within the enterprise 110 communicate.\n The computer system 113 in enterprise 110 may initiate communication with, e.g., computer system 123 of domain 120 through a gateway or proxy device 115 that is connected to the Internet 150.  That same traffic 152, in the form of packet flows\n154, may also be provided to a central server computer 117.\n In an example of a network threat incident, computer system 140, also connected to Internet 150, may be a source of a network attack.  That is, computer system 140 may be configured to spoof domains 120 and 130.  Those skilled in the art will\nappreciate that domains 120 and 130 may, however, themselves, be a source of a network attack, which may have been able to penetrate enterprise 110.  Thus, the packet flows from computer system 123 (or 140) to enterprise computer system 113 may contain\nmalicious network attack packets.  In order to detect the network threat, malware detection logic 200 may be part of gateway system 115, in an embodiment.  As the traffic 152 is received by gateway system 115, the malware detection logic 200 may analyze\nthe traffic using the techniques described herein.\n In another embodiment, the malware detection logic 200 may be hosted in a separate computer system such as central server computer 117.  In this configuration, the malware detection logic 200 may import packet flow log data or files (\"logs\") and\nanalyze them rather than receiving packet flows 154 directly from the network.  For example, the malware detection logic 200 may receive logs from gateway system 115.  Thus, using techniques described herein, the malware detection logic 200 may detect\nthe network threat incident on computer system 113 and also may trace the incident to the originating computer system 140 or 123.  In a specific implementation, malware detection logic 200 may be configured to learn indicators of compromise (IOCs) via a\nneural network and thereby determine whether a given computer system (e.g., computer system 113) is infected with malware or not.\n In an embodiment, malware detection logic 200 is configured to learn a security breach from training data.  Notably, the number of needed labels in training data to be provided by humans is effectively much lower than usual in standard ML\ntechniques due to the label coverage of groups of samples (\"bags\") instead of single samples, i.e., groups of (groups of .  . . ) samples.  Each bag can consist of an arbitrary number of samples and thus vectors arranged with such samples may be of\narbitrary size.  This enables automatic learning from large data sets.  Malware detection logic 200 is further configured to automatically reveal to human analysts previously unknown patterns in data that signify malicious activity (i.e., automatically\ndiscover previously unknown IOCs).\n Malware detection logic 200 may be implemented in the form of a multi-layer feed-forward neural network (NN) trained by back-propagation.  The neural network according to the instant embodiments, consumes traffic data in a hierarchy of \"bags.\"\n That is, individual samples are represented by vectors obtained by extracting numeric features from source network data (e.g., logs).  Which features are employed depends on the type of domain/network being analyzed.  Thus, for example, in the\ncase of telemetry, features may include, e.g., n-gram statistics from URL strings, bytes received, bytes sent, etc. In the case of a sandbox, captured features may include, e.g., timings of system calls, size of allocated resources, etc.\n No individual labels (benign/malicious) need to be known for individual samples in the training set.  Only coarser labels are needed on the top-most bag level, marking bags as presumed benign or presumed malicious; here benign or malicious does\nnot mean that all samples in a bag are of the same type--it is only assumed that the distribution of samples in malicious bags is in some (unknown) way different from the distribution of samples in benign bags.\n Hidden NN layers are designed to model patterns inside bags.  The hierarchy of NN layers reflects the hierarchy of bags.  Note that the number of neurons in hidden NN layers may be manually selected, keeping in mind that it should be high enough\nto cover the possible variety of patterns that can be expected to appear in input data.\n A notable feature of malware detection logic 200 is in its approach to learning.  In standard NNs every sample consists of one input vector.  As such, every neuron reads the input vector representing one sample and immediately computes and\npasses one output value to the next layer.  In the instant embodiments, each sample consists of multiple input vectors.  Every neuron processes all input vectors of one sample, computes their respective output values, and only when all input vectors from\na sample have been read the neuron aggregates all computed output values to a single output value representing the whole sample.  This is then passed to the next layer as the neuron's output.  Note that the aggregation function may be selected so as to\nenable calculation of partial derivatives with respect to its input and possibly with respect to internal parameters.  This ensures proper operation of the back-propagation operations.\nUser-Centric System Based on Telemetry Data\n The ultimate goal of the instant embodiments is to achieve high accuracy of detecting users infected by malware where the detection is based on traffic logs (e.g., HTTP proxy logs, statistics provided by network devices through NetFlow protocol)\nof user's network activity.  In the tested embodiment, individual samples are HTTP(s) requests.  Lower level bags are viewed here as collections of requests from one user to one domain.  Higher level bags then represent all traffic of one user.\n FIG. 2 is a representation of traffic flowing between computers 113a, 113b, 113c within enterprise 110 and multiple external domains, with individual flows.  The individual flows from multiple domains to a given single user can be aggregated or\norganized as a bag in accordance with an example embodiment.\n Reference is now made to FIG. 3, which shows how traffic 350 (e.g., traffic 152 in FIG. 1, and the individual flows shown in FIG. 2, arranged into bags by user (e.g., user_j_smith, user_s_dawn), is broken down into feature vectors 360 per flow,\nwhich are aggregated into per-destination-domain vectors 370, which are then further aggregated to obtain a per-user aggregated vector 380.\n The object of these aggregations is to learn the difference in behavior of clean versus infected users.  Observations confirm that discovery of high-end malware improves greatly with structural and context-sensitive IOCs.  Examples of IOCs\ninclude, as indicated at the bottom of FIG. 3, \"flow active as connection check,\" \"flow representing search request,\" \"communication to this domain is mostly API based,\" \"communication to this domain has high number of connection checks,\" \"communication\nto this domain contained empty path,\" user accesses search engines through API, \" or \"user often reads mail and news,\" among other possibilities.\n The methodology described below aims to learn IOCs automatically from labels available only on the level of users.  To create training data, an analyst need only mark each network node (user) as infected or clean, instead of tediously analyzing\nand labeling an excessive number of individual HTTP(s) requests.  The embodiments described herein then find how the users' traffic differs, and finds the common as well as distinct traffic patterns.  Thus, the approach described herein effectively\nlearns IOCs and also their weighted combination leading to accurate detection of infected users.\n In one implementation, the neural network of the malware detection logic 200 comprises two hidden NN layers, that are supplied initial per-flow feature vectors 360 via inputs 375.\n The first hidden NN layer, domain model, shown as 385 in FIG. 3, is intended to model types of user's traffic to domains (or destination IP addresses).  Each neuron in the layer 385 learns to recognize particular (unknown) types of connections. \nThus, neurons in the domain model 385 receive, via inputs 375, bags of connections between a single user and single domain, and output their decision, which is a single real number.  Since the domain model 385 is composed of multiple neurons, its output\nis one real vector for every domain the user has visited (i.e., for each bag entering layer 385).\n The second hidden NN layer, user model, shown as 390 in FIG. 3, is intended to model types of users.  Its neurons learn the particular patterns in traffic that are specific for infected users.  The neurons consume the output of the domain model. Since the user has probably visited many domains, the domain model can produce a varying number of output vectors, forming a higher-level \"bag of domain connection behaviors\".  Thus, neurons in the user model 390 take such higher-level bags and output\ntheir decision which is again a single real number.\n Note that while processing samples in a bag, both the domain model 385 and user model 390 aggregate individual-sample outputs per neuron to produce a single output value.  For this function, the use of a maximum or average aggregator, among\nothers, may be employed to produce or generate the single output value.\n In one implementation, input vectors are representative of individual HTTP(s) requests.  Referring to FIG. 2, the black dots represent a plurality of HTTP sessions, which are sampled to produce the input vector.  In one example, individual\nsamples were transformed into vectors of 550 features.  Feature examples include: 1) HTTP protocol used, 2) number of subdomains present in URL, 3) length of User-Agent string (indicates also whether the user agent is present), 4) domain popularity\n(e.g., taken from Alexa database), 5) (logarithm of the number of) bytes transferred from client to server, 6) bytes transferred from server to client, 7) presence of a specific \"xyz\" n-gram in the domain name (e.g., from all possible n-grams use those\nwith below-average frequency), 8) presence of self-signed certificate, 9) referrer for the request is available?, 10) HTTP status of the connection (e.g., connection is a redirection/success/error/ .  . . ), 11) maximum length of continuous lower case\ncharacters in URL, 12) maximum length of continuous upper case characters in URL, 13) duration of TCP connection, 14) length of URL path, 15) length of URL query, 16) status of the HTTP request, 17) client port, 18) server port, 19) is connection to raw\nIP address, among others.\n Input bags enter the first hidden layer which outputs for each bag one output vector; a collection of such vectors then enters as higher-level bag the second hidden layer to eventually produce one final output vector.  See FIG. 3, vector 380. \nThis vector is then transformed to a single value 295 that expresses the classifier decision.  Back-propagation then propagates back weight changes depending on how the classifier output corresponded to the known user-level label in the training set.\n Note that while the classifier is learned using the user-level labels and is thus learned in a supervised way, the underlying structure of neuron weights in lower-level hidden layers is effectively trained in an unsupervised way; the top-level\nlabel does affect the weight updates but does not determine the model's ability to reveal the patterns in underlying data distribution.\n A more formal description of the foregoing follows.\n As explained, the goal is to classify users on the basis of their connections into two or more classes (in this case, just two classes: clean and infected).  This means that one sample (bag), b; consists of all network connection of one user and\nit is not known which connections within are caused by the malware and which are of the user or the operating system.\n The set of all bags (also users or samples), is denoted by b.di-elect cons..  Set of all observed connections of all users will be denoted by x.di-elect cons.X,x.di-elect cons..sup.m, with X.sub.b being set of connections of user b; and\nx.sub.b:d being set of connections of a user b going to domain d. The set .sub.b is the set of domains contacted by the user b. Finally y.sub.b is a label of a user b (infected/clean).  In this work y.sub.b.di-elect cons.{-1, +1}\n The essence of the approach described herein is to model the classification system in two layers.  The first layer, domain model, is intended to model traffic to domains (or destination IP addresses), d. Its detectors (neurons) within should\nrecognize particular type of connections, for example download of advertisements, the domain name in the request was generated by a domain generating algorithm (DGA), etc. The second layer, user model, is intended to model users and its detectors learn\nthat particular combinations of traffic are unique for infected users.\n Both layers can be described by functions f.sub.d: .orgate..sub.k&gt;1.sup.k,m.fwdarw..sup.m.sup.d and f.sub.b:.orgate..sub.k&gt;1.sup.k,m.sup.d.fwdarw..sup.m.sup.b, respectively.  Since the number of user's connections and number of contacted\ndomains by user varies across users, both functions f.sub.d and f.sub.b are be configured to accept unknown number of input vectors of size .sup.m, .sup.m.sup.d respectively.  This is symbolized in the above notation by domain of functions being\nrectangular matrices with any number of rows.  The composite function f.sub.b (.orgate..sub.d.di-elect cons..sub.bf.sub.d(x.sub.b:d)) models the user by a single m.sub.d-dimensional vector.  Thus, there is one vector of fixed dimension to one label,\nwhich enables the use any machine-learning algorithms.  The challenge is, how to choose/learn functions (models) f.sub.d and f.sub.b.\n The problem can be elegantly solved if the function composition is viewed as a neural network with aggregation functions of choice, e.g., minimum, maximum, applied in two separate layers and learning f.sub.d and f.sub.b using a backpropagation\nalgorithm.  The approach is outlined in FIG. 3.  Units (neurons) i.sub.1, .  . . , i.sub.m in the input layer 375 do not have any weights as they simply distribute the input feature vector, x.di-elect cons.  (describing one connection) to neurons in the\nnext layer 385 modeling the domains (domain models denoted as h.sub.d:1, .  . . , h.sub.d:m.sub.d).  The domain models aggregate feature vectors of all user's connections to each domain, d; and for each domain outputs a single vector x.sub.b:d.di-elect\ncons..sup.m.sup.b.  This set of vectors (one for each domain contacted by the user) is forwarded to user models (users models denoted as h.sub.b:1, .  . . , h.sub.b:m.sub.b), which aggregate them and output a single vector x.sub.b:d.di-elect\ncons..sup.m.sup.b describing the user.  This vector is used in the final classifier to provide the decision.  As noted, both layers f.sub.d, f.sub.b and the final classifier can be implemented as a multi-layer neural network.  Moreover, the model can be\nextended to have more aggregation layers, if for example there is meta-information that allows further sub-grouping of connections or connections can be broken down to simpler entities.\n Example of an Implementation\n In an example implementation, neurons in the domain and user models layer were implemented by reluMax units as f.sub.d(x.sub.b:d)=(max{{0,max.sub.x.di-elect cons.X.sub.b:d{w.sub.d,1.sup.Tx+v.sub.d,1}}, .  . . , max{{0,max.sub.x.di-elect\ncons.X.sub.b:d{w.sub.d,m.sub.d.sup.Tx+v.sub.d,m.sub.d}}) (1) and f.sub.d(x.sub.b)=(max{{0,max.sub.d.di-elect cons.D.sub.b{w.sub.b,1.sup.Tf.sub.d(x.sub.b:d)+v.sub.b,1}}, .  . . , max{{0,max.sub.d.di-elect\ncons.D.sub.b{w.sub.b,1.sup.Tf.sub.d(x.sub.b:d)+v.sub.b,m.sub.d}}) (2)\n Inner maximums max.sub.x.di-elect cons.X.sub.b:d (1) aggregate multiple connections to single domain (domain models) and max.sub.d.di-elect cons..sub.b in (2) aggregates user's connections to multiple domains.  The output classifier o; is a\nsimple linear classifier sign(w.sub.o.sup.Tf.sub.d(X.sub.b)+v.sub.o) with sign being the signum function.  The whole model is parametrized by weights w.sub.d.di-elect cons..sup.k,m.sup.d, w.sub.b.di-elect cons..sup.m.sup.d.sup.,m.sup.b, w.sub.o.di-elect\ncons..sup.m.sup.b, v.sub.d.di-elect cons..sup.m.sup.d, v.sub.b.di-elect cons..sup.m.sup.b, v.sub.o.di-elect cons.  which are optimized with a gradient calculated by a back-propagation algorithm.  The error function is a simple hinge-loss of the form\nl(y.sub.b,o.sub.b)=max{0,1-y.sub.bo.sub.b}.  The ADAM algorithm (Kingma, D. et al., \"Adam: A Method for Stochastic Optimization,\" 2015) was used with default settings and with gradient estimated from 50 randomly selected clean bags and 50 randomly\nselected infected bags.\n The above neural network was evaluated in two scenarios in network intrusion detection differing by the source of the data.  In the first scenario the data included HTTP proxy logs obtained from a cloud web security system.  In the second\nscenario the data included NetFlow records enhanced by the information about length and timings of individual packets.\n First Embodiment--HTTP Proxy Logs\n One HTTP(s) connection is described by a 287 dimensional vector developed for the classification layer of a Cognitive Threat Analytics (CTA) engine.  One sample (bag) describing user's traffic from a five-minute long observation window consists\nof a set of 287-dimensional vectors, each describing one HTTP(s) connection.  The CTA engine identified users infected by the malware using DGA algorithm, and this labeling was used as a ground truth.  Although this labeling is not perfect, the goal here\nwas to demonstrate that the instant approach can learn from labels provided on the level of bags rather than on level of individual HTTP requests.  The dataset contained 15,055,979 HTTP requests from 24,627 users out of which 1,471 were labeled as\ninfected.  Note that some bags contain as many as 50,000 requests which demonstrates the difficulty of labeling requests within.  The data were split into training and testing sets on the level of users, such that 50% of users were used for training and\nthe rest for testing.  Both domain and user models consisted of single layer of 20 reluMax neurons as described in (1) and (2).  The ADAM algorithm was allowed to run for 210.sup.4 iterations with gradient estimated from 100 randomly selected bags in\neach iteration (50 clean, 50 infected).\n After the training, the error was 3.2910.sup.-4 (P.sub.FP=3.2910.sup.-4, P.sub.FN=0.0) on the training set and 0.0048 (P.sub.FP=0.0021, P.sub.FN=0.0506) on the testing set.  The fact that the described embodiment achieved nearly zero error on\nthe training set and optimistic 0.2% error on the testing set demonstrates that it has learned to identify infected users.  Moreover, notice that the false positive rate on the testing set is about 0.1% which is very favorable.\n Recall that one of the goals of the instant embodiments is to learn the type of traffic typical for malware that indicate infected users (indicators of compromise, IOC).  The learnt IOCs can be very weak if used independently of each other, but\ntheir combination enabled by user model layer yields a very strong classifier, since the error probability is less than 0.5%.\n In this regard, investigation was undertaken related to the types of HTTP connections to which neurons in the domain model layer are the most sensitive.  The sensitivity of i.sup.th-neuron to HTTP request with feature vector x was measured as\nmax{0,w.sub.d,i.sup.Tx+v.sub.d,i}.  By looking at connections with the highest score, it was possible to identify which connections the neuron recognizes.  Listed below are several connections of interest together with the assumed type of learnt traffic. HTTPs connections to raw IP addresses like hxxps://62.249.33.21/ DGA domains like hxxp://ivdyxgqtwqsztopjrijlnhqwcnbtk.com, hxxp://pojxofukqskfhajvizdhmdxwwghq.biz, and hxxp://twwkgihmmvspblrnzpnjnhexcqgtkrk.com HTTPs connections to live.com domain like\nhxxps://roaming.officeapps.live.com/[ow] Download of images like http://www.biglots.com/images/aprimo/common/holiday_header/110714-04.gif Seemingly legitimate traffic like hxxp://banners.itunes.apple.com/js/banner-main-built.j s or\nhxxp://www.slfn.co.uk/today_matchsheet.php.\n The first two types are well known to be related to malware.  The third one is interesting, since live.com is a legitimate domain belonging to Microsoft.  Nevertheless, search on malware blogs revealed that it is indeed used by malware.  The\nlast two types seem to be related to advertisements, which would suggest that the system has learnt that advertisement is a sign of malware infection.  This also makes sense as the malware monetize infections by doing click-fraud.  The learnt indicators\nof compromise seem to make sense, but they are not the type security researchers would create manually, because using them individually would lead to very high false positive rate.  Notably, the system learned DGA indicator of compromise without knowing\nwhat it is, which demonstrates its practical utility.\n Second Embodiment--NetFlow Records\n Enhanced Threat Telemetry and Analytics (ETTA) is a project whose goal is to investigate how statistics exported in NetFlow and IPFIX about network connections can be enriched to get better visibility into the flows and to improve the accuracy\nin detecting infected users/malware.  An experimental database was created by using traffic of few users captured during ETTA.  This traffic was used as a background (considered clean), since all computers came from a protected network.  The bags were\ncreated by grouping the flows by an IP address, which means that bags contained mixture of users and servers.  Infected users were simulated by mixing the traffic captured during 5-minute long execution of the malware within ThreatGRID sandbox.  The\nmixing was implemented by changing the IP address, which was sufficient.  Note that in this case, we are effectively trying to detect an event, when user has just downloaded the malicious payload which is executed.  As in the previous case, the time\nwindow of one bag was set to five minutes, which is also the length of malware's execution in ThreatGRID sandbox.  A notable difference to experiment with HTTP logs is the replacement of domains by destination IP addresses, since not all flows were HTTP\nconnection.  The training set contained traffic from 6132 users collected during 11 days (with a 14 day break after the fifth day), which after division into batches yielded to 2,132,446 samples.  Thus, the training set contained traffic from five days,\nand the testing set contained traffic from six days, such that the time-difference between both sets was 14 days.  To simulate infected users, traffic collected in sandbox was mixed into randomly selected users.  Samples of 237,100 binaries were taken\nfrom the sandbox.\n Each connection was described by an 80-dimensional vector, consisting of sizes and inter-packet times of first twenty incoming and first twenty outgoing packets.  These features are very simple and used as is, without any other processing except\nnormalizing by log(1+x) to reduce their dynamic range.  All settings as the configuration of the neural network, the training algorithm, its number of allowed connections, etc. were the same as in the previous HTTP case.\n The error of the classifier was 0.0093 (P.sub.FP=0.0061, P.sub.FN=0.0124) on the training set and 0.0111 (P.sub.FP=0.0083, P.sub.FN=0.0139) on the testing set.  As in the previous case, investigation was made as to the types of connections the\ndomain (destination IP) models have learned, though this was more difficult, since there was no information about the URLs.  Nevertheless, it could be surmised according to the information about destination IP address and if the flow came from the\nsandbox or from a user.  Based on this information, neurons have been found to be sensitive to connection checks or pings, with only one incoming and outgoing packet typically to google servers, to connections to sinkhole server located to Poland, but\nalso neurons sensitive web conferencing (this is due to the nature of our background data).  These again demonstrate that the neurons in the domain (destination IP) modeling layer learn useful types of traffic, albeit not being always individually usable\nIOC.\n Third Embodiment--Domain-centric System Based on Telemetry Data\n The first embodiment assumed availability of user-level labels.  A \"shallower\" system can be defined with only one hidden NN layer, taking use of domain-centric labels, e.g., from blacklists.  Such a neural network may be trained only from bags\nrepresenting user-domain connections.  The resulting IOCs would represent patterns extracted from traffic per domain.\n Fourth Embodiment--Binary-hash-centric System Based on Sandbox Data\n A version of the approach from the previously described embodiments can be modified for analyzing behaviors of executable files.  The training data would come from a sandbox or any other network sensor capable attaching a hash of the process to\neach log entry (e.g., ThreatGRID or Advanced Malware Protection (AMP)), covering logs of respective executable's system calls, network traffic, caused changes to file system, etc. Labels are available per executable (hash), highest level bags would cover\ncomplete executable activity capture.  Lower-level bags can be certainly defined taking use of inherent data structure (activity time windows, sys calls of various types, structuring of file system access according to destination, etc.).\n The result would be a classification system as well as IOCs describing in previously unknown detail the inner patterns in executable activities.\nOther Possible Implementation and Enhancements\n More Descriptive Power by Extended Aggregators\n In the first embodiment all neurons employed the same aggregator function: maximum.  The intuition is to let the system emphasize such individual detailed pattern(s) in the bag traffic that have the most striking effect on overall efficacy.  It\nis fast and well interpretable.  The maximum function, however, does not take into the account the volume of the type of the IOCs, which can be undesirable (e.g., IOC of showing advertisement).\n If maximum function is replaced by average, the system would put emphasis on less detail but more on prevailing characteristics over whole bags.  This would improve robustness against noise but reduce the attention to detail--will not trigger on\nspikes in traffic.  Replacing maximum by average in all neurons can lead to loss of detection power according to experiments.\n However, there are other ways to modify neural networks where one or multiple of the following are implemented to enrich the model's expressive power:\n In one approach, the aggregator function is defined as a parametrized generic function where the parameter gets optimized as part of network learning.  A good example is q-th root of a sum of q-th powers.  Higher q moves the function closer to\nmaximum, lower q&gt;1 moves it closer to average.\n In another approach, multiple aggregator functions, maximum, average and/or the generic aggregator are performed in parallel, increasing the number of neurons per layer.\n Deepening the Neural Network to Learn a Hierarchy of IOCs\n In the foregoing embodiments, a small number of NN layers are implemented, and can be described as \"shallow\".  They partially emulate the power of deep networks by modeling the structure in bags; however, the hierarchy of bags is pre-specified,\ne.g., flows over a predetermined amount of time.  The power of deep neural networks in image analysis consists in the ability to model arbitrarily deep hierarchies of patterns in data.  The embodiments described herein can also be extended to allow\nautomatic learning of hierarchy depth as well.\n Specifically, an equivalent to the \"convolution kernel\" trick from classical Deep Learning is employed, where a parameterized function reflects context in data to various extent while the learning algorithms optimizes the parameter.  This is\nmore difficult in network security than in image analysis due to less inherent structure in data--unlike the regular matrix of pixels in images the network data are highly irregular and diverse.\n However, a \"convolution\" can be defined over network events in time.  It is known that context in traffic matters (e.g., if certain signals are preceded by connection check, their maliciousness probability is higher).  Hence it is possible to\ndefine time context windows of parameterized size (the window would aggregate information from network events within its reach), allowing for both automated window parameter optimization and hierarchical stacking of such windows in varying levels of\ncoarseness.\n A difference from the first embodiment is in the definition of what is a bag and consequently how NN layers are constructed.  In the first embodiment bags have pre-specified types, i.e., humans have defined how to group samples to bags, and the\nNN has consequently a corresponding number of levels.  In contrast, a deep architecture can decide itself as part of the optimization process.  Hence a parameterized definition of a bag is employed with optimizable parameters.  The time window is a\nviable installment of parameterized bag.  The number of NN layers would then be set significantly higher to let the optimizer assign weights to neurons as needed for the accuracy of the model.\n Correlating Multiple Types of Data for More Complex IOC Discovery\n In the foregoing embodiments it is assumed that the network is trained on a single type of data.  Experience confirms that correlating various sources of data often leads to better efficacy.\n Thus, the embodiments described herein can be extended to build joint models over multiple types of data.  This can be achieved in multiple ways, including: connecting multiple input vectors to the first network layer; or using supplemental data\nto define bags in primary data.\n Reference is now made to FIG. 4, which is a flowchart depicting a series of operations in accordance with an example embodiment.  These operations may be performed by malware detection logic 200 that is loaded on, e.g., gateway 115 or separate\nserver 117, either of which can function as a security analysis device.  The operations include, at 410, receiving at a security analysis device, traffic flows from a plurality of entities destined for a plurality of users, at 412, aggregating the\ntraffic flows into discrete bags of traffic, wherein the bags of traffic comprise a plurality of flows of traffic for a given user over a predetermined period of time, at 414, extracting features from the bags of traffic and aggregating the features into\nper-flow feature vectors, at 416, aggregating the per-flow feature vectors into per-destination domain aggregated vectors, at 418, combining the per-destination-domain aggregated vectors into a per-user aggregated vector, and, at 420, classifying a\ncomputing device used by the given user as infected with malware when indicators of compromise detected in the bags of traffic indicate that the per-user aggregated vector includes suspicious features among the extracted features\n In sum, described is a hierarchical Neural Network system capable of detecting network threats.  The system has unique practical properties.  I performs as high-precision detector of threats.  It is learned from training data with dramatically\nlower requirements on labels provided by human analysts--this makes it practical for Big Data analytics.  It is capable of learning and revealing novel Indicators of Compromise (IOCs) that are unlikely to be discovered by human analysts.  The system can\nbe implemented to consume various types of data.  An important feature underlying the ability to accomplish the foregoing, is to treat data samples in a hierarchy of groups (\"bags\"); individual hidden Neural Network layers then represent the respective\nbag granularity and learn otherwise invisible structures inside bags on the given level of granularity.  Only the last output layer uses labels.\n The methodology described herein is capable of learning from Big Data due to its ability to learn from very small number of labels describing very grossly a large amount of training data.  This removes a significant bottleneck caused by cost of\nhuman analysis.\n The methodology further learns to classify entities on various levels of abstraction; in the tested installment it provides verdicts about users (infected/clean); the verdict can be analyzed and explained.\n The methodology also automatically learns Indicators of Compromise (IOCs) that would otherwise be difficult or impossible to define by human analysts.\n The methodology is general, and has been verified on two types of input data.  In both cases its efficacy was proven.\n Finally, the methodology has the potential to increase accuracy of many products classifying users based on observed network connection.  In other embodiments, it has the potential to improve efficacy of sandbox solutions as well as boost the\nefficacy of security products as a whole when utilizing correlations between data of various type.\n FIG. 5 depicts an apparatus that is configured to operate as a security analysis device or apparatus that hosts malware detection logic 200 according to an example embodiment.  The apparatus may be implemented on a computer system 501.  The\ncomputer system 501 may be programmed to implement a computer based device.  The computer system 501 includes a bus 502 or other communication mechanism for communicating information, and a processor 503 coupled with the bus 502 for processing the\ninformation.  While the figure shows a single block 503 for a processor, it should be understood that the processor 503 represents a plurality of processors or processing cores, each of which can perform separate processing.  The computer system 501 may\nalso include a main memory 504, such as a random access memory (RAM) or other dynamic storage device (e.g., dynamic RAM (DRAM), static RAM (SRAM), and synchronous DRAM (SD RAM)), coupled to the bus 502 for storing information and instructions to be\nexecuted by processor 503.  In addition, the main memory 504 may be used for storing temporary variables or other intermediate information during the execution of instructions by the processor 503.  Main memory may also be used to store logic\ninstructions or software for performing the operations shown in FIG. 4.\n The computer system 501 may further include a read only memory (ROM) 505 or other static storage device (e.g., programmable ROM (PROM), erasable PROM (EPROM), and electrically erasable PROM (EEPROM)) coupled to the bus 502 for storing static\ninformation and instructions for the processor 503.\n The computer system 501 may also include a disk controller 506 coupled to the bus 502 to control one or more storage devices for storing information and instructions, such as a magnetic hard disk 507, and a removable media drive 508 (e.g.,\nfloppy disk drive, read-only compact disc drive, read/write compact disc drive, compact disc jukebox, tape drive, and removable magneto-optical drive).  The storage devices may be added to the computer system 701 using an appropriate device interface\n(e.g., small computer system interface (SCSI), integrated device electronics (IDE), enhanced-IDE (E-IDE), direct memory access (DMA), or ultra-DMA).\n The computer system 501 may also include special purpose logic devices (e.g., application specific integrated circuits (ASICs)) or configurable logic devices (e.g., simple programmable logic devices (SPLDs), complex programmable logic devices\n(CPLDs), and field programmable gate arrays (FPGAs)), that, in addition to microprocessors and digital signal processors may individually, or collectively, are types of processing circuitry.  The processing circuitry may be located in one device or\ndistributed across multiple devices.\n The computer system 501 may also include a display controller 509 coupled to the bus 502 to control a display 510, such as a cathode ray tube (CRT) or liquid crystal display (LCD), for displaying information to a computer user.  The computer\nsystem 501 may include input devices, such as a keyboard 511 and a pointing device 512, for interacting with a computer user and providing information to the processor 503.  The pointing device 512, for example, may be a mouse, a trackball, or a pointing\nstick for communicating direction information and command selections to the processor 503 and for controlling cursor movement on the display 510.  In addition, a printer may provide printed listings of data stored and/or generated by the computer system\n501.\n The computer system 501 performs a portion or all of the processing operations of the embodiments described herein in response to the processor 503 executing one or more sequences of one or more instructions contained in a memory, such as the\nmain memory 504.  Such instructions may be read into the main memory 504 from another computer readable medium, such as a hard disk 507 or a removable media drive 508.  One or more processors in a multi-processing arrangement may also be employed to\nexecute the sequences of instructions contained in main memory 504.  In alternative embodiments, hard-wired circuitry may be used in place of or in combination with software instructions.  Thus, embodiments are not limited to any specific combination of\nhardware circuitry and software.\n As stated above, the computer system 501 includes at least one computer readable medium or memory for holding instructions programmed according to the embodiments presented, for containing data structures, tables, records, or other data\ndescribed herein.  Examples of computer readable media are compact discs, hard disks, floppy disks, tape, magneto-optical disks, PROMs (EPROM, EEPROM, flash EPROM), DRAM, SRAM, SD RAM, or any other magnetic medium, compact discs (e.g., CD-ROM), or any\nother optical medium, punch cards, paper tape, or other physical medium with patterns of holes, or any other medium from which a computer can read.\n Stored on any one or on a combination of non-transitory computer readable storage media, embodiments presented herein include software for controlling the computer system 501, for driving a device or devices for implementing the described\nembodiments, and for enabling the computer system 501 to interact with a human user (e.g., print production personnel).  Such software may include, but is not limited to, device drivers, operating systems, development tools, and applications software. \nSuch computer readable storage media further includes a computer program product for performing all or a portion (if processing is distributed) of the processing presented herein.\n The computer code may be any interpretable or executable code mechanism, including but not limited to scripts, interpretable programs, dynamic link libraries (DLLs), Java classes, and complete executable programs.  Moreover, parts of the\nprocessing may be distributed for better performance, reliability, and/or cost.\n The computer system 501 also includes a communication interface 513 coupled to the bus 502.  The communication interface 513 provides a two-way data communication coupling to a network link 514 that is connected to, for example, a local area\nnetwork (LAN) 515, or to another communications network 516.  For example, the communication interface 513 may be a wired or wireless network interface card or modem (e.g., with SIM card) configured to attach to any packet switched (wired or wireless)\nLAN or WWAN.  As another example, the communication interface 513 may be an asymmetrical digital subscriber line (ADSL) card, an integrated services digital network (ISDN) card or a modem to provide a data communication connection to a corresponding type\nof communications line.  Wireless links may also be implemented.  In any such implementation, the communication interface 513 sends and receives electrical, electromagnetic or optical signals that carry digital data streams representing various types of\ninformation.\n The network link 514 typically provides data communication through one or more networks to other data devices.  For example, the network link 514 may provide a connection to another computer through a local are network 515 (e.g., a LAN) or\nthrough equipment operated by a service provider, which provides communication services through a communications network 516.  The local network 514 and the communications network 516 use, for example, electrical, electromagnetic, or optical signals that\ncarry digital data streams, and the associated physical layer (e.g., CAT 5 cable, coaxial cable, optical fiber, etc.).  The signals through the various networks and the signals on the network link 514 and through the communication interface 513, which\ncarry the digital data to and from the computer system 501 may be implemented in baseband signals, or carrier wave based signals.  The baseband signals convey the digital data as unmodulated electrical pulses that are descriptive of a stream of digital\ndata bits, where the term \"bits\" is to be construed broadly to mean symbol, where each symbol conveys at least one or more information bits.  The digital data may also be used to modulate a carrier wave, such as with amplitude, phase and/or frequency\nshift keyed signals that are propagated over a conductive media, or transmitted as electromagnetic waves through a propagation medium.  Thus, the digital data may be sent as unmodulated baseband data through a \"wired\" communication channel and/or sent\nwithin a predetermined frequency band, different than baseband, by modulating a carrier wave.  The computer system 501 can transmit and receive data, including program code, through the network(s) 515 and 516, the network link 514 and the communication\ninterface 513.  Moreover, the network link 514 may provide a connection to a mobile device 517 such as a personal digital assistant (PDA) laptop computer, cellular telephone, or modem and SIM card integrated with a given device.\n In summary, in one form, a method is provided comprising: receiving, at a security analysis device, traffic flows from a plurality of entities destined for a plurality of users; aggregating the traffic flows into discrete bags of traffic,\nwherein the bags of traffic comprise a plurality of flows of traffic for a given user over a predetermined period of time; extracting features from the bags of traffic and aggregating the features into per-flow feature vectors; aggregating the per-flow\nfeature vectors into per-destination domain aggregated vectors; combining the per-destination-domain aggregated vectors into a per-user aggregated vector; and classifying a computing device used by a given user as infected with malware when indicators of\ncompromise detected in the bags of traffic indicate that the per-user aggregated vector for the given user includes suspicious features among the extracted features.\n In another form, an apparatus is provided comprising: a network interface unit configured to enable communications via a network; a memory configured to store logic instructions; and a processor, when executing the logic instructions, configured\nto receive traffic flows from a plurality of entities destined for a plurality of users; aggregate the traffic flows into discrete bags of traffic, wherein the bags of traffic comprise a plurality of flows of traffic for a given user over a predetermined\nperiod of time; extract features from the bags of traffic and aggregating the features into per-flow feature vectors; aggregate the per-flow feature vectors into per-destination domain aggregated vectors; combine the per-destination-domain aggregated\nvectors into a per-user aggregated vector; and classify a computing device used by a given user as infected with malware when indicators of compromise detected in the bags of traffic indicate that the per-user aggregated vector for the given user\nincludes suspicious features among the extracted features.\n In still another form, one or more non-transitory computer readable storage media are provided encoded with software comprising computer executable instructions and when the software is executed operable to: receive traffic flows from a\nplurality of entities destined for a plurality of users; aggregate the traffic flows into discrete bags of traffic, wherein the bags of traffic comprise a plurality of flows of traffic for a given user over a predetermined period of time; extract\nfeatures from the bags of traffic and aggregating the features into per-flow feature vectors; aggregate the per-flow feature vectors into per-destination domain aggregated vectors; combine the per-destination-domain aggregated vectors into a per-user\naggregated vector; and classify a computing device used by a given user as infected with malware when indicators of compromise detected in the bags of traffic indicate that the per-user aggregated vector for the given user includes suspicious features\namong the extracted features.\n The above description is intended by way of example only.  Various modifications and structural changes may be made therein without departing from the scope of the concepts described herein and within the scope and range of equivalents of the\nclaims.", "application_number": "15248252", "abstract": " Presented herein are techniques for classifying devices as being infected\n     with malware based on learned indicators of compromise. A method includes\n     receiving at a security analysis device, traffic flows from a plurality\n     of entities destined for a plurality of users, aggregating the traffic\n     flows into discrete bags of traffic, wherein the bags of traffic comprise\n     a plurality of flows of traffic for a given user over a predetermined\n     period of time, extracting features from the bags of traffic and\n     aggregating the features into per-flow feature vectors, aggregating the\n     per-flow feature vectors into per-destination domain aggregated vectors,\n     combining the per-destination-domain aggregated vectors into a per-user\n     aggregated vector, and classifying a computing device used by a given\n     user as infected with malware when indicators of compromise detected in\n     the bags of traffic indicate that the per-user aggregated vector for the\n     given user includes suspicious features among the extracted features.\n", "citations": ["7296288", "8555383", "20100284283", "20120210423", "20140280898", "20170099314"], "related": []}, {"id": "20180082106", "patent_code": "10372969", "patent_name": "Information processing apparatus, object recognition apparatus, method of\n     controlling information processing apparatus, and storage medium", "year": "2019", "inventor_and_country_data": " Inventors: \nInaba; Masaki (Tokyo, JP), Suzuki; Masahiro (Kawasaki, JP), Kobayashi; Kazuhiko (Yokohama, JP)  ", "description": "BACKGROUND OF THE INVENTION\n Field of the Invention\n The present invention relates to an information processing apparatus, an object recognition apparatus, a method of controlling an information processing apparatus, and a storage medium.\n Description of the Related Art\n Conventionally, a method for learning a feature or a pattern and for recognizing a target object from a depth image or an image in which the target object is captured is known.  However, in conventional methods, there is a problem in that a\nrecognition rate falls in a case where there is a large difference between an image prepared in advance and an image desired to be actually recognized.  For example, images greatly differ from each other in such cases in which a part of the target object\nis concealed by other objects, a surface of a target object reflects specularly causing saturation to occur, and the like.\n In contrast to this, a method for reproducing a state in which a target object is concealed by providing a plurality of appropriate masks to an image prepared in advance is proposed in Japanese Patent No. 4291757.  A recognition method that is\nrobust with respect to a concealed target object is realized by learning features and patterns from a plurality of images in which a concealed target object is reproduced.\n However, there are a huge number of variations of an appearance of a packaged object in a case where an object (packaged object) wrapped in a transparent packaging member (such as transparent vinyl or bubble wrap) or an object covered in a\npackaging member is the target object.  This is because packaging members are often non-rigid bodies whose shape changes and on whose surface specular reflections can easily occur, and thereby the appearance of the target object changes depending on\ncolor of the packaging member.  For this reason, there is a problem in that image recognition is still difficult if only masking of a region of a part of the target object is performed as in Japanese Patent No. 4291757.\n Also, although a method of capturing a large number of various patterns of appearances (images) of a packaged object by varying the orientation of the target object or the shape of the packaging member can be considered, there is a problem in\nthat it is cumbersome to capture such images.\n The present invention is conceived in view of the above-described problems, and provides a technique for reducing the effort in obtaining images (learning data) of an object that is covered by a transparent object.\nSUMMARY OF THE INVENTION\n According to one aspect of the present invention, there is provided an information processing apparatus comprising: an image generation unit configured to generate, based on a first image in which a transparent object having transparency is\ncaptured and a second image in which a target object is captured, a reproduced image in which the target object which is at least partially covered by the transparent object is reproduced; and a creation unit configured to create, based on the reproduced\nimage, a model for recognizing the target object which is at least partially covered by the transparent object.\n Further features of the present invention will become apparent from the following description of exemplary embodiments with reference to the attached drawings. BRIEF DESCRIPTION OF THE DRAWINGS\n FIG. 1 is a block diagram illustrating an example of a configuration of an information processing apparatus of a first embodiment.\n FIG. 2A-FIG. 2B are flowcharts for describing processing by the information processing apparatus of the first embodiment.\n FIG. 3 is a view for describing processing of an image generation unit of the first embodiment.\n FIG. 4 is a block diagram illustrating an example of a configuration of an information processing apparatus of a second embodiment.\n FIG. 5 is a flowchart for describing processing by the information processing apparatus of the second embodiment.\n FIG. 6 is a block diagram illustrating an example of a configuration of an information processing apparatus of a third embodiment.\n FIG. 7 is a flowchart for describing processing by the information processing apparatus of the third embodiment.\n FIG. 8 is a view for describing a GUI that the information processing apparatus of the third embodiment illustrates.\n FIG. 9 is a block diagram illustrating an example of a configuration of an information processing apparatus of the fourth embodiment.\n FIG. 10 is a flowchart for describing processing by the information processing apparatus of the fourth embodiment.\n FIG. 11 is a block diagram illustrating an example of a configuration of an information processing apparatus of a fifth embodiment.\n FIG. 12A-FIG. 12B are flowcharts for describing processing by the information processing apparatus of the fifth embodiment.\nDESCRIPTION OF THE EMBODIMENTS\n An exemplary embodiment(s) of the present invention will now be described in detail with reference to the drawings.  It should be noted that the relative arrangement of the components, the numerical expressions and numerical values set forth in\nthese embodiments do not limit the scope of the present invention unless it is specifically stated otherwise.\n (First Embodiment)\n An information processing apparatus described in the first embodiment is an object recognition apparatus for recognizing an object by generating an image (hereinafter referred to as a reproduced image) in which an appearance (outer appearance)\nof a packaged object is reproduced from an image of a target object and an image of a packaging member, and learning, by a CNN (Convolution Neural Network) which is a type of network used in deep learning, a feature or pattern from the generated\nreproduced image.  A useful feature or pattern can be learned for recognition because the reproduced image is similar to an appearance (outer appearance) of an actual packaged object.  Also, effort in capturing images is reduced because various patterns\nof appearances can be generated by image processing.  As a result, recognition of an object wrapped in a packaging member can be realized at a lower cost.\n [Apparatus Configuration]\n An example of an object recognition apparatus 1000 equipped with an information processing apparatus 1100 of the first embodiment is illustrated by the block diagram of FIG. 1.  The object recognition apparatus 1000 is equipped with an image\ncapturing apparatus 1, a recognition unit 2, and the information processing apparatus 1100.\n The image capturing apparatus 1 is an apparatus that obtains a color (RGB) image of an image capturing target.  The image capturing target, at a time of recognition, is a target object (packaged object) wrapped in a packaging member, and at a\ntime of learning, is the target object not wrapped in the packaging member and the packaging member not wrapping the target object.  The captured image is sent to the recognition unit 2 in order to recognize a target object within the image or is sent to\nthe information processing apparatus 1100 in order to learn a CNN model used for recognition.  The recognition unit 2, by inputting an image sent from the image capturing apparatus 1 to a CNN model stored in the information processing apparatus 1100,\nrecognizes whether or not the target object is included within the image.\n The information processing apparatus 1100 is equipped with an image generation unit 101, an image storage unit 102, a learning unit 103, and a model storage unit 104.  The image generation unit 101 obtains the image of the target object and the\nimage of the packaging member from the image capturing apparatus 1, and then generates a reproduced image of the packaged object from these images.  Then, it stores the generated reproduced image to the image storage unit 102.\n The image storage unit 102 stores the reproduced image generated by the image generation unit 101.  The learning unit 103 creates and learns a CNN model by using reproduced images stored in the image storage unit 102.  Then, it stores the\nlearned CNN model to the model storage unit 104.  The model storage unit 104 stores information of the CNN model created by the learning unit 103.\n [Recognition Processing]\n Next, a procedure of recognition processing by the object recognition apparatus 1000 according to a first embodiment is described with reference to a flowchart of FIG. 2A.\n (Step S11)\n In step S11, the object recognition apparatus 1000 checks whether or not a CNN model is stored in the information processing apparatus 1100 and determines whether or not the CNN model has already been learned.  In a case where the CNN model has\nalready been learned, the CNN model is read from the information processing apparatus 1100, and step S13 is transitioned to.  In a case where the CNN model has not already been learned, step S12 is transitioned to.\n (Step S12)\n In step S12, the information processing apparatus 1100 performs learning of a CNN model.  Details are described later.\n (Step S13)\n In step S13, the image capturing apparatus 1 captures an image in which a target object or the like appears and sends the image to the recognition unit 2.\n (Step S14)\n In step S14, the recognition unit 2 inputs the image sent from the image capturing apparatus 1, and obtains a binary value indicating whether or not the target object appears within the image by performing an arithmetic operation of the already\nlearned CNN model.  Then, it is decided whether or not the target object appears within the image according to the obtained binary value.  Then, a recognition result is outputted and the processing ends.\n [Learning Processing]\n Next, a procedure of learning processing by the information processing apparatus 1100 according to a first embodiment is described with reference to a flowchart of FIG. 2B.\n (Step S121)\n In step S121, the information processing apparatus 1100 obtains images of the target object captured by the image capturing apparatus 1.  Prior to this, the image capturing apparatus 1 captures the images of the target object not wrapped in a\npackaging member.  At this time, in order to be recognizable from various directions, various changes to the relative position and orientation between the image capturing apparatus 1 and the target object are made and images are obtained for each of the\norientations.  For example, the image capturing apparatus 1 may perform respective captures after moving by a robot to positions sampled at regular intervals on a spherical surface centered on the target object.  The image capturing apparatus 1 sends the\ncaptured images to the image generation unit 101.\n (Step S122)\n In step S122, the information processing apparatus 1100 obtains an image of the packaging member captured by the image capturing apparatus 1.  Prior to this, the image capturing apparatus 1 captures the image of the packaging member.  At this\ntime, the same packaging member that will actually be used for packaging is spread out and captured in order to obtain a packaging member image having a variety of appearances.  It is desirable to spread out the packaging member in a state close to that\nof the packaging of the packaged object.  In other words, the packaging member to be captured may be stretched and spread out in a case where the packaging member is cleanly stretched in the packaging, and the packaging member to be captured may also be\nspread out such that wrinkles remain to a certain extent in a case where wrinkles will remain on the packaging member in the packaging.  Also, it is desirable to make a background white in order to reduce an influence of the color of the background at\nthe time of capturing.  For example, an image such as reference numeral 42 of FIG. 3 is obtained when a transparent vinyl bag is spread out and captured.  Within this image, a variety of textures (appearances) that appear due to shape distortion of a\ntransparent vinyl bag and the light source environment of surroundings are included.  The image capturing apparatus 1 sends the captured image to the image generation unit 101.\n (Step S123)\n In step S123, the image generation unit 101 generates reproduced images of the packaged object based on the image of the packaging member and each of the images of the target object sent from the image capturing apparatus 1.  A partial image\nextracted from the image of the packaging member is superimposed on each of the images of the target object captured in the various orientations to produce the reproduced images.  Hereinafter, a process by which a reproduced image is generated is\ndescribed by using FIG. 3.  In FIG. 3, reference numeral 41 represents an image of a target object of a certain orientation, the reference numeral 42 represents the image of a packaging member, reference numeral 43 represents an extraction region, and\nreference numeral 44 represents a generated reproduced image.\n Firstly, the position of the extraction region 43 is decided at random from the image 42 of the packaging member, and an image C which is a part extracted from the image 42 of the packaging member is obtained.  The size of the extraction region\n43 may be made to be large enough that the target object fits therein.  Then, the obtained image C is superimposed on the image I of the target object.  When the reproduced image is represented by R, the reproduced image is generated based on the\nfollowing equation.\n .times..times..function..alpha..times..times..function..alpha..times..fun- ction..function.&lt;.function..function..gtoreq.  ##EQU00001##\n Here, x and y represent pixel positions within the image, .alpha.  represents a coefficient when alpha blending between the image C of the packaging member and an image I of the target object is performed, and t represents a threshold for\ndetermining whether or not saturation (specular reflection) is occurring.  Specifically, the reproduced image is generated by, for a pixel whose luminance value is greater than or equal to the threshold, making the luminance value of the pixel be the\nluminance value of the reproduced image.  Because the target object will become invisible in a case where light on the surface of the packaging member is specularly reflected, composition processing is separated according to whether or not the light is\nspecularly reflected.  By performing such an image composition, a concealment of the target object by a specular reflection of the surface of the packaging member or a change in color of the target object due to a color of the packaging member is\nrepresented, and a reproduced image such as reference numeral 44 is generated.  It is possible to generate a plurality of patterns of reproduced images of the packaged object by compositing while randomly changing the position of the extraction region\n43.\n It is possible to generate a large number of reproduced images of the packaged object by performing a sequence of composition processes with respect to each orientation of the target object image.  The image generation unit 101 sends the\ngenerated reproduced images to the image storage unit 102.\n (Step S124)\n In step S124, the image storage unit 102 stores the reproduced images generated by the image generation unit 101 within a memory.\n (Step S125)\n In step S125, the learning unit 103 creates and learns a CNN model (creation of a model) by using the reproduced images stored in the image storage unit 102.  In the present embodiment, the creation and the learning of the model is referred to\nas obtaining CNN model parameters.  Regarding the design of the CNN model, an RGB three-dimensional image of the reproduced images is taken as input, three convolution layers and two fully-connected layers are used, and the output is a model for\ndiscrimination of two classes corresponding to whether or not the input is a target object, for example.\n The learning of the CNN model takes, as learning data input, data that labels a reproduced image True and labels an image that is not the target object False, and optimizes a backpropagation used commonly in deep learning.  For an image that is\nnot the target object, an image uploaded to the Web may be used for example.  The learning unit 103 sends the CNN model that is learned in this way to the model storage unit 104.\n (Step S126)\n In step S126, the model storage unit 104 stores information of the CNN model learned in the learning unit 103 within the memory, and the processing is ended.\n As described above, by virtue of the present embodiment, reproduced images of a packaged object can be generated from images of a target object and an image of a packaging member, and by learning a CNN model from the reproduced images, a\npackaged object can be recognized without requiring the effort of capturing a large number of images of packaged objects.\n [Variation of First Embodiment]\n Although variation (multiformity) of appearance is obtained by spreading out and then capturing the packaging member in step S122, limitation to this is not required.  A plurality of images may be captured while each time causing the shape of\nthe packaging member to change, and a plurality of images may be captured while each time causing a light source environment of the surroundings to change.\n Although the reproduced image is generated based on Equation 1 in step S123, a composition method is not limited to this.  For example, the reproduced image may be generated by a product as in Equation 2, and additionally, an image A of a\nbackground against which the target object or the packaging member is placed may be captured, and a division by the background color may be performed for normalization as in Equation 3.  Also, it may not be necessary for the entire packaging member to be\ntransparent, and there may be some regions that are not transparent.  For a non-transparent region, a luminance value C(x,y) of the image of the packaging member may be set to be the luminance value R(x,y) of the reproduced image.\n .times..times..function..function..times..function..function.&lt;.functio- n..function..gtoreq..times..times..function..times..function..function..fu- nction.&lt;.function..function..gtoreq.  ##EQU00002##\n Additionally, in a case where there is CAD data of the target object whose reflectance is determined, the reproduced image may be generated by setting a virtual surface on which the packaging member is reproduced around the target object from\nthe actual transparency and color of the packaging member (a transparent object having a transparency) and then performing CG rendering.  At this time, the CG rendering may be performed after setting a virtual light source environment that reproduces an\nactual light source environment obtained by a zenith camera or the like.\n (Second Embodiment)\n An information processing apparatus described in the second embodiment is an object recognition apparatus for recognizing an object by learning, by a CNN model, a feature or a pattern from a generated reproduced image, similarly to the first\nembodiment.  However, the second embodiment differs in that it further includes an image determination unit for determining whether or not there is sufficient variation (multiformity) within an image of the captured packaging member for recognition.  It\nis possible to further reduce the effort of capturing because it can be understood how many images of the packaging member it is sufficient to obtain.  As a result, recognition of an object wrapped in a packaging member can be realized at a lower cost.\n [Apparatus Configuration]\n An example of an object recognition apparatus 2000 equipped with an information processing apparatus 2100 of the second embodiment is illustrated by the block diagram of FIG. 4.\n The object recognition apparatus 2000 is equipped with the image capturing apparatus 1, the recognition unit 2, a display apparatus 3, and the information processing apparatus 2100.  Note, because the image capturing apparatus 1 and the\nrecognition unit 2 are substantially the same as those in the first embodiment, description thereof is omitted.  The display apparatus 3 displays information such as text sent from the information processing apparatus 2100.\n The information processing apparatus 2100 is equipped with an image generation unit 201, an image storage unit 202, a learning unit 203, a model storage unit 204, and an image determination unit 205.  Note, because the image generation unit 201,\nthe image storage unit 202, the learning unit 203, and the model storage unit 204 are substantially the same as the image generation unit 101, the image storage unit 102, the learning unit 103, and the model storage unit 104 of the first embodiment,\ndescription thereof is omitted.\n The image determination unit 205 receives images of the target object and an image of the packaging member captured by the image capturing apparatus 1, and determines whether or not there is sufficient variation (multiformity) within the images\nof the packaging member for recognition.  Information indicating that the image is sent to the image generation unit 201 when this is the case and that it is necessary to capture an additional packaging member when this is not the case is caused to be\ndisplayed on the display apparatus 3.\n [Recognition Processing]\n Because recognition processing by the object recognition apparatus 2000 of the second embodiment is substantially the same as the recognition processing of the first embodiment (FIG. 2A), description thereof is omitted.\n Next, a procedure of learning processing by the information processing apparatus 2100 according to a second embodiment is described with reference to a flowchart of FIG. 5.  Note, because step S221, step S225, step S226, and step S227 are\nsubstantially respectively the same as step S121, step S124, step S125, and step S126 of the first embodiment, description thereof is omitted.\n (Step S222)\n In step S222, the information processing apparatus 1100 obtains an image of the packaging member captured by the image capturing apparatus 1.  Prior to this, the image capturing apparatus 1 captures the image of the packaging member and sends\nthe captured image to the image determination unit 205 similarly to step S122 of the first embodiment.  Also, an image of the packaging member is recaptured and the captured image is sent to the image determination unit 205 each time a request for an\nadditional capture comes from the image determination unit 205.  At this time, because variation does not increase if an additional capture is performed without changing the shape of the packaging member, the packaging member is spread out again and\nrecaptured.\n (Step S223)\n In step S223, the image determination unit 205 determines from the image of the packaging member sent from the image capturing apparatus 1 whether or not there is sufficient variation within images of the packaging member.\n In a case where the image of the packaging member is newly sent from the image capturing apparatus 1, images of the packaging member sent in the past are also included, and it is confirmed whether or not an image similar to the image of the\nextraction region 43 of a certain position also exists within an extraction region of another position.  For example, a sum of absolute differences (SAD) for luminance differences between extraction regions may be obtained, extraction regions may be\ntreated as similar to each other if the SAD is within a certain threshold.  Because in a case where there is not a combination of similar extraction regions, the extraction regions are isolated patterns of appearances, it can be considered that there is\nstill variation similar to those extraction regions.  Accordingly, display control for displaying information indicating that an additional capture of the packaging member is necessary is performed on the display apparatus 3 to convey this to the user,\nand a request for additional capture to the image capturing apparatus 1 is made.\n Conversely, in a case where there is no isolated pattern of appearances, it can be considered that all possible patterns of appearances have been obtained without exception.  Accordingly, information indicating that images of sufficient\nvariation for recognition have been obtained is displayed on the display apparatus 3 and is thereby conveyed to the user.  With such a configuration, the user can easily determine whether there are enough images.  Also, images of the target object and\nimages of the packaging member obtained previously are sent to the image generation unit 201.\n (Step S224)\n In step S224, the image generation unit 201 generates reproduced images of the packaged object based on the image of the packaging member and each of the images of the target object sent from the image determination unit 205.  Although in\ngeneral reproduced images are generated similarly to step S123 of the first embodiment, their generation differs in that, because there are a plurality of packaging member images, a position of the extraction region 43 among the plurality of packaging\nmember images is randomly selected.  The generated reproduced images are sent to the image storage unit 102.\n As described above, by virtue of the present embodiment, it is possible to determine whether or not there is sufficient variation within images of a packaging member for recognition, and by feeding this back to the user, it is possible to\nrecognize a packaged object while further reducing the effort for capturing an image of the packaging member.\n [Variation of Second Embodiment]\n Although an SAD is used as an indicator for determining whether or not an extraction region is similar in step S223, limitation to this is not required.  Any indicator that represents a degree of similarity such as an SSD (Sum of Squared\nDifferences), an NCC (Normalized Cross-Correlation), or a ZNCC (Zero-mean Normalized Cross-Correlation) may be used.\n Also, although in the second embodiment the image determination unit 205 is provided, and it is determined whether images of a sufficient variation for recognition have been obtained, it is not necessary to limit to this.  Additionally, test\ndata may be prepared by capturing a plurality of images of the packaged object and it may be decided whether or not an additional capture should be performed in accordance with a recognition rate of the test data.  Firstly, a reproduced image is\ngenerated from an image of the target object and an image of the packaging member already captured, and a CNN model is learned.  Then, the learned CNN model is used and test data recognition is performed to obtain a recognition success rate. \nConfiguration may be taken to indicate the recognition rate to the user and allow the user to decide whether or not an additional capture is performed, and configuration may be taken so as to perform additional captures until a predetermined recognition\nrate is reached.\n (Third Embodiment)\n An information processing apparatus described in a third embodiment is an object recognition apparatus for recognizing an object by learning, by using a CNN model, a feature or a pattern from generated reproduced images, similarly to the first\nembodiment.  However, the third embodiment differs in that an image selection unit by which a user adjusts a parameter for generating a reproduced image and decides whether or not to use a generated reproduced image for learning is further included.  It\nis possible to recognize with better accuracy a packaged object at a lower cost by learning after a user generates and selects an appropriate reproduced image.\n [Apparatus Configuration]\n An example of an object recognition apparatus 3000 equipped with an information processing apparatus 3100 of the third embodiment is illustrated by the block diagram of FIG. 6.  The object recognition apparatus 3000 is equipped with the image\ncapturing apparatus 1, the recognition unit 2, the display apparatus 3, and the information processing apparatus 3100.  Note, because the image capturing apparatus 1 and the recognition unit 2 are substantially the same as those in the first embodiment,\ndescription thereof is omitted.  The display apparatus 3 displays a reproduced image sent from the information processing apparatus 3100 and displays a UI (user interface) for adjusting a parameter (such as .alpha.  or t in Equation 1) in order to\ngenerate the reproduced image.\n The information processing apparatus 3100 is equipped with an image generation unit 301, an image storage unit 302, a learning unit 303, a model storage unit 304, and an image selection unit 306.  Note, because the image generation unit 301, the\nimage storage unit 302, the learning unit 303, and the model storage unit 304 are substantially the same as the image generation unit 101, the image storage unit 102, the learning unit 103, and the model storage unit 104 of the first embodiment,\ndescription thereof is omitted.\n The image selection unit 306 receives reproduced images that the image generation unit 301 generated and presents the reproduced images to the user via the display apparatus 3.  Also, generation and display of the reproduced images are performed\nagain in accordance with a parameter adjustment by the user.  A reproduced image to be used for learning is ultimately selected by the user and sent to the image storage unit 302.\n [Recognition Processing]\n Because recognition processing by the object recognition apparatus 3000 of the third embodiment is substantially the same as the recognition processing of the first embodiment (FIG. 2A), description thereof is omitted.\n Next, a procedure of learning processing by the information processing apparatus 3100 according to the third embodiment is described with reference to a flowchart of FIG. 7.  Note, because step S321, step S322, step S323, step S325, step S326,\nand step S327 are substantially respectively the same as step S121, step S122, step S123, step S124, step S125, and step S126 of the first embodiment, description thereof is omitted.\n (Step S323-2)\n In step S323-2, the image selection unit 306 presents the reproduced images sent from the image generation unit 301 to the user.  Then, it is determined whether or not a reproduced image is to be regenerated in accordance with feedback from the\nuser (user instruction).  Hereinafter, processing of the image selection unit 306 is described using FIG. 8.  In FIG. 8, reference numeral 81 represents a screen presented to the user, reference numeral 82 represents a display region in which reproduced\nimages are lined up and displayed, reference numeral 83 represents a reproduced image selected for use in learning, reference numerals 84 and 85 represent UIs for adjusting parameters, and reference numeral 86 represents a decide button pressed when\nadjustment and selection of reproduced images has ended.  Operation of a slide bar for a parameter adjustment or operation for selecting a button or the like is performed by a UI device (a mouse for example) (not shown).\n Firstly, the user, by changing the UI 84 or the UI 85 while viewing the reproduced images displayed on the display region 82, performs adjustments to parameters so that reproduced images closer to the actual appearance of a packaged object can\nbe obtained.  Specifically, the parameters to be adjusted here are a and t in Equation 1.  Step S323 is returned to and the image generation unit 301 is caused to generate the reproduced images, and the reproduced images are displayed again on the\ndisplay region 82 in accordance with a change of the parameters.\n (Step S324)\n In step S324, the image selection unit 306 selects the reproduced images 83 to be used in learning from the display region 82 in accordance with an instruction of the user.  Although over-learning or a failure to learn result when only similar\nimages are learned or images that differ greatly in actual appearance are used to learn, it is possible to suppress such occurrences by selection of appropriate images by the user.  When the user presses the decide button 86 and all selection is ended,\nthe selected reproduced images are sent to the image storage unit 302.\n As described above, by virtue of the present embodiment, learning is performed by using reproduced images that are close in appearance to an actual packaged object by the user adjusting the appearance of the reproduced images and selecting\nreproduced images to be used for learning.  Accordingly, it is possible to recognize a packaged object with a higher accuracy at a lower cost.\n (Fourth Embodiment)\n An information processing apparatus described in the fourth embodiment is an object recognition apparatus for recognizing an object by learning, by using a CNN model, a feature or a pattern from a generated reproduced image, similarly to the\nfirst embodiment.  However, the fourth embodiment differs in that a reproduced image is generated by considering a shape of the target object.  By generating and learning a reproduced image that is closer to the actual appearance by considering the shape\nof the target object, it is possible to recognize with better accuracy a packaged object at a lower cost.\n [Apparatus Configuration]\n An example of an object recognition apparatus 4000 equipped with an information processing apparatus 4100 of the fourth embodiment is illustrated by the block diagram of FIG. 9.  The object recognition apparatus 4000 is equipped with the image\ncapturing apparatus 1, the recognition unit 2, a three-dimensional measurement apparatus 4, and the information processing apparatus 4100.  Note, because the image capturing apparatus 1 and the recognition unit 2 are substantially the same as those in\nthe first embodiment, description thereof is omitted.\n The three-dimensional measurement apparatus 4 is an apparatus for obtaining a depth image.  A calibration between the three-dimensional measurement apparatus 4 and the image capturing apparatus 1 is performed in advance, and a color image\ncaptured by the image capturing apparatus 1 and a depth image obtained by the three-dimensional measurement apparatus 4 are already aligned.  In other words, a certain pixel (x, y) of the color image and the same pixel (x, y) of the depth image indicate\nthe same object.  The obtained depth image is sent to the information processing apparatus 4100.\n The information processing apparatus 4100 is equipped with an image generation unit 401, an image storage unit 402, a learning unit 403, a model storage unit 404, and a normal obtainment unit 407.  Note, because the image storage unit 402, the\nlearning unit 403, and the model storage unit 404 are substantially the same as the image storage unit 102, the learning unit 103, and the model storage unit 104 of the first embodiment, description thereof is omitted.\n The image generation unit 401 uses images of the target object and an image of the packaging member sent from the image capturing apparatus 1 and normal information sent from the normal obtainment unit 407 to generate a reproduced image of the\npackaged object.  Then, it sends the generated reproduced image to the image storage unit 402.\n The normal obtainment unit 407 calculates and obtains normal information for each pixel from the depth image of the target object sent from the three-dimensional measurement apparatus 4, and sends it to the image generation unit 401.\n [Recognition Processing]\n Because recognition processing by the object recognition apparatus 4000 of the fourth embodiment is substantially the same as the recognition processing of the first embodiment (FIG. 2A), description thereof is omitted.\n Next, a procedure of learning processing by the information processing apparatus 4100 according to a fourth embodiment is described with reference to a flowchart of FIG. 10.  Note, because step S421, step S426, step S427, and step S428 are\nsubstantially the same as each of step S121, step S124, step S125, and step S126 of the first embodiment, description thereof is omitted.\n (Step S422)\n In step S422, the information processing apparatus 4100 obtains a depth image of the target object obtained by the three-dimensional measurement apparatus 4.  Prior to this, the three-dimensional measurement apparatus 4 obtains the depth image\nof the target object from the same image capture position as in step S421.  Then, the obtained depth image is sent to the normal obtainment unit 407.\n (Step S423)\n In step S423, the information processing apparatus 4100 obtains an image of the packaging member captured by the image capturing apparatus 1.  Prior to this, the image capturing apparatus 1 captures the image of the packaging member.  At this\ntime, because the appearance (hereinafter represented by (r, g, b)) of the packaging member of each normal direction (hereinafter represented by (nx, ny, nz)) is obtained, capturing is performed such that the packaging member spread out into a spherical\nshape appears at the center.  The appearances of the packaging member corresponding to each normal (a list in which (r, g, b)s corresponding to (nx, ny, nz)s are arrayed) is obtained by capturing such images.  The captured images are sent to the image\ngeneration unit 401.\n (Step S424)\n In step S424, the normal obtainment unit 407 calculates and obtains normals (nx, ny, nz) for each pixel (hereinafter, a pixel position is represented by (x, y)) from the depth image of the target object sent from the three-dimensional\nmeasurement apparatus 4.  For the normal obtainment, depth values in the vicinity of each pixel may be converted to a three-dimensional point group and principal component analysis may be performed on the three-dimensional point group for example.  A\nnormal for each pixel (a list in which (nx, ny, nz)s corresponding to (x, y)s are arrayed) obtained in this way is sent to the image generation unit 401.\n (Step S425)\n In step S425, the image generation unit 401 uses images of the target object and an image of the packaging member sent from the image capturing apparatus 1 and normal information sent from the normal obtainment unit 407 to generate a reproduced\nimage of the packaged object.  For each pixel (x, y) of the image of the target object, in accordance with the normal information ((nx, ny, nz) corresponding to (x, y)), the corresponding appearance of the packaging member (the (r, g, b) corresponding to\n(nx, ny, nz)) is obtained and is superimposed in accordance with Equation 1.  At this time, a plurality of patterns of reproduced images are generated by adding Gaussian noise to the normal of each pixel ((nx, ny, nz) corresponding to (x, y)) because it\ncan be considered that, for actual packaged objects, normal directions will vary due to shape distortions of the packaging member.  The generated reproduced images are sent to the image storage unit 402.\n As described above, by virtue of the present embodiment, learning is performed by using reproduced images that are close to the actual appearance by generating reproduced images considering the shape of the target object.  Accordingly, it is\npossible to recognize a packaged object with a higher accuracy at a lower cost.\n (Fifth Embodiment)\n An information processing apparatus described in the fifth embodiment is an object recognition apparatus for recognizing an object by learning, by a CNN model, a feature or a pattern from a generated reproduced image, similarly to the first\nembodiment.  However, the fifth embodiment differs in that, in addition to the appearance of the packaged object, the shape of the packaged object is reproduced from a depth image of a target object and a depth image of the packaging member, and the\nreproduced shape is also included to learn a feature or pattern.  It is possible to recognize with better accuracy a packaged object at a lower cost by also learning a feature or pattern of the shape in addition to the appearance of the object.\n [Apparatus Configuration]\n An example of an object recognition apparatus 5000 equipped with an information processing apparatus 5100 of the fifth embodiment is illustrated by the block diagram of FIG. 11.  Note, because the object recognition apparatus 5000 is\nsubstantially the same as those in the fourth embodiment, description thereof is omitted.\n The information processing apparatus 5100 is equipped with an image generation unit 501, an image storage unit 502, a learning unit 503, a model storage unit 504, and a shape information generation unit 508.  Note, because the image generation\nunit 501 and the model storage unit 504 are substantially the same as the image generation unit 101 and the model storage unit 104 of the first embodiment, description thereof is omitted.\n The image storage unit 502 stores the reproduced images sent from the image generation unit 501 and reproduced shapes sent from the shape information generation unit 508.  The learning unit 503 creates and learns a CNN model by using the\nreproduced images and the reproduced shapes stored in the image storage unit 502.  Then, it sends the learned CNN model to the model storage unit 504.\n The shape information generation unit 508 receives a depth image of the target object and a depth image of the packaging member sent from the three-dimensional measurement apparatus 4 and generates a reproduced shape of the packaged object from\nthese images.  The generated reproduced shape is sent to the image storage unit 502.\n [Recognition Processing]\n Next, a procedure of recognition processing by the object recognition apparatus 5000 according to a fifth embodiment is described with reference to a flowchart of FIG. 12A.  Note, because step S51, step S52, and step S53 are substantially the\nsame as step S11, step S12, and step S13 of the first embodiment, description thereof is omitted.\n (Step S54)\n In step S54, the three-dimensional measurement apparatus 4 obtains a depth image in which a target object or the like appears, and sends the image to the recognition unit 2.\n (Step S55)\n In step S55, the recognition unit 2 inputs the image sent from the image capturing apparatus 1 and the depth image sent from the three-dimensional measurement apparatus 4, and obtains a binary value indicating whether or not the target object\nappears within the image by performing an arithmetic operation of an already learned CNN model.  Then, it is decided whether or not the target object appears within the image according to the obtained binary value.  Then, a recognition result is\noutputted and the processing ends.\n [Learning Processing]\n Next, a procedure of learning processing by the information processing apparatus 5100 according to a fifth embodiment is described with reference to a flowchart of FIG. 12B.  Note, step S521, step S523, step S525, and step S529 are substantially\nrespectively the same as step S121, step S122, step S123, and step S126 of the first embodiment.  Description thereof is omitted.\n (Step S522)\n In step S522, the information processing apparatus 1100 obtains a depth image of the target object captured by the three-dimensional measurement apparatus 4.  Prior to this, the three-dimensional measurement apparatus 4 obtains the depth image\nof the target object from the same image capture position as in step S521.  Then, the obtained depth image is sent to the shape information generation unit 508.\n (Step S524)\n In step S524, the information processing apparatus 1100 obtains a depth image of the packaging member captured by the three-dimensional measurement apparatus 4.  Prior to this, the three-dimensional measurement apparatus 4 obtains the depth\nimage for the spread out packaging member from the same image capture position as in step S523.  Then, the obtained depth image is sent to the shape information generation unit 508.\n (Step S526)\n In step S526, the shape information generation unit 508 based on the depth image of the target object and the depth image of the packaging member sent from the three-dimensional measurement apparatus 4, generates a reproduced shape of the target\nobject wrapped in the packaging member.  Similarly to the generation of the reproduced images of step S525, in relation to each depth image of the target object captured at various orientations, a depth image that is a part that is extracted from the\ndepth image of the packaging member is superimposed to produce a reproduced shape.\n Here, although the general flow is the same as in step S525, the composition equation is different.  The depth image of the packaging member from which a part is extracted is represented by D, the depth image of the target object is represented\nby J, and the reproduced shape is represented by S, and the reproduced shape is generated by the following equation.  [EQUATION 4] S(x,y)=D(x,y)+J(x,y) (4)\n In other words, an amount of shape distortion simply due to being wrapped in the packaging member may be added.  Then, the generated reproduced shapes are sent to the image storage unit 502.\n (Step S527)\n In step S527, the image storage unit 502 stores the reproduced images generated by the image generation unit 501 and the reproduced shapes generated by the shape information generation unit 508 within the memory.\n (Step S528)\n In step S528, the learning unit 503 creates and learns a CNN model by using the reproduced images and the reproduced shapes stored in the image storage unit 502.  For a design of the CNN model, an RGBD four-dimensional image, in which the color\n(RGB) of a reproduced image and the depth (D) of a reproduced shape are combined are inputted, for example.  Also, a design may be taken such that three convolution layers are used, two fully-connected layers are used, and the output is a model for\ndiscrimination of two classes corresponding to whether or not the input is a target object, for example.  Because the following processing is the same as the processing of step S124 of the first embodiment, description thereof is omitted.\n As described above, by virtue of the present embodiment, an object that is difficult to recognize by appearance only can also be recognized using shape information by reproducing the shape in addition to the appearance of the packaged object to\nlearn a CNN model.  Accordingly, it is possible to recognize a packaged object with a higher accuracy at a lower cost.\n [Variation of Fifth Embodiment]\n Although the reproduced shapes are generated based on Equation 4 in step S526, a composition method is not limited to this.  Additionally, configuration may be such that a depth image B of a background, against which the target object or the\npackaging member is placed, is captured, and a reproduced shape is represented as in Equation 5, and configuration may be such that a thickness of the packaging member is measured in advance and the depth image is caused to expand by the thickness\namount.  [EQUATION 5] S(x,y)=D(x,y)+P(x,y)-B(x,y) (5)\n In a case where the reproduced image is generated by CG rendering as is illustrated in the variation of the first embodiment, depth values obtained from rendering result may be made to be the reproduced shape.\n Although a CNN model is learned by using both a reproduced image and a reproduced shape in the fifth embodiment, the CNN model may be learned by using only the reproduced shape.\n [First Variation]\n Although in all embodiments, an apparatus for obtaining an RGB color image is used as the image capturing apparatus 1, it is not necessary to be limited to this.  It may be an apparatus for obtaining a monochrome image or may be an apparatus for\nobtaining an infrared light image.\n [Second Variation]\n Although in all of the embodiments an image of the target object or the packaging member is captured by the apparatus itself, it is not necessary to be limited to this.  A published image database for research may be used or images uploaded on\nthe Web may be used.\n [Third Variation]\n Although the extraction region 43 uses the shape of a square in every embodiment, it is not necessary to be limited to this.  It may be circular or may conform to the external form of the target object.\n [Fourth Variation]\n Although in all of the embodiments, learning of a CNN model is performed after reproduced images or reproduced shapes are stored in the memory of the image storage unit 502, it is not necessary to be limited to this.  The learning may be\nperformed while generating the reproduced images or the reproduced shapes by inputting the generated shapes into the CNN model.\n [Fifth Variation]\n Although, in all of the embodiments, the design of the CNN model is such that there are three convolution layers and two fully-connected layers, it is not necessary to be limited to this.  There can be any number of overlapping convolution\nlayers and fully-connected layers and the design may be changed according to a purpose of recognition.\n [Sixth Variation]\n Although, in all of the embodiments, it is recognized whether or not a target object appears within an image, it is not necessary to be limited to this.  The position of the target object may be detected during a sliding window within the image\nby recognizing whether or not the target object appears within each window.  Also, an output of a CNN model need not be a binary value indicating whether or not the target object appears, and an orientation of the target object may also be recognized by\nincreasing the output so as to perform a classification of orientations also.  For example, the output may be made to be nine values: eight orientation classifications and a value indicating that the image is not the target object.  At this time,\nconfiguration may be such that the labeling of learning data is not the binary values of True and False, but rather is a number of labels proportional to the number of orientations to be recognized.\n [Seventh Variation]\n Although, in all of the embodiments, a CNN which is a type of deep learning network is learned and a recognition device is configured, but it is not necessary to be limited to this.  An LBP (Local Binary Pattern) may be used or a BoF (Bag of\nFeatures) or the like may be used as a feature.  Also, a decision tree may be used, or an SVM (Support Vector Machine) or the like may be used as a discrimination unit.\n &lt;Effects of the Embodiments&gt;\n By virtue of the first embodiment, reproduced images of a packaged object can be generated from images of a target object and an image of a packaging member, and by learning a CNN model from the reproduced images, a packaged object can be\nrecognized without requiring the effort of capturing a large number of images of packaged objects.\n By virtue of the second embodiment, it is possible to determine whether or not there is sufficient variation within images of a packaging member for recognition, and by feedback to the user, it is possible to recognize a packaged object while\nfurther reducing the effort for capturing an image of the packaging member.\n By virtue of the third embodiment, learning is performed by using reproduced images that are close in appearance to an actual packaged object by the user adjusting the appearance of the reproduced images and selecting reproduced images to be\nused for learning.  Accordingly, it is possible to recognize a packaged object with a higher accuracy at a lower cost.\n By virtue of the fourth embodiment, by generating reproduced images by considering the shape of the target object, learning is performed by using reproduced images closer to the actual appearance.  Accordingly, it is possible to recognize a\npackaged object with a higher accuracy at a lower cost.\n By virtue of the fifth embodiment, an object that is difficult to recognize by appearance alone can be recognized using shape information by reproducing the shape in addition to the appearance of the packaged object to learn a CNN model. \nAccordingly, it is possible to recognize a packaged object with a higher accuracy at a lower cost.\n &lt;Definitions&gt;\n It is not necessary that the entire packaging member in the present invention be transparent, and it is sufficient if a part thereof is transparent.  A part may also be opaque due to a bar code sticker being affixed to a transparent vinyl, and\nthe packaging member may be transparent only on a window portion as with an envelope with a window.  Also, transparency (approximately how transparent it is) of a transparent portion is not limited, and the packaging member may also have color.\n Also, although an example in which a target object is covered in a packaging member is illustrated in each of the embodiments, there is no limitation to a case in which a target object is completely covered in a packaging member, and there may\nbe cases in which a part of the target object is covered by a packaging member for example.  In such a case, the extraction region 43 extracted from the image 42 of the packaging member may be extracted as a region smaller than the target object, and a\nreproduced image may be generated by superimposing this extraction region 43 on the position of the target object that the packaging member covers.\n The image generation unit in the present invention may use any composition method if images similar to the actual appearance of a packaged object can be generated, as described in the first embodiment.  It may be an addition by alpha blending\nand may also be a multiplication.  Also, a reproduced image may be generated by CG rendering.  Furthermore, a reproduced image may be generated from the appearance of a packaging member for each normal direction, as described in the fourth embodiment.\n The image determination unit in the present invention may use any indicator if a degree of similarity between clipped images can be represented, as described in the second embodiment.  It may be an SAD or may be an SSD.  Also, test data may be\nprepared in advance, and a necessity of additional images may be determined by a recognition rate of the test data.\n The shape information generation unit in the present invention may use any composition method if shapes similar to the actual shape of a packaged object can be generated, as described in the fifth embodiment.  Configuration may be taken to add\ndepth images of the target object and the packaging member to expand the shape of the target object in proportion to the thickness of the packaging member.  Also, a reproduced shape may be generated by CG rendering.\n By virtue of the present invention, it is possible to reduce the effort for obtaining images (learning data) of an object covered by another object that is transparent.\n Other Embodiments\n Embodiment(s) of the present invention can also be realized by a computer of a system or apparatus that reads out and executes computer executable instructions (e.g., one or more programs) recorded on a storage medium (which may also be referred\nto more fully as a `non-transitory computer-readable storage medium`) to perform the functions of one or more of the above-described embodiment(s) and/or that includes one or more circuits (e.g., application specific integrated circuit (ASIC)) for\nperforming the functions of one or more of the above-described embodiment(s), and by a method performed by the computer of the system or apparatus by, for example, reading out and executing the computer executable instructions from the storage medium to\nperform the functions of one or more of the above-described embodiment(s) and/or controlling the one or more circuits to perform the functions of one or more of the above-described embodiment(s).  The computer may comprise one or more processors (e.g.,\ncentral processing unit (CPU), micro processing unit (MPU)) and may include a network of separate computers or separate processors to read out and execute the computer executable instructions.  The computer executable instructions may be provided to the\ncomputer, for example, from a network or the storage medium.  The storage medium may include, for example, one or more of a hard disk, a random-access memory (RAM), a read only memory (ROM), a storage of distributed computing systems, an optical disk\n(such as a compact disc (CD), digital versatile disc (DVD), or Blu-ray Disc (BD).TM.), a flash memory device, a memory card, and the like.\n While the present invention has been described with reference to exemplary embodiments, it is to be understood that the invention is not limited to the disclosed exemplary embodiments.  The scope of the following claims is to be accorded the\nbroadest interpretation so as to encompass all such modifications and equivalent structures and functions.\n This application claims the benefit of Japanese Patent Application No. 2016-181937, filed Sep. 16, 2016, which is hereby incorporated by reference wherein in its entirety.", "application_number": "15678659", "abstract": " An information processing apparatus comprises an image generation unit\n     configured to generate, based on a first image in which a transparent\n     object having transparency is captured and a second image in which a\n     target object is captured, a reproduced image in which the target object\n     which is at least partially covered by the transparent object is\n     reproduced; and a creation unit configured to create, based on the\n     reproduced image, a model for recognizing the target object which is at\n     least partially covered by the transparent object.\n", "citations": [], "related": []}, {"id": "20180107642", "patent_code": "10372814", "patent_name": "Methods and system for fast, adaptive correction of misspells", "year": "2019", "inventor_and_country_data": " Inventors: \nGliozzo; Alfio M. (Brooklyn, NY), Molino; Piero (New York, NY)  ", "description": "BACKGROUND\n Query spellchecking is a commonly-available feature in modern Internet search engines.  However, enterprise search engines often lack this functionality as implementing spellcheck without web query logs may require a language dependent and a\nvocabulary dependent solution that is difficult to implement.  Off-the-shelf, open source spellcheckers are insufficient as such software, while able to identify errors in grammatically-correct texts such as those used in a word processors, fails in\nadapting to a specific domain such as a particular enterprise.\n Thus, there is a need for an enterprise search engine query spell correction that is adaptable and customizable to customer-specific data.\nSUMMARY\n Embodiments are directed to a spellcheck module for an enterprise search engine.\n According to embodiments, a computer-implemented method, system, and computer program product are provided for adaptive correction of misspelling.  The system includes a processor coupled to one or more user devices, the processor configured to\nreceive user-generated search queries from the one or more user devices.  The computer program product comprises a computer readable storage medium having program instructions embodied therewith, the program instructions executable by a processor.  The\nprocessor is configured to implement the computer-implemented method of receiving a text for spelling analysis; creating a table of entries of words from a corpus including for each word, a number of occurrences, links to the occurrences, and alternative\nwords having character deletions; comparing an entry in the text to the table of entries having an edit distance from the entry and a minimum frequency of occurrence in the corpus to form a candidate set of entries; ranking the candidate set of entries\nutilizing a context sensitive scoring so that each candidate has a ranking; and ordering at least some of the each candidates based on the ranking to identify corrections to the entry.\n Additional features and advantages are apparent from the following detailed description that proceeds with reference to the accompanying drawings. BRIEF DESCRIPTION OF THE DRAWINGS\n The foregoing and other aspects of the present invention are best understood from the following detailed description when read in connection with the accompanying drawings.  For the purpose of illustrating the invention, there is shown in the\ndrawings embodiments that are presently preferred, it being understood, however, that the invention is not limited to the specific instrumentalities disclosed.  Included in the drawings are the following Figures:\n FIG. 1 is a diagram depicting a system in which an enterprise search engine and a spellcheck module may be implemented, according to embodiments herein;\n FIG. 2 is a block diagram illustrating components of a spellcheck module, according to embodiments herein;\n FIG. 3 is an example representation of the concept of edit distance, according to embodiments herein;\n FIGS. 4A-4C are example indexing operations, according to embodiments;\n FIGS. 5A-5D are example searching operations, according to embodiments;\n FIG. 6 illustrates non-contextual scoring aspects, according to an embodiment;\n FIG. 7 illustrates context sensitive scoring aspects, according to an embodiment;\n FIG. 8 is a runtime example of the implementation of a spellcheck module, according to embodiments herein;\n FIG. 9 is a screenshot output example of the implementation of a spellcheck module, according to embodiments herein;\n FIG. 10 is a flowchart illustrating a method for adaptive correction of misspelling, according to embodiments herein;\n FIG. 11 is a flowchart illustrating a method for adaptive correction of misspelling, according to an additional embodiment; and\n FIG. 12 is a block diagram of an example data processing system in which aspects of the illustrative embodiments may be implemented.\nDETAILED DESCRIPTION\n The present description and claims may make use of the terms \"a,\" \"at least one of,\" and \"one or more of,\" with regard to particular features and elements of the illustrative embodiments.  It should be appreciated that these terms and phrases\nare intended to state that there is at least one of the particular feature or element present in the particular illustrative embodiment, but that more than one can also be present.  That is, these terms/phrases are not intended to limit the description\nor claims to a single feature/element being present or require that a plurality of such features/elements be present.  To the contrary, these terms/phrases only require at least a single feature/element with the possibility of a plurality of such\nfeatures/elements being within the scope of the description and claims.\n In addition, it should be appreciated that the following description uses a plurality of various examples for various elements of the illustrative embodiments to further illustrate example implementations of the illustrative embodiments and to\naid in the understanding of the mechanisms of the illustrative embodiments.  These examples are intended to be non-limiting and are not exhaustive of the various possibilities for implementing the mechanisms of the illustrative embodiments.  It will be\napparent to those of ordinary skill in the art in view of the present description that there are many other alternative implementations for these various elements that may be utilized in addition to, or in replacement of, the example provided herein\nwithout departing from the spirit and scope of the present invention.\n According to embodiments disclosed herein, a spellcheck module for an enterprise search engine is provided.  Several constraints related to an enterprise search engine are addressed, according to embodiments provided herein, including:\nunavailability of query logs, language independence, operability for non-syntactical sentences (i.e., the solution cannot rely on classical linguistic features), availability of only raw text data (i.e., document collection indexed by the search engine),\nand speed.  According to embodiments, the spellcheck module employs a self-adaptable misspell detection strategy on customer data.  The spellcheck module is an advantageous feature for an enterprise search engine as, according to some research, 26% of\nsearch engine queries contain misspells, 25-40% of which are real words (e.g., \"them\" misspelled as \"then\") and the remaining of which are out of vocabulary words.  Misspell detection improves search results and strongly improves user experience.\n According to an embodiment disclosed herein, a candidate generation algorithm is provided for generating a candidate set of entries for an entry in a text that is part of a user query.  According to additional embodiments, candidate suggestion\nranking functions are provided for ranking the generated candidate set of entries.  In an embodiment, a candidate suggestion ranking function utilizes a non-contextual approach, while in another embodiment, a candidate suggestion ranking function\nutilizes a context sensitive approach.\n FIG. 1 is a diagram depicting a system 100 in which an enterprise search engine 110 and a spellcheck module 200 may be implemented, according to embodiments herein.  The system 100 includes the enterprise search engine 110 coupled directly or\nvia a network 120 to various computing devices 102a, 102b, 102c, and 102d.  Each device 102a, 102b, 102c, and 102d may be a user computing device from which one or more queries are generated and transmitted to the enterprise search engine 110. \nAdditional or fewer devices 102 may be included in the system 100.  A storage device 130 coupled to the enterprise search engine 110 is also provided in the system 100.  The storage device 130 receives and stores information related to, for example, the\nenterprise search engine 110 and the user devices 102a, 102b, 102c, and 102d.\n According to an embodiment, the enterprise search engine 110 comprises one or more processors and/or modules for implementing search functionality to provide results to user-generated queries.  One such module is the spellcheck module 200,\nfurther described in accordance with embodiments herein.\n FIG. 2 is a block diagram illustrating components of a spellcheck module 200 of the enterprise search engine 110, according to embodiments.  As shown in FIG. 2, the spellcheck module 200 is comprised of an index/text collection 230, which may,\nin an embodiment, be part of the storage device 130.  In an alternative embodiment, the index/text collection 230 is separate from the storage device 130.  The spellcheck module 200 further includes a candidate suggestion generation module 210 and a\ncandidate suggestion ranking module 220.  In an embodiment, a query is provided to the spellcheck module 200, which utilizes the modules 210 and 220 as well as the index/text collection 230, each further described herein, to generate results in the form\nof, for example, a ranked list of generated candidate entries.  The ranked list comprises candidates that may be an entry a user accidentally misspelled.\n The candidate suggestion generation module 210 generates a number of candidate words that may be the correction of the misspelled word.  The candidate suggestion generation module 210 implements an algorithm for indexing, searching, and storing\nterms from an index (i.e., 230) with a constrained edit distance.  According to an embodiment, words in the collection of documents of the enterprise search engine 110 (i.e., the index/text collection 230) that are within a distance of two are searched. \nThe distance of two is utilized based on the premise that 99.5% of all errors are within Damerau-Levenshtein edit distance of two from the misspelled word.  In other embodiments, other edit distances may be utilized by the candidate suggestion generation\nmodule 210.\n A maximum edit distance (\"med\") (e.g., two) and a threshold frequency (\"tf\") are defined prior to implementation of the spellcheck module 200.  The threshold frequency is a frequency of a term that will be considered as a candidate word.  A term\nthat appears only one time in a dataset is most probably a misspell, while a term that appears tf times is more likely to be a meaningful word.  In an embodiment, tf is dependent on the size of the dataset.\n FIG. 3 is an example representation 300 of the concept of edit distance for the word \"them,\" with an edit distance of one and an edit distance of two illustrated.  As shown in FIG. 3, the entries in the \"edit distance 1\" have one character\nremoved from the text \"them,\" and the entries in the \"edit distance 2\" comprise entries with a second character removed.  The delete of length one are the concatenations of the two substrings of a string obtained removing a character from the string. \nThe deletes of length n are the set of deletes of the deletes of length (n-1) of a string.  As another example, for the word \"home\" and for a med=1, the deletes are: \"ome\", \"hme\", \"hoe\", and \"hom\"; for med=2 they are: \"me\", \"he\", and \"ho.\"\n The indexing portion of the algorithm for the candidate suggestion generation module 210 comprises sorting through the dataset to collect words and their frequencies.  As soon as a word reaches the \"tf,\" added to the index are all of the deletes\nof length \"med\" of the word.\n When adding the deletes, the word that originated them is also tracked so that each delete entry contains a frequency of \"0\" and a set of words that originated them.  The frequency is needed because if an actual word in the dataset that is\nidentical to a previously added delete (e.g., \"me\" as a word and as a delete of \"home\") is encountered, its deletes need to be added when it reaches the threshold \"tf.\" At the same time, a word that was found in the dataset can become later the delete of\na longer word, so a link to the longer word is added.  Both real words and deletes have two values: frequency and set of links.  For memory efficiency, it is sufficient to store a link (an integer if words are mapped to integers) for deletes that never\nappear in the dataset, and an object containing an integer for frequency and a set/list/array of links.\n The indexing concept is illustrated in FIGS. 4A-4C, which show example indexing operations, according to an embodiment.  Diagram 400 of FIG. 4A illustrates the operation of indexing the word \"the\" and the deletes of length \"l;\" diagram 410 of\nFIG. 4B illustrates the operation of indexing the word \"he\" and the deletes of length \"l;\" and diagram 420 of FIG. 4C illustrates the operation of indexing the word \"her\" and the deletes of length \"l.\"\n The searching portion of the algorithm for the candidate suggestion generation module 210 comprises the following operation: a list of candidates are maintained with the misspell and its deletes added to it; the searched word is added to the\ncandidate list; for an element from the candidate list, if it is not already visited and it is more frequent than \"tf,\" it is added to the suggestion list; for every element in its links that is not already visited, the distance with respect to the\nsearched word is computed; if the distance is below the \"med,\" the linked word is added to the suggestion list; if the length of the candidate and the length of the misspell is less than the \"med,\" all of the deletes of the candidate are added to the\ncandidates and the process is repeated.\n FIGS. 5A-5D are example searching operations, according to an embodiment.  Diagram 500 of FIG. 5A illustrates an initial search query to search words at a distance less than or equal to one and with a frequency greater than zero from \"the.\" The\nnext search step is illustrated in diagram 510 of FIG. 5B, obtaining the words \"the\" links to.  As shown in diagram 520 of FIG. 5C, the deletes of \"the\" are then obtained; and as shown in diagram 530 of FIG. 5D, the words linked by the deletes of \"the\"\nare obtained.\n For the storage aspect of the candidate suggestion generation module 210, in addition to the words in the dataset, more items are stored in the index.  The amount of additional items for each word depends on the length of the word and the \"med.\"\nFor a maximum edit distance of two, an average word length of 5, and 100,000 dictionary entries, 1,500,000 deletes also need to be stored.\n The algorithm implemented by the candidate suggestion generation module 210, according to embodiments herein, trades memory for speed and recall: it is faster than the n-gram method (which has about 65% recall), but guarantees a 100% recall,\nwhile being 100 to 10,000 times faster than the fastest 100% recall method so far, at the cost of increased memory usage.\n Now turning to the candidate suggestion ranking module 220, in an embodiment, a candidate suggestion ranking function that utilizes a non-contextual approach uses a linear combination of distance and probability scores, while in another\nembodiment, a candidate suggestion ranking function that utilizes a context sensitive approach accounts for real-word misspells and adopts deep learning models.  In both approaches, only the documents in the collection are needed to build a model to be\nused for candidate ranking.\n According to an embodiment in which a non-contextual approach is utilized, the candidate suggestion ranking module 120 implements a weighted linear combination of Damerau-Levenshtein distance and smoothed term probability.  This linear\ncombination, weighted by a parameter alpha, is consistently better than using probability or distance alone, based on experimentation.  The absence of contextual features allows the system to be fully language independent and to be very fast, reaching\n90% average accuracy.\n According to an embodiment, a ranking score is a linear combination of Smoothed Term Probability (STP) and Edit Similarity (ES).  The probability of a term considering the log of the frequency and the log of all occurrences is computed:\nSTP=log(freq(correction))/log(freq(all)).\n The Edit Similarity is computed as: ES=1-(Damerau-Levenshtein(misspell,correction))/|misspell|.\n This normalizes the distance and transforms it into a similarity.\n The final score is computed as: wscore=(alpha)STP(correction)+(1-alpha)ES(correction,misspell).\n Alpha is a hyper parameter to be found, ideally on a validation set; in some embodiments, good results are obtained with alpha=0.65.\n FIG. 6 provides a diagram 600 of results of scoring in which the candidate suggestion ranking module 220 utilizes the non-contextual scoring aspects described herein.\n In an alternative embodiment in which a context sensitive approach is utilized, the candidate suggestion ranking module 120 implements a deep neural language model.  Given a sentence of length \"s,\" all possible candidate suggestions for\nspellchecking the word \"w.sub.i\" are ranked.  All the words in the sentence are mapped to pre-trained word vectors.  The sequence of words w.sub.i, .  . . , w.sub.i-1 are inputted to a Recurrent Neural Network; and its last output vector is used as a\nrepresentation of the left context of the word \"w.sub.i\".\n The same is done for the right side: the sequence w.sub.s, .  . . , w.sub.i+1 (reversed order) is inputted to obtain a vector representing the right context of the word w.sub.i.\n The left context vector, the word w.sub.1 vector, and the right context vector are inputted to a fully connected layer connected to a logistic unit.  The final output of the logistic unit is in [0, 1] and can be interpreted as the score of the\nword w.sub.i in the context it appears.\n A forward pass in the network is run for each candidate correction, and the computed scores are collected.  Those scores are finally used for ranking.\n Diagram 700 of FIG. 7 illustrates context sensitive scoring aspects, according to the embodiment described herein.  As shown, the words 710 are mapped to a pre-trained word vector 720.  A Recurrent Neural Network (RNN) 730 is used to obtain a\nvector representing the left context (742) and another one for the right context of w.sub.i (744).  A fully connected (FC) layer 750 is connected to a logistic unit that outputs a score in [0, 1].  Training with negative sampling is done, and w.sub.1 is\nreplaced with candidate corrections to output the score 760 to use for ranking.\n According to embodiments, the deep learning approach allows for alternative instantiations of the model.  According to an embodiment, pre-trained word vectors can be carried out with various methods, including but not limited to: cbow,\nskip-gram, GloVe, LSA, PLSA, LDA, HAL, NNMF, and any other embedding method.  In experimentation, the skip-gram model performed best.\n There are also different Recurrent Neural Network alternatives to choose from: simple RNN, LSTM, and GRU, for example.  A simple alternative would be to also consider a specific window of k elements and simply concatenate the word vectors for\nthose k elements to represent the context.  In experimentation, LSTM was the best performing alternative.\n The fully connected layer at the end can vary in size, deepness (there could be several stacked fully connected layers, for example), and activation function.  In experimentation, one layer with hyperbolic tangent activation was the best\nperforming alternative.\n FIG. 8 is a runtime example 800 of the implementation of a spellcheck module 200 utilizing the candidate suggestion generation module 210 and the candidate suggestion ranking module 220, according to embodiments herein; and FIG. 9 is a\nscreenshot output example 900 of the implementation of the spellcheck module 200, according to embodiments herein.\n FIG. 10 is a flowchart 1000 illustrating a method for adaptive correction of misspelling utilizing the spellcheck module 200, according to embodiments herein.\n At 1010, a text for spelling analysis is received.  The text may be a portion of a user-generated search query from a user device 102 sent to the enterprise search engine 110 for generating search results based on the search query.\n At 1020, the spellcheck module 200 of the enterprise search engine 110 creates a table of entries of words from a corpus, such as the text collection 230 of the enterprise search engine 110.  For each word, the following parameters may be\nincluded: a number of occurrences (e.g., a frequency), links to the occurrences, and alternative words having character deletions.\n At 1030, the spellcheck module 200 compares an entry in the text, which is part of the user-generated search query, to the table of entries having a pre-defined edit distance (e.g., \"med\"=2) from the entry and a minimum frequency of occurrence\nin the corpus (e.g., the text collection 230) to form a candidate set of entries.\n At 1040, the spellcheck module 200 ranks the candidate set of entries so that each candidate has a ranking.  In an embodiment, context sensitive scoring according to embodiments herein is used for the ranking.  In an alternative embodiment,\nnon-contextual scoring according to embodiments herein is utilized for the ranking.\n At 1050, the spellcheck module 200 orders at least some of the ranked candidates based on the ranking to identify corrections to the entry.\n FIG. 11 is a flowchart 1100 illustrating a method for adaptive correction of misspelling utilizing the spellcheck module 200, according to an additional embodiment.\n At 1110, a maximum edit distance and a threshold frequency are defined by the spellcheck module 200.  These parameters may be inputted by a user or administrator and may vary based on features of the enterprise search engine 110 or other\nconsiderations.  The maximum edit distance (\"med\") and the threshold frequency (\"tf\") are defined for words of a dataset (i.e., the text collection 230) to be added to an index.\n At 1120, the spellcheck module 200 sorts the dataset to identify the words of the dataset to add to the index based on the threshold frequency.  At 1130, the identified words and alternative words having character deletions in accordance with\nthe \"med\" are added to the index to create entries.  That is, as soon as a word reaches the \"tf,\" added to the index are all of the deletes of length \"med\" of the word.\n At 1140, a text for spelling analysis is received.  The text may be a portion of a user-generated search query from a user device 102 sent to the enterprise search engine 110 for generating search results based on the search query.\n At 1150, the spellcheck module 200 identifies one or more candidate entries from the entries of the index by obtaining from the index the entries associated with the text.\n At 1160, the spellcheck module 200 ranks the candidate set of entries so that each candidate has a ranking.  In an embodiment, non-contextual scoring according to embodiments herein is used for the ranking.  In an alternative embodiment, context\nsensitive scoring according to embodiments herein is utilized for the ranking.\n In an embodiment, the enterprise search engine 110 and the spellcheck module 200 may be part of a cognitive system.  A cognitive system is a specialized computer system, or set of computer systems, configured with hardware and/or software logic\n(in combination with hardware logic upon which the software executes) to emulate human cognitive functions.  These cognitive systems apply human-like characteristics to conveying and manipulating ideas which, when combined with the inherent strengths of\ndigital computing, can solve problems with high accuracy and resilience on a large scale.  IBM Watson.TM.  is an example of one such cognitive system which can process human readable language and identify inferences between text passages with human-like\naccuracy at speeds far faster than human beings and on a much larger scale.\n The present invention may be a system, a method, and/or a computer program product.  The computer program product may include a computer readable storage medium (or media) having computer readable program instructions thereon for causing a\nprocessor to carry out aspects of the present invention.\n The computer readable storage medium can be a tangible device that can retain and store instructions for use by an instruction execution device.  The computer readable storage medium may be, for example, but is not limited to, an electronic\nstorage device, a magnetic storage device, an optical storage device, an electromagnetic storage device, a semiconductor storage device, or any suitable combination of the foregoing.  A non-exhaustive list of more specific examples of the computer\nreadable storage medium includes the following: a portable computer diskette, a head disk, a random access memory (RAM), a read-only memory (ROM), an erasable programmable read-only memory (EPROM or Flash memory), a static random access memory (SRAM), a\nportable compact disc read-only memory (CD-ROM), a digital versatile disk (DVD), a memory stick, a floppy disk, a mechanically encoded device such as punch-cards or raised structures in a groove having instructions recorded thereon, and any suitable\ncombination of the foregoing.  A computer readable storage medium, as used herein, is not to be construed as being transitory signals per se, such as radio waves or other freely propagating electromagnetic waves, electromagnetic waves propagating through\na waveguide or other transmission media (e.g., light pulses passing through a fiber-optic cable), or electrical signals transmitted through a wire.\n Computer readable program instructions described herein can be downloaded to respective computing/processing devices from a computer readable storage medium or to an external computer or external storage device via a network, for example, the\nInternet, a local area network (LAN), a wide area network (WAN) and/or a wireless network.  The network may comprise copper transmission cables, optical transmission fibers, wireless transmission, routers, firewalls, switches, gateway computers, and/or\nedge servers.  A network adapter card or network interface in each computing/processing device receives computer readable program instructions from the network and forwards the computer readable program instructions for storage in a computer readable\nstorage medium within the respective computing/processing device.\n Computer readable program instructions for carrying out operations of the present invention may be assembler instructions, instruction-set-architecture (ISA) instructions, machine instructions, machine dependent instructions, microcode, firmware\ninstructions, state-setting data, or either source code or object code written in any combination of one or more programming languages, including an object-oriented programming language such as Java, Smalltalk, C++ or the like, and conventional\nprocedural programming languages, such as the \"C\" programming language or similar programming languages.  The computer readable program instructions may execute entirely on the user's computer, partly on the user's computer, as a stand-alone software\npackage, partly on the user's computer and partly on a remote computer, or entirely on the remote computer or server.  In the latter scenario, the remote computer may be connected to the user's computer through any type of network, including LAN or WAN,\nor the connection may be made to an external computer (for example, through the Internet using an Internet Service Provider).  In some embodiments, electronic circuitry including, for example, programmable logic circuitry, field-programmable gate arrays\n(FPGA), or programmable logic arrays (PLA) may execute the computer readable program instructions by utilizing state information of the computer readable program instructions to personalize the electronic circuitry, in order to perform aspects of the\npresent invention.\n Aspects of the present invention are described herein with reference to flowchart illustrations and/or block diagrams of methods, apparatuses (systems), and computer program products according to embodiments of the invention.  It will be\nunderstood that each block of the flowchart illustrations and/or block diagrams, and combinations of blocks in the flowchart illustrations and/or block diagrams, can be implemented by computer readable program instructions.\n These computer readable program instructions may be provided to a processor of a general purpose computer, special purpose computer, or other programmable data processing apparatus to produce a machine, such that the instructions, which execute\nvia the processor of the computer or other programmable data processing apparatus, create means for implementing the functions/acts specified in the flowchart and/or block diagram block or blocks.  These computer readable program instructions may also be\nstored in a computer readable storage medium that can direct a computer, a programmable data processing apparatus, and/or other devices to function in a particular manner, such that the computer readable storage medium having instructions stored therein\ncomprises an article of manufacture including instructions which implement aspects of the function/act specified in the flowchart and/or block diagram block or blocks.\n The computer readable program instructions may also be loaded onto a computer, other programmable data processing apparatus, or other device to cause a series of operations steps to be performed on the computer, other programmable apparatus, or\nother device to produce a computer implemented process, such that the instructions which execute on the computer, other programmable apparatus, or other device implement the functions/acts specified in the flowchart and/or block diagram block or blocks.\n The flowchart and block diagrams in the Figures illustrate the architecture, functionality, and operation of possible implementations of systems, methods, and computer program products according to various embodiments of the present invention. \nIn this regard, each block in the flowchart or block diagrams may represent a module, segment, or portion of instructions, which comprises one or more executable instructions for implementing the specified logical functions.  In some alternative\nimplementations, the functions noted in the block may occur out of the order noted in the Figures.  For example, two blocks shown in succession may, in fact, be executed substantially concurrently, or the blocks may sometimes be executed in the reverse\norder, depending upon the functionality involved.  It will also be noted that each block of the block diagrams and/or flowchart illustration, and combinations of blocks in the block diagrams and/or flowchart illustration, can be implemented by special\npurpose hardware-based systems that perform the specified functions or acts or carry out combinations of special purpose hardware and computer instructions.\n FIG. 12 is a block diagram of an example data processing system 1200 in which aspects of the illustrative embodiments are implemented.  Data processing system 1200 is an example of a computer, such as a server or client, in which computer usable\ncode or instructions implementing the process for illustrative embodiments of the present invention are located.  In one embodiment, FIG. 12 may represent a server computing device.\n In the depicted example, data processing system 1200 can employ a hub architecture including a north bridge and memory controller hub (NB/MCH) 1201 and south bridge and input/output (I/O) controller hub (SB/ICH) 1202.  Processing unit 1203, main\nmemory 1204, and graphics processor 1205 can be connected to the NB/MCH 1201.  Graphics processor 1205 can be connected to the NB/MCH 1201 through, for example, an accelerated graphics port (AGP).\n In the depicted example, a network adapter 1206 connects to the SB/ICH 1202.  An audio adapter 1207, keyboard and mouse adapter 1208, modem 1209, read only memory (ROM) 1210, hard disk drive (HDD) 1211, optical drive (e.g., CD or DVD) 1212,\nuniversal serial bus (USB) ports and other communication ports 1213, and PCI/PCIe devices 1214 may connect to the SB/ICH 1202 through bus system 1216.  PCI/PCIe devices 1214 may include Ethernet adapters, add-in cards, and PC cards for notebook\ncomputers.  ROM 1210 may be, for example, a flash basic input/output system (BIOS).  The HDD 1211 and optical drive 1212 can use an integrated drive electronics (IDE) or serial advanced technology attachment (SATA) interface.  A super I/O (SIO) device\n1215 can be connected to the SB/ICH 1202.\n An operating system can run on processing unit 1203.  The operating system can coordinate and provide control of various components within the data processing system 1200.  As a client, the operating system can be a commercially available\noperating system.  An object-oriented programming system, such as the Java.TM.  programming system, may run in conjunction with the operating system and provide calls to the operating system from the object-oriented programs or applications executing on\nthe data processing system 1200.  As a server, the data processing system 1200 can be an IBM.RTM.  eServer.TM.  System p.RTM.  running the Advanced Interactive Executive operating system or the Linux operating system.  The data processing system 1200 can\nbe a symmetric multiprocessor (SMP) system that can include a plurality of processors in the processing unit 1203.  Alternatively, a single processor system may be employed.\n Instructions for the operating system, the object-oriented programming system, and applications or programs are located on storage devices, such as the HDD 1211, and are loaded into the main memory 1204 for execution by the processing unit 1203. The processes for embodiments described herein can be performed by the processing unit 1203 using computer usable program code, which can be located in a memory such as, for example, main memory 1204, ROM 1210, or in one or more peripheral devices.\n A bus system 1216 can be comprised of one or more busses.  The bus system 1216 can be implemented using any type of communication fabric or architecture that can provide for a transfer of data between different components or devices attached to\nthe fabric or architecture.  A communication unit such as the modem 1209 or the network adapter 1206 can include one or more devices that can be used to transmit and receive data.\n Those of ordinary skill in the art will appreciate that the hardware depicted in FIG. 12 may vary depending on the implementation.  Other internal hardware or peripheral devices, such as flash memory, equivalent non-volatile memory, or optical\ndisk drives may be used in addition to or in place of the hardware depicted.  Moreover, the data processing system 1200 can take the form of any of a number of different data processing systems, including but not limited to, client computing devices,\nserver computing devices, tablet computers, laptop computers, telephone or other communication devices, personal digital assistants, and the like.  Essentially, data processing system 1200 can be any known or later developed data processing system\nwithout architectural limitation.\n The system and processes of the figures are not exclusive.  Other systems, processes, and menus may be derived in accordance with the principles of embodiments described herein to accomplish the same objectives.  It is to be understood that the\nembodiments and variations shown and described herein are for illustration purposes only.  Modifications to the current design may be implemented by those skilled in the art, without departing from the scope of the embodiments.  As described herein, the\nvarious systems, subsystems, agents, managers, and processes can be implemented using hardware components, software components, and/or combinations thereof.  No claim element herein is to be construed under the provisions of 35 U.S.C.  112(f) unless the\nelement is expressly recited using the phrase \"means for.\"\n Although the invention has been described with reference to exemplary embodiments, it is not limited thereto.  Those skilled in the art will appreciate that numerous changes and modifications may be made to the preferred embodiments of the\ninvention and that such changes and modifications may be made without departing from the true spirit of the invention.  It is therefore intended that the appended claims be construed to cover all such equivalent variations as fall within the true spirit\nand scope of the invention.", "application_number": "15296794", "abstract": " Embodiments are directed to a spellcheck module for an enterprise search\n     engine. The spellcheck module includes a candidate suggestion generation\n     module that generates a number of candidate words that may be the\n     correction of the misspelled word. The candidate suggestion generation\n     module implements an algorithm for indexing, searching, and storing terms\n     from an index with a constrained edit distance, using words in a\n     collection of documents. The spellcheck module further includes a\n     candidate suggestion ranking module. In one embodiment, a non-contextual\n     approach using a linear combination of distance and probability scores is\n     utilized; while in another embodiment, a context sensitive approach\n     accounting for real-word misspells and adopting deep learning models is\n     utilized. In use, a query is provided to the spellcheck module to\n     generate results in the form of a ranked list of generated candidate\n     entries that may be an entry a user accidentally misspelled.\n", "citations": ["4674065", "5148367", "5258909", "5572423", "5604897", "5659771", "5699441", "5907839", "6047300", "6616704", "7047493", "7254774", "7321892", "7660806", "7809744", "8290968", "8441454", "8621344", "8775341", "8799237", "9015036", "9020822", "9031293", "9037967", "9557916", "9740767", "20020087604", "20030145285", "20070016616", "20080077396", "20080155398", "20080167858", "20090089666", "20100076972", "20110193797", "20120191357", "20120229388", "20130066896", "20130262096", "20130283156", "20140104175", "20140249799", "20140358831", "20150332670", "20160110343", "20160210551", "20160306876", "20160350653", "20170032243", "20170358293", "20170372200", "20180061439", "20180150605"], "related": []}, {"id": "20180130203", "patent_code": "10373312", "patent_name": "Automated skin lesion segmentation using deep side layers", "year": "2019", "inventor_and_country_data": " Inventors: \nAbedini; Mani (Melbourne, AU), Bozorgtabar; SeyedBehzad (Melbourne, AU), Chakravorty; Rajib (Melbourne, AU), Demyanov; Sergey (Melbourne, AU), Garnavi; Rahil (Melbourne, AU), Ge; Zongyuan (Melbourne, AU)  ", "description": "BACKGROUND\n The present invention relates to the medical arts, and more specifically, to computer-aided dermoscopy.\n Skin lesion segmentation is the first and a key step of computer-aided dermoscopy for skin lesion diagnosis and has significant implications for diagnosis of melanoma.  While the task of segmenting a skin lesion is important, it is particularly\nchallenging due to high variability of the lesion shape, presence of artefacts (e.g. hair and fiducial markers) and/or the possibility of a large color distribution across the skin lesion area.\nSUMMARY\n Principles of the invention provide techniques for automated skin lesion segmentation using deep side layers.  In one aspect, an exemplary computer-implemented method includes obtaining a dermoscopic image, convolving the dermoscopic image in a\nplurality of convolutional layers, obtaining deconvolved outputs of at least two convolutional layers of the plurality of convolutional layers, obtaining side-output feature maps by applying loss functions to the deconvolved outputs of the at least two\nconvolutional layers, obtaining a first concatenated feature map by concatenating the side-output feature maps with different first weights, obtaining a second concatenated feature map by concatenating the side-output feature maps with different second\nweights, and producing a final score map by convolving the first and second concatenated feature maps in a final convolutional layer followed by a loss layer.\n As used herein, \"facilitating\" an action includes performing the action, making the action easier, helping to carry the action out, or causing the action to be performed.  Thus, by way of example and not limitation, instructions executing on one\nprocessor might facilitate an action carried out by instructions executing on a remote processor, by sending appropriate data or commands to cause or aid the action to be performed.  For the avoidance of doubt, where an actor facilitates an action by\nother than performing the action, the action is nevertheless performed by some entity or combination of entities.\n One or more embodiments of the invention or elements thereof can be implemented in the form of a computer program product including a computer readable storage medium with computer usable program code for performing the method steps indicated. \nFurthermore, one or more embodiments of the invention or elements thereof can be implemented in the form of a system (or apparatus) including a memory, and at least one processor that is coupled to the memory and operative to perform exemplary method\nsteps.  Yet further, in another aspect, one or more embodiments of the invention or elements thereof can be implemented in the form of means for carrying out one or more of the method steps described herein; the means can include (i) hardware module(s),\n(ii) software module(s) stored in a computer readable storage medium (or multiple such media) and implemented on a hardware processor, or (iii) a combination of (i) and (ii); any of (i)-(iii) implement the specific techniques set forth herein.\n In view of the foregoing, techniques of the present invention can provide substantial beneficial technical effects.  For example, one or more embodiments provide one or more of:\n Improved segmentation accuracy for identifying boundaries of lesions in medical images.\n Unsupervised segmentation of lesion areas from background areas in dermoscopic images, thereby enabling rapid diagnostics.\n An automated system for melanoma skin cancer diagnosis, in which the modules can include: a lesion segmentation module implemented in a plurality of convolutional layers as well as at least two deconvolutional layers producing feature maps from\noutputs of at least two convolutional layers of the plurality of convolutional layers; a feature generation module implemented in the plurality of convolutional layers and the at least two deconvolutional layers; a classification and risk assessment\nmodule operating on the output of the lesion segmentation and feature generation modules; a visualization and similarity retrieval module operating on the output of the lesion segmentation and feature generation modules; a user feedback module operating\non the output of the classification and risk assessment module and the visualization and similarity retrieval module; and an active learning module implemented in loss layers associated with the plurality of convolutional layers.\n Additional features and advantages of the present invention will become apparent from the following detailed description of illustrative embodiments thereof, which is to be read in connection with the accompanying drawings. BRIEF\nDESCRIPTION OF THE DRAWINGS\n FIG. 1 depicts a cloud computing environment according to an embodiment of the present invention;\n FIG. 2 depicts abstraction model layers according to an embodiment of the present invention;\n FIG. 3 depicts a block diagram of a VGG-16 neural network model with added side layers, according to an exemplary embodiment;\n FIGS. 4A-4B depict a CAFFE script for implementing the neural network model shown in FIG. 3, according to an exemplary embodiment;\n FIG. 5 depicts an exemplary kernel of a neural network convolution layer, according to an exemplary embodiment;\n FIG. 6 depicts an exemplary kernel of a neural network pooling layer, according to an exemplary embodiment;\n FIG. 7 depicts a computer-aided diagnostic module in which the neural network model of FIG. 3 is used according to an exemplary embodiment;\n FIG. 8 depicts a similarity retrieval module of the computer-aided diagnostic module, according to an exemplary embodiment;\n FIG. 9 depicts a skin disease pattern retrieval module of the computer-aided diagnostic module, according to an exemplary embodiment; and\n FIG. 10 depicts a computer system that may be useful in implementing one or more aspects and/or elements of the invention, also representative of a cloud computing node according to an embodiment of the present invention.\nDETAILED DESCRIPTION\n It is understood in advance that although this disclosure includes a detailed description on cloud computing, implementation of the teachings recited herein are not limited to a cloud computing environment.  Rather, embodiments of the present\ninvention are capable of being implemented in conjunction with any other type of computing environment now known or later developed.\n Cloud computing is a model of service delivery for enabling convenient, on-demand network access to a shared pool of configurable computing resources (e.g. networks, network bandwidth, servers, processing, memory, storage, applications, virtual\nmachines, and services) that can be rapidly provisioned and released with minimal management effort or interaction with a provider of the service.  This cloud model may include at least five characteristics, at least three service models, and at least\nfour deployment models.\n Characteristics are as follows:\n On-demand self-service: a cloud consumer can unilaterally provision computing capabilities, such as server time and network storage, as needed automatically without requiring human interaction with the service's provider.\n Broad network access: capabilities are available over a network and accessed through standard mechanisms that promote use by heterogeneous thin or thick client platforms (e.g., mobile phones, laptops, and PDAs).\n Resource pooling: the provider's computing resources are pooled to serve multiple consumers using a multi-tenant model, with different physical and virtual resources dynamically assigned and reassigned according to demand.  There is a sense of\nlocation independence in that the consumer generally has no control or knowledge over the exact location of the provided resources but may be able to specify location at a higher level of abstraction (e.g., country, state, or datacenter).\n Rapid elasticity: capabilities can be rapidly and elastically provisioned, in some cases automatically, to quickly scale out and rapidly released to quickly scale in. To the consumer, the capabilities available for provisioning often appear to\nbe unlimited and can be purchased in any quantity at any time.\n Measured service: cloud systems automatically control and optimize resource use by leveraging a metering capability at some level of abstraction appropriate to the type of service (e.g., storage, processing, bandwidth, and active user accounts). Resource usage can be monitored, controlled, and reported providing transparency for both the provider and consumer of the utilized service.\n Service Models are as follows:\n Software as a Service (SaaS): the capability provided to the consumer is to use the provider's applications running on a cloud infrastructure.  The applications are accessible from various client devices through a thin client interface such as a\nweb browser (e.g., web-based email).  The consumer does not manage or control the underlying cloud infrastructure including network, servers, operating systems, storage, or even individual application capabilities, with the possible exception of limited\nuser-specific application configuration settings.\n Platform as a Service (PaaS): the capability provided to the consumer is to deploy onto the cloud infrastructure consumer-created or acquired applications created using programming languages and tools supported by the provider.  The consumer\ndoes not manage or control the underlying cloud infrastructure including networks, servers, operating systems, or storage, but has control over the deployed applications and possibly application hosting environment configurations.\n Infrastructure as a Service (IaaS): the capability provided to the consumer is to provision processing, storage, networks, and other fundamental computing resources where the consumer is able to deploy and run arbitrary software, which can\ninclude operating systems and applications.  The consumer does not manage or control the underlying cloud infrastructure but has control over operating systems, storage, deployed applications, and possibly limited control of select networking components\n(e.g., host firewalls).\n Deployment Models are as follows:\n Private cloud: the cloud infrastructure is operated solely for an organization.  It may be managed by the organization or a third party and may exist on-premises or off-premises.\n Community cloud: the cloud infrastructure is shared by several organizations and supports a specific community that has shared concerns (e.g., mission, security requirements, policy, and compliance considerations).  It may be managed by the\norganizations or a third party and may exist on-premises or off-premises.\n Public cloud: the cloud infrastructure is made available to the general public or a large industry group and is owned by an organization selling cloud services.\n Hybrid cloud: the cloud infrastructure is a composition of two or more clouds (private, community, or public) that remain unique entities but are bound together by standardized or proprietary technology that enables data and application\nportability (e.g., cloud bursting for load balancing between clouds).\n A cloud computing environment is service oriented with a focus on statelessness, low coupling, modularity, and semantic interoperability.  At the heart of cloud computing is an infrastructure comprising a network of interconnected nodes.\n Referring now to FIG. 1, illustrative cloud computing environment 50 is depicted.  As shown, cloud computing environment 50 includes one or more cloud computing nodes 10 with which local computing devices used by cloud consumers, such as, for\nexample, personal digital assistant (PDA) or cellular telephone 54A, desktop computer 54B, laptop computer 54C, and/or automobile computer system 54N may communicate.  Nodes 10 may communicate with one another.  They may be grouped (not shown) physically\nor virtually, in one or more networks, such as Private, Community, Public, or Hybrid clouds as described hereinabove, or a combination thereof.  This allows cloud computing environment 50 to offer infrastructure, platforms and/or software as services for\nwhich a cloud consumer does not need to maintain resources on a local computing device.  It is understood that the types of computing devices 54A-N shown in FIG. 1 are intended to be illustrative only and that computing nodes 10 and cloud computing\nenvironment 50 can communicate with any type of computerized device over any type of network and/or network addressable connection (e.g., using a web browser).\n Referring now to FIG. 2, a set of functional abstraction layers provided by cloud computing environment 50 (FIG. 1) is shown.  It should be understood in advance that the components, layers, and functions shown in FIG. 2 are intended to be\nillustrative only and embodiments of the invention are not limited thereto.  As depicted, the following layers and corresponding functions are provided:\n Hardware and software layer 60 includes hardware and software components.  Examples of hardware components include: mainframes 61; RISC (Reduced Instruction Set Computer) architecture based servers 62; servers 63; blade servers 64; storage\ndevices 65; and networks and networking components 66.  In some embodiments, software components include network application server software 67 and database software 68.\n Virtualization layer 70 provides an abstraction layer from which the following examples of virtual entities may be provided: virtual servers 71; virtual storage 72; virtual networks 73, including virtual private networks; virtual applications\nand operating systems 74; and virtual clients 75.\n In one example, management layer 80 may provide the functions described below.  Resource provisioning 81 provides dynamic procurement of computing resources and other resources that are utilized to perform tasks within the cloud computing\nenvironment.  Metering and Pricing 82 provide cost tracking as resources are utilized within the cloud computing environment, and billing or invoicing for consumption of these resources.  In one example, these resources may include application software\nlicenses.  Security provides identity verification for cloud consumers and tasks, as well as protection for data and other resources.  User portal 83 provides access to the cloud computing environment for consumers and system administrators.  Service\nlevel management 84 provides cloud computing resource allocation and management such that required service levels are met.  Service Level Agreement (SLA) planning and fulfillment 85 provide pre-arrangement for, and procurement of, cloud computing\nresources for which a future requirement is anticipated in accordance with an SLA.\n Workloads layer 90 provides examples of functionality for which the cloud computing environment may be utilized.  Examples of workloads and functions which may be provided from this layer include: mapping and navigation 91; software development\nand lifecycle management 92; virtual classroom education delivery 93; data analytics processing 94; transaction processing 95; and a computer-aided diagnostics module 96.\n Referring to FIG. 3, aspects of the invention formulate skin lesion segmentation as an image-to-image regression task, by incorporating a skin lesion segmentation module into the computer-aided diagnostics module 96 as a multi-layer convoluted\nneural network (\"CNN\") architecture 300 that is based on the VGG-16 visual recognition model (available for download from Visual Geometry Group, Department of Engineering Science, University of Oxford).  FIGS. 4A-4B present Caffe code that describes the\narchitecture 300.  Caffe is a software engine that implements an artificial intelligence scripting language for defining neural networks, particularly convolutional neural networks.  Caffe is developed and maintained by the Berkeley Artificial\nIntelligence Research Lab at the University of California, Berkeley.  An ordinary skilled worker in the field of neural networks can interpret Caffe code.\n Generally, a neural network includes a plurality of computer processors that are configured to work together to implement one or more machine learning algorithms.  The implementation may be synchronous or asynchronous.  In a neural network, the\nprocessors simulate thousands or millions of neurons, which are connected by axons and synapses.  Each connection is enforcing, inhibitory, or neutral in its effect on the activation state of connected neural units.  Each individual neural unit has a\nsummation function which combines the values of all its inputs together.  In some implementations, there is a threshold function or limiting function on at least some connections and/or on at least some neural units, such that the signal must surpass the\nlimit before propagating to other neurons.  A neural network can implement supervised, unsupervised, or semi-supervised machine learning.\n A convolutional neural network is a type of feed-forward artificial neural network in which the connectivity pattern between its neurons is inspired by the organization of the animal visual cortex.  A CNN is formed by a stack of distinct layers\nthat transform the input volume into an output volume (e.g. holding the class scores) through a differentiable function.  A few distinct types of layers are commonly used.  These include convolutional layers, pooling layers, rectified linear unit\n(\"ReLU\") layers, fully connected layers, and loss layers.  Additionally, \"deconvolutional\" layers may be used, although it is important to understand from the outset that deconvolutional layers do not reverse the work of convolutional layers.\n A convolutional layer applies a spatial filter to its input.  The filter also is known as a \"kernel\", and is a matrix of weighting factors.  FIG. 5 depicts an exemplary kernel 500 of size 3.times.3 pixels.  A kernel or filter is applied to an\ninput image (\"convolved\" with the input image) by repeatedly performing matrix dot multiplication of the kernel with corresponding receptive field portions of the input image as the kernel moves across the image.  How far the kernel moves between dot\nmultiplications is known as the \"stride\" of the kernel.  For example, if an input image has pixel size 224.times.224 (as shown in FIG. 3), and if a 2.times.2 kernel is applied with stride size 1, the resultant output will have pixel size 222.times.222. \nTo preserve the pixel size of the input image, \"zero padding\" may be used in which zero values are added around the edges of the input image.  For example, if an input image of pixel size 224.times.224 is zero padded with 1 layer of zeros around its\nedges, and a 3.times.3 kernel is applied with stride size 1, the resultant output will have pixel size 224.times.224 as shown in FIG. 3 (\"data\" as the input and \"conv1_1\" as the output).\n Each layer of the CNN typically has three dimensions: two pixel size dimensions, i.e. height and width of a two dimensional image, as well as a filter depth dimension, i.e. how many different \"feature maps\" the layer obtains by applying\ndifferent kernels to its input.  Note that the receptive field of each kernel spans all the feature maps of the input layer.\n A pooling layer also applies a spatial filter to its input, however, unlike a typical convolutional layer, the pooling layer downsamples its input to produce an output of lesser spatial dimensions.  For example, in FIG. 3 the first pooling layer\npool1 receives the second convolutional layer conv1_2 (pixel size 224.times.224) as its input, and applies a kernel 600 of size 2.times.2 with a stride of 2 (as defined in FIG. 6).  Accordingly, the first pooling layer pool1 produces an output of pixel\nsize 112.times.112.  The first pooling layer pool1 applies 128 different filters to its input, so its output has a total dimension of 112.times.112.times.128.  One exemplary function that can be used for downsampling is a \"max\" function that takes the\nmaximum value of the receptive field.  Another exemplary function is to take the average value of the receptive field.\n A ReLU layer applies a non-linear function, e.g., f(x)=max(0,x), to the pixels of the preceding layer.  The result is to produce an output of the same dimensions as the preceding (input) layer, but with more non-linearity.  The increased\nnon-linearity enhances the speed of machine learning.\n A fully connected layer has each pixel connected to all the pixels of the preceding layer, so that its receptive field includes the entire input data.  In practice, this means that a fully connected layer has pixel size 1.times.1.  Typically, a\nfully connected layer is \"deep\" in terms of the number of different kernels or feature maps.  FIG. 3 depicts two fully connected layers of size 1.times.1.times.4096, which are used for training the filter weights of the convolutional layers.  However, in\none or more embodiments of the invention the convolutional layer weights are pretrained, thus, the fully connected layer do not need to be implemented.\n A loss layer specifies how the network training penalizes a deviation between the predicted and true labels, and is normally the last layer in the network.  Because the convolutional layer weights are pretrained in one or more embodiments of the\ninvention, the softmax loss layer size 1.times.1.times.1000 does not need to be implemented.\n One or more embodiments utilize five blocks of convolutional layers conv2_2, conv3_3, conv4_3, conv5_3, and conv5_4 from the VGG-16 network, between pooling layers pool1, pool5.  The first convolutional layer conv2_2 has 112.times.112 pixels and\n128 kernels; the second convolutional layer conv3_3 has 56.times.56 pixels and 256 kernels; the third convolutional layer conv4_3 has 28.times.28 pixels and 512 kernels; the fourth convolutional layer conv5_3 has 14.times.14 pixels and 512 kernels; the\nfifth convolutional layer conv5_4 has 14.times.14 pixels and 512 kernels.  All of the convolutional layer kernels are size 3.times.3 pixels, stride 1.  The pooling layer pool1 has 112.times.112 pixels and 128 kernels size 2.times.2 with stride 2; the\npooling layer pool5 has 7.times.7 pixels and 512 kernels of size 2.times.2 with stride 2.  The pooling layers preferably implement max pooling, although in one or more embodiments average pooling can be used.\n One or more embodiments add side-output layers to the convolutional layers mentioned above, in order to take advantage of multi-scale convolutional features.  Each side-output layer includes subordinate layers (a de-convolutional layer and a\nloss layer, e.g., a softmax layer).  The deconvolutional layer of each side-output layer generates feature maps from the image that is input to that side-output layer (i.e., from the features of the multi-channel image that was generated by the\nconvolutional layer).  For example, the deconvolutional layer of side-output layer conv2_2_side produces 16 feature maps of spatial dimension 224.times.224 pixels based on the 128 feature maps of layer conv2_2, whereas conv4_3_side produces 16 feature\nmaps of spatial dimension 224.times.224 pixels based on the 512 feature maps of layer conv4_3.  The de-convolutional layers upsample the different size feature maps to match the spatial dimensions of the original image data, 224.times.224 pixels.  A\nfinal convolutional layer map, size 224.times.224.times.1, linearly combines the upsampled feature maps to produce a grayscale lesion feature map 306.  The softmax loss layers compute the final objective of the lesion border map 304 from the upsampled\nfeature maps.\n In one or more embodiments, the final layer directly notifies each side-output layer about the final objective of segmenting the skin lesion, rather than relying on the final layer to back-propagate the final objective through each preceding\nconvolutional layer in the VGG-16 architecture.\n One or more embodiments of the invention use varying feature maps with varying resolutions.  Early layers of the convolutional neural network have high spatial resolution while deeper layers return high semantic features.  Thus, proceeding\ndeeper in the network, the convolutional layers are associated with larger receptive fields in the input image.  Small kernels have a small receptive field, so at the earlier layers the kernels can provide information only regarding local relations, but\nat each deeper layer of the network, the receptive field of each neuron with respect to the earlier layer becomes larger and collects information from a larger spatial context at the input.  Thus, deep layers can provide features with global semantic\nmeaning, while using only small kernels.\n Advantageously, each side-output layer produces a feature map.  The early layers contribute to develop the lesion border map, whereas the deeper layers provide more semantic information and contribute to the global lesion map.  The global map\nmodels the global lesion's shape while the lesion border map represents the lesion boundary details such as the lesion fuzzy boundary.  For example, layers conv2_2, conv3_3, conv4_3, and conv5_3 contribute to the lesion border map 304, whereas layers\nconv3_3, conv4_3, conv5_3, and conv5_4 contribute to the lesion feature map 306.  The pixels of the lesion border map provide useful information about lesion shape as input to another computer vision process, as shown in FIG. 6.\n Additionally, one or more embodiments assign different weights to the output of different side-output layers to fuse the corresponding score maps.  One or more embodiments implement a class-balanced cross-entropy loss function, which relates to\nthe layers' parameters, as follows: (W)=-.beta..SIGMA..sub.j.di-elect cons.Y.sub.+log Pr(y.sub.j=1|X;W)-(1-.beta.).SIGMA..sub.j.di-elect cons.Y.sub.-log Pr(y.sub.j=0|X;W) (1) where .beta.  is the ratio of negative samples over all samples, Y.sub.+ is the\nground truth for positive samples, Y.sub.- is the ground truth for negative samples, Pr(y.sub.j=1|X;W)=.sigma.(a.sub.j.sup.(m)) is the probability score of being lesion area rather than background skin applied on the activation value, and a.sub.j.sup.(m)\nis the activation value of pixel j using the sigmoid function.\n The volumes of the features obtained at each of the side-output layers then are concatenated with the different weights, which are found empirically and are the same for all three scales (0.5, 1.0 and 1.5).  The concatenated feature volumes are\npassed to the last convolutional layer followed by the softmax layer to produce the final score map.  The class-balanced cross-entropy loss function is used at the end for the segmentation problem.\n In addition to the CNN 300, in one or more embodiments the segmentation module 300 passes the map 308 to a similarity retrieval module 800, shown in FIGS. 7 and 8.  The similarity retrieval module 800 makes use of a similarity function 806 that\ncan identify images of lesions from a database 804 that have local patterns close to the map 308.  Before entering the similarity function 806, the map 308 is normalized to be the standard resolution and illumination.  Disease patterns in the map 308,\nsegmented by the CNN 300, are circled by region boundaries.  The similarity function 806 extracts mages with the most similar patterns from the database.  At step 808, the similarity retrieval module 800 displays the extracted images and their diagnoses\nto a user, with the similarity regions and patterns highlighted for further comparison.\n The similarity function 806 of the similarity retrieval module 800 makes use of a skin disease pattern discovery module 900, shown in FIGS. 7 and 9.  The skin disease pattern recovery module 900 fuses two different types of neural networks to\ndiscover new patterns from the datasets.  A shallow network 902 and a bilinear pooling network 904 are used to learn different features.  Global average pooling layers 906, 908 are applied to the outputs of both feature learning components, so that\ndiscovered patterns are generated by using bilinear pooling with global average pooling in a novel way.  Then a classification layer 910, which is a fully connected layer with N class outputs, processes the discovered patterns to identify a corresponding\nclassification of skin disease.  Thus, the similarity measures of the similarity function 806 are not based on final outcome only, but also reference local patterns within the map 308.  In other words, the similarity can be calculated not only on the\npresence/absence of certain local patterns but how these contributed to the final outcome (the rank).\n Given the discussion thus far, it will be appreciated that, in general terms, an exemplary computer-implemented method, according to an aspect of the invention, includes obtaining a dermoscopic image 300; convolving the dermoscopic image in a\nplurality of convolutional layers 302; obtaining deconvolved outputs of at least two convolutional layers (e.g., conv2_2, conv3_3) of the plurality of convolutional layers; obtaining side-output feature maps by applying loss functions to the deconvolved\noutputs (e.g., conv2_2_side, conv3_3_side) of the at least two convolutional layers; obtaining a first concatenated feature map 304 by concatenating the side-output feature maps with different first weights; obtaining a second concatenated feature map\n306 by concatenating the side-output feature maps with different second weights; and producing a final score map 308 by convolving the first and second concatenated feature maps in a final convolutional layer followed by a loss layer.  In one or more\nembodiments, the at least two convolutional layers include first, second, third, fourth, and fifth convolutional layers (e.g., conv2_2, conv3_3, conv4_3, conv5_3, conv5_4).  In further embodiments, the exemplary method includes obtaining a training\ndermoscopic image and a training final score map; convolving the training dermoscopic image in the plurality of convolutional layers; deconvolving the outputs of the at least two convolutional layers of the plurality of convolutional layers; obtaining\ntrial side-output feature maps by applying loss functions to the deconvolved outputs of the at least two convolutional layers; obtaining a trial concatenated feature map by concatenating the trial side-output feature maps with different weights;\nconvolving the trial concatenated feature map in the final convolutional layer followed by the loss layer to produce a trial final score map; assessing variances of the trial final score map from the training final score map; adjusting filters of the\nplurality of convolutional layers in response to the variances; and repeating the preceding steps until the variances are less than a threshold variance vector.  Particular embodiments include adjusting deconvolution filters in response to the variances. According to certain embodiments, the plurality of convolutional layers comprise a VGG-16 neural network.  For example, the at least two convolutional layers include conv2_2, conv3_3, conv4_3, and conv5_3 layers of the VGG-16 neural network. \nAdditionally, the plurality of convolutional layers may include a conv5_4 layer of dimensions 14.times.14.times.512, in addition to the layers of the VGG-16 neural network, and the conv5_4 layer is one of the at least two convolutional layers.\n One or more embodiments of the invention, or elements thereof, can be implemented in the form of an apparatus including a memory and at least one processor that is coupled to the memory and operative to perform exemplary method steps, or in the\nform of a non-transitory computer readable medium embodying computer executable instructions which when executed by a computer cause the computer to perform exemplary method steps.  FIG. 10 depicts a computer system that may be useful in implementing one\nor more aspects and/or elements of the invention, also representative of a cloud computing node according to an embodiment of the present invention.  Referring now to FIG. 10, cloud computing node 10 is only one example of a suitable cloud computing node\nand is not intended to suggest any limitation as to the scope of use or functionality of embodiments of the invention described herein.  Regardless, cloud computing node 10 is capable of being implemented and/or performing any of the functionality set\nforth hereinabove.\n In cloud computing node 10 there is a computer system/server 12, which is operational with numerous other general purpose or special purpose computing system environments or configurations.  Examples of well-known computing systems,\nenvironments, and/or configurations that may be suitable for use with computer system/server 12 include, but are not limited to, personal computer systems, server computer systems, thin clients, thick clients, handheld or laptop devices, multiprocessor\nsystems, microprocessor-based systems, set top boxes, programmable consumer electronics, network PCs, minicomputer systems, mainframe computer systems, and distributed cloud computing environments that include any of the above systems or devices, and the\nlike.\n Computer system/server 12 may be described in the general context of computer system executable instructions, such as program modules, being executed by a computer system.  Generally, program modules may include routines, programs, objects,\ncomponents, logic, data structures, and so on that perform particular tasks or implement particular abstract data types.  Computer system/server 12 may be practiced in distributed cloud computing environments where tasks are performed by remote\nprocessing devices that are linked through a communications network.  In a distributed cloud computing environment, program modules may be located in both local and remote computer system storage media including memory storage devices.\n As shown in FIG. 10, computer system/server 12 in cloud computing node 10 is shown in the form of a general-purpose computing device.  The components of computer system/server 12 may include, but are not limited to, one or more processors or\nprocessing units 16, a system memory 28, and a bus 18 that couples various system components including system memory 28 to processor 16.\n Bus 18 represents one or more of any of several types of bus structures, including a memory bus or memory controller, a peripheral bus, an accelerated graphics port, and a processor or local bus using any of a variety of bus architectures.  By\nway of example, and not limitation, such architectures include Industry Standard Architecture (ISA) bus, Micro Channel Architecture (MCA) bus, Enhanced ISA (EISA) bus, Video Electronics Standards Association (VESA) local bus, and Peripheral Component\nInterconnect (PCI) bus.\n Computer system/server 12 typically includes a variety of computer system readable media.  Such media may be any available media that is accessible by computer system/server 12, and it includes both volatile and non-volatile media, removable and\nnon-removable media.\n System memory 28 can include computer system readable media in the form of volatile memory, such as random access memory (RAM) 30 and/or cache memory 32.  Computer system/server 12 may further include other removable/non-removable,\nvolatile/non-volatile computer system storage media.  By way of example only, storage system 34 can be provided for reading from and writing to a non-removable, non-volatile magnetic media (not shown and typically called a \"hard drive\").  Although not\nshown, a magnetic disk drive for reading from and writing to a removable, non-volatile magnetic disk (e.g., a \"floppy disk\"), and an optical disk drive for reading from or writing to a removable, non-volatile optical disk such as a CD-ROM, DVD-ROM or\nother optical media can be provided.  In such instances, each can be connected to bus 18 by one or more data media interfaces.  As will be further depicted and described below, memory 28 may include at least one program product having a set (e.g., at\nleast one) of program modules that are configured to carry out the functions of embodiments of the invention.\n Program/utility 40, having a set (at least one) of program modules 42, may be stored in memory 28 by way of example, and not limitation, as well as an operating system, one or more application programs, other program modules, and program data. \nEach of the operating system, one or more application programs, other program modules, and program data or some combination thereof, may include an implementation of a networking environment.  Program modules 42 generally carry out the functions and/or\nmethodologies of embodiments of the invention as described herein.\n Computer system/server 12 may also communicate with one or more external devices 14 such as a keyboard, a pointing device, a display 24, etc.; one or more devices that enable a user to interact with computer system/server 12; and/or any devices\n(e.g., network card, modem, etc.) that enable computer system/server 12 to communicate with one or more other computing devices.  Such communication can occur via Input/Output (I/O) interfaces 22.  Still yet, computer system/server 12 can communicate\nwith one or more networks such as a local area network (LAN), a general wide area network (WAN), and/or a public network (e.g., the Internet) via network adapter 20.  As depicted, network adapter 20 communicates with the other components of computer\nsystem/server 12 via bus 18.  It should be understood that although not shown, other hardware and/or software components could be used in conjunction with computer system/server 12.  Examples, include, but are not limited to: microcode, device drivers,\nredundant processing units, and external disk drive arrays, RAID systems, tape drives, and data archival storage systems, etc.\n Thus, one or more embodiments can make use of software running on a general purpose computer or workstation.  With reference to FIG. 10, such an implementation might employ, for example, a processor 16, a memory 28, and an input/output interface\n22 to a display 24 and external device(s) 14 such as a keyboard, a pointing device, or the like.  The term \"processor\" as used herein is intended to include any processing device, such as, for example, one that includes a CPU (central processing unit)\nand/or other forms of processing circuitry.  Further, the term \"processor\" may refer to more than one individual processor.  The term \"memory\" is intended to include memory associated with a processor or CPU, such as, for example, RAM (random access\nmemory) 30, ROM (read only memory), a fixed memory device (for example, hard drive 34), a removable memory device (for example, diskette), a flash memory and the like.  In addition, the phrase \"input/output interface\" as used herein, is intended to\ncontemplate an interface to, for example, one or more mechanisms for inputting data to the processing unit (for example, mouse), and one or more mechanisms for providing results associated with the processing unit (for example, printer).  The processor\n16, memory 28, and input/output interface 22 can be interconnected, for example, via bus 18 as part of a data processing unit 12.  Suitable interconnections, for example via bus 18, can also be provided to a network interface 20, such as a network card,\nwhich can be provided to interface with a computer network, and to a media interface, such as a diskette or CD-ROM drive, which can be provided to interface with suitable media.\n Accordingly, computer software including instructions or code for performing the methodologies of the invention, as described herein, may be stored in one or more of the associated memory devices (for example, ROM, fixed or removable memory)\nand, when ready to be utilized, loaded in part or in whole (for example, into RAM) and implemented by a CPU.  Such software could include, but is not limited to, firmware, resident software, microcode, and the like.\n A data processing system suitable for storing and/or executing program code will include at least one processor 16 coupled directly or indirectly to memory elements 28 through a system bus 18.  The memory elements can include local memory\nemployed during actual implementation of the program code, bulk storage, and cache memories 32 which provide temporary storage of at least some program code in order to reduce the number of times code must be retrieved from bulk storage during\nimplementation.\n Input/output or I/O devices (including but not limited to keyboards, displays, pointing devices, and the like) can be coupled to the system either directly or through intervening I/O controllers.\n Network adapters 20 may also be coupled to the system to enable the data processing system to become coupled to other data processing systems or remote printers or storage devices through intervening private or public networks.  Modems, cable\nmodem and Ethernet cards are just a few of the currently available types of network adapters.\n As used herein, including the claims, a \"server\" includes a physical data processing system (for example, system 12 as shown in FIG. 10) running a server program.  It will be understood that such a physical server may or may not include a\ndisplay and keyboard.\n One or more embodiments can be at least partially implemented in the context of a cloud or virtual machine environment, although this is exemplary and non-limiting.  Reference is made back to FIGS. 1-2 and accompanying text.\n It should be noted that any of the methods described herein can include an additional step of providing a system comprising distinct software modules embodied on a computer readable storage medium; the modules can include, for example, any or\nall of the appropriate elements depicted in the block diagrams and/or described herein; by way of example and not limitation, any one, some or all of the modules/blocks and or sub-modules/sub-blocks described.  The method steps can then be carried out\nusing the distinct software modules and/or sub-modules of the system, as described above, executing on one or more hardware processors such as 16.  Further, a computer program product can include a computer-readable storage medium with code adapted to be\nimplemented to carry out one or more method steps described herein, including the provision of the system with the distinct software modules.\n One example of user interface that could be employed in some cases is hypertext markup language (HTML) code served out by a server or the like, to a browser of a computing device of a user.  The HTML is parsed by the browser on the user's\ncomputing device to create a graphical user interface (GUI).\n Exemplary System and Article of Manufacture Details\n The present invention may be a system, a method, and/or a computer program product.  The computer program product may include a computer readable storage medium (or media) having computer readable program instructions thereon for causing a\nprocessor to carry out aspects of the present invention.\n The computer readable storage medium can be a tangible device that can retain and store instructions for use by an instruction execution device.  The computer readable storage medium may be, for example, but is not limited to, an electronic\nstorage device, a magnetic storage device, an optical storage device, an electromagnetic storage device, a semiconductor storage device, or any suitable combination of the foregoing.  A non-exhaustive list of more specific examples of the computer\nreadable storage medium includes the following: a portable computer diskette, a hard disk, a random access memory (RAM), a read-only memory (ROM), an erasable programmable read-only memory (EPROM or Flash memory), a static random access memory (SRAM), a\nportable compact disc read-only memory (CD-ROM), a digital versatile disk (DVD), a memory stick, a floppy disk, a mechanically encoded device such as punch-cards or raised structures in a groove having instructions recorded thereon, and any suitable\ncombination of the foregoing.  A computer readable storage medium, as used herein, is not to be construed as being transitory signals per se, such as radio waves or other freely propagating electromagnetic waves, electromagnetic waves propagating through\na waveguide or other transmission media (e.g., light pulses passing through a fiber-optic cable), or electrical signals transmitted through a wire.\n Computer readable program instructions described herein can be downloaded to respective computing/processing devices from a computer readable storage medium or to an external computer or external storage device via a network, for example, the\nInternet, a local area network, a wide area network and/or a wireless network.  The network may comprise copper transmission cables, optical transmission fibers, wireless transmission, routers, firewalls, switches, gateway computers and/or edge servers. \nA network adapter card or network interface in each computing/processing device receives computer readable program instructions from the network and forwards the computer readable program instructions for storage in a computer readable storage medium\nwithin the respective computing/processing device.\n Computer readable program instructions for carrying out operations of the present invention may be assembler instructions, instruction-set-architecture (ISA) instructions, machine instructions, machine dependent instructions, microcode, firmware\ninstructions, state-setting data, configuration data for integrated circuitry, or either source code or object code written in any combination of one or more programming languages, including an object oriented programming language such as Smalltalk, C++,\nor the like, and procedural programming languages, such as the \"C\" programming language or similar programming languages.  The computer readable program instructions may execute entirely on the user's computer, partly on the user's computer, as a\nstand-alone software package, partly on the user's computer and partly on a remote computer or entirely on the remote computer or server.  In the latter scenario, the remote computer may be connected to the user's computer through any type of network,\nincluding a local area network (LAN) or a wide area network (WAN), or the connection may be made to an external computer (for example, through the Internet using an Internet Service Provider).  In some embodiments, electronic circuitry including, for\nexample, programmable logic circuitry, field-programmable gate arrays (FPGA), or programmable logic arrays (PLA) may execute the computer readable program instructions by utilizing state information of the computer readable program instructions to\npersonalize the electronic circuitry, in order to perform aspects of the present invention.\n Aspects of the present invention are described herein with reference to flowchart illustrations and/or block diagrams of methods, apparatus (systems), and computer program products according to embodiments of the invention.  It will be\nunderstood that each block of the flowchart illustrations and/or block diagrams, and combinations of blocks in the flowchart illustrations and/or block diagrams, can be implemented by computer readable program instructions.\n These computer readable program instructions may be provided to a processor of a general purpose computer, special purpose computer, or other programmable data processing apparatus to produce a machine, such that the instructions, which execute\nvia the processor of the computer or other programmable data processing apparatus, create means for implementing the functions/acts specified in the flowchart and/or block diagram block or blocks.  These computer readable program instructions may also be\nstored in a computer readable storage medium that can direct a computer, a programmable data processing apparatus, and/or other devices to function in a particular manner, such that the computer readable storage medium having instructions stored therein\ncomprises an article of manufacture including instructions which implement aspects of the function/act specified in the flowchart and/or block diagram block or blocks.\n The computer readable program instructions may also be loaded onto a computer, other programmable data processing apparatus, or other device to cause a series of operational steps to be performed on the computer, other programmable apparatus or\nother device to produce a computer implemented process, such that the instructions which execute on the computer, other programmable apparatus, or other device implement the functions/acts specified in the flowchart and/or block diagram block or blocks.\n The flowchart and block diagrams in the Figures illustrate the architecture, functionality, and operation of possible implementations of systems, methods, and computer program products according to various embodiments of the present invention. \nIn this regard, each block in the flowchart or block diagrams may represent a module, segment, or portion of instructions, which comprises one or more executable instructions for implementing the specified logical function(s).  In some alternative\nimplementations, the functions noted in the blocks may occur out of the order noted in the Figures.  For example, two blocks shown in succession may, in fact, be executed substantially concurrently, or the blocks may sometimes be executed in the reverse\norder, depending upon the functionality involved.  It will also be noted that each block of the block diagrams and/or flowchart illustration, and combinations of blocks in the block diagrams and/or flowchart illustration, can be implemented by special\npurpose hardware-based systems that perform the specified functions or acts or carry out combinations of special purpose hardware and computer instructions.\n The descriptions of the various embodiments of the present invention have been presented for purposes of illustration, but are not intended to be exhaustive or limited to the embodiments disclosed.  Many modifications and variations will be\napparent to those of ordinary skill in the art without departing from the scope and spirit of the described embodiments.  The terminology used herein was chosen to best explain the principles of the embodiments, the practical application or technical\nimprovement over technologies found in the marketplace, or to enable others of ordinary skill in the art to understand the embodiments disclosed herein.", "application_number": "15642717", "abstract": " A method for computer-aided diagnosis of skin lesions includes obtaining\n     a dermoscopic image, convolving the dermoscopic image in a plurality of\n     convolutional layers, obtaining deconvolved outputs of at least two\n     convolutional layers of the plurality of convolutional layers, obtaining\n     side-output feature maps by applying loss functions to the deconvolved\n     outputs of the at least two convolutional layers, obtaining a first\n     concatenated feature map by concatenating the side-output feature maps\n     with different first weights, obtaining a second concatenated feature map\n     by concatenating the side-output feature maps with different second\n     weights, and producing a final score map by convolving the first and\n     second concatenated feature maps in a final convolutional layer followed\n     by a loss layer. Also disclosed: a computer-readable medium embodying\n     instructions for the method, and an apparatus configured to implement the\n     method.\n", "citations": ["8971609", "20060112092", "20160133011"], "related": ["62418208"]}, {"id": "20180174022", "patent_code": "10373049", "patent_name": "Generating an output for a neural network output layer", "year": "2019", "inventor_and_country_data": " Inventors: \nYoung; Reginald Clifford (Palo Alto, CA)  ", "description": "BACKGROUND\n This specification relates to generating outputs for neural network output layers.\n Neural networks are machine learning models that employ one or more layers of nonlinear units to predict an output for a received input.  Some neural networks include one or more hidden layers in addition to an output layer.  The output of each\nhidden layer is used as input to another layer in the network, i.e., the next hidden layer or the output layer.  Each layer of the network generates an output from a received input in accordance with current values of a respective set of parameters.\nSUMMARY\n This specification describes how a system can process the output of a neural network.  To do so, the system determines the number of occurrences of each member of a finite set of potential output values among the output generated by the initial\nneural network layers of the neural network.  The system determines a softmax layer output for each value occurring in the output of the initial neural network layers by determining a respective exponentiation measure for each occurring value.\n In general, one innovative aspect of the subject matter described in this specification can be embodied in methods of processing a network input through a neural network having one or more initial neural network layers followed by a softmax\noutput that include the actions of obtaining a layer output generated by processing the network input through the one or more initial neural network layers, the layer output having a plurality of layer output values and each layer output value being a\nrespective one of a predetermined finite set of possible output values; and processing the layer output through the softmax output layer to generate a neural network output for the network input, including determining, for each possible output value in\nthe predetermined finite set, a number of occurrences of the possible output value in the plurality of layer output values; for each possible output value occurring in the plurality of layer output values, determining a respective exponentiation measure\nof the possible output value; determining a normalization factor for the layer output by combining the exponentiation measures in accordance with the number of occurrences of the possible output values; and determining, for each of the plurality of layer\noutput values, a softmax probability value from the respective exponentiation measure for the layer output value and the normalization factor.\n Other embodiments of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods.  A system of one or more computers\ncan be configured to perform particular operations or actions by virtue of software, firmware, hardware, or any combination thereof installed on the system that in operation may cause the system to perform the actions.  One or more computer programs can\nbe configured to perform particular operations or actions by virtue of including instructions that, when executed by data processing apparatus, cause the apparatus to perform the actions.\n The foregoing and other embodiments can each optionally include one or more of the following features, alone or in combination.  In particular, one embodiment includes all the following features in combination.\n In some implementations, obtaining the layer output includes receiving a plurality of initial layer output values from a processing system that does the processing for the one or more initial neural network layers, the plurality of initial layer\noutput values being unmapped output values of the one or more initial neural network layers; obtaining mapping data defining a mapping from the plurality of initial layer output values to the plurality of layer output values; and determining, for each\ninitial layer output value, a layer output value based on the mapping data.  In some of those implementations, the mapping data specifies a scaling factor for scaling each of the plurality of initial layer output values to generate the layer output\nvalues.\n In some implementations, the layer output is generated by a processing device that performs computation specified by the one or more initial neural network layers using quantized arithmetic.  In some implementations, the layer output is\ngenerated by a processing device that performs computation specified by the one or more initial neural network layers using fixed-point arithmetic.\n In some implementations, each of the finite set of possible output values map to a respective value of an integer data type.  In some implementations, the methods further include generating the network input by converting one or more floating\npoint values to fixed point values.  In some implementations, determining the respective exponentiation measure of the possible output value includes exponentiating Euler's number by a multiplication of each respective possible output value.  In some\nimplementations, each softmax probability value is determined by dividing each respective exponentiation measure by the normalization factor.\n In some implementations, each of the finite set of possible output values is an output of a mapping function.  In some implementations, each of the finite set of possible output values is an output of a compression function.\n Particular embodiments of the subject matter described in this specification can be implemented so as to realize one or more of the following advantages.  The computational complexity of determining an output of a softmax output layer of a\nneural network can be reduced.  In processing systems that perform computations using quantized arithmetic, the range of potential values on which operations can be performed is limited by the range of countable values to which a set of values are\nmapped.  Particular implementations of the subject matter described in this specification leverage this quality of such processing systems by pre-computing normalized values for countable values that occur among the output of the neural network, thus\nincreasing the efficiency of normalizing output values of neural networks.  In these implementations, precomputing the value of exponentiation measures needed for computing normalized values can eliminate the need for hardware or software support for\nexponentiation operations.\n The details of one or more embodiments of the subject matter of this specification are set forth in the accompanying drawings and the description below.  Other features, aspects, and advantages of the subject matter will become apparent from the\ndescription, the drawings, and the claims. BRIEF DESCRIPTION OF THE DRAWINGS\n FIG. 1 shows an example neural network processing system.\n FIG. 2 is a flow chart of an example process for generating a network output from a layer output.\n FIG. 3 is a flow chart of an example process for mapping initial layer output values to layer output values.\n Like reference numbers and designations in the various drawings indicate like elements.\nDETAILED DESCRIPTION\n FIG. 1 shows an example neural network processing system 100.  The neural network processing system 100 is an example of a system implemented as computer programs on one or more computers in one or more locations, in which the systems,\ncomponents, and techniques described below are implemented.\n The neural network processing system 100 includes a neural network 101.  The neural network 101 receives a network input 141 and processes the input 141 to generate a network output 161.  The neural network processing system 100 can store the\ngenerated network output 161 in an output data repository or provide the network output for use for some other immediate purpose, e.g., for presentation on a user device or for further processing by another system.\n The neural network 101 can be configured to receive any kind of digital data input and to generate any kind of score or classification output based on the input.\n For example, if the inputs to the neural network 101 are images or features that have been extracted from images, the output generated by the neural network 101 for a given image may be scores for each of a set of object categories, with each\nscore representing an estimated likelihood that the image contains an image of an object belonging to the category.\n As another example, if the inputs to the neural network 101 are Internet resources (e.g., web pages), documents, or portions of documents or features extracted from Internet resources, documents, or portions of documents, the output generated by\nthe neural network 101 for a given Internet resource, document, or portion of a document may be a score for each of a set of topics, with each score representing an estimated likelihood that the Internet resource, document, or document portion is about\nthe topic.\n As another example, if the inputs to the neural network processing system 100 are features of an impression context for a particular advertisement, the output generated by the neural network 101 may be a score that represents an estimated\nlikelihood that the particular advertisement will be clicked on.\n As another example, if the inputs to the neural network 101 are features of a personalized recommendation for a user, e.g., features characterizing the context for the recommendation, e.g., features characterizing previous actions taken by the\nuser, the output generated by the neural network 101 may be a score for each of a set of content items, with each score representing an estimated likelihood that the user will respond favorably to being recommended the content item.\n As another example, if the input to the neural network 101 is text in one language, the output generated by the neural network 101 may be a score for each of a set of pieces of text in another language, with each score representing an estimated\nlikelihood that the piece of text in the other language is a proper translation of the input text into the other language.\n As another example, if the input to the neural network 101 is a spoken utterance, a sequence of spoken utterances, or features derived from one of the two, the output generated by the neural network 101 may be a score for each of a set of pieces\nof text, each score representing an estimated likelihood that the piece of text is the correct transcript for the utterance or sequence of utterances.\n In particular, the neural network 101 includes one or more initial neural network layers 110 and a softmax output layer 120.  The initial neural network layers 110 can be the input 151 and hidden 152 and 153 of a feedforward or recurrent neural\nnetwork.  The initial neural network layers 110 are configured to receive a neural network input and process the neural network input to generate an initial layer output 160.  Generally, the initial layer output 160 is a vector or other ordered\ncollection of numeric values that includes a predetermined number of layer output values, i.e., the number of values that the final initial layer is configured to output.\n The softmax output layer 120 is configured to receive the initial layer output 160 and generate a network output 161 based on the initial layer output 160 by applying a softmax function to the layer output 160.\n In some implementations, the neural network processing system 100 modifies the initial layer output 160 prior to the initial layer output 160 being processed by the softmax output layer 120.  In particular, in these implementations, the neural\nnetwork processing system 100 modifies the layer output 160 by mapping each initial layer output value to another output value using mapping data.  The mapping data may map each value in the initial layer output 160 to a different value, e.g., a more\ncomplex value, or include a scaling factor to be applied to the values in the initial layer output 160 before the values are processed by the softmax output layer 120.  Mapping initial layer output values before the values are processed by a softmax\noutput layer is described in greater detail below with reference to FIG. 3.\n Generally, the softmax function normalizes the layer output 160 so each value in the network output 161 is a value within a predefined range (e.g., the predefined range of real values between 0 and 1, inclusive).  The softmax function can also\nbe referred to as the normalized exponential function.\n In particular, the neural network processing system 100 processes the network input 141 through the initial neural network layers 110 such that each value in the layer output 160 or, in implementations in which the neural network processing\nsystem 100 modifies the values in the layer output 160, each modified layer output value belongs to a predetermined finite set of possible values.  Reasons for why the layer output 160 may have this characteristic are described below with reference to\nFIG. 2.\n Because of this, the neural network processing system 100 can process the layer output 160 through the softmax output layer 120 in an efficient manner by computing a count of each of the possible values in the predetermined range and calculating\nexponentiation measures and normalization factors for the occurring values only once.  Generating a corresponding network output 161 for a given layer output 160 in this manner is described in more detail below with reference to FIG. 2.\n FIG. 2 is a flow chart of an example process 200 for generating a network output from a layer output.  For convenience, the process 200 will be described as being performed by a system of one or more computers located in one or more locations. \nFor example, a neural network processing system, e.g., the neural network processing system 100 of FIG. 1, appropriately programmed in accordance with this specification, can perform the process 200.\n The system obtains a layer output generated by processing a network input through the one or more initial neural network layers (210).  The layer output includes a predetermined number of layer output values, where each layer output value is a\nrespective one of a predetermined finite set of possible output values.  The predetermined finite set of possible output values is a finite set of values that may include a smaller number of values than the number of values that may be supplied to a\nsoftmax function over a period of time.\n The layer output values may include only values from the finite set as a result of the manner in which the processing device generating the layer output performs computations.  In some implementations, the layer output is generated by a\nprocessing device that performs computation specified by the one or more initial neural network layers with reduced precision, i.e., using quantized arithmetic.\n Quantized arithmetic involves representing a first set of values by a second set of values, where the second set includes fewer values than the first set.  For instance, the first set of values may be values of a real data type while the second\nset may be values of a fixed point data type.  Fixed point data types represent real values by a number that has a fixed number of digits after the radix point and as such can limit the size of each stored value and the range of possible values that can\nbe stored.  In processing systems that use quantized arithmetic, the second set of values to which the values of the first set are mapped may include a predetermined finite set of possible values.\n In some implementations, the finite set of possible output values include the set of values of an integer data type that has a bounded number of bits.  Examples of the predetermined finite set of possible output values include the integers\nincluded in the interval [-128, 127], if the system generates layer outputs that are of an 8-bit signed integer type, or the integers included in the interval [0, 255], if the system generates layer outputs that are of an 8-bit unsigned integer type. \nThe predetermined finite set of output values may include values of any one or more data types.  In exemplary implementations, the predetermined finite set of possible output values may include integers in the interval [0, 100], all ASCII characters, and\nthe following real numbers: 1.1, 2.2, 3.3, 4.4, 5.5, 6.6, 7.7, 8.8, and 9.9.\n The system determines, for each possible output value in the predetermined finite set, a number of occurrences of the possible output value in the layer output values (220).  In exemplary implementations, when the predetermined set of possible\noutput values includes integer values in the range [0, 7], the system counts the number of occurrences each of those integer values in the layer output values.  For instance, if the integer 0 has appeared five times in the layer output values, the system\ndetermines that count(0)=5.\n The system determines an exponentiation measure for each possible output value occurring at least once in the layer output values (230).  That is, for each x that is one of the possible output values in the predetermined finite set, if the count\nof x among the layer output values is one or more, then the system determines an exponentiation measure of x.\n Generally, the system determines the respective exponentiation measure of a given possible output value by exponentiating Euler's number (i.e. the number e) by the possible output value or by the result of the multiplication of the possible\noutput value by another value.  In some implementations, the system determines the respective exponentiation measure of each respective possible output value by computing e^X, where X is the respective possible output value.  In some other\nimplementations, the system determines the respective exponentiation measure of each respective possible output value by computing e^(X/T), where X is the respective possible output value and T is a temperature value.\n In some implementations, the system performs the exponentiation operation on the output of a function, such as an A-law mapping function, a .mu.-law mapping function, or a compression function.\n In some implementations, the system has pre-computed a respective exponentiation measure for each of the possible output values and accesses the corresponding precomputed value to determine the exponentiation measure of the possible output\nvalue.\n The system determines a normalization factor (i.e., the denominator of the softmax function) for the layer output by combining the exponentiation measures in accordance with the number of occurrences of the possible output values (240).  In some\nimplementations, for each member of the set of the possible output values in the predetermined finite set whose corresponding count is one or more, the system multiplies the count for the member by the exponentiation measure for the member.  The system\nthen generates the normalization factor by adding the results of those multiplications.\n The system determines a softmax probability value for each of the layer output values from the respective exponentiation measure for the layer output value and the normalization factor (250).  In particular, the system determines each softmax\nprobability value for a given layer output value by dividing the exponentiation measure for the layer output value by the normalization factor.\n In some implementations, each softmax probability value in the network output represents a normalized likelihood of a corresponding condition being satisfied.\n For example, when the neural network generates scores for object categories, each softmax probability value corresponds to a different object category and represents the normalized likelihood that the image includes an object belonging to the\ncategory.\n As another example, when the neural network generates scores for topics of Internet resources, documents, or portions of documents, each softmax probability value corresponds to a different topic and represents the normalized likelihood that the\nInternet resource, document, or portion of document is about the topic.\n As another example, when the neural network generates scores representing the estimated likelihood that advertisements will be clicked on, each softmax probability value represents the normalized likelihood that a particular advertisement will\nbe clicked on.\n FIG. 3 is a flow chart of an example 300 for mapping initial layer output values to layer output values.  For convenience, the process 300 will be described as being performed by a system of one or more computers located in one or more\nlocations.  For example, a neural network processing system, e.g., the neural network processing system 100 of FIG. 1, appropriately programmed in accordance with this specification, can perform the process 300.\n The system receives initial layer output values from a processing system that does the processing for the one or more initial neural network layers (310).  The initial layer output values are unmapped output values of the one or more initial\nneural network layers.\n The system obtains mapping data defining a mapping from the initial layer output values to the layer output values (320).  In exemplary implementations, when the predetermined set of possible output values includes integer values in the range\n[0, 7], the system obtains data mapping 0 to .pi., 1 to 2.pi., 2 to 3.pi., and so on (i.e. for each integer value x, the system maps x to (x+1)*.pi.).  In some implementations, the mapping data specifies a scaling factor for scaling each of the initial\nlayer output values to generate the layer output values.  A scaling factor is a number by which an initial layer output value will be multiplied to generate a corresponding layer output value.\n The system determines a layer output value for each initial layer output value based on the mapping data (330).  If the mapping data specify a layer output value corresponding to an initial layer output value, the system uses that specified\nvalue to determine the layer output value.  If the mapping data specify a scaling factor by which a layer output value is determined based on the initial layer output value, the system scales the initial layer output value with the scaling factor to\ndetermine the layer output value.\n Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, in tangibly-embodied computer software or firmware, in computer hardware, including the\nstructures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.  Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or\nmore modules of computer program instructions encoded on a tangible non-transitory storage medium for execution by, or to control the operation of, data processing apparatus.  The computer storage medium can be a machine-readable storage device, a\nmachine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them.  Alternatively, or in addition, the program instructions can be encoded on an artificially-generated propagated signal, e.g., a\nmachine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus.\n The term \"data processing apparatus\" refers to data processing hardware and encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors\nor computers.  The apparatus can also be, or further include, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).  The apparatus can optionally include, in addition to\nhardware, code that creates an execution environment for computer programs, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.\n A computer program which may also be referred to or described as a program, software, a software application, an app, a module, a software module, a script, or code) can be written in any form of programming language, including compiled or\ninterpreted languages, or declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment.  A program may, but\nneed not, correspond to a file in a file system.  A program can be stored in a portion of a file that holds other programs or data, e.g., one or more scripts stored in a markup language document, in a single file dedicated to the program in question, or\nin multiple coordinated files, e.g., files that store one or more modules, sub-programs, or portions of code.  A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across\nmultiple sites and interconnected by a data communication network.\n For a system of one or more computers to be configured to perform particular operations or actions means that the system has installed on it software, firmware, hardware, or a combination of them that in operation cause the system to perform the\noperations or actions.  For one or more computer programs to be configured to perform particular operations or actions means that the one or more programs include instructions that, when executed by data processing apparatus, cause the apparatus to\nperform the operations or actions.\n As used in this specification, an \"engine,\" or \"software engine,\" refers to a software implemented input/output system that provides an output that is different from the input.  An engine can be an encoded block of functionality, such as a\nlibrary, a platform, a software development kit (\"SDK\"), or an object.  Each engine can be implemented on any appropriate type of computing device, e.g., servers, mobile phones, tablet computers, notebook computers, music players, e-book readers, laptop\nor desktop computers, PDAs, smart phones, or other stationary or portable devices, that includes one or more processors and computer readable media.  Additionally, two or more of the engines may be implemented on the same computing device, or on\ndifferent computing devices.\n The processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output.  The processes\nand logic flows can also be performed by special purpose logic circuitry, e.g., an FPGA or an ASIC, or by a combination of special purpose logic circuitry and one or more programmed computers.\n Computers suitable for the execution of a computer program can be based on general or special purpose microprocessors or both, or any other kind of central processing unit.  Generally, a central processing unit will receive instructions and data\nfrom a read-only memory or a random access memory or both.  The essential elements of a computer are a central processing unit for performing or executing instructions and one or more memory devices for storing instructions and data.  The central\nprocessing unit and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.  Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage\ndevices for storing data, e.g., magnetic, magneto-optical disks, or optical disks.  However, a computer need not have such devices.  Moreover, a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a\nmobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device, e.g., a universal serial bus (USB) flash drive, to name just a few.\n Computer-readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash\nmemory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks.\n To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for\ndisplaying information to the user and a keyboard and pointing device, e.g., a mouse, trackball, or a presence sensitive display or other surface by which the user can provide input to the computer.  Other kinds of devices can be used to provide for\ninteraction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic,\nspeech, or tactile input.  In addition, a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a web browser on a user's device in response to\nrequests received from the web browser.  Also, a computer can interact with a user by sending text messages or other forms of message to a personal device, e.g., a smartphone, running a messaging application, and receiving responsive messages from the\nuser in return.\n Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that\nincludes a front-end component, e.g., a client computer having a graphical user interface, a web browser, or an app through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one\nor more such back-end, middleware, or front-end components.  The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network.  Examples of communication networks include a local area\nnetwork (LAN) and a wide area network (WAN), e.g., the Internet.\n The computing system can include clients and servers.  A client and server are generally remote from each other and typically interact through a communication network.  The relationship of client and server arises by virtue of computer programs\nrunning on the respective computers and having a client-server relationship to each other.  In some embodiments, a server transmits data, e.g., an HTML page, to a user device, e.g., for purposes of displaying data to and receiving user input from a user\ninteracting with the device, which acts as a client.  Data generated at the user device, e.g., a result of the user interaction, can be received at the server from the device.\n While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any invention or on the scope of what may be claimed, but rather as descriptions of features that may be\nspecific to particular embodiments of particular inventions.  Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment.  Conversely, various features\nthat are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination.  Moreover, although features may be described above as acting in certain combinations and even\ninitially be claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.\n Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations\nbe performed, to achieve desirable results.  In certain circumstances, multitasking and parallel processing may be advantageous.  Moreover, the separation of various system modules and components in the embodiments described above should not be\nunderstood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.\n Particular embodiments of the subject matter have been described.  Other embodiments are within the scope of the following claims.  For example, the actions recited in the claims can be performed in a different order and still achieve desirable\nresults.  As one example, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results.  In certain some cases, multitasking and parallel processing may be\nadvantageous.", "application_number": "15385642", "abstract": " Systems, methods, and apparatus, including computer programs encoded on a\n     computer storage medium for processing a network input through a neural\n     network having one or more initial neural network layers followed by a\n     softmax output layer. In one aspect, the methods include obtaining a\n     layer output generated by the one or more initial neural network layers\n     and processing the layer output through the softmax output layer to\n     generate a neural network output. Processing the layer output through the\n     softmax output layer includes determining, for each possible output\n     value, a number of occurrences in the layer output values; for each\n     possible output value occurring in the layer output values, determining a\n     respective exponentiation measure; determining a normalization factor for\n     the layer output by combining the exponentiation measures in accordance\n     with the number of occurrences of the possible output values; and\n     determining, for each of layer output values, a softmax probability\n     value.\n", "citations": ["20030236662", "20100094787", "20160063359", "20170068887"], "related": []}, {"id": "20180189544", "patent_code": "10372985", "patent_name": "System for simplified generation of systems for broad area geospatial\n     object detection", "year": "2019", "inventor_and_country_data": " Inventors: \nEstrada; Adam (Bethesda, MD), Green; Kevin (Aldie, VA), Jenkins; Andrew (Waterford, VA)  ", "description": "BACKGROUND OF THE\nINVENTION\n Field of the Art\n The present invention is in the field of image analysis, and more particularly in the field of the use of deep learning model computer vision systems for automated object identification from geospatial imagery.\n Discussion of the State of the Art\n Image analysis has been an important field of technology at least since the period of World War 2, when extensive use of image analysis, photogrammetry, and related technologies were used in conjunction with aerial photography for intelligence\nand bombing damage assessment purposes (among others).  However, the extent of the use of image analysis (particularly image analysis of remotely-sensed images), particularly for identifying or locating targets of interest, has always been limited by the\nneed for highly-trained, specialized image analysts or interpreters.  The need for specialized (and expensive) skills has limited the use of image analysis to a correspondingly limited range of applications (notably military, homeland defense, and law\nenforcement).\n The market for image analysis has also historically been limited by the high cost of obtaining images to analyze.  In the military arena, the benefits were sufficiently apparent that large numbers of military reconnaissance flights have been\nmade over regions of interest since World War 2.  But the cost of such flights virtually excluded all commercial applications of image analysis.  Starting in the 1970s with the Landsat satellite, conditions began to change as low resolution satellite\nimages became publicly available.  A series of new satellites has allowed for progressively more applications for quality geospatial imagery as the resolution, spectral coverage, geographic coverage, and cost per image have all continuously improved;\naccordingly, a significant market in commercial remote sensing imagery has emerged.  Unfortunately, even this market has been limited from achieving its full potential because of the still-present requirement for expensive, scarce image analysis talent.\n One common type of geospatial image analysis task is the \"search and locate\" task.  In this task, one or more targets of interest need to be identified and precisely located.  A well-known example of \"search and locate\" is the discovery and\npinpointing of warships, tanks, or other military targets of interest.  Recently, focused geospatial image analysis of geographically specific data has been used for search and rescue efforts of downed planes or lost shipping.  However, these efforts\nhave required the work of image analysts, significantly limiting the extent of the search.  Development of a method to identify targets of interest more rapidly, using less resources would allow the pursuit of less urgent but promising applications which\ninclude assessing the scope of a refugee crisis by for example counting tents in an area of interest, analyzing the change in infrastructure in developing nations, assessing numbers of endangered species, finding military hardware in areas previously not\nexpected to comprise such equipment, identifying previously unknown airstrips or camps where crime or terrorism may be in operation.  The ability to extend \"search and locate\" like tasks to large geological areas and efficiently perform them repetitively\nover time would allow the use of geospatial imagery to map remote regions, to track deforestation and re-forestation and to detect natural disasters in remote areas of the world.\n The notion of computer vision, specifically the reliable identification by a computer of particular objects has been an active pursuit within the field of computer science since the late 1960s.  Unfortunately, until recently, this pursuit has\nmet with little success except when both the object of interest and the background against which it is presented have been tightly controlled.  Barriers to advancement in computer object identification have been both technological and logical.  The\ntechnological barriers have been present because, like its biological counterpart, computer visual processing requires computational power and amounts of memory storage that have been prohibitive up until the last 15 years.  Advancement in the ability to\npack more transistors into the same volume while also reducing cost and the development of such specialized components as the graphics processing unit, which is optimized to perform calculations encountered during manipulation of visual data has brought\ncurrent hardware to the point where rapid, even real time, object identification is possible.  There has also been a significant maturation process in how computer scientists in the field program computers to analyze objects of interest.  Some early\nmethods have been to break each object of interest into a unique grouping of simple geometric shapes or to take advantage of unique shading patterns of each object to identify new instances of the desired object.  All of these early attempts gave results\nthat were extremely sensitive to such variables as lighting, exact object placement in the field of sample, and exact object orientation, sometimes to the degree that the object of interest was not identifiable in the original image without great care. \nCurrently, after great advancement in computer capabilities, advances in our understanding of biological vision, and advances in computer vision theory, methods of training computers to reliably identify specific objects of interest has emerged.  One\nextensively used method combines a convolutional neural network with deep learning to train the system to recognize an existing object of interest both when presented against many backgrounds and when the object is in different orientations.  The\nconvolutional neural network which consists of several layers of filters with partial, local field interconnections between layers interspersed with data complexity reduction pooling layers affords computer learning of object recognition with a minimum\nof pre-supposition on the part of the programmer as the convolutional neural network determines the best filters to employ to identify the target object.  Deep learning consists of a period of \"supervised learning\" which uses a moderate sized set of\ntraining images where each image comprises an example of the object to identify, for example, the human face, which is clearly demarcated or \"labeled\" followed by a period of \"unsupervised learning\" on a very large number of unlabeled images, a portion\nof which do not have the object to identify present in them.  The number of training images is proportional to the overall system's accuracy, specifically the precision and recall of the classification results.  Accordingly, the number of training images\nis inversely proportional to the amount of time the convolutional neural network-deep learning model spends training and further, searching and accurately finding objects of interest.  This convolutional neural network-deep learning model method has\ngiven rise to computer systems that have been reliably used in human facial recognition, optical character recognition, and identification of complex sets of parts during manufacturing.  Indeed, the convolutional neural network-deep learning model method\nhas been found so widely useful for object identification that there are multiple programming libraries now publicly available for download and use for that purpose.  These include, for example, the Caffe library (BerkeleyVision and Learning Center), the\nTorch7 library (Nagadomi) and the cuda-covnet2 library (Alex Krizhevsky).  Very recently the convolutional neural network-deep learning model has begun to be applied to the field of object identification and classification in orthorectified geospatial\nimages as set forth in U.S.  Pat.  No. 9,589,210 to Estrada, A et al. Despite this inroad into the use of automated computer system mediated object identification, which promises to significantly reduce cost of geospatial image analysis for\nidentification of specific objects of interest, the creation of the trained computer system remains labor intensive as the needed software engineering must be largely repeated for each object of interest.  While the existing automated geospatial object\nclassification system is a significant advancement, the remaining requirement to create each geospatial object classification system de novo reduces classification system generation efficiency and drives up cost, restricting usefulness from some\nworthwhile applications.\n What is needed in the art is an automated system that generates synthetic training images to augment the number of real training images needed for an automated system to both identify and determine the precise location of a number of objects of\ninterest from geospatial imagery.\nSUMMARY OF THE INVENTION\n The inventor has developed an engine for simplified generation of systems for analysis of satellite images to geolocate one or more targets of interest, or to identify objects or their types.\n According to an aspect, a pre-existing framework of reusable modules may drive the selection of unique features of an object of interest from a plurality of orthorectified geospatial training images where the object of interest is clearly\ndemarcated and a second plurality of orthorectified geospatial training images where objects bearing features that are to be excluded from the generation of one or more feature model for the object of interest are clearly labeled.  A generated feature\nmodel may then be used to seed a second pre-existing framework, which comprises at least one machine learning object classifier elements each pre-programmed to run one of the known machine learning protocols.  Using feature model as one set of parameters\nthe second framework accepts a large plurality of geospatial training images comprising the object of interest, either demarcated or unlabeled to train the system to reliably and reproducibly identify the object of interest.  Late in the training stage\nof this machine learning classifier element framework, geospatial images without instances of the object of interest may be submitted to further test classification specificity.  A third framework may accept the engineered, object of interest trained,\nmachine learning classifier module for use in identifying instances of the object of interest within resolution scale matched but unanalyzed production geospatial images.  Classification results from search and locate campaign for the object of interest\nmay be presented to the campaign author in a pre-determined format according to the specifications of the campaign.  The aspect greatly reduces the time and programming knowledge needed to run a search and locate campaign for one or more specific search\nobjects by using pre-existing pre-programmed frameworks for the stages of system preparation known to the art.\n According to an aspect, a system for simplified generation of systems for broad area geospatial object detection comprising: an object model creation module comprising a processor, a memory, and a plurality of programming instructions stored in\nthe memory and operable on the processor, wherein the plurality of programming instructions: retrieves a plurality orthorectified geospatial images where an object of interest has been demarcated from an established data store, retrieves a plurality of\northorectified geospatial images where objects that are not the object of interest, at least one of which closely resembles the object of interest have been demarcated from an established data store, programmatically isolates features found in the object\nof interest but not in the irrelevant training objects, creates at least one object of interest identification model using the features unique to the object of interest; and a machine learning classifier element training and verification module\ncomprising a processor, a memory, and a plurality of programming instructions stored in the memory and operable on the processor, wherein the plurality of programming instructions: accepts at least one identification model, retrieves a plurality of\nlabeled and unlabeled orthorectified geospatial training images comprising the object of interest, trains at least one pre-engineered machine learning classifier element each running a pre-programmed machine learning protocol parameterized with one\nidentification model using the plurality of labeled and unlabeled orthorectified geospatial training images comprising the object of interest, confirms specificity of trained machine learning classifier element for the object of interest using a\nplurality of unlabeled orthorectified geospatial training images comprising the object of interest and a plurality of unlabeled orthorectified geospatial training images that do not contain the object of interest and a model mediated object\nclassification module comprising a processor, a memory, and a plurality of programming instructions stored in the memory and operable on the processor, wherein the plurality of programming instructions: retrieve the trained machine learning element for\nthe object of interest analyze a plurality of resolution scale corrected, unanalyzed orthorectified geospatial image segments for presence of at least one objects of interest, reports the results of a predetermined orthorectified geospatial image survey\nfor the object of interest in a report format predirected by a survey author.\n According to another aspect, a method for simplified generation of systems for broad area geospatial object detection the step comprising: (a) retrieving a plurality of color and spectrally optimized geospatial training images comprising an\nobject of interest that is clearly labeled and a second plurality of color and spectrally optimized geospatial training images that do not contain the object of interest to isolate a set of visual features unique to the object of interest using an\npre-engineered object model creation module comprising a processor, a memory, and a plurality of programming instructions stored in the memory and operable on the processor; (b) employing the set of visual features unique to the object of interest to\nparameterize at least one pre-engineered machine learning classifier element running at least one pre-programmed machine learning programming protocol using a machine learning classifier element training and verification module comprising a processor, a\nmemory, and a plurality of programming instructions stored in the memory and operable on the processor; (c) training pre-engineered machine learning classifier elements to identify the object of interest using a plurality of training geospatial images\nwith the object of interest labeled in one subset and not labeled in a second subset within the machine learning classifier element training and verification module; (d) refining and confirming the fine specificity of trained machine learning classifier\nelements for the object of interest to the exclusion of other objects using geospatial training images not containing the object of interest, some of which comprise other irrelevant objects; and (e) analyzing previously unanalyzed, scale corrected\ngeospatial images for presence of the object of interest using trained machine learning classified element and reporting the results of the study in a format pre-determined by the study author. BRIEF DESCRIPTION OF THE DRAWING FIGURES\n The accompanying drawings illustrate several aspects and, together with the description, serve to explain the principles of the invention according to the aspects.  It will be appreciated by one skilled in the art that the particular\narrangements illustrated in the drawings are merely exemplary, and are not to be considered as limiting of the scope of the invention or the claims herein in any way.\n FIG. 1 is a diagram of an exemplary architecture for a software engineered engine for simplified generation of systems for automated classification of objects in broad area geospatial images that uses the deep learning model and machine learning\nclassifiers, according to an aspect.\n FIG. 2 is a process flow diagram of a method for feature selection from objects to be identified during geospatial image analysis that uses the deep learning model and machine learning classifiers using an embodiment.\n FIG. 3A is a process flow diagram of a method for training of machine learning classifiers for later use in production geospatial image analysis using an aspect.\n FIG. 3B is a process flow diagram of a method for confirming object identification fidelity of trained machine learning classifiers prior to later use in production geospatial image analysis using an aspect.\n FIG. 4 is a process flow diagram of a method for employing newly trained machine learning classifiers to identify the trained object in production geospatial image analysis using an aspect.\n FIG. 5 is a diagram that illustrates the training of machine learning identifiers using an aspect.\n FIG. 6 is a process flow diagram of a method for training and verification of machine learning classifiers for later use in production geospatial image analysis using an aspect.\n FIG. 7 is a diagram that illustrates the optimization and normalization of geospatial images prior to object identification using an aspect.\n FIG. 8 is a diagram that illustrates the use of a sliding window module to translate the coordinate system used within the cache of geospatial images being analyzed and the geographical longitude and latitude system as part of the automated\nsystem for geospatial image analysis of the invention.\n FIG. 9 is a two part, FIG. 9A and FIG. 9B, process flow diagram summarizing the generation of synthetic training geospatial images comprising the campaign object.\n FIG. 10 is a block diagram illustrating an exemplary hardware architecture of a computing device used in various embodiments of the invention.\n FIG. 11 is a block diagram illustrating an exemplary logical architecture for a client device, according to various embodiments of the invention.\n FIG. 12 is a block diagram illustrating an exemplary architectural arrangement of clients, servers, and external services, according to various embodiments of the invention.\n FIG. 13 is a block diagram illustrating an exemplary overview of a computer system as may be used in any of the various locations throughout the system\nDETAILED DESCRIPTION\n The inventor has conceived, and reduced to practice, an engine for simplified generation of systems for broad area geospatial object detection using auto-generated deep learning models trained by actual and/or virtual images.\n One or more different inventions may be described in the present application.  Further, for one or more of the inventions described herein, numerous alternative embodiments may be described; it should be understood that these are presented for\nillustrative purposes only.  The described embodiments are not intended to be limiting in any sense.  One or more of the inventions may be widely applicable to numerous embodiments, as is readily apparent from the disclosure.  In general, embodiments are\ndescribed in sufficient detail to enable those skilled in the art to practice one or more of the inventions, and it is to be understood that other embodiments may be utilized and that structural, logical, software, electrical and other changes may be\nmade without departing from the scope of the particular inventions.  Accordingly, those skilled in the art will recognize that one or more of the inventions may be practiced with various modifications and alterations.  Particular features of one or more\nof the inventions may be described with reference to one or more particular embodiments or figures that form a part of the present disclosure, and in which are shown, by way of illustration, specific embodiments of one or more of the inventions.  It\nshould be understood, however, that such features are not limited to usage in the one or more particular embodiments or figures with reference to which they are described.  The present disclosure is neither a literal description of all embodiments of one\nor more of the inventions nor a listing of features of one or more of the inventions that must be present in all embodiments.\n Headings of sections provided in this patent application and the title of this patent application are for convenience only, and are not to be taken as limiting the disclosure in any way.\n Devices that are in communication with each other need not be in continuous communication with each other, unless expressly specified otherwise.  In addition, devices that are in communication with each other may communicate directly or\nindirectly through one or more intermediaries, logical or physical.\n A description of an embodiment with several components in communication with each other does not imply that all such components are required.  To the contrary, a variety of optional components may be described to illustrate a wide variety of\npossible embodiments of one or more of the inventions and in order to more fully illustrate one or more aspects of the inventions.  Similarly, although process steps, method steps, algorithms or the like may be described in a sequential order, such\nprocesses, methods and algorithms may generally be configured to work in alternate orders, unless specifically stated to the contrary.  In other words, any sequence or order of steps that may be described in this patent application does not, in and of\nitself, indicate a requirement that the steps be performed in that order.  The steps of described processes may be performed in any order practical.  Further, some steps may be performed simultaneously despite being described or implied as occurring\nnon-simultaneously (e.g., because one step is described after the other step).  Moreover, the illustration of a process by its depiction in a drawing does not imply that the illustrated process is exclusive of other variations and modifications thereto,\ndoes not imply that the illustrated process or any of its steps are necessary to one or more of the invention(s), and does not imply that the illustrated process is preferred.  Also, steps are generally described once per embodiment, but this does not\nmean they must occur once, or that they may only occur once each time a process, method, or algorithm is carried out or executed.  Some steps may be omitted in some embodiments or some occurrences, or some steps may be executed more than once in a given\nembodiment or occurrence.\n When a single device or article is described, it will be readily apparent that more than one device or article may be used in place of a single device or article.  Similarly, where more than one device or article is described, it will be readily\napparent that a single device or article may be used in place of the more than one device or article.\n The functionality or the features of a device may be alternatively embodied by one or more other devices that are not explicitly described as having such functionality or features.  Thus, other embodiments of one or more of the inventions need\nnot include the device itself.\n Techniques and mechanisms described or referenced herein will sometimes be described in singular form for clarity.  However, it should be noted that particular embodiments include multiple iterations of a technique or multiple instantiations of\na mechanism unless noted otherwise.  Process descriptions or blocks in figures should be understood as representing modules, segments, or portions of code which include one or more executable instructions for implementing specific logical functions or\nsteps in the process.  Alternate implementations are included within the scope of embodiments of the present invention in which, for example, functions may be executed out of order from that shown or discussed, including substantially concurrently or in\nreverse order, depending on the functionality involved, as would be understood by those having ordinary skill in the art.\n Definitions\n As used herein, \"orthorectified geospatial image\" refers to satellite imagery of the earth that has been digitally corrected to remove terrain distortions introduced into the image by either angle of incidence of a particular point from the\ncenter of the satellite imaging sensor or significant topological changes inherent to the region of the earth that the image depicts.  This correction is accomplished using a digital elevation model.  One example of a digital elevation model in use today\nis Shuttle Radar Topography Mission (SRTM) 90m DEM data set, but others of equal or greater precision have also been created using high definition stereoscopic satellite imagery of the same regions as are being analyzed or using topographical maps of\nsufficient detail available for that region.  Geospatial images used as part of the invention may be orthorectified using digital elevation model datasets obtained by any method known to the art.\n A \"database\" or \"data storage subsystem\" (these terms may be considered substantially synonymous), as used herein, is a system adapted for the long-term storage, indexing, and retrieval of data, the retrieval typically being via some sort of\nquerying interface or language.  \"Database\" may be used to refer to relational database management systems known in the art, but should not be considered to be limited to such systems.  Many alternative database or data storage system technologies have\nbeen, and indeed are being, introduced in the art, including but not limited to distributed non-relational data storage systems such as Hadoop, column-oriented databases, in-memory databases, and the like.  While various embodiments may preferentially\nemploy one or another of the various data storage subsystems available in the art (or available in the future), the invention should not be construed to be so limited, as any data storage architecture may be used according to the embodiments.  Similarly,\nwhile in some cases one or more particular data storage needs are described as being satisfied by separate components (for example, an expanded private capital markets database and a configuration database), these descriptions refer to functional uses of\ndata storage systems and do not refer to their physical architecture.  For instance, any group of data storage systems of databases referred to herein may be included together in a single database management system operating on a single machine, or they\nmay be included in a single database management system operating on a cluster of machines as is known in the art.  Similarly, any single database (such as an expanded private capital markets database) may be implemented on a single machine, on a set of\nmachines using clustering technology, on several machines connected by one or more messaging systems known in the art, or in a master/slave arrangement common in the art.  These examples should make clear that no particular architectural approaches to\ndatabase management is preferred according to the invention, and choice of data storage technology is at the discretion of each implementer, without departing from the scope of the invention as claimed.\n As used herein, \"search and locate\" refers to a general class of tasks wherein a set of images is searched for particular classes of stationary targets (such as buildings, tanks, railroad terminals, downed airplanes, etc.) or relocatable targets\n(such as missile launchers, aircraft carriers, oil rigs, earthmovers, tower cranes, etc).  It is common that the set of images may be searched to find more than one class of targets (for example, to find all targets of military interest), although single\ntarget class searches may also be performed (\"find all cars\").  The second part of the search and locate task is to precisely locate any resulting targets of interest (where is the air base or refugee camp?).\n As used herein, \"image manipulation\" refers to a way of generating artificial, \"manipulated images\" and respectively labeling them to simulate real geospatial images, such that a plurality of synthetic images may be automatically generated\nwithout the need for manual effort.  Image manipulation greatly reduces the extensive manual effort currently required to extract and label existing data, even when it is relatively abundant.  Further, image manipulation may be utilized to create\nsynthetic data for an object class for rare or even theoretical items.  By using synthetic data, any object that may be simulated, modeled or otherwise created by computer-aided design (CAD), may be searched by a preferred embodiment of the invention.\n As referenced herein, \"manipulated images\" refer to synthetic images, which may be modeled, flattened, or otherwise virtually generated or modified images, to replicate real, existing orthorectified geospatial images created from image\nmanipulation.  These images may be used to generate a collection of training images for a searchable object class.\n As used herein, \"cache of pre-labeled geospatial images\" refers to any source of a plurality of orthorectified geospatial image segments that have been pre-analyzed and have had instances of one or more objects of interest tagged or labeled in\nsuch a way that the recipient computer system is able to associate a specific region of that image with the object of interest for the purpose of subsequent identification of like objects.  These images may be stored in an image database, either\nrelational or flat file, or within a directory of image files, any of which may be stored on the same computer on which the images are being used, a storage device or storage system directly connected to that computer or may be on a computer or storage\nsystem connected to the recipient computer through any of the networking methods as are known in the art.\n As used herein, \"cache of multi-scale geospatial images\" refers to any source of a plurality of overlapping orthorectified geospatial image segments that, due to optical differences at the time of capture or processing differences at the time of\ntransmission, storage or analysis, show the same geographical region at different functional resolutions.  There is the further requirement that the correspondence of coordinate system used to catalog these segments within the cache, whether proprietary\nor open, to standard geographic latitude and longitude coordinates be known so that the location being analyzed on a given image segment from the cache is known at all times.  These images may be stored in an image database, either relational or flat\nfile, or within a directory of image files, any of which may be stored on the same computer on which the images are being used, a storage device or storage system directly connected to that computer or may be on a computer or storage system connected to\nthe recipient computer through any of the networking methods as are known in the art.\n As used herein, \"image analysis\" refers to the analysis of images obtained from one or more image sensors; generally, a single analysis task focuses on a set of images of a single region of interest on the earth, but image analysis may be done\non multiple contiguous regions as captured by several image sensors.  Satellite and aerial imagery are common examples of imagery that are subjected to large scale image analysis.\n As used herein \"image correction and optimization module\" refers to a set of programming functions that during its operation receives a plurality of orthorectified geospatial images from a cache of pre-labeled geospatial images, normalizes these\nimages to account for image quality differences which include but are not limited to variations in color balance, brightness, and contrast.  This module also analyzes images for aberrations which might include cloud cover, lens artifact, mechanical\nobstruction of portions of the image and the software within the module may then reject the image from analysis when certain pre-set thresholds are exceeded.\n As used herein \"category\" refers to a set of specific objects that are of the same type and function, but which may vary to some degree in appearance.  An example of this might be the United States Capitol Building, the White House and the\nPentagon in Washington DC all appear different in geospatial images but are all in the category \"buildings.\" Another example might be that the Airbus 310, Lockheed L1011, Boeing 727, Boeing 777 and Boeing 747 all differ in size and fine level\nconfiguration, but are all in the category \"airliners.\"\n Conceptual Architecture\n FIG. 1 is a diagram of an exemplary architecture 100 for a software engineered engine for simplified generation of systems for automated classification of objects in broad area geospatial images that uses the deep learning model and machine\nlearning classifiers, according to an aspect.  The engine to generate accurate systems for classification of objects of interest from broad area geospatial images may be broken down into three sub-assemblies: The object model creation sub-assembly 111;\nthe machine learning classifier element training and verification sub-assembly 112; and use of the trained machine learning classifier elements to identify the trained objects on unanalyzed geospatial images 113.  A plurality of training images labeled\nfor presence of the object to be searched as selected by a campaign analyst 190 are generated in the training image synthesis module 145 as described in greater depth in FIG. 9, 900, 950.  Briefly, a 3D model of the object to be searched is selected from\na cache of such models (not drawn).  This 3D model may be flattened to a 2D model image from an overhead perspective and then overlaid into a geospatial image.  This synthetic image may then be compared to a real image 150, 190 comprising the object in\nthe same orientation and, if recognized as the same object, a plurality of synthetic geospatial training images overlaid with the model object in several orientations and modified to recreate multiple factors such as but not limited to geographic\nlocations, environmental factors, seasonal effects, and or shadowing effects accounting for sun angle, time of day, year as well as shadowing from localized light sources among other factors know to those skilled in the field.  The object of interest may\nthen be digitally demarcated for use in object feature selection by the object model creation sub-assembly 111 and for machine learning classifier element training and verification.  Training images, either synthetic as described or unmodified geospatial\nimages that have been manually analyzed and the object demarcated, are placed in a training image data store 110.\n During object model creation 111 the analyst may introduce multiple images in which the object to be searched is clearly demarcated such that the object feature extraction module encounters the object to be searched is present in many different\norientations and under a plurality of environmental conditions.  Concurrently, a plurality of training geospatial images with a plurality of objects, at least some of which closely resemble the campaign search object (confounding objects) may be released\ninto the object model creation assembly and clearly labeled as \"negative samples\" to assist the selection of object features highly specific to the search object to the exclusion of both confounding irrelevant and visually distinct objects during optical\nmodel generation.  Prior to use in the object feature extraction module 115a image segments both for positive and negative feature selection from the cache of pre-labeled geospatial image segments 110 may undergo one or more steps of digital image\ncorrection and optimization 115a.  A digital correction that may be done to image segments to be used by deep learning model training module is conversion from color to grayscale as this correction reduces image segment complexity, which aids in the\nfeature selection process.  In a preferred embodiment of the system the conversion is done by first converting the image from the RGB colorspace to the YCrCb colorspace and then discarding all but Y channel data which results in a grayscale image of\ntonal quality known to work well in deep learning model training used in the invention.  The method of color to grayscale image conversion outlined here is meant only to be exemplary and should not be seen to limit conversion method that may be used,\nincluding the absence of this color to grayscale conversion in the training image preparation process.  Another type of image correction that may be employed to prepare pre-labeled geospatial images for use in feature selection for object model\ngeneration is histogram normalization which often increases image contrast and serves to reduce the effects of exposure by producing sets of image segments with similar dynamic range profiles prior to use in feature extraction 120.  Examples of histogram\nnormalization filters that may be used to prepare geospatial images for training are linear histogram normalization, histogram equalization, adaptive histogram equalization, and contrast limiting adaptive histogram equalization.  One skilled in the art\nwill realize that while the use of these image histogram manipulation methods may produce image segments significantly better suited for the object feature extraction 120 during object model creation 125, the system described herein does not absolutely\nrely on histogram normalization in its generalized form or and of the exemplary histogram manipulation methods specifically.  In addition to or in lieu of to those mentioned in some depth, image filters such as Gaussian, median filter, and bilateral\nfilter to enhance edge contrast may be applied to pre-labeled geospatial image segments as is common in the art, however the listing of these filters is meant only to provide examples and should not be taken to bar the use of other filters that are not\nlisted as part of the aspects.  Upon programmatic analysis of features belonging to the search object in comparison to features found in irrelevant, visually unrelated and visually similar irrelevant objects, the object model creation sub-assembly 111\nwill generate at least one model derived from the unique features of the search object to be employed during the subsequent machine deep learning stage of the aspect using the object models creation module 125.\n At least one of the generated models from the object model creation subassembly 111 may be employed to train a model mediated object classification module 165 by a machine learning classifier element training and verification sub-assembly 112. \nThe embodiment may select a model for use based upon pre-programmed parameters stemming from previous campaign successes although many other methods of selection known to those skilled in the art may also be used 130.  Both supervised and unsupervised\nlearning is employed at this stage using demarcated images retrieved from a training image data store 110.  Digital correction and optimization may be done 115b to image segments to be used by a model training module 135 such as but not limited to,\nconversion from color to grayscale as this correction reduces image segment complexity, which aids in the training process.  In a preferred aspect of the system the conversion is done by first converting the image from the RGB colorspace to the YCrCb\ncolorspace and then discarding all but Y channel data which results in a grayscale image of tonal quality known to work well in deep learning model training used in the embodiment.  The method of color to grayscale image conversion outlined here is meant\nonly to be exemplary and should not be seen to limit conversion method that could be used, including the absence of this color to grayscale conversion in the training image preparation process.  Another type of image correction that may be employed to\nprepare both pre-labeled and non-labeled geospatial images for use in training is histogram normalization which often increases image contrast and serves to reduce the effects of exposure by producing sets of image segments with similar dynamic range\nprofiles prior to use in training of the model training module 135.  Examples of histogram normalization filters that may be used to prepare geospatial images for training are linear histogram normalization, histogram equalization, adaptive histogram\nequalization, and contrast limiting adaptive histogram equalization.  One skilled in the art will realize that while the use of these image histogram manipulation methods may produce image segments significantly better suited for the supervised stage of\ndeep learning of machine learning classifiers 135 which may act individually or in combination as also illustrated in FIG. 5, the aspect described herein does not absolutely rely on histogram normalization in its generalized form or and of the exemplary\nhistogram manipulation methods specifically.  In addition to or in lieu of to those mentioned in some depth, image filters such as Gaussian, median filter, and bilateral filter to enhance edge contrast may be applied to pre-labeled geospatial image\nsegments 115b as is common in the art, however the listing of these filters is meant only to provide examples and should not be taken to bar the use of other filters that are not listed as part of the embodiment.  Supervised learning, which may take\nplace in a model training module 135, comprises the introduction of images in which the object of interest is clearly demarcated with the analyst monitoring object classification success rate although some unlabeled images known to have the object\npresent may be used.  In unsupervised learning which may also take place in a model training module 135, a large plurality of images, many of which include the object of which some may be demarcated while others may be unlabeled but contain the object\nand still others may not contain the object, but may contain irrelevant and even confounding irrelevant objects, objects felt to visually resemble the object of interest in some way, but are not of interest to the campaign.  For example, if the campaign\nis to determine presence of tanker trucks, confounding objects may comprise flatbed trucks, box trucks, and rows of parked passenger vehicles among other objects visually similar to tanker trucks but not listed here.  As the name implies, unsupervised\nlearning may occur with minimal analyst intervention and monitoring, review of the session possibly occurring only after its completion to assess the efficacy of object classification tuning brought about and the endpoint fidelity of the trained system\nat the end of the process.  Depending on the complexity of the search object and training success, further rounds of supervised and unsupervised training may be undertaken or, in some cases, another model generated by the object model creation\nsubassembly 111 may be substituted 130 and training repeated to attain the specifications of the campaign.  There may also be a separate round of object identification fidelity testing 140, depending on such factors that may range from the design of the\ncampaign, the desire of the analyst or the complexity of the search object, to expected background conditions (ex.  special atmospheric or lighting conditions at time of geospatial image sampling, density confounding objects such as classifying tanker\ntrucks in a congested area with many box, flatbed and other similar machinery present, or other situation known to those skilled in the field to make object identification more difficult).\n The final sub-assembly of the engine involves use of the newly trained model mediated classification module 165 in monitored production use 113.  Trained machine learning classifier elements 165 to identify the trained search objects on\nunanalyzed geospatial images 155 which may have been optimized using previously mentioned desaturation and histogram normalization filters among others.  Geospatial images used may come from multiple caches originating from collection systems with widely\ndifferent specifications and therefore may differ greatly in resolution scale in addition to the previously addressed chromatic and spectral differences 160.  These production images may thus be normalized during analysis using a multi-scale sliding\nwindow filter (see FIG. 8) to equalize the resolution scale of the geospatial image region under analysis.  Alternatively, the campaign analyst may choose a sub-region of a geospatial image that is felt to most likely have the search object and convert\nthe sub-region to a resolution scale equal to that of the training images 160 as resolution is an important controlled factor for successful object classification.  Identification of the search object within a segment of a geospatial image may result in\nthe recording of a unique identifier for the point where the object is found such as the longitude and latitude and the number of objects present, although use of other identifiers is possible.  These object search related data are then presented to\ncampaign analysts in a pre-designated format 170.\n FIG. 2 is a process flow diagram of a method for feature selection from objects to be identified during geospatial image analysis that uses the deep learning model and machine learning classifiers using an embodiment 200.  To select features\nthat may be useful in classifying a specific object of interest to a search and locate campaign, training images where the object of interest which is to be identified by the trained module are clearly labeled are used 201.  Features are selected by the\naspect using unique combinations such as but not limited to shape, texture, color and spectral properties.  Feature selection may rely on the use of both positive samples, where the object of interest is presented in a plurality of orientations and\nlighting effects, the presence and location of the object clearly demarcated for system training and the use of negative samples, where geospatial images may comprise labeled objects that appear completely visually distinct from the object of interest to\nassist in the construction of a set of features that are not present on the search object, and may also include a plurality of pre-demarcated objects that are not the search object but visually closely resemble the search object, for example fighter\njets, prop driven passenger craft and large cargo planes, prop or jet driven when the search object to be classified is passenger jets.  This is particularly important where the feature differences between the search object and objects to be excluded are\nsmall in number as it provides not only features that may identify the search object, but also features which may exclude visually related irrelevant objects 202.  All images are then normalized and optimized as previously described in FIG. 1 for the\nfeature selection purpose 203.  The embodiment object feature extraction module 120 then works to select visual features to predictively best isolate the campaign search object from all other possible objects that may be found in geospatial images using\nat least one round of supervised learning with a plurality of clearly demarcated positive samples and negative samples and unsupervised learning with a large plurality of synthetic and unmodified training images 204.  Once training is completed one or\nmode classification models, groupings of visual relationships and rules (features) uniquely found on the search item in geospatial images of controlled resolution as well as groupings of visual relationships and rules (features) never found on the search\nobject are generated for use in training an aspect to automatically classify the search object in unanalyzed production geospatial images 205.\n FIG. 3A is a process flow diagram of a method for training of machine learning classifiers for later use in production geospatial image analysis using an aspect 300.  Machine learning classifier training begins with the retrieval 301 and\nselection 302 of one of the object models generated by the object models creation module 125.  With the selected model active in the training aspect, a plurality optimized training images where the object of interest, present in multiple orientations is\nclearly labeled 303 which are used to iteratively train one or more deep learning object classifier elements which may make use machine learning paradigms such as but not limited to convolutional neural network, vector support machines or random forest\nfilters among other machine learning methods known to those skilled in the art 305.  To increase the number of training images available, a great plurality of training images where the search object is present but not labelled may be used to further\ntrain the machine learning elements that may eventually be used for search object classification in a production setting 304.\n FIG. 3B is a process flow diagram of a method for confirming object identification fidelity of trained machine learning classifiers prior to later use in production geospatial image analysis using an aspect 310.  To confirm the specificity of\nthe trained machine learning element(s) that make up the search object specific trained model mediated object classification module 165, the aspect is selected 311 and subjected to testing using a plurality of training geospatial images that have 312 or\ndo not have the search item present within them and may instead comprise an image with a confounding object to confirm fine specificity 313.  Test 314 classification specificity is then measure to determine whether fidelity is high enough for campaign\nspecifications 315.  If specificity is at or above requirement, the trained model mediated object classification module may be used for the production campaign 316.  Otherwise, further training may be undertaken with the same object identification module\nmodel or a second model may be chosen, if available 317.\n FIG. 4 is a process flow diagram of a method for employing newly trained machine learning classifiers to identify the trained object in production geospatial image analysis using an aspect 400.  Once trained to the level of specificity needed, a\ntrained model mediated classification module may be retrieved into operation 401 and then used to search for the object of interest in a plurality of unanalyzed geospatial image segments from a geographical region of interest to the search campaign 402. \nThese unanalyzed geospatial images may be programmatically optimized for object identification which may include use of a multi-scale sliding window function to place the image in the resolution for object search 403.  Once the object search and\nclassification using the trained machine learning classifiers is completed 404, data gathered about the search objects found which may include, among other information, latitude and longitude coordinates, are presented to the analyst in a format\npre-designated by the campaign author 405.\n FIG. 5 is a diagram that illustrates the training of machine learning identifiers using an aspect 500.  Training starts with the retrieval of a plurality of training images 510 which may be real or synthesized as described in FIG. 9A and FIG.\n9B.  These training images may either be labeled or unlabeled.  A sub-set of these retrieved images may not have the search object.  In an aspect, training occurs within the model training module illustrated in FIG. 1, 135 Within the model training\nmodule 135, training images may be routed to one or more machine learning elements 550a-550n depending on the design of the campaign and the combination of machine learning protocols found to work optimally for the search object.  Machine learning\nelements may be running protocols for one of the machine learning algorithms known to those skilled in the art such as but not limited to vector support machines, random forest, Bayesian network, native Bayes classifier, and convoluted neural network. \nSearch object classification may use a single machine learning element, for example machine learning element C 550c, or may use multiple machine learning elements, for example machine learning element B 550b and machine learning element C 550c in a\nseries that leads to the most accurate and reproducible identification of the search object.  For example, machine learning element A 550a running the vector support machines protocol may be used followed by machine learning element B 550b running the\nrandom forest protocol.  Of course, any progression of machine learning protocols may be used in any order depending on the requirements of attaining reliable and reproducible classification of the current search object.\n It should be noted that not all possible connections between the machine learning elements have been drawn for the sake of presentation clarity.  Search object classification data may flow directly between any machine learning element 550a-550n\nin any order required for optimal search object classification according to the aspect.  While ensembles of only up to two machine learning elements have been described for presentation brevity, it should be understood that there is no programmatic limit\non the number of machine learning elements that may participate in an ensemble and no design restriction that prevents a single machine learning element from participating twice in the same ensemble should optimal search object classification dictate\nsuch.\n FIG. 6 is a process flow diagram of a method for training and verification of machine learning classifiers for later use in production geospatial image analysis using an aspect 600.  Training progresses as first a plurality of optimized\northorectified, geospatial images a number of which may be synthetically generated (see FIG. 9A and FIG. 9B), all bearing a labeled example of a chosen campaign search object, are provided to a model training module (FIG. 1, 135) followed by mixtures of\nboth labeled and unlabeled optimized orthorectified, geospatial images 601.  Within the model training module, programming functions comprising multiple machine learning protocols such as but not limited to vector support machines, random forest,\nBayesian network, native Bayes classifier, and convoluted neural network among other machine learning protocols known applicable by those skilled in the art, are employed either individually or in ensemble as required for optimally reliable and\nreproducible classification of the campaign search object.  Ensembles may employ two or more machine learning functions during search object training and there are no aspect design restrictions against reuse of the same machine learning protocol should\nobject classification reliability and reproducibility dictate such iterative application 602.  Additionally, trained candidate model mediated classification modules may be tested for reliable and reproducible search object classification using a\nplurality of unlabeled optimized orthorectified geospatial images that include the search object in many different orientations lighting and environmental conditions as well as irrelevant and confounding objects that have visual features that resemble\nthose of the search object also in a plurality of orientations, lighting and environmental settings 603.\n FIG. 7 is a diagram that illustrates the optimization and normalization of geospatial images prior to object identification using an aspect 700.  Geospatial images must be optimized and normalized prior to analysis for the presence of a search\nobject.  Images in one of more image stores 150 that are from a region or regions included in a search and locate campaign may undergo modification specific to that campaign during the selection process 710.  Based upon the requirements of the campaign,\nthe level of certainty regarding the predicted location of the search object and the pre-planning of campaign analysts, geospatial images may be subdivided into segments of resolution scale to allow object identification 720.  The selected geospatial\nimage or selected geospatial image segment may then undergo color correction to create working images with color tone similar to those of training images 730 and spectral normalization to create uniformity between campaign selected images and training\nimages to promote optimal object classification success within the study image data 740.  Upon completing this normalization, image segments that are already appropriately scaled 720 may be passed directly to the model mediated object classification\nmodule (see FIG. 1, 165) whereas geospatial images that are of differing resolution scale from training images may be analyzed employing a scale factoring multi-scale sliding window resolution normalizing filter 750, (See also FIG. 8) prior to object\nclassification analysis.\n FIG. 8 is a three panel diagram 800 that illustrates the operation of a sliding window module to translate the coordinate system used within the cache of geospatial images being analyzed and the geographical longitude and latitude system as part\nof the automated system (see FIG. 1, 113) for geospatial image analysis of the aspect.  Establishment of the location of any identified objects of interest generally requires that conversion of the coordinate system used internal to the cache of\nmulti-scale unanalyzed geospatial image segments 150 to the coordinates of earth latitude and longitude takes place.  Cached orthorectified geospatial image segments, or tiles 810, are regularly stored in caches using cache-internal coordinate systems\n811, 812 of which the tile map service system and the web map tile service are two standardized examples that the invention might encounter.  It should be clear, however, that the mention of these two systems should not be construed as the only possible\ncoordinate systems that method 400 might use, as the aspect does not depend on any specific system and tiles encoded by any coordinate system known to those skilled in the art may be used, provided that that system supplies a conversion pathway to\ngeographical latitude and longitude and also includes scaling information.  In one embodiment of this design, the image cache service might provide internal row and column numbers of an image segment's origin 813 in addition to a scaling factor of the\nsegment that is supplied.  From this segment-specific information and any additional cache coordinate system specification information, sliding window subroutines of the invention may easily and accurately convert from the cache's coordinate system to\nstandard geographical longitude 821 and latitude 822 for any point of the image segment that is undergoing analysis 820.  Specifically this is done by subroutines in the sliding window software module which, as the window of focus scans a first image\nsegment for objects of interest 830, keeps track of the change in latitude and longitude using the equation new=asin(sin old.times.cos+cos old.times.sin.times.cos) for change in latitude and the equation new=old+atan2(sin.times.sin.times.cos\nold.times.cos-sin old.times.sin new) for longitude where is latitude, is longitude, is bearing (clockwise from north) and is angular distance traveled ((scale corrected distance traveled/radius of the earth)) 831.\n FIG. 9 is a two part, FIG. 9A, 900 and FIG. 9B, 950 process flow diagram summarizing the generation of synthetic training geospatial images comprising the campaign object for geospatial image analysis that uses a method similar to that of 200,\ndescribed above; however, the deep learning models 135, 550a-550n and the image analysis software 113 are combined with an image manipulation software module 145 using an aspect.  According to the embodiment, a search and locate function FIG. 9A, 901 is\nconducted to identify and locate an object or item of interest from orthorectified geospatial imagery, regardless of whether the object is real or imaginary, so long as the searched object of interest was, is or could be, now or in the future, tangible\nand occupy 3-dimensional space.  Method 900, 950 allows for a \"search and locate\" action 901 to be executed on any pre-labeled searchable object or set of objects.  Using method 900, 950, a synthetic image may be generated for a specified object or item\nof interest, as specified in the search function 901.  Searchable objects may be retrieved from a database of modeled objects 903 in response to the search function 901 for the object of interest.  By searching the 3-dimensional database of models 903,\nthe training image manipulation software module 145 retrieves a 3-dimensional model 902 of the object of interest, creates a flattened 2-dimensional synthetic image 904 from the selected 3-dimensional model and compares 905 the 2-dimensional synthetic\nimage 904 to real, geospatial imagery of the same object, such that the training image manipulation software module 145 may scale, orient, or and align the flattened 2-dimensional image to replicate 906 the real orthorectified geospatial image.  Once the\nsynthetic image has been initially replicated 906, the training image manipulation software module 145 separates the synthetic image layer from the real image layer 907, such that pre-overlay filters FIG. 9B, 951, color correction 953 and resolution\ncorrection 952 may be applied for initial tuning of the synthetic image to a plurality of backgrounds.  Once the pre-overlay filters 951 have been applied to the separated synthetic image layer 907, the module 145 will again overlay the synthetic image\non the real geospatial image 954 before adjusting the initial image to replicate a plurality of geographic locations, environmental factors, seasonal effects, and or shadowing effects accounting for sun angle, time of day, year as well as shadowing from\nlocalized light sources 955.  Post-overlay image filters 956, such as color correction 957 and resolution correction, smoothing, blurring or pixelating 958, finalize the synthetic image before a synthetic footprint is demarcated 959.  The synthetic\nfootprint demarcation 959 is important as it may comprise not only the synthetic image but also any associated shadowing associated with the synthetic object.  The demarcated synthetic image 959 is overlain onto existing real imagery background, and\nusing a masking function 960 to set the background of the synthetic image to transparent such that existing imagery is not occluded.  Finally, the software module 145 runs a check 961, to ascertain whether the synthetic image matches the pre-labeled real\nimage, and if so, created a labeled synthetic image 962 to be deposited into a labeled corpus training set 110.  Otherwise further adjustments to the synthetic object model may be made FIG. 9A, 905.\n Hardware Architecture\n Generally, the techniques disclosed herein may be implemented on hardware or a combination of software and hardware.  For example, they may be implemented in an operating system kernel, in a separate user process, in a library package bound into\nnetwork applications, on a specially constructed machine, on an application-specific integrated circuit (ASIC), or on a network interface card.\n Software/hardware hybrid implementations of at least some of the aspects disclosed herein may be implemented on a programmable network-resident machine (which should be understood to include intermittently connected network-aware machines)\nselectively activated or reconfigured by a computer program stored in memory.  Such network devices may have multiple network interfaces that may be configured or designed to utilize different types of network communication protocols.  A general\narchitecture for some of these machines may be described herein in order to illustrate one or more exemplary means by which a given unit of functionality may be implemented.  According to specific aspects, at least some of the features or functionalities\nof the various aspects disclosed herein may be implemented on one or more general-purpose computers associated with one or more networks, such as for example an end-user computer system, a client computer, a network server or other server system, a\nmobile computing device (e.g., tablet computing device, mobile phone, smartphone, laptop, or other appropriate computing device), a consumer electronic device, a music player, or any other suitable electronic device, router, switch, or other suitable\ndevice, or any combination thereof.  In at least some aspects, at least some of the features or functionalities of the various aspects disclosed herein may be implemented in one or more virtualized computing environments (e.g., network computing clouds,\nvirtual machines hosted on one or more physical computing machines, or other appropriate virtual environments).\n Referring now to FIG. 10, there is shown a block diagram depicting an exemplary computing device 10 suitable for implementing at least a portion of the features or functionalities disclosed herein.  Computing device 10 may be, for example, any\none of the computing machines listed in the previous paragraph, or indeed any other electronic device capable of executing software- or hardware-based instructions according to one or more programs stored in memory.  Computing device 10 may be configured\nto communicate with a plurality of other computing devices, such as clients or servers, over communications networks such as a wide area network a metropolitan area network, a local area network, a wireless network, the Internet, or any other network,\nusing known protocols for such communication, whether wireless or wired.\n In one aspect, computing device 10 includes one or more central processing units (CPU) 12, one or more interfaces 15, and one or more busses 14 (such as a peripheral component interconnect (PCI) bus).  When acting under the control of\nappropriate software or firmware, CPU 12 may be responsible for implementing specific functions associated with the functions of a specifically configured computing device or machine.  For example, in at least one aspect, a computing device 10 may be\nconfigured or designed to function as a server system utilizing CPU 12, local memory 11 and/or remote memory 16, and interface(s) 15.  In at least one aspect, CPU 12 may be caused to perform one or more of the different types of functions and/or\noperations under the control of software modules or components, which for example, may include an operating system and any appropriate applications software, drivers, and the like.\n CPU 12 may include one or more processors 13 such as, for example, a processor from one of the Intel, ARM, Qualcomm, and AMD families of microprocessors.  In some aspects, processors 13 may include specially designed hardware such as\napplication-specific integrated circuits (ASICs), electrically erasable programmable read-only memories (EEPROMs), field-programmable gate arrays (FPGAs), and so forth, for controlling operations of computing device 10.  In a particular aspect, a local\nmemory 11 (such as non-volatile random access memory (RAM) and/or read-only memory (ROM), including for example one or more levels of cached memory) may also form part of CPU 12.  However, there are many different ways in which memory may be coupled to\nsystem 10.  Memory 11 may be used for a variety of purposes such as, for example, caching and/or storing data, programming instructions, and the like.  It should be further appreciated that CPU 12 may be one of a variety of system-on-a-chip (SOC) type\nhardware that may include additional hardware such as memory or graphics processing chips, such as a QUALCOMM SNAPDRAGON.TM.  or SAMSUNG EXYNOS.TM.  CPU as are becoming increasingly common in the art, such as for use in mobile devices or integrated\ndevices.\n As used herein, the term \"processor\" is not limited merely to those integrated circuits referred to in the art as a processor, a mobile processor, or a microprocessor, but broadly refers to a microcontroller, a microcomputer, a programmable\nlogic controller, an application-specific integrated circuit, and any other programmable circuit.\n In one aspect, interfaces 15 are provided as network interface cards (NICs).  Generally, NICs control the sending and receiving of data packets over a computer network; other types of interfaces 15 may for example support other peripherals used\nwith computing device 10.  Among the interfaces that may be provided are Ethernet interfaces, frame relay interfaces, cable interfaces, DSL interfaces, token ring interfaces, graphics interfaces, and the like.  In addition, various types of interfaces\nmay be provided such as, for example, universal serial bus (USB), Serial, Ethernet, FIREWIRE.TM., THUNDERBOLT.TM., PCI, parallel, radio frequency (RF), BLUETOOTH.TM., near-field communications (e.g., using near-field magnetics), 802.11 (WiFi), frame\nrelay, TCP/IP, ISDN, fast Ethernet interfaces, Gigabit Ethernet interfaces, Serial ATA (SATA) or external SATA (ESATA) interfaces, high-definition multimedia interface (HDMI), digital visual interface (DVI), analog or digital audio interfaces,\nasynchronous transfer mode (ATM) interfaces, high-speed serial interface (HSSI) interfaces, Point of Sale (POS) interfaces, fiber data distributed interfaces (FDDIs), and the like.  Generally, such interfaces 15 may include physical ports appropriate for\ncommunication with appropriate media.  In some cases, they may also include an independent processor (such as a dedicated audio or video processor, as is common in the art for high-fidelity A/V hardware interfaces) and, in some instances, volatile and/or\nnon-volatile memory (e.g., RAM).\n Although the system shown in FIG. 10 illustrates one specific architecture for a computing device 10 for implementing one or more of the aspects described herein, it is by no means the only device architecture on which at least a portion of the\nfeatures and techniques described herein may be implemented.  For example, architectures having one or any number of processors 13 may be used, and such processors 13 may be present in a single device or distributed among any number of devices.  In one\naspect, a single processor 13 handles communications as well as routing computations, while in other aspects a separate dedicated communications processor may be provided.  In various aspects, different types of features or functionalities may be\nimplemented in a system according to the aspect that includes a client device (such as a tablet device or smartphone running client software) and server systems (such as a server system described in more detail below).\n Regardless of network device configuration, the system of an aspect may employ one or more memories or memory modules (such as, for example, remote memory block 16 and local memory 11) configured to store data, program instructions for the\ngeneral-purpose network operations, or other information relating to the functionality of the aspects described herein (or any combinations of the above).  Program instructions may control execution of or comprise an operating system and/or one or more\napplications, for example.  Memory 16 or memories 11, 16 may also be configured to store data structures, configuration data, encryption data, historical system operations information, or any other specific or generic non-program information described\nherein.\n Because such information and program instructions may be employed to implement one or more systems or methods described herein, at least some network device aspects may include nontransitory machine-readable storage media, which, for example,\nmay be configured or designed to store program instructions, state information, and the like for performing various operations described herein.  Examples of such nontransitory machine-readable storage media include, but are not limited to, magnetic\nmedia such as hard disks, floppy disks, and magnetic tape; optical media such as CD-ROM disks; magneto-optical media such as optical disks, and hardware devices that are specially configured to store and perform program instructions, such as read-only\nmemory devices (ROM), flash memory (as is common in mobile devices and integrated systems), solid state drives (SSD) and \"hybrid SSD\" storage drives that may combine physical components of solid state and hard disk drives in a single hardware device (as\nare becoming increasingly common in the art with regard to personal computers), memristor memory, random access memory (RAM), and the like.  It should be appreciated that such storage means may be integral and non-removable (such as RAM hardware modules\nthat may be soldered onto a motherboard or otherwise integrated into an electronic device), or they may be removable such as swappable flash memory modules (such as \"thumb drives\" or other removable media designed for rapidly exchanging physical storage\ndevices), \"hot-swappable\" hard disk drives or solid state drives, removable optical storage discs, or other such removable media, and that such integral and removable storage media may be utilized interchangeably.  Examples of program instructions\ninclude both object code, such as may be produced by a compiler, machine code, such as may be produced by an assembler or a linker, byte code, such as may be generated by for example a JAVA.TM.  compiler and may be executed using a Java virtual machine\nor equivalent, or files comprising higher level code that may be executed by the computer using an interpreter (for example, scripts written in Python, Perl, Ruby, Groovy, or any other scripting language).\n In some aspects, systems may be implemented on a standalone computing system.  Referring now to FIG. 11, there is shown a block diagram depicting a typical exemplary architecture of one or more aspects or components thereof on a standalone\ncomputing system.  Computing device 20 includes processors 21 that may run software that carry out one or more functions or applications of aspects, such as for example a client application 24.  Processors 21 may carry out computing instructions under\ncontrol of an operating system 22 such as, for example, a version of MICROSOFT WINDOWS.TM.  operating system, APPLE OSX.TM.  or iOS.TM.  operating systems, some variety of the Linux operating system, ANDROID.TM.  operating system, or the like.  In many\ncases, one or more shared services 23 may be operable in system 20, and may be useful for providing common services to client applications 24.  Services 23 may for example be WINDOWS.TM.  services, user-space common services in a Linux environment, or\nany other type of common service architecture used with operating system 21.  Input devices 28 may be of any type suitable for receiving user input, including for example a keyboard, touchscreen, microphone (for example, for voice input), mouse,\ntouchpad, trackball, or any combination thereof.  Output devices 27 may be of any type suitable for providing output to one or more users, whether remote or local to system 20, and may include for example one or more screens for visual output, speakers,\nprinters, or any combination thereof.  Memory 25 may be random-access memory having any structure and architecture known in the art, for use by processors 21, for example to run software.  Storage devices 26 may be any magnetic, optical, mechanical,\nmemristor, or electrical storage device for storage of data in digital form (such as those described above, referring to FIG. 10).  Examples of storage devices 26 include flash memory, magnetic hard drive, CD-ROM, and/or the like.\n In some aspects, systems may be implemented on a distributed computing network, such as one having any number of clients and/or servers.  Referring now to FIG. 12, there is shown a block diagram depicting an exemplary architecture 30 for\nimplementing at least a portion of a system according to one aspect on a distributed computing network.  According to the aspect, any number of clients 33 may be provided.  Each client 33 may run software for implementing client-side portions of a\nsystem; clients may comprise a system 20 such as that illustrated in FIG. 11.  In addition, any number of servers 32 may be provided for handling requests received from one or more clients 33.  Clients 33 and servers 32 may communicate with one another\nvia one or more electronic networks 31, which may be in various aspects any of the Internet, a wide area network, a mobile telephony network (such as CDMA or GSM cellular networks), a wireless network (such as WiFi, WiMAX, LTE, and so forth), or a local\narea network (or indeed any network topology known in the art; the aspect does not prefer any one network topology over any other).  Networks 31 may be implemented using any known network protocols, including for example wired and/or wireless protocols.\n In addition, in some aspects, servers 32 may call external services 37 when needed to obtain additional information, or to refer to additional data concerning a particular call.  Communications with external services 37 may take place, for\nexample, via one or more networks 31.  In various aspects, external services 37 may comprise web-enabled services or functionality related to or installed on the hardware device itself.  For example, in one aspect where client applications 24 are\nimplemented on a smartphone or other electronic device, client applications 24 may obtain information stored in a server system 32 in the cloud or on an external service 37 deployed on one or more of a particular enterprise's or user's premises.\n In some aspects, clients 33 or servers 32 (or both) may make use of one or more specialized services or appliances that may be deployed locally or remotely across one or more networks 31.  For example, one or more databases 34 may be used or\nreferred to by one or more aspects.  It should be understood by one having ordinary skill in the art that databases 34 may be arranged in a wide variety of architectures and using a wide variety of data access and manipulation means.  For example, in\nvarious aspects one or more databases 34 may comprise a relational database system using a structured query language (SQL), while others may comprise an alternative data storage technology such as those referred to in the art as \"NoSQL\" (for example,\nHADOOP CASSANDRA.TM., GOOGLE BIGTABLE.TM., and so forth).  In some aspects, variant database architectures such as column-oriented databases, in-memory databases, clustered databases, distributed databases, or even flat file data repositories may be used\naccording to the aspect.  It will be appreciated by one having ordinary skill in the art that any combination of known or future database technologies may be used as appropriate, unless a specific database technology or a specific arrangement of\ncomponents is specified for a particular aspect described herein.  Moreover, it should be appreciated that the term \"database\" as used herein may refer to a physical database machine, a cluster of machines acting as a single database system, or a logical\ndatabase within an overall database management system.  Unless a specific meaning is specified for a given use of the term \"database\", it should be construed to mean any of these senses of the word, all of which are understood as a plain meaning of the\nterm \"database\" by those having ordinary skill in the art.\n Similarly, some aspects may make use of one or more security systems 36 and configuration systems 35.  Security and configuration management are common information technology (IT) and web functions, and some amount of each are generally\nassociated with any IT or web systems.  It should be understood by one having ordinary skill in the art that any configuration or security subsystems known in the art now or in the future may be used in conjunction with aspects without limitation, unless\na specific security 36 or configuration system 35 or approach is specifically required by the description of any specific aspect.\n FIG. 13 shows an exemplary overview of a computer system 40 as may be used in any of the various locations throughout the system.  It is exemplary of any computer that may execute code to process data.  Various modifications and changes may be\nmade to computer system 40 without departing from the broader scope of the system and method disclosed herein.  Central processor unit (CPU) 41 is connected to bus 42, to which bus is also connected memory 43, nonvolatile memory 44, display 47,\ninput/output (I/O) unit 48, and network interface card (NIC) 53.  I/O unit 48 may, typically, be connected to keyboard 49, pointing device 50, hard disk 52, and real-time clock 51.  NIC 53 connects to network 54, which may be the Internet or a local\nnetwork, which local network may or may not have connections to the Internet.  Also shown as part of system 40 is power supply unit 45 connected, in this example, to a main alternating current (AC) supply 46.  Not shown are batteries that could be\npresent, and many other devices and modifications that are well known but are not applicable to the specific novel functions of the current system and method disclosed herein.  It should be appreciated that some or all components illustrated may be\ncombined, such as in various integrated applications, for example Qualcomm or Samsung system-on-a-chip (SOC) devices, or whenever it may be appropriate to combine multiple capabilities or functions into a single hardware device (for instance, in mobile\ndevices such as smartphones, video game consoles, in-vehicle computer systems such as navigation or multimedia systems in automobiles, or other integrated hardware devices).\n In various aspects, functionality for implementing systems or methods of various aspects may be distributed among any number of client and/or server components.  For example, various software modules may be implemented for performing various\nfunctions in connection with the system of any particular aspect, and such modules may be variously implemented to run on server and/or client components.\n The skilled person will be aware of a range of possible modifications of the various aspects described above.  Accordingly, the present invention is defined by the claims and their equivalents.", "application_number": "15906348", "abstract": " A system for simplified generation of systems for analysis of satellite\n     images to geolocate one or more objects of interest. A plurality of\n     training images labeled for a study object or objects with irrelevant\n     features loaded into a preexisting feature identification subsystem\n     causes automated generation of models for the study object. This model is\n     used to parameterize pre-engineered machine learning elements that are\n     running a preprogrammed machine learning protocol. Training images with\n     the study are used to train object recognition filters. This filter is\n     used to identify the study object in unanalyzed images. The system\n     reports results in a requestor's preferred format.\n", "citations": ["6109270", "6718196", "8352395", "8401979", "9904849", "20040066966", "20080077669", "20140236851", "20140237386", "20160078273", "20170169567", "20170258433"], "related": ["15608894", "15194541", "14835736", "62301554"]}, {"id": "20180190280", "patent_code": "10373609", "patent_name": "Voice recognition method and apparatus", "year": "2019", "inventor_and_country_data": " Inventors: \nCui; Weiwei (Beijing, CN), Sun; Yu (Beijing, CN)  ", "description": "CROSS-REFERENCE TO RELATED APPLICATION\n This application claims the priority of Chinese Patent Application No. 201611244370.5, entitled \"Voice Recognition Method and Apparatus,\" filed on Dec.  29, 2016, the content of which is incorporated herein by reference in its entirety.\nTECHNICAL FIELD\n The present application relates to the field of computer technology, specifically to the field of Internet technology, and more specifically to a voice recognition method and apparatus.\nBACKGROUND\n With the development of computer technologies, voice recognition technologies have been applied in more and more areas, such as smart home, industrial control, and voice interaction systems of terminal devices.  Information can be processed and\nacquired more conveniently with the voice recognition technologies, thereby improving working efficiency of users.\n However, existing voice recognition methods usually include inputting a voice signal to be recognized into an acoustic model obtained by training a pure voice signal, to obtain a voice recognition result.  Since the voice signal to be recognized\nusually differ significantly from the pure voice signal, such voice recognition methods have the problem of low recognition success rate.\nSUMMARY\n An objective of the present application is to provide an improved voice recognition method and apparatus, in order to solve the technical problem mentioned in the foregoing Background section.\n According to a first aspect, the present application provides a voice recognition method, the method comprising: performing high-pass filtering on a voice signal, in response to detecting the microphone receiving the voice signal containing\ninterfering sound signal; cancelling the interfering sound signal in the voice signal subjected to high-pass filtering; performing automatic gain control on the voice signal subjected to cancelling the interfering sound signal, to obtain target voice\nsignal; and extracting a feature vector from the target voice signal and inputting the feature vector into a pre-trained acoustic model, to obtain a voice recognition result matching the target voice signal, the acoustic model being used for representing\na corresponding relationship between the feature vector and the voice recognition result.\n In some embodiments, the terminal device is further equipped with a loudspeaker, the interfering sound signal comprises an echo signal and a noise signal, and the echo signal is a sound signal sent by the loudspeaker and transmitted to the\nmicrophone.\n In some embodiments, the cancelling the interfering sound signal in the voice signal to obtain target voice signal comprises: performing adaptive filtering on the voice signal subjected to high-pass filtering using a time delay estimation\nalgorithm, to cancel the echo signal; and cancelling the noise signal in the voice signal subjected to adaptive filtering using a noise suppression algorithm\n In some embodiments, before the performing high-pass filtering on the voice signals, in response to detecting the microphone receiving the voice signal, the method further comprises: pre-processing a pre-acquired training sample to generate a\ntarget training sample, the target training sample comprising a voice identifier; extracting the feature vector from the target training sample; and training, using a convolutional neural network, a deep neural network, and a restricted Boltzmann\nmachine, and assigning the feature vector extracted from the target training sample as an input and the voice identifier as an output, to obtain the acoustic model.\n In some embodiments, the pre-processing a pre-acquired training sample to generate a target training sample comprises: performing high-pass filtering on the pre-acquired training sample; performing sequentially echo cancellation and noise\nsuppression on the training sample subjected to high-pass filtering; and performing automatic gain control on the training sample subjected to noise suppression, to generate the target training sample.\n In some embodiments, before the performing high-pass filtering on the voice signals, in response to detecting the microphone receiving a voice signal, the method further comprises: clustering the voice identifier outputted by the acoustic model\nusing a clustering algorithm, and determining voice identifier obtained after clustering as a voice recognition result matching the training sample.\n According to a second aspect, the present application provides a voice recognition apparatus, the apparatus comprising: a first processing unit, configured to perform high-pass filtering on the voice signal, in response to detecting the\nmicrophone receiving voice signal containing interfering sound signal; a cancellation unit, configured to cancel the interfering sound signal in the voice signal subjected to high-pass filtering; a second processing unit, configured to perform automatic\ngain control on the voice signal subjected to cancelling the interfering sound signal, to obtain target voice signal; and an input unit, configured to extract a feature vector from the target voice signal and input the feature vector into a pre-trained\nacoustic model, to obtain a voice recognition result matching the target voice signal, the acoustic model being used for representing a corresponding relationship between the feature vector and the voice recognition result.\n In some embodiments, the terminal device is further equipped with a loudspeaker, the interfering sound signal comprises an echo signal and a noise signal, and the echo signal is a sound signal sent by the loudspeaker and transmitted to the\nmicrophone.\n In some embodiments, the cancellation unit comprises: first cancellation module, configured to perform adaptive filtering on the voice signal subjected to high-pass filtering using a time delay estimation algorithm, to cancel the echo signal;\nand a second cancellation module, configured to cancel the noise signal in the voice signal subjected to adaptive filtering using a noise suppression algorithm.\n In some embodiments, the apparatus further comprises: a pre-processing unit, configured to pre-process a pre-acquired training sample to generate a target training sample, the target training sample comprising a voice identifier; an extraction\nunit, configured to extract the feature vector from the target training sample; and a training unit, configured to train, using a convolutional neural network, a deep neural network, and a restricted Boltzmann machine, and assigning the feature vector\nextracted from the target training sample as an input and the voice identifier as an output, to obtain the acoustic model.\n In some embodiments, the pre-processing unit comprises: a first processing module, configured to perform high-pass filtering on the pre-acquired training sample; a second processing module, configured to perform sequentially echo cancellation\nand noise suppression on the training sample subjected to high-pass filtering; and a third processing module, configured to perform automatic gain control on the training sample subjected to noise suppression, to generate the target training sample.\n In some embodiments, the apparatus further comprises: a clustering unit, configured to cluster the voice identifier outputted by the acoustic model using a clustering algorithm, and determine a voice identifier obtained after clustering as a\nvoice recognition result matching the training sample.\n According to the voice recognition method and apparatus provided in the present application, high-pass filtering, cancellation of interfering sound signals, and automatic gain control are sequentially performed on a voice signal, to obtain a\ntarget voice signal, and afterwards, a feature vector is extracted from the target voice signal, and the feature vector is inputted into a pre-trained acoustic model, to obtain a voice recognition result matching the target voice signal, thereby\nimproving the success rate of voice recognition. BRIEF DESCRIPTION OF THE DRAWINGS\n Other features, objectives and advantages of the present application will become more apparent upon reading the detailed description to non-limiting embodiments with reference to the accompanying drawings:\n FIG. 1 is an architectural diagram of an exemplary system in which the present application may be implemented;\n FIG. 2 is a flow chart of a voice recognition method according to an embodiment of the present application;\n FIG. 3 is a schematic diagram of an application scenario of the voice recognition method according to the present application;\n FIG. 4 is a flow chart of a voice recognition method according to another embodiment of the present application;\n FIG. 5 is a schematic structural diagram of a voice recognition apparatus according to an embodiment of the present application; and\n FIG. 6 is a structural schematic diagram of a computer system adapted to implement a terminal device of the embodiments of the present application\nDETAILED DESCRIPTION OF EMBODIMENTS\n The present application will be further described below in detail in combination with the accompanying drawings and the embodiments.  It should be appreciated that the specific embodiments described herein are merely used for explaining the\nrelevant invention, rather than limiting the invention.  In addition, it should be noted that, for the ease of description, only the parts related to the relevant invention are shown in the accompanying drawings.\n It should also be noted that the embodiments in the present application and the features in the embodiments may be combined with each other on a non-conflict basis.  The present application will be described below in detail with reference to the\naccompanying drawings and in combination with the embodiments.\n FIG. 1 shows an exemplary architecture of a system 100 which may be used by a voice recognition method or a voice recognition apparatus according to an embodiment of the present application.\n As shown in FIG. 1, the system architecture 100 may include terminal devices 101, 102 and 103, and a server 104.  In addition, the above system architecture 100 may also include a network serving as a medium providing a communication link\nbetween the terminal devices 101, 102 and 103 and the server 104.  The network may include various types of connections, such as wired or wireless transmission links, or optical fibers.\n Various electronic parts, such as speakers, microphones and cameras, and various communication client applications, such as voice input applications, web browsing applications, shopping applications, search applications, instant messaging tools,\nmailbox clients, and social platform software may be installed on the terminal devices 101, 102 and 103.\n The terminal devices 101, 102 and 103 may detect the voice signal received by the installed microphone, perform high-pass filtering on the detected voice signal, cancelling an interfering sound signal, perform automatic gain control, and\ninteract with a server 104 through a network, to recognize the processed voice signal.  Here, the server 104 may be used to store acoustic models.\n The terminal devices 101, 102 and 103 may be various electronic devices having microphones, including but not limited to, smart phones, tablet computers, e-book readers, MP3 (Moving Picture Experts Group Audio Layer III) players, MP4 (Moving\nPicture Experts Group Audio Layer IV) players, laptop computers and desktop computers.\n It should be noted that the terminal devices 101, 102 and 103 may also be used to store the acoustic models directly, and to recognize the processed voice signal.  Therefore, it is possible that the terminal devices 101, 102 and 103 do not\ninteract with the server 104.  In this event, the server 104 and the network may be omitted in the above system architecture 100.\n It should be noted that the voice recognition method according to the embodiments of the present application is generally executed by the terminal devices 101, 102 and 103, and accordingly, a voice recognition apparatus is generally installed on\nthe terminal devices 101, 102 and 103.\n It should be appreciated that the numbers of the terminal devices, the networks and the servers in FIG. 1 are merely illustrative.  Any number of terminal devices, networks and servers may be provided based on the actual requirements.\n Referring to FIG. 2, showing a flow 200 of a voice recognition method for a terminal device according to an embodiment of the present application.  The voice recognition method includes the following steps.\n Step 201: In response to detecting a microphone receiving a voice signal containing an interfering sound signal, perform high-pass filtering on the voice signal.\n In this embodiment, an electronic device (for example, the terminal device 101, 102, or 103 shown in FIG. 1) on which the voice recognition method is run may be equipped with a sound receiving device.  In practice, the sound receiving device may\nbe a sensor that converts an acoustic signal into a corresponding electrical signal, for example, a microphone.\n In this embodiment, the above-mentioned electronic device may detect in real time whether the installed microphone receives voice signal based on a voice activity detection (VAD) algorithm.  In response to detecting the microphone receiving a\nvoice signal, the electronic device may perform high-pass filtering (HPF) processing on the voice signal, to block or weaken voice signals having frequencies less than a preset frequency threshold (for example, 100 Hz, 200 Hz).  Here, an acoustic signal\npropagated to the microphone through a medium such as air during a user's speech may be used as the voice signal.  In addition, since the electronic device may be placed in various environments, the voice signal usually contains interfering sound\nsignals, emitted by environmental sound sources other than the user and propagated to the microphone through a medium such as air.\n In practice, the VAD algorithm can be used for recognizing and cancelling long-time silence periods from a sound signal stream; high-pass filtering is a signal filtering method for enabling high-frequency signal having frequencies higher than a\nset threshold to pass therethrough, while blocking or weakening low-frequency signal having frequencies lower than the set threshold.  It should be noted that the VAD algorithm and the high-pass filtering processing method are well-known technologies\nwidely studied and applied at present, and will not be described here again.\n Step 202: Cancel the interfering sound signal in the voice signal subjected to high-pass filtering.\n In this embodiment, the above-mentioned electronic device may cancel the interfering sound signal in the voice signal subjected to high-pass filtering in various manners.\n In some optional implementations of this embodiment, the above-mentioned interfering sound signal may be a noise signal.  In practice, noise signals may exist in various environments, for example, public places, running vehicles, and white noise\nmay exist even in a quiet environment.  The above-mentioned electronic device may cancel the noise signal in the voice signal subjected to high-pass filtering using various noise suppression algorithms.  As an example, the noise suppression algorithm may\nbe a least mean square (LMS) filtering algorithm, a Lyapunov active noise control algorithm.\n In some optional implementations of this embodiment, the above-mentioned electronic device may be equipped with a loudspeaker.  In the case that the user plays music using the electronic device or plays a sound (for example, a navigation voice\nor a news reading and broadcasting voice) using a text to speech (TTS) playing function of the electronic device, the sound emitted by the above-mentioned loudspeaker may be transmitted to the above-mentioned microphone through a medium such as air to\nform an acoustic echo.  At this time, the above-mentioned interfering sound signal may be an echo signal, the echo signal being a sound signal sent by the loudspeaker and transmitted to the microphone.  The above-mentioned electronic device may cancel\nthe echo signal in the voice signal subjected to high-pass filtering using an acoustic echo cancellation (AEC) technology.  Specifically, the electronic device may perform the following steps.  First, perform high-pass filtering on a sound signal emitted\nby the above-mentioned loudspeaker, to block or weaken sound signal having frequencies less than the above-mentioned preset frequency threshold.  Afterwards, calculate a time delay between sending the sound signal by the loudspeaker and receiving the\necho signal by the microphone using various time delay estimation (TDE) algorithms (for example, a generalized cross-correlation function method, a least mean square adaptive filtering method, a cross-power-spectrum phase method, a bispectrum time delay\nestimation method based on high-order statistics), and perform time delay compensation on the echo signal received by the microphone.  Finally, perform adaptive filtering on the voice signal subjected to high-pass filtering, to cancel the echo signal in\nthe voice signal subjected to high-pass filtering.  It should be noted that the above-mentioned noise suppression algorithms, echo cancellation technology, and time delay estimation algorithms are well-known technologies widely studied and applied at\npresent, and will not be described here again.\n In some optional implementations of this embodiment, the above-mentioned interfering sound signal may include the above-mentioned echo signal and the above-mentioned noise signal.  The above-mentioned electronic device may first perform adaptive\nfiltering on voice signal subjected to high-pass filtering using a time delay estimation algorithm, to cancel an echo signal.  The noise signal in the voice signal subjected to adaptive filtering may be cancelled afterwards using a noise cancellation\nalgorithm.\n Step 203: Perform automatic gain control on the voice signal subjected to cancelling the interfering sound signal, to obtain a target voice signal.\n In this embodiment, the above-mentioned electronic device may perform automatic gain control (AGC) on the voice signal subjected to cancelling the interfering sound signal, and determine a voice signal obtained after automatic gain control as\ntarget a voice signal.  Specifically, when the strength of a voice signal subjected to cancelling the interfering sound signal is less than a preset minimum amplitude threshold, the electronic device may increase the amplitude of the voice signal; and\nwhen the amplitude of a voice signal subjected to cancelling the interfering sound signal is greater than a preset maximum amplitude threshold, the electronic device may reduce the amplitude of the voice signal.  In practice, AGC is an automatic control\nmethod for adjusting the gain automatically with a signal amplitude, and can automatically control the amplitude of the gain by changing a signal input/output compression ratio.\n In some optional implementations of this embodiment, after performing AGC on the voice signal subjected to cancelling the interfering sound signal, the above-mentioned electronic device may further perform dynamic range control (DRC) on a voice\nsignal having amplitudes less than a preset amplitude, to map the range of the amplitudes of the voice signal to a preset amplitude range, and determine a voice signal obtained after DRC as a target voice signal.\n It should be noted that the above-mentioned AGC method and DRC method are well-known technologies widely studied and applied at present, and will not be described here again.\n Step 204: Extract a feature vector from the target voice signal and input the feature vector into a pre-trained acoustic model, to obtain a voice recognition result matching the target voice signal.\n In this embodiment, the above-mentioned electronic device may first extract a feature vector from the above-mentioned target voice signal using various acoustic feature extraction methods; and afterwards, input the feature vector into a\npre-trained acoustic model, to obtain a voice recognition result matching the target voice signal.  The acoustic model can be used for representing a corresponding relationship between the feature vector and the voice recognition result.  It should be\nnoted that the above-mentioned feature vector may contain multiple values; the above-mentioned voice recognition result may be a voice recognition identifier for distinguishing information such as phonemes, syllables, morphemes, or phoneme states, and\nthe voice identifier may be represented in various forms such as alphabets, numbers, symbols, or texts.  It should be pointed out that the above-mentioned acoustic model may be established based on various models such as a hidden Markov model (HMM), a\nrecurrent neural network (RNN), or a deep neural network (DNN), or may be established based on a combination of multiple models.  As an example, to establish the acoustic model based on a hidden Markov model, first, statistics may be collected on\npronunciation probabilities of texts, and an expression probability matrix of hidden states, where the hidden states may be text sequences corresponding to a voice signal; afterwards, statistics are collected on transition probabilities of words, a\ntransition probability matrix is established, and a hidden Markov model is obtained based on the expression probability matrix of the hidden states; then, the hidden Markov model may be evaluated using a forward algorithm and so on, and a machine\nlearning method is used for training to determine parameters of the hidden Markov model, to obtain the above-mentioned acoustic model.\n In some optional implementations of this embodiment, the above-mentioned electronic device may extract feature vector from the above-mentioned target voice signal based on a Mel frequency cepstrum coefficient (MFCC).  Specifically, the\nelectronic device may first convert the target voice signal from a time domain to a frequency domain using a fast algorithm of discrete Fourier transform (fast Fourier transformation, FFT), to obtain energy frequencies; afterwards, the electronic device\nmay perform convolution computation on the energy frequency spectrum of the target voice signal using a triangular band-pass filtering method and according to Mel scale distribution, to obtain multiple output logarithmic energies, and finally perform\ndiscrete cosine transform (DCT) on vectors formed by the multiple output logarithmic energies, to generate a feature vector.\n In some optional implementations of this embodiment, before extracting the feature vector from the above-mentioned target voice signal based on the MFCC, the above-mentioned electronic device may further perform processing such as pre-emphasis\nor windowing on the target voice signal.  In practice, since the target voice signal is a non-stationary signal, in order to process the target voice signal, it is further required to divide the target voice signal by short time periods each of one\nframe.  Each frame may be of a preset duration, for example, 20 ms, 25 ms, 30 ms, or the like.\n In some optional implementations of this embodiment, the above-mentioned electronic device may further use a linear predictive coding (LPC) method to parse the above-mentioned target voice signal to generate parameters of a channel excitation\nand transfer function, and use the generated parameters as feature parameters to generate the feature vector.\n Referring to FIG. 3, which is a schematic diagram of an application scenario of the voice recognition method according to this embodiment.  In the application scenario of FIG. 3, a mobile phone terminal 301 is equipped with a microphone 302, and\nupon detecting the microphone 301 receiving a voice signal containing interfering sound signal, the mobile phone terminal 301 performs high-pass filtering on the detected voice signal (indicated by the reference sign 303); afterwards, cancels the\ninterfering sound signal in the voice signal subjected to high-pass filtering (indicated by the reference sign 304); then, performs automatic gain control on the voice signal subjected to cancelling the interfering sound signal (indicated by the\nreference sign 305), to obtain a target voice signal 306; and finally, the mobile phone terminal 301 extracts a feature vector from the target voice signal 306 and inputs the feature vector into a pre-trained acoustic model (indicated by the reference\nsign 307), to obtain a voice recognition result 308 matching the target voice signal.\n According to the method provided in the above-mentioned embodiment of the present application, high-pass filtering, cancellation of an interfering sound signal, and automatic gain control are sequentially performed on a voice signal, to obtain a\ntarget voice signal, and afterwards, a feature vector is extracted from the target voice signal, and the feature vector is inputted into a pre-trained acoustic model, to obtain a voice recognition result matching the target voice signal, thereby\nimproving the success rate of voice recognition.\n Further referring to FIG. 4, showing a flow 400 of a voice recognition method according to another embodiment.  The flow 400 of the voice recognition method includes the following steps.\n Step 401: Pre-process a pre-acquired training sample to generate a target training sample.\n In this embodiment, an electronic device (for example, the terminal device 101, 102, or 103 shown in FIG. 1) on which the voice recognition method is run may pre-acquire a training sample that may be a large number of a pre-collected voice\nsignal.  It should be noted that the large number of a voice signal may contain a noise signal and/or an echo signal.  It should be pointed out that the above-mentioned target training sample may include a voice identifier.  In practice, the voice\nidentifier may be an identifier for distinguishing phonemes, syllables, morphemes, or phoneme states, and the voice identifier may be represented in various forms such as alphabets, numbers, symbols, or texts.\n In this embodiment, the above-mentioned electronic device may pre-process the above-mentioned training sample according to the following steps.  First, perform high-pass filtering on a pre-acquired training sample, to block or weaken a voice\nsignal having frequencies less than a preset frequency threshold.  Afterwards, the electronic device may perform echo cancellation and noise suppression on the training sample subjected to high-pass filtering sequentially using an echo cancellation\ntechnology and a noise suppression algorithm, to cancel an interfering sound signal in the training sample.  It should be noted that the cancellation of the interfering sound signal may also be weakening or suppression of the interfering sound signal. \nFinally, the electronic device may perform automatic gain control on the training sample subjected to noise suppression, and determine a training sample obtained after automatic gain control as a target training sample.\n Step 402: Extract a Feature Vector from the Target Training Sample.\n In this embodiment, the above-mentioned electronic device may first perform processing such as pre-emphasis, framing, or windowing on the above-mentioned target training sample, and afterwards, extract a feature vector from the above-mentioned\ntarget voice signal based on an MFCC.  Specifically, the electronic device may first convert the target training sample from a time domain to a frequency domain using a fast algorithm of discrete Fourier transform, to obtain energy frequencies;\nafterwards, the electronic device may perform convolution computation on the energy frequency spectrum of the target training sample using a triangular band-pass filtering method and according to Mel scale distribution, to obtain multiple output\nlogarithmic energies, and finally perform discrete cosine transform on vectors formed by the multiple output logarithmic energies, to generate a feature vector.\n Step 403: Train, using a convolutional neural network, a deep neural network, and a restricted Boltzmann machine, and assigning the feature vector extracted from the target training sample as an input and the voice identifier as an output, to\nobtain an acoustic model.\n In this embodiment, first, a multi-layer neural network model may be established using a convolutional neural network and using an S-function (namely, a sigmoid function) as a non-linear activation function.  Afterwards, the above-mentioned\nelectronic device may train the established multi-layer neural network model using the feature vector extracted from the above-mentioned target training sample as an input and the above-mentioned voice identifier as an output and using a restricted\nBoltzmann machine (RBM), to obtain an acoustic model.  In practice, the above-mentioned neural network model may include an input layer, a convolutional layer, a pooling layer, a fully-connected layer, and an output layer, and the above-mentioned\nconvolutional neural network may include multiple convolutional layers and multiple pooling layers.  The above-mentioned restricted Boltzmann machine is a randomly generated neural network capable of learning probability distribution through input data\nsets.  The restricted Boltzmann machine may consist of a visible neuron layer and a hidden neuron layer, the hidden layer of neurons are not interconnected, and the hidden layer of neurons are independent of the above-mentioned target training sample,\nand the visible layer of neurons are also not interconnected.  A Markov chain sampling process is performed on the states of the hidden layer of neurons obtained from the target training sample, to estimate expected value independent of data, and values\nof all the visible layer of neurons and the hidden layer of neurons are updated alternately in parallel.  It should be noted that the above-mentioned convolutional neural network and restricted Boltzmann machine are well-known technologies widely studied\nand applied at present, and will not be described here again.\n In this embodiment, after the above-mentioned acoustic model is obtained, a voice identifier outputted by the acoustic model may be further clustered using various clustering algorithms, and a voice identifier obtained after clustering are\ndetermined as a voice recognition result matching the above-mentioned training sample.  As an example, the voice identifier outputted by the acoustic model may be clustered based on a relative entropy (Kullback-Leibler Divergence, KLD), a hierarchical\nclustering algorithm, a self-organizing map (SOM) clustering algorithm, a fuzzy c-means (FCM) clustering algorithm, and so on.\n It should be noted that the above-mentioned acoustic model may be stored in the above-mentioned electronic device, or may be stored in a server (for example, the server 104 shown in FIG. 1) connected to the electronic device.\n Step 404: In response to detecting a microphone receiving a voice signal containing an interfering sound signal, perform high-pass filtering on the voice signal.\n In this embodiment, the above-mentioned electronic device may be equipped with a microphone and a loudspeaker.  The electronic device may detect in real time whether the installed microphone receives a voice signal based on a VAD algorithm.  In\nresponse to detecting the microphone receiving a voice signal, the electronic device may perform high-pass filtering on the voice signal received by the microphone, to block or weaken a voice signal having frequencies less than the above-mentioned preset\nfrequency threshold in step 401.  It should be noted that the voice signal received by the microphone may contain an interfering sound signal including the above-mentioned echo signal and the above-mentioned noise signal, where the echo signal contained\nin the voice signal received by the microphone may be a sound signal sent by the above-mentioned loudspeaker and transmitted to the microphone.\n Step 405: Perform adaptive filtering on the voice signal subjected to high-pass filtering using a time delay estimation algorithm, to cancel an echo signal.\n In this embodiment, the above-mentioned electronic device may first perform high-pass filtering on a sound signal emitted by the above-mentioned loudspeaker, to block or weaken a voice signal having frequencies less than the above-mentioned\npreset frequency threshold.  Afterwards, a time delay between sending the sound signal by the loudspeaker and receiving a corresponding echo signal by the above-mentioned microphone may be calculated using a time delay estimation algorithm, and time\ndelay compensation is performed on the echo signal received by the microphone.  Finally, adaptive filtering is performed on the voice signal subjected to high-pass filtering, to cancel an echo signal in the voice signal subjected to high-pass filtering.\n Step 406: Cancel noise signal in the voice signal subjected to adaptive filtering using a noise suppression algorithm.\n In this embodiment, the above-mentioned noise signal may be a white noise existing in the environment.  The above-mentioned electronic device may cancel a noise signal in the voice signal subjected to high-pass filtering using a noise\nsuppression algorithm.\n Step 407: Perform automatic gain control on the voice signal subjected to cancelling the interfering sound signal, to obtain a target voice signal.\n In this embodiment, the above-mentioned electronic device may perform automatic gain control on the voice signal subjected to cancelling the interfering sound signal, and determine a voice signal obtained after automatic gain control as a target\nvoice signal.\n Step 408: Extract a feature vector from the target voice signal, and input the feature vector into the pre-trained acoustic model, to obtain a voice recognition result matching the target voice signal.\n In this embodiment, the above-mentioned electronic device may extract a feature vector from the above-mentioned target voice signal based on an MFCC.  Afterwards, the extracted feature vector are inputted into the acoustic model trained in step\n402, to obtain a voice recognition result matching the above-mentioned target voice signal.\n It should be noted that the concrete operation of the above-mentioned step 405 to step 408 is basically the same as the concrete operation of step 201 to step 204, and will not be described here again.\n It can be seen from FIG. 4 that as compared with the embodiment corresponding to FIG. 4, the flow 400 of the voice recognition method according to in this embodiment emphasizes the step of pre-processing the training sample of the acoustic\nmodel.  Therefore, the solution described in this embodiment can reduce the difference between the training sample used for training the acoustic model and the actual voice signal received by the microphone, thereby further improving the success rate of\nvoice recognition.\n Further referring to FIG. 5, as an implementation of the methods shown in the above-mentioned figures, the present application provides an embodiment of a voice recognition apparatus.  The apparatus embodiment corresponds to the method\nembodiment shown in FIG. 2.  The apparatus may be specifically applied to various electronic devices.\n As shown in FIG. 5, the voice recognition apparatus 500 of this embodiment includes: a first processing unit 501, configured to, in response to detecting the above-mentioned microphone receiving voice signal containing an interfering sound\nsignal, perform high-pass filtering on the voice signal; a cancellation unit 502, configured to cancel the interfering sound signal in the voice signal subjected to high-pass filtering; a second processing unit 503, configured to perform automatic gain\ncontrol on the voice signal subjected to cancelling the interfering sound signal, to obtain a target voice signal; and an input unit 504, configured to extract a feature vector from the target voice signal and input the feature vector into a pre-trained\nacoustic model, to obtain a voice recognition result matching the target voice signal, the acoustic model being used for representing a corresponding relationship between the feature vector and the voice recognition result.\n In this embodiment, the voice recognition apparatus 500 may be equipped with a microphone.  The first processing unit 501 of the voice recognition apparatus 500 may detect in real time whether the installed microphone receives a voice signal\nbased on a VAD algorithm.  In response to detecting the microphone receiving a voice signal, the first processing unit 501 may perform high-pass filtering on the voice signal, to block or weaken a voice signal having frequencies less than a preset\nfrequency threshold.  Since the above-mentioned electronic device may be in various environments, the voice signal usually contain an interfering sound signal.\n In this embodiment, the above-mentioned cancellation unit 502 may cancel the interfering sound signal in the voice signal subjected to high-pass filtering in various manners.  In response to the interfering sound signal being noise signal, the\ncancellation unit 502 may cancel the noise signal in the voice signal subjected to high-pass filtering using an NS algorithm.  In response to the interfering sound signal being an echo signal, the cancellation unit 502 may cancel the echo signal in the\nvoice signal subjected to high-pass filtering using an AEC technology.\n In some optional implementations of this embodiment, the above-mentioned voice recognition apparatus 500 is further equipped with a loudspeaker, the interfering sound signal includes an echo signal and a noise signal, and the echo signal is\nsound signal sent by the loudspeaker and transmitted to the above-mentioned microphone.\n In some optional implementations of this embodiment, the above-mentioned cancellation unit 502 may further include a first cancellation module and a second cancellation module (not shown).  The first cancellation module may be configured to\nperform adaptive filtering on the voice signal subjected to high-pass filtering using a time delay estimation algorithm, to cancel an echo signal; and the second cancellation module may be configured to cancel a noise signal in the voice signal subjected\nto adaptive filtering using a noise suppression algorithm.\n In this embodiment, the second unit 503 may perform automatic gain control on the voice signal subjected to cancelling the interfering sound signal, and determine a voice signal obtained after automatic gain control as a target voice signal. \nSpecifically, when the strength of a voice signal subjected to cancelling the interfering sound signal is less than a preset minimum amplitude threshold, the above-mentioned electronic device may increase the amplitude of the voice signal.  When the\namplitude of a voice signal subjected to cancelling the interfering sound signal is greater than a preset maximum amplitude threshold, the above-mentioned electronic device may reduce the amplitude of the voice signal.\n In this embodiment, the input unit 504 may first extract a feature vector from the above-mentioned target voice signal using various acoustic feature extraction methods; and afterwards, input the feature vector into a pre-trained acoustic model,\nto obtain a voice recognition result matching the target voice signal.  The acoustic model can be used for representing a corresponding relationship between the feature vector and the voice recognition result.\n In some optional implementations of this embodiment, the above-mentioned voice recognition apparatus 500 may further include a pre-processing unit, an extraction unit, and a training unit (not shown).  The pre-processing unit may be configured\nto pre-process a pre-acquired training sample to generate a target training sample, the target training sample including a voice identifier; the extraction unit may be configured to extract a feature vector from the target training sample; and the\ntraining unit may be configured to train, using a convolutional neural network, a deep neural network, and a restricted Boltzmann machine, and assigning the feature vector extracted from the target training sample as an input and the voice identifier as\nan output, to obtain the above-mentioned acoustic model.\n In some optional implementations of this embodiment, the above-mentioned pre-processing unit may further include a first processing module, a second processing module, and a third processing module (not shown).  The first processing module may\nbe configured to perform high-pass filtering on the pre-acquired training sample; the second processing module may be configured to perform sequentially echo cancellation and noise suppression on the training sample subjected to high-pass filtering; and\nthe third processing module may be configured to perform automatic gain control on the training sample subjected to noise suppression, to generate the target training sample.\n In some optional implementations of this embodiment, the above-mentioned voice recognition apparatus 500 may further include a clustering unit (not shown).  The clustering unit is configured to cluster the voice identifier outputted by the\nacoustic model using a clustering algorithm, and determine a voice identifier obtained after clustering as a voice recognition result matching the above-mentioned training sample.\n According to the apparatus provided in the above-mentioned embodiment of the present application, the first processing unit 501, the cancellation unit 502, and the second processing unit 503 respectively perform sequentially high-pass filtering,\ncancellation of an interfering sound signal, and automatic gain control on voice signal, to obtain a target voice signal, and afterwards, the input unit 504 extracts a feature vector from the target voice signal and inputs the feature vector into a\npre-trained acoustic model, to obtain a voice recognition result matching the target voice signal, thereby improving the success rate of voice recognition.\n Referring to FIG. 6, a schematic structural diagram of a computer system 600 adapted to implement a terminal apparatus of the embodiments of the present application is shown.\n As shown in FIG. 6, the computer system 600 includes a central processing unit (CPU) 601, which may execute various appropriate actions and processes in accordance with a program stored in a read-only memory (ROM) 602 or a program loaded into a\nrandom access memory (RAM) 603 from a storage portion 608.  The RAM 603 also stores various programs and data required by operations of the system 600.  The CPU 601, the ROM 602 and the RAM 603 are connected to each other through a bus 604.  An\ninput/output (I/O) interface 605 is also connected to the bus 604.\n The following components are connected to the I/O interface 605: an input portion 606 including a keyboard, a mouse etc.; an output portion 607 comprising a cathode ray tube (CRT), a liquid crystal display device (LCD), a speaker etc.; a storage\nportion 608 including a hard disk and the like; and a communication portion 609 comprising a network interface card, such as a LAN card and a modem.  The communication portion 609 performs communication processes via a network, such as the Internet.  A\ndriver 610 is also connected to the I/O interface 605 as required.  A removable medium 611, such as a magnetic disk, an optical disk, a magneto-optical disk, and a semiconductor memory, may be installed on the driver 610, to facilitate the retrieval of a\ncomputer program from the removable medium 611, and the installation thereof on the storage portion 608 as needed.\n In particular, according to an embodiment of the present disclosure, the process described above with reference to the flow chart may be implemented in a computer software program.  For example, an embodiment of the present disclosure includes a\ncomputer program product, which comprises a computer program that is tangibly embedded in a machine-readable medium.  The computer program comprises program codes for executing the method as illustrated in the flow chart.  In such an embodiment, the\ncomputer program may be downloaded and installed from a network via the communication portion 609, and/or may be installed from the removable media 611.  The computer program, when executed by the CPU 601, implements the functions as defined by the\nmethods of the present disclosure.\n The flowcharts and block diagrams in the figures illustrate architectures, functions and operations that may be implemented according to the system, the method and the computer program product of the various embodiments of the present invention. In this regard, each block in the flow charts and block diagrams may represent a module, a program segment, or a code portion.  The module, the program segment, or the code portion comprises one or more executable instructions for implementing the\nspecified logical function.  It should be noted that, in some alternative implementations, the functions denoted by the blocks may occur in a sequence different from the sequences shown in the figures.  For example, in practice, two blocks in succession\nmay be executed, depending on the involved functionalities, substantially in parallel, or in a reverse sequence.  It should also be noted that, each block in the block diagrams and/or the flow charts and/or a combination of the blocks may be implemented\nby a dedicated hardware-based system executing specific functions or operations, or by a combination of a dedicated hardware and computer instructions.\n The units or modules involved in the embodiments of the present application may be implemented by way of software or hardware.  The described units or modules may also be provided in a processor, for example, described as: a processor,\ncomprising a first processing unit, a cancellation unit, a second processing unit and an input unit, where the names of these units or modules are not considered as a limitation to the units or modules.  For example, the first unit may also be described\nas \"a unit for performing high-pass filtering.\"\n In another aspect, the present application further provides a non-volatile computer storage medium.  The non-volatile computer storage medium may be the non-volatile computer storage medium included in the apparatus in the above embodiments, or\na stand-alone non-volatile computer storage medium which has not been assembled into the apparatus.  The non-volatile computer storage medium stores one or more programs.  The one or more programs, when executed by a device, cause the device to: perform\nhigh-pass filtering on a voice signal, in response to detecting the microphone receiving the voice signal containing interfering sound signal; cancel the interfering sound signal in the voice signal subjected to high-pass filtering; perform automatic\ngain control on the voice signal subjected to cancelling the interfering sound signal, to obtain target voice signal; and extract a feature vector from the target voice signal and inputting the feature vector into a pre-trained acoustic model, to obtain\na voice recognition result matching the target voice signal, the acoustic model being used for representing a corresponding relationship between the feature vector and the voice recognition result.\n The foregoing is only a description of the preferred embodiments of the present application and the applied technical principles.  It should be appreciated by those skilled in the art that the inventive scope of the present application is not\nlimited to the technical solutions formed by the particular combinations of the above technical features.  The inventive scope should also cover other technical solutions formed by any combinations of the above technical features or equivalent features\nthereof without departing from the concept of the invention, such as, technical solutions formed by replacing the features as disclosed in the present application with (but not limited to), technical features with similar functions.", "application_number": "15619252", "abstract": " The present application discloses a voice recognition method and\n     apparatus. A specific implementation of the method includes: in response\n     to detecting a microphone receiving voice signal containing interfering\n     sound signal, performing high-pass filtering on the voice signal;\n     cancelling the interfering sound signal in the voice signal subjected to\n     high-pass filtering; performing automatic gain control on the voice\n     signal subjected to cancelling the interfering sound signal, to obtain\n     target voice signal; and extracting a feature vector from the target\n     voice signal and inputting the feature vector into a pre-trained acoustic\n     model, to obtain a voice recognition result matching the target voice\n     signal, the acoustic model being used for representing a corresponding\n     relationship between the feature vector and the voice recognition result.\n     This implementation improves the success rate of voice recognition.\n", "citations": ["5434912", "6049607", "6363344", "20020103639", "20020136417", "20080101622", "20130297299", "20140270150", "20150126255"], "related": []}, {"id": "20180196874", "patent_code": "10372756", "patent_name": "Control system using scoped search and conversational interface", "year": "2019", "inventor_and_country_data": " Inventors: \nSeiber; Scott E. (Medina, WA), Ripsher; Lawrence B. (Seattle, WA)  ", "description": "BACKGROUND\n The industry offers numerous control mechanisms for use in interacting with various devices in a local setting, such as media playback equipment provided in a user's home or work environment.  For instance, a user may build a local network\nhaving a control device and one or more media playback devices (e.g., speakers).  In operation, an authorized user may interact with the control device to select one or more media items (e.g., songs) for playback on the media playback equipment, e.g., by\ncalling up a list of available media items, and selecting one or more items from the list.  While the above approach is relatively straightforward, some users may experience it as cumbersome and inefficient.\n The technical literature generally describes more complex man-machine control interfaces compared to the mechanism described above.  But these types of interfaces can also be expensive and time-consuming to develop, test and maintain.  And once\ndeveloped, these interfaces can potentially offer uneven performance due to their increased complexity.\nSUMMARY\n A computer-implemented technique is described herein for controlling media playback equipment (and other kinds of equipment) in a resource-efficient manner, while offering good user experience.  In one media-related implementation, the technique\nuses a media BOT to intercept a message entered by a user via a message interface component.  The media BOT then determines whether the message includes a selection control command that specifies a target media item (such as a desired song) to be played\nby the media playback equipment.  If such a command is detected, the media BOT formulates an input query that includes the message.  The media BOT sends the input query to a search system, with an instruction that the search system is to perform a\ndomain-specific search.\n In response to the input query, the search system generates a query response that identifies a top-ranked media item, if any, and sends the query response to the media BOT.  The search system performs this operation by searching a curated\nmedia-specific corpus of media items.  The media-specific corpus corresponds to a particular search domain within a much broader informational universe that is otherwise available to the search system.\n Upon receiving the query response, the media BOT generates selection command information based on the query response.  A local system uses the selection control information to control the media playback equipment.\n According to another illustrative aspect, the message interface component corresponds to a conversational interface component.  The conversational interface component provides one or more instances of a collaborative user interface presentation\nfor use by respective users in a group of users, including the particular user.  The users can interact with the conversational interface component to converse with each other (by formulating user-to-user messages), and to control the playback equipment\n(by formulating user-to-BOT messages).\n According to another illustrative aspect, the search system operates by performing preprocessing on the input query, searching the curated media-specific corpus to identify a set of media items (if any) that match the preprocessed input query,\nand ranking the media items in the set to generate the top-ranked media item (if any).\n According to another illustrative aspect, the media BOT posts the selection command information to a command buffering component.  The local system pulls the selection command information from the command buffering component.\n According to another illustrative aspect, the technique allows a group of users to control any piece of equipment in a local setting via the conversational interface component, with or without interaction with the search system.\n By virtue of the use of the search system, the technique can successfully leverage preexisting linguistic processing mechanisms, thereby providing an enhanced control interface without the need to develop custom-built one-off linguistic\nprocessing mechanisms for exclusive use in controlling the media playback equipment.  By virtue of the use of the conversational interface component, the technique provides an effective and enjoyable way by which users can coordinate amongst themselves\nto control media playback equipment in a shared local setting.  By virtue of the use of the command buffering component, the local system can interact with an external network environment in a secure manner.\n The above technique can be manifested in various types of systems, devices, components, methods, computer-readable storage media, data structures, graphical user interface presentations, articles of manufacture, and so on.\n This Summary is provided to introduce a selection of concepts in a simplified form; these concepts are further described below in the Detailed Description.  This Summary is not intended to identify key features or essential features of the\nclaimed subject matter, nor is it intended to be used to limit the scope of the claimed subject matter. BRIEF DESCRIPTION OF THE DRAWINGS\n FIG. 1 shows an illustrative system by which one or more users can control media playback equipment using a search system.\n FIG. 2 shows computing equipment that can be used to implement the system of FIG. 1.\n FIG. 3 shows an illustrative collaborative user interface presentation provided by the system of FIG. 1.\n FIG. 4 shows an illustrative flow of operations performed by various components of the system of FIG. 1.\n FIG. 5 shows one implementation of a media BOT, which is a component of the system of FIG. 1.\n FIG. 6 shows one implementation of a profile updating component and an automated item-selecting component, for use in the system of FIG. 1.\n FIG. 7 shows a process that represents one manner of operation of the system of FIG. 1, from the perspective of the media BOT of FIG. 5.\n FIG. 8 shows a process that represents one manner operation of the search system of FIG. 1.\n FIG. 9 shows a process that represents one manner of operation of the functionality of FIG. 6.\n FIG. 10 shows illustrative computing functionality that can be used to implement any aspect of the features shown in the foregoing drawings.\n The same numbers are used throughout the disclosure and figures to reference like components and features.  Series 100 numbers refer to features originally found in FIG. 1, series 200 numbers refer to features originally found in FIG. 2, series\n300 numbers refer to features originally found in FIG. 3, and so on.\nDETAILED DESCRIPTION\n This disclosure is organized as follows.  Section A describes a computer-implemented system for controlling media playback equipment and other equipment.  Section B sets forth illustrative methods which explain the operation of the system of\nSection A. And Section C describes illustrative computing functionality that can be used to implement any aspect of the features described in Sections A and B.\n As a preliminary matter, some of the figures describe concepts in the context of one or more structural components, also referred to as functionality, modules, features, elements, etc. In one implementation, the various components shown in the\nfigures can be implemented by software running on computer equipment, or other logic hardware (e.g., FPGA devices), etc., or any combination thereof.  In one case, the illustrated separation of various components in the figures into distinct units may\nreflect the use of corresponding distinct physical and tangible components in an actual implementation.  Alternatively, or in addition, any single component illustrated in the figures may be implemented by plural actual physical components. \nAlternatively, or in addition, the depiction of any two or more separate components in the figures may reflect different functions performed by a single actual physical component.  Section C provides additional details regarding one illustrative physical\nimplementation of the functions shown in the figures.\n Other figures describe the concepts in flowchart form.  In this form, certain operations are described as constituting distinct blocks performed in a certain order.  Such implementations are illustrative and non-limiting.  Certain blocks\ndescribed herein can be grouped together and performed in a single operation, certain blocks can be broken apart into plural component blocks, and certain blocks can be performed in an order that differs from that which is illustrated herein (including a\nparallel manner of performing the blocks).  In one implementation, the blocks shown in the flowcharts can be implemented by software running on computer equipment, or other logic hardware (e.g., FPGA devices), etc., or any combination thereof.\n As to terminology, the phrase \"configured to\" encompasses various physical and tangible mechanisms for performing an identified operation.  The mechanisms can be configured to perform an operation using, for instance, software running on\ncomputer equipment, or other logic hardware (e.g., FPGA devices), etc., or any combination thereof.\n The term \"logic\" encompasses various physical and tangible mechanisms for performing a task.  For instance, each operation illustrated in the flowcharts corresponds to a logic component for performing that operation.  An operation can be\nperformed using, for instance, software running on computer equipment, or other logic hardware (e.g., FPGA devices), etc., or any combination thereof.  When implemented by computing equipment, a logic component represents an electrical component that is\na physical part of the computing system, in whatever manner implemented.\n Any of the storage resources described herein, or any combination of the storage resources, may be regarded as a computer-readable medium.  In many cases, a computer-readable medium represents some form of physical and tangible entity.  The term\ncomputer-readable medium also encompasses propagated signals, e.g., transmitted or received via a physical conduit and/or air or other wireless medium, etc. However, the specific terms \"computer-readable storage medium\" and \"computer-readable storage\nmedium device\" expressly exclude propagated signals per se, while including all other forms of computer-readable media.\n The following explanation may identify one or more features as \"optional.\" This type of statement is not to be interpreted as an exhaustive indication of features that may be considered optional; that is, other features can be considered as\noptional, although not explicitly identified in the text.  Further, any description of a single entity is not intended to preclude the use of plural such entities; similarly, a description of plural entities is not intended to preclude the use of a\nsingle entity.  Further, a \"set\" or \"subset,\" as used herein, refers to a collection of zero, one or more items.  Further, while the description may explain certain features as alternative ways of carrying out identified functions or implementing\nidentified mechanisms, the features can also be combined together in any combination.  Finally, the terms \"exemplary\" or \"illustrative\" refer to one implementation among potentially many implementations.\nA. Illustrative System\n FIG. 1 shows a system 102 (also referred to as a system environment) by which one or more users 104 can control media playback equipment 106.  As will be described at the end of this section, the system 102 can also be extended to controlling\nother equipment within a local setting, such as lighting equipment, home security equipment, etc. However, to facilitate and simplify the explanation, the system 102 will be primarily described below in the context in which the equipment to be controlled\nconstitutes the media playback equipment 106.\n The media playback equipment 106 presents media items in a local environment.  For instance, the media playback equipment 106 can corresponding to music-playing equipment for playing songs and other audio items, video-playing equipment for\nplaying movies, etc., or any combination thereof.  The local environment can correspond to any shared setting in which the users 104 consume the media items.  For example, the local environment can correspond to one or more rooms of a building and/or an\noutdoor venue in which the users 104 listen to music and/or video items provided by the media playback equipment 106.\n The users 104 interact with the system 102 via a conversational interface component 108.  In one implementation, the conversational interface component 108 provides a chat-like experience via one or more instances of a collaborative user\ninterface (UI) presentation 110.  That is, each user can interact with a separate instance of the collaborative UI presentation 110 via a user computing device.  Each instance of the collaborative UI presentation 110 reveals an historical flow of\nmessages entered by the users 104 over the course of a conversation.  Further, at any given instance of time, a user may enter a new message via his or her instance of the collaborative UI presentation 110.\n The messages entered by the users 104, via the conversational interface component 108, fall at least three classes of messages.  A user enters a message of a first type with the primary intent of communicating with another user in the group. \nFor instance, a first user may enter the message, \"Does anyone want to hear some music?\" with the intent of soliciting a response from other users.  A user enters a message of a second type with the primary intent of instructing the media playback\nequipment 106 to play a particular media item, such as a particular song, movie, etc. For instance, a user may enter the message, \"Play something new by Neil Young\" with the intent of instructing the media playback equipment 106 to play a recently\nreleased song by the artist Neil Young.  A user may enter a message of a third type with the intent of controlling the manner in which the media playback equipment 106 plays a media item that has already been selected or has yet to be selected.  For\ninstance, a user may enter the message, \"Turn up the volume to 8\" to instruct the media playback equipment 106 to increase the volume at which it plays a media item.\n The above categories of messages are not mutually exclusive for various reasons.  First, the conversational interface component 108 reveals a message entered by a user to all of the other users in the group, even if the primary intent of the\nmessage is to control the media playback equipment 106, rather than communicate with other users.  Hence, any message directed to the media playback equipment 106 inherently also conveys information to other users.  Second, a user may deliberately create\na message that has at least two parts, including a first part intended to communicate information to other users, and a second part intended to control the media playback equipment 106.  For instance, a user may enter the message, \"Play some Neil Young\nfrom Harvest Moon, unless anyone objects.\" The first part of that message instructs the media playback equipment 106 to play music from a particular album release by the artist Neil Young.  The second part of that message invites other users to object to\nthe user's selection.\n More generally, the system 102 can incorporate any message interface component for receiving messages from at least one user.  In the embodiment of FIG. 1, the message interface component corresponds to a conversational interface component 108\nthat provides one or more instances of a collaborative UI presentation 110.  But in other implementations, the message interface component provides an input mechanism for receiving messages from a user without revealing those messages to other users who\nare also entitled to interact with the message interface component.  To nevertheless facilitate and simplify the explanation, the message interface component will be described below in the context of the implementation of FIG. 1, where it corresponds to\nthe conversational interface component 108.\n A media BOT 112 refers to a computer-automated agent.  In operation, the media BOT 112 intercepts each message entered by a user.  The media BOT 112 then determines whether the message pertains to the first type of message (which is primarily\ndirected to conveying information to other users), or whether the message pertains to either the second or third types of message (which are primarily directed to controlling the media playback equipment 106).  In the former case, the media BOT 112 takes\nno action with respect to the message.  In the latter case, the media BOT 112 initiates processes within the system 102 that have the ultimate objective of controlling the media playback equipment 106.  With respect to the latter case, the media BOT 112\ncan also provide feedback to the users 104 via the conversational interface component 108 which informs the users 104 of changes that have been made, or will be made, to the media playback equipment 106; alternatively, or in addition, the media BOT 112\ncan solicit additional information from the users 104 with the intent of clarifying the intent of the users 104.\n In other words, the media BOT 112 analyzes each message to determine whether it is a user-to-user message or a user-to-BOT message.  A user-to-user message has the sole intent of conveying information to another user, rather than conveying an\ninstruction to the media playback equipment 106.  A user-to-BOT message includes at least a part which has the primary intent of controlling the media playback equipment 106.  With respect to a user-to-BOT message, the media BOT 112 perform the further\ntask of determining whether it contains: a selection control command, in which a user specifies a target media item to be played by the media playback equipment 106; and/or a non-selection control command, corresponding to any media control command other\nthan a selection control command.\n Additional details regarding the operation of the media BOT 112 will be set forth below with respect to FIG. 5.  As a preview of that explanation, the media BOT 112 can discriminate among the above-described types of messages by detecting\ntriggering features in the messages, such as telltale keywords.  For example, the media BOT 112 can determine that the user has entered a message containing a selection control command when that message contains the keyword \"play.\"\n Assume the media BOT 112 determines that a particular message entered by a user contains a selection control command.  If so, the media BOT 112 passes the entirety of the message, or a portion thereof, as an input query to a search system 114. \nThe search system 114 generates a query response that identifies zero, one or more media items that match the input query, and then sends the query response to the media BOT 112.  Upon receipt of the query response, the media BOT 112 generates selection\ncontrol information based on the query response.  The selection control information conveys instructions to the media playback equipment 106 to play the media item(s) identified in the query response.  The media BOT 112 can also optionally send a\nBOT-generated message to the conversational interface component 108, which informs the users 104 of the media item(s) that have been selected.\n In one implementation, the search system 114 is \"scoped\" insofar as it matches the input query against a curated media-specific corpus 116 of media items, rather than a much larger informational universe 118 of result items pertaining to other\ndomains.  For instance, the curated media-specific corpus 116 may pertain to a collection of songs provided by particular network-accessible service.  For example, the curated media-specific corpus 116 may correspond to any podcast service, such as the\nSPOTIFY service provided by Spotify Technology S.A., of Stockholm, Sweden.  In contrast, the larger information universe 118 of result items may correspond to all items accessible via a wide area network, such as all items available via the Web.\n The curated media-specific corpus 116 is \"curated\" in the sense that a provider of the corpus 116 applies one or more provider-specific rules to govern: (a) what kinds of items are stored in the corpus 116; and (b) what format(s) are used to\nstore the items in the corpus 116.  For instance, the provider can adopt a rule that specifies that only media items that meet certain digital rights management (DMR) criteria are permitted to be stored in the corpus 116.  Further, the provider can adopt\na rule that specifies that each media item that is stored is accompanied by well-defined descriptive metadata.  That metadata can convey various attributes pertaining to the media item, such as: one or more identifiers associated with the media item; a\ntitle pertaining to the media item; an artist associated with the media item; a release date associated with the media item, a brief description of the media item, and so on.  Finally, the provider can adopt a rule that specifies that the playable\ncontent of each media item is stored in a prescribed format (or formats).\n In the case of an existing provider such as SPOTIFY, a well-established and highly uniform media-specific corpus already exists.  In another implementation, a community of authorized individuals can create a custom media-specific corpus for\nspecific use with the system 102.  The community can adopt a standard which specifies the preferred format of media items stored by the corpus.  The community can also establish an enforcement mechanism for rejecting newly submitted media items that do\nnot conform to the preferred format.\n In one case, the search system 114 is implemented using at least some of the processing resources provided by a commercially available general-purpose search engine, such as the BING search engine provided by Microsoft Corporation of Redmond,\nWash.  In some cases, the general-purpose search engine is originally designed and configured to perform a domain-agnostic search over all candidate result items provided in the larger information universe 118.  In the context of the system 102, however,\nthe search system 114 is constrained to match the input query against just the curated media-specific corpus 116.\n In one implementation, the media BOT 112 can instruct the search system 114 to perform a scoped search (instead of a normal domain-agnostic search) by submitting an instruction to that effect, along with the input query.  For example, the media\nBOT 112 can provide an instruction that specifies that the input query is to be searched with respect to a domain associated with a particular uniform resource locator (URL).  For instance, the media BOT 112 can convey this instruction as a parameter in\nan application programming interface (API) exposed by the search system 114.  The system 102 responds to the instruction by performing its search operations with respect to the identified corpus, rather than the larger informational universe 118.\n In another scenario, assume that the media BOT 112 intercepts a message that contains a non-selection control command.  As noted above, a non-selection control command pertains to any command that controls the operation of the media playback\nequipment 106, other than a command to select a particular media item.  Without limitation, illustrative non-selection control commands correspond to requests to increase the volume, decrease the volume, pause playback, resume playback, stop the\nplayback, move forward within the playback (e.g., by fast-forwarding within the playback), move backward within the playback (e.g., by rewinding within the playback), and so on.  Upon detecting a non-selection control command, the media BOT 112 can\nimmediately generate non-selection control information associated with that command, without interacting with the search system 114.  The system 102 uses the non-selection control information to control the media playback equipment 106.  The media BOT\n112 can optionally also send a BOT-generated message to the conversational interface component 108 which identifies the nature of the control information that has been (or will be) sent to the media playback equipment 106.\n To summarize, the media BOT 112 generates selection control information upon encountering a selection control command in a message.  This operation involves sending the message containing the selection control command to the search system 114. \nThe media BOT 112 generates non-selection control information upon encountering a non-selection control command in a message, without interacting with the search system 114.  And the media BOT 112 generates no control information when the message\ncontains neither a selection control command nor a non-selection control command.\n A command buffering component 120 receives instances of control information from the media BOT 112 and stores those instances in a data store 122.  More specifically, assume that the system 102 assigns a particular account ID to the particular\ngroup of users 104.  The command buffering component 120 can store all instances of control information created by the users 104 in a particular storage space (e.g., a particular file) within data store 122, associated with that account ID.  The command\nbuffering component 120 can store instances of control information associated with another group of users (not shown) in another storage space within the data store 122, associated with another account ID.\n The command buffering component 120 can ensure the privacy of information stored in the data store 122 using any security mechanisms, such as password-protection mechanisms, encryption mechanisms, etc. An entity can retrieve control information\nfrom the data store 122 only after specifying a valid account ID and submitting the appropriate credentials (e.g., a valid password and/or encryption key(s) associated with the specified account ID).\n In one implementation, and as described more fully in FIG. 2 (below), at least some of the components of the system 102 are implemented by one or more remote computing systems, through which users interact using respective user computing\ndevices.  A local system 124, by contrast, corresponds to any equipment provided in the local environment in which media items are presented to the users 104 via the media playback equipment 106.  For instance, the local system 124 may correspond to all\nequipment provided within one or more rooms of a building.  The users 104 may correspond to a group of individuals who work together as a team in that building.  Or the users 104 may correspond to family members who work together in that building, etc.\n The local system 124 can include a command retrieval component 126 which pulls instances of control information from the command buffering component 120 over a computer network 128.  For instance, in a first implementation, the command retrieval\ncomponent 126 can periodically interrogate the command buffering component 120 (e.g., every 3 seconds).  Upon each interrogation, the command retrieval component 126 identifies an account ID and the appropriate credentials associated with that account\nID.  In response, the command buffering component 120 validates the inquiry, and then determines whether the data store 122 includes any new instances of control information that have not yet been downloaded to the local system 124.  Upon identifying new\ninstance of command information, the command buffering component 120 forwards the new instances to the command retrieval component 126.\n In a second implementation, the command buffering component 120 can proactively notify the command retrieval component 126 when any new instances of control information have been received by the media BOT 112.  The command retrieval component\n126 can then access the new instances of control information in the manner specified above.\n The first and second implementations both involve receiving new instances in response to a polling request by the local system 124, which helps help reduce the risk that malicious agents in the external network environment can gain unauthorized\naccess to the resources of the local system 124.  But in a third implementation, the command buffering component 120 can automatically forward the new instances of control information to the command retrieval component 126, without first notifying the\ncommand retrieval component 126 of the arrival of those new instances.\n A media controller 130 interacts with the media playback equipment 106 to execute each instance of control information.  In some implementations, the media controller 130 represents a component that is separate from the media playback equipment\n106.  In other implementations, the media controller 130 represents a component that is integrated into the media playback equipment 106.  A local network 132 couples all components of the local system 124 together.  For instance, the local network 132\nmay include a router which couples the media controller 130 to each playback device associated with the media playback equipment 106.  In one implementation, the local network 132 uses the Universal Plug and Play (UPnP) protocol to coordinate interaction\namong its components, and to handle the addition and removal of components from the local network 132.\n In a first scenario, assume that a new instance of selection control information has been received.  The selection control information specifies an item ID of at least one media item.  The media controller 130 establishes a connection with the\nsource(s) of the identified media item(s), and then directs the media playback equipment 106 to play the media item(s).  For instance, in one implementation, the media controller 130 can establish a connection with a remote computing system associated\nwith the curated media-specific corpus 116.  The media controller 130 can then request the remote computing system to stream the requested media item(s), associated with the specified item ID(s), to the local system 124, for consumption by the media\nplayback equipment 106.  In another implementation, the media controller 130 can establish a connection with a local source 134 of the media item(s), such as a local storage device which stores the media item(s).  The media controller 130 can then\nrequest the media playback equipment 106 to play the media item(s) obtained from the specified local source 134.\n The media playback equipment 106 itself can include any combination of music playback equipment (e.g., corresponding to one or more speakers), video playback equipment (e.g., corresponding to one or more video presentation devices), etc. To cite\nmerely one example, at least one playback device can include a speaker device provided by SONOS, INC., of Santa Barbara, Calif.\n Now referring to the search system 114 in greater detail, the search system 114 can include a variety of processing components that it uses to process any input query, regardless of the origin of the input query.  The system 102 leverages these\npreexisting processing components to perform the specific task of interpreting a message submitted by a user that contains a selection control command.\n To begin with, the search system 114 includes an index updating component 136 for crawling the resources of the larger informational universe 118, e.g., corresponding to the resources provided by the entire Web.  In doing so, the index updating\ncomponent 136 inherently crawls the resources provided in the curated media-specific corpus 116.  The index updating component 136 can use any strategy to crawl the Web.  For instance, the index updating component 136 can use a collection of spiders to\nmethodically investigate the linked resources of the Web.  The index updating component 136 can then extract the contents from any new (or updated) resource that it encounters.  In one implementation, the search system 114 can perform its crawling\noperation on a periodic basis.\n The index updating component 136 then updates index information provided in an index data store 138.  The index information maps the terms discovered in the web resources with the uniform resource locators (URLs) associated with those web\nresources.  The index information provides a way of later identifying the web resources that contain a term identified in an input query.  In one case, the index updating component 136 creates a conventional inverted index.\n In a real-time phase of operation, an interface component (not shown) receives a new input query from either an actual user or the media BOT 112.  In the case of the media BOT 112, the input query contains a message entered by a user which\ncontains a selection control command, such as the message \"I want to play a popular song by Neil Diamond.\" The interface component also receives an instruction that the input query is to be searched against the media-specific corpus 116, rather than the\nlarger information universe 118.\n A query preprocessing component 140 performs preprocessing on the input query to generate a preprocessed input query.  The preprocessing can include any combination of operations, including, but not limited to: tokenization, parsing analysis,\nspelling correction, stemming, synonym expansion, stop word removal, named entity recognition, semantic analysis, term weighting, etc. As a general objective, the query preprocessing component 140 adds (and/or removes) information to (or from) the input\nquery, which allows downstream components in the search system 114 to more efficiently process it.\n Tokenization entails breaking the input query into its constituent parts (e.g., its constituent words).  In one case, the query preprocessing component 140 performs tokenization by breaking the input query into parts demarcated by whitespace\ncharacters, and by the start and end of the input query string.\n Parsing analysis entails assigning a descriptive label to each part of the query.  The query preprocessing component 140 can use a hidden Markov model, a conditional random fields model, or any other statistical technique to perform the parsing.\n Spelling correction entails correcting the spelling of a term in the input query.  The query preprocessing component 140 can perform spelling correction by determining whether or not an input term is present within a dictionary.  If the term is\nnot present, the query preprocessing component 140 can identify the word in the dictionary which is closest to the input term, as assessed based on any metric of word similarity (e.g., edit distance).  The query preprocessing component 140 can also\nconsider word frequency information in choosing the correct spelling of a term, e.g., by more favorably weighting popular word candidates compared to unpopular word candidates.\n Stemming analysis entails finding a root form of each word specified in the input query.  The query preprocessing component 140 can perform stemming analysis using any strategy, such as by using a lookup table to map an input term to its\ncanonical stemmed counterpart.  Or the query preprocessing component 140 can apply a collection of suffix-stripping rules to generate the stemmed counterpart, etc.\n Synonym expansion entails identifying the synonyms (if any) of a term in the input query and adding those synonyms to the input query.  The query preprocessing component 140 can perform synonym expansion using a synonym dictionary which maps an\ninput term to its semantically equivalent terms.\n Stop word removal entails removing terms in the input query that constitute informational noise.  The query preprocessing component 140 can perform stop word removal by removing any term in the input query that is present in an established stop\nword list.\n Named entity recognition entails identifying parts of the input query, if any, which identify named entities.  A named entity, in turn, refers to a particular person, organization, place, event, etc., often associated with a proper noun.  The\nquery preprocessing component 140 can identify named entities in the input query using a lookup dictionary that contains established named entities, and/or by using a machine-trained statistical model, etc.\n Semantic analysis entails more generally interpreting the meaning of terms in the input query and/or the input query as a whole.  The query preprocessing component 140 can perform semantic analysis in any manner, e.g., by using machine-trained\ndeep-learning neural network.  The deep-learning neural network can map an encoded version of the input query (or part thereof) into a vector in an abstract semantic space.  The vector conveys information regarding the meaning of the input query.\n Weighting analysis entails assigning weights to the individual terms in the input query.  The query preprocessing component 140 can assign weights to a query term based on various factors, e.g., based on the frequency at which the term occurs in\na corpus of documents, based on the frequency at which the term appears in previously encountered search terms, based on the position of the term in the input query, and so on.\n The above-summarized query processing operations are cited by way of example, not limitation.  Other implementations can apply one or more additional query processing operations not mentioned above, and/or can omit one or more query processing\noperations mentioned above.\n A lookup component 142 finds a set of media items that match the preprocessed input query, culled from the curated media-specific corpus 116.  For instance, in one manifestation, the lookup component 142 finds each media item that contains all\nthe terms in the preprocessed input query, e.g., within the metadata associated with a song.  The lookup component 142 uses the index information in the data store 138 to find matching media items.  But the lookup component 142 relies on only that part\nof the index information in the data store 138 that is relevant to the curated media-specific corpus 116.\n To facilitate and simplify explanation, the examples presented here correspond to the case in which the matching media items correspond to individual songs, movies, etc. But a matching media item may also refer to a collection of media items,\nsuch as a song playlist.  For example, the search system 114 may identify a playlist as a candidate matching item when the user enters the command, \"Play Neil Young's greatest hits.\"\n A ranking component 144 ranks the media items identified in the set of media items in order of likelihood of matching the input query.  The ranking component 144 can use any algorithm to perform this approach.  In one case, the ranking component\n144 uses a machine-trained statistical model to generate a confidence score for each candidate media item.  It then picks the candidate media item(s) having the highest confidence score(s).  The machine-trained statistical model operates by receiving a\nset of descriptive feature values for each candidate media item.  The feature values describe the preprocessed input query and the candidate media item, and, optionally, other contextual information.  The machine-trained statistical model then applies\nits machine-trained weight values to map the feature values into a score.\n In one specific implementation, the machine-trained model can use a deep-learning neural network model.  That model maps the preprocessed input query into a first vector in abstract semantic space, and then maps the candidate media item into a\nsecond vector in the abstract semantic space (wherein the second mapping operation can alternatively be performed beforehand, in offline fashion).  The model then determines the degree of similarity between the input query and the candidate media item by\ndetermining the distance between the two vectors in the semantic space.  The model can use any technique to assess the distance, such as a cosine similarity metric.\n Overall, the system 102 offers various potential benefits.  By virtue of the use of the search system 114, scoped in the manner described above, the system 102 can successfully leverage preexisting linguistic processing mechanisms, thereby\nproviding an enhanced control interface while avoiding the need to develop custom-built one-off linguistic processing mechanisms.  By virtue of the use of the conversational interface component 108, the system 102 provides an effective and enjoyable way\nby which users can coordinate amongst themselves to control the media playback equipment 106 in a shared local setting.\n FIG. 2 shows computing equipment 202 that can be used to implement the system 102 of FIG. 1.  The computing equipment 202 includes plural user computing devices 204, one or more remote computing systems 206, and the local system 124.  The local\nsystem 124, in turn, includes the various components described above in connection with the explanation of FIG. 1.  A computer network 128 enables interaction among the user computing devices 204, the remote computing system(s) 206 and the local system\n124.  Further, the local system 124 can interact with the other parts of the computing equipment 202 via a firewall 208.  For instance, with reference to FIG. 1, the command retrieval component 126 can interact with the command buffering component 120\nvia the firewall 208.\n Any user computing device shown in FIG. 2 can correspond to any type of computing equipment, such as a desktop or laptop personal computing device, a handheld computing device of any type (e.g., smartphone, tablet-type device, etc.), a set-top\nbox, a game console, a wearable computing device, an intelligent appliance, etc. Any remote computing system shown in FIG. 2 can be implemented by one or more server computing devices and other computing equipment (e.g., routers, etc.).  The computer\nnetwork 128 can correspond to a wide area network (such as the Internet), a local area network, one or more point-to-point links, or any combination thereof.\n Different implementations can delegate the components shown in FIG. 1 to the computing equipment 202 of FIG. 2 in different respective ways.  FIG. 2 makes this general point by showing that any user computing device, any remote computing system,\nand the local system 124 can implement any part of the system functionality shown in FIG. 1.  For instance, in some implementations, the conversational interface component 108 can correspond to a collaborative messaging application.  That application can\nrun on each user computing device and/or on a remote computing system.  Likewise, the media BOT 112 can correspond to a computer program that runs on each user computing device and/or on a remote computing system.  The collaborative message application\ncan include a backend service which passes messages entered by the user to the media BOT 112.\n The command buffering component 120 can correspond to a storage service provided by another remote computing system.  The media BOT 112 can forward instances of control information to the command buffering component 120 via the computer network\n128.  The local system 124 can download instances of control information from the command buffering component 120 via the computer network 128, through the firewall 208.\n The search system 114 can correspond to yet another remote computing system.  The media BOT 112 can interact with the search system 114 via the computer network 128.  As described above, in one implementation, the search system 114 uses the\nresources of a general-purpose search engine, which is configured, upon instruction from the media BOT 112, to perform its search over the curated media-specific corpus 116.\n In other implementations, two or more components shown in FIG. 1 can be implemented by a single device or remote computing system.  For example, the same remote computing system can host the media BOT 112 and the command buffering component 120.\n From a management perspective, different entities can administer different respective components of the system 102 of FIG. 1.  For example, a first service provider can host the conversational interface component 108, while a second service\nprovider can host the media BOT 112.  In other cases, a single entity can administer two or more components of the system 102 of FIG. 1.  For example, the same entity can provide both the conversational interface component 108 and the media BOT 112.\n FIG. 3 shows an instance of the collaborative user interface (UI) presentation 110 provided by the conversational interface component 108 of FIG. 1.  Note that the particular collaborative UI presentation 110 shown in FIG. 3 is presented in the\nspirit of illustration, not limitation.  For instance, other implementations can add other functional features to the set of UI features shown in FIG. 3.  Alternatively, or in addition, other implementations can omit one or more functional UI features\nshown in FIG. 3.  Alternatively, or in addition, other implementations can vary the appearance and organization of the UI features shown in FIG. 3.  Alternatively, or in addition, other implementations can use different graphical control mechanisms for\nreceiving input from the user, compared to the graphical control mechanisms shown in FIG. 3.  Finally, other implementations can interact with users using other modes compared to a visual graphical user interface presentation; for example, other\nimplementations can receive input from users using voice recognition technology, and provide output to the users in spoken form.\n Assume that a particular user, Tom, interacts with the collaborative UI presentation 110 via a first user computing device.  Other users (e.g., users John, Jean and Sally) can interact with other instances of the collaborative UI presentation\n110 via other user computing devices.  Alternatively, or in addition, two or more users can interact with the same instance of the collaborative UI presentation 110 via the same user computing device.\n A first section 302 invites the user, Tom, to select a particular channel, among a set of offered channels.  Each channel corresponds to a particular focus of discussion.  The focus can be defined with respect to any attribute(s), such as the\nsubject matter of a conversation and/or the individuals that are included as participants of the conversation.  Further, different channels may provide backend integration with different respective media BOTs.\n For instance, a first channel (Channel A) may allow a first group of users to control music playback equipment in a shared environment.  The first channel can provide backend integration with a first type of media BOT.  A second channel (Channel\nB) may allow a second group of users to control video playback equipment in another shared environment.  The second channel can provide backend integration with a second type of media BOT.  Assume that the user (Tom) has selected the first channel\n(Channel A) because that user wishes to control music playback equipment within a work setting he shares with John, Jean and Sally.\n In one case, each channel is associated with a restricted group of defined participants.  Other users cannot take part in the conversation associated with that channel, unless first invited by one of the current participants of the conversation. In another case, each channel is associated with an open-ended group of participants.  In that case, any user who is present within the shared environment can contribute to a conversation hosted by the channel.  In the example of FIG. 3, assume that the\nuser (Tom) chooses channel A. Further assume that the channel A involves at least four participants, Tom, John, Jean and Sally.\n A second section 304 displays an ongoing conversation among the four users.  For instance, in message 306, Tom first asks his colleagues whether they wish to hear music at the present time.  This message 306 corresponds to a user-to-user message\nbecause it does not contain any command directed to the media playback equipment 106.  After receiving input from others, Tom enters another message 308 that reads, \"How about, play Living in America from the Rocky movie.\" This message 308 corresponds to\na user-to-BOT message because the user is instructing the media playback equipment 106 to play a particular song.  The media BOT 112 determines that the user has entered a selection control command via the message 308 because the user has typed the\ntriggering keyword \"play.\"\n In response, the media BOT 112 submits the message 308 as an input query to the search system 114, along with an instruction to perform a scoped media-specific search.  The media BOT 112 receives, in response to the input query, a query response\nwhich identifies a top-ranked song.  The media BOT 112 then formulates selection control information which describes the top-ranked song for consumption by the local system 124.  Further, in message 310, the media BOT 112 displays descriptive information\nregarding the top-ranked song.  The message 310 allows the users to verify that the media BOT 112 has correctly interpreted Tom's message.\n Next, assume that the user Tom formulates another message 312 which reads, \"Increase volume to 8.\" The media BOT 112 will interpret this message 312 as a non-selection control command, e.g., due to the presence of \"volume\" and/or \"increase\" in\nthe message.  In response, the media BOT 112 directly formulates non-selection control information without interacting with the search system 114.\n In the subsequent messages, other users react to Tom's media selection, e.g., by selecting additional media items, controlling the volume of the music playback, controlling the state of the music playback, and so on.  In making these choices,\nthe users are also communicating with each other.\n Overall, the conversational interface component 108 provides an effective and enjoyable way of controlling shared equipment.  In particular, the first section 302 informs each user of the control-related actions made by others, shedding light on\nwho made the actions, and why the actions were made.  This allows the user to react to the control-related actions in an appropriate manner, and to more effectively reach consensus as to future control-related actions.\n For example, assume that a first user selects a song that a second user dislikes.  Without the conversational interface component 108, the second user would need to manually canvas his or her colleagues to understand who made the song selection,\nand to gauge that user's \"investment\" in their song selection.  The second user would then need to manually consult his or colleagues to pick a new song.  The second user would then need to separately interact with a control device to choose a new song. \nSuch a manual operation is potentially cumbersome, time-consuming and imprecise.  It is also potentially the source of mutual irritation within the group.\n Although not shown, there may be occasions in which the search system 114 cannot identify any media items with a desired degree of confidence.  For instance, the ranking component 144 can assign a confidence score to each candidate item.  The\nsearch system 114 can be said to generate uncertain results when the top-ranked media item has a confidence score below a prescribed threshold value.  In this case, the media BOT 112 can generate a message which invites the user to rephrase his\nselection.  Or the media BOT 112 can present the top-ranked media item to the user, and ask the user to confirm whether that media item is the item being sought by the user.\n An optional third section 314 shows a queue of media items that have been, and will be, presented on the media playback equipment 106.  The third section indicates that a particular song 316 is currently selected for playback, but that the\nplayback is paused at the current time.\n An optional fourth section 318 may provide control mechanisms (not specifically shown in FIG. 3) that allow each user to manually control the media playback equipment 106.  For instance, the fourth section 318 can provide control mechanisms that\nallow the user to select a particular song, change the volume at which the song is played, pause the song, stop the song, etc.\n FIG. 4 shows an illustrative flow of operations performed by various components of the system 102 of FIG. 1.  The flow of operations is triggered by the user's input of the message 308 and the message 312 shown in FIG. 3.\n In operation (1), the conversational interface component 108 receives the message, \"How about play Living in America from the Rocky movie.\" In operation (2), the media BOT 112 detects that the user has invoked a selection control command, e.g.,\nby virtue of the fact that the message contains the word \"play.\" In operation (3), the media BOT 112 sends an input query which includes the message entered by the user.\n In operation (4), the search system 114 performs a search based on the input query to generate a query response.  The query response identifies the song that most likely matches the media item being sought by the user.  In operation (5), the\nsearch system 114 sends the query response to the media BOT 112.\n In operation (6), the media BOT 112 receives the query response and optionally presents a BOT message to the conversational interface component 108.  The BOT message informs the user of the song that has been identified by the search system 114. In operation (7), the conversational interface component 108 displays the BOT message to the user (as well as all of the other users).  In operation (8), the media BOT 112 formulates selection command information based on the query response and posts\nthat instance of selection command information to the command buffering component 120.\n In operation (9), the local system 124 retrieves the selection control information from the command buffering component 120.  In operation (10), the local system 124 plays the media item identified by the selection control information on the\nmedia playback equipment 106.\n In operation (11), the conversational interface component 108 receives another message from the user, this time reading, \"Increase volume to 8.\" In operation (12), the media BOT 112 intercepts the message, and detects that the user has entered a\nnon-selection control command, e.g., by virtue of the fact that the message contains the words \"volume\" and/or \"increase.\" In operation (13), the media BOT 112 formulates a BOT message which informs the user of its interpretation of the message, and its\nintent to control the media playback equipment 106 in a corresponding manner.  Further, the media BOT 112 generates non-selection control information and posts that information to the command buffering component 120.  In operation (14), the\nconversational interface component 108 presents the media BOT's message.  In operation (15), the local system 124 retrieves the non-selection control information from the command buffering component 120.  In operation (16), the local system 124 adjusts\nthe volume of the media playback equipment 106, as instructed by the non-selection command information.\n Although not shown, consider the scenario in which a single message includes both a selection control command and a non-selection control command, e.g., as when the user inputs the message, \"Play Neil Young's Heart of Gold at volume level 5.\"\nThe media BOT 112 will detect that the message contains both a selection control command and a non-selection control command.  In response, the media BOT 112 will perform both types of control operations described in FIG. 4, e.g., by submitting the\nmessage as an input query to the search system 114, and formulating non-selection control information on the basis of the message.  The search system 114 will likely inherently discount the trailing phrase \"at volume level 5,\" unless there is a song by\nNeil Young that includes a similar phrase in its title.  Similarly, the media BOT 114 can be configured to ignore content of the message (such as \"Play Neil Young's Heart of Gold\") that is not directly relevant to the instruction to control the volume of\nthe media playback equipment 106.\n FIG. 5 shows one implementation of the media BOT 112.  A message receiving component 502 intercepts a message provided by a user to the conversational interface component 108.  The message receiving component 502 relies on a backend channel\nbetween the conversation interface component 108 and the media BOT 112.\n A command extraction component 504 determines whether the message includes a selection control command or a non-selection control command, or a combination thereof.  The command extraction component 504 can make this conclusion by determining\nwhether the message contains one or more prescribed keywords associated with a selection control command (such as \"play,\" \"show,\" etc.), or one or more prescribed keywords associated with a non-selection control command (such as \"increase,\" \"decrease,\"\n\"volume,\" \"pause,\" \"stop,\" \"forward,\" \"rewind,\" etc.).  For a user-to-user message, the command extraction component 504 will determine that it contains no control command.  In that case, the media BOT 112 will perform no further processing on the\nmessage\n Alternatively, or in addition, the command extraction component 504 can apply linguistic analysis to determine the intent of the message, e.g., by using a machined-learned statistical model to interpret the intent of the message.\n Alternatively, or in addition, the command extraction component 504 can determine whether the user has explicitly invoked the services of the media BOT 112 by determining whether the user's message includes explicit reference to the BOT.  For\ninstance, the user may create a message, \"BOT play Neil Diamond,\" or \"BOT increase volume to 8,\" etc.\n A result processing component 506 generates control information, corresponding to either selection control information and/or non-selection control information.  The result processing component 506 then posts the control information to the\ncommand buffering component 120.  The result processing component 506 can also generate a confirmatory BOT message which informs the users of what control action will be (or has been) taken.\n The result processing component 506 can generate selection control information by extracting the item ID of the top-ranked media item from the query response.  The result processing component 506 can also provide an instruction to the play the\nmedia item associated with the identified item ID.\n FIG. 6 shows one implementation of a profile updating component 602 and an automated item-selecting component 604, for use in the system 102 of FIG. 1.  The profile updating component 602 generates a user profile for each user who interacts with\nthe system 102, and each group which interacts with the system 102.  Overall, the profile updating component 602 generates a plurality of user profiles 606 and a plurality of group profiles 608.\n Each user profile can identify the prior media selections made by a particular individual user, and/or other control selections made by that user.  Each group profile can identify the prior media selections made by a particular group of users,\nand/or other control selections made by that group of users.  The profile updating component 602 can also store metadata for each such selection.  The metadata describes the contextual circumstance in which each selection was made.  For example, a\nprofile can identify that a user selected a media item on a particular day, at an identified time, etc.\n The automated item-selecting component 604 automatically selects a media item based on the user profiles 606 and/or the group profiles 608.  For instance, assume that a particular group of users is currently interacting with the system 102 in a\ncurrent context.  The automated item-selecting component 604 can first generate metadata that describes the current contextual situation, such as the current day of the week and the current time of the day (and/or any other environmental attribute(s)). \nThe automated item-selecting component 604 can then consult a group profile of the group to identify the selections made by that group of users in the past in the same contextual situation.  The automated item-selecting component 604 can then identify at\nleast one media item that is similar to the previous selections.\n The automated item-selecting component 604 can make the above similarity determination in different ways.  In one such approach, the automated item-selecting component 604 can use a deep-learning neural network to map the prior media selections\nto respective vectors in an abstract semantic space.  The vectors may define a cluster within a particular region of the semantic space.  The automated item-selecting component 604 can then choose a media item which maps to the same region of the\nabstract semantic space defined by the cluster of vectors.  In a variant of this approach, the profile updating component 602 can generate the cluster of abstract semantic vectors as an offline process, and include those vectors as part of the group\nprofile.\n In another approach, the automated item-selecting component 604 can leverage a click log to find a similar media item (or items).  The click log identifies each occasion in which a user selected two or more media items in a similar search\ncontext, e.g., in response to submitting the same input query, or within a same search session, etc. The automated item-selecting component 604 can treat the prior media selections of a group of users as anchor selections.  It can then use the click log\nto find additional media selections that are linked to the anchor selections.  The automated item-selecting component 604 can then choose one or more of the additional media items to present to the group.  In a variant of this approach, the profile\nupdating component 602 can identify the additional media selections as part of an offline process.\n In still another approach, the automated item-selecting component 604 can randomly choose from among the media items that the group users have previously chosen on prior occasions.  The automated item-selecting component 604 can then replay\nthose media items.  Still other implementations of the automated item-selecting component 604 are possible.\n In one implementation, the search system 114 implements both the profile updating component 602 and the automated item-selecting component 604.  In another implementation, the media BOT 112 implements at least the profiling updating component\n602, and the search system 114 implements the automated item-selection component 604.  In that latter case, the media BOT 112 passes profile information to the search system 114, which allows the search system 114 to perform its automated selection based\non the profile information.\n Each group of users can add one or more rules that determine when the automated item-selecting component 604 will automatically select media items.  In one case, a group of users may provide a first rule that specifies that the automated\nitem-selecting component 604 will automatically generate media items during a specified span of time (e.g., weekday afternoons).  A second rule can specify that any media item explicitly selected by a user (via the conversational interface component 108)\nwill override a media item selected by the automated item-selecting component 604; this means that the automated item-selecting component 604 will only select media items in the absence of explicit selections by the users.  Furthermore, the system 102\nwill allow any user to disable the automated item-selecting component 604 at any time.\n As a closing topic, the system 102 has been described above in the context of the control of the media playback equipment 106.  But the system 102 can also be applied to controlling other equipment in a shared local setting, such as lighting\nequipment, home security equipment, etc.\n Consider the application of the system 102 to the control of a home security system.  As a preliminary operation, some individual or community of authorized individuals can prepare a curated corpus of searchable items pertaining to the security\ndomain, if not already created.  Each such document parallels the format of a media item in the curated media-specific corpus 116.  For instance, instead of providing metadata which describes a song, a security-related document can provide metadata that\ndescribes a particular security situation.  One such security situation can include the parameter values {garage door open, no occupants present in home}.  And instead of providing an identifier that describes a particular song, the security document can\nprovide an identifier associated with a particular action to be taken.  For example, in the above security situation, the specified action might entail generating a security alarm.  In essence, therefore, each such created document describes a rule that\ngoverns when a particular security-related action will be performed.\n To ensure uniformity in the documents created by individuals, the community of authorized individuals can establish and promulgate rules that describe the preferred format of the documents.  The community can also use an automated and/or\nsemi-automated enforcement mechanism to monitor new documents that are supplied to the corpus to ensure that they conform to the preferred format.  The enforcement mechanism operates by comparing each submitted document against a template that describes\nthe preferred format.  The enforcement mechanism can notify an individual who attempts to submit a non-conforming document, and invite the individual to resubmit the document in the correct format.  In this implementation, the corpus constitutes a\ncurated marketplace of rules.\n In a real-time application phase, at least one user can enter a message into a message interface component.  A security BOT can determine whether the message contains a triggering feature which indicates that the user has made a security control\ncommand, as when the user's message reads, \"Tom's garage door is open and he is away on vacation.  Assess security.\" Here, the term \"assess security\" might constitute a key term by which the security BOT concludes that the user is asking the system 102\nto identify an appropriate security action to be performed, and then perform the security action.  In other words, the user's message can be said to include a selection control command, similar to the \"play\" command in the above-described media-related\ncontext.\n The security BOT may then forward the user's message to the search system 114.  The search system 114 can then perform a domain-specific search over the corpus of security documents, to identify a document which most closely matches the input\nquery.  The security BOT will receive a query response identified by the search system 114.  That query response will contain an ID that describes a preferred action to take, if any.  The BOT will then generate security control information based on the\nquery response.  The security system can consume this security control information and take appropriate action, e.g., by automatically closing the garage door, or by sending an alarm to Tom to notify him of a potential threat.\n In another scenario, the security BOT can determine that the user has made a non-selection command when the user enters a message with the intent of directly controlling the security system in a prescribed manner.  For example, the security BOT\ncan determine that the user has made a non-selection command when he inputs the message, \"Lock the front door.\"\n In another variation, the system 102 can use the above-described conversational interface component 108 in conjunction with the BOT 112 to control any type of local equipment (e.g., lighting equipment), but without interacting with the search\nsystem 114.  In other words, the BOT 112 can process non-selection control commands, but not selection control commands that involve interaction with the search system 114.  (Note that the BOT 112 is referred to as simply as a \"BOT\" in this\nimplementation, rather than a \"media BOT,\" because it need not serve a media-related purpose.)\n More specifically, in the above variation, the conversational interface component 108 uses the same type of collaborative UI presentation 110 described above to receive a message from a particular user within the group of users 104.  That\nmessage can include a control command to control the local equipment, as when the user writes, \"Turn lights down to dim level.\" By writing this message, the particular user is also inherently communicating with the other users in the group, who also have\nsimultaneous access to the collaborative UI presentation 110, and can see the particular user's message.  In another case, the particular user can more directly interact with the other users while controlling the local equipment, as when the user writes,\n\"I am turning the lights down to dim, unless anyone objects.\" In still other cases, the particular user may provide a user-to-user message that is solely directed to other users, and does not include any control command, as when the user writes, \"Can\nanybody see what they are doing at this light level?\" In one implementation, the BOT 112 can determine that the user has made a control command when the message includes one or more prescribed key terms.\n The BOT 112 outputs control information to the command buffering component 120 when it detects that the user's message includes a control command.  The BOT 112 performs this action without interacting with the search system 114.  The BOT 112\ntakes no action on the user's message when the BOT 112 determines that it includes no control command.\nB. Illustrative Processes\n FIGS. 7-9 show processes that explain the operation of the system 102 of Section A in flowchart form.  Since the principles underlying the operation of the system 102 have already been described in Section A, certain operations will be addressed\nin summary fashion in this section.  As noted in the prefatory part of the Detailed Description, each flowchart is expressed as a series of operations performed in a particular order.  But the order of these operations is merely representative, and can\nbe varied in any manner.\n FIG. 7 shows a process 702, implemented by one or more computing devices, for controlling any type of equipment, but the process 702 is described below in the context of controlling the media playback equipment 106.  The process 702 is also\ndescribed from the perspective of the media BOT 112.  In block 704, the media BOT 112 receives a message provided by a particular user, from a message interface component with which the particular user interacts.  For instance, the message interface\ncomponent may correspond to the conversational interface component 108 shown in FIG. 1.  In block 706, the media BOT 112 determines whether the message includes: a selection control command, in which the particular user specifies a target media item to\nbe played by the playback equipment; and/or a non-selection control command, corresponding to any media control command other than a selection control command.  First assume that the message includes a selection control comment.\n In block 708, the media BOT 112 sends the message as an input query to a search system 114, together with information that instructs the search system 114 to perform a domain-specific search.  In block 710, the media BOT 112 receives a query\nresponse from the search system 114, containing a top-ranked media item, if any.  In block 712, the media BOT 112 outputs selection control information that describes the top-ranked media item to the command buffering component 120, for use in\ncontrolling the media playback equipment 106.\n Alternatively, assume that the message provided by the particular user is determined to contain a non-selection control command.  If so, in block 714, the media BOT 112 outputs non-selection control information to the command buffering component\n120 that describes the non-selection control command, without formulating an input query for processing by the search system 114.\n Alternatively, assume that the message provided by the particular user contains neither a selection control command nor a non-selection control command.  If so, in block 716, the media BOT 112 provides no control information to the command\nbuffering component 120.\n In yet other cases, the process 702 is configured to process just the non-selection control commands (and user-to-user messages that do not include control commands), but not selection control commands.  In that case, the process 702 omits\ninteraction with the search system 114 associated with blocks 708 and 710.\n FIG. 8 shows a process 802 performed by the search system 114, upon receiving an input query from the media BOT 112, together with an instruction to perform a domain-specific search.  In block 804, the search system 114 performs preprocessing on\nthe input query to provide a preprocessed input query.  In block 806, the search system 114 identifies a set of media items, if any, that match the preprocessed input query, selected from a curated media-specific corpus 116 of media items.  The curated\nmedia-specific corpus 116 is associated with a particular network-accessible domain.  In block 808, the search system 114 ranks the set of media items to provide a query response.  The media response identifies a top-ranked media item, if any, in the set\nof media items.\n FIG. 9 shows a process 902 performed by the functionality shown in FIG. 6.  In block 904, the profile updating component 602 maintains a group profile for the group of users, the group profile describing media selections made by the group of\nusers over a span of time.  In block 906, the automated item-selecting component 604 automatically selects a new item based on the group profile, without any user in the group of users explicitly specifying the new item.\nC. Representative Computing Functionality\n FIG. 10 shows computing functionality 1002 that can be used to implement any aspect of the mechanisms set forth in the above-described figures.  For instance, the type of computing functionality 1002 shown in FIG. 10 can be used to implement any\nlocal computing device shown in FIG. 2, and/or any remote server computing device provided by a remote computing system shown in FIG. 2, and/or any control mechanism provided by the local system 124 of FIG. 1.  In all cases, the computing functionality\n1002 represents one or more physical and tangible processing mechanisms.\n The computing functionality 1002 can include one or more hardware processor devices 1004, such as one or more central processing units (CPUs), and/or one or more graphical processing units (GPUs), and so on.  The computing functionality 1002 can\nalso include any storage resources (also referred to as computer-readable storage media or computer-readable storage medium devices) 1006 for storing any kind of information, such as machine-readable instructions, settings, data, etc. Without limitation,\nfor instance, the storage resources 1006 may include any of RAM of any type(s), ROM of any type(s), flash devices, hard disks, optical disks, and so on.  More generally, any storage resource can use any technology for storing information.  Further, any\nstorage resource may provide volatile or non-volatile retention of information.  Further, any storage resource may represent a fixed or removable component of the computing functionality 1002.  The computing functionality 1002 may perform any of the\nfunctions described above when the hardware processor device(s) 1004 carry out computer-readable instructions stored in any storage resource or combination of storage resources.  For instance, the computing functionality 1002 may carry out\ncomputer-readable instructions to perform each block of the processes described in Section B. The computing functionality 1002 also includes one or more drive mechanisms 1008 for interacting with any storage resource, such as a hard disk drive mechanism,\nan optical disk drive mechanism, and so on.\n The computing functionality 1002 also includes an input/output component 1010 for receiving various inputs (via input devices 1012), and for providing various outputs (via output devices 1014).  Illustrative input devices include a keyboard\ndevice, a mouse input device, a touchscreen input device, a digitizing pad, one or more video cameras, one or more depth cameras, a free space gesture recognition mechanism, one or more microphones, a voice recognition mechanism, any movement detection\nmechanisms (e.g., accelerometers, gyroscopes, etc.), and so on.  One particular output mechanism may include a display device 1016 and an associated graphical user interface presentation (GUI) 1018.  The conversational interface component 108 can present\nthe collaborative UI presentation 110 shown in FIG. 3 on the display device 1016.  The display device 1016 may correspond to a charge-coupled display device, a cathode ray tube device, a projection mechanism, etc. The computing functionality 1002 can\nalso include one or more network interfaces 1020 for exchanging data with other devices via one or more communication conduits 1022.  One or more communication buses 1024 communicatively couple the above-described components together.\n The communication conduit(s) 1022 can be implemented in any manner, e.g., by a local area computer network, a wide area computer network (e.g., the Internet), point-to-point connections, etc., or any combination thereof.  The communication\nconduit(s) 1022 can include any combination of hardwired links, wireless links, routers, gateway functionality, name servers, etc., governed by any protocol or combination of protocols.\n Alternatively, or in addition, any of the functions described in the preceding sections can be performed, at least in part, by one or more hardware logic components.  For example, without limitation, the computing functionality 1002 (and its\nhardware processor) can be implemented using one or more of: Field-programmable Gate Arrays (FPGAs); Application-specific Integrated Circuits (ASICs); Application-specific Standard Products (ASSPs); System-on-a-chip systems (SOCs); Complex Programmable\nLogic Devices (CPLDs), etc. In this case, the machine-executable instructions are embodied in the hardware logic itself.\n The following summary provides a non-exhaustive list of illustrative aspects of the technology set forth herein.\n According to a first aspect, a system environment, including one or more computing devices, is described for controlling media playback equipment.  The system environment includes a message interface component configured to receive a message\nfrom a particular user via a user interface presentation provided by the message interface component.  The system environment also includes a media BOT configured to: receive the message; determine whether the message includes a selection control\ncommand, in which the particular user specifies a target media item to be played by the media playback equipment; and when the message is determined to contain the selection control command, send the message as an input query to a search system, with\ninstruction that the search system is to perform a domain-specific search.  The search system subsequently processes the input query by: performing preprocessing on the input query to provide a preprocessed input query; identifying a set of media items,\nif any, that match the preprocessed input query, selected from a curated media-specific corpus of media items; and ranking the set of media items to provide a query response, the query response identifying a top-ranked media item, if any, in the set of\nmedia items.  The media BOT is also configured to receive the query response from the search system, containing the top-ranked media item, if any; and output selection control information that describes the top-ranked media item.  The system environment\nalso includes a command buffering component configured to store the selection control information in a data store.  The system environment also includes a local system configured to: receive the selection control information from the command buffering\ncommand component; and control the media playback equipment based on the selection control information, by playing the top-ranked media item on the media playback equipment.\n According to a second aspect, the message interface component is a conversational interface component that is configured to provide one or more instances of a collaborative user interface presentation for use by respective users in a group of\nusers, including the particular user.  Each instance of the collaborative user interface presentation reveals messages entered by the users in the group.\n According to a third aspect, a set of messages entered by the users of the group includes: a first subset of user-to-BOT messages that includes commands directed to the media BOT, and a second subset of user-to-user messages that do not include\ncommands directed to the media BOT.  The media BOT is configured to discriminate between the first subset of messages and the second subset of messages.\n According to a fourth aspect, the media BOT is configured to detect whether a particular message is a user-to-BOT message by determining whether the particular message includes a triggering feature associated with a command.\n According to a fifth aspect, the triggering feature corresponds to at least one triggering keyword.\n According to a sixth aspect, the system environment further includes a profile updating component configured to maintain a group profile for the group of users, the group profile describing media selections made by the group of users over a span\nof time.  Each entry in the group profile includes: an identity of a prior media item that has been selected; and context information pertaining to a circumstance in which the prior media item has been selected.\n According to a seventh aspect, the system environment further includes an automated item-selecting component configured to automatically select a new media item based on the group profile, without any user in the group of users explicitly\nspecifying the new media item in a message.\n According to an eighth aspect, the media BOT is also configured to: determine whether the message includes a non-selection control command, corresponding to any media control command other than the selection control command; and when the message\nis determined to contain the non-selection control command, send non-selection control information that describes the non-selection control command to the command buffering component, without formulating an input query for processing by the search\nsystem.  The local system is also configured to: receive the non-selection control information from the command buffering command component; and control the media playback equipment based on the non-selection control information.\n According to a ninth aspect, the curated media-specific corpus is associated with a particular network-accessible domain.  Further, the search system is implemented using processing resources provided by a general-purpose search system, the\ngeneral-purpose search system being originally configured to perform a domain-agnostic search, in which the general-purpose search system identifies result items that match a user query without respect to respective domains associated with those result\nitems.\n According to a tenth aspect, the local system is configured to receive the selection control information by polling the command buffering component to determine whether it includes any instances of command information not yet received by the\nlocal system.\n According to an eleventh aspect, the selection control information includes an identifier that identifies the top-ranked media item.  Further, the local system is configured to play the top-ranked media item by retrieving the top-ranked media\nitem from the curated media-specific corpus.\n According to a twelfth aspect, one or more computing devices are described that implement a BOT for use in controlling equipment in a local setting.  The computing device(s) include a message-receiving component configured to receive a message\nprovided by a particular user, from a message interface component with which the particular user interacts.  The computing device(s) also include a command extraction component configured to: determine whether the message provided by the particular user\nincludes a selection control command, in which the particular user specifies a target item; and when the message is determined to contain the selection control command, send the message as an input query to a search system, with instruction that the\nsearch system is to perform a domain-specific search.  The search system generates a query response in response to the input query by searching a curated domain-specific corpus of items, the query response identifying a top-ranked item, if any.  The\ncomputing device(s) also includes a result processing component configured to: receive the query response from the search system, containing the top-ranked item, if any; and provide selection control information that describes the top-ranked item to a\nlocal control system for use in controlling the equipment.\n According to a thirteenth aspect, the message interface component (associated with the twelfth aspect) is a conversational interface component that is configured to provide one or more instances of a collaborative user interface presentation for\nuse by respective users in a group of users, including the particular user.  Each instance of the collaborative user interface presentation reveals messages entered by the users in the group.\n According to a fourteenth aspect, the command extraction component is also configured to: determine whether the message includes a non-selection control command, corresponding to any control command other than the selection control command; and\nwhen the message is determined to contain the non-selection control command, send non-selection control information that describes the non-selection control command to the local system, without formulating an input query for processing by the search\nsystem.\n According to a fifteenth aspect, the curated domain-specific corpus (associated with the twelfth aspect) is associated with a particular network-accessible domain.  Further, the search system is implemented using processing resources provided by\na general-purpose search system, the general-purpose search system being originally configured to perform a domain-agnostic search, in which the general-purpose search system identifies result items that match a user query without respect to respective\ndomains associated with those result items.\n According to a sixteenth aspect, a method is described, implemented by one or more computing devices, for controlling equipment in a local setting.  The method includes receiving a message provided by a particular user, from a message interface\ncomponent with which the particular user interacts.  The message interface component corresponds to a conversational interface component that is configured to provide one or more instances of a collaborative user interface presentation for use by\nrespective users in a group of users, including the particular user.  Each instance of the collaborative user interface presentation reveals messages entered by the users in the group.  The method further includes determining whether the message provided\nby the particular user: includes a control command directed to the equipment in the local setting; or is a user-to-user message that does not contain a control command.  The method further includes, when the message is determined to contain a control\ncommand, outputting control information that describes the control command for use in controlling the equipment in the local setting.\n According to a seventeenth aspect, the equipment in the local setting is media playback equipment for playing media items.  Further, the above-referenced determining of whether the message includes a control command includes determining whether\nthe message contains: a selection control command, in which the particular user specifies a target media item to be played by the media playback equipment; and/or a non-selection control command, corresponding to any media control command other than a\nselection control command.  The method further includes when the message is determined to contain the selection control command, sending the message as an input query to a search system, with instruction that the search system is to perform a\ndomain-specific search.  The search system subsequently processes the input query by: performing preprocessing on the input query to provide a preprocessed input query; identifying a set of media items, if any, that match the preprocessed input query,\nselected from a curated media-specific corpus of media items; and ranking the set of media items to provide a query response, the media response identifying a top-ranked media item, if any, in the set of media items.  The method further includes\nreceiving the query response from the search system, containing the top-ranked media item, if any.  The above-referenced outputting corresponds to outputting selection control information that describes the top-ranked media item, for use in controlling\nthe media playback equipment.\n According to an eighteenth aspect, the method further includes: maintaining a group profile for the group of users, the group profile describing media selections made by the group of users over a span of time; and automatically selecting a new\nmedia item based on the group profile, without any user in the group of users explicitly specifying the new media item.\n According to a nineteenth aspect, when the message is determined to contain the non-selection control command, the above-referenced outputting corresponds to outputting non-selection control information that describes the non-selection control\ncommand, without formulating an input query for processing by the search system.\n According to a twentieth aspect, the curated media-specific corpus (associated with the sixteenth aspect) is associated with a particular network-accessible domain.  The search system is implemented using processing resources provided by a\ngeneral-purpose search system, the general-purpose search system being configured to perform a domain-agnostic search, in which the general-purpose search system identifies result items that match a user query without respect to respective domains\nassociated with those result items.\n A twenty-first aspect corresponds to any combination (e.g., any permutation or subset that is not logically inconsistent) of the above-referenced first through twentieth aspects.\n A twenty-second aspect corresponds to any method counterpart, device counterpart, system counterpart, means-plus-function counterpart, computer-readable storage medium counterpart, data structure counterpart, article of manufacture counterpart,\ngraphical user interface presentation counterpart, etc. associated with the first through twenty-first aspects.\n In closing, the functionality described herein can employ various mechanisms to ensure that any user data is handled in a manner that conforms to applicable laws, social norms, and the expectations and preferences of individual users.  For\nexample, the functionality can allow a user to expressly opt in to (and then expressly opt out of) the provisions of the functionality.  The functionality can also provide suitable security mechanisms to ensure the privacy of the user data (such as\ndata-sanitizing mechanisms, encryption mechanisms, password-protection mechanisms, etc.).\n Although the subject matter has been described in language specific to structural features and/or methodological acts, it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific\nfeatures or acts described above.  Rather, the specific features and acts described above are disclosed as example forms of implementing the claims.", "application_number": "15910593", "abstract": " A technique is described herein for controlling equipment in a local\n     environment. The technique can use a conversational interface component\n     to receive messages from any user in a group of users. In a media-related\n     context, a media BOT receives each message and determines whether it\n     contains a command to play a particular media item. If so, the media BOT\n     formulates the message as an input query and sends the input query to a\n     search system, with instruction to perform a domain-specific search. The\n     search system identifies a top-ranked media item based on the input\n     query, selected from among a curated media-specific corpus. By virtue of\n     the use of the scoped search system, the technique can successfully\n     leverage preexisting linguistic processing mechanisms. The technique can\n     also generate group profiles associated with users who interact with the\n     conversational interface component, and then use the group profiles to\n     automatically select media items.\n", "citations": ["5696500", "6505160", "8306976", "8704073", "8745742", "9031216", "9043407", "9916127", "20020101854", "20070043878", "20070050191", "20070073651", "20070203875", "20070214182", "20070244903", "20080019516", "20080040759", "20080140796", "20080147215", "20090298474", "20100223223", "20110145581", "20120089626", "20130111526", "20130191122", "20130332262", "20140259032", "20140280288", "20140359027", "20150020011", "20150081361", "20150188854", "20150200880", "20150269200", "20160044380", "20160094506", "20160094507", "20160125074", "20160127262", "20160212090", "20170124092", "20170300527", "20180025004", "20180063057", "20180089315"], "related": ["15278003"]}, {"id": "20180247639", "patent_code": "10373610", "patent_name": "Systems and methods for automatic unit selection and target decomposition\n     for sequence labelling", "year": "2019", "inventor_and_country_data": " Inventors: \nLiu; Hairong (San Jose, CA), Zhu; Zhenyao (Sunnyvale, CA), Satheesh; Sanjeev (Sunnyvale, CA)  ", "description": "BACKGROUND\nA. Technical Field\n The present disclosure relates generally to systems and methods for computer learning that can provide improved computer performance, features, and uses.\nB. Background\n Computing devices play an ever increasingly complex and integral role in people's lives.  Interestingly, even though computing devices perform substantially more complex and varied tasks, the skill level needed for lay people to use such\ncomputing devices tends to become simpler--thereby making them more accessible to a general population.  To achieve computing devices that provide complex services but do so in an accessible manner requires the computing devices to become more\nintelligent.  Increasing the ability of a computing device to learn, allows it to provide more robust services and provides easier, more intuitive ways for people to interface with the computing device.\n One task that computing devices have been configured to learn is sequence prediction tasks.  Most existing sequence labelling models rely on a fixed decomposition of a target sequence into a sequence of basic units.  These methods suffer from at\nleast two major drawbacks: 1) the set of basic units is fixed, such as the set of words, characters, or phonemes in speech recognition; and 2) the decomposition of target sequences is fixed.  These drawbacks usually result in sub-optimal performance of\nmodeling sequences.\n Accordingly, what is needed are systems and methods that allow for automatic unit selection and target decomposition for sequence labelling, which improves the functioning and use of computing devices.  Such systems and methods may be used to\nprovide or improve computing device services, such as, by way of example, automatic speech recognition (ASR), handwriting recognition, machine translation, and image captioning, which in turn helps improve performance of, use of, and interfacing with\ncomputing devices. BRIEF DESCRIPTION OF THE DRAWINGS\n References will be made to embodiments of the invention, examples of which may be illustrated in the accompanying figures.  These figures are intended to be illustrative, not limiting.  Although the invention is generally described in the\ncontext of these embodiments, it should be understood that it is not intended to limit the scope of the invention to these particular embodiments.  Items in the figures are not to scale.\n FIG. 1 depicts an illustration of the states and the forward-backward transitions for the label `CAT` according to embodiments of the present disclosure.\n FIG. 2 depicts a method for iterative Gram selection according to embodiments of the present disclosure.\n FIG. 3(a) compares the training curves before (305) and after (310) auto-refinement of grams according to embodiments of the present patent document.\n FIG. 3(b) depicts the training curves of models with and without joint-training according to embodiments of the present patent document.\n FIG. 3(c) depicts a typical joint-training model architecture according to embodiments of the present disclosure.\n FIG. 4 depicts a model structure trained end-to-end with the CTC, Gram CTC, or a weighted combination of both, according to embodiments of the present disclosure.\n FIG. 5 depicts a method for training a deep learning model with Gram-CTC function according to embodiments of the present disclosure.\n FIG. 6 depicts max decoding results (without collapsing) by CTC and Gram-CTC embodiments on utterances from the Switchboard dataset, according to embodiments of the present disclosure.\n FIG. 7 depicts a simplified block diagram of a computing device/information handling system, in accordance with embodiments of the present document.\nDETAILED DESCRIPTION OF EMBODIMENTS\n In the following description, for purposes of explanation, specific details are set forth in order to provide an understanding of the invention.  It will be apparent, however, to one skilled in the art that the invention can be practiced without\nthese details.  Furthermore, one skilled in the art will recognize that embodiments of the present invention, described below, may be implemented in a variety of ways, such as a process, an apparatus, a system, a device, or a method on a tangible\ncomputer-readable medium.\n Components, or modules, shown in diagrams are illustrative of exemplary embodiments of the invention and are meant to avoid obscuring the invention.  It shall also be understood that throughout this discussion that components may be described as\nseparate functional units, which may comprise sub-units, but those skilled in the art will recognize that various components, or portions thereof, may be divided into separate components or may be integrated together, including integrated within a single\nsystem or component.  It should be noted that functions or operations discussed herein may be implemented as components.  Components may be implemented in software, hardware, or a combination thereof.\n Furthermore, connections between components or systems within the figures are not intended to be limited to direct connections.  Rather, data between these components may be modified, re-formatted, or otherwise changed by intermediary\ncomponents.  Also, additional or fewer connections may be used.  It shall also be noted that the terms \"coupled,\" \"connected,\" or \"communicatively coupled\" shall be understood to include direct connections, indirect connections through one or more\nintermediary devices, and wireless connections.\n Reference in the specification to \"one embodiment,\" \"preferred embodiment,\" \"an embodiment,\" or \"embodiments\" means that a particular feature, structure, characteristic, or function described in connection with the embodiment is included in at\nleast one embodiment of the invention and may be in more than one embodiment.  Also, the appearances of the above-noted phrases in various places in the specification are not necessarily all referring to the same embodiment or embodiments.\n The use of certain terms in various places in the specification is for illustration and should not be construed as limiting.  A service, function, or resource is not limited to a single service, function, or resource; usage of these terms may\nrefer to a grouping of related services, functions, or resources, which may be distributed or aggregated.\n The terms \"include,\" \"including,\" \"comprise,\" and \"comprising\" shall be understood to be open terms and any lists the follow are examples and not meant to be limited to the listed items.  Any headings used herein are for organizational purposes\nonly and shall not be used to limit the scope of the description or the claims.  Each reference mentioned in this patent document is incorporate by reference herein in its entirety.\n Furthermore, one skilled in the art shall recognize that: (1) certain steps may optionally be performed; (2) steps may not be limited to the specific order set forth herein; (3) certain steps may be performed in different orders; and (4) certain\nsteps may be done concurrently.\nA. Introduction\n In recent years, there has been an explosion of interest in sequence prediction tasks.  Connectionist Temporal Classification (CTC) loss and Sequence-to-sequence (seq2seq) models present powerful approaches to multiple applications, such as\nAutomatic Speech Recognition (ASR), machine translation and parsing.  These methods may be based on: 1) a fixed and carefully chosen set of basic units, such as words, phonemes or characters; and 2) a fixed and pre-determined decomposition of target\nsequences into these basic units.  While these two preconditions greatly simplify the problems, especially the training processes, they are also strict and unnecessary constraints, which usually lead to suboptimal solutions.  CTC models are especially\nharmed by fixed basic units in target space, because they build on the independence assumption between successive outputs in that space--an assumption which is often violated in practice.\n One of the problems with fixed set of basic units is that it is really hard, if not impossible, to determine the optimal set of basic units beforehand.  For example, in English ASR, if words are used as basic units, a large vocabulary-sized\nsoftmax must be dealt with, as well as rare words and data sparsity problem.  On the other hand, if characters are used as basic units, the model is forced to learn the complex rules of English spelling and pronunciation.  For example, the \"oh\" sound can\nbe spelled in any of following ways, depending on the word it occurs in: {o, oa, oe, ow, ough, eau, oo, ew}.  While CTC can easily model commonly co-occurring grams together, it is impossible to give roughly equal probability to many possible spellings\nwhen transcribing unseen words.  Some speech recognition systems model phonemes, sub-phoneme units and senones to get around these problems.  Similarly, some state-of-the-art neural machine translation systems use pre-segmented word pieces aiming to find\nthe best of both worlds.\n In reality, groups of characters are typically cohesive units for many tasks.  For the ASR task, words can be decomposed into groups of characters that can be associated with sound (such as `tion` and `eaux`).  For the machine translation task,\nthere may be values in decomposing words as root words and extensions (so that meaning may be shared explicitly between `paternal` and `paternity`).  Since this information is already available in the training data, it is perhaps better to let the model\nfigure it out by itself.  At the same time, it raises another important question: how to decompose a target sequence into basic units? This is coupled with the problem of automatic selection of basic units, thus also better to let the model determine. \nRecently, there are some interesting attempts in these directions in the seq2seq framework.  For example, Latent Sequence Decomposition has been proposed to decompose target sequences with variable length units as a function of both input sequence and\nthe output sequence.\n Presented herein are embodiments of a Gram-CTC--which may be considered as a strictly more general version of CTC--to automatically seek the best set of basic units from the training data, called grams, and automatically decompose target\nsequences into sequences of grams.  Just as sequence prediction with cross-entropy training can be seen as special case of the CTC loss with a fixed alignment, CTC can be seen as a special case of Gram-CTC with a fixed decomposition of target sequences. \nSince it is a loss function, it can be applied to many seq2seq tasks to enable automatic selection of grams and decomposition of target sequences without modifying the underlying networks.  Extensive experiments on multiple scales of data validate that\nembodiments of Gram-CTC can improve CTC in terms of both performance and efficiency, and that using Gram-CTC the models outperform state-of-the-arts on standard speech benchmarks.\nB. Related Work\n The basic text units that previous works utilized for text prediction tasks (e.g., automatic speech recognition, handwriting recognition, machine translation and image captioning) can be generally divided into two categories: hand-crafted ones\nand learning-based ones.\n 1.  Hand-Crafted Basic Units\n Fixed sets of characters (graphemes), word-pieces, words, and phonemes have been widely used as basic units for text prediction, but all of them have drawbacks.  Word-segmented models remove the component of learning to spell and thus enable\ndirect optimization towards reducing Word Error Rate (WER).  However, these models may suffer from having to handle a large vocabulary, out-of-vocabulary words and data sparsity problems.  Using characters results in much smaller vocabularies (e.g., 26\nfor English and thousands for Chinese), but it requires much longer contexts compared to using words or word-pieces and poses the challenge of composing characters to words, which is very noisy for languages like English.  Word-pieces lie at the\nmiddle-ground of words and characters, providing a good trade-off between vocabulary size and context size, while the performance of using word pieces is sensitive to the choice of the word-piece set and its decomposition.  For the ASR task, the use of\nphonemes was popular in the past few decades as it eases acoustic modeling and good results were reported with phonemic models.  However, it introduces the uncertainties of mapping phonemes to words during decoding, which becomes less robust especially\nfor accented speech data.\n Using these aforementioned fixed deterministic decompositions of text sequences defines a prior, which is not necessarily optimal for end-to-end learning.\n 2.  Learning-Based Basic Units\n More recently, attempts have been made to learn basic unit sets along with the selection and decomposition of them automatically.  For example, a multiscale RNN architecture has been leveraged to model the characters and words hierarchically, by\nbuilding word-level representations on top of character-level representations.  A hybrid Word-Character model was proposed to translate mostly at the word level and consults the character components for rare words.  Latent Sequence Decompositions\nframework has been proposed to decompose target sequences with variable length-ed basic units as a function of both input sequence and the output sequence.\nC. Embodiments of Gram-CTC\n In this section, embodiment of the proposed Gram-CTC algorithm are presented.\n 1.  CTC\n CTC is a very popular method in seq2seq learning since it does not require the alignment information between inputs and outputs, which is usually expensive, if not impossible, to obtain.\n Since there is no alignment information, CTC marginalizes over all possible alignments.  That is, it tries to maximize p(l|x)=.SIGMA..sub..pi.p(.pi.|x), where x is input, and .pi.  represent a valid alignment.  For example, if the size of input\nis 3, and the output is `hi`, whose length is 2, there are three possible alignments, `-hi`, `h-i` and `hi-`, where `-` represents blank.\n 2.  From CTC to Gram-CTC\n In CTC, the basic units are fixed, which may not be desirable in some applications.  In this disclosure, the CTC is generalized by considering a sequence of basic units, called gram, as a whole, which is usually more desirable in many\napplications.\n Let G be a set of n-grams of the set of basic units C of the target sequence, and .tau.  be the length of the longest gram in G. A Gram-CTC network has a softmax output layer with |G|+1 units, that is, the probability over all grams in G and one\nadditional symbol, blank.\n To simplify the problem, it may be assumed that CG.  This is because there may be no valid decompositions for some target sequences if CG.  Since Gram-CTC will figure out the ideal decomposition of target sequences into grams during training,\nthis condition is harmless, and guarantees that there is at least one valid decomposition for every target sequence.\n For an input sequence x of length T, let y=N.sub.w(x) be the sequence of network outputs, and denote by y.sub.k.sup.t as the probability of the k-th gram at time t, where k is the index of grams in G'=G.orgate.{blank}, then\n .function..pi..times..pi..A-inverted..pi..di-elect cons.'.times..times.  ##EQU00001##\n Just as in the case of CTC, here the elements of G'.sup.T may be referred to as paths, and denote them by .pi., which represents a possible alignment between input and output.  The difference is that for each word in the target sequence, it may\nbe decomposed into different sequences of grams.  For example, the word hello can only be decomposed into the sequence [`h`, `e`, `l`, `l`, `o`] for CTC (assume uni-gram CTC here), but it also can be decomposed into the sequence [`he`, `ll`, `o`] if `he`\nand `ll` are in G.\n In embodiments, for each .pi., it is mapped into a target sequence the same way as CTC using the collapsing function that 1) removes all repeated labels from the path and then 2) removes all blanks.  Note that essentially it is these rules which\ndetermine the transitions between the states of adjacent time steps in FIG. 1.\n FIG. 1 is an illustration of the states and the forward-backward transitions for the label `CAT` according to embodiments of the present disclosure.  Here G is the set of all uni-grams and bi-grams of the English alphabet.  The set of all valid\nstates S for the label l=`CAT` are listed to the left.  The set of states and transitions that are common to both vanilla (or plain) CTC and gram-CTC are in black, and those that are unique to Gram-CTC are in gray.  In general, any extension that\ncollapses back to 1 is a valid transition.  For example, (`CAT`, 1) can be transited from (`CAT`, 1), (`CA`, 2), (`CA`, 1) and (`CA`, 0) but not from (VAT', 0) or (`CAT`, 2).\n This is a many-to-one mapping and it is denoted by B. Other rules may be adopted here and the general idea presented in this disclosure does not depend on these specific rules.  Thus, for a target sequence l, B.sup.-1(l) represents all paths\nmapped to l. Then, this yields\n .function..pi..di-elect cons..function..times..function..pi.  ##EQU00002##\n This equation allows training sequence labeling models without any alignment information using CTC loss, because it marginalizes over all possible alignments during training.  Gram-CTC uses the same effect to marginalize over not only\nalignments, but also a decomposition of the target sequence.\n In embodiments, for each target sequence l, the set B.sup.-1(l) has O(.tau..sup.2) more paths than it does in vanilla CTC.  This is because there are O(.tau.) times more valid states per time step, and each state may have a valid transition from\nO(.tau.) states in the previous time step.  Thus, the original CTC method may be considered a special case of Gram-CTC when G=C and .tau.=1.  While the quadratic increase in the complexity of the algorithm is nontrivial, it may be asserted that it is a\ntrivial increase in the overall training time of typical neural networks, where the computation time is dominated by the neural networks themselves.  Additionally, the algorithm extends generally to any arbitrary G and need not have all possible n-grams\nup to length T.\n 3.  Embodiments of the Forward-Backward Method\n To efficiently compute p(l|x), in an embodiment, a dynamic programming method is also adopted.  The essence of all dynamic programming is in identifying the states of the problem, so that future states may be solved by reusing solutions to\nearlier states.  In embodiments, the state contains all the information required to identify all valid extensions of an incomplete path .pi.  such that the collapsing function will eventually collapse the complete .pi.  back to l. For embodiments of\nGram-CTC, this may be done by collapsing all but the last element of the path .pi..  Therefore, the state is a tuple (l.sub.1:i, j), where the first item is a collapsed path, representing a prefix of the target label sequence, and j.di-elect cons.{0, . \n. . , .tau.} is the length of the last gram (l.sub.i-j+1:i) used for making the prefix.  Note that j=0 is valid and means that blank was used.  The gram l.sub.i-j+1:i is denoted by g.sub.i.sup.j(l) and the state (l.sub.1:i, j) as s.sub.i.sup.j(l).  For\nreadability, s.sub.i.sup.j(l) is further shortened to s.sub.i.sup.j and g.sub.i.sup.j(l) to g.sub.i.sup.j.  For a state s, its corresponding gram, is denoted by s.sub.g, and the positions of the first character and last character of s.sub.g are denoted\nby b(s) and e(s), respectively.  During dynamic programming, we are dealing with sequence of states, for a state sequence .zeta., its corresponding gram sequences is unique, denoted by .zeta..sub.g.\n FIG. 1 illustrates partially the dynamic programming process for the target sequence `CAT`.  Here, suppose G contains all possible uni-grams and bi-grams.  Thus, for each character in `CAT`, there are three possible states associated with it: 1)\nthe current character, 2) the bi-gram ending in current character, and 3) the blank after current character.  There is also one blank at the beginning.  In total, there are 10 states.\n Suppose the maximum length of grams in G is .tau., in embodiments, l is scanned to get the set S of all possible states, such that for all s.sub.i.sup.j.di-elect cons.S, its corresponding g.sub.i.sup.j.di-elect cons.G'.  Note that i.di-elect\ncons.{0, .  . . , |l|} and j.di-elect cons.{0, .  . . , .tau.}.\n For a target sequence l, define the forward variable .alpha..sub.t(s) for any s.di-elect cons.S to the total probability of all valid paths prefixes that end at state s at time t.\n .alpha..function..times..times..zeta..function..zeta..function..zeta..tim- es.'.times..zeta.'.times.' ##EQU00003##\n Following this definition, one can have the following rules for initialization\n .alpha..function..times..A-inverted..di-elect cons..times..tau.  ##EQU00004##\n and recursion\n .alpha..function..alpha..times..times..alpha..alpha..function..times..tim- es..times..times..times..times..noteq..alpha..alpha..function..alpha..func- tion..times..times..times..times..times..times.  ##EQU00005##\n where {circumflex over (.alpha.)}.sub.t.sup.i=.SIGMA..sub.j=0.sup..tau..alpha..sub.t(s.sub.i.sup- .j) and y.sub.b.sup.t is the probability of blank at time t.\n The total probability of the target sequence l is then expressed in the following way:\n .function..tau..times..alpha..function.  ##EQU00006##\n similarly, the backward variable .beta..sub.t(s) may be defined as:\n .beta..function..times..times..zeta..function..zeta..function..zeta..time- s.'.times..zeta.'.times.' ##EQU00007##\n For the initialization and recursion of .beta..sub.t(s), one can have:\n .beta..function..times..A-inverted..di-elect cons..times..tau..times..times..beta..function..beta..times..times..beta.- .beta..function..times..times..times..times..times..times..noteq..beta..be-\nta..function..beta..function..times..times..times..times..times..times.  ##EQU00008##\n Where {circumflex over (.beta.)}.sub.t.sup.i=.SIGMA..sub.j=0.sup..tau..beta..sub.t(s.sub.i+j.sup- .j).\n 4.  Embodiments of BackPropagation\n Similar to CTC, there is the following expression:\n .function..di-elect cons..times..alpha..function..times..beta..function..times..A-inverted..d- i-elect cons..times.  ##EQU00009##\n The derivative with regards to y.sub.k.sup.t may be expressed as:\n .differential..function..differential..times..di-elect cons..function..times..alpha..function..times..beta..function.  ##EQU00010##\n where lab(l, k) is the set of states in S whose corresponding gram is k. This is because there may be multiple states corresponding to the same gram.\n For the backpropagation, one of the most important formulas is the partial derivative of loss with regard to the unnormalized output u.sub.k.sup.t.\n .differential..times..times..function..differential..times..times..di-ele- ct cons..function..times..alpha..function..times..beta..function..times..t- imes..times..times..times..times..di-elect\ncons..times..alpha..function..times..beta..function.  ##EQU00011##\nD. Methodology Embodiments\n In this section, additional techniques are described that have been found to be useful in practice with embodiments to help the Gram-CTC to work efficiently as well as effectively.\n 1.  Iterative Gram Selection Embodiments\n Although Gram-CTC can automatically select useful grams, it is challenging to train with a large G. The total number of possible grams is usually huge.  For example, in English, there are 26 characters, then the total number of bi-grams is\n26.sup.2=676, the total number of tri-grams are 26.sup.3=17576, .  . . , which grows exponentially and quickly becomes intractable.  However, it is unnecessary to consider many grams, such as \"aaaa\", which are obviously useless.  In some embodiments,\nmost of useless grams were eliminated from the statistics of a huge corpus.  FIG. 2 depicts a method for iterative Gram selection according to embodiments of the present disclosure.  In step 205, a Gram-CTC model is first trained using all uni-grams and\nbi-grams (29 uni-grams and 26.sup.2=676 bi-grams, in total 705 grams).  In step 210, the trained model is decoded on another speech dataset to get the statistics of the usage of grams.  By applying (decoding) the trained model on a large speech dataset,\nthe real statistics of gram's usage are obtained.  In step 215, grams with usages above a threshold are selected.  Ultimately, the high frequency grams together with all uni-grams were chosen as a final gram set G. That is, the frequency of each gram in\nthe corpus was counted and these grams with rare frequencies were dropped.  For example, the Top 100 bi-grams together with all 29 uni-grams (auto-refined grams) were used for the second round of training.  Then, in step 220, the Gram-CTC model was\ntrained on all of the remaining grams.  Table 1 shows the impact of iterative gram selection on WSJ (without LM).\n TABLE-US-00001 TABLE 1 Results of different gram selection methods on the Wall Street Journal (WSJ) dataset Loss WER CTC, uni-gram 16.91 CTC, bi-gram 21.63 Gram-CTC, handpick 17.01 Gram-CTC, all uni-grams + bi-grams 16.89 Gram-CTC,\nauto-refinement 16.66\n FIG. 3(a) shows its corresponding training curve 300.  FIG. 3(a) compares the training curves before (305) and after (310) auto-refinement of grams according to embodiments of the present patent document.  They look very similar, although the\nnumber of grams is greatly reduced after refinement, which makes training faster and potentially more robust due to less gram sparsity.  Further details can be found in Section E.2.\n 2.  Embodiments of Joint Training with Vanilla CTC\n In embodiments, Gram-CTC solves both decomposition and alignment tasks, which is a harder task for a model to learn than CTC.  This is often manifested in unstable training curves, which can force the lowering of the learning rate, which in turn\nresults in models converging to a worse optima.  To overcome this difficulty, it may be beneficial to train a model with both the Gram-CTC, as well as the vanilla CTC loss.\n A typical joint-training model looks like FIG. 3(c), and the corresponding training curve is shown in FIG. 3(b) (340).  Specifically, FIG. 3(b) depicts the training curve of models with 350 and without 345 joint-training according to embodiments\nof the present patent document.  The model corresponding to training curve 350 is jointly trained together with vanilla CTC--such models are often more stable during training.  FIG. 3(c) depicts a typical joint-training model architecture 380 according\nto embodiments of the present disclosure.  Note that, in embodiments, the vanilla CTC loss 385 is typically best applied a few levels lower than the Gram-CTC loss 390.  Some of the effects of joint-training are shown in Table 4 and Table 5 in the\nexperiments section, below.\nE. Experiments\n It shall be noted that these experiments and results are provided by way of illustration and were performed under specific conditions using a specific embodiment or embodiments; accordingly, neither these experiments nor their results shall be\nused to limit the scope of the disclosure of the current patent document.\n Embodiments of the Gram-CTC loss were tested on the ASR task, while it should be noted that both CTC and the presently introduced Gram-CTC are generic techniques for other sequence labelling tasks.  For all of the experiments, the model\nspecification and training procedure were the same as an embodiment depicted in by Amodei et al., in Deep speech 2: End-to-end speech recognition in English and Mandarin, arXiv preprint arXiv:1512.02595, 2015.  FIG. 4 shows the exemplary model structure\nused for the training procedure.  The model 400 was a recurrent neural network (RNN) with 2 two-dimensional convolutional input layers 410, followed by K forward (Fwd) or bidirectional (Bidi) Gated Recurrent layers 415, N cells each, and one fully\nconnected layer before a softmax layer 420.  In short hand, such a model is written as `2.times.2D Conv--K.times.N GRU`.  The network was trained end-to-end with the CTC, Gram-CTC, or a weighted combination of both 525.  This combination is described in\nthe earlier section.\n FIG. 5 depicts a method for training the RNN model with Gram-CTC function according to embodiments of the present disclosure.\n In step 505, a sequence of utterances is inputted into the recurrent neural network (RNN) model, wherein the utterances and associated labels are sampled from a training set.  In all experiments, audio data was sampled at 16 kHz.  Linear Fast\nFourier transform (FFT) features were extracted with a hop size of 10 ms and window size of 20 ms, and were normalized so that each input feature has zero mean and unit variance.  The network inputs were thus spectral magnitude maps ranging from 0-8 kHz\nwith 161 features per 10 ms frame.  At each epoch, 40% of the utterances were randomly selected to add background noise to.  The optimization method used was a stochastic gradient descent with Nesterov momentum.  Learning hyperparameters (batch-size,\nlearning-rate, momentum, and etc.) varied across different datasets and were tuned for each model by optimizing a hold-out set.  Typical values were a learning rate of 10.sup.-3 and momentum of 0.99.\n 1.  Data and Setup\n Wall Street Journal (WSJ).\n This corpora consists primarily of read speech with texts drawn from a machine-readable corpus of Wall Street Journal news text, and contains about 80 hours speech data.  The standard configuration of train si284 dataset is used for training,\ndev93 for validation and eval92 for testing.  This is a relatively `clean` task and often used for model prototyping.\n Fisher-Switchboard.\n This is a commonly used English conversational telephone speech (CTS) corpora, which contains 2300 hours CTS data.  Following previous works, evaluation is carried out on the NIST 2000 CTS test set, which comprises both Switchboard (SWB) and\nCallHome (CH) subsets.\n 10K Speech Dataset.\n Large scale ASR experiments were conducted on a noisy internal dataset of 10,000 hours.  This dataset contains speech collected from various scenarios, such as different background noises, far-field, different accents, and so on.  Due to its\ninherent complexities, it is a very challenging task, and can thus validate the effectiveness of the proposed method for real-world application.\n 2.  Gram Selection\n In step 510, a set of grams is established for training the RNN model.  The WSJ dataset was employed for demonstrating different strategies of selecting grams for Gram-CTC, since it is a widely used dataset and also small enough for rapid idea\nverification.  However, because it is small, large grams could not be used here due to data sparsity problem.  Thus, the auto-refined gram set on WSJ is not optimal for other larger datasets, where larger grams could be effectively used, but the\nprocedure of refinement is the same for them.\n In an embodiment, a model is first trained using all uni-grams and bi-grams (29 uni-grams and 26.sup.2=676 bi-grams, in total 705 grams), and then decoded with the obtained model on another speech dataset to get the statistics of the usage of\ngrams.  The Top 100 bi-grams together with all 29 uni-grams (auto-refined grams) were used for the second round of training.  For comparison, the result of the best handpicked grams, as well as the results on uni-grams, are presented in Table 1.\n Some interesting observations can be found in Table 1.  First, the performance of auto-refined grams is slightly better than the combination of all uni-grams and all bi-grams.  This is likely because WSJ is so small that gram learning suffers\nfrom the data sparsity problem here (similar to word-segmented models).  Auto-refined grams contains only a small subset of bi-grams, thus are more robust.  This is also why, in embodiments, only bi-grams are tried, not including higher-order grams. \nSecond, the performance of best handpicked grams is worse than auto-refined grams.  This is desirable.  It is time-consuming to handpick grams, especially when you consider high-order grams.  The method of iterative gram selection is not only fast, but\nusually better.  Third, the performance of Gram-CTC on auto-refined grams is only slightly better than CTC on uni-grams.  This is because Gram-CTC is inherently difficult to train, since it needs to learn both decomposition and alignment.  WSJ is too\nsmall to provide enough data to train Gram-CTC.\n 3.  Sequence Labelling in Large Stride\n Using a large global stride in recurrent neural networks can greatly boost its overall efficiency, since it effectively reduces the time steps for recurrent computation, thus speeds up the process of both forward inference and backward\npropagation.  However, the largest stride that can be used is limited by the gram set that is used.  The (uni-gram) CTC typically has to work in a high time resolution (small stride) in order to have enough number of frames to output every character. \nThis is very inefficient as it is known that the same acoustic features could correspond to grams of different lengths (e.g., {`i`, `igh`, `eye`}).  The larger the grams are, the larger stride are potentially to be used.\n In step 515, striding is employed in processing the input utterance sequence, with the striding size depending on the largest gram of the gram set established in step 410.  In Deep Speech 2 (DS2), Amodei et al. employed non-overlapping bigram\noutputs in embodiments to allow for a larger stride in convolutions.  This imposes an artificial constraint forcing the model to learn, not only the spelling of each word, but also how to split words into bigrams.  For example, in embodiments, part is\nsplit as [pa, rt] but the word apart is forced to be decomposed as [ap, ar, t].  Gram-CTC removes this constraint by allowing the model to decompose words into larger units into the most convenient or sensible decomposition.  Comparison results show this\nchange enables Gram-CTC to work much better than bi-gram CTC, as in Table 2.\n TABLE-US-00002 TABLE 2 Performances with different model strides Loss WER Epoch Time (mins) (stride) 2 4 2 4 CTC, uni-gram 16.91 23.76 29 16 CTC, bi-gram 20.57 21.63 23 12 Gram-CTC 16.66 18.87 35 18\n In Table 2, the performance of trained model and training efficiency are compared on two strides, 2 and 4.  For Gram-CTC, the auto-refined gram set from previous section is used.  As expected, using stride 4 almost cuts the training time per\nepoch into half, compared to stride 2.  From stride 2 to stride 4, the performance of uni-gram CTC drops quickly.  This is because small grams inherently needs higher time resolutions.  As for Gram-CTC, from stride 2 to stride 4, its performance\ndecreases a little bit, though in the experiments using the other datasets, Gram-CTC is better in stride 4.  One possible explanation is that WSJ is too small for Gram-CTC to learn large grams well.  In contrast, the performance of bi-gram CTC is not as\ngood in either stride.\n 4.  Decoding Examples\n In step 520, the RNN model outputs one or more predicted characters for each utterance of the input utterance sequence.  The one or more predicted characters are selected from the established set of grams.  In step 525, a Connectionist Temporal\nClassification (CTC) loss function is implemented to obtain a loss of the predicted one or more characters for each utterance given the associated labels.  The CTC loss function implementation involving forward-backward transitions, as described in FIG.\n1, section C.1., and C.2.  The obtained loss is then used to calculate a derivative of the loss with respect to parameters of the RNN model and to update the RNN model through back-propagation, which is described in Section C.3.\n FIG. 6 illustrates the decoding results 600 of both CTC and Gram-CTC embodiments on nine utterances according to embodiments of the present patent document.  The predicted characters (by CTC) or grams (by Gram-CTC) at each timestep are separated\nby \"|\".  As the Gram-CTC model is trained with doubled stride as that of CTC model, the grams are placed at a doubled width as characters for better viewing.  The \"_\" represents blank.\n In the depicted embodiments, the label set for CTC is the set of all characters, and the label set for Gram-CTC is an auto-refined gram set containing all uni-grams and some high frequent high-order grams.  In the depicted embodiments, Gram-CTC\nuses stride 4 while CTC uses stride 2.  From FIG. 6, it can be found that:\n 1) Gram-CTC does automatically find many intuitive and meaningful grams, such as `the`, `ng`, and `are`.\n 2) It also decomposes the sentences into segments which are meaningful in terms of pronunciation.  This decomposition resembles the phonetic decomposition, but in larger granularity and arguably more natural.\n 3) Since Gram-CTC predicts a chunk of characters (a gram) each time, each prediction utilizes larger context and these characters in the same predicted chunk are dependent, thus potentially more accurate.  One example is the word `will` in the\nlast sentence in FIG. 6.\n 4) Since the output of network is the probability over all grams, the decoding process is almost the same as CTC, still end-to-end.  This makes such decomposition superior to phonetic decomposition.\n In summary, Gram-CTC combines the advantages of both CTC on characters and CTC on phonemes.\n 5.  Comparison with Other Methods\n a) WSJ Dataset\n The model used here is [2.times.2D cony, 3.times.1280 Bidi GRU] with a CTC or Gram-CTC loss.  The results are shown in Table 3.\n TABLE-US-00003 TABLE 3 Comparison with previous published results with end-to-end training on WSJ speech dataset Architecture WER Phoneme CTC + trigram LM [1] 7.3 Grapheme CTC + trigram LM [1] 9.0 Attention + trigram LM [2] 9.3 DeepConv LAS + no\nLM [3] 10.5 DeepConv LAS + LSD + no LM [4] 9.6 Temporal LS + Cov + LM [5] 6.7 Vanilla CTC + no LM (by present disclosure) 16.91 Vanilla CTC + LM (by present disclosure) 7.11 Gram-CTC + no LM (by present disclosure) 16.66 Gram-CTC + LM (by present\ndisclosure) 6.75 Where: [1] = Yajie Miao, Mohammed Gowayyed, and Florian Metze.  \"Eesen: End-to-end speech recognition using deep rnn models and wfst-based decoding\", which is available at Automatic Speech Recognition and Understanding (ASRU), 2015 IEE\nWorkshop on, pages 167-174.  IEEE, 2015, and which is incorporated by reference herein in its entirety; [2] = Dzmitry Bahdanau, Jan Chorowski, Dmitriy Serdyuk, Yoshua Bengio, et al. \"End-to-end attention-based large vocabulary speech recognition\", which\nis available at In 2016 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), pages 4945-4949, IEEE, 2016, and which is incorporated by reference herein in its entirety; [3] = Yu Zhang, William Chan, and Navdeep Jaitly. \n\"Very deep convolutional networks for end-to-end speech recognition\", which is available at arXiv preprint arXiv:1610.03022, 2016, and which is incorporated by reference herein in its entirety; [4] = William Chan, Yu Zhang, Quoc Le, and Navdeep Jaitly. \n\"Latent sequence decompositions\", which is available at Arxiv, 2016b, and which is incorporated by reference herein in its entirety; [5] = Jan Chorowski and Jaitly Navdeep.  \"Towards better decoding and language model integration in sequence to sequence\nmodels\", which is available at. arXiv preprint arXiv:16212.02695, 2016.  and which is incorporated by reference herein in its entirety.\n For all models trained, language models can greatly improve their performances, in term of WER.  Though this dataset contains very limited text data for learning gram selection and decomposition, the Gram-CTC can still improve the vanilla CTC\nnotably.\n b) Fisher-Switchboard\n The acoustic model trained here is composed of two 2D convolutions and six bidirectional GRU layer in 2048 dimension.  The corresponding labels are used for training N-gram language models.  Switchboard English speech 97S62 Fisher English speech\nPart 1-2004S13, 2004T19 Fisher English speech Part 2-2005S13, 2005T19\n A sample of the Switchboard-1 portion of the NIST 2002 dataset (2004S11 RT-02) was used for tuning language model hyper-parameters.  The evaluation was done on the NIST 2000 set.  Together, this configuration forms a standard benchmark for\nevaluating ASR models.  The results are shown in Table 4.\n TABLE-US-00004 TABLE 4 Comparison with the best published results on Fisher-Switchboard benchmark benchmark (\"SWBD\" and \"CH\" represent Switchboard and Callhome portions, respectively) using in-domain data (only results using single models are\nlisted here).  SWBD CH Architecture WER WER Iterated-CTC [1] 11.3 18.7 BLSTM + LF MMI [2] 8.5 15.3 LACE + LF MMI [3] 8.3 14.8 Dilated convolutions [4] 7.7 14.5 Vanilla CTC (by present disclosure) 9.0 17.7 Gram-CTC (by present disclosure) 7.9 15.8 Vanilla\nCTC + Gram-CTC (by present disclosure) 7.3 14.7 Where: [1] = Geoffery Zweig, Ghengzhu Yu, Jasha Droppo, and Andreas Stolcke.  \"Advances in all-neural speech recognition\", which is available at arXiv preprint arXiv: 1609.05935, 2016 and which is\nincorporated by reference herein in its entirety; [2] = Daniel Povey, Vijayaditya Peddinti, Daniel Galvez, Pegah Ghahrmani, Vimal Manohar, Xingyu Na, Yiming Wang, and Sanjeev Khudanpur.  \"Purely sequence-trained neural networks for asr based on\nlattice-free mmi\", which was submitted to Interspeech, 2016 and which is incorporated by reference herein in its entirety; [3] = Wayne Xiong, Jasha Droppo, Xuedong Huang, Frank Seide, Mike Seltzer, Andreas Stolcke, Dong Yu, and Geoffrey Zweig. \n\"Achieving human parity in conversational speech recognition\", which is available at arXiv preprint arXiv: 1610.05256, 2016 and which is incorporated by reference herein in its entirety; [4] = Tom Sercu and Vaibhava Goel.  \"Dense prediction on sequences\nwith time-dilated convolutions for speech recognition\", which is available at arXiv preprint arXiv: 1611.09288, 2016 and which is incorporated by reference herein in its entirety.\n An embodiment of the model was compared against the best published results on in-domain data.  These results can often be improved using out-of-domain data for training the language model, and sometimes the acoustic model as well.  Together\nthese techniques allow (Xiong et al., 2016b) to reach a WER of 5.9 on the SWBD set.\n c) 10K Speech Dataset\n Finally, a large noisy dataset collected for building large-vocabulary Continuous Speech Recognition (LVCSR) systems was experimented.  This dataset contains about 10000 hours speech in a diversity of scenarios, such as far-field, background\nnoises, accents.  In all cases, the embodiment of the model was [2.times.2D Cony, 3.times.2560 Fwd GRU, LA Cony] with only a change in the loss function.  `LA Cony` refers to a look ahead convolution layer as seen in (Amodei et al., 2015), which works\ntogether with forward-only RNNs for deployment purpose.\n The same as the Fisher-Switchboard dataset, the optimal stride is 4 for Gram-CTC and 2 for vanilla CTC on this dataset.  Thus, in both experiments, both Gram-CTC and vanilla CTC+Gram-CTC were trained much faster than the vanilla CTC itself.  The\nresult are shown in Table 5.\n TABLE-US-00005 TABLE 5 LVCSR results on 10K speech dataset.  Architecture WER (No LM) WER (With LM) Vanilla CTC 29.1 19.77 Gram-CTC 27.56 19.53 Vanilla CTC + Gram-CTC 25.59 18.52\n As shown in Table 5, the Gram-CTC performs better than CTC.  After joint-training with vanilla CTC, its performance is further boosted, which verifies joint-training helps training.  In fact, with only a small cost of time, it effectively\nreduces the WER from 27.59% to 24.92% (without language model).\nF. Some Conclusions\n In this patent document, embodiments of a Gram-CTC loss to enable automatic decomposition of target sequences into learned grams were presented.  Also presented were techniques to train a Gram-CTC in a clean and stable way.  Extensive\nexperiments demonstrate the proposed Gram-CTC enables the models to run more efficiently than the vanilla CTC, by using larger stride, while obtaining better performance of sequence labelling.  Comparison experiments on multiple-scale datasets show the\nproposed Gram-CTC obtains state-of-the-art results on various ASR tasks.  It shall be noted that such systems and methods not only improve computing system performance by being more efficient and more effective but may also be employed in numerous\nsequence labelling tasks, which can aid human-computer interactions.\n Also, in embodiments, an interesting observation is that the learning of Gram-CTC implicitly avoids the \"degenerated solution\" that occur in a traditional \"unit discovery\" task, without involving any Bayesian priors or the \"minimum description\nlength\" constraint.  Using a small gram set that contains only short (e.g., up to 5) as well as high frequency grams helps explain the success.\nG. System Embodiments\n In embodiments, aspects of the present patent document may be directed to or implemented on information handling systems/computing systems.  For purposes of this disclosure, a computing system may include any instrumentality or aggregate of\ninstrumentalities operable to compute, calculate, determine, classify, process, transmit, receive, retrieve, originate, route, switch, store, display, communicate, manifest, detect, record, reproduce, handle, or utilize any form of information,\nintelligence, or data for business, scientific, control, or other purposes.  For example, a computing system may be a personal computer (e.g., laptop), tablet computer, phablet, personal digital assistant (PDA), smart phone, smart watch, smart package,\nserver (e.g., blade server or rack server), a network storage device, or any other suitable device and may vary in size, shape, performance, functionality, and price.  The computing system may include random access memory (RAM), one or more processing\nresources such as a central processing unit (CPU) or hardware or software control logic, ROM, and/or other types of memory.  Additional components of the computing system may include one or more disk drives, one or more network ports for communicating\nwith external devices as well as various input and output (I/O) devices, such as a keyboard, a mouse, touchscreen and/or a video display.  The computing system may also include one or more buses operable to transmit communications between the various\nhardware components.\n FIG. 7 depicts a simplified block diagram of a computing device/information handling system (or computing system) according to embodiments of the present disclosure.  It will be understood that the functionalities shown for system 700 may\noperate to support various embodiments of an information handling system--although it shall be understood that an information handling system may be differently configured and include different components.\n As illustrated in FIG. 7, system 700 includes one or more central processing units (CPU) 701 that provides computing resources and controls the computer.  CPU 701 may be implemented with a microprocessor or the like, and may also include one or\nmore graphics processing units (GPU) 717 and/or a floating-point coprocessor for mathematical computations.  System 700 may also include a system memory 702, which may be in the form of random-access memory (RAM), read-only memory (ROM), or both.\n A number of controllers and peripheral devices may also be provided, as shown in FIG. 7.  An input controller 703 represents an interface to various input device(s) 704, such as a keyboard, mouse, or stylus.  There may also be a scanner\ncontroller 705, which communicates with a scanner 706.  System 700 may also include a storage controller 707 for interfacing with one or more storage devices 708 each of which includes a storage medium such as magnetic tape or disk, or an optical medium\nthat might be used to record programs of instructions for operating systems, utilities, and applications, which may include embodiments of programs that implement various aspects of the present invention.  Storage device(s) 708 may also be used to store\nprocessed data or data to be processed in accordance with the invention.  System 700 may also include a display controller 709 for providing an interface to a display device 711, which may be a cathode ray tube (CRT), a thin film transistor (TFT)\ndisplay, or other type of display.  The computing system 700 may also include a printer controller 712 for communicating with a printer 713.  A communications controller 714 may interface with one or more communication devices 715, which enables system\n700 to connect to remote devices through any of a variety of networks including the Internet, a cloud resource (e.g., an Ethernet cloud, a Fiber Channel over Ethernet (FCoE)/Data Center Bridging (DCB) cloud, etc.), a local area network (LAN), a wide area\nnetwork (WAN), a storage area network (SAN) or through any suitable electromagnetic carrier signals including infrared signals.\n In the illustrated system, all major system components may connect to a bus 716, which may represent more than one physical bus.  However, various system components may or may not be in physical proximity to one another.  For example, input data\nand/or output data may be remotely transmitted from one physical location to another.  In addition, programs that implement various aspects of this invention may be accessed from a remote location (e.g., a server) over a network.  Such data and/or\nprograms may be conveyed through any of a variety of machine-readable medium including, but are not limited to: magnetic media such as hard disks, floppy disks, and magnetic tape; optical media such as CD-ROMs and holographic devices; magneto-optical\nmedia; and hardware devices that are specially configured to store or to store and execute program code, such as application specific integrated circuits (ASICs), programmable logic devices (PLDs), flash memory devices, and ROM and RAM devices.\n Embodiments of the present invention may be encoded upon one or more non-transitory computer-readable media with instructions for one or more processors or processing units to cause steps to be performed.  It shall be noted that the one or more\nnon-transitory computer-readable media shall include volatile and non-volatile memory.  It shall be noted that alternative implementations are possible, including a hardware implementation or a software/hardware implementation.  Hardware-implemented\nfunctions may be realized using ASIC(s), programmable arrays, digital signal processing circuitry, or the like.  Accordingly, the \"means\" terms in any claims are intended to cover both software and hardware implementations.  Similarly, the term\n\"computer-readable medium or media\" as used herein includes software and/or hardware having a program of instructions embodied thereon, or a combination thereof.  With these implementation alternatives in mind, it is to be understood that the figures and\naccompanying description provide the functional information one skilled in the art would require to write program code (i.e., software) and/or to fabricate circuits (i.e., hardware) to perform the processing required.\n It shall be noted that embodiments of the present invention may further relate to computer products with a non-transitory, tangible computer-readable medium that have computer code thereon for performing various computer-implemented operations. \nThe media and computer code may be those specially designed and constructed for the purposes of the present invention, or they may be of the kind known or available to those having skill in the relevant arts.  Examples of tangible computer-readable media\ninclude, but are not limited to: magnetic media such as hard disks, floppy disks, and magnetic tape; optical media such as CD-ROMs and holographic devices; magneto-optical media; and hardware devices that are specially configured to store or to store and\nexecute program code, such as application specific integrated circuits (ASICs), programmable logic devices (PLDs), flash memory devices, and ROM and RAM devices.  Examples of computer code include machine code, such as produced by a compiler, and files\ncontaining higher level code that are executed by a computer using an interpreter.  Embodiments of the present invention may be implemented in whole or in part as machine-executable instructions that may be in program modules that are executed by a\nprocessing device.  Examples of program modules include libraries, programs, routines, objects, components, and data structures.  In distributed computing environments, program modules may be physically located in settings that are local, remote, or\nboth.\n One skilled in the art will recognize no computing system or programming language is critical to the practice of the present invention.  One skilled in the art will also recognize that a number of the elements described above may be physically\nand/or functionally separated into sub-modules or combined together.\n It will be appreciated to those skilled in the art that the preceding examples and embodiments are exemplary and not limiting to the scope of the present disclosure.  It is intended that all permutations, enhancements, equivalents, combinations,\nand improvements thereto that are apparent to those skilled in the art upon a reading of the specification and a study of the drawings are included within the true spirit and scope of the present disclosure.  It shall also be noted that elements of the\nclaims may be arranged differently including having multiple dependencies, configurations, and combinations.", "application_number": "15698593", "abstract": " Described herein are systems and methods for automatic unit selection and\n     target decomposition for sequence labelling. Embodiments include a new\n     loss function called Gram-Connectionist Temporal Classification (CTC)\n     loss that extend the popular CTC loss function criterion to alleviate\n     prior limitations. While preserving the advantages of CTC, Gram-CTC\n     automatically learns the best set of basic units (grams), as well as the\n     most suitable decomposition of target sequences. Unlike CTC, embodiments\n     of Gram-CTC allow a model to output variable number of characters at each\n     time step, which enables the model to capture longer term dependency and\n     improves the computational efficiency. It is also demonstrated that\n     embodiments of Gram-CTC improve CTC in terms of both performance and\n     efficiency on the large vocabulary speech recognition task at multiple\n     scales of data, and that systems that employ an embodiment of Gram-CTC\n     can outperform the state-of-the-art on a standard speech benchmark.\n", "citations": ["6292772", "20060031069", "20130317755", "20160171974", "20160321777", "20170011738", "20170098153", "20170103752", "20170148431", "20170148433", "20180011688", "20180061439", "20180247643"], "related": ["62463503"]}, {"id": "20180253837", "patent_code": "10373313", "patent_name": "Spatially consistent multi-scale anatomical landmark detection in\n     incomplete 3D-CT data", "year": "2019", "inventor_and_country_data": " Inventors: \nGhesu; Florin Cristian (Erlangen, DE), Georgescu; Bogdan (Plainsboro, NJ), Grbic; Sasa (Princeton, NJ), Comaniciu; Dorin (Princeton Junction, NJ)  ", "description": "BACKGROUND OF THE INVENTION\n The present invention relates to anatomical landmark detection in medical image data, and more particularly, to spatially consistent multi-scale deep learning based detection of anatomical landmarks in medical image data.\n Fast and robust anatomical object detection is a fundamental task in medical image analysis that supports the entire clinical imaging workflow from diagnosis, patient stratification, therapy planning, intervention, and follow-up.  Automatic\ndetection of an anatomical object is a prerequisite for many medical image analysis tasks, such as segmentation, motion tracking, and disease diagnosis and quantification.\n Machine learning based techniques have been developed for anatomical landmark detection in medical images.  For example, machine learning techniques for quickly identifying anatomy in medical images include Marginal Space Learning (MSL),\nMarginal Space Deep Learning (MSDL), Marginal Space Deep Regression (MSDR), and Approximated Marginal Space Deep Learning (AMSD).  While machine learning techniques are often applied to address the problem of detecting anatomical structures in medical\nimages, the traditional object search scheme used in such techniques is typically driven by suboptimal and exhaustive strategies.  Furthermore, these techniques do not effectively address cases of incomplete data, i.e., scans taken with a partial\nfield-of-view.  Addressing these limitations of conventional anatomical landmark detection techniques is important to enable artificial intelligence to directly support and increase the efficiency of the clinical workflow from admission through\ndiagnosis, clinical care, and patient follow-up.\nBRIEF SUMMARY OF THE INVENTION\n The present disclosure relates to methods and systems for automated computer-based spatially consistent multi-scale detection of anatomical landmarks in medical images.  Embodiments of the present invention provide robust and fast multi-scale\ndetection of anatomical landmarks in medical images and are capable of reliable landmark detection in incomplete medical images (i.e., medical images with partial field-of-views).  Embodiments of the present invention enforce spatial coherence of\nmulti-scale detection of a set of anatomical landmarks in a medical image.\n In one embodiment of the present invention, a discrete scale-space representation of a medical image of a patient is generated, wherein the discrete scale-space representation of the medical image includes a plurality of scale-levels.  A\nplurality of anatomical landmarks are detected at a coarsest scale-level of the discrete scale-space representation of the medical image using a respective trained search model trained to predict a trajectory from a starting location to a predicted\nlandmark location at the coarsest scale-level for each of the plurality of anatomical landmarks.  Spatial coherence of the detected anatomical landmarks is enforced by fitting a learned shape model of the plurality of anatomical landmarks to the detected\nanatomical landmarks at the coarsest scale-level to robustly determine a set of the anatomical landmarks within a field-of-view of the medical image.  The detected landmark location for each of the landmarks in the set of anatomical landmarks is refined\nat each remaining scale-level of the discrete scale-space representation of the medical image using, for each landmark in the set of anatomical landmarks, a respective trained search model trained to predict a trajectory to a predicted landmark location\nat each remaining scale-level, wherein the trained search model for each remaining scale-level for each landmark is constrained based on a range surrounding the predicted landmark location for that landmark at a previous scale-level.\n These and other advantages of the invention will be apparent to those of ordinary skill in the art by reference to the following detailed description and the accompanying drawings. BRIEF DESCRIPTION OF THE DRAWINGS\n FIG. 1 illustrates a method of training an intelligent multi-scale navigation model for anatomical landmark detection according to an embodiment of the present invention;\n FIG. 2 illustrates an exemplary search model for anatomical landmark detection in a medical image according to an embodiment of the present invention;\n FIG. 3 illustrates a method for automated computer-based multi-scale anatomical landmark detection in medical images according to an embodiment of the present invention;\n FIG. 4 illustrates exemplary results of the automated multi-scale anatomical landmark detection method of FIG. 3;\n FIG. 5 illustrates a comparison of landmark detection results obtained using the method of FIG. 3 and Marginal Space Deep Learning (MSDL); and\n FIG. 6 is a high-level block diagram of a computer capable of implementing the present invention.\nDETAILED DESCRIPTION\n The present disclosure relates to methods and systems for automated computer-based spatially consistent multi-scale detection of anatomical landmarks in medical images.  Embodiments of the present invention are described herein to give a visual\nunderstanding of the anatomical landmark detection method.  A digital image is often composed of digital representations of one or more objects (or shapes).  The digital representation of an object is often described herein in terms of identifying and\nmanipulating the objects.  Such manipulations are virtual manipulations accomplished in the memory or other circuitry/hardware of a computer system.  Accordingly, is to be understood that embodiments of the present invention may be performed within a\ncomputer system using data stored within the computer system.\n Robust and fast computer-based automated detection of anatomical structures in medical images is an important task for next-generation automated medical support tools.  While machine learning techniques are often applied to address this problem,\nthe traditional object search scheme is typically driven by suboptimal and exhaustive strategies.  One limitation with traditional machine learning anatomical landmark detection techniques is that they do not effectively addresses cases of incomplete\ndata, i.e., medical image scans taken with a partial field-of-view.  Deep scanning-based methods represent one main category of machine learning based anatomical landmark detection solutions.  In deep-scanning based methods, such as Marginal Space Deep\nLearning (MSDL), the problem of anatomical landmark detection in medical images is typically reformulated to a patch-wise classification between positive and negative hypotheses, sampled as volumetric boxes of image intensities.  Alternatively,\nend-to-end deep learning systems based on fully convolutional architectures approach the problem of anatomical landmark detection in medical images by learning a direct mapping f(I)=M between the original image I and a coded map M highlighting the\nlocations of anatomical landmarks.  However, in cases of over thousands of large range 3D CT scans at high spatial resolution (e.g., 2 mm or less), the training of such deep learning systems becomes infeasible due to excessive memory requirements and\nhigh computational complexity.  Furthermore, for incomplete data, all of these deep learning based systems share a common limitation in that they rely on suboptimal or inaccurate heuristics such as probability thresholding to recognize whether an\nanatomical landmark is visible in the field-of-view of the 3D scan.\n Embodiments of the present invention provide improvements to the technology of computer-based automated anatomical landmark detection in medical images, as compared to traditional machine learning based techniques for anatomical landmark\ndetection.  Embodiments of the present invention provide faster and more accurate detection of anatomical landmarks, as compared to existing deep learning based techniques for anatomical landmark detection.  Embodiments of the present invention provide\nincreased robustness for landmark detection in cases of incomplete data.  As used herein, \"incomplete data\" refers to a medical image scan with a partial field-of-view that is missing one or more of the landmarks to be detected.  Embodiments of the\npresent invention also utilize a multi-scale landmark detection method that reduces memory requirements and computational complexity as compared to existing deep learning based techniques.  Embodiments of the present invention address the above described\nlimitations of existing deep learning based anatomical landmark detection techniques by using a scale-space model and robust statistical shape modeling for multi-scale spatially-coherent landmark detection.\n In general, the continuous scale-space of a 3D image signal I.di-elect cons..sup.3.fwdarw.  is defined as: L(x, t)=T(.xi.,t)I(x-.xi.), where t.di-elect cons.  denotes the continuous scale-level, x.di-elect cons., L(x, 0)=I(x), and T defines a\none-parameter kernel-family.  The main properties of such a scale-space representation are the non-enhancement of local extrema and implicitly the causality of structure across scales.  These properties are important for the robustness of a search\nprocess, starting from coarse to fine scale.  According to an advantageous embodiment of the present invention, a discrete approximation of the continuous scale-space L is used while best preserving these properties.  This discrete scale-space is defined\nas: L.sub.d(t)=.PSI..sub..rho.(.sigma.(t-1))*L.sub.d(t-1), where L.sub.d(0)=I, t.di-elect cons..sub.0 denotes the discrete scale-level, a represents a scale-dependent smoothing function, and .PSI..sub..rho.  denotes a signal operator that reduces the\nspatial resolution with factor .rho.  using down-sampling.\n Assuming without loss of generality a discrete scale-space of M scale levels and .rho.=2, embodiments of the present invention search for anatomical landmarks in a medical image using a navigation model across the M scales, starting with from\ncoarsest scale-level (t=M-1) and ending with the finest scale-level (t=0).  According to an advantageous embodiment, for a given anatomical landmark, each scale-space is searched by iteratively approximating an optimal action value function Q* for a\ncurrent state s using a learned model .theta.  and applying an action a based on the approximated optimal action value function.  For this, the optimal action value function Q* is redefined by conditioning the state-representation s and model parameters\n.theta.  on the scale-space L.sub.d and the current scale t.di-elect cons.[0, .  . . , M-1]: Q*(s, a|L.sub.d,t).apprxeq.Q(s, a; .theta..sub.t|L.sub.d,t).  This results in M independent navigation sub-models .theta.=[.theta..sub.0, .theta..sub.1, .  . . ,\n.theta..sub.M-1], one for each scale-level.  In an advantageous embodiment, the respective navigation sub-model for each scale-level is a deep neural network (DNN) trained at that scale level using deep reinforcement learning (DRL), i.e., the navigation\nsub-models are trained by optimizing the Bellman criterion on each scale-level t&lt;M. Additional details regarding training a model for landmark detection using DRL are described in U.S.  Publication No. 2017/0103532, entitled \"Intelligent Medical Image\nLandmark Detection,\" and U.S.  Publication No. 2017/0116497, entitled \"Intelligent Multi-scale Medical Image Landmark Detection,\" the disclosures of which are incorporated herein in their entirety by reference.  In order to search for the landmark in a\ngiven scale-level t, a state-representation s representing a current location of the landmark search at that scale-level is input to the trained DNN .theta..sub.t for that scale-level, the trained DNN calculates action values (Q-values) for a defined set\nof actions (e.g., left, right, up, down, front, back), and an action with the highest Q-value is selected and applied to move the current location.  These operations are repeated until the landmark search at that scale-level converges (or for a\npredetermined maximum number of iterations).\n The multi-scale detection workflow for each anatomical landmark is performed as follows: the search starts in the image center at the coarsest scale level M-1.  Upon convergence of the search at the coarsest scale-level M-1, the scale-level is\nchanged to M-2 and the search continues from the convergence point determined at M-1.  The same process is repeated at the following scales until convergence on the finest scale t=0.  The present inventors have empirically observed that optimal\ntrajectories converge on minimal oscillatory cycles.  As such, in an advantageous implementation, the convergence point can be defined as the center of gravity of this cycle.  The search-model for the coarsest scale-level Q(.,.; .theta..sub.M-1|L.sub.d,\nM-1) is trained for global convergence (i.e., convergence over the entire reduced resolution image at that scale), while the models for each of the subsequent scales t&lt;M-1 are trained in a constrained range around the ground-truth.  This range may be\nrobustly estimated from the accuracy upper-bound on the previous scale t+1.  Note that the spatial coverage of a fixed-size state s.di-elect cons.S increases exponentially with the scale.  This multi-scale navigation model allows the system to\neffectively exploit the image information and increase the robustness of the search.\n According to an advantageous embodiment of the present invention, the global search model .theta..sub.M-1 (i.e., the search model for the coarsest scale-level) is explicitly trained for missing landmarks in order to further improve the accuracy\nfor such cases.  In particular, the global search model .theta..sub.M-1 is trained to reward trajectories that leave the image space through the correct image/volume border when the landmark being searched for is missing in the field of view in the\ntraining data.  For example, assuming that computed tomography (CT) scans are cut only horizontally, the global search model .theta..sub.M-1 is trained to reward trajectories that leave the image space through the top border or the bottom border\ndepending on whether the missing landmark in the training data is above or below the field of view.  In order to perform this training, an annotation is required for each missing landmark in the training data indicating whether the missing landmark is\nabove the field of view or below the field of view.\n FIG. 1 illustrates a method of training an intelligent multi-scale navigation model for anatomical landmark detection according to an embodiment of the present invention.  The intelligent multi-scale navigation model trained using the method of\nFIG. 1 includes trained search models for a plurality of scale-levels (spatial resolutions) for one or more anatomical landmarks.  The method of FIG. 1 is performed in an offline training phase prior to performing online detection of anatomical landmarks\nin newly received/acquired medical images using the trained multi-scale navigation model.\n Referring to FIG. 1, at step 102, training images are received.  The training images can be received by loading a plurality of training images from a database.  In an advantageous implementation, the training images are 3D medical images\n(volumes) in order to train the intelligent multi-scale navigational model for 3D anatomical landmark detection, but the present invention is not limited thereto and the training images may also be 2D images.  It is to be understood that the training\nimages can be n-dimensional, as there is no limitations as the number of dimensions.  The training images can be medical images acquired using any medical imaging modality, such as but not limited to Computed Tomography (CT), Magnetic Resonance Imaging\n(MRI), Ultrasound, DynaCT, Positron Emission Tomography (PET), PET-CT, MR-PET, X-ray, etc. The training images are each annotated with ground truth locations for a set of N landmarks.  If a given landmark is missing from a given training image (outside\nthe field of view), the annotation of that training image for the missing landmark indicates which direction (e.g., above or below) the missing landmark is in relation to the field of view of the training image.  The training images may include medical\nimages from scans having different fields of view.  For example, the training images may include full body scans, partial body scans, and other variable scan ranges.  In an advantageous embodiment, additional training samples may be generated from the\nreceived training images by cropping received training images to generate images with multiple different fields of view.  This can provide a more robust set of training data with different landmarks missing from various training examples.  The set of N\nanatomical landmarks can include various anatomical landmarks that can be used for image navigation and/or image parsing.  For instance, in an exemplary implementation, the set of anatomical landmarks for which ground truth locations are annotated in the\ntraining data can include the left kidney, right kidney, left hip bone, right hip bone, left common carotid, brachiocephalic artery, left subclavian artery, and bronchial bifurcation, but the present invention is not limited to this specific set of\nanatomical landmarks.\n At step 104, a discrete scale-space representation is generated for each training image.  The discrete scale-space representation for a training image I is defined as: L.sub.d(t)=.PSI..sub..rho.(.sigma.(t-1))*L.sub.d(t-1), where L.sub.d(0)=I,\nt.di-elect cons..sub.0 denotes the discrete scale-level, .sigma.  represents a scale-dependent smoothing function, and .PSI..sub..rho.  denotes a signal operator that reduces the spatial resolution with factor .rho.  using down-sampling.  Accordingly,\ngenerating the discrete scale-space representation for a training image I results an image pyramid of M images L.sub.d(0), L.sub.d(1), .  . . , L.sub.d(M-1), where L.sub.d(0) is the original training image I and L.sub.d(1), .  . . , L.sub.d(M-1) are\nreduced resolution image at different spatial resolutions (scale-space levels).  In an exemplary implementation .rho.=2, but the present invention is not limited thereto.  For example, a scale-space of 4 scale-levels (M=4) can be used with isotropic\nresolutions of 2 mm, 4 mm, 8 mm, and 16 mm defined for the respective scale-levels.\n At step 106, for each landmarks, a respective search model is trained for each of the scale-levels (t=0, 1, .  . . , M-1) in the discrete scale-space.  That is, for each of the N anatomical landmarks in the set of anatomical landmarks, M search\nmodels are trained, each trained to search for the anatomical landmark in a respective one of the M scale-levels (resolutions).  In an advantageous embodiment of the present invention, each of the M search models for a given anatomical landmark is a DNN\ntrained based on the training data at the respective scale-level using DRL.  A method for training a DNN-based search model .theta.  for a particular anatomical landmark using DRL is described herein.  It is to be understood that, other than where\nspecific differences between training the search models for the different scale-levels are noted, the training method can be similarly applied to train the search model at each of the scale-levels.  Additional details regarding training a model for\nlandmark detection using DRL are described in U.S.  Publication No. 2017/0103532, entitled \"Intelligent Medical Image Landmark Detection,\" and U.S.  Publication No. 2017/0116497, entitled \"Intelligent Multi-scale Medical Image Landmark Detection,\" the\ndisclosures of which are incorporated herein in their entirety by reference.\n In an advantageous implementation, the trained DNN can be a deep convolutional neural network (CNN).  Inspired by the feed-forward type of information processing observable in the early visual cortex, the deep CNN represents a powerful\nrepresentation learning mechanism with an automated feature design, closely emulating the principles of animal and human receptive fields.  The architecture of the deep CNN is comprised of hierarchical layers of translation-invariant convolutional\nfilters based on local spatial correlations observable in images.  Denoting the l-th convolutional filter kernel in the layer k by w.sup.(k,l), the representation map generated by this filter can be expressed as:\no.sub.i,j=.sigma.((w.sup.(k,l)*x).sub.i,j+b.sup.(k,l)), where x denotes the representation map from the previous layer (used as input), (i,j) define the evaluation location of the filter and b.sup.(k,l) represents the neuron bias.  The function a\nrepresents the activation function used to synthesize the input information.  In an exemplary implementation, rectified linear unit activations (ReLU) can be used given their excellent performance.  In a supervised training setup, i.e., given a set of\nindependent observations as input patches X with corresponding value assignments y, the network response function can be defined as R(.; w,b) and Maximum Likelihood Estimation can be used to estimate the optimal network parameters: w, {circumflex over\n(b)}=argmin.sub.w,b.parallel.R(X; w, b)-y.parallel..sub.2.sup.2.  This optimization problem can be solved using a stochastic gradient descent (SGD) approach combined with the backpropagation algorithm to compute the network gradients.\n Reinforcement learning (RL) is a technique aimed at effectively describing learning as an end-to-end cognitive process.  A typical RL setting involves an artificial agent that can interact with an uncertain environment, thereby aiming to reach\npredefined goals.  The agent can observe the state of the environment and choose to act on it, similar to a trial-and-error search, maximizing the future reward signal received as a supervised response from the environment.  This reward-based decision\nprocess is modeled in RL theory as a Markov Decision Process (MDP), :=(S, A, T, R, .gamma.), where S represents a finite series of states over time, A represents a finite series of actions allowing the agent to interact with the environment,\nT:S.times.A.times.S.fwdarw.[0,1] is a stochastic transition function, where T.sub.s,a.sup.s' describes the probability of arriving in state s' after performing action a in state s, R:S.times.A.times.S is a scalar reward function, where R.sub.s,a.sup.s'\ndenotes the expected reward after a state transition, and .gamma.  is the discount factor controlling future versus immediate rewards.\n Formally, the future discounted reward of an artificial agent at time can be written as = with marking the end of a learning episode and r.sub.t defining the immediate reward the agent receives at time .  Especially in model-free reinforcement\nlearning, the target is to find the optimal so called action-value function, denoting the maximum expected future discounted reward when starting in state s and performing action a: Q*(s, a)=max.sub..pi.[R.sub.t|s.sub.t=s, a.sub.t=a, .pi.], where .pi. \nis an action policy, in other words a probability distribution over actions in each given state.  Once the optimal action-value function is estimated, the optimal action policy, determining the behavior of the artificial agent, can be directly computed\nin each state: .A-inverted.s.di-elect cons.S:.pi.*(s)=argmax.sub.a.di-elect cons.AQ*(s, a).  One important relation satisfied by the optimal action-value function Q* is the Bellman optimality equations, which is defined as:\nQ*(s,a)=.SIGMA..sub.s'T.sub.s,a.sup.s'(R.sub.s,a.sup.s'+.gamma.max.sub.a'- Q*(s',a'))=.sub.s'(R+.gamma.max.sub.a'Q*(s',a')) (1) where s' defines a possible state visited after s, a' the corresponding action, and r=R.sub.s,a.sup.s' represents a compact\nnotation for the current, immediate reward.  Viewed as an operator .tau., the Bellman equation defines a contraction mapping.  Strong theoretical results show that by applying Q.sub.i+1=r(Q.sub.i), .A-inverted.(s, a), the function Q.sub.i converges to Q*\nat infinity.  This model-based policy iteration approach is however not always feasible in practice.  An alternative is the use of model-free temporal difference methods, such as Q-learning, which exploit correlations of consecutive states.  The use of\nparametric functions to approximate the Q-function provides a step further toward higher computational efficiency.  Considering the expected non-linear structure of the Q-function, neural networks represent a potentially powerful solution for policy\napproximation.  According to an advantageous embodiment of the present invention, deep neural networks are leveraged to approximate the Q-function (at each scale-level) in order to provide automated machine-driven intelligence for landmark detection in\nmedical images.\n According to an advantageous embodiment, the landmark detection problem is formulated as a deep-learning driven behavior policy encoding automatic, intelligent paths in parametric space toward the correct solution.  In particular, for the\nlandmark detection problem, the optimal search policy represents a trajectory in image space (at the respective scale-level) converging to the landmark location p.di-elect cons..sup.d (d is the image dimensionality).  The reward-based decision process\nfor determining the trajectory to the landmark location is modeled with an MDP .  While the system dynamics T are implicitly modeled through the deep-learning-based policy approximation, the state space S, the action space A, and the reward/feedback\nscheme are explicitly designed for the landmark detection problem.  The states describe the surrounding environment.  According to an advantageous implementation, the state for the landmark detection search model is defined as a region-of-interest in the\nimage (at the given scale-level) with its center representing the current position of the agent (i.e., the current estimate for the landmark location).  The actions denote the moves of the artificial agent in the parametric space.  According to an\nadvantageous implementation, a discrete action scheme can be selected allowing the agent to move a predetermined distance (i.e., one pixel/voxel) in all directions: up, down, left, right, front, back, corresponding to a shift of the image patch.  This\nallows the agent to explore the entire image space (for the global search model at scale level M-1) or the entire search space of the constrained search regions (for the search models at the remaining scales).  The rewards encode the supervised feedback\nreceived by the agent.  As opposed to typical reward choices for RL problems, embodiments of the present invention follow more closely to a standard human learning environment, where rewards are scaled according to the quality of a specific move.  In an\nadvantageous implementation, the reward is selected to be .delta.d, the supervised relative distance change to the ground truth landmark location after executing an action.\n FIG. 2 illustrates an exemplary search model for anatomical landmark detection in a medical image according to an embodiment of the present invention.  As illustrated in FIG. 2, the search model acts as an intelligent artificial agent 200 that\ninteracts with an environment 210.  A deep neural network 202 is trained to learn a reward behavior policy.  The trained deep neural network 202 inputs a current state S.sub.t, which is a current ROI view centered at a current estimated landmark\nlocation, and estimates a reward function r.sub.t that includes an action value for each action in the set of actions.  An action a.sub.t having the highest action value (Q-value) is selected and applied to move the current ROI, which results in a next\nstate s.sub.t+1.  The next state s.sub.t+1 is then input to the deep neural network 202 and the process is iterated until a stop condition (e.g., convergence, maximal number of iterations) is met.\n Deep reinforcement learning is used to train the intelligent artificial agent.  Given the model definition, the goal of the agent is to select actions by interacting with the environment in order to maximize the cumulative future reward.  The\noptimal behavior is defined by the optimal policy .pi.* and implicitly optimal action value function Q*.  In an advantageous implementation, a model-free, temporal difference approach using a deep convolutional neural network (CNN) can be used to\napproximate the optimal active-value function Q*.  Defining the parameters of a deep CNN as .theta., this architecture can be used as a generic, non-linear function approximator Q(s, a; .theta.).apprxeq.Q* (s, a), referred to herein as a deep Q network\n(DQN).  A deep Q network can be trained in this context using an iterative approach to minimize a mean squared error based on the Bellman optimality criterion (see Equation 1).  At any learning iteration i, the optimal expected target values can be\napproximated using a set of reference parameters Q.sub.i.sup.ref:=.theta..sub.j from a previous iteration j&lt;i:y=r+.gamma.  max.sub.a'Q (s', a'; .theta..sub.i.sup.ref).  As such, a sequence of well-defined optimization problems driving the evolution of\nthe network parameters is obtained.  The error function at each step i is defined as: {circumflex over (.theta.)}.sub.i=arg min.sub..theta..sub.i.sub.s,a,r,s'[(y-Q(s,a;.theta..sub.i)).sup.2]+.sub.s- ,a,r.sub.s'[y]].  (2)\n Using a different network to compute the reference values for training can bring robustness to the algorithm.  In such a setup, changes to the current parameters .theta..sub.i and implicitly to the current approximator Q(.; .theta..sub.i) cannot\ndirectly impact the reference output y, introducing an update-delay and thereby reducing the probability to diverge and oscillate in suboptimal regions of the optimization space.  To ensure the robustness of the parameter updates and train more\nefficiently, experience replay can be used.  In experience replay, the agent stored a limited experience memory (204 of FIG. 2) of previously visited states as a set of explored trajectories: .epsilon.=[.sub.1, .sub.2, .  . . .sub.p].  This memory is\nconstantly sampled randomly to generate mini-batches guiding the robust training of the CNN and implicitly the agent behavior policy.\n For each anatomical landmark to be detected, the above described training algorithm is used to train a respective search model .theta..sub.t for each of the scale-levels t=0, 1, .  . . M-1.  The search-model for the coarsest scale-level Q(.,.;\n.theta..sub.M-1|L.sub.d, M-1) is trained for global convergence (i.e., convergence over the entire reduced resolution image at that scale), while the models for each of the subsequent scales t&lt;M 1 are trained in a constrained range around the\nground-truth.  This range may be robustly estimated from the accuracy upper-bound on the previous scale t+1.  According to an advantageous embodiment of the present invention, the global search model .theta..sub.M-1 (i.e., the search model for the\ncoarsest scale-level) is explicitly trained for missing landmarks in order to further improve the accuracy for such cases.  In particular, the global search model .theta..sub.M-1 is trained to reward trajectories that leave the image space through the\ncorrect image/volume border when the landmark being searched for is missing in the field of view in the training data.  Once the trained search models for each landmark are trained, the trained search models can be stored, for example on a memory or\nstorage of a computer system or on a remote cloud-based storage device, and used to perform automated computer-based landmark detection in a newly received medical image.\n FIG. 3 illustrates a method for automated computer-based multi-scale anatomical landmark detection in medical images according to an embodiment of the present invention.  The method of FIG. 3 utilizes trained search models using the method of\nFIG. 1 to detect a set of anatomical landmarks in a medical image of a patient.  At step 302, a medical image of a patient is received.  The medical image can be a 2D, 3D, or n-D medical image and can be acquired using any type of medical imaging\nmodality, such as but not limited to CT, MRI, ultrasound, X-ray fluoroscopy, DynaCT, etc. The medical image can be received directly from an image acquisition device, such as a CT scanner, MRI scanner, etc. Alternatively, the medical image can be\nreceived by loading a previously acquired medical image of the patient from a memory or storage of a computer system or can be received as an electronic transmission from a remote computer system.\n At step 304, a discrete scale-space representation is generated for the medical image.  The discrete scale-space representation for the received medical image I is defined as: L.sub.d(t)=.PSI..sub..rho.(.sigma.(t-1))*L.sub.d(t-1), where\nL.sub.d(0)=I, t.di-elect cons..sub.0 denotes the discrete scale-level, .sigma.  represents a scale-dependent smoothing function, and .PSI..sub..rho.  denotes a signal operator that reduces the spatial resolution with factor .rho.  using down-sampling. \nAccordingly, generating the discrete scale-space representation for the medical image I results an image pyramid of M images L.sub.d(0), L.sub.d(1), .  . . , L.sub.d(M-1), where L.sub.d(0) is the original resolution medical image I and L.sub.d(1), .  . .\n, L.sub.d(M-1) are reduced resolution images at different spatial resolutions (scale-space levels) generated by down-sampling the medical image.  In an exemplary implementation .rho.=2, but the present invention is not limited thereto.  For example, a\nscale-space of 4 scale-levels (M=4) can be used with isotropic resolutions of 2 mm, 4 mm, 8 mm, and 16 mm defined for the respective scale-levels.\n At step 306, a set of anatomical landmarks are detected at the coarsest scale-level of the scale-space representation of the medical image using a respective trained search model for each landmark.  For each anatomical landmark, a plurality of\nsearch models, each corresponding to a respective scale-level (i.e., spatial resolution) of the discrete scale-space representation, are trained in an offline training stage using the method of FIG. 1.  As described above, each search model uses a\ntrained deep neural network (DNN) to calculate action values and select actions to search for the landmark in the respective scale-level of the scale-space representation of the medical image.  For each landmark, the search model .theta..sub.M-1 at the\ncoarsest scale-level t=M-1 is a global search model that is trained to perform a global search for the location of that landmark over the coarsest reduced resolution image L.sub.d(M-1) of the discrete scale-space representation.  The search for each\nlandmark using the respective coarsest resolution search model .theta..sub.M-1 can start at the center of the coarsest reduced resolution image L.sub.d(M-1).  However, the present invention is not limited to starting at the center of the coarsest reduced\nresolution image and other starting positions are possible as well.  For each landmark, the trained coarsest search model .theta..sub.M-1 (i.e., trained DNN) for that landmark iteratively predicts a series of actions that predict a path from the starting\nlocation (i.e., the center of the coarsest reduced resolution image) to a final estimated landmark location in the coarsest reduced resolution image.  At each iteration, a predetermined size region of interest centered at the current location is input to\nthe trained DNN and the trained DNN calculates action-values for each of a plurality of predetermined actions.  For example, the actions can be moving the current location by one pixel/voxel up, down, left, right, forward, or backward.  The action having\nthe highest action-value is selected and applied.  These steps are repeated for a given landmark until the path/trajectory for that landmark converges or until a preset maximum number of iterations is reached.  It can be determined that the\npath/trajectory for a landmark has converged when a loop/oscillatory cycle is detected in the path/trajectory.  In this case, the final estimated landmark location for the landmark can be determined to be the center of gravity of the cycle.  As described\nabove, the coarsest search model .theta..sub.M-1 can be trained for missing landmarks by rewarding trajectories that leave the image space through the correct volume border when a landmark is missing from the field-of-view of an image.  Accordingly, for\na given landmark, the coarsest scale-space search model .theta..sub.M-1 can predict a trajectory that leaves the field-of-view of the medical image, and thus predicts that the landmark is missing from the medical image.  In an exemplary implementation,\nthe set of landmarks to be detected can include the bronchial bifurcation, the aortic arch bifurcations (i.e., the left subclavian artery, left common carotid artery, and the brachiocephalic artery), the center of the left and right kidneys, and the\nfront corner of the left and right hip bones, but the present invention is not limited thereto.\n FIG. 4 illustrates exemplary results of the automated multi-scale anatomical landmark detection method of FIG. 3.  As shown in FIG. 4, image 400 shows results for detecting anatomical landmarks at the coarsest scale-level (t=M-1) of a discrete\nscale-space representation of a computed tomography (CT) image.  In the example of FIG. 4, the coarsest scale-level corresponds to a spatial resolution of 16 mm.  In the example of FIG. 4, the set of landmarks to be detected includes the bronchial\nbifurcation, the left subclavian artery bifurcation, the left common carotid artery bifurcation, the brachiocephalic artery bifurcation, the left kidney (center of the left kidney), the right kidney (center of the right kidney), the left hip (front\ncorner of the left hip bone) and the right hip (front corner of the right hip bone).  As shown in image 400, trajectories to estimated landmark locations are predicted at the coarsest scale-level for the bronchial bifurcation 408, left subclavian artery\nbifurcation 406, left common carotid artery bifurcation 404, brachiocephalic artery bifurcation 402, left kidney 410, right kidney 412, left hip 414, and right hip 416.  Each trajectory 402, 404, 406, 408, 410, 412, 414, and 416 is independently\npredicted by a respective coarsest scale-level search model .theta..sub.M-1 trained for that landmark.  The predicted trajectory for the left hip 414 leaves the field-of-view of the coarsest reduced resolution image 400.  This means the coarsest\nscale-level search model .theta..sub.M-1 trained for the left hip landmark predicts that the left hip landmark is missing from the field-of-view of the image.\n Returning to FIG. 3, at step 308, spatially coherency is enforced for the set of anatomical landmarks by fitting a learned shape model to the anatomical landmarks detected at the coarsest scale-level.  This step detects eliminates landmarks\nmissing from the field-of-view of the medical image and detects and corrects outliers in the detected anatomical landmarks.  To ensure the robust recognition of missing anatomical structures and outliers, the spatial distribution of the set of considered\nanatomical landmarks can be modeled using robust statistical shape modeling.  This step constrains the output of landmarks detected at the coarsest scale-level by the global search model .theta..sub.M-1 for each landmark.  A statistical shape model of\nthe spatial distribution of the set of anatomical landmarks is learned in an offline training stage from a set of training images.  Assuming a complete set of N anatomical landmarks, the distribution of these landmark points over the complete set of\ntraining images is normalized to zero mean and unit variance.  In this space, the distribution of each individual landmark i.di-elect cons.[0, .  . . , N-1] can be modeled as a multi-variate normal distribution p.sub.i.about.(.mu..sub.i, .SIGMA..sub.i),\nwhere .mu..sub.i and .SIGMA..sub.i are estimated using maximum likelihood.  This defines a mean shape-model for the landmark set, defined as .mu.=[.mu..sub.0, .mu..sub.1, .  . . , .mu..sub.N-1].sup.T.\n Given an unseen configuration of detected landmark points at scale M-1 as {tilde over (P)}=[{tilde over (p)}.sub.0, {tilde over (p)}.sub.1, .  . . , {tilde over (p)}.sub.N-1], the set of detected landmark points {tilde over (P)} can be\napproximated with a translated and anisotropic-scaled version of the mean model using least linear squares.  However, for the case of incomplete data the cardinality of |{tilde over (P)}|.ltoreq.N.  In addition outliers can corrupt the data.  According\nto an advantageous implementation, an M-estimator sample consensus can be used enable the robust fitting of the shape model to the set of landmarks detected at the coarsest scale-level.  Based on random 3-point samples from the set of all triples (i.e.,\nthe set of all possible combinations of three of the landmark points), the mean-model fit {circumflex over (.omega.)}=[t, s] can be obtained, where t and s are the translation and scaling parameters to fit the mean shape model to the detected landmarks. \nThe target is to optimize the cost function based on the re-descending M-estimator and implicitly maximize the cardinality of the consensus set.  In an advantageous implementation, the following cost function can be used:\n .rarw..times..times..di-elect cons..function..times..times..times..function..times..PHI..function..mu..- times..SIGMA..function..PHI..function..mu.  ##EQU00001## The target is to minimize this cost function (based on the redescending\nM-estimator) which results in maximizing the cardinality of the consensus set S. Z.sub.i is a normalization factor for the distance-based sample score which defines an ellipsoid around the mean landmark location.  If a detected landmark is within the\nellipsoid, it is considered an inlier and part of the consensus set (with cost &lt;1), if outside, it is an outlier (with fixed cost 1).  Standard random sampling is used to select the minimum set of 3 detected landmark points required to fit the model\nwith linear least squares.  Given a fitted model, the cost function is evaluated with the aim to maximize the size of the consensus set.  This results in a robust set of landmarks {circumflex over (P)} that are present in the field-of-view of the medical\nimage with missing landmarks eliminated from the initial set of landmarks to be detected and spatially coherent locations of the set of landmarks {circumflex over (P)} in the coarsest scale-level that are used to constrain the search for the set of\nlandmarks {circumflex over (P)} at the next scale-level.  Enforcing spatial coherency by fitting the learned shape model not only corrects for diverging trajectories by re-initializing the search, but also significantly reduces the false-negative rate by\ncorrecting for border cases, in which landmarks very close to the border of the image (e.g., &lt;2 cm) are falsely labeled as missing by the search model at the coarsest scale M-1.\n Referring to FIG. 4, image 420 shows results of fitting the learned mean-shape model to the landmarks detected at the coarsest scale in image 400.  The mean-shape model fit the landmarks detected in image 400 results in predicted landmark\nlocations in the field-of-view of the medical image for the bronchial bifurcation 422, left subclavian artery bifurcation 424, left common carotid artery bifurcation 426, brachiocephalic artery bifurcation 428, left kidney 430, and right kidney 432. \nGiven the detected locations of the landmarks in image 400, a translated and isotropically scaled version of the mean model is fitted as described above, resulting in the landmark locations shown in image 420.  The shape model predicts a range of\nlocations for the left and right hip landmarks 434 and 436, respectively.  Since the range of locations for the left and right hip landmarks 434 and 436 predicted by the learned shape model fit to the detected landmarks are outside of the field of view\nof the coarsest reduced resolution image 400, the left and right hip landmarks are determined to be missing from the field-of-view of the medical image and excluded from the set of landmarks {circumflex over (P)} detected at the remaining scale levels. \nNote that since the trajectory to the left hip landmark 434 (414 in image 400) is going outside the image, that landmark is not used to fit the shape-model.  The landmark locations for the bronchial bifurcation 422, left subclavian artery bifurcation\n424, left common carotid artery bifurcation 426, brachiocephalic artery bifurcation 428, left kidney 430, and right kidney 432 at the coarsest scale-level (t=M-1) are used to constrain the search for those landmarks at the next scale-level (t=M-2).\n Returning to FIG. 3, at step 310, anatomical landmarks are detected at the next scale-level of the scale-space representation of the medical image using a respective trained search model for each landmark.  At this step the search moves to the\nnext scale-level of the scale-space representation (i.e., t=t-1), and the set of landmarks {circumflex over (P)} defined in step 308 is detected at the next scale-level using a respective search model .theta..sub.t trained for the next scale-level for\neach landmark.  As described above, each search model .theta..sub.t uses a trained DNN to predict action-values for a set of actions and iteratively selects actions to predict a trajectory/path from a starting location (given by the estimated location at\nthe previous scale-level) to an estimated landmark location at the particular scale-level.  The search for each landmark by the respective search model .theta..sub.t is constrained to a range surrounding the estimated landmark location at the previous\nscale-level.  The first time step 310 is performed (for scale-level M-2), the search for each landmark by the respective search model .theta..sub.M-2 is constrained by the landmark locations in the shape model fit to the detected landmarks at the\ncoarsest scale level in step 308.  The range in each scale-level can be robustly estimated from the accuracy of the upper-bound on the previous scale-level t+1.\n At step 312, it is determined if the landmark detection at the final scale-level (t=0) of the scale-space representation of the medical image has been completed.  If the landmark detection at the final scale-level (t=0) has not yet been\ncompleted, the method returns to step 310, moves to the next scale-level and detects anatomical landmarks at the next scale-level using search models for the anatomical landmarks trained for the next-scale level.  Accordingly, the method sequentially\nperforms landmark detection at each scale-level going from coarse to fine resolutions until the landmark detection at the final scale-level (t=0) corresponding to the original resolution medical image is performed.  When the landmark detection at the\nfinal scale-level (t=0) has been completed, the method proceeds to steps 314 and 316.  Referring to FIG. 4, images 440 and 450 show landmark detection results at the scale-levels of 8 mm resolution and 4 mm resolution respectively.\n Returning to FIG. 3, at step 314 the landmark detection results are output.  In particular, the detected locations of the landmarks in the set of landmarks P in the original resolution medical image (i.e., the final scale-level t=0) are output. \nThese landmark locations can be output by displaying, on a display device of a computer system, a visualization of the medical image with markers indicating the detected landmark locations displayed at the detected locations in the visualization of the\nmedical image.  For landmarks included in the original set of landmarks to be detected, but not included in the set of landmarks {circumflex over (P)} defined in step 308, an indication that these landmarks are missing from the field-of-view of the\nmedical image can be output, for example by being displayed on a display device of computer system.\n At step 316, a scan range of the medical image can be automatically determined based on the detected anatomical landmarks.  The robust fitting of the shape-model also enables the estimation of the body-region captured in the medical image scan. \nThe learned shape model of the spatial distribution of the set of landmarks can be used to model of continuous range along a normalized z-axis (i.e., along a length of the human body), to ensure consistency among different patients.  For a set of defined\nlandmarks P in a normalized shape-space, the point p.sub.min.sup.z determines the 0% point, while the point p.sub.max.sup.z determines the 100% point.  For a given set of landmarks to be detected {tilde over (P)}, the fitted robust subset of landmarks\n(defined in step 308) is represented by {circumflex over (P)}{tilde over (P)}.  Using the definition of the range based on the shape-space of the landmark points, the span of the point-set {tilde over (P)} can be determined between 0%-100% in the\nnormalized shape-space.  This also allows the linear extrapolation of the body-range outside the z-span of the point set {tilde over (P)} in the medical image.  That is, the body range between detected locations of p.sub.min.sup.z and p.sub.max.sup.z in\nthe medical image is interpolated between 0%-100%.  The body range above p.sub.max.sup.z and below p.sub.min.sup.z in the medical image are linearly extrapolated above 100% and below 0%, respectively.  When p.sub.min.sup.z or p.sub.max.sup.z is missing\nfrom the field-of-view of the medical image, the interpolation is performed to the bottom or top border of the medical image based on the locations of the landmarks in {circumflex over (P)}.\n In an exemplary implementation, for the set of landmarks including bronchial bifurcation, left subclavian artery bifurcation, left common carotid artery bifurcation, brachiocephalic artery bifurcation, left kidney, right kidney, left hip bone\ncorner, and right hip bone corner, the continuous body range model can be defined based on the set of landmarks in the training data with the left hip bone (LHB) corner at 0% and the left common carotid artery (LCCA) bifurcation at 100%.  The levels of\nthe remaining landmarks are determined in the normalized shape-space using linear interpolation.  When applied to a newly received medical image based on a set of detected landmarks, each detected landmark is assigned the corresponding body range level\nas in the learned body range model, interpolation is performed between the landmarks, and extrapolation is performed above LCCA bifurcation and/or below the LHB corner.  In the example of FIG. 4, since the LHB coroner landmark is missing from the scan\nrange of the medical image, the scan range (body range) of the medical image is determined by interpolation along the z-axis between the LCCA bifurcation landmark location 452 detected at the final scale-level and the lower border of the medical image\nbased on the landmark locations detected at the final scale-level.  The scan range (body range) of the medical image above the LCCA bifurcation is determined by linear extrapolation along the z-axis between the LCCA bifurcation landmark location 452\ndetected at the final scale-level and the upper border of the medical image.  The scan range of the medical image in the example of FIG. 4 extends from 21.3% to 109%.\n Returning to FIG. 3, at step 318, the estimated scan-range is output.  The estimated scan-range can be output by displaying the estimated scan-range on a display device of a computer system.  The estimated scan-range can also be output by\nlabeling the medical image scan with the scan range and/or storing the scan-range associated with the medical image scan in a storage or memory of a computer system.  The scan-range can be output by outputting the numerical scan range determined in step\n316.  In addition, body labels can be automatically associated with the numerical values determined for the scan range and the body labels describing the scan range can be output as well.\n In an exemplary implementation of the methods of FIGS. 1 and 3, a scale-space of 4 scales was defined at isotropic resolutions of 2 mm, 4 mm, 8 mm, and 16 mm.  The set of landmarks detected included the bronchial bifurcation, left subclavian\nartery bifurcation, left common carotid artery bifurcation, brachiocephalic artery bifurcation, left kidney, right kidney, left hip bone corner, and right hip bone corner.  For the kidney center (left and right), the fine resolution was set to 4 mm,\ngiven the higher variability of the annotations in the training data.  For each scale and landmark, the network structure of the DNN was the same: convolutional layer (40 kernels: 5.times.5.times.5, rectified linear unit (ReLU)), pooling\n(2.times.2.times.2), convolutional layer (58 kernels: 3.times.3.times.3), pooling (2.times.2.times.2), and four fully connected layers (512.times.256.times.6 units, ReLU).  The following meta-parameters for training were also shared across scales and\nlandmarks: training iterations (750), episode length (1500), replay-memory size (100000), learning rate (0.0005), batch-size (128), and discount factor (0.9).  The dimensionality of the state (ROI input to the DNN) was also fixed across scales to\n25.times.25.times.25 voxels.  As described above, on all scales except M-1, the training is performed in a constrained range around the ground-truth p.sub.GT.+-.r. Depending on scale and landmark: r.di-elect cons.[-12, +12].sup.3 voxels.\n Given the trained multi-scale models for each landmark .THETA..sub.0, .THETA..sub.1, .  . . , .THETA..sub.N (N=8 in the exemplary implementation), the search starts on the lowest (coarsest) scale in the center of the scan.  Let {tilde over (P)}\nbe the output of the navigation sub-models (search models) on the coarsest scale.  Robust shape-model fitting was performed on {tilde over (P)} to eliminate outliers and correct for miss-aligned landmarks, resulting in a robust set of landmarks\n{circumflex over (P)}.  This reduced the false positive (FP) and false negative (FN) rates from around 2.5% to under 0.5%.  Applying the training range r to constrain the navigation of the subsequent scale-levels [M-2, .  . . , 0], it was empirically\nobserved that the shape-constraint was preserved and both the FP and FN rates were reduced to zero.\n The present inventors compared the method described herein to a previous landmark detection technique of Marginal Space Deep Learning (MSDL).  MSDL uses a cascade of sparse deep neural networks to scan the complete image space.  Missing\nstructures are detected in MSDL using a fixed cross-validated threshold on the hypothesis-probability.  The operating point was selected to maintain a FP-rate of less than 1.5%.  FIG. 5 illustrates a comparison of landmark detection results obtained\nusing the method of FIG. 3 and MSDL.  As shown in FIG. 5, table 500 compares the false positive (FP) rate, false negative (FN) rate, mean error, median error, and standard deviation (STD) of the error for detection of the left kidney, right kidney, left\nhip-bone, right hip-bone, left common carotid artery bifurcation, brachiocephalic artery bifurcation, left subclavian artery bifurcation, and bronchial bifurcation using the method of FIG. 3 (\"Ours\") and Marginal Deep Space Learning (\"MSDL\").  As can be\nobserved in table 500 of FIG. 5, the method of FIG. 3 significantly outperforms the MSDL method in recognizing the presence/absence of structures (see FP and FN rates).  In terms of accuracy, the improvement using the method of FIG. 3 averages 20%-30% on\nall landmarks except the kidneys.  The increased relative performance of the MSDL method on detecting the kidney centers may be explained by the high FN rate as well as the robust candidate aggregation used in the MSDL method, which accounts for the high\nvariability of the kidney center annotations.\n For the method of FIG. 3, learning the multi-scale search trajectory leads to a runtime for anatomical landmark detection fast enough for real-time detection.  With an average speed of 35-40 ms per landmark, the method described herein is 15-20\ntimes faster than the MSDL method, which achieved and average speed of around 0.8 seconds.  The capability to reliably detect landmarks in real-time in incomplete 3D medical images, such as CT, MR, Ultrasound, PET-CT, MR-PET, etc., opens new\npossibilities and applications for medical images, starting from the image formation process, inside the scanner, to fast image analysis/diagnosis used in trauma scans (when time is of the essence), and image-based guidance applications in the operating\nroom.\n The above-described methods for training an intelligent multi-scale navigation model for anatomical landmark detection and automated computer-based multi-scale anatomical landmark detection in medical images can be implemented on a computer\nusing well-known computer processors, memory units, storage devices, computer software, and other components.  A high-level block diagram of such a computer is illustrated in FIG. 6.  Computer 602 contains a processor 604, which controls the overall\noperation of the computer 602 by executing computer program instructions which define such operation.  The computer program instructions may be stored in a storage device 612 (e.g., magnetic disk) and loaded into memory 610 when execution of the computer\nprogram instructions is desired.  Thus, the steps of the methods of FIGS. 1 and 3 may be defined by the computer program instructions stored in the memory 610 and/or storage 612 and controlled by the processor 604 executing the computer program\ninstructions.  An image acquisition device 620, such as an MR scanning device, CT scanning device, ultrasound device, x-ray image acquisition device, etc., can be connected to the computer 602 to input image data to the computer 602.  It is possible to\nimplement the image acquisition device 620 and the computer 602 as one device.  It is also possible that the image acquisition device 620 and the computer 602 communicate wirelessly through a network.  In a possible embodiment, the computer 602 may be\nlocated remotely from the image acquisition device 620, and the computer 602 may perform method steps as part of a server or cloud based service.  The computer 602 also includes one or more network interfaces 606 for communicating with other devices via\na network.  The computer 602 also includes other input/output devices 608 that enable user interaction with the computer 602 (e.g., display, keyboard, mouse, speakers, buttons, etc.).  Such input/output devices 1508 may be used in conjunction with a set\nof computer programs as an annotation tool to annotate volumes received from the image acquisition device 620.  One skilled in the art will recognize that an implementation of an actual computer could contain other components as well, and that FIG. 6 is\na high level representation of some of the components of such a computer for illustrative purposes.\n The foregoing Detailed Description is to be understood as being in every respect illustrative and exemplary, but not restrictive, and the scope of the invention disclosed herein is not to be determined from the Detailed Description, but rather\nfrom the claims as interpreted according to the full breadth permitted by the patent laws.  It is to be understood that the embodiments shown and described herein are only illustrative of the principles of the present invention and that various\nmodifications may be implemented by those skilled in the art without departing from the scope and spirit of the invention.  Those skilled in the art could implement various other feature combinations without departing from the scope and spirit of the\ninvention.", "application_number": "15695112", "abstract": " A method and system for automated spatially-consistent multi-scale\n     detection of anatomical landmarks in medical images is disclosed. A\n     discrete scale-space representation of a medical image of a patient is\n     generated. A plurality of anatomical landmarks are detected at a coarsest\n     scale-level of the discrete scale-space representation of the medical\n     image using a respective trained search model trained at the coarsest\n     scale-level for each of the plurality of anatomical landmarks. Spatial\n     coherence of the detected anatomical landmarks is enforced by fitting a\n     learned robust shape model of the plurality of anatomical landmarks to\n     the detected anatomical landmarks at the coarsest scale-level to robustly\n     determine a set of the anatomical landmarks within a field-of-view of the\n     medical image. The detected landmark location for each of the landmarks\n     in the set of anatomical landmarks is refined at each remaining\n     scale-level of the discrete scale-space representation of the medical\n     image using, for each landmark, a respective trained search model trained\n     at each remaining scale-level and constrained based on the predicted\n     landmark location at a previous scale-level.\n", "citations": ["20070269111", "20140219526", "20140294276", "20170323447", "20170372473", "20180061058", "20180330207"], "related": ["62466036"]}, {"id": "20180260498", "patent_code": "10372859", "patent_name": "System and method for designing system on chip (SoC) circuits using single\n     instruction multiple agent (SIMA) instructions", "year": "2019", "inventor_and_country_data": " Inventors: \nNagaraja; Nagendra (Bangalore, IN)  ", "description": "BACKGROUND\nTechnical Field\n The embodiments herein is related to apparatuses/systems for designing Integrated Circuits (IC).  The embodiments herein is particularly related to apparatuses/systems for designing IC such as system on chip (SoC).  The embodiments herein is\nmore particularly related to a system and method for designing system on chip (SoC) using artificial intelligence (AI) and reinforcement learning processor.\nDescription of Related Art\n System on Chip (SOC) is an integrated circuit (IC) that integrates all the components of an electrical/electronic system into a single chip.  The integrated circuit typically comprises digital functions, analog functions, mixed-signal functions\nand radio-frequency functions inter-alia embedded onto a single chip substrate.  The integrated circuit typically includes hardware (for example, microprocessors, and microcontrollers) and also the software necessary for controlling the functionalities\nand implementation of the hardware.\n Typically, SoCs are developed from pre-qualified hardware blocks corresponding to the hardware elements to be included there into, and along with the software drivers that control the functionalities of the said hardware elements.  Typically,\nthe hardware elements are assembled using well known Computer Aided Design (CAD) tools, and the corresponding software modules are integrated using an integrated software development environment subsequent to the finalization of SoC circuit architecture.\n The design flow for typical SoC circuits are fragmented and implemented manually.  Further, the process of converting a design flow (of a SoC circuit) into a physical design is not free from (design related) errors due to a manual implementation\nand fragmented nature (of the corresponding design flow).  Typically, functional verification (for determining whether the logic design of the SoC circuit confirms to the design specification), which forms a critical part of the transformation from\ndesign flow (of a SoC circuit) to physical design, demands the largest pool of resources (in terms of time, manpower inter-alia), due to the complexity of the SoC circuit design as well as the enormity of possible test cases/test scenarios necessary to\ncompletely verify the SoC circuit design in its entirety.  Typically, the design phase of a SoC circuit involves use of a plurality of discrete tools along with the use of multiple formats and implementation procedures.\n Current SoC design process includes the steps of receiving a system specification, architectural design, functional and logic design, circuit design, physical design, physical verification, and fabrication of chip.  Further, each process\ngenerates data that are highly correlated.  At present, there is no system and method available and exist for generalization and learnability of data generated in chip designing process.\n The existing process of SoC chip design does not teach about implementing an AI framework into the manual chip design process.  AI framework helps in automating the process of chip design, thereby making the design process quick, speedy and\nefficient.  Further, the AI framework helps in learning the data generated during the chip design process.\n The advanced AI based framework for chip design is not supported by general purpose computer based on RISC and GPGPU and even FPGAs.  Further, existing hardware do not provide Application specific Instructions configured to perform SoC chip\ndesign.\n Hence, there is a need for an improvised system for achieving SoC design circuits, apart from mitigating the risks associated with SoC circuit design process and rendering the (SoC circuit) design process cost effective.  Further, there is a\nneed for a system that automates the process of SoC design using application specific instructions, thereby improving efficiency due to the complexities associated with the design and implementation of SoC circuits.  Furthermore, there exists a need for\nan advanced system that supports the AI based chip design framework.\n The abovementioned shortcomings, disadvantages and problems are addressed herein, which will be understood by reading and studying the following specification.\nOBJECTIVES OF THE EMBODIMENTS\n An object of the embodiments herein is to provide a system with AI processor implementing SoC design framework.\n Another object of the embodiments herein is to provide a system with application specific instructions or Single Instruction Multiple Agent instructions for designing SoC.\n Yet another object of the embodiments herein SoC design framework that employs reinforcement learning techniques to improve the design and implementation of SoC circuit design.\n Yet another object of the embodiments herein is to provide an artificial intelligence based framework optimized for SoC design and implementation.\n Yet another object of the embodiments herein is to provide a method for learning and inferencing from SoC design that can be used for future designing of SoC.\n Yet another object of the embodiments herein is to provide a SoC design framework that automates the process of SoC design and implementation.\n Yet another object of the embodiments herein is to provide a SoC design framework by synchronizing a hierarchy of SMDPs.\n Another object of the embodiments herein is to provide a SoC design framework that is flexible and modular in construction.\n Yet another object of the embodiments herein is to provide a SoC design framework that reduces a risk and uncertainty associated with SoC design and implementation process.\n Yet another object of the embodiments herein is to provide a SoC design framework that generates optimal SoC configurations.\n Yet another object of the embodiments herein is to provide a SoC design framework that provides easy insights on the architecture of a SoC circuit, and enables easy customization of SoC circuits.\n These and other objects and advantages of the present invention will become readily apparent from the following detailed description taken in conjunction with the accompanying drawings.\nSUMMARY\n The present disclosure discloses a processor (termed as `reinforcement learning processor` hereafter) specifically configured to design SoC.  In accordance with the present disclosure, the application-specific instruction set executed by the\nreinforcement learning processor incorporates `Single Instruction Multiple Agents (SIMA)` type instructions.  SIMA type instructions when executed by the reinforcement learning processor are specifically designed to be implemented simultaneously on a\nplurality of reinforcement learning agents which in turn are interacting with corresponding reinforcement learning environments.  The SIMA type instructions when executed by the reinforcement learning processor are configured to create a plurality of\ndomains and a plurality of subdomains for artificial intelligence (AI) setup to generate chip specific graph library.\n According to an embodiment herein, a computer-implemented system for designing SoC, includes a first processor configured to create at least one reinforcement learning agent and at least one corresponding reinforcement learning environment, said\nfirst processor further configured to assign a reinforcement learning agent ID to said reinforcement learning agent, and a reinforcement learning environment ID to said reinforcement learning environment.  The system further includes a first memory\nmodule communicably coupled to said first processor, said first memory module configured to store an application-domain specific instruction set (ASI).  SoC specification input and a chip database library.  The application-domain specific instruction set\nincludes instructions for initializing a chip topology from the SoC specification, the application-domain specific instruction set further configured to create a plurality of domains and a plurality of subdomains for artificial intelligence (AI) setup to\ngenerate chip specific graph library, each of said instructions incorporating at least one of said reinforcement learning agent ID and said reinforcement learning environment ID as an operand.  The system includes a complex instruction fetch and decode\n(CISFD) unit communicably coupled to said memory module.  The CISFD unit configured to decode at least one of said instructions, and generate a plurality of predetermined threads corresponding to decoded instruction.  The CISFD unit still further\nconfigured to embed into the predetermined threads, at least one of said reinforcement learning agent ID and reinforcement learning environment ID associated with the decoded instruction.  The system includes a second processor configured to initiate an\ninteraction between the reinforcement learning agent and the reinforcement learning agent environment.  The second processor includes a plurality of cores, and configured to apply a corresponding instruction onto at least one of a reinforcement learning\nagent and reinforcement learning environment identified by the reinforcement learning agent ID and reinforcement learning environment ID to map each of the SoC sub domains from the plurality of SoC sub domains to a combination of environment, rewards and\nactions.  The second processor is further configured to generate Q values for each domain using the reinforcement learning agent.  The system further includes a second memory module cooperating with said second processor.  The second memory module is an\non-chip memory, external DRAM, hard disk drive.  The second memory module is configured to store the extracted Q values of each domain and subdomain in a hierarchical SMDP structure in a form of SMDP Q table for optimization for planning SoC.  According\nto an embodiment herein, the second processor is further configured to acquire optimal chip architecture for designing SoC using application-domain specific instruction set (ASI).  The application-domain specific instruction set are single instruction\nmultiple agents (SIMA) based instructions configured to map reinforcement learning environment to Electronic design automation EDA tools associated with the corresponding AI SoC subdomain.  A plurality of reinforcement learning agents are configured to\ninteract with the reinforcement learning environment for a predefined number of times and update Q values, V values, R value, and A value in the second memory module.\n According to an embodiment herein, the second processor is further configured to create an empty environment corresponding to the reinforcement learning environment ID.  The second processor is also configured to create an empty agent within the\nreinforcement learning environment denoted by the reinforcement learning environment ID.  The plurality of reinforcement learning agents are associated to at least one reinforcement learning environment.  Further, a training is initiated on the\nreinforcement learning agent represented by the reinforcement learning agent ID by using exploration instruction.  Thereafter, second processor associates the chip topology to a Markov Decision Process (MDP) with HAM constraints, second processor\nbranches the chip topology to a plurality of SMDPs or MDPs.  The second processor activates the plurality of SMDPs or MDPS, and wherein the plurality of activated SMDPs or MDPs is terminated on achieving a preset Q-value or a preset objective.  Further,\nthe second processor synchronizes the plurality of SMDPs or MDPs after termination.  Further, second processor activates a plurality of subdomains at a same level, and wherein the plurality of activated subdomains is terminated on achieving a preset\nQ-value or a preset objective.  The second processor synchronizes the plurality of subdomains after termination; and initiates a physical verification of the plurality of SMDPs or MDPs.\n According to an embodiment herein, the method of SoC chip design includes receiving a SoC specification input from a first memory module.  Further, the chip design (of SoC) is initialized by extracting details regarding chip, chip skeleton,\nclock, Input outputs, partitions that are retrieved from the received SoC specification input and a chip database library.  Subsequently, a plurality of domains and a plurality of subdomains is created by artificial intelligence (AI) setup in form of\nMarkov Decision Process (MDP), Semi Markov Decision Process (SMDP)s, Hierarchical Abstract Machines (HAM)s and MAX-Q Q using application specific instruction set to generate chip specific graph library.  The artificial intelligence setup comprises a\ncombination of reinforcement learning (AI) agent, a reinforcement learning environment, and a task.  The AI setup is created using application specific instructions retrieved from a first memory module.  Thereafter, an interaction is initiated between\nthe reinforcement learning agent created and the reinforcement learning environment using the application specific instructions.  Each of the SoC sub domains from the plurality of SoC sub domains is mapped to a combination of environment, rewards and\nactions by a second processor.  Mapping is performed on executing an instruction with a reinforcement learning environment ID by the second processor.  The Q values for each domain are generated using the reinforcement learning agent.  The AI agent is\nconfigured to interact with AI environment through task to extract Q values for each domain.  Thereafter, the reinforcement learning environment is mapped to respective Electronic design automation EDA tools associated with the corresponding AI SoC\nsubdomain.  Sequentially, an interaction of a plurality of agents is initiated with the reinforcement learning environment for a predefined number of times and further Q value, V value, R value, and A value is updated in the second memory module. \nThereby, an optimal chip architecture for designing SoC is acquired using application-domain specific instruction set (ASI).  The optimal chip architecture corresponds to a maximum Q value of a top level in a SMDP Q table.\n According to an embodiment herein, an empty environment corresponding to the reinforcement learning environment ID is created.  Further, an empty agent within the reinforcement learning environment denoted by the reinforcement learning\nenvironment ID is created.  Thereafter, the plurality of reinforcement learning agents is associated to at least one reinforcement teaming environment.  Subsequently, training is initiated on the reinforcement learning agent represented by the\nreinforcement learning agent ID by using exploration instruction.  Further, the chip topology is associated to a Markov Decision Process (MDP) with HAM constraints.  The chip topology is branched to a plurality of SMDPs or MDPs.  The plurality of SMDPs\nor MDPS are activated.  Further, the plurality of activated SMDPs or MDPs is terminated on achieving a preset Q-value or a preset objective.  The plurality of SMDPs or MDPs are synchronized after termination.  A plurality of subdomains at a same level is\nactivated.  Further, the plurality of activated subdomains is terminated on achieving a preset Q-value or a preset objective.  Thereafter, the plurality of subdomains are synchronized after termination.\n According to an embodiment herein, a backup of each of the plurality of subdomains hierarchically through Hierarchical Deep SMDP network (HDSN).  The step of storing the extracted Q values comprises storing Q values in a Q table for leaf level\nMDPs and options macros.\n According to an embodiment herein, in order to achieve the preset Q-value the reinforcement learning techniques is executed and terminated.  The reinforcement learning techniques is executed and terminated, when Q value converges below a preset\nthreshold value for each domain or when a change of Q value is reduced below the preset threshold value for each SoC domain.  Alternatively, the reinforcement learning techniques is executed and terminated, when a predefined number of episodes is\ncompleted for each specific domain.  According to an embodiment herein, the reinforcement learning techniques is executed and terminated, when a predefined target performance or predefined exit state is reached for each specific domain.  The\nreinforcement learning techniques is executed and terminated, when a predefined hierarchically optimal state for each domain is achieved.\n According to an embodiment herein, the AI agents are configured to feedback the inferences into the graph database and a database which hold Q-values and neural network weights.  Examples of AI Agent include but not limited to SoC Noc agent, SoC\nPower agent, SoC Physical Agent, SoC multiprocessor agent, SoC security agent and SoC clock agent.  The AI agent (hereafter referred to as AI agent) is configured to generate Q values for each domain.  The AI agent is configured to interact with AI\nenvironment through task to extract Q values for each domain.  Subsequently, the extracted Q values of each domain and subdomain is stored in a hierarchical SMDP structure is stored in a form of SMDP Q table in a big data database for optimization for\nplanning SoC.\n According to an embodiment herein, reinforcement learning process is initiated for each domain to extract Q value.  Subsequently, a maximum Q value for each domain is determined through an initialization process of any one of MDPs, SMDPs, and a\nhierarchy of MDPs.  Thereafter, an optimal chip architecture is estimated corresponding to a maximum Q value of a top level in the SMDP Q table.  Thereafter, the optimal chip architecture is stored in a database for learning and inference.  The desired\nSoC configuration is generated and optimized based on the optimal chip architecture and the generated chip specific graph library.  The step of acquiring an optimal chip architecture from the maximum Q value comprises deriving optimal policies associated\nwith the optimal Q value, and executing actions associated with the derived optimal policy.\n According to an embodiment herein, a relation between lower level SMDP, MDPs and options macro Q values is determined using the Deep neural network, when Q values of upper level in the Q tables are calculated.  The lower level SMDP, MDPs and\noptions macro Q values are related to Q values of the upper level in Q tables.  The Deep neural network is selected from a group consisting of a recurrent network, convolutional network, LSTM, GANs and hierarchy of these networks.  A Hierarchical Deep\nSMDP Network (HDSN) is trained or employed for acquiring optimal value of weights for estimating a relationship function between the pluralities of Q values in the Q tables.  The optimal values of weights obtained by training are used to determine a\nprocess of estimating higher level Q values as a function of lower level Q values in a hierarchical RL setups.  The HDSN is further trained or employed to determine a synchronization mechanism between SMDPS at the same level in the hierarchical RL\nimplementation, by determining appropriate level of threshold Q values to terminate one Semi Markov Decision Process (SMDP) and start another SMDP in tandem.  According to an embodiment herein, the step of acquiring an optimal chip architecture from the\nmaximum Q value comprises deriving optimal policies associated with the optimal Q value, and executing actions associated with the derived optimal policy.  Further, a relation between lower level SMDPs, and MDPs is determined using a neural network, when\nQ values of upper level in the Q tables are calculated.\n According to an embodiment herein, the plurality of domains comprises SoC Communication and Network on a Chip, SoC power optimization, SoC clocking, SoC physical design, SoC logic verification, SoC physical verification, SoC timing, SoC DRC\n(Design Rule check), SoC ERC (Electric Rule check), and SoC Package.  The plurality of sub domains comprises SoC throughput optimization, SoC dynamic power optimization, SoC timing optimization, SoC placement optimization, SoC safety features, SoC\nsecurity features, placement, floorplan, Input/Outputs and routing.\n According to an embodiment herein, the step of creating a plurality of domains and a plurality of subdomains AI setup includes configuring the plurality of agents with tasks associated to domains and sub domains.  Further, the AI environment is\nconfigured to provide rewards and observation to a one or more AI agents.  Furthermore, the one or more AI agents are configured to receive a feedback regarding the plurality of domains and sub domains.\n According to an embodiment herein, the step of optimizing and generating desired SoC configuration includes initiating a planning process for designing Optimal SoC based on hierarchically optimal SMPDs, MDPs or option macros or a combination of\nall.  Further, a model of MDP, SMDP and options is created using a deterministic or stochastic model selected from a group consisting of lookup table model, Linear expectation model, Linear Gaussian model.  Gaussian process model, and Deep Belief network\nmodel.  Furthermore, a planning and learning process is executed for a plurality of AI domains in parallel by using simulated models that are derived based on real time feedback received from the plurality of AI domains and subdomains.  The hierarchical\nrepresentation of planned SoC is stored in the one or more databases, by storing the learnt information about SoC in a preset format.  Thereafter, the planned and stored information about SoC is retrieved from a database hierarchically to debug and\ncreate new SoC design from the database.\n According to an embodiment herein, inferences are generated from extracted SMDP, options macros, MDPs, HAMs, max-Q functions.  The generated inferences are stored in the form of document databases, Graph databases or SQL databases.  The stored\ninferences in the form of document databases, Graph databases or SQL databases are loaded into domain specific AI setup in the form of extracted MDPs, SMDPs or options macro or as a set of Environment, Agent, task and experiment in an episodic setup. \nEach SoC domain AI setup in the form of episodic experiment setup or MDP or SMDP or HAM or MAXQ setup with or without options macros interact with each other to arrive at Optimal Chip Architecture (OCA), when the inferences for each SoC design domain are\nextracted.\n According to an embodiment herein, new policies of mapping state to action are validated for extracted MDPs, SMDPs, options macro, HAMs MAXQ based on policy gradient methods used in Reinforcement learning and storing the validated new policies\nin the databases for each of AI SOC domains and any AI subdomain in SoC design.  The new policies generated in one domain is optimized and iterated with the policies generated in other AI SoC domains to derive context and to obtain hierarchically\noptimal, multi-domain wise, and subdomain wise optimal policies.  The hierarchically optimal and individually optimal policies are stored in the database, through equations, functions, SW, options macros for future use and referred in future learning\napplications of all AI SoC domains and subdomains.\n According to an embodiment herein, synchronization is just one SMDPs communicating with other SMDP, to notify that it has reached terminal state, when SMDPs have no dependency with each other (for example, active power and placement SMDPs).  The\nsynchronization is one of a global broadcast signal, to suggest that SMDP reached terminal state.  Synchronization process also involves in making the terminated SMDP to upload its end results such as metrics of time closure by timing SMDP or when bugs\nare found in verification of SMDPs into global database.  Alternatively, then each of these SMDPs operate in a lock step or iterative loop when there is a dependency of SMDP on another SMDP.  The synchronization process involves a separate sync state\nwhen SMDP communicates with other SMDPs (for example, logic Verification and logic design).  Each SMDP communicate their end results and wait for bug fixes, optimization and call that design is in decent shape to send it to PD or synthesis.  According to\nan embodiment herein, synchronization includes using two or more SMDPs and sharing end results among many SMDPs, options macro and MDPs.\n These and other aspects of the embodiments herein will be better appreciated and understood when considered in conjunction with the following description and the accompanying drawings.  It should be understood, however, that the following\ndescriptions, while indicating the preferred embodiments and numerous specific details thereof, are given by way of illustration and not of limitation.  Many changes and modifications may be made within the scope of the embodiments herein without\ndeparting from the spirit thereof, and the embodiments herein include all such modifications. BRIEF DESCRIPTION OF THE DRAWINGS\n The other objects, features and advantages will be apparent to those skilled in the art from the following description and the accompanying drawings in which:\n FIG. 1A is a block diagram of a system for designing SoC by synchronizing a hierarchy of SMDPs, according to an embodiment herein.\n FIG. 1B illustrates a flowchart explaining a method for designing SoC by synchronizing a hierarchy of SMDPs, according to an embodiment herein.\n FIG. 2 illustrates a flowchart explaining a method for SoC design using an AI driven design setup with hierarchical multi-objective AI, according to an embodiment herein.\n FIG. 3 illustrates a block diagram of an Artificial intelligence (AI) logic design set up for each domain and subdomain in the SoC design framework, according to an embodiment herein.\n FIG. 4 illustrates a functional block diagram of an AI framework implemented in in the SoC design framework, according to an embodiment herein.\n FIG. 5 illustrates a block diagram of an AI framework configured for hierarchical reinforcement learning, according to an embodiment herein.\n FIG. 6 illustrates a flowchart explaining an interaction process between main MDP and SMDP in SoC design framework, according to an embodiment herein.\n FIG. 7 illustrates a flowchart explaining a method of terminating Reinforcement Learning (RL) process on convergence of Q value for each SoC design domain in SoC design framework, according to an embodiment herein.\n FIG. 8 illustrates a flowchart explaining a method of terminating Reinforcement Learning (RL) process on completing predefined number of experiments for each SoC design domain individually in SoC design framework, according to an embodiment\nherein.\n FIG. 9 illustrates a flowchart explaining a method of terminating Reinforcement Learning (RL) process on reaching or achieving predefined target performance or target state for each SoC design domain individually in SoC design framework,\naccording to an embodiment herein.\n FIG. 10 illustrates a flowchart explaining a method of terminating Reinforcement Learning (RL) process on reaching or achieving predefined hierarchically optimal state or goal derived for each SoC design domain individually in SoC design\nframework by a top down approach, according to an embodiment herein.\n FIGS. 11A and 11B jointly illustrates a flowchart explaining a method of synchronizing a hierarchy of SMDPs in SoC design process, according to an embodiment herein.\n FIG. 12 illustrates a top view or a schematic view of a hierarchical Q-tables stored in a SQL schema or document database in SoC design process, according to an embodiment herein.\n FIG. 13 illustrates a block diagram or a schematic representation of Deep Neural network used for experience replay in a HDSN in SoC design process, according to an embodiment herein.\n FIG. 14 illustrates a functional block diagram of a system for generating an optimal Chip Architecture (OCA) using AI flow of hierarchical Reinforcement learning in SoC design process, according to an embodiment herein.\n FIG. 15 illustrates a flow chart explaining a method of extracting MDP for each domain in SoC design process using a reinforcement learning process, according to an embodiment herein.\n FIG. 16 illustrates a functional block diagram of a Big data framework in SoC design process, according to an embodiment herein.\n FIG. 17 illustrates a flowchart explaining a method of abstracting SoC domains and subdomains in SoC design process, according to an embodiment herein.\n FIG. 18 illustrates a block diagram of a database architecture for SoC design, according to an embodiment herein.\n Although the specific features of the embodiments herein are shown in some drawings and not in others.  This is done for convenience only as each feature may be combined with any or all of the other features in accordance with the embodiments\nherein.\nDETAILED DESCRIPTION OF THE EMBODIMENTS HEREIN\n In the following detailed description, a reference is made to the accompanying drawings that form a part hereof, and in which the specific embodiments that may be practiced is shown by way of illustration.  These embodiments are described in\nsufficient detail to enable those skilled in the art to practice the embodiments and it is to be understood that other changes may be made without departing from the scope of the embodiments.  The following detailed description is therefore not to be\ntaken in a limiting sense.\n The present disclosure discloses a processor (termed as `reinforcement learning processor` hereafter) specifically configured to design SoC.  In accordance with the present disclosure, the application-specific instruction set executed by the\nreinforcement learning processor incorporates `Single Instruction Multiple Agents (SIMA)` type instructions.  SIMA type instructions are specifically designed to be implemented simultaneously on a plurality of reinforcement learning agents which in turn\nare interacting with corresponding reinforcement learning environments.  The SIMA type instructions are configured to create a plurality of domains and a plurality of subdomains for artificial intelligence (AI) setup to generate chip specific graph\nlibrary.\n According to an embodiment herein, the SIMA type instructions are specifically configured to receive either a reinforcement learning agent ID or a reinforcement learning environment ID as the operand.  The reinforcement learning agent ID (RL\nagent ID) corresponds to a reinforcement learning agent, while the reinforcement learning environment ID (RL environment ID) corresponds to a reinforcement learning environment (with which the reinforcement learning agent represented by reinforcement\nlearning agent ID interacts).  The SIMA type instructions envisaged by the present disclosure, when executed by the reinforcement learning processor trigger an interaction between the reinforcement learning agent and reinforcement learning environment to\nderive values corresponding to an optimal chip design.\n The SoC design framework is embedded within the SIMA based processor and configured to adapt and implement reinforcement learning techniques.  The SoC design framework disclosed in the embodiments herein is designed to reduce the complexity in\nthe decision making process associated with design of SoC circuits.  The decisions associated with the designing of SoC circuits are multitude and typically heterogeneous in nature.  The SoC design framework of the embodiments herein is configured to\ngeneralize the multitude of heterogeneous decisions into a plurality of generalized, homogenous decisions, which are in turn utilized to finalize and implement the design for the SoC circuits.  The SoC design framework disclosed in the embodiments herein\nis designed to utilize a combination of artificial intelligence and reinforcement learning principles to arrive at optimal decisions regarding the design and implementation of SoC circuits.\n According to an embodiment herein, a computer-implemented system for designing SoC, includes a first processor configured to create at least one reinforcement learning agent and at least one corresponding reinforcement learning environment, said\nfirst processor further configured to assign a reinforcement learning agent ID to said reinforcement learning agent, and a reinforcement learning environment ID to said reinforcement learning environment.  The system further includes a first memory\nmodule communicably coupled to said first processor, said first memory module configured to store an application-domain specific instruction set (ASI), SoC specification input and a chip database library.  The application-domain specific instruction set\nincludes instructions for initializing a chip topology from the SoC specification, the application-domain specific instruction set further configured to create a plurality of domains and a plurality of subdomains for artificial intelligence (AI) setup to\ngenerate chip specific graph library, each of said instructions incorporating at least one of said reinforcement learning agent ID and said reinforcement learning environment ID as an operand.  The system includes a complex instruction fetch and decode\n(CISFD) unit communicably coupled to said memory module.  The CISFD unit configured to decode at least one of said instructions, and generate a plurality of predetermined threads corresponding to decoded instruction.  The CISFD unit still further\nconfigured to embed into the predetermined threads, at least one of said reinforcement learning agent ID and reinforcement learning environment ID associated with the decoded instruction.  The system includes a second processor configured to initiate an\ninteraction between the reinforcement learning agent and the reinforcement learning agent environment.  The second processor includes a plurality of cores, and configured to apply a corresponding instruction onto at least one of a reinforcement learning\nagent and reinforcement learning environment identified by the reinforcement learning agent ID and reinforcement learning environment ID to map each of the SoC sub domains from the plurality of SoC sub domains to a combination of environment, rewards and\nactions.  The second processor is further configured to generate Q values for each domain using the reinforcement learning agent.  The system further includes a second memory module cooperating with said second processor.  The second memory module is an\non-chip memory, external DRAM, hard disk drive.  The second memory module is configured to store the extracted Q values of each domain and subdomain in a hierarchical SMDP structure in a form of SMDP Q table for optimization for planning SoC.  According\nto an embodiment herein, the second processor is further configured to acquire optimal chip architecture for designing SoC using application-domain specific instruction set (ASI).  The application-domain specific instruction set are single instruction\nmultiple agents (SIMA) based instructions configured to map reinforcement learning environment to Electronic design automation EDA tools associated with the corresponding AI SoC subdomain.  The Electronic design automation (EDA), also referred to as\nelectronic computer-aided design (ECAD), is a category of software tools for designing electronic systems such as integrated circuits and printed circuit boards.  The tools, for example tools used for synthesis, simulation, design entry and physical\ndesign work together in a design flow that chip designers use to design and analyze entire semiconductor chips.  A plurality of reinforcement learning agents are configured to interact with the reinforcement learning environment for a predefined number\nof times and update Q values, V values, R value, and A value in the second memory module.\n In accordance with an exemplary embodiment of the present disclosure, the SIMA type instructions when executed by the reinforcement processor, trigger a reinforcement learning agent to interact with a corresponding reinforcement learning\nenvironment and further enable the reinforcement learning agent to explore the reinforcement learning environment and deduce relevant learnings from the reinforcement learning environment.  Additionally, SIMA type instructions also provide for the\ndeduced learnings to be iteratively applied onto the reinforcement learning environment to deduce furthermore learnings therefrom.\n Further, the SIMA type instructions when executed by the reinforcement learning processor, also enable the reinforcement learning agent to exploit the learnings deduced from any previous interactions between the reinforcement learning agent and\nthe reinforcement learning environment.  Further, the SIMA type instructions also enable the reinforcement learning agent to iteratively exploit the learnings deduced from the previous interactions, in any of the subsequent interactions with the\nreinforcement learning environment.  Further, the SIMA type instructions also provide for construction of a Markov Decision Process (MDP) and a Semi-Markov Decision Process (SMDP) based on the interaction between the reinforcement learning agent and the\ncorresponding reinforcement learning environment.\n Further, the SIMA type instructions also enable selective updating of the MDP and SMDP, based on the interactions between the reinforcement learning agent and the corresponding reinforcement learning environment.  The SIMA type instructions,\nwhen executed by the reinforcement learning processor, also backup the MDP and SMDP.  Further, the SIMA type instructions when executed on the reinforcement learning agent, enable the reinforcement learning agent to initiate a Q-learning procedure, and a\ndeep-learning procedure and also to associate a reward function in return for the Q-learning and the deep-learning performed by the reinforcement learning agent.\n Further, the SIMA type instructions, upon execution by the reinforcement learning processor, read and analyze the `learning context` corresponding to the reinforcement learning agent and the reinforcement learning environment.  Further, the SIMA\ntype instructions determine an optimal Q-value corresponding to a current state of the reinforcement learning agent, and trigger the reinforcement learning agent to perform generalized policy iteration, and on-policy and off-policy learning methods. \nFurther, the SIMA type instructions, upon execution, approximate a state-value function and a reward function for the current state of the reinforcement learning agent.  Further, the SIMA type instructions, when executed by the reinforcement learning\nprocessor, train at least one of a deep neural network (DNN) and a recurrent neural network (RNN) using a predetermined learning context, and further trigger the deep neural network or the recurrent neural network for approximating at least one of a\nreward function and state-value function corresponding to the current state of the reinforcement learning agent.\n Referring to FIG. 1A, there is shown a block diagram illustrating the components of the system 100 for implementing the tasks/operations pertinent to reinforcement learning.  The system 100, as shown in FIG. 1A includes a first memory module 10\n(preferably an IRAM).  The first memory module stores the application-specific instruction set (ASI), which incorporates the SIMA instructions (referred to as `instructions` hereafter) for performing predetermined reinforcement learning tasks.  The\ninstructions, as described in the above paragraphs, are configured to incorporate either a reinforcement learning agent ID or a reinforcement learning environment ID as the operand.  The reinforcement learning agent ID represents a reinforcement learning\nagent (not shown in figures) trying to achieve a predetermined goal in an optimal manner by the way of interacting with a reinforcement learning environment (represented by reinforcement learning environment ID).  Each of the instructions stored in the\nfirst memory module 10 are linked to corresponding `opcodes`.  The `opcodes` corresponding to each of the instructions are also stored in the first memory module 10.  Further, the first memory module 10 also stores the reinforcement learning agent ID and\nreinforcement learning environment ID corresponding to each of the reinforcement learning agents and the reinforcement learning environments upon which the instructions (of the application-specific instruction set) are to be implemented.\n The system 100 further includes a Complex Instruction Fetch and Decode (CISFD) unit 12 communicably coupled to the first memory module 10.  The CISFD unit 12 fetches from the first memory unit 10, an instruction to be applied to a reinforcement\nlearning agent or a reinforcement learning environment.  Subsequently, the CISFD retrieves the `opcode` corresponding to the fetched instruction, from the first memory module 10.  As explained earlier, the instruction fetched by the CISFD unit 12\nincorporates at least one of a reinforcement learning agent ID and a reinforcement learning environment ID as the operand.  Depending upon the value of the operand, the CISFD unit 12 determines the reinforcement learning agent/reinforcement learning\nenvironment on which the fetched instruction is to be implemented.\n Subsequently, the CISFD unit 12, based on the `opcode` and `operand` corresponding to the fetched instruction, generates a plurality of predetermined threads, namely a `v-thread`, `a-thread`, `q-thread` and an `r-thread`, corresponding to the\nfetched instruction.  The threads generated by the CISFD unit 12 are representative of the characteristics of either the reinforcement learning agent or the reinforcement learning environment or both, upon which the fetched instruction is executed.  The\ncharacteristics represented by the predetermined threads include at least the action(s) performed by the reinforcement learning agent at every state, value associated with each state of the reinforcement learning agent, reward(s) gained by the\nreinforcement learning agent during the interaction with the reinforcement learning environment.  In order to associate each of the threads with the corresponding reinforcement learning agent/reinforcement learning environment, the operand of the\ninstruction (the instruction for which the threads are created) is embedded into the v-thread, a-thread, q-thread and r-thread.\n In accordance with the present disclosure, the `v-thread` upon execution determines the `state-value functions` corresponding to each state of the reinforcement learning agent.  The `state-value functions` indicate the `value` associated with\neach of the states of the reinforcement learning agent.  Similarly, the `a-thread` upon execution determines the `actions` performed by the reinforcement learning agent in every state thereof, and subsequently generates `control signals`, for\nimplementing the `actions` associated with the reinforcement learning agent.  Similarly, the `q-thread` upon execution determines `Q-values` which are generated using a state-action function representing the actions performed by the reinforcement\nlearning agent at every corresponding state.  Similarly, the `r-thread` on execution determines the rewards obtained by the reinforcement learning agent for performing a specific action while being in a specific state.\n In accordance with the present disclosure, the system 100 further includes a second processor 14 (referred to as `reinforcement learning processor` hereafter) specifically configured for executing the instructions embodied in the\napplication-specific instruction set (ASI), and for implementing the reinforcement tasks represented by the said instructions to design SoC.  The reinforcement learning processor 14 executes the instruction fetched by the CISFD unit 12, by the way of\nexecuting the corresponding v-thread, a-thread, q-thread and r-thread.  The reinforcement learning processor 14 is preferably a multi-core processor comprising a plurality of processor cores.\n According to an embodiment herein, a system with reinforcement learning processor for designing SoC is disclosed.  The system includes the first processor or host processor configured to create at least one reinforcement learning agent and at\nleast one corresponding reinforcement learning environment, said first processor further configured to assign a reinforcement learning agent ID to said reinforcement learning agent, and a reinforcement learning environment ID to said reinforcement\nlearning environment.  The system further includes a first memory module communicably coupled to said first processor, said first memory module configured to store an application-domain specific instruction set (ASI), SoC specification input and a chip\ndatabase library.  The application-domain specific instruction set includes instructions for initializing a chip topology from the SoC specification, the application-domain specific instruction set further configured to create a plurality of domains and\na plurality of subdomains for artificial intelligence (AI) setup to generate chip specific graph library, each of said instructions incorporating at least one of said reinforcement learning agent ID and said reinforcement learning environment ID as an\noperand.  The system includes a complex instruction fetch and decode (CISFD) unit communicably coupled to said memory module.  The CISFD unit configured to decode at least one of said instructions, and generate a plurality of predetermined threads\ncorresponding to decoded instruction.  The CISFD unit still further configured to embed into the predetermined threads, at least one of said reinforcement learning agent ID and reinforcement learning environment ID associated with the decoded\ninstruction.  The system includes a second processor or reinforcement learning processor configured to initiate an interaction between the reinforcement learning agent and the reinforcement learning agent environment.  The second processor includes a\nplurality of cores, and configured to apply a corresponding instruction onto at least one of a reinforcement learning agent and reinforcement learning environment identified by the reinforcement learning agent ID and reinforcement learning environment ID\nto map each of the SoC sub domains from the plurality of SoC sub domains to a combination of environment, rewards and actions.  The second processor is further configured to generate Q values for each domain using the reinforcement learning agent.  The\nsystem further includes a second memory module cooperating with said second processor.  The second memory module is an on-chip memory, external DRAM, hard disk drive.  The second memory module is configured to store the extracted Q values of each domain\nand subdomain in a hierarchical SMDP structure in a form of SMDP Q table for optimization for planning SoC.  According to an embodiment herein, the second processor is further configured to acquire optimal chip architecture for designing SoC using\napplication-domain specific instruction set (ASI).  The application-domain specific instruction set are single instruction multiple agents (SIMA) based instructions configured to map reinforcement learning environment to Electronic design automation EDA\ntools associated with the corresponding AI SoC subdomain.  A plurality of reinforcement learning agents are configured to interact with the reinforcement learning environment for a predefined number of times and update Q values, V values, R value, and A\nvalue in the second memory module.\n According to an embodiment herein, the second processor is further configured to create an empty environment corresponding to the reinforcement learning environment ID.  The second processor is also configured to create an empty agent within the\nreinforcement learning environment denoted by the reinforcement learning environment ID.  The plurality of reinforcement learning agents are associated to at least one reinforcement learning environment.  Further, a training is initiated on the\nreinforcement learning agent represented by the reinforcement learning agent ID by using exploration instruction.  Thereafter, second processor associates the chip topology to a Markov Decision Process (MDP) with HAM constraints, second processor\nbranches the chip topology to a plurality of SMDPs or MDPs.  The second processor activates the plurality of SMDPs or MDPS, and wherein the plurality of activated SMDPs or MDPs is terminated on achieving a preset Q-value or a preset objective.  Further,\nthe second processor synchronizes the plurality of SMDPs or MDPs after termination.  Further, second processor activates a plurality of subdomains at a same level, and wherein the plurality of activated subdomains is terminated on achieving a preset\nQ-value or a preset objective.  The second processor synchronizes the plurality of subdomains after termination; and initiates a physical verification of the plurality of SMDPs or MDPs.\n In accordance with the present disclosure, each of the processor cores of the reinforcement learning processor 14 incorporate at least `four` execution units (FIG. 1A describes a processor core 140 having `four` execution units 140A, 140B, 140C\nand 140D).  The threads, i.e., v-thread, a-thread, q-thread and r-thread are preferably assigned to individual execution units of a processor core respectively, thereby causing the threads (v-thread, a-thread, q-thread and r-thread) to be executed in\nparallel (simultaneously).  The reinforcement learning processor 14, based on the operand associated with the fetched instruction, determines the reinforcement learning agent or the reinforcement learning environment upon which the threads (i.e.,\nv-thread, a-thread, q-thread and r-thread) are to be executed.  In an example, the reinforcement learning processor 14 executes the v-thread, a-thread, q-thread and r-thread on a reinforcement learning agent identified by corresponding reinforcement\nlearning agent ID, and determines at least one `state-value function`, at least one `action`, at least one `Q-value`, and at least one `rewards corresponding to the reinforcement learning agent identified by the reinforcement learning agent ID.\n The `state-value function`, `action`, `Q-value` and `reward` thus determined by the reinforcement learning processor 14 are stored in a second memory module 16.  In accordance with the present disclosure, the second memory module 16 is\npreferably bifurcated into at least `four` memory partitions, namely, an `a-memory module` 16A, a `v-memory module` 16B, a `q-memory module` 16C, and an `r-memory module` 16D.  The `a-memory module` 16A stores the information corresponding to the actions\nperformed by the reinforcement learning agent (identified by the reinforcement learning agent ID) at every state.  The actions are stored on the `a-memory module` 16A in a binary encoded format.\n The `v-memory module` 16B stores the `state-value functions` indicative of the value associated with every state of the reinforcement learning agent (identified by the reinforcement learning agent ID) while the reinforcement learning agent\nfollows a predetermined policy.  The `v-memory module` 16B also stores the `optimal state-value functions` indicative of an optimal state-value associated with the reinforcement learning agent under an optimal policy.  Further, the `q-memory module` 16C\nstores `Q-values` which are generated using a state-action function representative of a correlation between the actions performed by the reinforcement learning agent at every state and under a predetermined policy.  The `q-memory module` 16C also stores\nthe `optimal Q-value` for every state-action pair associated with the reinforcement learning agent, and adhering to an optimal policy.  The term `state-action function` denotes the action performed by the reinforcement learning agent at a specific state. Further, the `r-memory module` 16D stores the `rewards` (reward values) obtained by the reinforcement learning agent, in return for performing a specific action while being in a specific state.\n Subsequently, the reinforcement learning processor 14 selectively retrieves the `state-value functions`, `actions`, `Q-values` and `rewards` corresponding to the reinforcement learning agent (and indicative of the interaction between the\nreinforcement learning agent and the reinforcement learning environment) from the `a-memory module` 16A, `v-memory module` 16B, `q-memory module` 16C, and `r-memory module` 16D respectively, and transmits the retrieved `state-value functions`, `actions`,\n`Q-values` and `rewards` to a neural network (illustrated in FIG. 7A) via a corresponding neural network data path 18.  Subsequently, the reinforcement learning processor 14 trains the neural network to approximate reward functions that in turn associate\na probable reward with the current state of the reinforcement learning agent, and also with the probable future states and future actions of the reinforcement learning agent.  Further, the reinforcement learning processor 14 also trains the neural\nnetwork to approximate state-value functions that in turn approximate a probable value for all the probable future states of the reinforcement learning agent.\n In accordance with an embodiment of the present disclosure, the reinforcement learning processor is further configured to train the reinforcement learning agents using application-domain specific instruction set (ASI), wherein the\napplication-domain specific instruction set are single instruction multiple agents (SIMA) based instructions, and wherein a plurality of agents are configured to work on a copy of same environment and work on mutually different policies in parallel, and\nwherein the plurality of agents is selected and merged in a synchronous or asynchronous manner.\n In accordance with the present disclosure, the CISFD unit 12 is configured to receive the SIMA type instructions fetched from the first memory module 10 and identify the `opcode` corresponding to the received instruction.  Subsequently, the\nCISFD unit 12 determines and analyzes the `operand` (either the reinforcement learning agent ID or the reinforcement learning environment ID) and identifies the corresponding reinforcement learning agent or the reinforcement learning environment upon\nwhich the instruction is to be executed.  Subsequently, the CISFD unit 12 converts the instruction into `a-thread`, `v-thread`, `q-thread` and `r-thread` (collectively referred to as a `thread block`).  The CISFD unit 12 also embeds the corresponding\nreinforcement learning agent ID or the reinforcement learning environment ID, so as to associate the instruction (received from the first memory module 10) with the corresponding thread block and the corresponding reinforcement learning\nagent/reinforcement learning environment.  Subsequently, each of the threads, i.e., the `a-thread`, `v-thread`, `q-thread` and `r-thread` are assigned to respective execution units of a processor core of the reinforcement learning processor 14.  In this\ncase, each of the threads are simultaneously executed by `four` execution units of the processor core.\n In accordance with an exemplary embodiment of the present disclosure, if the CISFD unit 12 fetches the instruction `optval agent ID`, then the CISFD unit 12 decodes the instruction to determine the `opcode` corresponding to the instruction, and\nsubsequently determines the function to be performed in response to the said instruction, based on the `opcode`.  Subsequently, the CISFD unit 12 triggers the creation of the `a-thread`, `v-thread`, `q-thread` and `r-thread` corresponding to the\ninstruction `optval`, and triggers the reinforcement learning processor 14 to execute the `a-thread`, `v-thread`, `q-thread` and `r-thread` as predetermined Arithmetic Logic Unit (ALU) operations.  The CISFD unit 12 instructs the reinforcement learning\nprocessor 14 to execute the `a-thread`, `v-thread`, `q-thread` and `r-thread` on the reinforcement learning agent/reinforcement learning environment identified by the `operand` (reinforcement learning agent ID/reinforcement learning environment ID).  The\nresultant of the execution of the threads are stored in `a-memory module` 16A, `v-memory module` 16B, `q-memory module` 16C, and `r-memory module` 16D respectively.\n The system 100 further includes a (scalable) scheduler module 20 that provides the second processor 14 with selective access to the neural network data paths 18.  The scheduler module 20 also controls the operations of a Digital Signal\nProcessing (DSP) unit 26, a Memory Management Unit (MMU) 28, and the software driver modules (`Hostif` drivers) 30 that facilitates communication between the reinforcement learning processor 14 and a first processor 24 (referred to as `host processor`\nhereafter).  Further, the scheduler module 20 allocates memory space in each of the aforementioned memory modules (i.e., `a-memory module` 16A, `v-memory module` 16B, `q-memory module` 16C, and `r-memory module` 16D) for the results of the execution of\nthe `a-thread`, `v-thread`, `q-thread` and `r-thread` to be written thereto.  Additionally, the scheduler module 20 is also configured to selectively prioritize the activities (exploration and exploitation) of predetermined reinforcement learning agents. Further, the scheduler module 20 also prioritizes the activities (exploration and exploitation) performed upon predetermined reinforcement learning environments.  Additionally, the scheduler module 20 selectively prioritizes the interaction between the\nreinforcement learning processor 14 and the host processor 24.\n Referring to FIG. 1 again, the `a-memory module` 16A stores information corresponding to the actions performed by the reinforcement learning agent.  The `v-memory module` 16B stores the `state-value functions` indicative of the value associated\nwith every state of the reinforcement learning agent, under a predetermined policy.  The `v-memory module` 16B also stores the `optimal state-value function` indicative of an optimal value associated with every state of the reinforcement learning agent\nunder an optimal policy.  Further, the `q-memory module` 16C stores `Q-values` which are generated using a state-action function which represents a correlation between the actions performed by the reinforcement learning agent at every state and under a\npredetermined policy.  The `q-memory module` 16C also stores the `optimal Q-values` for every state-action pair associated with the reinforcement learning agent, and adhering to an optimal policy.  The term `state-action pair` denotes the action\nperformed by the reinforcement learning agent at a specific state.  Further, the `r-memory module` 16D stores the `rewards` (reward values) obtained by the reinforcement learning agent, in return for performing a specific action while being in a specific\nstate.\n According to an embodiment, the system includes a neural network which is triggered via a corresponding neural network data path, and the determined actions, state-value function, Q-value and reward value are transferred to the neural network\nfor analysis.  Subsequently, the neural network is trained using the determined actions, state-value function, Q-value and reward value, to approximate a value-function and a reward function corresponding to either a current state or a future state of\nthe reinforcement learning agent.\n Shown herein below is a table incorporating a non-exhaustive, exemplary list of instructions for performing predetermined reinforcement learning operations.\n TABLE-US-00001 Opcode Operand Instruction (8 bit) (8, 16, 32, 64 bit) description abs 00000000 INT32 absolute add 00000001 2x INT32 Add two 32 bit intergers vapproxlin 00000010 AGENT ID 32 bit Value function approximation using 32 bit agent ID\nusing linear method and 00000011 2x INT32 Logical ADD of 2 32 bit integers brkpt 00000100 Nil Break point cls 00000101 Nil div 00000110 2xFLOAT32 Division on 32 bit floating point mv 00000111 regA, regB Move content from RegA register to RegB rmw\n00001000 Memory mapped Read modify right offset nn 00001001 Neural net ID Call Neural net, is used as a (32 bit) function to feed data and start training extoptk 00001010 32 bit Agent ID Exploitation only mode for Agent K dwnwdnnk 00001011 Neuralnet ID\nDownload weights for neural net k (32 bit), mem segment ID (32 bit) optval 00001100 Agent ID (32 bit) Optimal value function for current state of MDP of the Agent k explrk 00001101 Agent ID (32 bit) Enter into exploration mode agent k explrall 00001110\nEnv ID (32 bit) Enter into Exploration mode for all agent in a given env ID vapproxgd 00001111 Agent ID (32 bit) Approximate value function of agent k mul 00010000 2x Float 32 Multiplication of 2 32 bit floating point, result is 64 bit register not\n00010001 INT 32 Not of 32 bit integer nor 00010010 INT 32 Nor operation on 32 bit integer or 00010011 INT 32 OR operation on 32 bit integer xor 00010100 INT 32 XOR operation on 32 bit integer dnn 00010101 Dnn ID (32 bit) Call deep neural network with ID\nk dwnctxk 00010110 Core Id (32 bit), Download context to core ID k memoffet intrDNNk 00010111 Dnn ID (32 bit) Interrupt DNN with ID k plank 00011000 Agent ID (32 bit) Plan for agent k (algorithm for planning hardcoded currently) policygradk 00011001\nAgent ID (32 bit) Perform Policy gradient on Agent k bkpk 00011010 Agent ID (32 bit) Back up Agent k (currently on- policy trajectory based backup is supported) vapproxNNk 00011011 Agent ID (32 bit) Value function approximation using DNN for agent ID k\nvapproxtilek 00011100 Agent ID (32 bit) Value function approximation using tilecoding for agent ID k politerk 00011101 Agent ID (32 bit) Policy iteration thr agent ID k valiterk 00011110 Agent ID (32 bit) Value iteration for Agent ID k fetchq 00011111\nAgent ID (32 bit) Fetch q values for Agent ID k from Q memory.  Q value for last state will be fetched, with stack based access fetchv 00100000 Agent ID (32 bit) Fetch q values for Agent ID k from V memory.  V value for last state will be fetched, with\nstack based access dmar 00100001 DMA length DMA on R memory to or from (16 bit), DMA start External memory when LSB set offset (15 bit), to `0`, it is from external direction (1 bit) memory, else it is to external memory dmaa 00100010 DMA length DMA on A\nmemory to or from (16 bit), DMA start External memory, when LSB set offset (15 bit), to `0`, it is from external direction (1 bit) memory, else it is to external memory upctx 00100011 Context ID (16 bit), Upload context ID k to location upload address 16\nspecified in location ID bit flushdnnk 00100100 DNN ID (32 bit) Flush DNN ID k, clear all weights of DNN indrl 00100101 Td 00100110 Agent ID (32 bit) Temporal Difference on agent ID k for an episode dmav 00100111 DMA length DMA on V memory to or from (16\nbit), DMA start External memory, when LSB set offset (15 bit), to `0`, it is from external direction (1 bit) memory, else it is to external memory dmaq 00101000 DMA length DMA on Q memory to or from (16 bit), DMA start External memory, When LSB set\noffset (15 bit), to `0`, it is from external direction (1 bit) memory, else it is to external memory ql 00101001 Agent ID (32 bit) Initiate Q learning for Agent ID k for an Episode srl 00101010 sra 00101011 gpi 00101100 Agent ID (32 bit) Generic Policy\nIteration on Agent k gpiall 00101101 Env ID (32 bit) Generic policy iteration on all Agents in env k modelenv 00101110 Agent ID (32 bit) Model environment for agent k modelenvall 00101111 Env ID (32 bit) Model environment for all agents in env k\ntraindnnk 00110000 Training context ID Train deep neural network, with (32 bit) training context k upwdnnk 00110001 Training context ID Upload or backup of trained deep (32 bit) neural network with training context Id k drlk 00110010 Agent Id (32 bit)\nDeep rl on agent k tdlk 00110011 Agent ID (28 bit), TD Lambda on agent k Lamda length (4 bits) fetchr 00110100 Agent ID (32 bit) Fetch r values for Agent ID k from R memory.  R value for last state will be fetched, with stack based access optq 00110101\nAgent ID (32 bit) Optimal q value of current state of MDP of agent k crtenv 00110110 Env ID (32 bit) Creates empty env with env ID k crtagent 00110111 Env ID (32 bit) Create an empty agent in an env with ID k and assign 32 bit ID to agent assocrewardk\n00110111 Agent ID (32 bit), Associate reward function to reward function ID Agent ID k. Each agent can have (8 bit) upto 32 reward functions.  Most reward functions are programmable and downloaded from host and when reward function point to dNN, it can\nbe trained by data provided by host.  Reward function resides in memory, which can be retrieved to combine intrctenvk 00111000 Agent ID(32 bit), Agent k to Interact with env for N (32 bit) N times and update Q, V, A and R mem rdagentctx 00111001 Env ID\n(32 bit), Reads the agent context and Agent ID (32 bit) stores it into external memory or DRAM rdenvctx 00111010 Env ID (32 bit) Reads env context and stores it into external memory or DRAM\n According to an embodiment herein, each of the AI SoC sub domains are mapped to combination of reinforcement learning environment, corresponding observation and actions onto the reinforcement learning processor, using the application specific\ninstructions such as `crtenv`, `cragent` and `assocrewardk`.  Further, the application specific instructions, for example `intrctenvk` is configured to enable an agent `k` to interact with the environment for a predetermined number of times (N times) to\nupdate values of `Q`, `V`, `A`, and `R` in the second memory.\n FIG. 1B illustrates a flowchart explaining a method for designing SoC by using a reinforcement learning processor depicted in FIG. 1A, according to an embodiment herein.  According to the embodiments herein, the entire SoC circuit along with the\nprocessor, digital design, circuit design process is represented in the form of a hierarchical model (with emphasis on optimality) subsequent to implementation of reinforcement learning experiments.\n The method involves receiving a SoC specification input from a first memory module.  Further, the chip design (of SoC) is initialized by extracting details regarding chip, chip skeleton, clock, Input outputs, partitions that are retrieved from\nthe received SoC specification input and a chip database library.  Subsequently, a plurality of domains and a plurality of subdomains is created by artificial intelligence (AI) setup in form of Markov Decision Process (MDP), Semi Markov Decision Process\n(SMDP)s, Hierarchical Abstract Machines (HAM)s and MAX-Q Q using application specific instruction set to generate chip specific graph library (103).  The artificial intelligence setup comprises a combination of reinforcement learning (AI) agent, a\nreinforcement learning environment, and a task.  The AI setup is created using application specific instructions retrieved from a first memory module.  Thereafter, an interaction is initiated between the reinforcement learning agent created and the\nreinforcement learning environment using the application specific instructions.  Each of the SoC sub domains from the plurality of SoC sub domains is mapped to a combination of environment, rewards and actions by a second processor.  Mapping is performed\non executing an instruction with a reinforcement learning environment ID by the second processor.  The Q values for each domain are generated using the reinforcement learning agent.  The AI agent is configured to interact with AI environment through task\nto extract Q values for each domain.  Thereafter, the reinforcement learning environment is mapped to respective Electronic design automation EDA tools associated with the corresponding AI SoC subdomain.  Sequentially, an interaction of a plurality of\nagents is initiated with the reinforcement learning environment for a predefined number of times and further Q value, V value, R value, and A value is updated in the second memory module.  Thereby, an optimal chip architecture for designing SoC is\nacquired using application-domain specific instruction set (ASI).  The optimal chip architecture corresponds to a maximum Q value of a top level in a SMDP Q table.\n According to an embodiment herein, the Initialization of the SoC chip design flow includes library initialization.  In Library initialization process, standard libraries like technology libraries from foundry, standard interface IP from AI SOCD\ndatabase, processor IPs from database and other standard IPs which are reused are loaded into chip database.  These libraries are pulled together into chip database after parsing the SoC specification input.  The Soc specification input is at least one\nof a specification of the chip or a database which represents the previous chip plus the change in description in text or xml format.  The pulled in libraries and the specific SoC specification or database provide an IO, chip skeleton, relevant IPs and\npartition the chip.\n According to an embodiment herein, an empty environment corresponding to the reinforcement learning environment ID is created.  Further, an empty agent within the reinforcement learning environment denoted by the reinforcement learning\nenvironment ID is created.  Thereafter, the plurality of reinforcement learning agents is associated to at least one reinforcement learning environment.  Subsequently, training is initiated on the reinforcement learning agent represented by the\nreinforcement learning agent ID by using exploration instruction.  Further, the chip topology is associated to a Markov Decision Process (MDP) with HAM constraints.  The chip topology is branched to a plurality of SMDPs or MDPs.  The plurality of SMDPs\nor MDPS are activated.  Further, the plurality of activated SMDPs or MDPs is terminated on achieving a preset Q-value or a preset objective.  The plurality of SMDPs or MDPs are synchronized after termination.  A plurality of subdomains at a same level is\nactivated.  Further, the plurality of activated subdomains is terminated on achieving a preset Q-value or a preset objective.  Thereafter, the plurality of subdomains are synchronized after termination.\n A physical verification of the plurality of SMDPs or MDPs is initiated.  The extracted Q values of each domain and subdomain is stored in a hierarchical SMDP structure in a form of SMDP Q table in a big data database for optimization for\nplanning SoC.  Subsequently, a maximum Q value for each domain is determined through an initialization process of any one of MDPs, SMDPs, and a hierarchy of MDPs.  Thereafter, an optimal chip architecture is estimated corresponding to a maximum Q value\nof a top level in the SMDP Q table.  Thus, an optimal chip architecture is acquired for designing SoC.  Thereafter, the optimal chip architecture is stored in a database for learning and inference.  The desired SoC configuration is generated and\noptimized based on the optimal chip architecture and the generated chip specific graph library.  The step of acquiring an optimal chip architecture from the maximum Q value comprises deriving optimal policies associated with the optimal Q value, and\nexecuting actions associated with the derived optimal policy.\n According to an embodiment herein, a backup of each of the plurality of subdomains hierarchically through Hierarchical Deep SMDP network (HDSN).  The step of storing the extracted Q values comprises storing Q values in a Q table for leaf level\nMDPs and options macros.\n According to an embodiment herein, in order to achieve the preset Q-value the reinforcement learning techniques is executed and terminated.  The reinforcement learning techniques is executed and terminated when Q value converges below a preset\nthreshold value for each domain or when a change of Q value is reduced below the preset threshold value for each SoC domain.  Alternatively, the reinforcement learning techniques is executed and terminated, when a predefined number of episodes is\ncompleted for each specific domain.  According to an embodiment herein, the reinforcement learning techniques is executed and terminated, when a predefined target performance or predefined exit state is reached for each specific domain.  The\nreinforcement learning techniques is executed and terminated, when a predefined hierarchically optimal state for each domain is achieved.\n According to an embodiment herein, the multi-objective AI agents is configured to feedback the inferences into the graph database and a database which hold Q-values and neural network weights.  The database which stores the inferences/learnt\ndata is used for \"transfer learning\" process.  The \"Transfer learning\" process is defined as a process in which the learnt task is transferred to similar tasks which are not learnt, but are estimated using transferred learning means, when a set of\nlogically close tasks are learnt already.  Examples of multi objective Agent include but not limited to SoC Noc agent, SoC Power agent, SoC Physical Agent, SoC multiprocessor agent, SoC security agent and SoC clock agent.  The multi-objective AI agent\n(hereafter referred to as AI agent) is configured to generate Q values for each domain.  The AI agent is configured to interact with AI environment through AI task to extract Q values for each domain.\n According to an embodiment herein, the extracted Q values of each domain and subdomain in a hierarchical SMDP structure are stored hierarchically in a form of SMDP Q table in a big data database for optimization of SoC design planning process. \nIn order to extract Q value, reinforcement learning process is initiated for each domain.  The domain is any one of a NoC generation, Power optimization, Interface integration, Logic Verification, Logic synthesis, Pre layout timing closure, Physical\ndesign, post Layout timing closure and Physical verification.\n FIG. 2 illustrates a flowchart explaining a method for SoC design using an AI driven design setup with hierarchical multi-objective AI, according to an embodiment herein.  The SoC specification input is received (210).  An abstract graph\ndatabase libraries are created to store details regarding a chip design technology and standards involved and required for a chip designing process.  Further a chip design knowledge library and a generic hardware and software libraries are also created\nand maintained (211).  The details regarding chip, chip skeleton, clock.  Input outputs, partitions that are retrieved from the received SoC specification input and a chip database library or graph database library.  A chip design (of SoC) is initialized\nbased on the extracted details regarding chip, chip skeleton, clock, Input outputs, partitions that are retrieved from the received SoC specification input and a chip database library or graph database library, (212).  After initialization, AI are setup\nfor various domains in the form of MDPs, SMDPs, HAMs and MAX-Q to further optimize and generate desired SoC configuration.  The AI setup represents a Multi objective AI agent.  The AI setup for a domain includes multi-objective AI agents with\nenvironment, and a plurality of tasks.  The AI setup for a domain is executed and repeated for a plurality of domains such as first domain, second domain, and a third domain (213, 214, 215).  Then a multi objective AI environment, agents and tasks are\nset up by compiling the data received from the AI set up for the plurality of domains (216).  The chip specific graph library and database is generated using AI environment based on the output data received from a multi objective AI environment, agents\nand tasks.  Further, data stored in the chip specific graph library and database is fed to the AI set up for the plurality of domains in a feedback loop (217).  The multi-objective AI agent for a chip is configured to generate Q values for each domain. \nThe AI agent is configured to interact with AI environment through task to extract Q values for each domain.  Subsequently, the extracted Q values of each domain and subdomain in a hierarchical SMDP structure are stored hierarchically in a form of SMDP Q\ntable in a big data database for optimization of SoC design planning process.  Further an optimal chip architecture is obtained from the optimal Q value (218).\n According to an embodiment herein, a combination of HAMs, options and Max-Q learning is used to achieve end to end SoC design framework.  Max-Q learning involves categorizing the entire SoC design into a hierarchy of SMDPs, for acquiring a Q\nvalue or learning a solution simultaneously across the plurality of SMDPs.  In the Max-Q learning process, the core SMDP .quadrature.  is divided into a plurality of sub tasks {M0, M1, M2, M3, .  . . MN}, Solving for M0, solves the entire M. Instead of\nclassic definition of Max-Q, where set M are tasks, they are recursive Max-Q, with HAM like constraints according to the embodiments herein.  Each one of M1, M1, M2, .  . . MN represents another Max-Q recursively.\n According to an embodiment herein, a relation between a lower level SMDP, MDPs and options macro Q values is determined using a Deep Neural Network, when Q values of the upper level in the Q tables are calculated.  The lower level SMDP, MDPs and\noptions macro Q values are related to Q values of the upper level in the Q tables.  The Deep neural network is selected from a group consisting of a recurrent network, convolutional network, LSTM, GANs and hierarchy of these networks.  Further, a\nHierarchical Deep SMDP Network (HDSN) is trained for acquiring an optimal value of weights for estimating a relationship function between the pluralities of Q values in the Q tables.  The optimal values of weights are obtained by training the neural\nnetwork.  The optimal values are used to determine a process of estimating higher level Q values as a function of lower level Q values in a hierarchical RL setups.  The HDSN is further trained or employed to determine a synchronization mechanism between\nSMDPS at the same level in the hierarchical RL implementation, by determining an appropriate level of threshold Q values to terminate one Semi Markov Decision Process (SMDP) and start another SMDP in tandem.\n According to an embodiment herein, each of the SoC domains, i.e., Network On Chip (NoC) generation, power optimization, interface integration, logic verification, logic synthesis, pre layout timing closure, physical design, post layout timing\nclosure and physical verification, are configured to implement (execute) an algorithm, such as, greedy algorithm or a tuned greedy algorithm, to balance the exploitation and exploration thereof.  Subsequently the learning is estimated based on the number\nof inferences drawn from each SoC domain, when the Q values converge, or when a predetermined number of experiments are completed, or predetermined Quality of Results (QoR) standards are met.  Corresponding new policies (which are a resultant of\nlearning) for mapping `state` to `action` are devised and stored in domain specific databases.  New policies learnt in a particular domain are optimized and combined/iterated with the policies learnt in other relevant domains, and hierarchically\nrelevant, optimal policies are generated.  Typically, new policies learnt in one domain are optimized and iterated with the policies learnt in other AI SoC domains to derive a context and to obtain hierarchically optimal, multi-domain wise, and sub\ndomain wise optimal policies.  Such hierarchically relevant, optimal policies are generated based on at least one of policy based methods, value based methods and actor-critic method of reinforcement learning.  The learning methods implemented across SoC\ndomains include DQN, linear functions, windowing functions, regression functions, and value function approximation.  Hierarchically optimal and individually optimal policies are stored in the database in the form of equations, functions, SW, options\nmacros to be reused and referred in future learning applications of all AI SoC domains and sub domains.\n According to an embodiment herein, the model of environment along with options are constructed for each SoC domains and the corresponding sub domains, using the interaction with the AI environment, and in line with the accuracy required for each\ndomain and sub domain.  Supervised Reinforcement Learning principles are utilized for constructing a training model for each SoC domain and sub domain.  The training model is any one of a lookup table model, linear expectation model, linear Gaussian\nmodel, Gaussian process model or any other similar deterministic or stochastic model that is designed or configured to model the MDP, SMDP and option macros together.\n FIG. 3 illustrates a block diagram of an Artificial intelligence (AI) logic design set up for each domain and subdomain in the SoC design framework, according to an embodiment herein.  The Artificial intelligence (AI) logic design set up for\neach domain and subdomain in the SoC design framework is configured to execute a hierarchical Reinforcement Learning (RL) process or technique using options, MAXQ framework, HAM and any other similar techniques.  The Markov Decision Process (MDP) is\nderived for the main SoC design initially, with the MDP executing the hierarchical tasks.  The domain specific Q-values or neural weights are utilized to extract Markov Decision Process (MDP) or Semi Markov Decision Process (SMDP) or option macros or HAM\nor Max-Q values.  Subsequently, the extracted values are stored in domain-specific databases.  The extracted SMDPs or MDPs, HAMs or option macros or Max Q-values are utilized to generate inferences pertinent to SoC circuit design, which are in turn\nloaded on a domain specific Artificial Intelligence setup, thereby initiating an interaction between the inferences and generating an Optimal Chip Architecture (OCA).\n With respect to FIG. 3, an artificial intelligence logic design setup 220 comprising an artificial intelligence logic design task 222 and an artificial intelligence logic design agent 225.  The Artificial intelligence (AI) design task 222\ncomprising an artificial intelligence logic design environment 224 is configured to interact with the AI logic design agent 225, to make observations, initiate appropriate actions and subsequently receive rewards for the (performed) actions.  The AI\nlogic design agent 225 is designed as a learner module to learn from the interactions with the AI logic design environment 224.  The AI logic design agent 225 is configured to perform various actions in the AI environment 224 to acquire or collect\nobservations and rewards from the AI logic design environment 224 for the executed/performed actions/operations.\n FIG. 4 illustrates a functional block diagram of an AI framework implemented in the SoC design framework, according to an embodiment herein.  With respect to FIG. 4, the AI framework 303 for chip design process receives input from a generic\nhardware IP block 301, generic software IP block 302.  The AI framework is configured to provide optimal chip design.  The optimal chip design is provided for Application software development 304 and software testing 305.  Further, the optimal chip\ndesign undergoes hardware/software verification on the application prototype 306.  The optimal chip design is provided for prototype IC fabrication 307.  After verification, the optimal chip design is sent for volume IC fabrication 308.\n FIG. 5 illustrates a block diagram of an AI framework configured for hierarchical reinforcement learning, according to an embodiment herein.  With respect to FIG. 5, the multi objective AI is Multi objective reinforcement learning agent is\ndesigned and configured to learn from an interaction with environment based on the inputs received from the sample chip description file 501 after performing an initialization process 502.  The example of Chip multi objective Agent includes but not\nlimited to SoC Noc agent 503.  SoC Power agent 504, SoC Physical Agent 505, SoC multiprocessor agent 507, SoC security agent and SoC clock agent 506.  AI Framework is so modular in design that a plurality of similar agents is added and plugged them to AI\nenvironment and big data database called alpha DB.  A Big data inference engine 512 is configured to interact with the plurality of Multi objective reinforcement learning agents 503, 504, 505, 506, 507 to receive inferences for storing into libraries 510\nafter performing a verification process with a verification agent 509.  The output of the reinforcement learning agents are fed to GDS (Graphic Database System) and drivers 508 for planning SoC design.\n According to an embodiment herein, the AI frame work comprises SoC Communication and Network on a Chip (NoC) module for connecting a plurality of blocks to one another thereby establishing connection and integration of SoC.  The NoC is\nimplemented using a SoC communication AI agent 503.\n According to an embodiment herein, the SoC Power optimization module is loaded with techniques for optimizing various power components of the SoC such as leakage and dynamic power and decreasing effects of ageing in hardware due to electron\nmigration on hardware performance using a SoC Power Agent 504.\n According to an embodiment herein, SoC clocking module is a domain for clock design of entire SoC, and involves global and local optimization of clocking using a SoC clock Agent 506.  This also involves estimating and clock tree optimization.\n According to an embodiment herein, SoC Placement and Floor plan is arrived at optimally.  SoC Placement and Floor plan includes placement of logic blocks according to reinforcement learnt algorithms and policy.  SoC Placement and Floor plan\nincludes power Island creation but is not limited to coming up with policies to optimize timing and power through placement.\n According to an embodiment herein, SoC Physical design is carried out, by mapping logical design onto physical libraries of cells which contains information like power, physical layout of transistors, physical delay format and physical power\nrouting using a SoC Physical Agent 505.\n According to an embodiment herein, SoC logic verification module is configured to verify chip logic using verification agent 509.  The verification process involves using various verification process and collecting metrics for reporting bugs to\nimprove quality of SoC and provide confidence in functionality of SoC.\n According to an embodiment herein, SoC physical Verification module is designed to verify the physical implementation of the SoC using verification agent 509.  The verification process involves a verification of LVS Vs DRC, post layout timing,\nrouting, IR drop, Power feature correctness, Electric rule check.  Further, various policies are explored in this domain to provide optimal tradeoff for physical design (of the SoC circuit) considering all these factors.\n According to an embodiment herein, SoC timing module is designed for executing pre-layout and post-layout timing closure.  Timing closure involves taking particular actions and policies like solving setup time and hold time, assigning false\npaths.\n According to an embodiment herein, SoC DRC (Design Rule check), SoC ERC (Electric Rule check) are designed and configured to learn, apply and plan ERC and DRC closure policies using deep reinforcement learning techniques.\n According to an embodiment herein, packaging related policies and actions are learnt in a SoC Packaging process.  The policies related to substrate design, pin-muxing, padding, power ball assignments, possible PCB load on I/O pins and the like. \nPolicies for selecting type of packages are also learnt at the end of all tasks in this domain.\n FIG. 6 illustrates a flowchart explaining an interaction process between main MDP and SMDP in SoC design framework, according to an embodiment herein.  With respect to FIG. 6, the AI framework is configured to perform hierarchical Reinforcement\nLearning (RL) process using MDP, options, MAXQ framework, HAM and any other similar techniques.  The Markov Decision Process (MDP) is derived from the SoC design specification initially.  Since, SoC design requires continuous tasks for long time without\nmaking decisions, Semi Markov Decision Process (SMDP) is used rather than MDP.  The main MDP or multiagent support for Soc design is created (520).  The Main MDP is divided into a plurality of SMDPs that correspond to a plurality of domains in SoC design\n(520a, 520b, .  . . 520n).  For each domain, a plurality of subdomains are created (521a, 521b, .  . . 521n).  Each subdomains is configured to execute Reinforcement learning algorithm to obtain a plurality of option macros (522a.  522b, .  . . 522n). \nOption/macros are used in arriving at optimal chip Architecture (OCA).\n According to an embodiment herein, the SMDPs are configured to focus on closed loop action over an extended period, which is called as options macros.  In SoC design, many decisions are extended over a period of time, in a closed loop format. \nHence, the AI framework is designed to use option macros in SoC design activities.  The options macros is applied to smaller hierarchical tasks to form HRL.  The option macros are applied across SoC AI subdomains to acquire a human level intelligence in\ndesigning the SoC.  The Options macros include a generalization of primitive actions to include temporally extended actions.  For example, the process of fixing a setup time violation involves tasks such as retiming data path, buffering clock path,\nchanging the VT type of the cells in the path, reflow planning and the like.  Each of these tasks is included as options macro which is extended in time with closed feedback loop with EDA tools.\n According to an embodiment herein, options macros or options are used in arriving at optimal chip Architecture (OCA).  Each options includes a policy .pi.: S.times.A.fwdarw.[0, 1], Termination condition .beta.: S+.fwdarw.[0, 1], and Initial\nstate IS.  The option is defined by the triplet parameters [.pi., .beta., I].  Some of the options are also derived from a past experiment of the AI setup, by storing the triplet in the database and slightly altering the policy.  Policy is a mapping\nbetween the option and the following action.  By slightly modifying the policy, distribution of action that needs to be taken is obtained.  Further, the agent includes a chain of actions.  The agent initiates another action, once the selected option is\nterminated using SMDP transitions.  The Options macros is one of a Markov or semi-Markov with a specific timeout mechanism.  Thus, the option macros are used for iterative optimization part of OCA, for example floorplan and placement optimization with\nrespect to timing closure issue or verification test case that has long run time and is run by the agent to prove verification coverage of the design.\n According to an embodiment herein, the AI framework incorporates SMDPS implemented with max-Q and Hierarchies of Abstract Machines (HAM) framework to achieve faster and most optimal architecture and design closure for SoC.  The SMDPs deploy\nMax-Q framework and HAMs.  These are Semi-Markov Options (SMO).  SMO indicates that the options represents more detailed states than available to policy that selects the options.  Further, HAMs are used to place constraints on the policy learnt by agent\nSMDPs for policy improvement.  Furthermore, the HAMs are used to apply constraints across hierarchy of SMDPs.  Each SMDP domain has its own HAM to provide constraints about mapping of higher level SMDP tasks to lower level SMDP tasks in a hierarchy. \nAlso HAM provides ways to relate hierarchy of SMDPs.  Since SoC design is always hierarchical, HAMs play large role in determining optimal RL algorithm to be applied across hierarchy of designs (from IP way up to SoC top) and hierarchy of tasks (from\nextracting optimal chip specification to optimal chip architecture (OCA) to GDSII tape out to layout).  According to an embodiment herein, the HAMs are used to derive constraints for SMDPs and relation between low level and higher level tasks.\n According to an embodiment herein, HAMs contain four states as used in this invention to call AI subdomain SMDPs.  These states are run in all SMDPs or MDPs to determine when to call next SMDPs MDPs and option macros.  According to an embodiment\nherein, the AI framework includes AI SOCD Action states, AI SOCD Call states, AI SOCD Choice states, AI SOCD Stop states.  The AI SOCD Action states execute an action in the AI SOCD hierarchical environment.  Further, AI SOCD Call states execute another\nAI SOCD HAM or SMDP as a subroutine.  AT SOCD Choice states non-deterministically select a SMDP of AI SOCD.  AI SOCD Stop states halt execution of the machine and return control to the previous call state.  A top level hierarchy of the AI SOCD HAM or\nSMDP is the chip top agent and it calls all other agents with domain specific HAMs.  Each of the HAM based SMDP is multi objective with vectored rewards, actions and Q values.\n According to an embodiment herein, the SoC design framework includes the following modules/domains:\n SoC Communication and Network on a Chip (NoC): This module connects blocks to one another thereby establishing connection and integration of SoC and is implemented using a SoC communication AI agent.  In this sub domain, main goal of single or\nmultiple agents represented by single or multiple SDMPs (Which can be Max-Q, HAMs or options implementation) is to create optimal communication.  This SOC AI subdomain comprises environment which provides rewards and observation to single or multiple\ntype agents.  Rewards and observation is related to ease of communication among SoC end points (Masters and slaves, where example master is a processor and slave is a memory port or a configuration interface).  Single or multiple agents are configured to\nreceive feedback from environment regarding reduction in latency, Power consumption, increase in throughput of the system, optimal communication topology (NoC topology), decrease in congestion of the physically designed chip as positive reward for a\ngiven action like buffering the datapath, retiming the datapath, regrouping the masters and slave into a cluster, creating power islands and rerouting the connectivity wires through dedicated channel to decrease congestion.  Furthermore this domain is\nconfigured to create and discover polices as combination of these actions and implement them to obtain optimal SoC communication architecture.  These AI SOC domain NoC agents are configured to interact with NoC environment to extract MDP, Implement\npolicy gradient, store resulting Q values in the Big database and plan the optimal SoC communication structures\n SoC Power optimization: The module incorporates techniques used in optimizing various power components of the SoC such as leakage and dynamic power and decreasing effects of ageing in hardware due to electron migration on hardware performance\nusing a SoC Power Agent.  SoC Power optimization subdomain involves environment which is configured to provide a feedback to an agent regarding a reduction or increase in power consumption based on agent actions.  This AI SOC subdomain contains one or\nmore power agents, which are configured to provide the balance between performance and various power components.  Environment is configured to provide a feedback on performance measures like throughput, clock frequency and power consumption measures like\nleakage power and dynamic power of a particular block or entire SoC.  This AI SOCD subdomain is configured to implement further hierarchy of Max-q framework with HAM constraints to optimize SoC power consumption per performance.  Actions taken by agents\nare clock gating, power collapsing a block, partitioning the SoC, power rail redesign etc. These AI SOC domain Power agents are configured to interact with Power environment to extract MDP, Implement policy gradient, store resulting Q values in the Big\ndatabase and plan the optimal Power structures\n SoC clocking is a domain for clock design of entire SoC, which involves global and local optimization of clocking using a SoC clock Agent.  This also involves estimating and clock tree optimization.\n The SoC clocking AI SOC subdomain has environment which is configured to provide a feedback on number of PLLs required, estimated clock power, clock routing congestion and possible issues with clock skew.  This AI SOC domain with one or many\nagents is configured to take actions of inserting PLLs, Clock generation logic (CGL), clock gating and clock programming register implementation.  These AI SOC domain clock agents are configured to interact with clock environment to extract MDP,\nImplement policy gradient, store resulting Q values in the Big database and plan the optimal clocking circuitry\n SoC Placement and Floor plan: in this domain SoC floor plan is arrived optimally.  It includes placement of logic blocks according to reinforcement learnt algorithms and policy.  It includes power Island creation, it includes (but is not limited\nto) coming up with policies to optimize for timing and power through placement.\n SoC Placement and Floorplan AI SOC Domain is configured to implement the Placement and Floorplan environment, which as hooks to Floorplan and placement EDA tools.  The Environment is configured to use Floorplan and Placement EDA tools as\ndatabases and wraps around them to obtain different rewards and observation like congestion, timing, routing channels available, routing utilization and Quality of results (QoR), which may be in tool specific format, but is converted to AI framework\ngeneric format so that the AI framework is portable across the EDA vendors.  This AI SOC domain is designed to implement one or more subdomains according to HAM and Max-Q implementation as explained.  Each subdomain is also configured to implement one or\nmore agents to derive optimal placement and floorplan.  The actions taken by agents include, but not limited to moving the sub blocks in the design, adding buffers, reshaping the sub blocks and timing closure of the design, interaction with other AI SoC\ndomain to obtain optimal QoR for placement and SoC floor planning.\n SoC Physical design: In the domain SoC Physical design is carried out, including mapping logical design onto physical libraries of cells which contains information like power, physical layout of transistors, physical delay format and physical\npower routing using a SoC Physical Agent.  SoC Physical design AI SoC domain is configured to implement environment to provide observation and reward for physical design quality such as routing congestion, timing, power analysis, power routing.  This AI\nSoC domain is configured to implement multiple sub domains related to physical design for mapping logical design to technology library.  It is also configured to implement one or more agents to take actions such as buffering, altering power routing,\ninserting power switches, inserting clock gates, retiming the data path, removing or adding additional PLLs.  Agents are also configured to interact with other AI SoC domains and sub domains to obtain optimal SoC Physical design.  Further this AI SoC\ndomain and its sub domains are configured to implement the hooks to various EDA tools to obtain generic format of the observation from a tool specific format.\n SoC logic verification: The module involves verifying chip logic using verification agent.  The verification process involves various verifications and collecting metrics and reporting bugs to improve quality of SoC and provide confidence in\nfunctionality of SoC.  SoC logic verification AI SoC domain is configured to implement the environment, to provide observation about bugs found in the design, verification coverage, test case failure, assertion failures, inferring debug log.  This AI SoC\ndomain is also configured to implement one or more sub domains according to Max-Q, HAM and options frame work to obtain the verification closure of the SoC.  Each AI SoC subdomain of this AI SoC domain is configured to implement one or more agents to\ntake actions such as generating test cases using libraries of test function, running test cases with functional simulator EDA tool, debug actions such as reading verification log files, Coverage log files, fixing the design bug and accessing databases to\nstore Q-values at the end of each testing.  There are multiple Q values that reflect the amount of verification done, number of bugs getting closed after an action such as test case generation.  Also design bug fixing is done by a separate bigger SMDP in\nimplementing Max-Q, options or HAM or combination hybrid.  Also the agents related to this AI SOC domain and its sub domain are configured to interact with other AI SOC domains like top level SoC MDP via database to obtain verified optimal SoC circuit\nand close the verification highest coverage possible.  This AI SoC domain is configured to implement hooks to various EDA Functional simulator and debug tools to obtain the information observation like code coverage, assertion coverage and start and end\nof a test case.\n SoC physical Verification: The module verifies the physical implementation of the SoC using verification agent.  The verification process involves verification of LVS Vs DRC, post layout timing, routing, IR drop, Power feature correctness,\nElectric rule check.  Further, various policies are explored in this domain to provide optimal tradeoff for physical design (of the SoC circuit) considering all these factors.\n SoC Physical Verification AI SoC domain is configured to implement environment to provide observation about the LVS, DRC errors, logs, post layout timing, IR Drop and QoR of physical design.  These observations are obtained by environment from\nthe EDA tools which is further processed to generate generic format from tool specific format and fed as reward or observation to agent.  This AI SoC domain is configured to implement the multiple AI SOC subdomain, options macros via hybrid MAX-Q and HAM\nframework.  Actions taken by agents of each of these subdomains include, but not limited to remapping libraries, reporting the error ERC errors and obtaining standard fix from database, updating Q values to the database.\n SoC timing: The domain involves pre-layout and post-layout timing closure.  Timing closure involves taking particular actions and policies like solving setup time and hold time, assigning false paths, etc. SoC Timing AI SoC domain has a main\nobjective to close timing of the SoC.  Timing violation includes setup and hold violation, false paths, multicycle paths and general exceptions.  The environment of Timing closure AI domain, which is hooked to timing closure tool to provide rewards and\nobservation about setup time violation, hold time violation, total negative slack, and in worst case negative slack and cross talk violations.  The AI SoC Timing agents are configured to contain one or more levels of Max-Q recursive structure implemented\nwith hybrid HAM constraints described previously.  This AI SoC domain and its sub domain are configured to implement Agents, whose main actions are to fix the setup and hold violation by retiming, redesigning data paths and balancing Clock trees.  Each\nof these actions are implemented as options as well.\n SoC DRC (Design Rule check), SoC ERC (Electric Rule check): In this domain, ERC and DRC closure policies are learnt, applied and planned using deep reinforcement learning techniques.  SoC DRC/ERC AI subdomains are configured to implement\nefficient fixing of the ERC violations like Floating and nets and unconnected VDD/VCC pins, Shorts of Power nets, power domain crossing without a proper level shifter and isolation cells.  DRC violations includes, spacing rules between metals, minimum\nwidth rules, via rules, another set of rules are LVS rules, which ensures Layout that is generated is equivalent to netlist that is generated before physical design.  It checks for shorts, opens and circuit parameter mismatch.  This AI SoC domain is\nconfigured to implement the hierarchical Max-Q framework with HAM constraints as suggested above.  This AI SoC domain is also configured to implement the one or more agents which are configured to take actions to fix floating grounds by connecting them,\ncorrects shorted VDD to remove shorts by separating shorts.  They are configured to take action to fix the LVS violations by fixing floating nets, correcting parameters of the layout, so become equivalent to netlist.  Agents are also configured to take\nactions to fix DRC, such that the spacing between metals are provided according to design rules and width of the routes are set according to definition of the process rules.\n SoC Package: In this domain, packaging related policies and actions are learnt.  Policies related to substrate design, pin-muxing, padding, power ball assignments, possible PCB load on I/O pins and the like.  Policies for selecting type of\npackages are also learnt at the end of all tasks in this domain.  SoC Package AI SOC domain involves in providing optimal package, I/O, substrate design.  This AI SoC domain is configured to implement environment which is hooked to I/O models, I/O cell\nlibraries, Substrate models, PCB models, pad-ring configuration and pin-muxing.  The environment is configured to provide the feedback, observations and rewards about I/O constraints being met, substrate constraint being met or not met, I/O models being\nfunctionally competent or qualified to meet the I/O load and drive requirement, capability of I/O cell in meeting the DRC, current and transient characteristic required to drive the load on the board as well sink in current from board components.  It\nalso gives observation about scenarios involved in the I/O usage like concurrency on pin-muxing to provide optimal utilization of I/Os.\n According to an embodiment herein, the aforementioned domains are implemented using the AI framework.  Each domain is configured to incorporate an AI logic design environment, AI logic design agent, AI logic design task, AI logic design\nexperiment.  The SoC design framework envisaged by the embodiments herein aims to extract the learning from the experimentation of all the aforementioned modules, and stores the learning results in a database, preferably in the form of Q-values or neural\nnetwork weights or a lookup table, or in any other appropriate format.  For example, Q-values are derived from the Bellman's equation for policy iteration.  Alternatively, Q-values are also derived from the options or macros or HAM or Max-q methods and\nstored in the database for deriving the MDP.\n Typically, Q-Values are construed as action value functions, which provide the value of particular state for an action given a policy .pi..  Q-values are defined by the function q.pi.(s,a)=E.pi.[Gt|St=s,At=a]\n Where q.pi.  are Q values, Gt is total reward for the policy from state `s` and action `a`, St and At are state and action space.  The Q-values are generated in the learning process and stored on the database as program variables.  Apart from\nthe value function defining the total value from state `s`, the policy .pi.  is defined as V.pi.(s)=E.pi.[Gt|St=s]\n Values with or without value function approximation are stored in the local program variable or separate database at the end of learning process.\n According to an embodiment herein, Reinforcement Learning (RL) algorithm terminates for each SoC domain at the convergence of Q-values below a preset threshold value.  Alternatively, the Reinforcement Learning (RL) algorithm is terminated on\ncompletion of predefined number of experiments/episodes.  Further, the Reinforcement Learning (RL) algorithm is terminated once it reaches a predefined target performance or exit state.  Examples of predefined state include predefined throughput target\nof NoC (Network on a Chip) in inter-chip Communication domain setup or predefined power numbers of the dynamic power of the chip, or predefined clock skew, or predefined number of clocks.  Yet the Reinforcement Learning (RL) algorithm is terminated when\na predefined hierarchically optimal goal or state is reached.\n FIG. 7 illustrates a flowchart illustrating the steps involved in termination of Reinforcement Learning (RL) algorithm on convergence of Q value.  According to an embodiment herein, reinforcement learning is initiated by an AI setup with a\nplurality of domains corresponding to the SoC design (601).  Further, domain AI setup is done with domain specific AI agents, domain specific environment, and domain specific tasks.  The domain AI are setup in form of MDPs, SMDPs, HAMs and MAX-Q for the\ninitialized, compiled and non-optimized chip database to further optimize and generate desired SoC configuration.  Further, AI experiment setup is done for each SoC domain.  The AI experiment setup includes configuring each SoC domain to implement\n(execute) an algorithm, such as, greedy algorithm or a tuned greedy algorithm, to perform the exploitation and exploration (602).  Subsequently, the domain specific AI agent is configured to interact with the domain specific environment via AI task to\nextract Q values pertaining to a specific domain.  Further, the extracted Q values are stored in a database/big data (603).  The process is continued for each domain until the Q values is converged, or rates of change of Q value is below a preset\nthreshold .lamda.  (604).  The learning is estimated based on the number of inferences drawn from the Q values of each SoC domain.  Accordingly new policies which are a resultant of learning are devised/derived and stored in domain specific databases. \nNew policies learnt in a particular domain are optimized and combined/iterated with policies learnt in other relevant domains, and hierarchically relevant, optimal policies for planning SoC are generated (605).\n FIG. 8 illustrates a flowchart explaining a method of terminating Reinforcement Learning (RL) process on completing predefined number of experiments or episodes for each SoC design domain individually in SoC design framework, according to an\nembodiment herein.  According to an embodiment herein, reinforcement learning is initiated by an AI setup with a plurality of domains corresponding to the SoC design (701).  Further, domain AI setup is done with domain specific AI agents, domain specific\nenvironment, and domain specific tasks.  The domain AI are setup in form of MDPs, SMDPs, HAMs and MAX-Q for the initialized, compiled and non-optimized chip database to further optimize and generate desired SoC configuration.  Further, AI experiment\nsetup is done for each of the SoC domains.  The AI experiment setup includes configuring each SoC domain to implement (execute) an algorithm, such as, greedy algorithm or a tuned greedy algorithm, to perform the exploitation and exploration (702). \nSubsequently, the domain specific AI agent interacts with the domain specific environment via AI task to extract Q values pertaining to a specific domain.  Further, the extracted Q values are stored in a database/big data (703).  The Reinforcement\nLearning (RL) algorithm is terminated on completion of predefined number of experiments/episodes (704).  The learning is estimated based on the number of inferences drawn from the Q values of each SoC domain.  Corresponding new policies which are a\nresultant of learning are devised/derived and stored in domain specific databases.  New policies learnt in a particular domain are optimized and combined/iterated with policies learnt in other relevant domains, and hierarchically relevant, optimal\npolicies for planning SoC are generated (705).\n FIG. 9 illustrates a flowchart explaining a method of terminating Reinforcement Learning (RL) process on reaching or achieving predefined target performance or target state for each SoC design domain individually in SoC design framework,\naccording to an embodiment herein.  According to an embodiment herein, reinforcement learning is initiated by an AI setup with a plurality of domains corresponding to the SoC design (801).  Further, domain AI setup is done with domain specific AI agents,\ndomain specific environment, and domain specific tasks.  The domain AI are setup in form of MDPs, SMDPs, HAMs and MAX-Q for the initialized, compiled and non-optimized chip database to further optimize and generate desired SoC configuration.  Further, AI\nexperiment setup is done for each of the SoC domains.  The AI experiment setup includes configuring each of the SoC domain to implement (execute) an algorithm, such as, greedy algorithm or a tuned greedy algorithm, to perform the exploitation and\nexploration (802).  Subsequently, the domain specific AI agent interacts with the domain specific environment via AI task to extract Q values related to a specific domain.  Further, the extracted Q values are stored in a database/big data (803).  The\nReinforcement Learning (RL) algorithm is terminated on reaching predefined target performance or exit state (804).  The learning is estimated based on the number of inferences drawn from the Q values of each SoC domain.  Corresponding new policies which\nare a resultant of learning are devised/derived and stored in domain specific databases.  New policies learnt in a particular domain are optimized and combined/iterated with policies learnt in other relevant domains, and hierarchically relevant, optimal\npolicies for planning SoC are generated (805).\n FIG. 10 illustrates a flowchart explaining a method of terminating Reinforcement Learning (RL) process on reaching or achieving predefined hierarchically optimal state or goal derived for each SoC design domain individually in SoC design\nframework by a top down approach, according to an embodiment herein.  According to an embodiment herein, reinforcement learning is initiated by an AI setup with a plurality of domains corresponding to the SoC design (901).  Further, domain AI setup is\ndone with domain specific AI agents, domain specific environment, and domain specific tasks.  The domain AI are setup in form of MDPs, SMDPs, HAMs and MAX-Q for the initialized, compiled and non-optimized chip database to further optimize and generate\ndesired SoC configuration.  Further, AI experiment setup is done for each SoC domain.  The AI experiment setup includes configuring each SoC domain to implement (execute) an algorithm, such as, greedy algorithm or a tuned greedy algorithm, to perform the\nexploitation and exploration (902).  Subsequently, the domain specific AI agent interacts with the domain specific environment via a task to extract Q values pertaining to a specific domain.  Further, the extracted Q values are stored in a database/big\ndata (903).  The Reinforcement Learning (RL) algorithm is terminated on reaching preset hierarchically optimal state (904).  The learning is estimated based on the number of inferences drawn from the Q values of each SoC domains.  Corresponding new\npolicies which are a resultant of learning are devised and stored in domain specific databases.  New policies learnt in a particular domain are optimized and combined/iterated with policies learnt in other relevant domains, and hierarchically relevant,\noptimal policies for planning SoC are generated (905).\n FIGS. 11A and 11B jointly illustrates a flowchart explaining a method of synchronizing a hierarchy of SMDPs in SoC design process, according to an embodiment herein.  The method includes initializing a chip topology from a specification or a\ndatabase (1101).  Further, the chip topology is associated to a Markov Decision Process (MDP) with HAM constraints (1102).  Subsequently, the chip topology is associated to a plurality of SMDPs or MDPs.  The plurality of SMDPs or MDPs are activated\n(1103).  Thereafter, the plurality of SMDPs or MDPs are synchronized when the plurality of activated SMDPs or MDPs is terminated on achieving a preset Q-values or a preset objective (1104).  A plurality of subdomains at a same level is activated (1105). \nThe plurality of activated subdomains is terminated on achieving a preset Q-values or a preset objective (1106).  Further, the plurality of subdomains is synchronized, when the plurality of activated subdomains is terminated on achieving a preset\nQ-values or a preset objective (1107).  The subdomains which are dependent on synchronized subdomains are activated (1108).  The dependent subdomains are terminated after achieving a preset objective (1109).  Subsequently, a physical verification of the\nplurality of SMDPs is initiated (1110).  Physical verification process is terminated after achieving a preset goal number of experiments (1111).  Thereby, an optimal chip architecture for designing SoC is obtained.  The chip design are stored in GDS\n(Graphic Database system) and forwarded to foundry for chip manufacturing 1112.\n According to an embodiment herein, synchronization is done by making just one SMDPs to communicate with other SMDP, that it has reached terminal state, when one SMDP does not depend on another SMDP (for example, active power and placement\nSMDPs).  The synchronization is one of a global broadcast signal, suggesting that SMDP has reached terminal state.  The terminated SMDP also is configured to upload its end results such as metrics of time closure by timing SMDP or when bugs are found in\nverification SMDPs into global database.  Alternatively, each SMDP is operated in a lock step or iterative loop, when one SMDP depends on another SMDP.  Each SMDP communicates with other SMDPs in a separate synchronization state, (for example, logic\nVerification SMDP and logic design SMDP).  Each SMDP communicate their end results and wait for bug fixation, optimization and call that design is in decent shape to send it to PD or synthesis.\n According to an embodiment herein, two or more SMDPs are involved in a synchronization process and the end results are shared among a plurality of SMDPs, options macro and MDPs.\n FIG. 12 illustrates a top view or a schematic view of a hierarchical Q-tables stored in a SQL schema or document database in SoC design process, according to an embodiment herein.  The AI framework is configured to perform a hierarchical backup\nto AI SOCD top level SMDP from leaf level options macros or MDPs.  The HAM frame work of states is used to synchronize SMDPs, when one SMDP calls other SMDP during a calls state of HAM constraints.\n According to an embodiment herein, the entire SoC design is modified based on a Max-Q Framework.  The SoCD 1201 is the top level SoC design task, which is decomposed into SocD0, SoCD1, SoCD2, .  . . SoCDN.  Each of these SocD are mapped to a\nplurality of AI SoC domain 1201a, 1201b, 1201c, 1201d.  In an example, a root task SoCD implements a policy .pi., and the hierarchy of tasks {SocD0, SoCD1, SoCD2, .  . . SoCDN} is used to implement the policies {.pi.0, .pi.1, .pi.2, .  . . .pi.N}, which\ncollectively represents the policy .pi..  Further, each domain is divided into further sub domains 1202a, 1202b, 1202c to implement a hierarchy of policy and SMDPs like recursive Max-Q, HAMs or end point options/macros 1204.\n According to an embodiment herein, the SMDP Q-learning is applied across AI framework using SoCD subdomains.  Q values are given by\n .pi..function..pi..function.'.tau..times..pi..function.'.tau..times..gamm- a..tau..times..pi..function.'.pi..function.' ##EQU00001##\n Where Q.pi.(i,s,a) is Q value following policy .pi.i of the ith SMDP,\n V.pi.  is the value function at the state S, with action a, following policy .pi., and\n Pi.pi.  is the transition probability to next state following policy .pi.\n According to an embodiment herein, storage of Q-value and usage are very much different because of recursive structure of Max-Q framework.  Different Q-Values are generated at each level of SMDP.  So equation for Q value is given by:\n .pi..function..times..times..times..times..times..times..pi..function.'.t- imes..times..times..times..times..times..times..times..times..times..pi..f- unction.'.tau..times..gamma..tau..times..pi..function..times..times..times-\n..times..times..times..times..times..times..times.'.pi..function.' ##EQU00002##\n where i1, i2, i3 .  . . in are introduced by iterative SMDPs involved, and n denotes nth level of Q value.  As millions of levels of SMDPs are involved and there are billions of Q values, the Q values are stored in the database.  The database\nhas a capacity of 100s peta bytes to enable in combining and obtaining final Q value of a current SoC design.  Final Q value of top level SMDP will be of the form: Q.sup..pi.(s,a)=V.sup..pi.(a,s)+.SIGMA..sub.s',t.sup.p.pi.(s',.tau.|s,a).-\ngamma..sup..tau.Q.sup..pi.(s',.pi.(s'))\n The Q value is a function of Q values of all the hierarchical SMDP Q values at a given state of top level SMDP.  The Q value functions require scaling each reward before performing the summation.  The scaling factor of all other Q value is\ndetermined by Deep neural net (DNN).\n With respect to FIG. 12, the Q values of Hierarchical SMDP structure is stored in a Table based Databases (such as MYSQL and MSSQL) or a document based databases (such as NOSQL, Mongodb) or graph databases.  Each database include a foreign key\nor a pointer which points to child SMDP Q-Values.  Further, the top level SMDPs Q values depend on a maximum of Q values of the lower level SMDPs from the state of entry.  The process of mapping top level SMDP Q values to lower level Q values continues\ndown the hierarchy until the lowest level MDP or options macro is reached.  The top level SMDP Q values is given by the following equation: Q.pi.(s,a)(top SMDP)=.tau.gi(.phi.)*f(max{Qi.pi.(s,a)})\n where gi(.phi.) and f( ) are obtained using training on a Deep neural network and Qi.pi.(s,a)) is ith level Q value of a child SMDP.\n According to an embodiment herein, the deep neural network is a Hierarchical Deep SMDP Network (HDSN) or Deep Max-Q network or Deep HAM networks.  In HDSN (unlike DQN), the experience replay is hierarchical, where one or more Deep neural\nnetworks are used to obtain the relation between SMDPs and further to obtain relation function approximation between different Q values.  In HDSN, then experience replay is used to obtain relation between different SMDPs Q value.\n FIG. 13 illustrates a block diagram or a schematic representation of Deep Neural network used for experience replay in a HDSN in SoC design process, according to an embodiment herein.  The HDSN is used to obtain a relation between different\nSMDPS in the hierarchical AI.  Further, the HDSN is configured to obtain relation function approximation between different Q values.\n According to an embodiment herein, the SMDP Q values obtained from the lower level SMDP tables are passed through DNN to obtain higher level Q values.  Thus, DNN is used for transforming the Lower level Q values to higher level SMDP Q values. \nFurther, the hierarchy of Q value are networked through DNN.  Different network configurations are used at different level of SMDP.  OCA (Optimal Chip architecture) is defined as the collection of Q tables, SMDPs, Relation between them across all\nhierarchy.  Optimal Chip Architecture (OCA) corresponds to a maximum Q value of the top level SMDP.\n According to an embodiment herein, the maximum Q value of top level SMDP is obtained across the AI SOC domain.  The AI SoC domain is further decomposed into a recursive Max-Q framework or called through HAM choice states or a combination of\nMax-Q and HAM.  For example, front end integration of the SoC is purely a decomposition of MAX-Q value function, while bottom up synthesis is a HAM, which calls for a synthesis of different states or sub block synthesis after the completion of synthesis\nof one module level.  Max-Q is also used in functional verification, in which whole chip verification value function is split into hierarchical tasks of block level verification, top level verification.  The verification process is performed to ensure\nthat a block level verification is completed.  Thus, the AI SOC design is split into a number of SMDPs to achieve the maximum Q value for top level SMDP, and OCA is obtained from the maximum Q value at the top level SMDP.\n FIG. 14 illustrates a functional block diagram of a system for generating an optimal Chip Architecture (OCA) using AI flow of hierarchical Reinforcement learning in SoC design process, according to an embodiment herein.  According to an\nembodiment herein, a chip architect optimizer based on AI hierarchical Reinforcement learning is configured to receive input from application software specification 1401, component cost metrics 1402, component performance metrics 1404, software function\nvia a System Description Language (SDL) 1403, and hardware function via a Chip Description Language (CDL) 1405.  The chip architect optimizer 1406 is further configured to perform Chip communication optimization, Chip I/O optimization, Chip placement\noptimization, Chip DFT/DFD optimization, and Chip verification optimization.  Finally, an optimal chip architecture 1407 is generated.\n FIG. 15 illustrates a flow chart explaining a method of extracting MDP for each domain in SoC design process using a reinforcement learning process, according to an embodiment herein.  According to an embodiment herein, a trial and error\nlearning of environment is executed with a plurality of single step actions.  Each step action is associated to a state (1410).  The execution of steps is terminated after N iteration.  The value function (V*) and Q values for each episode/step are\nstored in a database (for example, alpha DB) for each SMDPs (1412).  States are identified as a major variation in value functions and q values, like Architecture performance or results when interacting with EDA tools (1413).  Sample States from N steps\nare obtained to form MDP, with some probability distribution of transitions (1414).  MDP refined and Convergence seen to optimal Q values.  Thereafter, a backup is conducted to obtain value function and action values of top level MDP (1415).  The MDP is\nrefined and converged to obtain optimal Q values (1416).  Thus, a top level MDP is finalized (1417).\n FIG. 16 illustrates a functional block diagram of a Big data framework in SoC design process, according to an embodiment herein.  Reinforcement learning is carried out hierarchically using Hierarchical reinforcement learning or Deep Hierarchical\nreinforcement learning or any methods of reinforcement learning that involves learning from environment and planning the Chip architecture from learnt data (1601).  The learnt data is stored in database.  The stored data include Q values.  SMDP\nAction-value table, value functions, neural network weights used for function approximation, and relation between SMDPs (1602).  The stored data is retrieved for learnt information on SoC.  Using the stored data, AI planning of the SoC is performed by\nderiving various MDPs, SMDPs, options macros, HRL structures like HAM and Max-Q (1603).  The planned SoC information is stored for future reuse.  The planned stored include SoC information like plurality of MDPs, SMDPs planned, HRL structures, OCA, and\ninferences (1604).\n FIG. 17 illustrates a flowchart explaining a method of abstracting SoC domains and subdomains in SoC design process, according to an embodiment herein.  A new specification for electronics systems design is obtained from using learnt information\nand inferences stored in Big data processor (1700).  Big data processor (1700) receives and uses the board level PCB information from old and existing PCB design.  It uses machine readable format of existing component specification and Learnt information\nfrom SoC design from database.  The big data processor (1700) is configured to receive and utilize big data for chip function (1701), big data for component performance (1702), Big Data for customer requirements and business intelligence (1703), Big Data\non process and technologies nodes (1704), Real time information retrieval and technology research (1705).\n By combining learnt information, the big data processor 1700 is configured to provide insight regarding new electronics systems.  Other information that is used by Big data processor are cost of components, quality and availability.  Output of\nbig data processor contains the estimation of cost of new electronics components 1706, component performance 1707, application software specification 1708, CDL 1709, and SDL 1710, applications and target customers who may be interested in SoC targeted to\nnew electronics system.\n FIG. 18 illustrates a block diagram of a database architecture for SoC design, according to an embodiment herein.  With respect to FIG. 18, the database scheme for generating AI based SoC chip design comprises CDL parser 1801, relational\ndatabase 1802, Input Output ring and chip boundary 1803, graph database 1804, physical database 1805, and chip topology 1806.\n A new specification for electronics systems design is obtained from using learnt information and inferences stored in Big data processor.  Big data processor uses board level PCB information from old and existing PCB design.  It uses machine\nreadable format of existing component specification and Learnt information from SoC design from database.  By combining learnt information, the big data processor provides an insight of new electronics systems.  Other information that is stored into Big\ndata processor are cost of components, quality and availability.  Output of big data processor contains the estimation of cost of new electronics systems, applications and target customers who are interested in SoC targeted to new electronics system.\n According to an embodiment herein, several new policies are learnt during experimentation and several options are discovered, and applied in designing the SoC architecture and circuits.  Once all SMDP Q tables are filled with optimal Q values\nand the associated DNN is configured to approximate the relation function between SMDPs hierarchy, the optimal SoC arch is derived from top level SMDP.  SoC architecture involves getting PoR SoC correct, which is done through feature SMDP sub domain. \nThen it requires selecting correct IP and internal connection through Power domain arch, NoC arch and Design partitioning, clocking that are mapped to each AI SoC subdomain.  Optimal policies are learnt based on the stored Q values.  Future SoC circuit\nis obtained from trained database.\n The technical advantages include the realization of a SoC design and implementation framework that incorporates artificial intelligence and is capable of implementing the principles of reinforcement learning.  The SoC design framework of the\nembodiments herein mitigates the risks and uncertainty associated with the design of SoC circuits.  Further, the SoC framework allows the circuit designers to proactively experiment with the architecture of SoC circuits, in addition to enabling them to\nexplore previously unexplored design spaces.  Further, the SoC design framework disclosed by the embodiments herein, also reduces the occurrence of bugs therein while enhancing the creativity associated with SoC circuit design.  Further, the SoC\nframework also enables the circuit designers to gain insights on the design of SoC circuits while enabling them to experiment with the (SoC circuit) design architecture.  Further, the SoC design framework also reduces the turnaround time associated with\nthe design and implementation of SoC circuits.\n The foregoing description of the specific embodiments will so fully reveal the general nature of the embodiments herein that others can, by applying current knowledge, readily modify and/or adapt for various applications such as specific\nembodiments without departing from the generic concept, and, therefore, such adaptations and modifications should and are intended to be comprehended within the meaning and range of equivalents of the disclosed embodiments.\n It is to be understood that the phraseology or terminology employed herein is for the purpose of description and not of limitation.  Therefore, while the embodiments herein have been described in terms of preferred embodiments, those skilled in\nthe art will recognize that the embodiments herein can be practiced with modifications.  However, all such modifications are deemed to be within the scope of the claims.", "application_number": "15859698", "abstract": " The embodiments herein discloses a system and method for designing SoC by\n     using a reinforcement learning processor. An SoC specification input is\n     received and a plurality of domains and a plurality of subdomains is\n     created using application specific instruction set to generate chip\n     specific graph library. An interaction is initiated between the\n     reinforcement learning agent and the reinforcement learning environment\n     using the application specific instructions. Each of the SoC sub domains\n     from the plurality of SoC sub domains is mapped to a combination of\n     environment, rewards and actions by a second processor. Further,\n     interaction of a plurality of agents is initiated with the reinforcement\n     learning environment for a predefined number of times and further Q\n     value, V value, R value, and A value is updated in the second memory\n     module. Thereby, an optimal chip architecture for designing SoC is\n     acquired using application-domain specific instruction set (ASI).\n", "citations": ["20110010164", "20160063992"], "related": ["15697803", "15499832", "15859698", "15455126"]}, {"id": "20180277962", "patent_code": "10374323", "patent_name": "Slot array antenna and radar having the slot array antenna", "year": "2019", "inventor_and_country_data": " Inventors: \nKamo; Hiroyuki (Kyoto, JP), Kirino; Hideki (Kyoto, JP)  ", "description": "BACKGROUND\n1.  Technical Field\n The present disclosure relates to a slot array antenna.\n2.  Description of the Related Art\n An array antenna (also referred to as an \"antenna array\") including a plurality of radiating elements (also referred to as \"antenna elements\") that are arrayed on a line or a plane finds its use in various applications, e.g., radar and\ncommunication systems.  In order to radiate electromagnetic waves from an array antenna, it is necessary to supply electromagnetic waves (e.g., radio-frequency signal waves) to each antenna element, from a circuit which generates electromagnetic waves\n(\"feed\").  Such feed is performed via a waveguide.  A waveguide is also used to send electromagnetic waves that are received at the antenna elements to a reception circuit.\n Conventionally, feed to an array antenna has often been achieved by using a microstrip line(s).  However, in the case where the frequency of an electromagnetic wave to be transmitted or received by an array antenna is a high frequency above 30\ngigahertz (GHz), as in the millimeter band, a microstrip line will incur a large dielectric loss, thus detracting from the efficiency of the antenna.  Therefore, in such a radio frequency region, an alternative waveguide to replace a microstrip line is\nneeded.\n As alternative waveguide structures to the microstrip line, Patent Documents 1 to 3, and Non-Patent Documents 1 and 2 disclose structures which guide electromagnetic waves by utilizing an artificial magnetic conductor (AMC) extending on both\nsides of a ridge-type waveguide.  Patent Document 1: International Publication No. 2010/050122 Patent Document 2: the specification of U.S.  Pat.  No. 8,803,638 Patent Document 3: the specification of European Patent Application Publication No. 1331688\nNon-Patent Document 1: Kirino et al., \"A 76 GHz Multi-Layered Phased Array Antenna Using a Non-Metal Contact Metamaterial Waveguide\", IEEE Transaction on Antennas and Propagation, Vol. 60, No. 2, February 2012, pp 840-853 Non-Patent Document 2: Kildal et\nal., \"Local Metamaterial-Based Waveguides in Gaps Between Parallel Metal Plates\", IEEE Antennas and Wireless Propagation Letters, Vol. 8, 2009, pp 84-87\nSUMMARY\n Array antennas which are capable of transmitting or receiving electromagnetic waves in a wide frequency band are needed.  The present disclosure provides a slot array antenna having a novel structure that is suitable for transmitting or\nreceiving such a wide band of electromagnetic waves.\n A slot array antenna according to an implementation of the present disclosure includes: a first electrically conductive member having a first electrically conductive surface and a plurality of slots arranged in a first direction along the first\nelectrically conductive surface; a second electrically conductive member having a second electrically conductive surface that opposes the first electrically conductive surface; a waveguide member being interposed between the first and second electrically\nconductive members and having an electrically-conductive waveguide face that opposes the first or second electrically conductive surface, the waveguide member extending alongside the first or second electrically conductive surface; and an artificial\nmagnetic conductor extending around the waveguide member.  The waveguide face, the first or second electrically conductive surface opposing the waveguide face, and the artificial magnetic conductor define a waveguide.  The waveguide member includes a\nstem extending along a direction, a first joint portion and a second joint portion branching out from an end of the stem, the first joint portion and a second joint portion each extending into a region between two adjacent slots among the plurality of\nslots, a first branch connected to an end of the first joint portion, the first branch extending from the end of the first joint portion in the first direction and coupling to one or more of the plurality of slots, and a second branch connected to an end\nof the second joint portion, the second branch extending from the end of the second joint portion in an opposite direction of the first direction and coupling to other one or more of the plurality of slots.  Each of the plurality of slots intersects the\nwaveguide member or splits the waveguide member.  The end of the stem is not located in the region between the two slots as viewed perpendicularly to the first electrically conductive surface.  A length of the first joint portion as measured along the\nwaveguide is not equal to a length of the second joint portion as measured along the waveguide.\n According to an embodiment of the present disclosure, there is provided an array antenna which, even if a wide frequency band is to be used, allows good characteristics to be maintained. BRIEF DESCRIPTION OF THE DRAWINGS\n FIG. 1 is a perspective view schematically showing a non-limiting example of the fundamental construction of a waveguide device.\n FIG. 2A is a diagram schematically showing a cross-sectional construction of a waveguide device 100 as taken parallel to the XZ plane.\n FIG. 2B is a diagram schematically showing another cross-sectional construction of the waveguide device 100 as taken parallel to the XZ plane.\n FIG. 3 is a perspective view schematically showing the waveguide device 100, illustrated so that the spacing between a conductive member 110 and a conductive member 120 is exaggerated for ease of understanding.\n FIG. 4 is a diagram showing an exemplary range of dimension of each member in the structure shown in FIG. 2.\n FIG. 5A is a diagram schematically showing an electromagnetic wave that propagates in a narrow space, i.e., a gap between a waveguide face 122a of a waveguide member 122 and a conductive surface 110a of a conductive member 110.\n FIG. 5B is a diagram schematically showing a cross section of a hollow waveguide 130.\n FIG. 5C is a cross-sectional view showing an implementation where two waveguide members 122 are provided on the conductive member 120.\n FIG. 5D is a diagram schematically showing a cross section of a waveguide device in which two hollow waveguides 130 are placed side-by-side.\n FIG. 6 is a diagram showing an array antenna which is disclosed in FIG. 25 of Patent Document 1.\n FIG. 7 is a diagram showing an exemplary structure which, by utilizing a waveguide member 122 having a branching structure, feeds power toward both ends from somewhere in the middle of an array of plural slots 112.\n FIG. 8 is a perspective view schematically showing partially the structure of a slot array antenna 200 according to an illustrative embodiment of the present disclosure.\n FIG. 9 is an upper plan view showing a portion of the second conductive member 120, the waveguide member 122, and the plurality of conductive rods 124.\n FIG. 10 is an upper plan view showing a variant of a slot array antenna 200 according to an embodiment of the present disclosure.\n FIG. 11A is a plan view showing the structure of a first conductive member 110 in a slot array antenna 300 according to an illustrative embodiment of the present disclosure.\n FIG. 11B is a plan view showing the structure of a second conductive member 110 in a slot array antenna 300 according to an illustrative embodiment of the present disclosure.\n FIG. 12A is a cross-sectional view taken along line A-A in FIG. 11A.\n FIG. 12B is a cross-sectional view showing an example where a plurality of ridge-shaped waveguide members 122 and a plurality of conductive rods (artificial magnetic conductor) are disposed on the conductive surface 110a of the first conductive\nmember 110.\n FIG. 13 is a diagram showing an example of dimensions of one waveguide member 122 and intervals between four slots 112 that are included in one slot row.\n FIG. 14A is an upper plan view showing an exemplary structure of the first conductive member 110 (radiation layer)\n FIG. 14B is an upper plan view showing an exemplary structure of the second conductive member 120 (feeding layer).\n FIG. 14C is a diagram showing an exemplary structure of a cover 140 which protects a circuit board 290.\n FIG. 14D is a diagram showing an appearance of a slot array antenna 300 which is produced by combining the first conductive member 110, the second conductive member 120, and the cover 140.\n FIG. 14E is a diagram showing an appearance of the slot array antenna 300 of FIG. 14D in lateral view, as well as its thickness.\n FIG. 15A is a cross-sectional view showing an exemplary structure in which only a waveguide face 122a, defining an upper face of the waveguide member 122, is electrically conductive, while any portion of the waveguide member 122 other than the\nwaveguide face 122a is not electrically conductive.\n FIG. 15B is a diagram showing a variant in which the waveguide member 122 is not formed on the conductive member 120.\n FIG. 15C is a diagram showing an exemplary structure where the conductive member 120, the waveguide member 122, and each of the plurality of conductive rods 124 are composed of a dielectric surface that is coated with an electrically conductive\nmaterial such as a metal.\n FIG. 15D is a diagram showing an exemplary structure in which dielectric layers 110b and 120b are respectively provided on the outermost surfaces of conductive members 110 and 120, a waveguide member 122, and conductive rods 124.\n FIG. 15E is a diagram showing another exemplary structure in which dielectric layers 110b and 120b are respectively provided on the outermost surfaces of conductive members 110 and 120, a waveguide member 122, and conductive rods 124.\n FIG. 15F is a diagram showing an example where the height of the waveguide member 122 is lower than the height of the conductive rods 124, and a portion of a conductive surface 110a of the conductive member 110 that opposes the waveguide face\n122a protrudes toward the waveguide member 122.\n FIG. 15G is a diagram showing an example where, further in the structure of FIG. 15F, portions of the conductive surface 110a that oppose the conductive rods 124 protrude toward the conductive rods 124.\n FIG. 16A is a diagram showing an example where a conductive surface 110a of the conductive member 110 is shaped as a curved surface.\n FIG. 16B is a diagram showing an example where also a conductive surface 120a of the conductive member 120 is shaped as a curved surface.\n FIG. 17 is a diagram showing another exemplary shape of a slot.\n FIG. 18 is a diagram showing a planar layout where the four kinds of slots 112a through 112d shown in FIG. 17 are disposed on a waveguide member 122.\n FIG. 19 is a diagram showing a driver's vehicle 500, and a preceding vehicle 502 that is traveling in the same lane as the driver's vehicle 500.\n FIG. 20 is a diagram showing an onboard radar system 510 of the driver's vehicle 500.\n FIG. 21A is a diagram showing a relationship between an array antenna AA of the onboard radar system 510 and plural arriving waves k.\n FIG. 21B is a diagram showing the array antenna AA receiving the k.sup.th arriving wave.\n FIG. 22 is a block diagram showing an exemplary fundamental construction of a vehicle travel controlling apparatus 600 according to the present disclosure.\n FIG. 23 is a block diagram showing another exemplary construction for the vehicle travel controlling apparatus 600.\n FIG. 24 is a block diagram showing an example of a more specific construction of the vehicle travel controlling apparatus 600.\n FIG. 25 is a block diagram showing a more detailed exemplary construction of the radar system 510 according to this Application Example.\n FIG. 26 is a diagram showing change in frequency of a transmission signal which is modulated based on the signal that is generated by a triangular wave generation circuit 581.\n FIG. 27 is a diagram showing a beat frequency fu in an \"ascent\" period and a beat frequency fd in a \"descent\" period.\n FIG. 28 is a diagram showing an exemplary implementation in which a signal processing circuit 560 is implemented in hardware including a processor PR and a memory device MD.\n FIG. 29 is a diagram showing a relationship between three frequencies f1, f2 and f3.\n FIG. 30 is a diagram showing a relationship between synthetic spectra F1 to F3 on a complex plane.\n FIG. 31 is a flowchart showing the procedure of a process of determining relative velocity and distance.\n FIG. 32 is a diagram concerning a fusion apparatus in which a radar system 510 having a slot array antenna and an onboard camera system 700 are included.\n FIG. 33 is a diagram illustrating how placing a millimeter wave radar 510 and a camera at substantially the same position within the vehicle room may allow them to acquire an identical field of view and line of sight, thus facilitating a\nmatching process.\n FIG. 34 is a diagram showing an exemplary construction for a monitoring system 1500 based on millimeter wave radar.\n FIG. 35 is a block diagram showing a construction for a digital communication system 800A.\n FIG. 36 is a block diagram showing an exemplary communication system 800B including a transmitter 810B which is capable of changing its radio wave radiation pattern.\n FIG. 37 is a block diagram showing an exemplary communication system 800C implementing a MIMO function.\nDETAILED DESCRIPTION\n Prior to describing embodiments of the present disclosure, findings that form the basis of the present disclosure will be described.\n A ridge waveguide which is disclosed in the aforementioned Patent Documents 1 to 3, and Non-Patent Documents 1 and 2 is provided in a waffle iron structure which is capable of functioning as an artificial magnetic conductor.  A ridge waveguide\nin which such an artificial magnetic conductor is utilized based on the present disclosure (which hereinafter may be referred to as a WRG: Waffle-iron Ridge waveguide) is able to realize an antenna feeding network with low losses in the microwave or the\nmillimeter wave band.  Moreover, use of such a ridge waveguide allows antenna elements to be disposed with a high density.  Hereinafter, an exemplary fundamental construction and operation of such a waveguide structure will be described.\n An artificial magnetic conductor is a structure which artificially realizes the properties of a perfect magnetic conductor (PMC), which does not exist in nature.  One property of a perfect magnetic conductor is that \"a magnetic field on its\nsurface has zero tangential component\".  This property is the opposite of the property of a perfect electric conductor (PEC), i.e., \"an electric field on its surface has zero tangential component\".  Although no perfect magnetic conductor exists in\nnature, it can be embodied by an artificial structure, e.g., an array of a plurality of electrically conductive rods.  An artificial magnetic conductor functions as a perfect magnetic conductor in a specific frequency band which is defined by its\nstructure.  An artificial magnetic conductor restrains or prevents an electromagnetic wave of any frequency that is contained in the specific frequency band (propagation-restricted band) from propagating along the surface of the artificial magnetic\nconductor.  For this reason, the surface of an artificial magnetic conductor may be referred to as a high impedance surface.\n In the waveguide devices disclosed in Patent Documents 1 to 3 and Non-Patent Documents 1 and 2, an artificial magnetic conductor is realized by a plurality of electrically conductive rods which are arrayed along row and column directions.  Such\nrods are projections which may also be referred to as posts or pins.  Each of these waveguide devices includes, as a whole, a pair of opposing electrically conductive plates.  One conductive plate has a ridge protruding toward the other conductive plate,\nand stretches of an artificial magnetic conductor extending on both sides of the ridge.  An upper face (i.e., its electrically conductive face) of the ridge opposes, via a gap, an electrically conductive surface of the other conductive plate.  An\nelectromagnetic wave (signal wave) of a wavelength which is contained in the propagation-restricted band of the artificial magnetic conductor propagates along the ridge, in the space (gap) between this conductive surface and the upper face of the ridge.\n FIG. 1 is a perspective view schematically showing a non-limiting example of a fundamental construction of such a waveguide device.  FIG. 1 shows XYZ coordinates along X, Y and Z directions which are orthogonal to one another.  The waveguide\ndevice 100 shown in the figure includes a plate-like first electrically conductive member 110 and a plate shape (plate-like) second electrically conductive member 120, which are in opposing and parallel positions to each other.  A plurality of\nelectrically conductive rods 124 are arrayed on the second conductive member 120, together with a waveguide member 122.\n Note that any structure appearing in a figure of the present application is shown in an orientation that is selected for ease of explanation, which in no way should limit its orientation when an embodiment of the present disclosure is actually\npracticed.  Moreover, the shape and size of a whole or a part of any structure that is shown in a figure should not limit its actual shape and size.\n FIG. 2A is a diagram schematically showing the construction of a cross section of the waveguide device 100 in FIG. 1, taken parallel to the XZ plane.  As shown in FIG. 2A, the conductive member 110 has an electrically conductive surface 110a on\nthe side facing the conductive member 120.  The conductive surface 110a has a two-dimensional expanse along a plane which is orthogonal to the axial direction (i.e., the Z direction) of the conductive rods 124 (i.e., a plane which is parallel to the XY\nplane).  Although the conductive surface 110a is shown to be a smooth plane in this example, the conductive surface 110a does not need to be a plane, as will be described later.\n FIG. 3 is a perspective view schematically showing the waveguide device 100, illustrated so that the spacing between the conductive member 110 and the conductive member 120 is exaggerated for ease of understanding.  In an actual waveguide device\n100, as shown in FIG. 1 and FIG. 2A, the spacing between the conductive member 110 and the conductive member 120 is narrow, with the conductive member 110 covering over all of the conductive rods 124 on the conductive member 120.\n FIG. 1 to FIG. 3 only show portions of the waveguide device 100.  The conductive members 110 and 120, the waveguide member 122, and the plurality of conductive rods 124 actually extend to outside of the portions illustrated in the figures.  At\nan end of the waveguide member 122, as will be described later, a choke structure for preventing electromagnetic waves from leaking into the external space is provided.  The choke structure may include a row of conductive rods that are adjacent to the\nend of the waveguide member 122, for example.\n See FIG. 2A again.  The plurality of conductive rods 124 arrayed on the conductive member 120 each have a leading end 124a opposing the conductive surface 110a.  In the example shown in the figure, the leading ends 124a of the plurality of\nconductive rods 124 are on the same plane.  This plane defines the surface 125 of an artificial magnetic conductor.  Each conductive rod 124 does not need to be entirely electrically conductive, so long as it at least includes an electrically conductive\nlayer that extends along the upper face and the side face of the rod-like structure.  Although this electrically conductive layer may be located at the surface layer of the rod-like structure, the surface layer may be composed of an insulation coating or\na resin layer with no electrically conductive layer existing on the surface of the rod-like structure.  Moreover, each conductive member 120 does not need to be entirely electrically conductive, so long as it can support the plurality of conductive rods\n124 to constitute an artificial magnetic conductor.  Of the surfaces of the conductive member 120, a face 120a carrying the plurality of conductive rods 124 may be electrically conductive, such that the electrical conductor electrically interconnects the\nsurfaces of adjacent ones of the plurality of conductive rods 124.  Moreover, the electrically conductive layer of the second conductive member 120 may be covered with an insulation coating or a resin layer.  In other words, the entire combination of the\nconductive member 120 and the plurality of conductive rods 124 may at least include an electrically conductive layer with rises and falls opposing the conductive surface 110a of the conductive member 110.\n On the conductive member 120, a ridge-like waveguide member 122 is provided among the plurality of conductive rods 124.  More specifically, stretches of an artificial magnetic conductor are present on both sides of the waveguide member 122, such\nthat the waveguide member 122 is sandwiched between the stretches of artificial magnetic conductor on both sides.  As can be seen from FIG. 3, the waveguide member 122 in this example is supported on the conductive member 120, and extends linearly along\nthe Y direction.  In the example shown in the figure, the waveguide member 122 has the same height and width as those of the conductive rods 124.  As will be described later, however, the height and width of the waveguide member 122 may have different\nvalues from those of the conductive rod 124.  Unlike the conductive rods 124, the waveguide member 122 extends along a direction (which in this example is the Y direction) in which to guide electromagnetic waves along the conductive surface 110a. \nSimilarly, the waveguide member 122 does not need to be entirely electrically conductive, but may at least include an electrically conductive waveguide face 122a opposing the conductive surface 110a of the conductive member 110.  The conductive member\n120, the plurality of conductive rods 124, and the waveguide member 122 may be portions of a continuous single-piece body.  Furthermore, the conductive member 110 may also be a portion of such a single-piece body.\n The waveguide face of the waveguide member 122 has a stripe shape that extends along the conductive surface 110a of the first conductive member 110.  In the present specification, a \"stripe shape\" means a shape which is defined by a single\nstripe, rather than a shape constituted by stripes.  Not only shapes that extend linearly in one direction, but also any shape that bends or branches along the way is also encompassed by a \"stripe shape\".  Note that a portion that undergoes a change in\nheight or width may be provided on the waveguide face 122a.  In that case, too, the shape still falls under the meaning of \"stripe shape\" so long as it includes a portion that extends in one direction as viewed from the normal direction of the waveguide\nface 122a.  A \"stripe shape\" may be referred to as a \"strip shape\".\n On both sides of the waveguide member 122, the space between the surface 125 of each stretch of artificial magnetic conductor and the conductive surface 110a of the conductive member 110 does not allow an electromagnetic wave of any frequency\nthat is within a specific frequency band to propagate.  This frequency band is called a \"prohibited band\".  The artificial magnetic conductor is designed so that the frequency of an electromagnetic wave (signal wave) to propagate in the waveguide device\n100 (which may hereinafter be referred to as the \"operating frequency\") is contained in the prohibited band.  The prohibited band may be adjusted based on the following: the height of the conductive rods 124, i.e., the depth of each groove formed between\nadjacent conductive rods 124; the width of each conductive rod 124; the interval between conductive rods 124; and the size of the gap between the leading end 124a and the conductive surface 110a of each conductive rod 124.\n Next, with reference to FIG. 4, the dimensions, shape, positioning, and the like of each member will be described.\n FIG. 4 is a diagram showing an exemplary range of dimension of each member in the structure shown in FIG. 2A.  The waveguide device is used for at least one of transmission and reception of electromagnetic waves of a predetermined band (referred\nto as the \"operating frequency band\").  In the present specification, .lamda.o denotes a representative value of wavelengths in free space (e.g., a central wavelength corresponding to a center frequency in the operating frequency band) of an\nelectromagnetic wave (signal wave) propagating in a waveguide extending between the conductive surface 110a of the conductive member 110 and the waveguide face 122a of the waveguide member 122.  Moreover, .lamda.m denotes a wavelength, in free space, of\nan electromagnetic wave of the highest frequency in the operating frequency band.  The end of each conductive rod 124 that is in contact with the conductive member 120 is referred to as the \"root\".  As shown in FIG. 4, each conductive rod 124 has the\nleading end 124a and the root 124b.  Examples of dimensions, shapes, positioning, and the like of the respective members are as follows.\n (1) Width of the Conductive Rod\n The width (i.e., the size along the X direction and the Y direction) of the conductive rod 124 may be set to less than .lamda.m/2.  Within this range, resonance of the lowest order can be prevented from occurring along the X direction and the Y\ndirection.  Since resonance may possibly occur not only in the X and Y directions but also in any diagonal direction in an X-Y cross section, the diagonal length of an X-Y cross section of the conductive rod 124 is also preferably less than .lamda.m/2. \nThe lower limit values for the rod width and diagonal length will conform to the minimum lengths that are producible under the given manufacturing method, but is not particularly limited.\n (2) Distance from the Root of the Conductive Rod to the Conductive Surface of the Conductive Member\n The distance from the root 124b of each conductive rod 124 to the conductive surface 110a of the conductive member 110 may be longer than the height of the conductive rods 124, while also being less than .lamda.m/2.  When the distance is\n.lamda.m/2 or more, resonance may occur between the root 124b of each conductive rod 124 and the conductive surface 110a, thus reducing the effect of signal wave containment.\n The distance from the root 124b of each conductive rod 124 to the conductive surface 110a of the conductive member 110 corresponds to the spacing between the conductive member 110 and the conductive member 120.  For example, when a signal wave\nof 76.5.+-.0.5 GHz (which belongs to the millimeter band or the extremely high frequency band) propagates in the waveguide, the wavelength of the signal wave is in the range from 3.8923 mm to 3.9435 mm.  Therefore, .lamda.m equals 3.8923 mm in this case,\nso that the spacing between the conductive member 110 and the conductive member 120 may be set to less than a half of 3.8923 mm.  So long as the conductive member 110 and the conductive member 120 realize such a narrow spacing while being disposed\nopposite from each other, the conductive member 110 and the conductive member 120 do not need to be strictly parallel.  Moreover, when the spacing between the conductive member 110 and the conductive member 120 is less than .lamda.m/2, a whole or a part\nof the conductive member 110 and/or the conductive member 120 may be shaped as a curved surface.  On the other hand, the conductive members 110 and 120 each have a planar shape (i.e., the shape of their region as perpendicularly projected onto the XY\nplane) and a planar size (i.e., the size of their region as perpendicularly projected onto the XY plane) which may be arbitrarily designed depending on the purpose.\n Although the conductive surface 120a is illustrated as a plane in the example shown in FIG. 2A, embodiments of the present disclosure are not limited thereto.  For example, as shown in FIG. 2B, the conductive surface 120a may be the bottom parts\nof faces each of which has a cross section similar to a U-shape or a V-shape.  The conductive surface 120a will have such a structure when each conductive rod 124 or the waveguide member 122 is shaped with a width which increases toward the root.  Even\nwith such a structure, the device shown in FIG. 2B can function as the waveguide device according to an embodiment of the present disclosure so long as the distance between the conductive surface 110a and the conductive surface 120a is less than a half\nof the wavelength .lamda.m.\n (3) Distance L2 from the Leading End of the Conductive Rod to the Conductive Surface\n The distance L2 from the leading end 124a of each conductive rod 124 to the conductive surface 110a is set to less than .lamda.m/2.  When the distance is .lamda.m/2 or more, a propagation mode where electromagnetic waves reciprocate between the\nleading end 124a of each conductive rod 124 and the conductive surface 110a may occur, thus no longer being able to contain an electromagnetic wave.  Note that, among the plurality of conductive rods 124, at least those which are adjacent to the\nwaveguide member 122 do not have their leading ends in electrical contact with the conductive surface 110a.  As used herein, the leading end of a conductive rod not being in electrical contact with the conductive surface means either of the following\nstates: there being an air gap between the leading end and the conductive surface; or the leading end of the conductive rod and the conductive surface adjoining each other via an insulating layer which may exist in the leading end of the conductive rod\nor in the conductive surface.\n (4) Arrangement and Shape of Conductive Rods\n The interspace between two adjacent conductive rods 124 among the plurality of conductive rods 124 has a width of less than .lamda.m/2, for example.  The width of the interspace between any two adjacent conductive rods 124 is defined by the\nshortest distance from the surface (side face) of one of the two conductive rods 124 to the surface (side face) of the other.  This width of the interspace between rods is to be determined so that resonance of the lowest order will not occur in the\nregions between rods.  The conditions under which resonance will occur are determined based by a combination of: the height of the conductive rods 124; the distance between any two adjacent conductive rods; and the capacitance of the air gap between the\nleading end 124a of each conductive rod 124 and the conductive surface 110a.  Therefore, the width of the interspace between rods may be appropriately determined depending on other design parameters.  Although there is no clear lower limit to the width\nof the interspace between rods, for manufacturing ease, it may be e.g. .lamda.m/16 or more when an electromagnetic wave in the extremely high frequency range is to be propagated.  Note that the interspace does not need to have a constant width.  So long\nas it remains less than .lamda.m/2, the interspace between conductive rods 124 may vary.\n The arrangement of the plurality of conductive rods 124 is not limited to the illustrated example, so long as it exhibits a function of an artificial magnetic conductor.  The plurality of conductive rods 124 do not need to be arranged in\northogonal rows and columns; the rows and columns may be intersecting at angles other than 90 degrees.  The plurality of conductive rods 124 do not need to form a linear array along rows or columns, but may be in a dispersed arrangement which does not\npresent any straightforward regularity.  The conductive rods 124 may also vary in shape and size depending on the position on the conductive member 120.\n The surface 125 of the artificial magnetic conductor that are constituted by the leading ends 124a of the plurality of conductive rods 124 does not need to be a strict plane, but may be a plane with minute rises and falls, or even a curved\nsurface.  In other words, the conductive rods 124 do not need to be of uniform height, but rather the conductive rods 124 may be diverse so long as the array of conductive rods 124 is able to function as an artificial magnetic conductor.\n Each conductive rod 124 does not need to have a prismatic shape as shown in the figure, but may have a cylindrical shape, for example.  Furthermore, each conductive rod 124 does not need to have a simple columnar shape.  The artificial magnetic\nconductor may also be realized by any structure other than an array of conductive rods 124, and various artificial magnetic conductors are applicable to the waveguide device of the present disclosure.  Note that, when the leading end 124a of each\nconductive rod 124 has a prismatic shape, its diagonal length is preferably less than .lamda.m/2.  When the leading end 124a of each conductive rod 124 is shaped as an ellipse, the length of its major axis is preferably less than .lamda.m/2.  Even when\nthe leading end 124a has any other shape, the dimension across it is preferably less than .lamda.m/2 even at the longest position.\n The height of each conductive rod 124 (in particular, those conductive rods 124 which are adjacent to the waveguide member 122), i.e., the length from the root 124b to the leading end 124a, may be set to a value which is shorter than the\ndistance (i.e., less than .lamda.m/2) between the conductive surface 110a and the conductive surface 120a, e.g., .lamda.o/4.\n (5) Width of the Waveguide Face\n The width of the waveguide face 122a of the waveguide member 122, i.e., the size of the waveguide face 122a along a direction which is orthogonal to the direction that the waveguide member 122 extends, may be set to less than .lamda.m/2 (e.g.\n.lamda.o/8).  If the width of the waveguide face 122a is .lamda.m/2 or more, resonance will occur along the width direction, which will prevent any WRG from operating as a simple transmission line.\n (6) Height of the Waveguide Member\n The height (i.e., the size along the Z direction in the example shown in the figure) of the waveguide member 122 is set to less than .lamda.m/2.  The reason is that, if the distance is .lamda.m/2 or more, the distance between the root 124b of\neach conductive rod 124 and the conductive surface 110a will be .lamda.m/2 or more.\n (7) Distance L1 Between the Waveguide Face and the Conductive Surface\n The distance L1 between the waveguide face 122a of the waveguide member 122 and the conductive surface 110a is set to less than .lamda.m/2.  If the distance is .lamda.m/2 or more, resonance will occur between the waveguide face 122a and the\nconductive surface 110a, which will prevent functionality as a waveguide.  In one example, the distance L1 is .lamda.m/4 or less.  In order to ensure manufacturing ease, when an electromagnetic wave in the extremely high frequency range is to propagate,\nthe distance L1 is preferably .lamda.m/16 or more, for example.\n The lower limit of the distance L1 between the conductive surface 110a and the waveguide face 122a and the lower limit of the distance L2 between the conductive surface 110a and the leading end 124a of each conductive rod 124 depends on the\nmachining precision, and also on the precision when assembling the two upper/lower conductive members 110 and 120 so as to be apart by a constant distance.  When a pressing technique or an injection technique is used, the practical lower limit of the\naforementioned distance is about 50 micrometers (.mu.m).  In the case of using an MEMS (Micro-Electro-Mechanical System) technique to make a product in e.g. the terahertz range, the lower limit of the aforementioned distance is about 2 to about 3 .mu.m.\n In the waveguide device 100 of the above-described construction, a signal wave of the operating frequency is unable to propagate in the space between the surface 125 of the artificial magnetic conductor and the conductive surface 110a of the\nconductive member 110, but propagates in the space between the waveguide face 122a of the waveguide member 122 and the conductive surface 110a of the conductive member 110.  Unlike in a hollow waveguide, the width of the waveguide member 122 in such a\nwaveguide structure does not need to be equal to or greater than a half of the wavelength of the electromagnetic wave to propagate.  Moreover, the conductive member 110 and the conductive member 120 do not need to be interconnected by a metal wall that\nextends along the thickness direction (i.e., in parallel to the YZ plane).\n FIG. 5A schematically shows an electromagnetic wave that propagates in a narrow space, i.e., a gap between the waveguide face 122a of the waveguide member 122 and the conductive surface 110a of the conductive member 110.  Three arrows in FIG. 5A\nschematically indicate the orientation of an electric field of the propagating electromagnetic wave.  The electric field of the propagating electromagnetic wave is perpendicular to the conductive surface 110a of the conductive member 110 and to the\nwaveguide face 122a.\n On both sides of the waveguide member 122, stretches of artificial magnetic conductor that are created by the plurality of conductive rods 124 are present.  An electromagnetic wave propagates in the gap between the waveguide face 122a of the\nwaveguide member 122 and the conductive surface 110a of the conductive member 110.  FIG. 5A is schematic, and does not accurately represent the magnitude of an electromagnetic field to be actually created by the electromagnetic wave.  A part of the\nelectromagnetic wave (electromagnetic field) propagating in the space over the waveguide face 122a may have a lateral expanse, to the outside (i.e., toward where the artificial magnetic conductor exists) of the space that is delineated by the width of\nthe waveguide face 122a.  In this example, the electromagnetic wave propagates in a direction (i.e., the Y direction) which is perpendicular to the plane of FIG. 5A.  As such, the waveguide member 122 does not need to extend linearly along the Y\ndirection, but may include a bend(s) and/or a branching portion(s) not shown.  Since the electromagnetic wave propagates along the waveguide face 122a of the waveguide member 122, the direction of propagation would change at a bend, whereas the direction\nof propagation would ramify into plural directions at a branching portion.\n In the waveguide structure of FIG. 5A, no metal wall (electric wall), which would be indispensable to a hollow waveguide, exists on both sides of the propagating electromagnetic wave.  Therefore, in the waveguide structure of this example, \"a\nconstraint due to a metal wall (electric wall)\" is not included in the boundary conditions for the electromagnetic field mode to be created by the propagating electromagnetic wave, and the width (size along the X direction) of the waveguide face 122a is\nless than a half of the wavelength of the electromagnetic wave.\n For reference, FIG. 5B schematically shows a cross section of a hollow waveguide 130.  With arrows, FIG. 5B schematically shows the orientation of an electric field of an electromagnetic field mode (TE.sub.10) that is created in the internal\nspace 132 of the hollow waveguide 130.  The lengths of the arrows correspond to electric field intensities.  The width of the internal space 132 of the hollow waveguide 130 needs to be set to be broader than a half of the wavelength.  In other words, the\nwidth of the internal space 132 of the hollow waveguide 130 cannot be set to be smaller than a half of the wavelength of the propagating electromagnetic wave.\n FIG. 5C is a cross-sectional view showing an implementation where two waveguide members 122 are provided on the conductive member 120.  Thus, an artificial magnetic conductor that is created by the plurality of conductive rods 124 exists between\nthe two adjacent waveguide members 122.  More accurately, stretches of artificial magnetic conductor created by the plurality of conductive rods 124 are present on both sides of each waveguide member 122, such that each waveguide member 122 is able to\nindependently propagate an electromagnetic wave.\n For reference's sake, FIG. 5D schematically shows a cross section of a waveguide device in which two hollow waveguides 130 are placed side-by-side.  The two hollow waveguides 130 are electrically insulated from each other.  Each space in which\nan electromagnetic wave is to propagate needs to be surrounded by a metal wall that defines the respective hollow waveguide 130.  Therefore, the interval between the internal spaces 132 in which electromagnetic waves are to propagate cannot be made\nsmaller than a total of the thicknesses of two metal walls.  Usually, a total of the thicknesses of two metal walls is longer than a half of the wavelength of a propagating electromagnetic wave.  Therefore, it is difficult for the interval between the\nhollow waveguides 130 (i.e., interval between their centers) to be shorter than the wavelength of a propagating electromagnetic wave.  Particularly for electromagnetic waves of wavelengths in the extremely high frequency range (i.e., electromagnetic wave\nwavelength: 10 mm or less) or even shorter wavelengths, a metal wall which is sufficiently thin relative to the wavelength is difficult to be formed.  This presents a cost problem in commercially practical implementation.\n On the other hand, a waveguide device 100 including an artificial magnetic conductor can easily realize a structure in which waveguide members 122 are placed close to one another.  Thus, such a waveguide device 100 can be suitably used in an\narray antenna that includes plural antenna elements in a close arrangement.\n The structure of the waveguide device 100 as such is applicable to a slot array antenna which includes a plurality of slots as radiating elements.  An example of such a slot array antenna is disclosed in Patent Document 1, for example.  FIG. 25\nof Patent Document 1 is herein incorporated by reference, as FIG. 6.\n FIG. 6 shows an array antenna which is disclosed in FIG. 25 of Patent Document 1.  In this array antenna, on an electrically-conductive plate 12 which is opposed to a ridge 95, a row of radiating elements 112aa to 112aj (a row of plural slots)\nare disposed at equal intervals.  An electromagnetic wave is fed from the rear face side of the plate 13, having the ridge 95 provided thereon, to a waveguide extending on the ridge 95.  The row of radiating elements 112aa to 112aj are disposed in\npositions that are opposite from places on the ridge 95 where a maximum resonance current exists, this resonance current being in the same direction.  It is disclosed that such a construction allows electromagnetic waves of the same phase to be radiated\nfrom the plurality of radiating element.\n However, when a wide frequency band is used, discrepancies in phase of electromagnetic waves at the positions of the plurality of radiating elements will be a problem.  In particular, as in the construction shown in FIG. 6, in a construction\nwhere slots are disposed also at positions that are distant from one of the ends of the ridge (which is a feed portion), there will be large discrepancies in phase at the slot positions that are distant from the feed member, thus making it difficult to\nobtain the radiation characteristics that are required of an array antenna.\n Therefore, instead of the construction shown in FIG. 6, it might be conceivable to adopt a feeding structure shown in FIG. 7.  FIG. 7 shows an exemplary structure which, by utilizing a waveguide member 122 having a branching structure, feeds\npower toward slots 112 at both ends from somewhere in the middle of an array of plural slots 112.  For ease of understanding, with dotted lines, FIG. 7 shows the positions of four slots 112 that are made in the first conductive member 110.  Although the\nshape of the opening of each slot 112 in this example is an H shape resembling the alphabetical letter \"H\", it may be other shapes.  These slots 112 are opposed to two portions of the waveguide member 122 that branch out from a branching portion and\nextend in mutually opposite directions.  The plurality of slots 122 are disposed at equal intervals.  The slot interval is set equal to the wavelength .lamda.g of an electromagnetic wave propagating in the waveguide, for example.  By adopting such a\nstructure, the propagating distance of the electromagnetic wave can be reduced as compared to a structure in which feeding is performed in a serial manner from one end to the other end of a slot row.  Therefore, even in the case where the operating\nfrequency band is wide, the discrepancies in phase of electromagnetic waves at the positions of the plurality of slot 112 can be reduced.\n However, with such a structure, electromagnetic waves propagate in opposite directions in the two branched portions of the waveguide, and thus the slots 112 cannot be placed in positions that are symmetric with respect to the branching portion. \nIn the case where all slots 112 are to be excited in the same phase, the distance from the branching portion to the lower slot needs to be about .lamda.g/2 smaller than the distance from the branching portion to the upper slot.  As a result, the slot\nthat is located immediately below the branching portion would be disposed adjacent to the branching portion.\n The inventors faced the following problem: when a slot is disposed adjacent to a branching portion in a slot array antenna based on WRG, it is difficult to ensure matching at the branching portion.  In seeking a solution to this problem, the\ninventors have arrived at a novel slot array antenna construction which, while feeding power from near the center of a slot array antenna, attains matching at the branching portion.\n Hereinafter, constructions for embodiments of the present disclosure will be described.  Note however that unnecessarily detailed descriptions may be omitted.  For example, detailed descriptions on what is well known in the art or redundant\ndescriptions on what is substantially the same constitution may be omitted.  This is to avoid lengthy description, and facilitate the understanding of those skilled in the art.  The accompanying drawings and the following description, which are provided\nby the inventors so that those skilled in the art can sufficiently understand the present disclosure, are not intended to limit the scope of claims.  In the present specification, identical or similar constituent elements are denoted by identical\nreference numerals.\nEmbodiments\n FIG. 8 is a perspective view schematically showing partially the structure of a slot array antenna 200 according to an illustrative embodiment of the present disclosure.  Similarly to the above-described waveguide device 100, the slot array\nantenna 200 includes a first conductive member 110, a second conductive member 120, a waveguide member 122, and a plurality of conductive rods 124.  The first conductive member 110 has a plurality of slots 112 arranged along the first direction (i.e.,\nthe Y direction) alongside the conductive surface 110a.  Although the first conductive member 110 has H-shaped four slots 112 in the present embodiment, the number and shapes of slots 112 are not limited to this example.\n FIG. 9 is an upper plan view showing a portion of the second conductive member 120, the waveguide member 122, and the plurality of conductive rods 124.  For ease of understanding, with dotted lines, FIG. 9 shows the positions of the plurality of\nslots 112 in the first conductive member 110.  The waveguide member 122, which has an electrically-conductive waveguide face 122a that opposes the conductive surface 110a of the first conductive member 110, extends alongside the conductive surface 110a. \nThe plurality of conductive rods 124, which are disposed around the waveguide member 122, constitute an artificial magnetic conductor.  The conductive surface 110a, the waveguide face 122a of the waveguide member 122, and the artificial magnetic\nconductor define a waveguide in a gap extending between the first conductive surface 110a and the waveguide face 122a.\n The slot array antenna 200 is used for at least one of transmission and reception of electromagnetic waves of a predetermined band (referred to as the \"operating frequency band\").  Given that, among the electromagnetic waves of the operating\nfrequency band, an electromagnetic wave of the highest frequency has a wavelength .lamda.m in free space, then, the width of the waveguide member 122, the width of each conductive rod 124, the width of a space between two adjacent conductive rods 124,\nthe distance between the first conductive surface 110a and the second conductive surface 120a, and the width of a space between the waveguide member 122 and a conductive rod 124 that is adjacent to the waveguide member 122 are each less than .lamda.m/2.\n The waveguide member 122 according to the present embodiment is structured so that a plurality of linearly extending portions are interconnected via a branching portion and a plurality of bends.  The waveguide member 122 includes: a stem 122A\nextending along the Y direction; a first joint portion 122B and a second joint portion 122C branching out from an end (branching portion) of the stem 122A; and a first branch 122D and a second branch 122E respectively connected to ends of the first joint\nportion 122B and the second joint portion 122C.  In FIG. 9, for ease of understanding, the first joint portion 122B and the second joint portion 122C are shown hatched.  From the branching portion, which is an end of the stem 122A, the first joint\nportion 122B and the second joint portion 122C extend respectively into a region between two adjacent slots 112 among the plurality of slots 112.  Herein, the region between two adjacent slots 112 is defined as, within the region between the first\nconductive member 110 and the second conductive member 120, a subregion that exists between the two slots 112 as viewed from the normal direction (i.e., the Z direction) of the first conductive surface 110a.  In FIG. 9, a region which is surrounded by\ndot-dash lines represents the region 126 between two slots 112.  The end of the stem 122A is not located in the region between the two slots 112.  The first branch 122D extends in the first direction (i.e., the Y direction) from the end of the first\njoint portion 122B, and couples to some of the plurality of slots 112 (i.e., the upper two in FIG. 9).  On the other hand, from the end of the second joint portion 122C, the second branch 122E extends in the opposite direction (i.e., the -Y direction) of\nthe first direction, and couples to the other ones of the plurality of slots 112 (i.e., the lower two in FIG. 9).\n In the present embodiment, a half, i.e., two, of the four slots 112 couple to the first branch 122D, whereas the other two couple to the second branch 122E.  By ensuring that the number of slots coupling to each branch is equal, it becomes easy\nto enhance symmetry of directivity along the Y direction of the slot array antenna 200.  Note that the number of slots is not limited to four.  Generally speaking, an even number of slots will allow for a similar slot arrangement as in the present\nembodiment.  However, too large a number of slots will result in a long distance existing along the waveguide from the branching portion to a slot near either end, which makes it difficult to excite the respective slots in the same phase across a wide\nfrequency band.  Therefore, in order to excite the respective slots in the same phase across a wide frequency band, it is more preferable that the number of slots is eight or less.\n In the present specification, when it is said that a slot and a branch of a waveguide member \"couple\" to each other, it is meant that the slot and the waveguide face associated with that branch are of such a physical relationship that\ntransmission of an electromagnetic wave between them is possible.  For example, when an electromagnetic wave propagates along the waveguide face associated with that branch, if at least a portion of the electromagnetic wave passes through the slot and is\nradiated into the external space, the slot and the branch are said to be coupling to each other.  Conversely, when an electromagnetic wave impinges on a slot from the external space, if the electromagnetic wave after passing through the slot propagates\nalong the waveguide face of the branch, the slot and the branch are also said to be coupling to each other.  A typical example of a slot and a branch of a waveguide member \"coupling\" to each other is, as in the example of FIG. 9, where the slot is\nopposed to the waveguide face associated with that branch.  However, the slot may be shifted from the position opposing the waveguide face.  The slot may overlap neither of the two edges of the waveguide face as viewed from a direction perpendicular to\nthe waveguide face.  In such a construction, too, the slot and the branch are still \"coupling\" to each other so long as transmission of an electromagnetic wave is possible between the slot and the waveguide face associated with that branch.  Furthermore,\na construction may be adopted where a ridge-shaped waveguide member is connected to the first conductive member having a slot, such that the slot splits the waveguide member.  In such a construction, too, so long as an electromagnetic wave is capable of\npropagating between the slot and the waveguide face by passing between the two opposite end faces at the portion where the waveguide member is split, the slot and the branch of the waveguide member are said to be coupling to each other.  In such a\nconstruction, the waveguide face of the waveguide member is opposed to the second conductive surface, rather than the first conductive surface.\n Thus, each of the plurality of slots 112 intersects the waveguide member 122, or splits the waveguide member 122.  When it is said that a slot 112 \"intersects\" the waveguide member 122, it is meant that the direction that a main portion of the\nslot 112 extends and the direction that the waveguide member 122 extends intersect each other, as viewed from a direction perpendicular to the waveguide face 122a of the waveguide member 122.  The main portion of a slot 112 is defined as a portion which\nextends in a single direction and which includes the central portion where the electric field to be created in the slot 112 is strongest.  In the H-shaped slots 122 shown in FIG. 9, a lateral portion connecting between the central portions of a pair of\nvertical portions of the H shape corresponds to the main portion.\n The stem 122A of the waveguide member 122 is connected to a microwave integrated circuit (electronic circuit) not shown, e.g., an MMIC (Monolithic Microwave Integrated Circuit), either directly or via another waveguide.  During transmission, an\nelectromagnetic wave is fed from the microwave integrated circuit to the waveguide member 122.  A portion of the electromagnetic wave propagates along the first joint portion 122B and the first branch 122D, so as to be radiated through the upper two\nslots in FIG. 9.  Another portion of the electromagnetic wave propagates along the second joint portion 122C and the second branch 122E, and is radiated through the lower two slots in FIG. 9.  During reception, conversely, electromagnetic waves impinging\non the four slots 112 propagate along the waveguide on the waveguide member 122, so as to be sent to the microwave integrated circuit.\n In the example shown in FIG. 9, the stem 122A includes a portion extending in parallel to the first branch 122D and the second branch 122E.  Between the second branch 122E and the stem 122A, one row of conductive rods 124 that are arranged along\nthe Y direction are provided.  As a result, mutual interference of electromagnetic waves between the stem 122A and the second branch 122E can be suppressed.  Note that the stem 122A does not need to extend in the same direction that the first branch 122D\nor the second branch 122E extends, but may be structured so as to extend in a different direction therefrom.\n Conventionally, it has been believed that an \"artificial magnetic conductor\" requires a periodic structure, e.g., two or more rows of conductive rods 124, in order to prevent an electromagnetic wave from entering.  In actuality, however,\npractically sufficient insulation can be achieved with one row of conductive rods 124, or one conductive rod 124 alone, as has been confirmed by the inventors.  Taking this fact into consideration, it is intended in the present specification that any\nstructure qualifies as an \"artificial magnetic conductor\" so long as it is capable of suppressing electromagnetic wave propagation, even if it consists of a single row of conductive rods 124 or a single conductive rod 124.\n In the present embodiment, the first joint portion 122B includes a portion which extends in the Y direction from the end of the stem 122A, and a portion which bends from that portion so as to extend in the X direction, and further bends to\nconnect to the first branch 122D.  The second joint portion 122C includes a portion which extends in the X direction from the branching portion, and bends so as to connect to the second branch 122E.  Thus, at least a portion of the first joint portion\n122B and at least a portion of the second joint portion 122C are parallel to each other.  Between these parallel portions, at least one conductive rod 124 is located.  As a result, interference of electromagnetic waves between the joint portions 122B and\n122C is suppressed.\n Without being limited to the structure shown in the figure, the joint portions 122B and 122C may take a variety of structures.  The length of the first joint portion 122B as measured along the waveguide is not equal to the length of the second\njoint portion 122C as measured along the waveguide.  These lengths may be appropriately designed in accordance with the required antenna characteristics.  For example, in an application where electromagnetic waves of a specific frequency within the\noperating frequency band (e.g., the center frequency) are to be radiated from the plurality of slots 112 in the same phase, the difference between a phase variation to occur when that electromagnetic wave propagates from the end (branching portion) of\nthe stem 122A to the closest slot 112 to the first joint portion 122B and a phase variation to occur when that electromagnetic wave propagates from the branching portion to the closest slot 112 to the second joint portion 122C is designed to be near\n.pi./2 (i.e., 180 degrees).  Under such a design, the electromagnetic wave has opposite phases at the positions of the two adjacent slots 112 that are the closest to the joint portions 122B and 122C.  As a result, electric fields which oscillate in the\nsame phase is created inside the two slots 112.  So long as the distance between the centers of any two adjacent slots is designed to be equal to the wavelength of the electromagnetic wave, electromagnetic waves of the same phase will be radiated from\nall slots 112.\n A design such that electric fields of the same phase will occur inside all slots 112 is not an essential requirement.  In an application where electromagnetic waves of intentionally differentiated phases are to be radiated from the plurality of\nslots 112, the lengths of the joint portions 122B and 122C may be designed under a different condition from the above.  In one example, the lengths of the joint portions 122B and 122C may be designed so that the difference between a phase variation to\noccur when an electromagnetic wave within the operating frequency band propagates from the end (branching portion) of the stem 122A to the closest slot 112 to the first joint portion 122B and a phase variation to occur when the electromagnetic wave\npropagates from the branching portion to the closest slot 112 to the second joint portion 122C is greater than .pi./4 and smaller than (3/4).pi..  In another example, given that the electromagnetic wave used has a central wavelength .lamda.o in free\nspace, the difference between the distance from the branching portion to the closest slot 112 to the first joint portion 122C as measured along the waveguide and the distance from the branching portion to the closest slot 112 to the second joint portion\n122D as measured along the waveguide may be set to a value which is greater than .lamda.o/4 and smaller than (3/4).lamda.o (e.g., about .lamda.o/2).  In still another example, given that an electromagnetic wave having a wavelength .lamda.o in free space\nhas a wavelength .lamda.g in the waveguide, the difference between the distance from the branching portion to the closest slot 112 to the first joint portion 122C as measured along the waveguide and the distance from the branching portion to the closest\nslot 112 to the second joint portion 122D as measured along the waveguide may be set to a value which is greater than .lamda.g/4 and smaller than (3/4).lamda.g (e.g., about .lamda.g/2).\n The wavelength .lamda.g of an electromagnetic wave in the waveguide is usually different from the wavelength .lamda.o in free space.  The wavelength .lamda.g can be altered by providing modulation structures such as recesses or protrusions on\nthe waveguide face 122a.  In the present embodiment, the waveguide face 122a of any linearly extending portion of the waveguide member 122 is flat, and has a constant width.  However, in order to adjust the wavelength of the electromagnetic wave in the\nwaveguide, the height or width of the waveguide face 122a may be locally varied.  In such constructions, the lengths of the joint portions 122B and 122C may be appropriately designed by taking into account the changing wavelength within the waveguide.\n When the wavelength or phase of the signal wave is adjusted by providing a protrusion or a recess on the waveguide face 122a of at least one of the first joint portion 122B and the second joint portion 122C, or by varying the width of the\nwaveguide face 122a along the waveguide, the lengths of the first joint portion 122B and the second joint portion 122C along the waveguide may be equal.  Even when the distances from the branching portion to the inner two slots 112 along the joint\nportions 122B and 122C are equal, a desired phase difference can be conferred by appropriately designing the positions and sizes of the protrusions, recesses, broad portions, or narrow portions on the waveguide face 122a.  For example, the difference\nbetween a phase variation to occur when an electromagnetic wave within the operating frequency band propagates from the branching portion to the closest slot 112 to the first joint portion 122B and a phase variation to occur when the electromagnetic wave\npropagates from the branching portion to the closest slot 112 to the second joint portion 122C can be designed to be greater than .pi./4 and smaller than (3/4).pi..\n In the present embodiment, the first joint portion 122B has two bends, whereas the second joint portion 122C has one bend; however, such a structure is not a limitation.  The number of bends in each joint portion 122B or 122C may be arbitrary. \nThe bend may conform to a curve.  Not only the joint portions 122B and 122C, but also the stem 122A and the branches 122D and 122E may also have bent portions.\n In the construction shown in FIG. 9, each bend in the joint portions 122B and 122C defines a recess.  In other words, the distance between the waveguide face 122a and the first conductive surface 110a is greater at the bends than in any other\nsite.  Providing the recess allows to enhance the degree of impedance matching at the bend, and suppress reflection of the signal wave.  In the case where reflection is not a significant issue, the bends do not need to be recessed.\n A choke structure 129 is provided at the leading end of each of the two branches 122D and 122E according to the present embodiment.  Each choke structure 129 is composed of: an additional transmission line having a length of approximately\n.lamda.g/4; and a row of plural grooves having a depth of about .lamda.o/4, or plural rods having a height of about .lamda.o/4, that are disposed at a leading end of that additional transmission line.  The choke structures 129 confer a phase difference\nof about 180.degree.  (.pi.) between an incident wave and a reflected wave, thereby restraining electromagnetic waves from leaking at the leading ends of the branches 122D and 122E.  Such choke structures 129 may be provided on the first conductive\nmember 110, rather than on the second conductive member 120.  Instead of choke structures, as shown in FIG. 10, a \"T\" structure may be provided at the leading end of each branch 122D, 122E.  The central portion of each such portion is connected to the\nleading end of the branch 122D, 122E.  With these \"T\" structures, too, electromagnetic wave leakage from the leading ends of the branches 122D and 122E can be suppressed.\n Thus, according to the present embodiment, the waveguide member 122 includes the stem 122A, the two joint portions 122B and 122C branching out from the end (branching portion) of the stem 122A, and two branches 122D and 122E that are\nrespectively connected to the two joint portions 122B and 122C.  The two branches 122D and 122E are disposed on a single straight line, and opposed to the plurality of slots 112 arranged along the Y direction.  By appropriately adjusting the lengths of\nthe two joint portions 122B and 122C, desired antenna characteristics can be obtained.\n The slot array antenna 200 includes one waveguide member 122 and one slot row (a group of plural slots 112 that are arranged along the Y direction).  Alternatively, a slot array antenna may be constructed from a plurality of such sets, each set\nincluding the waveguide member 122 and the slot row.  In other words, the slot array antenna may further include at least another waveguide member that is adjacent to the aforementioned waveguide member.  The other waveguide member(s) may have a similar\nstructure to that of the aforementioned waveguide member: the other waveguide member(s) is interposed between the first and second conductive members, has an electrically-conductive waveguide face that opposes the first or second conductive surface, and\nextends alongside the first or second conductive surface.  The artificial magnetic conductor extends also around the other waveguide member(s).  The waveguide face of the other waveguide member, the first or second conductive surface opposing the\nwaveguide face, and the artificial magnetic conductor define a waveguide.  The first conductive member has at least another slot row that is adjacent to the slot row.  These slot rows respectively couple to the plurality of waveguide members.\n By using the plurality of slot rows flanking one another along a direction (e.g., the X direction) which intersects the Y direction and the plurality of waveguide members 122 respectively coupling to the plurality of slot rows, an array antenna\nhaving radiating elements arranged in a two-dimensional array can be realized.  Hereinafter, specific examples of such array antennas will be described.\n FIG. 11A, FIG. 11B, and FIG. 12A show an exemplary construction of a slot array antenna 300 including a plurality of radiating elements arranged in a two-dimensional array.  FIG. 11A is a plan view showing the structure of the first conductive\nmember 110 in the slot array antenna 300.  FIG. 11B is a plan view showing the structure of the second conductive member 110 in the slot array antenna 300.  FIG. 12A is a cross-sectional view taken along line A-A in FIG. 11A.\n As shown in FIG. 11A, the first conductive member 110 in the slot array antenna 300 has six slot rows flanking one another along the X direction.  Each slot row includes four slots 112 that are arranged along the Y direction.  Among the six slot\nrows, the outer two are for transmission, and the inner four are for reception.  The four slot rows for reception (which will be referred to as \"reception antennas Rx\") are arranged at equal intervals along the X direction.  The two slot rows for\ntransmission (which will be referred to as \"transmission antennas Tx\") are disposed outside of the slot rows for reception, at intervals which are greater than the intervals between the slot rows for reception.\n The first conductive member 110 has horns 114 each serving to widen the opening of the respective slot 112.  Each horn 114 according to the present embodiment has a staircase structure such that the size of the opening along the Y direction\nincreases away from the center of the slot 112 in the +Z direction.  Without being limited to this example, the horn 114 may have any arbitrary structure.  By providing such a horn 114, the radiation characteristics can be improved.\n In addition to the region in which the plurality of slot rows are formed, the first conductive member 110 in the present embodiment also has a region in which a circuit board 290, including MMIC, is disposed.  In this region, six throughholes\n115 functioning as rectangular waveguides are provided.  Among the six throughholes 115, four are arranged along the X direction, and two are arranged along the Y direction.  The circuit board 290 is disposed so as to cover the six throughholes 115.  The\ncircuit board 290 includes six microstrip lines which are respectively coupled to the six throughholes 115.  The microstrip lines are connected to terminals of the MMIC.  The end of each microstrip line extends to a portion where it covers the\ncorresponding throughhole 115.\n FIG. 11B shows a layout of a plurality of waveguide members 122 and a plurality of conductive rods 124 (artificial magnetic conductor) on the second conductive member 120.  For reference sake, FIG. 11B also shows the positions of the plurality\nof slots 112, the six throughholes 115, and the circuit board 290 on the first conductive member 110.  As can be seen from FIG. 11B, on the second conductive member 120 in the present embodiment, two waveguide members 122 respectively coupling to the two\nslot rows for transmission, four waveguide members 122 respectively coupling to the four slot rows for reception, and a plurality of conductive rods 124 (artificial magnetic conductor) disposed around each waveguide member 122 are provided.  Each\nwaveguide member 122 has a similar structure to the structure which has been described with reference to FIG. 9.  That is, each waveguide member 122 includes a stem extending in one direction (which is the Y direction in the example of FIG. 11B), first\nand second joint portions branching out and extending from an end of the stem, and first and second branches respectively connected to ends of the first and second joint portions.  Each of the first and second joint portions extends from the end of the\nstem to two positions in a region which opposes the region between two adjacent slots among the plurality of slots in the corresponding one of the plurality of slot rows.  The first branch extends along first direction (i.e., the Y direction) from the\nend of the first joint portion, and couples to some of the plurality of slots 112 (i.e., the two upper slots in FIG. 11B).  The second branch extends along the second direction (i.e., the -Y direction) from the end of the second joint portion, and\ncouples to the other ones of the plurality of slots 112 (i.e., the two lower slots in FIG. 11B).  In each waveguide member 122, the length of the first joint portion as measured along the waveguide is not equal to the length of the second joint portion\nas measured along the waveguide.  Such a structure makes it easy to adjust the phase of a signal wave at the position of each slot, through adjustments of the lengths of the first and second joint portions.  Note that, in FIG. 11B, the stem of the\nwaveguide member 122 that is opposed to the leftmost slot row for transmission extends so as to surround the five waveguide members 122 coupling to the four slot rows for reception and the rightmost slot row for transmission.  Thus, the stem of the\nwaveguide member 122 may be structured so that change in the direction that it extends is made at the plurality of bends.\n The stem of each of the six waveguide members 122 is coupled to the MMIC via a throughhole 115.  The MMIC feeds signal waves to the outer two waveguide members 122.  As a result, signal waves are emitted from the outer two slot rows\n(transmission antennas Tx).  The MMIC also receives signal waves from the inner four waveguide members 122.  As a result, the MMIC is able to receive electromagnetic waves which impinge on the inner four slot rows (reception antennas Rx).\n FIG. 12A shows the structure of a YZ cross section which passes through the center of one slot row and one throughhole 115.  The conductive surface on the rear face side of the first conductive member 110 is opposed to the plurality of waveguide\nmembers 122 and the plurality of conductive rods 124 on the second conductive member 120, via a gap.  A microstrip line 290a which is provided on the circuit board 290 extends from a signal output terminal or a signal input terminal of the MMIC to a\nposition where it covers the throughhole 115, and is coupled to the waveguide (rectangular waveguide) inside the throughhole 115.  The stem of the waveguide member 122 is located under the throughhole 115, such that the waveguide inside the throughhole\n115 and the waveguide upon the waveguide member 122 are coupled.  Such a structure allows a signal wave to be transmitted between the MMIC, the throughhole 115, and the waveguide member 122.\n The first conductive member 110 shown in FIG. 11A may be called a \"radiation layer\".  Moreover, the entirety of the second conductive member 120, the plurality of waveguide members 122, and the plurality of conductive rods 124 shown in FIG. 11B\nmay be called an \"excitation layer\" or a \"feeding layer\".  Each of the \"radiation layer\" and the \"feeding layer\" may be mass-produced by processing a single metal plate.  The radiation layer and the feeding layer may be made from an aluminum alloy, for\nexample.  The radiation layer, the feeding layer, and the circuit board 290 may be fabricated as a single-module product.\n In the array antenna of this example, as can be seen from FIG. 12A, the plate-like radiation layer and feeding layer are stacked in such a manner that, as a whole, a flat panel antenna which is flat and low-profile is realized.  For example, a\nmultilayer structure having the cross-sectional construction shown in FIG. 12A may have a height (thickness) of 10 mm or less.\n In the present embodiment, the plurality of waveguide members 122 and the plurality of conductive rods 124 are connected to the second conductive member 120.  The plurality of waveguide members 122 and the plurality of conductive rods 124 may be\nconnected to the first conductive member 110.  Such a construction also allows a waveguide to be formed.\n FIG. 12B is a cross-sectional view showing an example where a plurality of ridge-shaped waveguide members 122 and a plurality of conductive rods (artificial magnetic conductor) are disposed on the conductive surface 110a of the first conductive\nmember 110.  In this example, each waveguide member 122 is opposed to the conductive surface 120a of the second conductive member 120, and extends alongside the conductive surface 120a.  A plurality of waveguides are defined by the second conductive\nsurface 120a, the waveguide faces 122a of the plurality of waveguide members 122, and the artificial magnetic conductor.\n In the example of FIG. 12B, each waveguide member 122 is split into a plurality of portions at the position where it is connected to the slot 112.  Its opposing end faces existing at the plurality of split portions are connected to the inner\nwall surface of the slot 112.  The portion where the end faces and the inner wall surface of the slot 112 are connected may be stepped.  Although this example illustrates the artificial magnetic conductor to be on the first conductive member 110 side, it\nmay instead be on the second conductive member 120 side.  Stated otherwise, each conductive rod in the artificial magnetic conductor may have a root that is connected to one of the first conductive surface 110a and the second conductive surface 120a and\na leading end that is opposed to the other of the first conductive surface 110a and the second conductive surface 120a.  Even if the waveguide member 122 is split by the slot 112, the conductive surface 120a is still continuous; therefore, a signal wave\ncan propagate beyond the slot 112 unless the width of the slot 112 is too large.\n FIG. 13 is a diagram showing an example of dimensions of one waveguide member 122 and intervals between the four slots 112 that are included in one slot row according to the present embodiment.  In the present embodiment, the slot interval is\n1.15.lamda.o, and the difference W in length between the two joint portions 122B and 122C of the waveguide member 122 is .lamda.o/2.  However, such dimensions are not a limitation.  Generally speaking, W is set to a value which is close to a half of\nwavelength .lamda.g in the waveguide.  However, in order to satisfy the required phase condition, it may intentionally be set to a value which is different from .lamda.g/2 or .lamda.o/2.  Similarly, the slot interval may also be adjusted as appropriate,\nso as to satisfy the required phase condition.\n Next, with reference to FIGS. 14A through 14E, a more specific exemplary structure of the slot array antenna 300 will be described.  FIG. 14A and FIG. 14B are upper plan views respectively showing exemplary structures of the first conductive\nmember 110 (radiation layer) and the second conductive member 120 (feeding layer).  FIG. 14C is a diagram showing an exemplary structure of a cover 140 which protects the circuit board 290.  FIG. 14D is a diagram showing an appearance of a slot array\nantenna 300 which is produced by combining the first conductive member 110, the second conductive member 120, and the cover 140.  FIG. 14E is a diagram showing an appearance of the slot array antenna 300 of FIG. 14D in lateral view, as well as its\nthickness.  On the rear face side of the cover 140 shown in FIG. 14C, a circuit board 290 including an MMIC is provided.  Exemplary dimensions of the cover 140 as viewed from the front are 50 mm.times.25 mm.  As shown in FIG. 14D and FIG. 14E, the\ndimensions of the slot array antenna 300 are 53.2 mm.times.52.1 mm.times.8.5 mm, indicative of being a very small-sized array antenna.  Note that these dimensions are only an example.\n The slot array antenna 300 according to the present embodiment includes a plurality of slot rows for transmission (transmission antennas Tx) and a plurality of slot rows for reception (reception antennas Rx).  The MMIC may be adapted to be able\nto distinguish between signals from the plurality of transmission antennas Tx.  For example, by using TDM (Time Division Multiplexing), PDM (Phase Division Multiplexing), FDM (Frequency Division Multiplexing), or other techniques, the MMIC may be adapted\nto be able to distinguish between a plurality of transmission waves.  A method which switches between TDM and PDM depending on the situation, etc., may also be used.  Radio waves which are radiated from a plurality of transmission antennas Tx in\ndifferent positions can be distinguishably received by an array of plural reception antennas Rx.  This essentially provides a similar effect as if the number of reception antennas Rx were multiplied by the number of transmission antennas Tx.  Such a\nconstruction is called MIMO (Multiple-Input Multiple Output), which provides improvements in communications or radar quality.\n Although the present embodiment illustrates that all of the plurality of waveguide members 122 on the second conductive member 120 have the structure which has been described with reference to FIG. 9, it may be at least one waveguide member 122\nthat possesses this structure.  The slot array antenna may only include either the transmission antennas or the reception antennas.\n &lt;Other Variants&gt;\n Next, variants of waveguide structures including the waveguide member 122, the conductive members 110 and 120, and the plurality of conductive rods 124 will be described.  The following variants are applicable to the WRG structure in any place\nin each embodiment of the present disclosure.\n FIG. 15A is a cross-sectional view showing an exemplary structure in which only the waveguide face 122a, defining an upper face of the waveguide member 122, is electrically conductive, while any portion of the waveguide member 122 other than the\nwaveguide face 122a is not electrically conductive.  Both of the conductive member 110 and the conductive member 120 alike are only electrically conductive at their surface that has the waveguide member 122 provided thereon (i.e., the conductive surface\n110a, 120a), while not being electrically conductive in any other portions.  Thus, each of the waveguide member 122, the conductive member 110, and the conductive member 120 does not need to be electrically conductive.\n FIG. 15B is a diagram showing a variant in which the waveguide member 122 is not formed on the conductive member 120.  In this example, the waveguide member 122 is fixed to a supporting member (e.g., the inner wall of the housing) that supports\nthe conductive member 110 and the conductive member.  A gap exists between the waveguide member 122 and the conductive member 120.  Thus, the waveguide member 122 does not need to be connected to the conductive member 120.\n FIG. 15C is a diagram showing an exemplary structure where the conductive member 120, the waveguide member 122, and each of the plurality of conductive rods 124 are composed of a dielectric surface that is coated with an electrically conductive\nmaterial such as a metal.  The conductive member 120, the waveguide member 122, and the plurality of conductive rods 124 are connected to one another via the electrical conductor.  On the other hand, the conductive member 110 is made of an electrically\nconductive material such as a metal.\n FIG. 15D and FIG. 15E are diagrams each showing an exemplary structure in which dielectric layers 110b and 120b are respectively provided on the outermost surfaces of conductive members 110 and 120, a waveguide member 122, and conductive rods\n124.  FIG. 15D shows an exemplary structure in which the surface of metal conductive members, which are electrical conductors, are covered with a dielectric layer.  FIG. 15E shows an example where the conductive member 120 is structured so that the\nsurface of members which are composed of a dielectric, e.g., resin, is covered with an electrical conductor such as a metal, this metal layer being further coated with a dielectric layer.  The dielectric layer that covers the metal surface may be a\ncoating of resin or the like, or an oxide film of passivation coating or the like which is generated as the metal becomes oxidized.\n The dielectric layer on the outermost surface will allow losses to be increased in the electromagnetic wave propagating through the WRG waveguide, but is able to protect the conductive surfaces 110a and 120a (which are electrically conductive)\nfrom corrosion.  It also prevents influences of a DC voltage, or an AC voltage of such a low frequency that it is not capable of propagation on certain WRG waveguides.\n FIG. 15F is a diagram showing an example where the height of the waveguide member 122 is lower than the height of the conductive rods 124, and the portion of the conductive surface 110a of the conductive member 110 that opposes the waveguide\nface 122a protrudes toward the waveguide member 122.  Even such a structure will operate in a similar manner to the above-described embodiment, so long as the ranges of dimensions depicted in FIG. 4 are satisfied.\n FIG. 15G is a diagram showing an example where, further in the structure of FIG. 15F, portions of the conductive surface 110a that oppose the conductive rods 124 protrude toward the conductive rods 124.  Even such a structure will operate in a\nsimilar manner to the above-described embodiment, so long as the ranges of dimensions depicted in FIG. 4 are satisfied.  Instead of a structure in which the conductive surface 110a partially protrudes, a structure in which the conductive surface 110a is\npartially dented may be adopted.\n FIG. 16A is a diagram showing an example where a conductive surface 110a of the conductive member 110 is shaped as a curved surface.  FIG. 16B is a diagram showing an example where also a conductive surface 120a of the conductive member 120 is\nshaped as a curved surface.  As demonstrated by these examples, the conductive surfaces 110a and 120a may not be shaped as planes, but may be shaped as curved surfaces.  A conductive member having a conductive surface which is a curved surface is also\nqualifies as a conductive member having a \"plate shape\".\n Next, variants for the shape of each slot 112 will be described.  Although the above examples have illustrated the planar shape of each slot 112 to be mainly an H shape, the slots 112 may also have other shapes.  Hereinafter, with reference to\nFIG. 17, example shapes of slots will be described.\n In FIG. 17, (a) shows an exemplary slot 112a having the shape of an ellipse.  The semimajor axis La of the slot 112a, indicated by arrowheads in the figure, is chosen so that higher-order resonance will not occur and that the impedance will not\nbe too small.  More specifically, La may be set so that .lamda.o/4&lt;La&lt;.lamda.o/2, where .lamda.o is a wavelength in free space corresponding to the center frequency in the operating frequency band.\n In FIG. 17, (b) shows an exemplary slot 112b having an H shape which includes a pair of vertical portions 117L and a lateral portion 117T interconnecting the pair of vertical portions 117L.  The lateral portion 117T is substantially\nperpendicular to the pair of vertical portions 117L, and connects between substantial central portions of the pair of vertical portions 117L.  The shape and size of such an H-shaped slot 112b are also to be determined so that higher-order resonance will\nnot occur and that the impedance will not be too small.  The distance between a point of intersection between the center line g2 of the lateral portion 117T and the center line h2 of the entire H shape perpendicular to the lateral portion 117T and a\npoint of intersection between the center line g2 and the center line k2 of a vertical portion 117L is denoted as Lb.  The distance between a point of intersection between the center line g2 and the center line k2 and the end of the vertical portion 117L\nis denoted as Wb.  The sum of Lb and Wb is chosen so as to satisfy .lamda.o/4&lt;Lb+Wb&lt;.lamda.o/2.  Choosing the distance Wb to be relatively long allows the distance Lb to be relatively short.  As a result, the width of the H shape along the X\ndirection can be e.g. less than .lamda.o/2, whereby the interval between the lateral portions 117T along the length direction can be made short.\n In FIG. 17, (c) shows an exemplary slot 112c which includes a lateral portion 117T and a pair of vertical portions 117L extending from both ends of the lateral portion 117T.  The directions in which the pair of vertical portions 117L extend from\nthe lateral portion 117T are substantially perpendicular to the lateral portion 117T, and are opposite to each other.  The distance between a point of intersection between the center line g3 of the lateral portion 117T and the center line h3 of the\noverall shape which is perpendicular to the lateral portion 117T and a point of intersection between the center line g3 and the center line k3 of a vertical portion 117L is denoted as Lc.  The distance between a point of intersection between the center\nline g3 and the center line k3 and the end of the vertical portion 117L is denoted as Wc.  The sum of Lc and Wc is chosen so as to satisfy .lamda.o/4&lt;Lc+Wc&lt;.lamda.o/2.  Choosing the distance Wc to be relatively long allows the distance Lc to be\nrelatively short.  As a result, the width along the X direction of the overall shape in (c) of FIG. 17 can be e.g. less than .lamda.o/2, whereby the interval between the lateral portions 117T along the length direction can be made short.\n In FIG. 17, (d) shows an exemplary slot 112d which includes a lateral portion 117T and a pair of vertical portions 117L extending from both ends of the lateral portion 117T in an identical direction which is perpendicular to the lateral portion\n117T.  Such a shape may be referred to as a \"U shape\" in the present specification.  Note that the shape shown in (d) of FIG. 17 may be regarded as an upper half shape of an H shape.  The distance between a point of intersection between the center line\ng4 of the lateral portion 117T and the center line h4 of the overall U shape which is perpendicular to the lateral portion 117T and a point of intersection between the center line g4 and the center line k4 of a vertical portion 117L is denoted as Ld. \nThe distance between a point of intersection between the center line g4 and the center line k4 and the end of the vertical portion 117L is denoted as Wd.  The sum of Ld and Wd is chosen so as to satisfy .lamda.o/4&lt;Ld+Wd&lt;.lamda.o/2.  Choosing the\ndistance Wd to be relatively long allows the distance Ld to be relatively short.  As a result, the width along the X direction of the U shape can be e.g. less than .lamda.o/2, whereby the interval between the lateral portions 117T along the length\ndirection can be made short.\n FIG. 18 is a diagram showing a planar layout where the three kinds of slots 112b through 112d shown in FIG. 17 are disposed on a waveguide member 122.  As shown in the figure, using the slots 112b through 112d allows the size of the lateral\nportion 113T along its length direction (referred to as the \"lateral direction\") to be reduced as compared to the case of using the slot 112a.  Therefore, in a structure where a plurality of waveguide members 122 are adjacent to one another, the interval\nof slots along the lateral direction can be reduced.\n The above example illustrates that the longitudinal direction of each slot or the direction that its lateral portion extends coincides with the width direction of the waveguide member 122; however, these two directions may intersect each other. \nIn such constructions, the plane of polarization of the electromagnetic wave to be radiated can be tilted.  As a result, when used for an onboard radar, for example, an electromagnetic wave which has been radiated from the driver's vehicle can be\ndistinguished from an electromagnetic wave which has been radiated from an oncoming car.\n A slot array antenna according to an embodiment of the present disclosure can be suitably used in a radar device (hereinafter simply referred to as a \"radar\") or a radar system to be incorporated in moving entities such as vehicles, marine\nvessels, aircraft, robots, or the like, for example.  A radar device would include a slot array antenna according to an embodiment of the present disclosure and a microwave integrated circuit that is connected to the slot array antenna.  A radar system\nwould include the radar device and a signal processing circuit that is connected to the microwave integrated circuit of the radar device.  An antenna array according to an embodiment of the present disclosure includes a multilayered WRG structure which\npermits downsizing, and thus allows the area of the face on which antenna elements are arrayed to be significantly reduced, as compared to a construction in which a conventional hollow waveguide is used.  Therefore, a radar system incorporating the\nantenna device can be easily mounted in a narrow place such as a face of a rearview mirror in a vehicle that is opposite to its specular surface, or a small-sized moving entity such as a UAV (an Unmanned Aerial Vehicle, a so-called drone).  Note that,\nwithout being limited to the implementation where it is mounted in a vehicle, a radar system may be used while being fixed on the road or a building, for example.\n A slot array antenna according to an embodiment of the present disclosure can also be used in a wireless communication system.  Such a wireless communication system would include a slot array antenna according to any of the above embodiments and\na communication circuit (a transmission circuit or a reception circuit).  Details of exemplary applications to wireless communication systems will be described later.\n A slot array antenna according to an embodiment of the present disclosure can further be used as an antenna in an indoor positioning system (IPS).  An indoor positioning system is able to identify the position of a moving entity, such as a\nperson or an automated guided vehicle (AGV), that is in a building.  An array antenna can also be used as a radio wave transmitter (beacon) for use in a system which provides information to an information terminal device (e.g., a smartphone) that is\ncarried by a person who has visited a store or any other facility.  In such a system, once every several seconds, a beacon may radiate an electromagnetic wave carrying an ID or other information superposed thereon, for example.  When the information\nterminal device receives this electromagnetic wave, the information terminal device transmits the received information to a remote server computer via telecommunication lines.  Based on the information that has been received from the information terminal\ndevice, the server computer identifies the position of that information terminal device, and provides information which is associated with that position (e.g., product information or a coupon) to the information terminal device.\n The present specification employs the term \"artificial magnetic conductor\" in describing the technique according to the present disclosure, this being in line with what is set forth in a paper by one of the inventors Kirino (Non-Patent Document\n1) as well as a paper by Kildal et al., who published a study directed to related subject matter around the same time.  However, it has been found through a study by the inventors that the invention according to the present disclosure does not\nnecessarily require an \"artificial magnetic conductor\" under its conventional definition.  That is, while a periodic structure has been believed to be a requirement for an artificial magnetic conductor, the invention according to the present disclosure\ndoes not necessary require a periodic structure in order to be practiced.\n The artificial magnetic conductor that is described in the present disclosure consists of rows of conductive rods.  In order to prevent electromagnetic waves from leaking away from the waveguide face, it has been believed essential that there\nexist at least two rows of conductive rods on one side of the waveguide member(s), such rows of conductive rods extending along the waveguide member(s) (ridge(s)).  The reason is that it takes at least two rows of conductive rods for them to have a\n\"period\".  However, according to a study by the inventors, even when only one row of conductive rods, or only one conductive rod, exists between two waveguide members that extend in parallel to each other, the intensity of a signal that leaks from one\nwaveguide member to the other waveguide member can be suppressed to -10 dB or less, which is a practically sufficient value in many applications.  The reason why such a sufficient level of separation is achieved with only an imperfect periodic structure\nis so far unclear.  However, in view of this fact, in the present disclosure, the conventional notion of \"artificial magnetic conductor\" is extended so that the term also encompasses a structure including only one row of conductive rods, or only one\nconductive rod.\nApplication Example 1: Onboard Radar System\n Next, as an Application Example of utilizing the above-described slot array antenna, an instance of an onboard radar system including a slot array antenna will be described.  A transmission wave used in an onboard radar system may have a\nfrequency of e.g. 76 gigahertz (GHz) band, which will have a wavelength .lamda.o of about 4 mm in free space.\n In safety technology of automobiles, e.g., collision avoidance systems or automated driving, it is particularly essential to identify one or more vehicles (targets) that are traveling ahead of the driver's vehicle.  As a method of identifying\nvehicles, techniques of estimating the directions of arriving waves by using a radar system have been under development.\n FIG. 19 shows a driver's vehicle 500, and a preceding vehicle 502 that is traveling in the same lane as the driver's vehicle 500.  The driver's vehicle 500 includes an onboard radar system which incorporates a slot array antenna according to any\nof the above-described embodiments.  When the onboard radar system of the driver's vehicle 500 radiates a radio frequency transmission signal, the transmission signal reaches the preceding vehicle 502 and is reflected therefrom, so that a part of the\nsignal returns to the driver's vehicle 500.  The onboard radar system receives this signal to calculate a position of the preceding vehicle 502, a distance (\"range\") to the preceding vehicle 502, velocity, etc.\n FIG. 20 shows the onboard radar system 510 of the driver's vehicle 500.  The onboard radar system 510 is provided within the vehicle.  More specifically, the onboard radar system 510 is disposed on a face of the rearview mirror that is opposite\nto its specular surface.  From within the vehicle, the onboard radar system 510 radiates a radio frequency transmission signal in the direction of travel of the vehicle 500, and receives a signal(s) which arrives from the direction of travel.\n The onboard radar system 510 of this Application Example includes a slot array antenna according to any of the above embodiments.  This Application Example is arranged so that the plurality of slots in each slot row are arranged in a direction\nwhich coincides with the vertical direction, and that the plurality of slot rows are arranged in a direction which coincides with the horizontal direction.  As a result, the lateral dimension of the plurality of slots as viewed from the front can be\nreduced.\n As described above, with the constructions of the above-described embodiments, the intervals between a plurality of waveguide members (ridges) that are used in a transmission antenna can be made narrow.  Furthermore, the intervals between a\nplurality of slots on a conductive member can be made narrow.  This allows the overall dimensions of the onboard radar system 510 to be greatly reduced.  Exemplary dimensions of an antenna device including the above slot array antenna may be 60 mm\n(wide).times.30 mm (long).times.10 mm (deep).  It will be appreciated that this is a very small size for a millimeter wave radar system of the 76 GHz band.\n Note that many a conventional onboard radar system is provided outside the vehicle, e.g., at the tip of the front nose.  The reason is that the onboard radar system is relatively large in size, and thus is difficult to be provided within the\nvehicle as in the present disclosure.  The onboard radar system 510 of this Application Example may be installed within the vehicle as described above, but may instead be mounted at the tip of the front nose.  Since the footprint of the onboard radar\nsystem on the front nose is reduced, other parts can be more easily placed.\n The Application Example allows the interval between a plurality of waveguide members (ridges) that are used in the transmission antenna to be narrow, which also narrows the interval between a plurality of slots to be provided opposite from a\nnumber of adjacent waveguide members.  This reduces the influences of grating lobes.  For example, when the interval between the centers of two laterally adjacent slots is shorter than the free-space wavelength .lamda.o of the transmission wave (i.e.,\nless than about 4 mm), no grating lobes will occur frontward.  As a result, influences of grating lobes are reduced.  Note that grating lobes will occur when the interval at which the antenna elements are arrayed is greater than a half of the wavelength\nof an electromagnetic wave.  If the interval at which the antenna elements are arrayed is less than the wavelength, no grating lobes will occur frontward.  Therefore, in the case where no beam steering is performed to impart phase differences among the\nradio waves radiated from the respective antenna elements composing an array antenna, grating lobes will exert substantially no influences so long as the interval at which the antenna elements are arrayed is smaller than the wavelength.  By adjusting the\narray factor of the transmission antenna, the directivity of the transmission antenna can be adjusted.  A phase shifter may be provided so as to be able to individually adjust the phases of electromagnetic waves that are transmitted on plural waveguide\nmembers.  In this case, in order to avoid the influences of grating lobes, it is more preferable that the interval between antenna elements is less than the free-space wavelength .lamda.o of the transmission wave.  In that case, too, grating lobes will\nappear as the phase shift amount is increased.  However, when the intervals between the antenna elements is reduced to less than a half of the free space wavelength .lamda.o of the transmission wave, grating lobes will not appear irrespective of the\nphase shift amount.  By providing a phase shifter, the directivity of the transmission antenna can be changed in any desired direction.  Since the construction of a phase shifter is well-known, description thereof will be omitted.\n A reception antenna according to the Application Example is able to reduce reception of reflected waves associated with grating lobes, thereby being able to improve the precision of the below-described processing.  Hereinafter, an example of a\nreception process will be described.\n FIG. 21A shows a relationship between an array antenna AA of the onboard radar system 510 and plural arriving waves k (k: an integer from 1 to K; the same will always apply below.  K is the number of targets that are present in different\nazimuths).  The array antenna AA includes M antenna elements in a linear array.  Principlewise, an antenna can be used for both transmission and reception, and therefore the array antenna AA can be used for both a transmission antenna and a reception\nantenna.  Hereinafter, an example method of processing an arriving wave which is received by the reception antenna will be described.\n The array antenna AA receives plural arriving waves that simultaneously impinge at various angles.  Some of the plural arriving waves may be arriving waves which have been radiated from the transmission antenna of the same onboard radar system\n510 and reflected by a target(s).  Furthermore, some of the plural arriving waves may be direct or indirect arriving waves that have been radiated from other vehicles.\n The incident angle of each arriving wave (i.e., an angle representing its direction of arrival) is an angle with respect to the broadside B of the array antenna AA.  The incident angle of an arriving wave represents an angle with respect to a\ndirection which is perpendicular to the direction of the line along which antenna elements are arrayed.\n Now, consider a k.sup.th arriving wave.  Where K arriving waves are impinging on the array antenna from K targets existing at different azimuths, a \"k.sup.th arriving wave\" means an arriving wave which is identified by an incident angle\n.theta..sub.k.\n FIG. 21B shows the array antenna AA receiving the k.sup.th arriving wave.  The signals received by the array antenna AA can be expressed as a \"vector\" having M elements, by Math. 1.  S=[s1,s.sub.2, .  . . ,s.sub.M].sup.T (Math. 1)\n In the above, s.sub.m (where m is an integer from 1 to M; the same will also be true hereinbelow) is the value of a signal which is received by an m.sup.th antenna element.  The superscript .sup.T means transposition.  S is a column vector.  The\ncolumn vector S is defined by a product of multiplication between a direction vector (referred to as a steering vector or a mode vector) as determined by the construction of the array antenna and a complex vector representing a signal from each target\n(also referred to as a wave source or a signal source).  When the number of wave sources is K, the waves of signals arriving at each individual antenna element from the respective K wave sources are linearly superposed.  In this state, s.sub.m can be\nexpressed by Math. 2.\n .times..times..times..function..times..pi..lamda..times..times..times..ti- mes..theta..phi..times.  ##EQU00001##\n In Math. 2, a.sub.k, .theta..sub.k and .PHI..sub.k respectively denote the amplitude, incident angle, and initial phase of the k.sup.th arriving wave.  Moreover, .lamda.  denotes the wavelength of an arriving wave, and j is an imaginary unit.\n As will be understood from Math. 2, s.sub.m is expressed as a complex number consisting of a real part (Re) and an imaginary part (Im).\n When this is further generalized by taking noise (internal noise or thermal noise) into consideration, the array reception signal X can be expressed as Math. 3.  X=S+N (Math. 3) N is a vector expression of noise.\n The signal processing circuit generates a spatial covariance matrix Rxx (Math. 4) of arriving waves by using the array reception signal X expressed by Math. 3, and further determines eigenvalues of the spatial covariance matrix Rxx.\n .times..times..times.  .times..times..times.  ##EQU00002##\n In the above, the superscript .sup.H means complex conjugate transposition (Hermitian conjugate).\n Among the eigenvalues, the number of eigenvalues which have values equal to or greater than a predetermined value that is defined based on thermal noise (signal space eigenvalues) corresponds to the number of arriving waves.  Then, angles that\nproduce the highest likelihood as to the directions of arrival of reflected waves (i.e. maximum likelihood) are calculated, whereby the number of targets and the angles at which the respective targets are present can be identified.  This process is known\nas a maximum likelihood estimation technique.\n Next, see FIG. 22.  FIG. 22 is a block diagram showing an exemplary fundamental construction of a vehicle travel controlling apparatus 600 according to the present disclosure.  The vehicle travel controlling apparatus 600 shown in FIG. 22\nincludes a radar system 510 which is mounted in a vehicle, and a travel assistance electronic control apparatus 520 which is connected to the radar system 510.  The radar system 510 includes an array antenna AA and a radar signal processing apparatus\n530.\n The array antenna AA includes a plurality of antenna elements, each of which outputs a reception signal in response to one or plural arriving waves.  As mentioned earlier, the array antenna AA is capable of radiating a millimeter wave of a high\nfrequency.  Note that, without being limited to the slot array antennas according to the above embodiments, the array antenna AA may be any other array antenna that suitably performs reception.\n In the radar system 510, the array antenna AA needs to be attached to the vehicle, while at least some of the functions of the radar signal processing apparatus 530 may be implemented by a computer 550 and a database 552 which are provided\nexternally to the vehicle travel controlling apparatus 600 (e.g., outside of the driver's vehicle).  In that case, the portions of the radar signal processing apparatus 530 that are located within the vehicle may be perpetually or occasionally connected\nto the computer 550 and database 552 external to the vehicle so that bidirectional communications of signal or data are possible.  The communications are to be performed via a communication device 540 of the vehicle and a commonly-available\ncommunications network.\n The database 552 may store a program which defines various signal processing algorithms.  The content of the data and program needed for the operation of the radar system 510 may be externally updated via the communication device 540.  Thus, at\nleast some of the functions of the radar system 510 can be realized externally to the driver's vehicle (which is inclusive of the interior of another vehicle), by a cloud computing technique.  Therefore, an \"onboard\" radar system in the meaning of the\npresent disclosure does not require that all of its constituent elements be mounted within the (driver's) vehicle.  However, for simplicity, the present application will describe an implementation in which all constituent elements according to the\npresent disclosure are mounted in a single vehicle (i.e., the driver's vehicle), unless otherwise specified.\n The radar signal processing apparatus 530 includes a signal processing circuit 560.  The signal processing circuit 560 directly or indirectly receives reception signals from the array antenna AA, and inputs the reception signals, or a secondary\nsignal(s) which has been generated from the reception signals, to an arriving wave estimation unit AU.  A part or a whole of the circuit (not shown) which generates a secondary signal(s) from the reception signals does not need to be provided inside of\nthe signal processing circuit 560.  A part or a whole of such a circuit (preprocessing circuit) may be provided between the array antenna AA and the radar signal processing apparatus 530.\n The signal processing circuit 560 is configured to perform computation by using the reception signals or secondary signal(s), and output a signal indicating the number of arriving waves.  As used herein, a \"signal indicating the number of\narriving waves\" can be said to be a signal indicating the number of preceding vehicles (which may be one preceding vehicle or plural preceding vehicles) ahead of the driver's vehicle.\n The signal processing circuit 560 may be configured to execute various signal processing which is executable by known radar signal processing apparatuses.  For example, the signal processing circuit 560 may be configured to execute\n\"super-resolution algorithms\" such as the MUSIC method, the ESPRIT method, or the SAGE method, or other algorithms for direction-of-arrival estimation of relatively low resolution.\n The arriving wave estimation unit AU shown in FIG. estimates an angle representing the azimuth of each arriving wave by an arbitrary algorithm for direction-of-arrival estimation, and outputs a signal indicating the estimation result.  The\nsignal processing circuit 560 estimates the distance to each target as a wave source of an arriving wave, the relative velocity of the target, and the azimuth of the target by using a known algorithm which is executed by the arriving wave estimation unit\nAU, and output a signal indicating the estimation result.\n In the present disclosure, the term \"signal processing circuit\" is not limited to a single circuit, but encompasses any implementation in which a combination of plural circuits is conceptually regarded as a single functional part.  The signal\nprocessing circuit 560 may be realized by one or more System-on-Chips (SoCs).  For example, a part or a whole of the signal processing circuit 560 may be an FPGA (Field-Programmable Gate Array), which is a programmable logic device (PLD).  In that case,\nthe signal processing circuit 560 includes a plurality of computation elements (e.g., general-purpose logics and multipliers) and a plurality of memory elements (e.g., look-up tables or memory blocks).  Alternatively, the signal processing circuit 560\nmay be a set of a general-purpose processor(s) and a main memory device(s).  The signal processing circuit 560 may be a circuit which includes a processor core(s) and a memory device(s).  These may function as the signal processing circuit 560.\n The travel assistance electronic control apparatus 520 is configured to provide travel assistance for the vehicle based on various signals which are output from the radar signal processing apparatus 530.  The travel assistance electronic control\napparatus 520 instructs various electronic control units to fulfill predetermined functions, e.g., a function of issuing an alarm to prompt the driver to make a braking operation when the distance to a preceding vehicle (vehicular gap) has become shorter\nthan a predefined value; a function of controlling the brakes; and a function of controlling the accelerator.  For example, in the case of an operation mode which performs adaptive cruise control of the driver's vehicle, the travel assistance electronic\ncontrol apparatus 520 sends predetermined signals to various electronic control units (not shown) and actuators, to maintain the distance of the driver's vehicle to a preceding vehicle at a predefined value, or maintain the traveling velocity of the\ndriver's vehicle at a predefined value.\n In the case of the MUSIC method, the signal processing circuit 560 determines eigenvalues of the spatial covariance matrix, and, as a signal indicating the number of arriving waves, outputs a signal indicating the number of those eigenvalues\n(\"signal space eigenvalues\") which are greater than a predetermined value (thermal noise power) that is defined based on thermal noise.\n Next, see FIG. 23.  FIG. 23 is a block diagram showing another exemplary construction for the vehicle travel controlling apparatus 600.  The radar system 510 in the vehicle travel controlling apparatus 600 of FIG. 23 includes an array antenna\nAA, which includes an array antenna that is dedicated to reception only (also referred to as a reception antenna) Rx and an array antenna that is dedicated to transmission only (also referred to as a transmission antenna) Tx; and an object detection\napparatus 570.\n At least one of the transmission antenna Tx and the reception antenna Rx has the aforementioned waveguide structure.  The transmission antenna Tx radiates a transmission wave, which may be a millimeter wave, for example.  The transmission\nantenna Tx may be a slot array antenna according to any of the above-described embodiments, for example.  The transmission antenna Tx outputs a transmission signal whose directivity gain is strongest in the substantial front direction.  The transmission\nantenna Tx is to be used as a high-gain antenna for long ranges.  The reception antenna Rx that is dedicated to reception only outputs a reception signal in response to one or plural arriving waves (e.g., a millimeter wave(s)).\n A transmission/reception circuit 580 sends a transmission signal for a transmission wave to the transmission antenna Tx, and performs \"preprocessing\" for reception signals of reception waves received at the reception antenna Rx.  A part or a\nwhole of the preprocessing may be performed by the signal processing circuit 560 in the radar signal processing apparatus 530.  A typical example of preprocessing to be performed by the transmission/reception circuit 580 may be generating a beat signal\nfrom a reception signal, and converting a reception signal of analog format into a reception signal of digital format.\n In the present specification, any device that includes a transmission antenna, a reception antenna, a transmission/reception circuit, and a waveguide device which allows electromagnetic waves to propagate between the transmission antenna and\nreception antenna and the transmission/reception circuit is referred to as a \"radar device\".  Moreover, a system that includes a signal processing apparatus (including a signal processing circuit), e.g., an object detection apparatus, in addition to a\nradar device is referred to as a \"radar system\".\n Note that the radar system according to the present disclosure may, without being limited to the implementation where it is mounted in the driver's vehicle, be used while being fixed on the road or a building.\n Next, an example of a more specific construction of the vehicle travel controlling apparatus 600 will be described.\n FIG. 24 is a block diagram showing an example of a more specific construction of the vehicle travel controlling apparatus 600.  The vehicle travel controlling apparatus 600 shown in FIG. 24 includes a radar system 510 and an onboard camera\nsystem 700.  The radar system 510 includes an array antenna AA, a transmission/reception circuit 580 which is connected to the array antenna AA, and a signal processing circuit 560.\n The onboard camera system 700 includes an onboard camera 710 which is mounted in a vehicle, and an image processing circuit 720 which processes an image or video that is acquired by the onboard camera 710.\n The vehicle travel controlling apparatus 600 of this Application Example includes an object detection apparatus 570 which is connected to the array antenna AA and the onboard camera 710, and a travel assistance electronic control apparatus 520\nwhich is connected to the object detection apparatus 570.  The object detection apparatus 570 includes a transmission/reception circuit 580 and an image processing circuit 720, in addition to the above-described radar signal processing apparatus 530\n(including the signal processing circuit 560).  The object detection apparatus 570 detects a target on the road or near the road, by using not only the information which is obtained by the radar system 510 but also the information which is obtained by\nthe image processing circuit 720.  For example, while the driver's vehicle is traveling in one of two or more lanes of the same direction, the image processing circuit 720 can distinguish which lane the driver's vehicle is traveling in, and supply that\nresult of distinction to the signal processing circuit 560.  When the number and azimuth(s) of preceding vehicles are to be recognized by using a predetermined algorithm for direction-of-arrival estimation (e.g., the MUSIC method), the signal processing\ncircuit 560 is able to provide more reliable information concerning a spatial distribution of preceding vehicles by referring to the information from the image processing circuit 720.\n Note that the onboard camera system 700 is an example of a means for identifying which lane the driver's vehicle is traveling in. The lane position of the driver's vehicle may be identified by any other means.  For example, by utilizing an\nultra-wide band (UWB) technique, it is possible to identify which one of a plurality of lanes the driver's vehicle is traveling in. It is widely known that the ultra-wide band technique is applicable to position measurement and/or radar.  Using the\nultra-wide band technique enhances the range resolution of the radar, so that, even when a large number of vehicles exist ahead, each individual target can be detected with distinction, based on differences in distance.  This makes it possible to\naccurately identify distance from a guardrail on the road shoulder, or from the median strip.  The width of each lane is predefined based on each country's law or the like.  By using such information, it becomes possible to identify where the lane in\nwhich the driver's vehicle is currently traveling is.  Note that the ultra-wide band technique is an example.  A radio wave based on any other wireless technique may be used.  Moreover, LIDAR (Light Detection and Ranging) may be used together with a\nradar.  LIDAR is sometimes called \"laser radar\".\n The array antenna AA may be a generic millimeter wave array antenna for onboard use.  The transmission antenna Tx in this Application Example radiates a millimeter wave as a transmission wave ahead of the vehicle.  A portion of the transmission\nwave is reflected off a target which is typically a preceding vehicle, whereby a reflected wave occurs from the target being a wave source.  A portion of the reflected wave reaches the array antenna (reception antenna) AA as an arriving wave.  Each of\nthe plurality of antenna elements of the array antenna AA outputs a reception signal in response to one or plural arriving waves.  In the case where the number of targets functioning as wave sources of reflected waves is K (where K is an integer of one\nor more), the number of arriving waves is K, but this number K of arriving waves is not known beforehand.\n The example of FIG. 22 assumes that the radar system 510 is provided as an integral piece, including the array antenna AA, on the rearview mirror.  However, the number and positions of array antennas AA are not limited to any specific number or\nspecific positions.  An array antenna AA may be disposed on the rear surface of the vehicle so as to be able to detect targets that are behind the vehicle.  Moreover, a plurality of array antennas AA may be disposed on the front surface and the rear\nsurface of the vehicle.  The array antenna(s) AA may be disposed inside the vehicle.  Even in the case where a horn antenna whose respective antenna elements include horns as mentioned above is to be adopted as the array antenna(s) AA, the array\nantenna(s) with such antenna elements may be situated inside the vehicle.\n The signal processing circuit 560 receives and processes the reception signals which have been received by the reception antenna Rx and subjected to preprocessing by the transmission/reception circuit 580.  This process encompasses inputting the\nreception signals to the arriving wave estimation unit AU, or alternatively, generating a secondary signal(s) from the reception signals and inputting the secondary signal(s) to the arriving wave estimation unit AU.\n In the example of FIG. 24, a selection circuit 596 which receives the signal being output from the signal processing circuit 560 and the signal being output from the image processing circuit 720 is provided in the object detection apparatus 570. The selection circuit 596 allows one or both of the signal being output from the signal processing circuit 560 and the signal being output from the image processing circuit 720 to be fed to the travel assistance electronic control apparatus 520.\n FIG. 25 is a block diagram showing a more detailed exemplary construction of the radar system 510 according to this Application Example.\n As shown in FIG. 25, the array antenna AA includes a transmission antenna Tx which transmits a millimeter wave and reception antennas Rx which receive arriving waves reflected from targets.  Although only one transmission antenna Tx is\nillustrated in the figure, two or more kinds of transmission antennas with different characteristics may be provided.  The array antenna AA includes M antenna elements 11.sub.1, 11.sub.2, .  . . , 11.sub.M (where M is an integer of 3 or more).  In\nresponse to the arriving waves, the plurality of antenna elements 11.sub.1, 11.sub.2, .  . . , 11.sub.M respectively output reception signals s.sub.1, s.sub.2, .  . . , s.sub.M (FIG. 21B).\n In the array antenna AA, the antenna elements 11.sub.1 to 11.sub.M are arranged in a linear array or a two-dimensional array at fixed intervals, for example.  Each arriving wave will impinge on the array antenna AA from a direction at an angle\n.theta.  with respect to the normal of the plane in which the antenna elements 11.sub.1 to 11.sub.M are arrayed.  Thus, the direction of arrival of an arriving wave is defined by this angle .theta..\n When an arriving wave from one target impinges on the array antenna AA, this approximates to a plane wave impinging on the antenna elements 11.sub.1 to 11.sub.M from azimuths of the same angle .theta..  When K arriving waves impinge on the array\nantenna AA from K targets with different azimuths, the individual arriving waves can be identified in terms of respectively different angles .theta..sub.1 to .theta..sub.K.\n As shown in FIG. 25, the object detection apparatus 570 includes the transmission/reception circuit 580 and the signal processing circuit 560.\n The transmission/reception circuit 580 includes a triangular wave generation circuit 581, a VCO (voltage controlled oscillator) 582, a distributor 583, mixers 584, filters 585, a switch 586, an A/D converter 587, and a controller 588.  Although\nthe radar system in this Application Example is configured to perform transmission and reception of millimeter waves by the FMCW method, the radar system of the present disclosure is not limited to this method.  The transmission/reception circuit 580 is\nconfigured to generate a beat signal based on a reception signal from the array antenna AA and a transmission signal from the transmission antenna Tx.\n The signal processing circuit 560 includes a distance detection section 533, a velocity detection section 534, and an azimuth detection section 536.  The signal processing circuit 560 is configured to process a signal from the A/D converter 587\nin the transmission/reception circuit 580, and output signals respectively indicating the detected distance to the target, the relative velocity of the target, and the azimuth of the target.\n First, the construction and operation of the transmission/reception circuit 580 will be described in detail.\n The triangular wave generation circuit 581 generates a triangular wave signal, and supplies it to the VCO 582.  The VCO 582 outputs a transmission signal having a frequency as modulated based on the triangular wave signal.  FIG. 26 is a diagram\nshowing change in frequency of a transmission signal which is modulated based on the signal that is generated by the triangular wave generation circuit 581.  This waveform has a modulation width .DELTA.f and a center frequency of f0.  The transmission\nsignal having a thus modulated frequency is supplied to the distributor 583.  The distributor 583 allows the transmission signal obtained from the VCO 582 to be distributed among the mixers 584 and the transmission antenna Tx.  Thus, the transmission\nantenna radiates a millimeter wave having a frequency which is modulated in triangular waves, as shown in FIG. 26.\n In addition to the transmission signal, FIG. 26 also shows an example of a reception signal from an arriving wave which is reflected from a single preceding vehicle.  The reception signal is delayed from the transmission signal.  This delay is\nin proportion to the distance between the driver's vehicle and the preceding vehicle.  Moreover, the frequency of the reception signal increases or decreases in accordance with the relative velocity of the preceding vehicle, due to the Doppler effect.\n When the reception signal and the transmission signal are mixed, a beat signal is generated based on their frequency difference.  The frequency of this beat signal (beat frequency) differs between a period in which the transmission signal\nincreases in frequency (ascent) and a period in which the transmission signal decreases in frequency (descent).  Once a beat frequency for each period is determined, based on such beat frequencies, the distance to the target and the relative velocity of\nthe target are calculated.\n FIG. 27 shows a beat frequency fu in an \"ascent\" period and a beat frequency fd in a \"descent\" period.  In the graph of FIG. 27, the horizontal axis represents frequency, and the vertical axis represents signal intensity.  This graph is obtained\nby subjecting the beat signal to time-frequency conversion.  Once the beat frequencies fu and fd are obtained, based on a known equation, the distance to the target and the relative velocity of the target are calculated.  In this Application Example,\nwith the construction and operation described below, beat frequencies corresponding to each antenna element of the array antenna AA are obtained, thus enabling estimation of the position information of a target.\n In the example shown in FIG. 25, reception signals from channels Ch.sub.1 to Ch.sub.M corresponding to the respective antenna elements 11.sub.1 to 11.sub.M are each amplified by an amplifier, and input to the corresponding mixers 584.  Each\nmixer 584 mixes the transmission signal into the amplified reception signal.  Through this mixing, a beat signal is generated corresponding to the frequency difference between the reception signal and the transmission signal.  The generated beat signal\nis fed to the corresponding filter 585.  The filters 585 apply bandwidth control to the beat signals on the channels Ch.sub.1 to Ch.sub.M, and supply bandwidth-controlled beat signals to the switch 586.\n The switch 586 performs switching in response to a sampling signal which is input from the controller 588.  The controller 588 may be composed of a microcomputer, for example.  Based on a computer program which is stored in a memory such as a\nROM, the controller 588 controls the entire transmission/reception circuit 580.  The controller 588 does not need to be provided inside the transmission/reception circuit 580, but may be provided inside the signal processing circuit 560.  In other words,\nthe transmission/reception circuit 580 may operate in accordance with a control signal from the signal processing circuit 560.  Alternatively, some or all of the functions of the controller 588 may be realized by a central processing unit which controls\nthe entire transmission/reception circuit 580 and signal processing circuit 560.\n The beat signals on the channels Ch.sub.1 to Ch.sub.M having passed through the respective filters 585 are consecutively supplied to the A/D converter 587 via the switch 586.  In synchronization with the sampling signal, the A/D converter 587\nconverts the beat signals on the channels Ch.sub.1 to Ch.sub.M, which are input from the switch 586, into digital signals.\n Hereinafter, the construction and operation of the signal processing circuit 560 will be described in detail.  In this Application Example, the distance to the target and the relative velocity of the target are estimated by the FMCW method. \nWithout being limited to the FMCW method as described below, the radar system can also be implemented by using other methods, e.g., 2 frequency CW and spread spectrum methods.\n In the example shown in FIG. 25, the signal processing circuit 560 includes a memory 531, a reception intensity calculation section 532, a distance detection section 533, a velocity detection section 534, a DBF (digital beam forming) processing\nsection 535, an azimuth detection section 536, a target link processing section 537, a matrix generation section 538, a target output processing section 539, and an arriving wave estimation unit AU.  As mentioned earlier, a part or a whole of the signal\nprocessing circuit 560 may be implemented by FPGA, or by a set of a general-purpose processor(s) and a main memory device(s).  The memory 531, the reception intensity calculation section 532, the DBF processing section 535, the distance detection section\n533, the velocity detection section 534, the azimuth detection section 536, the target link processing section 537, and the arriving wave estimation unit AU may be individual parts that are implemented in distinct pieces of hardware, or functional blocks\nof a single signal processing circuit.\n FIG. 28 shows an exemplary implementation in which the signal processing circuit 560 is implemented in hardware including a processor PR and a memory device MD.  In the signal processing circuit 560 with this construction, too, a computer\nprogram that is stored in the memory device MD may fulfill the functions of the reception intensity calculation section 532, the DBF processing section 535, the distance detection section 533, the velocity detection section 534, the azimuth detection\nsection 536, the target link processing section 537, the matrix generation section 538, and the arriving wave estimation unit AU shown in FIG. 25.\n The signal processing circuit 560 in this Application Example is configured to estimate the position information of a preceding vehicle by using each beat signal converted into a digital signal as a secondary signal of the reception signal, and\noutput a signal indicating the estimation result.  Hereinafter, the construction and operation of the signal processing circuit 560 in this Application Example will be described in detail.\n For each of the channels Ch.sub.1 to Ch.sub.M, the memory 531 in the signal processing circuit 560 stores a digital signal which is output from the A/D converter 587.  The memory 531 may be composed of a generic storage medium such as a\nsemiconductor memory or a hard disk and/or an optical disk.\n The reception intensity calculation section 532 applies Fourier transform to the respective beat signals for the channels Ch.sub.1 to Ch.sub.M (shown in the lower graph of FIG. 26) that are stored in the memory 531.  In the present\nspecification, the amplitude of a piece of complex number data after the Fourier transform is referred to as \"signal intensity\".  The reception intensity calculation section 532 converts the complex number data of a reception signal from one of the\nplurality of antenna elements, or a sum of the complex number data of all reception signals from the plurality of antenna elements, into a frequency spectrum.  In the resultant spectrum, beat frequencies corresponding to respective peak values, which are\nindicative of presence and distance of targets (preceding vehicles), can be detected.  Taking a sum of the complex number data of the reception signals from all antenna elements will allow the noise components to average out, whereby the S/N ratio is\nimproved.\n In the case where there is one target, i.e., one preceding vehicle, as shown in FIG. 27, the Fourier transform will produce a spectrum having one peak value in a period of increasing frequency (the \"ascent\" period) and one peak value in a period\nof decreasing frequency (\"the descent\" period).  The beat frequency of the peak value in the \"ascent\" period is denoted by \"fu\", whereas the beat frequency of the peak value in the \"descent\" period is denoted by \"fd\".\n From the signal intensities of beat frequencies, the reception intensity calculation section 532 detects any signal intensity that exceeds a predefined value (threshold value), thus determining the presence of a target.  Upon detecting a signal\nintensity peak, the reception intensity calculation section 532 outputs the beat frequencies (fu, fd) of the peak values to the distance detection section 533 and the velocity detection section 534 as the frequencies of the object of interest.  The\nreception intensity calculation section 532 outputs information indicating the frequency modulation width .DELTA.f to the distance detection section 533, and outputs information indicating the center frequency f0 to the velocity detection section 534.\n In the case where signal intensity peaks corresponding to plural targets are detected, the reception intensity calculation section 532 find associations between the ascents peak values and the descent peak values based on predefined conditions. \nPeaks which are determined as belonging to signals from the same target are given the same number, and thus are fed to the distance detection section 533 and the velocity detection section 534.\n When there are plural targets, after the Fourier transform, as many peaks as there are targets will appear in the ascent portions and the descent portions of the beat signal.  In proportion to the distance between the radar and a target, the\nreception signal will become more delayed and the reception signal in FIG. 26 will shift more toward the right.  Therefore, a beat signal will have a greater frequency as the distant between the target and the radar increases.\n Based on the beat frequencies fu and fd which are input from the reception intensity calculation section 532, the distance detection section 533 calculates a distance R through the equation below, and supplies it to the target link processing\nsection 537.  R={cT/(2.DELTA.f)}{(fu+fd)/2}\n Moreover, based on the beat frequencies fu and fd being input from the reception intensity calculation section 532, the velocity detection section 534 calculates a relative velocity V through the equation below, and supplies it to the target\nlink processing section 537.  V={c/(2.about.f0)}{(fu-fd)/2}\n In the equation which calculates the distance R and the relative velocity V, c is velocity of light, and T is the modulation period.\n Note that the lower limit resolution of distance R is expressed as c/(2.DELTA.f).  Therefore, as .DELTA.f increases, the resolution of distance R increases.  In the case where the frequency f0 is in the 76 GHz band, when .DELTA.f is set on the\norder of 660 megahertz (MHz), the resolution of distance R will be on the order of 0.23 meters (m), for example.  Therefore, if two preceding vehicles are traveling abreast of each other, it may be difficult with the FMCW method to identify whether there\nis one vehicle or two vehicles.  In such a case, it might be possible to run an algorithm for direction-of-arrival estimation that has an extremely high angular resolution to separate between the azimuths of the two preceding vehicles and enable\ndetection.\n By utilizing phase differences between signals from the antenna elements 11.sub.1, 11.sub.2, .  . . , 11.sub.M, the DBF processing section 535 allows the incoming complex data corresponding to the respective antenna elements, which has been\nFourier transformed with respect to the time axis, to be Fourier transformed with respect to the direction in which the antenna elements are arrayed.  Then, the DBF processing section 535 calculates spatial complex number data indicating the spectrum\nintensity for each angular channel as determined by the angular resolution, and outputs it to the azimuth detection section 536 for the respective beat frequencies.\n The azimuth detection section 536 is provided for the purpose of estimating the azimuth of a preceding vehicle.  Among the values of spatial complex number data that has been calculated for the respective beat frequencies, the azimuth detection\nsection 536 chooses an angle .theta.  that takes the largest value, and outputs it to the target link processing section 537 as the azimuth at which an object of interest exists.\n Note that the method of estimating the angle .theta.  indicating the direction of arrival of an arriving wave is not limited to this example.  Various algorithms for direction-of-arrival estimation that have been mentioned earlier can be\nemployed.\n The target link processing section 537 calculates absolute values of the differences between the respective values of distance, relative velocity, and azimuth of the object of interest as calculated in the current cycle and the respective values\nof distance, relative velocity, and azimuth of the object of interest as calculated 1 cycle before, which are read from the memory 531.  Then, if the absolute value of each difference is smaller than a value which is defined for the respective value, the\ntarget link processing section 537 determines that the target that was detected 1 cycle before and the target detected in the current cycle are an identical target.  In that case, the target link processing section 537 increments the count of target link\nprocesses, which is read from the memory 531, by one.\n If the absolute value of a difference is greater than predetermined, the target link processing section 537 determines that a new object of interest has been detected.  The target link processing section 537 stores the respective values of\ndistance, relative velocity, and azimuth of the object of interest as calculated in the current cycle and also the count of target link processes for that object of interest to the memory 531.\n In the signal processing circuit 560, the distance to the object of interest and its relative velocity can be detected by using a spectrum which is obtained through a frequency analysis of beat signals, which are signals generated based on\nreceived reflected waves.\n The matrix generation section 538 generates a spatial covariance matrix by using the respective beat signals for the channels Ch.sub.1 to Ch.sub.M (lower graph in FIG. 26) stored in the memory 531.  In the spatial covariance matrix of Math. 4,\neach component is the value of a beat signal which is expressed in terms of real and imaginary parts.  The matrix generation section 538 further determines eigenvalues of the spatial covariance matrix Rxx, and inputs the resultant eigenvalue information\nto the arriving wave estimation unit AU.\n When a plurality of signal intensity peaks corresponding to plural objects of interest have been detected, the reception intensity calculation section 532 numbers the peak values respectively in the ascent portion and in the descent portion,\nbeginning from those with smaller frequencies first, and output them to the target output processing section 539.  In the ascent and descent portions, peaks of any identical number correspond to the same object of interest.  The identification numbers\nare to be regarded as the numbers assigned to the objects of interest.  For simplicity of illustration, a leader line from the reception intensity calculation section 532 to the target output processing section 539 is conveniently omitted from FIG. 25.\n When the object of interest is a structure ahead, the target output processing section 539 outputs the identification number of that object of interest as indicating a target.  When receiving results of determination concerning plural objects of\ninterest, such that all of them are structures ahead, the target output processing section 539 outputs the identification number of an object of interest that is in the lane of the driver's vehicle as the object position information indicating where a\ntarget is.  Moreover, when receiving results of determination concerning plural objects of interest, such that all of them are structures ahead and that two or more objects of interest are in the lane of the driver's vehicle, the target output processing\nsection 539 outputs the identification number of an object of interest that is associated with the largest count of target being read from the link processes memory 531 as the object position information indicating where a target is.\n Referring back to FIG. 24, an example where the onboard radar system 510 is incorporated in the exemplary construction shown in FIG. 24 will be described.  The image processing circuit 720 acquires information of an object from the video, and\ndetects target position information from the object information.  For example, the image processing circuit 720 is configured to estimate distance information of an object by detecting the depth value of an object within an acquired video, or detect size\ninformation and the like of an object from characteristic amounts in the video, thus detecting position information of the object.\n The selection circuit 596 selectively feeds position information which is received from the signal processing circuit 560 or the image processing circuit 720 to the travel assistance electronic control apparatus 520.  For example, the selection\ncircuit 596 compares a first distance, i.e., the distance from the driver's vehicle to a detected object as contained in the object position information from the signal processing circuit 560, against a second distance, i.e., the distance from the\ndriver's vehicle to the detected object as contained in the object position information from the image processing circuit 720, and determines which is closer to the driver's vehicle.  For example, based on the result of determination, the selection\ncircuit 596 may select the object position information which indicates a closer distance to the driver's vehicle, and output it to the travel assistance electronic control apparatus 520.  If the result of determination indicates the first distance and\nthe second distance to be of the same value, the selection circuit 596 may output either one, or both of them, to the travel assistance electronic control apparatus 520.\n If information indicating that there is no prospective target is input from the reception intensity calculation section 532, the target output processing section 539 (FIG. 25) outputs zero, indicating that there is no target, as the object\nposition information.  Then, on the basis of the object position information from the target output processing section 539, through comparison against a predefined threshold value, the selection circuit 596 chooses either the object position information\nfrom the signal processing circuit 560 or the object position information from the image processing circuit 720 to be used.\n Based on predefined conditions, the travel assistance electronic control apparatus 520 having received the position information of a preceding object from the object detection apparatus 570 performs control to make the operation safer or easier\nfor the driver who is driving the driver's vehicle, in accordance with the distance and size indicated by the object position information, the velocity of the driver's vehicle, road surface conditions such as rainfall, snowfall or clear weather, or other\nconditions.  For example, if the object position information indicates that no object has been detected, the travel assistance electronic control apparatus 520 may send a control signal to an accelerator control circuit 526 to increase speed up to a\npredefined velocity, thereby controlling the accelerator control circuit 526 to make an operation that is equivalent to stepping on the accelerator pedal.\n In the case where the object position information indicates that an object has been detected, if it is found to be at a predetermined distance from the driver's vehicle, the travel assistance electronic control apparatus 520 controls the brakes\nvia a brake control circuit 524 through a brake-by-wire construction or the like.  In other words, it makes an operation of decreasing the velocity to maintain a constant vehicular gap.  Upon receiving the object position information, the travel\nassistance electronic control apparatus 520 sends a control signal to an alarm control circuit 522 so as to control lamp illumination or control audio through a loudspeaker which is provided within the vehicle, so that the driver is informed of the\nnearing of a preceding object.  Upon receiving object position information including a spatial distribution of preceding vehicles, the travel assistance electronic control apparatus 520 may, if the traveling velocity is within a predefined range,\nautomatically make the steering wheel easier to operate to the right or left, or control the hydraulic pressure on the steering wheel side so as to force a change in the direction of the wheels, thereby providing assistance in collision avoidance with\nrespect to the preceding object.\n The object detection apparatus 570 may be arranged so that, if a piece of object position information which was being continuously detected by the selection circuit 596 for a while in.sup.L the previous detection cycle but which is not detected\nin the current detection cycle becomes associated with a piece of object position information from a camera-detected video indicating a preceding object, then continued tracking is chosen, and object position information from the signal processing\ncircuit 560 is output with priority.\n An exemplary specific construction and an exemplary operation for the selection circuit 596 to make a selection between the outputs from the signal processing circuit 560 and the image processing circuit 720 are disclosed in the specification of\nU.S.  Pat.  No. 8,446,312, the specification of U.S.  Pat.  No. 8,730,096, and the specification of U.S.  Pat.  No. 8,730,099.  The entire disclosure thereof is incorporated herein by reference.\n [First Variant]\n In the radar system for onboard use of the above Application Example, the (sweep) condition for a single instance of FMCW (Frequency Modulated Continuous Wave) frequency modulation, i.e., a time span required for such a modulation (sweep time),\nis e.g. 1 millisecond, although the sweep time could be shortened to about 100 microseconds.\n However, in order to realize such a rapid sweep condition, not only the constituent elements involved in the radiation of a transmission wave, but also the constituent elements involved in the reception under that sweep condition must also be\nable to rapidly operate.  For example, an A/D converter 587 (FIG. 25) which rapidly operates under that sweep condition will be needed.  The sampling frequency of the A/D converter 587 may be 10 MHz, for example.  The sampling frequency may be faster\nthan 10 MHz.\n In the present variant, a relative velocity with respect to a target is calculated without utilizing any Doppler shift-based frequency component.  In this variant, the sweep time is Tm=100 microseconds, which is very short.  The lowest frequency\nof a detectable beat signal, which is 1/Tm, equals 10 kHz in this case.  This would correspond to a Doppler shift of a reflected wave from a target which has a relative velocity of approximately 20 m/second.  In other words, so long as one relies on a\nDoppler shift, it would be impossible to detect relative velocities that are equal to or smaller than this.  Thus, a method of calculation which is different from a Doppler shift-based method of calculation is preferably adopted.\n As an example, this variant illustrates a process that utilizes a signal (upbeat signal) representing a difference between a transmission wave and a reception wave which is obtained in an upbeat (ascent) portion where the transmission wave\nincreases in frequency.  A single sweep time of FMCW is 100 microseconds, and its waveform is a sawtooth shape which is composed only of an upbeat portion.  In other words, in this variant, the signal wave which is generated by the triangular wave/CW\nwave generation circuit 581 has a sawtooth shape.  The sweep width in frequency is 500 MHz.  Since no peaks are to be utilized that are associated with Doppler shifts, the process is not one that generates an upbeat signal and a downbeat signal to\nutilize the peaks of both, but will rely on only one of such signals.  Although a case of utilizing an upbeat signal will be illustrated herein, a similar process can also be performed by using a downbeat signal.\n The A/D converter 587 (FIG. 25) samples each upbeat signal at a sampling frequency of 10 MHz, and outputs several hundred pieces of digital data (hereinafter referred to as \"sampling data\").  The sampling data is generated based on upbeat\nsignals after a point in time where a reception wave is obtained and until a point in time at which a transmission wave completes transmission, for example.  Note that the process may be ended as soon as a certain number of pieces of sampling data are\nobtained.\n In this variant, 128 upbeat signals are transmitted/received in series, for each of which some several hundred pieces of sampling data are obtained.  The number of upbeat signals is not limited to 128.  It may be 256, or 8.  An arbitrary number\nmay be selected depending on the purpose.\n The resultant sampling data is stored to the memory 531.  The reception intensity calculation section 532 applies a two-dimensional fast Fourier transform (FFT) to the sampling data.  Specifically, first, for each of the sampling data pieces\nthat have been obtained through a single sweep, a first FFT process (frequency analysis process) is performed to generate a power spectrum.  Next, the velocity detection section 534 performs a second FFT process for the processing results that have been\ncollected from all sweeps.\n When the reflected waves are from the same target, peak components in the power spectrum to be detected in each sweep period will be of the same frequency.  On the other hand, for different targets, the peak components will differ in frequency. \nThrough the first FFT process, plural targets that are located at different distances can be separated.\n In the case where a relative velocity with respect to a target is non-zero, the phase of the upbeat signal changes slightly from sweep to sweep.  In other words, through the second FFT process, a power spectrum whose elements are the data of\nfrequency components that are associated with such phase changes will be obtained for the respective results of the first FFT process.\n The reception intensity calculation section 532 extracts peak values in the second power spectrum above, and sends them to the velocity detection section 534.\n The velocity detection section 534 determines a relative velocity from the phase changes.  For example, suppose that a series of obtained upbeat signals undergo phase changes by every phase .theta.  [RXd].  Assuming that the transmission wave\nhas an average wavelength .lamda., this means there is a .lamda./(4.pi./.theta.) change in distance every time an upbeat signal is obtained.  Since this change has occurred over an interval of upbeat signal transmission Tm (=100 microseconds), the\nrelative velocity is determined to be {.lamda./(4.pi./.theta.)}/Tm.\n Through the above processes, a relative velocity with respect to a target as well as a distance from the target can be obtained.\n [Second Variant]\n The radar system 510 is able to detect a target by using a continuous wave(s) CW of one or plural frequencies.  This method is especially useful in an environment where a multitude of reflected waves impinge on the radar system 510 from still\nobjects in the surroundings, e.g., when the vehicle is in a tunnel.\n The radar system 510 has an antenna array for reception purposes, including five channels of independent reception elements.  In such a radar system, the azimuth-of-arrival estimation for incident reflected waves is only possible if there are\nfour or fewer reflected waves that are simultaneously incident.  In an FMCW-type radar, the number of reflected waves to be simultaneously subjected to an azimuth-of-arrival estimation can be reduced by exclusively selecting reflected waves from a\nspecific distance.  However, in an environment where a large number of still objects exist in the surroundings, e.g., in a tunnel, it is as if there were a continuum of objects to reflect radio waves; therefore, even if one narrows down on the reflected\nwaves based on distance, the number of reflected waves may still not be equal to or smaller than four.  However, any such still object in the surroundings will have an identical relative velocity with respect to the driver's vehicle, and the relative\nvelocity will be greater than that associated with any other vehicle that is traveling ahead.  On this basis, such still objects can be distinguished from any other vehicle based on the magnitudes of Doppler shifts.\n Therefore, the radar system 510 performs a process of: radiating continuous waves CW of plural frequencies; and, while ignoring Doppler shift peaks that correspond to still objects in the reception signals, detecting a distance by using a\nDoppler shift peak(s) of any smaller shift amount(s).  Unlike in the FMCW method, in the CW method, a frequency difference between a transmission wave and a reception wave is ascribable only to a Doppler shift.  In other words, any peak frequency that\nappears in a beat signal is ascribable only to a Doppler shift.\n In the description of this variant, too, a continuous wave to be used in the CW method will be referred to as a \"continuous wave CW\".  As described above, a continuous wave CW has a constant frequency; that is, it is unmodulated.\n Suppose that the radar system 510 has radiated a continuous wave CW of a frequency fp, and detected a reflected wave of a frequency fq that has been reflected off a target.  The difference between the transmission frequency fp and the reception\nfrequency fq is called a Doppler frequency, which approximates to fp-fq=2Vrfp/c. Herein, Vr is a relative velocity between the radar system and the target, and c is the velocity of light.  The transmission frequency fp, the Doppler frequency (fp-fq), and\nthe velocity of light c are known.  Therefore, from this equation, the relative velocity Vr=(fp-fq)c/2fp can be determined.  The distance to the target is calculated by utilizing phase information as will be described later.\n In order to detect a distance to a target by using continuous waves CW, a 2 frequency CW method is adopted.  In the 2 frequency CW method, continuous waves CW of two frequencies which are slightly apart are radiated each for a certain period,\nand their respective reflected waves are acquired.  For example, in the case of using frequencies in the 76 GHz band, the difference between the two frequencies would be several hundred kHz.  As will be described later, it is more preferable to determine\nthe difference between the two frequencies while taking into account the minimum distance at which the radar used is able to detect a target.\n Suppose that the radar system 510 has sequentially radiated continuous waves CW of frequencies fp1 and fp2 (fp1&lt;fp2), and that the two continuous waves CW have been reflected off a single target, resulting in reflected waves of frequencies\nfq1 and fq2 being received by the radar system 510.\n Based on the continuous wave CW of the frequency fp1 and the reflected wave (frequency fq1) thereof, a first Doppler frequency is obtained.  Based on the continuous wave CW of the frequency fp2 and the reflected wave (frequency fq2) thereof, a\nsecond Doppler frequency is obtained.  The two Doppler frequencies have substantially the same value.  However, due to the difference between the frequencies fp1 and fp2, the complex signals of the respective reception waves differ in phase.  By\nutilizing this phase information, a distance (range) to the target can be calculated.\n Specifically, the radar system 510 is able to determine the distance R as R=c.DELTA..PHI./4.pi.(fp2-fp1).  Herein, .DELTA..PHI.  denotes the phase difference between two beat signals, i.e., beat signal 1 which is obtained as a difference between\nthe continuous wave CW of the frequency fp1 and the reflected wave (frequency fq1) thereof and beat signal 2 which is obtained as a difference between the continuous wave CW of the frequency fp2 and the reflected wave (frequency fq2) thereof.  The method\nof identifying the frequency fb1 of beat signal 1 and the frequency fb2 of beat signal 2 is identical to that in the aforementioned instance of a beat signal from a continuous wave CW of a single frequency.\n Note that a relative velocity Vr under the 2 frequency CW method is determined as follows.  Vr=fb1c/2fp1 or Vr=fb2c/2fp2\n Moreover, the range in which a distance to a target can be uniquely identified is limited to the range defined by Rmax&lt;c/2(fp2-fp1).  The reason is that beat signals resulting from a reflected wave from any farther target would produce a\n.DELTA..PHI.  which is greater than 2.pi., such that they are indistinguishable from beat signals associated with targets at closer positions.  Therefore, it is more preferable to adjust the difference between the frequencies of the two continuous waves\nCW so that Rmax becomes greater than the minimum detectable distance of the radar.  In the case of a radar whose minimum detectable distance is 100 m, fp2-fp1 may be made e.g. 1.0 MHz.  In this case, Rmax=150 m, so that a signal from any target from a\nposition beyond Rmax is not detected.  In the case of mounting a radar which is capable of detection up to 250 m, fp2-fp1 may be made e.g. 500 kHz.  In this case, Rmax=300 m, so that a signal from any target from a position beyond Rmax is not detected,\neither.  In the case where the radar has both of an operation mode in which the minimum detectable distance is 100 m and the horizontal viewing angle is 120 degrees and an operation mode in which the minimum detectable distance is 250 m and the\nhorizontal viewing angle is 5 degrees, it is preferable to switch the fp2-fp1 value be 1.0 MHz and 500 kHz for operation in the respective operation modes.\n A detection approach is known which, by transmitting continuous waves CW at N different frequencies (where N is an integer of 3 or more), and utilizing phase information of the respective reflected waves, detects a distance to each target. \nUnder this detection approach, distance can be properly recognized up to N-1 targets.  As the processing to enable this, a fast Fourier transform (FFT) is used, for example.  Given N=64 or 128, an FFT is performed for sampling data of a beat signal as a\ndifference between a transmission signal and a reception signal for each frequency, thus obtaining a frequency spectrum (relative velocity).  Thereafter, at the frequency of the CW wave, a further FFT is performed for peaks of the same frequency, thus to\nderive distance information.\n Hereinafter, this will be described more specifically.\n For ease of explanation, first, an instance will be described where signals of three frequencies f1, f2 and f3 are transmitted while being switched over time.  It is assumed that f1&gt;f2&gt;f3, and f1-f2=f2-f3=.DELTA.f.  A transmission time\n.DELTA.t is assumed for the signal wave for each frequency.  FIG. 29 shows a relationship between three frequencies f1, f2 and f3.\n Via the transmission antenna Tx, the triangular wave/CW wave generation circuit 581 (FIG. 25) transmits continuous waves CW of frequencies f1, f2 and f3, each lasting for the time .DELTA.t.  The reception antennas Rx receive reflected waves\nresulting by the respective continuous waves CW being reflected off one or plural targets.\n Each mixer 584 mixes a transmission wave and a reception wave to generate a beat signal.  The A/D converter 587 converts the beat signal, which is an analog signal, into several hundred pieces of digital data (sampling data), for example.\n Using the sampling data, the reception intensity calculation section 532 performs FFT computation.  Through the FFT computation, frequency spectrum information of reception signals is obtained for the respective transmission frequencies f1, f2\nand f3.\n Thereafter, the reception intensity calculation section 532 separates peak values from the frequency spectrum information of the reception signals.  The frequency of any peak value which is predetermined or greater is in proportion to a relative\nvelocity with respect to a target.  Separating a peak value(s) from the frequency spectrum information of reception signals is synonymous with separating one or plural targets with different relative velocities.\n Next, with respect to each of the transmission frequencies f1 to f3, the reception intensity calculation section 532 measures spectrum information of peak values of the same relative velocity or relative velocities within a predefined range.\n Now, consider a scenario where two targets A and B exist which have about the same relative velocity but are at respectively different distances.  A transmission signal of the frequency f1 will be reflected from both of targets A and B to result\nin reception signals being obtained.  The reflected waves from targets A and B will result in substantially the same beat signal frequency.  Therefore, the power spectra at the Doppler frequencies of the reception signals, corresponding to their relative\nvelocities, are obtained as a synthetic spectrum F1 into which the power spectra of two targets A and B have been merged.\n Similarly, for each of the frequencies f2 and f3, the power spectra at the Doppler frequencies of the reception signals, corresponding to their relative velocities, are obtained as a synthetic spectrum F1 into which the power spectra of two\ntargets A and B have been merged.\n FIG. 30 shows a relationship between synthetic spectra F1 to F3 on a complex plane.  In the directions of the two vectors composing each of the synthetic spectra F1 to F3, the right vector corresponds to the power spectrum of a reflected wave\nfrom target A; i.e., vectors f1A, f2A and f3A, in FIG. 30.  On the other hand, in the directions of the two vectors composing each of the synthetic spectra F1 to F3, the left vector corresponds to the power spectrum of a reflected wave from target B;\ni.e., vectors f1B, f2B and f3B in FIG. 30.\n Under a constant difference .DELTA.f between the transmission frequencies, the phase difference between the reception signals corresponding to the respective transmission signals of the frequencies f1 and f2 is in proportion to the distance to a\ntarget.  Therefore, the phase difference between the vectors f1A and f2A and the phase difference between the vectors f2A and f3A are of the same value .theta.A, this phase difference .theta.A being in proportion to the distance to target A. Similarly,\nthe phase difference between the vectors f1B and f2B and the phase difference between the vectors f2B and f3B are of the same value .theta.B, this phase difference .theta.B being in proportion to the distance to target B.\n By using a well-known method, the respective distances to targets A and B can be determined from the synthetic spectra F1 to F3 and the difference .DELTA.f between the transmission frequencies.  This technique is disclosed in U.S.  Pat.  No.\n6,703,967, for example.  The entire disclosure of this publication is incorporated herein by reference.\n Similar processing is also applicable when the transmitted signals have four or more frequencies.\n Note that, before transmitting continuous waves CW at N different frequencies, a process of determining the distance to and relative velocity of each target may be performed by the 2 frequency CW method.  Then, under predetermined conditions,\nthis process may be switched to a process of transmitting continuous waves CW at N different frequencies.  For example, FFT computation may be performed by using the respective beat signals at the two frequencies, and if the power spectrum of each\ntransmission frequency undergoes a change over time of 30% or more, the process may be switched.  The amplitude of a reflected wave from each target undergoes a large change over time due to multipath influences and the like.  When there exists a change\nof a predetermined magnitude or greater, it may be considered that plural targets may exist.\n Moreover, the CW method is known to be unable to detect a target when the relative velocity between the radar system and the target is zero, i.e., when the Doppler frequency is zero.  However, when a pseudo Doppler signal is determined by the\nfollowing methods, for example, it is possible to detect a target by using that frequency.\n (Method 1) A mixer that causes a certain frequency shift in the output of a receiving antenna is added.  By using a transmission signal and a reception signal with a shifted frequency, a pseudo Doppler signal can be obtained.\n (Method 2) A variable phase shifter to introduce phase changes continuously over time is inserted between the output of a receiving antenna and a mixer, thus adding a pseudo phase difference to the reception signal.  By using a transmission\nsignal and a reception signal with an added phase difference, a pseudo Doppler signal can be obtained.\n An example of specific construction and operation of inserting a variable phase shifter to generate a pseudo Doppler signal under Method 2 is disclosed in Japanese Laid-Open Patent Publication No. 2004-257848.  The entire disclosure of this\npublication is incorporated herein by reference.\n When targets with zero or very little relative velocity need to be detected, the aforementioned processes of generating a pseudo Doppler signal may be adopted, or the process may be switched to a target detection process under the FMCW method.\n Next, with reference to FIG. 31, a procedure of processing to be performed by the object detection apparatus 570 of the onboard radar system 510 will be described.\n The example below will illustrate a case where continuous waves CW are transmitted at two different frequencies fp1 and fp2 (fp1&lt;fp2), and the phase information of each reflected wave is utilized to respectively detect a distance with respect\nto a target.\n FIG. 31 is a flowchart showing the procedure of a process of determining relative velocity and distance according to this variant.\n At step S41, the triangular wave/CW wave generation circuit 581 generates two continuous waves CW of frequencies which are slightly apart, i.e., frequencies fp1 and fp2.\n At step S42, the transmission antenna Tx and the reception antennas Rx perform transmission/reception of the generated series of continuous waves CW.  Note that the process of step S41 and the process of step S42 are to be performed in parallel\nfashion respectively by the triangular wave/CW wave generation circuit 581 and the transmission antenna element Tx/reception antenna Rx, rather than step S42 following only after completion of step S41.\n At step S43, each mixer 584 generates a difference signal by utilizing each transmission wave and each reception wave, whereby two difference signals are obtained.  Each reception wave is inclusive of a reception wave emanating from a still\nobject and a reception wave emanating from a target.  Therefore, next, a process of identifying frequencies to be utilized as the beat signals is performed.  Note that the process of step S41, the process of step S42, and the process of step S43 are to\nbe performed in parallel fashion by the triangular wave/CW wave generation circuit 581, the transmission antenna Tx/reception antenna Rx, and the mixers 584, rather than step S42 following only after completion of step S41, or step S43 following only\nafter completion of step S42.\n At step S44, for each of the two difference signals, the object detection apparatus 570 identifies certain peak frequencies to be frequencies fb1 and fb2 of beat signals, such that these frequencies are equal to or smaller than a frequency which\nis predefined as a threshold value and yet they have amplitude values which are equal to or greater than a predetermined amplitude value, and that the difference between the two frequencies is equal to or smaller than a predetermined value.\n At step S45, based on one of the two beat signal frequencies identified, the reception intensity calculation section 532 detects a relative velocity.  The reception intensity calculation section 532 calculates the relative velocity according to\nVr=fb1c/2fp1, for example.  Note that a relative velocity may be calculated by utilizing each of the two beat signal frequencies, which will allow the reception intensity calculation section 532 to verify whether they match or not, thus enhancing the\nprecision of relative velocity calculation.\n At step S46, the reception intensity calculation section 532 determines a phase difference .DELTA..PHI.  between two beat signals 1 and 2, and determines a distance R=c.DELTA..PHI./4.pi.(fp2-fp1) to the target.\n Through the above processes, the relative velocity and distance to a target can be detected.\n Note that continuous waves CW may be transmitted at N different frequencies (where N is 3 or more), and by utilizing phase information of the respective reflected wave, distances to plural targets which are of the same relative velocity but at\ndifferent positions may be detected.\n In addition to the radar system 510, the vehicle 500 described above may further include another radar system.  For example, the vehicle 500 may further include a radar system having a detection range toward the rear or the sides of the vehicle\nbody.  In the case of incorporating a radar system having a detection range toward the rear of the vehicle body, the radar system may monitor the rear, and if there is any danger of having another vehicle bump into the rear, make a response by issuing an\nalarm, for example.  In the case of incorporating a radar system having a detection range toward the sides of the vehicle body, the radar system may monitor an adjacent lane when the driver's vehicle changes its lane, etc., and make a response by issuing\nan alarm or the like as necessary.\n The applications of the above-described radar system 510 are not limited to onboard use only.  Rather, the radar system 510 may be used as sensors for various purposes.  For example, it may be used as a radar for monitoring the surroundings of a\nhouse or any other building.  Alternatively, it may be used as a sensor for detecting the presence or absence of a person at a specific indoor place, or whether or not such a person is undergoing any motion, etc., without utilizing any optical images.\n [Supplementary Details of Processing]\n Other embodiments will be described in connection with the 2 frequency CW or FMCW techniques for array antennas as described above.  As described earlier, in the example of FIG. 25, the reception intensity calculation section 532 applies a\nFourier transform to the respective beat signals for the channels Ch.sub.1 to Ch.sub.M (lower graph in FIG. 26) stored in the memory 531.  These beat signals are complex signals, in order that the phase of the signal of computational interest be\nidentified.  This allows the direction of an arriving wave to be accurately identified.  In this case, however, the computational load for Fourier transform increases, thus calling for a larger-scaled circuit.\n In order to solve this problem, a scalar signal may be generated as a beat signal.  For each of a plurality of beat signals that have been generated, two complex Fourier transforms may be performed with respect to the spatial axis direction,\nwhich conforms to the antenna array, and to the time axis direction, which conforms to the lapse of time, thus to obtain results of frequency analysis.  As a result, with only a small amount of computation, beam formation can eventually be achieved so\nthat directions of arrival of reflected waves can be identified, whereby results of frequency analysis can be obtained for the respective beams.  As a patent document related to the present disclosure, the entire disclosure of the specification of U.S. \nPat.  No. 6,339,395 is incorporated herein by reference.\n [Optical Sensor, e.g., Camera, and Millimeter Wave Radar]\n Next, a comparison between the above-described array antenna and conventional antennas, as well as an exemplary application in which both of the present array antenna and an optical sensor (e.g., a camera) are utilized, will be described.  Note\nthat LIDAR or the like may be employed as the optical sensor.\n A millimeter wave radar is able to directly detect a distance (range) to a target and a relative velocity thereof.  Another characteristic is that its detection performance is not much deteriorated in the nighttime (including dusk), or in bad\nweather, e.g., rainfall, fog, or snowfall.  On the other hand, it is believed that it is not just as easy for a millimeter wave radar to take a two-dimensional grasp of a target as it is for a camera.  On the other hand, it is relatively easy for a\ncamera to take a two-dimensional grasp of a target and recognize its shape.  However, a camera may not be able to image a target in nighttime or bad weather, which presents a considerable problem.  This problem is particularly outstanding when droplets\nof water have adhered to the portion through which to ensure lighting, or the eyesight is narrowed by a fog.  This problem similarly exists for LIDAR or the like, which also pertains to the realm of optical sensors.\n In these years, in answer to increasing demand for safer vehicle operation, driver assist systems for preventing collisions or the like are being developed.  A driver assist system acquires an image in the direction of vehicle travel with a\nsensor such as a camera or a millimeter wave radar, and when any obstacle is recognized that is predicted to hinder vehicle travel, brakes or the like are automatically applied to prevent collisions or the like.  Such a function of collision avoidance is\nexpected to operate normally, even in nighttime or bad weather.\n Hence, driver assist systems of a so-called fusion construction are gaining prevalence, where, in addition to a conventional optical sensor such as a camera, a millimeter wave radar is mounted as a sensor, thus realizing a recognition process\nthat takes advantage of both.  Such a driver assist system will be discussed later.\n On the other hand, higher and higher functions are being required of the millimeter wave radar itself.  A millimeter wave radar for onboard use mainly uses electromagnetic waves of the 76 GHz band.  The antenna power of its antenna is restricted\nto below a certain level under each country's law or the like.  For example, it is restricted to 0.01 W or below in Japan.  Under such restrictions, a millimeter wave radar for onboard use is expected to satisfy the required performance that, for\nexample, its detection range is 200 m or more; the antenna size is 60 mm.times.60 mm or less; its horizontal detection angle is 90 degrees or more; its range resolution is 20 cm or less; it is capable of short-range detection within 10 m; and so on. \nConventional millimeter wave radars have used microstrip lines as waveguides, and patch antennas as antennas (hereinafter, these will both be referred to as \"patch antennas\").  However, with a patch antenna, it has been difficult to attain the\naforementioned performance.\n By using a slot array antenna to which the technique of the present disclosure is applied, the inventors have successfully achieved the aforementioned performance.  As a result, a millimeter wave radar has been realized which is smaller in size,\nmore efficient, and higher-performance than are conventional patch antennas and the like.  In addition, by combining this millimeter wave radar and an optical sensor such as a camera, a small-sized, highly efficient, and high-performance fusion apparatus\nhas been realized which has existed never before.  This will be described in detail below.\n FIG. 32 is a diagram concerning a fusion apparatus in a vehicle 500, the fusion apparatus including an onboard camera system 700 and a radar system 510 (hereinafter referred to also as the millimeter wave radar 510) having a slot array antenna\nto which the technique of the present disclosure is applied.  With reference to this figure, various embodiments will be described below.\n [Installment of Millimeter Wave Radar within Vehicle Room]\n A conventional patch antenna-based millimeter wave radar 510' is placed behind and inward of a grill 512 which is at the front nose of a vehicle.  An electromagnetic wave that is radiated from an antenna goes through the apertures in the grill\n512, and is radiated ahead of the vehicle 500.  In this case, no dielectric layer, e.g., glass, exists that decays or reflects electromagnetic wave energy, in the region through which the electromagnetic wave passes.  As a result, an electromagnetic wave\nthat is radiated from the patch antenna-based millimeter wave radar 510' reaches over a long range, e.g., to a target which is 150 m or farther away.  By receiving with the antenna the electromagnetic wave reflected therefrom, the millimeter wave radar\n510' is able to detect a target.  In this case, however, since the antenna is placed behind and inward of the grill 512 of the vehicle, the radar may be broken when the vehicle collides into an obstacle.  Moreover, it may be soiled with mud or the like\nin rain, etc., and the soil that has adhered to the antenna may hinder radiation and reception of electromagnetic waves.\n Similarly to the conventional manner, the millimeter wave radar 510 incorporating a slot array antenna according to an embodiment of the present disclosure may be placed behind the grill 512, which is located at the front nose of the vehicle\n(not shown).  This allows the energy of the electromagnetic wave to be radiated from the antenna to be utilized by 100%, thus enabling long-range detection beyond the conventional level, e.g., detection of a target which is at a distance of 250 m or\nmore.\n Furthermore, the millimeter wave radar 510 according to an embodiment of the present disclosure can also be placed within the vehicle room, i.e., inside the vehicle.  In that case, the millimeter wave radar 510 is placed inward of the windshield\n511 of the vehicle, to fit in a space between the windshield 511 and a face of the rearview mirror (not shown) that is opposite to its specular surface.  On the other hand, the conventional patch antenna-based millimeter wave radar 510' cannot be placed\ninside the vehicle room mainly for the two following reasons.  A first reason is its large size, which prevents itself from being accommodated within the space between the windshield 511 and the rearview mirror.  A second reason is that an\nelectromagnetic wave that is radiated ahead reflects off the windshield 511 and decays due to dielectric loss, thus becoming unable to travel the desired distance.  As a result, if a conventional patch antenna-based millimeter wave radar is placed within\nthe vehicle room, only targets which are 100 m ahead or less can be detected, for example.  On the other hand, a millimeter wave radar according to an embodiment of the present disclosure is able to detect a target which is at a distance of 200 m or\nmore, despite reflection or decay at the windshield 511.  This performance is equivalent to, or even greater than, the case where a conventional patch antenna-based millimeter wave radar is placed outside the vehicle room.\n [Fusion Construction Based on Millimeter Wave Radar and Camera, Etc., being Placed within Vehicle Room]\n Currently, an optical imaging device such as a CCD camera is used as the main sensor in many a driver assist system (Driver Assist System).  Usually, a camera or the like is placed within the vehicle room, inward of the windshield 511, in order\nto account for unfavorable influences of the external environment, etc. In this context, in order to minimize the optical effect of raindrops and the like, the camera or the like is placed in a region which is swept by the wipers (not shown) but is\ninward of the windshield 511.\n In recent years, due to needs for improved performance of a vehicle in terms of e.g. automatic braking, there has been a desire for automatic braking or the like that is guaranteed to work regardless of whatever external environment may exist. \nIn this case, if the only sensor in the driver assist system is an optical device such as a camera, a problem exists in that reliable operation is not guaranteed in nighttime or bad weather.  This has led to the need for a driver assist system that\nincorporates not only an optical sensor (such as a camera) but also a millimeter wave radar, these being used for cooperative processing, so that reliable operation is achieved even in nighttime or bad weather.\n As described earlier, a millimeter wave radar incorporating the present slot array antenna permits itself to be placed within the vehicle room, due to downsizing and remarkable enhancement in the efficiency of the radiated electromagnetic wave\nover that of a conventional patch antenna.  By taking advantage of these properties, as shown in FIG. 32, the millimeter wave radar 510, which incorporates not only an optical sensor (onboard camera system) 700 such as a camera but also a slot array\nantenna according to the present disclosure, allows both to be placed inward of the windshield 511 of the vehicle 500.  This has created the following novel effects.  (1) It is easier to install the driver assist system on the vehicle 500.  The\nconventional patch antenna-based millimeter wave radar 510' has required a space behind the grill 512, which is at the front nose, in order to accommodate the radar.  Since this space may include some sites that affect the structural design of the\nvehicle, if the size of the radar device is changed, it may have been necessary to reconsider the structural design.  This inconvenience is avoided by placing the millimeter wave radar within the vehicle room.  (2) Free from the influences of rain,\nnighttime, or other external environment factors to the vehicle, more reliable operation can be achieved.  Especially, as shown in FIG. 33, by placing the millimeter wave radar (onboard camera system) 510 and the onboard camera system 700 at\nsubstantially the same position within the vehicle room, they can attain an identical field of view and line of sight, thus facilitating the \"matching process\" which will be described later, i.e., a process through which to establish that respective\npieces of target information captured by them actually come from an identical object.  On the other hand, if the millimeter wave radar 510' were placed behind the grill 512, which is at the front nose outside the vehicle room, its radar line of sight L\nwould differ from a radar line of sight M of the case where it was placed within the vehicle room, thus resulting in a large offset with the image to be acquired by the onboard camera system 700.  (3) Reliability of the millimeter wave radar device is\nimproved.  As described above, since the conventional patch antenna-based millimeter wave radar 510' is placed behind the grill 512, which is at the front nose, it is likely to gather soil, and may be broken even in a minor collision accident or the\nlike.  For these reasons, cleaning and functionality checks are always needed.  Moreover, as will be described below, if the position or direction of attachment of the millimeter wave radar becomes shifted due to an accident or the like, it is necessary\nto reestablish alignment with respect to the camera.  The chances of such occurrences are reduced by placing the millimeter wave radar within the vehicle room, whereby the aforementioned inconveniences are avoided.\n In a driver assist system of such fusion construction, the optical sensor, e.g., a camera, and the millimeter wave radar 510 incorporating the present slot array antenna may have an integrated construction, i.e., being in fixed position with\nrespect to each other.  In that case, certain relative positioning should be kept between the optical axis of the optical sensor such as a camera and the directivity of the antenna of the millimeter wave radar, as will be described later.  When this\ndriver assist system having an integrated construction is fixed within the vehicle room of the vehicle 500, the optical axis of the camera, etc., should be adjusted so as to be oriented in a certain direction ahead of the vehicle.  For these matters, see\nthe specification of US Patent Application Publication No. 2015/0264230, the specification of US Patent Application Publication No. 2016/0264065, U.S.  patent application Ser.  No. 15/248,141, U.S.  patent application Ser.  No. 15/248,149, and U.S. \npatent application Ser.  No. 15/248,156, which are incorporated herein by reference.  Related techniques concerning the camera are described in the specification of U.S.  Pat.  No. 7,355,524, and the specification of U.S.  Pat.  No. 7,420,159, the entire\ndisclosure of each which is incorporated herein by reference.\n Regarding placement of an optical sensor such as a camera and a millimeter wave radar within the vehicle room, see, for example, the specification of U.S.  Pat.  No. 8,604,968, the specification of U.S.  Pat.  No. 8,614,640, and the\nspecification of U.S.  Pat.  No. 7,978,122, the entire disclosure of each which is incorporated herein by reference.  However, at the time when these patents were filed for, only conventional antennas with patch antennas were the known millimeter wave\nradars, and thus observation was not possible over sufficient distances.  For example, the distance that is observable with a conventional millimeter wave radar is considered to be at most 100 m to 150 m. Moreover, when a millimeter wave radar is placed\ninward of the windshield, the large radar size inconveniently blocks the driver's field of view, thus hindering safe driving.  On the other hand, a millimeter wave radar incorporating a slot array antenna according to an embodiment of the present\ndisclosure is capable of being placed within the vehicle room because of its small size and remarkable enhancement in the efficiency of the radiated electromagnetic wave over that of a conventional patch antenna.  This enables a long-range observation\nover 200 m, while not blocking the driver's field of view.\n [Adjustment of Position of Attachment Between Millimeter Wave Radar and Camera, Etc.,]\n In the processing under fusion construction (which hereinafter may be referred to as a \"fusion process\"), it is desired that an image which is obtained with a camera or the like and the radar information which is obtained with the millimeter\nwave radar map onto the same coordinate system because, if they differ as to position and target size, cooperative processing between both will be hindered.\n This involves adjustment from the following three standpoints.\n (1) The optical axis of the camera or the like and the antenna directivity of the millimeter wave radar must have a certain fixed relationship.\n It is required that the optical axis of the camera or the like and the antenna directivity of the millimeter wave radar are matched.  Alternatively, a millimeter wave radar may include two or more transmission antennas and two or more reception\nantennas, the directivities of these antennas being intentionally made different.  Therefore, it is necessary to guarantee that at least a certain known relationship exists between the optical axis of the camera or the like and the directivities of these\nantennas.\n In the case where the camera or the like and the millimeter wave radar have the aforementioned integrated construction, i.e., being in fixed position to each other, the relative positioning between the camera or the like and the millimeter wave\nradar stays fixed.  Therefore, the aforementioned requirements are satisfied with respect to such an integrated construction.  On the other hand, in a conventional patch antenna or the like, where the millimeter wave radar is placed behind the grill 512\nof the vehicle 500, the relative positioning between them is usually to be adjusted according to (2) below.\n (2) A certain fixed relationship exists between an image acquired with the camera or the like and radar information of the millimeter wave radar in an initial state (e.g., upon shipment) of having been attached to the vehicle.\n The positions of attachment of the optical sensor such as a camera and the millimeter wave radar 510 or 510' on the vehicle 500 will finally be determined in the following manner.  At a predetermined position 800 ahead of the vehicle 500, a\nchart to serve as a reference or a target which is subject to observation by the radar (which will hereinafter be referred to as, respectively, a \"reference chart\" and a \"reference target\", and collectively as the \"benchmark\") is accurately positioned. \nThis is observed with an optical sensor such as a camera or with the millimeter wave radar 510.  The observation information regarding the observed benchmark is compared against previously-stored shape information or the like of the benchmark, and the\ncurrent offset information is quantitated.  Based on this offset information, by at least one of the following means, the positions of attachment of an optical sensor such as a camera and the millimeter wave radar 510 or 510' are adjusted or corrected. \nAny other means may also be employed that can provide similar results.\n (i) Adjust the positions of attachment of the camera and the millimeter wave radar so that the benchmark will come at a midpoint between the camera and the millimeter wave radar.  This adjustment may be done by using a jig or tool, etc., which\nis separately provided.\n (ii) Determine an offset amounts of the camera and the axis/directivity of the millimeter wave radar relative to the benchmark, and through image processing of the camera image and radar processing, correct for these offset amounts in the\naxis/directivity.\n What is to be noted is that, in the case where the optical sensor such as a camera and the millimeter wave radar 510 incorporating a slot array antenna according to an embodiment of the present disclosure have an integrated construction, i.e.,\nbeing in fixed position to each other, adjusting an offset of either the camera or the radar with respect to the benchmark will make the offset amount known for the other as well, thus making it unnecessary to check for the other's offset with respect to\nthe benchmark.\n Specifically, with respect to the onboard camera system 700, a reference chart may be placed at a predetermined position 750, and an image taken by the camera is compared against advance information indicating where in the field of view of the\ncamera the reference chart image is supposed to be located, thereby detecting an offset amount.  Based on this, the camera is adjusted by at least one of the above means (i) and (ii).  Next, the offset amount which has been ascertained for the camera is\ntranslated into an offset amount of the millimeter wave radar.  Thereafter, an offset amount adjustment is made with respect to the radar information, by at least one of the above means (i) and (ii).\n Alternatively, this may be performed on the basis of the millimeter wave radar 510.  In other words, with respect to the millimeter wave radar 510, a reference target may be placed at a predetermined position 800, and the radar information\nthereof is compared against advance information indicating where in the field of view of the millimeter wave radar 510 the reference target is supposed to be located, thereby detecting an offset amount.  Based on this, the millimeter wave radar 510 is\nadjusted by at least one of the above means (i) and (ii).  Next, the offset amount which has been ascertained for the millimeter wave radar is translated into an offset amount of the camera.  Thereafter, an offset amount adjustment is made with respect\nto the image information obtained by the camera, by at least one of the above means (i) and (ii).\n (3) Even after an initial state of the vehicle, a certain relationship is maintained between an image acquired with the camera or the like and radar information of the millimeter wave radar.\n Usually, an image acquired with the camera or the like and radar information of the millimeter wave radar are supposed to be fixed in the initial state, and hardly vary unless in an accident of the vehicle or the like.  However, if an offset in\nfact occurs between these, an adjustment is possible by the following means.\n The camera is attached in such a manner that portions 513 and 514 (characteristic points) that are characteristic of the driver's vehicle fit within its field of view, for example.  The positions at which these characteristic points are actually\nimaged by the camera are compared against the information of the positions to be assumed by these characteristic points when the camera is attached accurately in place, and an offset amount(s) is detected therebetween.  Based on this detected offset\namount(s), the position of any image that is taken thereafter may be corrected, whereby an offset of the physical position of attachment of the camera can be corrected for.  If this correction sufficiently embodies the performance that is required of the\nvehicle, then the adjustment per the above (2) may not be needed.  By regularly performing this adjustment during startup or operation of the vehicle 500, even if an offset of the camera or the like occurs anew, it is possible to correct for the offset\namount, thus helping safe travel.\n However, this means is generally considered to result in poorer accuracy of adjustment than with the above means (2).  When making an adjustment based on an image which is obtained by imaging a benchmark with the camera, the azimuth of the\nbenchmark can be determined with a high precision, whereby a high accuracy of adjustment can be easily achieved.  However, since this means utilizes a part of the vehicle body for the adjustment instead of a benchmark, it is rather difficult to enhance\nthe accuracy of azimuth determination.  Thus, the resultant accuracy of adjustment will be somewhat inferior.  However, it may still be effective as a means of correction when the position of attachment of the camera or the like is considerably altered\nfor reasons such as an accident or a large external force being applied to the camera or the like within the vehicle room, etc.\n [Mapping of Target as Detected by Millimeter Wave Radar and Camera or the Like: Matching Process]\n In a fusion process, for a given target, it needs to be established that an image thereof which is acquired with a camera or the like and radar information which is acquired with the millimeter wave radar pertain to \"the same target\".  For\nexample, suppose that two obstacles (first and second obstacles), e.g., two bicycles, have appeared ahead of the vehicle 500.  These two obstacles will be captured as camera images, and detected as radar information of the millimeter wave radar.  At this\ntime, the camera image and the radar information with respect to the first obstacle need to be mapped to each other so that they are both directed to the same target.  Similarly, the camera image and the radar information with respect to the second\nobstacle need to be mapped to each other so that they are both directed to the same target.  If the camera image of the first obstacle and the radar information of the second obstacle are mistakenly recognized to pertain to an identical object, a\nconsiderable accident may occur.  Hereinafter, in the present specification, such a process of determining whether a target in the camera image and a target in the radar image pertain to the same target may be referred to as a \"matching process\".\n This matching process may be implemented by various detection devices (or methods) described below.  Hereinafter, these will be specifically described.  Note that the each of the following detection devices is to be installed in the vehicle, and\nat least includes a millimeter wave radar detection section, an image detection section (e.g., a camera) which is oriented in a direction overlapping the direction of detection by the millimeter wave radar detection section, and a matching section. \nHerein, the millimeter wave radar detection section includes a slot array antenna according to any of the embodiments of the present disclosure, and at least acquires radar information in its own field of view.  The image acquisition section at least\nacquires image information in its own field of view.  The matching section includes a processing circuit which matches a result of detection by the millimeter wave radar detection section against a result of detection by the image detection section to\ndetermine whether or not the same target is being detected by the two detection sections.  Herein, the image detection section may be composed of a selected one of, or selected two or more of, an optical camera, LIDAR, an infrared radar, and an\nultrasonic radar.  The following detection devices differ from one another in terms of the detection process at their respective matching section.\n In a first detection device, the matching section performs two matches as follows.  A first match involves, for a target of interest that has been detected by the millimeter wave radar detection section, obtaining distance information and\nlateral position information thereof, and also finding a target that is the closest to the target of interest among a target or two or more targets detected by the image detection section, and detecting a combination(s) thereof.  A second match involves,\nfor a target of interest that has been detected by the image detection section, obtaining distance information and lateral position information thereof, and also finding a target that is the closest to the target of interest among a target or two or more\ntargets detected by the millimeter wave radar detection section, and detecting a combination(s) thereof.  Furthermore, this matching section determines whether there is any matching combination between the combination(s) of such targets as detected by\nthe millimeter wave radar detection section and the combination(s) of such targets as detected by the image detection section.  Then, if there is any matching combination, it is determined that the same object is being detected by the two detection\nsections.  In this manner, a match is attained between the respective targets that have been detected by the millimeter wave radar detection section and the image detection section.\n A related technique is described in the specification of U.S.  Pat.  No. 7,358,889, the entire disclosure of which is incorporated herein by reference.  In this publication, the image detection section is illustrated by way of a so-called stereo\ncamera that includes two cameras.  However, this technique is not limited thereto.  In the case where the image detection section includes a single camera, detected targets may be subjected to an image recognition process or the like as appropriate, in\norder to obtain distance information and lateral position information of the targets.  Similarly, a laser sensor such as a laser scanner may be used as the image detection section.\n In a second detection device, the matching section matches a result of detection by the millimeter wave radar detection section and a result of detection by the image detection section every predetermined period of time.  If the matching section\ndetermines that the same target was being detected by the two detection sections in the previous result of matching, it performs a match by using this previous result of matching.  Specifically, the matching section matches a target which is currently\ndetected by the millimeter wave radar detection section and a target which is currently detected by the image detection section, against the target which was determined in the previous result of matching to be being detected by the two detection\nsections.  Then, based on the result of matching for the target which is currently detected by the millimeter wave radar detection section and the result of matching for the target which is currently detected by the image detection section, the matching\nsection determines whether or not the same target is being detected by the two detection sections.  Thus, rather than directly matching the results of detection by the two detection sections, this detection device performs a chronological match between\nthe two results of detection and a previous result of matching.  Therefore, the accuracy of detection is improved over the case of only performing a momentary match, whereby stable matching is realized.  In particular, even if the accuracy of the\ndetection section drops momentarily, matching is still possible because of utilizing past results of matching.  Moreover, by utilizing the previous result of matching, this detection device is able to easily perform a match between the two detection\nsections.\n In the current match which utilizes the previous result of matching, if the matching section of this detection device determines that the same object is being detected by the two detection sections, then the matching section of this detection\ndevice excludes this determined object in performing matching between objects which are currently detected by the millimeter wave radar detection section and objects which are currently detected by the image detection section.  Then, this matching\nsection determines whether there exists any identical object that is currently detected by the two detection sections.  Thus, while taking into account the result of chronological matching, the detection device also makes a momentary match based on two\nresults of detection that are obtained from moment to moment.  As a result, the detection device is able to surely perform a match for any object that is detected during the current detection.\n A related technique is described in the specification of U.S.  Pat.  No. 7,417,580, the entire disclosure of which is incorporated herein by reference.  In this publication, the image detection section is illustrated by way of a so-called stereo\ncamera that includes two cameras.  However, this technique is not limited thereto.  In the case where the image detection section includes a single camera, detected targets may be subjected to an image recognition process or the like as appropriate, in\norder to obtain distance information and lateral position information of the targets.  Similarly, a laser sensor such as a laser scanner may be used as the image detection section.\n In a third detection device, the two detection sections and matching section perform detection of targets and performs matches therebetween at predetermined time intervals, and the results of such detection and the results of such matching are\nchronologically stored to a storage medium, e.g., memory.  Then, based on a rate of change in the size of a target in the image as detected by the image detection section, and on a distance to a target from the driver's vehicle and its rate of change\n(relative velocity with respect to the driver's vehicle) as detected by the millimeter wave radar detection section, the matching section determines whether the target which has been detected by the image detection section and the target which has been\ndetected by the millimeter wave radar detection section are an identical object.\n When determining that these targets are an identical object, based on the position of the target in the image as detected by the image detection section, and on the distance to the target from the driver's vehicle and/or its rate of change as\ndetected by the millimeter wave radar detection section, the matching section predicts a possibility of collision with the vehicle.\n A related technique is described in the specification of U.S.  Pat.  No. 6,903,677, the entire disclosure of which is incorporated herein by reference.\n As described above, in a fusion process of a millimeter wave radar and an imaging device such as a camera, an image which is obtained with the camera or the like and radar information which is obtained with the millimeter wave radar are matched\nagainst each other.  A millimeter wave radar incorporating the aforementioned array antenna according to an embodiment of the present disclosure can be constructed so as to have a small size and high performance.  Therefore, high performance and\ndownsizing, etc., can be achieved for the entire fusion process including the aforementioned matching process.  This improves the accuracy of target recognition, and enables safer travel control for the vehicle.\n [Other Fusion Processes]\n In a fusion process, various functions are realized based on a matching process between an image which is obtained with a camera or the like and radar information which is obtained with the millimeter wave radar detection section.  Examples of\nprocessing apparatuses that realize representative functions of a fusion process will be described below.\n Each of the following processing apparatuses is to be installed in a vehicle, and at least includes: a millimeter wave radar detection section to transmit or receive electromagnetic waves in a predetermined direction; an image acquisition\nsection, such as a monocular camera, that has a field of view overlapping the field of view of the millimeter wave radar detection section; and a processing section which obtains information therefrom to perform target detection and the like.  The\nmillimeter wave radar detection section acquires radar information in its own field of view.  The image acquisition section acquires image information in its own field of view.  A selected one, or selected two or more of, an optical camera, LIDAR, an\ninfrared radar, and an ultrasonic radar may be used as the image acquisition section.  The processing section can be implemented by a processing circuit which is connected to the millimeter wave radar detection section and the image acquisition section. \nThe following processing apparatuses differ from one another with respect to the content of processing by this processing section.\n In a first processing apparatus, the processing section extracts, from an image which is captured by the image acquisition section, a target which is recognized to be the same as the target which is detected by the millimeter wave radar\ndetection section.  In other words, a matching process according to the aforementioned detection device is performed.  Then, it acquires information of a right edge and a left edge of the extracted target image, and derives locus approximation lines,\nwhich are straight lines or predetermined curved lines for approximating loci of the acquired right edge and the left edge, are derived for both edges.  The edge which has a larger number of edges existing on the locus approximation line is selected as a\ntrue edge of the target.  The lateral position of the target is derived on the basis of the position of the edge that has been selected as a true edge.  This permits a further improvement on the accuracy of detection of a lateral position of the target.\n A related technique is described in the specification of U.S.  Pat.  No. 8,610,620, the entire disclosure of which is incorporated herein by reference.\n In a second processing apparatus, in determining the presence of a target, the processing section alters a determination threshold to be used in checking for a target presence in radar information, on the basis of image information.  Thus, if a\ntarget image that may be an obstacle to vehicle travel has been confirmed with a camera or the like, or if the presence of a target has been estimated, etc., for example, the determination threshold for the target detection by the millimeter wave radar\ndetection section can be optimized so that more accurate target information can be obtained.  In other words, if the possibility of the presence of an obstacle is high, the determination threshold is altered so that this processing apparatus will surely\nbe activated.  On the other hand, if the possibility of the presence of an obstacle is low, the determination threshold is altered so that unwanted activation of this processing apparatus is prevented.  This permits appropriate activation of the system.\n Furthermore in this case, based on radar information, the processing section may designate a region of detection for the image information, and estimate a possibility of the presence of an obstacle on the basis of image information within this\nregion.  This makes for a more efficient detection process.\n A related technique is described in the specification of U.S.  Pat.  No. 7,570,198, the entire disclosure of which is incorporated herein by reference.\n In a third processing apparatus, the processing section performs combined displaying where images obtained from a plurality of different imaging devices and a millimeter wave radar detection section and an image signal based on radar information\nare displayed on at least one display device.  In this displaying process, horizontal and vertical synchronizing signals are synchronized between the plurality of imaging devices and the millimeter wave radar detection section, and among the image\nsignals from these devices, selective switching to a desired image signal is possible within one horizontal scanning period or one vertical scanning period.  This allows, on the basis of the horizontal and vertical synchronizing signals, images of a\nplurality of selected image signals to be displayed side by side; and, from the display device, a control signal for setting a control operation in the desired imaging device and the millimeter wave radar detection section is sent.\n When a plurality of different display devices display respective images or the like, it is difficult to compare the respective images against one another.  Moreover, when display devices are provided separately from the third processing\napparatus itself, there is poor operability for the device.  The third processing apparatus would overcome such shortcomings.\n A related technique is described in the specification of U.S.  Pat.  No. 6,628,299 and the specification of U.S.  Pat.  No. 7,161,561, the entire disclosure of each of which is incorporated herein by reference.\n In a fourth processing apparatus, with respect to a target which is ahead of a vehicle, the processing section instructs an image acquisition section and a millimeter wave radar detection section to acquire an image and radar information\ncontaining that target.  From within such image information, the processing section determines a region in which the target is contained.  Furthermore, the processing section extracts radar information within this region, and detects a distance from the\nvehicle to the target and a relative velocity between the vehicle and the target.  Based on such information, the processing section determines a possibility that the target will collide against the vehicle.  This enables an early detection of a possible\ncollision with a target.\n A related technique is described in the specification of U.S.  Pat.  No. 8,068,134, the entire disclosure of which is incorporated herein by reference.\n In a fifth processing apparatus, based on radar information or through a fusion process which is based on radar information and image information, the processing section recognizes a target or two or more targets ahead of the vehicle.  The\n\"target\" encompasses any moving entity such as other vehicles or pedestrians, traveling lanes indicated by white lines on the road, road shoulders and any still objects (including gutters, obstacles, etc.), traffic lights, pedestrian crossings, and the\nlike that may be there.  The processing section may encompass a GPS (Global Positioning System) antenna.  By using a GPS antenna, the position of the driver's vehicle may be detected, and based on this position, a storage device (referred to as a map\ninformation database device) that stores road map information may be searched in order to ascertain a current position on the map.  This current position on the map may be compared against a target or two or more targets that have been recognized based\non radar information or the like, whereby the traveling environment may be recognized.  On this basis, the processing section may extract any target that is estimated to hinder vehicle travel, find safer traveling information, and display it on a display\ndevice, as necessary, to inform the driver.\n A related technique is described in the specification of U.S.  Pat.  No. 6,191,704, the entire disclosure of which is incorporated herein by reference.\n The fifth processing apparatus may further include a data communication device (having communication circuitry) that communicates with a map information database device which is external to the vehicle.  The data communication device may access\nthe map information database device, with a period of e.g. once a week or once a month, to download the latest map information therefrom.  This allows the aforementioned processing to be performed with the latest map information.\n Furthermore, the fifth processing apparatus may compare between the latest map information that was acquired during the aforementioned vehicle travel and information that is recognized of a target or two or more targets based on radar\ninformation, etc., in order to extract target information (hereinafter referred to as \"map update information\") that is not included in the map information.  Then, this map update information may be transmitted to the map information database device via\nthe data communication device.  The map information database device may store this map update information in association with the map information that is within the database, and update the current map information itself, if necessary.  In performing the\nupdate, respective pieces of map update information that are obtained from a plurality of vehicles may be compared against one another to check certainty of the update.\n Note that this map update information may contain more detailed information than the map information which is carried by any currently available map information database device.  For example, schematic shapes of roads may be known from\ncommonly-available map information, but it typically does not contain information such as the width of the road shoulder, the width of the gutter that may be there, any newly occurring bumps or dents, shapes of buildings, and so on.  Neither does it\ncontain heights of the roadway and the sidewalk, how a slope may connect to the sidewalk, etc. Based on conditions which are separately set, the map information database device may store such detailed information (hereinafter referred to as \"map update\ndetails information\") in association with the map information.  Such map update details information provides a vehicle (including the driver's vehicle) with information which is more detailed than the original map information, thereby rending itself\navailable for not only the purpose of ensuring safe vehicle travel but also some other purposes.  As used herein, a \"vehicle (including the driver's vehicle)\" may be e.g. an automobile, a motorcycle, a bicycle, or any autonomous vehicle to become\navailable in the future, e.g., an electric wheelchair.  The map update details information is to be used when any such vehicle may travel.\n (Recognition Via Neural Network)\n Each of the first to fifth processing apparatuses may further include a sophisticated apparatus of recognition.  The sophisticated apparatus of recognition may be provided external to the vehicle.  In that case, the vehicle may include a\nhigh-speed data communication device that communicates with the sophisticated apparatus of recognition.  The sophisticated apparatus of recognition may be constructed from a neural network, which may encompass so-called deep learning and the like.  This\nneural network may include a convolutional neural network (hereinafter referred to as \"CNN\"), for example.  A CNN, a neural network that has proven successful in image recognition, is characterized by possessing one or more sets of two layers, namely, a\nconvolutional layer and a pooling layer.\n There exists at least three kinds of information as follows, any of which may be input to a convolutional layer in the processing apparatus:\n (1) information that is based on radar information which is acquired by the millimeter wave radar detection section;\n (2) information that is based on specific image information which is acquired, based on radar information, by the image acquisition section; or\n (3) fusion information that is based on radar information and image information which is acquired by the image acquisition section, or information that is obtained based on such fusion information.\n Based on information of any of the above kinds, or information based on a combination thereof, product-sum operations corresponding to a convolutional layer are performed.  The results are input to the subsequent pooling layer, where data is\nselected according to a predetermined rule.  In the case of max pooling where a maximum value among pixel values is chosen, for example, the rule may dictate that a maximum value be chosen for each split region in the convolutional layer, this maximum\nvalue being regarded as the value of the corresponding position in the pooling layer.\n A sophisticated apparatus of recognition that is composed of a CNN may include a single set of a convolutional layer and a pooling layer, or a plurality of such sets which are cascaded in series.  This enables accurate recognition of a target,\nwhich is contained in the radar information and the image information, that may be around a vehicle.\n Related techniques are described in the U.S.  Pat.  No. 8,861,842, the specification of U.S.  Pat.  No. 9,286,524, and the specification of US Patent Application Publication No. 2016/0140424, the entire disclosure of each of which is\nincorporated herein by reference.\n In a sixth processing apparatus, the processing section performs processing that is related to headlamp control of a vehicle.  When a vehicle travels in nighttime, the driver may check whether another vehicle or a pedestrian exists ahead of the\ndriver's vehicle, and control a beam(s) from the headlamp(s) of the driver's vehicle to prevent the driver of the other vehicle or the pedestrian from being dazzled by the headlamp(s) of the driver's vehicle.  This sixth processing apparatus\nautomatically controls the headlamp(s) of the driver's vehicle by using radar information, or a combination of radar information and an image taken by a camera or the like.\n Based on radar information, or through a fusion process based on radar information and image information, the processing section detects a target that corresponds to a vehicle or pedestrian ahead of the vehicle.  In this case, a vehicle ahead of\na vehicle may encompass a preceding vehicle that is ahead, a vehicle or a motorcycle in the oncoming lane, and so on.  When detecting any such target, the processing section issues a command to lower the beam(s) of the headlamp(s).  Upon receiving this\ncommand, the control section (control circuit) which is internal to the vehicle may control the headlamp(s) to lower the beam(s) therefrom.\n Related techniques are described in the specification of U.S.  Pat.  No. 6,403,942, the specification of U.S.  Pat.  No. 6,611,610, the specification of U.S.  Pat.  No. 8,543,277, the specification of U.S.  Pat.  No. 8,593,521, and the\nspecification of U.S.  Pat.  No. 8,636,393, the entire disclosure of each of which is incorporated herein by reference.\n According to the above-described processing by the millimeter wave radar detection section, and the above-described fusion process by the millimeter wave radar detection section and an imaging device such as a camera, the millimeter wave radar\ncan be constructed so as to have a small size and high performance, whereby high performance and downsizing, etc., can be achieved for the radar processing or the entire fusion process.  This improves the accuracy of target recognition, and enables safer\ntravel control for the vehicle.\nApplication Example 2: Various Monitoring Systems (Natural Elements, Buildings, Roads, Watch, Security)\n A millimeter wave radar (radar system) incorporating an array antenna according to an embodiment of the present disclosure also has a wide range of applications in the fields of monitoring, which may encompass natural elements, weather,\nbuildings, security, nursing care, and the like.  In a monitoring system in this context, a monitoring apparatus that includes the millimeter wave radar may be installed e.g. at a fixed position, in order to perpetually monitor a subject(s) of\nmonitoring.  Regarding the given subject(s) of monitoring, the millimeter wave radar has its resolution of detection adjusted and set to an optimum value.\n A millimeter wave radar incorporating an array antenna according to an embodiment of the present disclosure is capable of detection with a radio frequency electromagnetic wave exceeding e.g. 100 GHz.  As for the modulation band in those schemes\nwhich are used in radar recognition, e.g., the FMCW method, the millimeter wave radar currently achieves a wide band exceeding 4 GHz, which supports the aforementioned Ultra Wide Band (UWB).  Note that the modulation band is related to the range\nresolution.  In a conventional patch antenna, the modulation band was up to about 600 MHz, thus resulting in a range resolution of 25 cm.  On the other hand, a millimeter wave radar associated with the present array antenna has a range resolution of 3.75\ncm, indicative of a performance which rivals the range resolution of conventional LIDAR.  Whereas an optical sensor such as LIDAR is unable to detect a target in nighttime or bad weather as mentioned above, a millimeter wave radar is always capable of\ndetection, regardless of daytime or nighttime and irrespective of weather.  As a result, a millimeter wave radar associated with the present array antenna is available for a variety of applications which were not possible with a millimeter wave radar\nincorporating any conventional patch antenna.\n FIG. 34 is a diagram showing an exemplary construction for a monitoring system 1500 based on millimeter wave radar.  The monitoring system 1500 based on millimeter wave radar at least includes a sensor section 1010 and a main section 1100.  The\nsensor section 1010 at least includes an antenna 1011 which is aimed at the subject of monitoring 1015, a millimeter wave radar detection section 1012 which detects a target based on a transmitted or received electromagnetic wave, and a communication\nsection (communication circuit) 1013 which transmits detected radar information.  The main section 1100 at least includes a communication section (communication circuit) 1103 which receives radar information, a processing section (processing circuit)\n1101 which performs predetermined processing based on the received radar information, and a data storage section (storage medium) 1102 in which past radar information and other information that is needed for the predetermined processing, etc., are\nstored.  Telecommunication lines 1300 exist between the sensor section 1010 and the main section 1100, via which transmission and reception of information and commands occur between them.  As used herein, the telecommunication lines may encompass any of\na general-purpose communications network such as the Internet, a mobile communications network, dedicated telecommunication lines, and so on, for example.  Note that the present monitoring system 1500 may be arranged so that the sensor section 1010 and\nthe main section 1100 are directly connected, rather than via telecommunication lines.  In addition to the millimeter wave radar, the sensor section 1010 may also include an optical sensor such as a camera.  This will permit target recognition through a\nfusion process which is based on radar information and image information from the camera or the like, thus enabling a more sophisticated detection of the subject of monitoring 1015 or the like.\n Hereinafter, examples of monitoring systems embodying these applications will be specifically described.\n [Natural Element Monitoring System]\n A first monitoring system is a system that monitors natural elements (hereinafter referred to as a \"natural element monitoring system\").  With reference to FIG. 34, this natural element monitoring system will be described.  Subjects of\nmonitoring 1015 of the natural element monitoring system 1500 may be, for example, a river, the sea surface, a mountain, a volcano, the ground surface, or the like.  For example, when a river is the subject of monitoring 1015, the sensor section 1010\nbeing secured to a fixed position perpetually monitors the water surface of the river 1015.  This water surface information is perpetually transmitted to a processing section 1101 in the main section 1100.  Then, if the water surface reaches a certain\nheight or above, the processing section 1101 informs a distinct system 1200 which separately exists from the monitoring system (e.g., a weather observation monitoring system), via the telecommunication lines 1300.  Alternatively, the processing section\n1101 may send information to a system (not shown) which manages the water gate, whereby the system if instructed to automatically close a water gate, etc. (not shown) which is provided at the river 1015.\n The natural element monitoring system 1500 is able to monitor a plurality of sensor sections 1010, 1020, etc., with the single main section 1100.  When the plurality of sensor sections are distributed over a certain area, the water levels of\nrivers in that area can be grasped simultaneously.  This allows to make an assessment as to how the rainfall in this area may affect the water levels of the rivers, possibly leading to disasters such as floods.  Information concerning this can be\nconveyed to the distinct system 1200 (e.g., a weather observation monitoring system) via the telecommunication lines 1300.  Thus, the distinct system 1200 (e.g., a weather observation monitoring system) is able to utilize the conveyed information for\nweather observation or disaster prediction in a wider area.\n The natural element monitoring system 1500 is also similarly applicable to any natural element other than a river.  For example, the subject of monitoring of a monitoring system that monitors tsunamis or storm surges is the sea surface level. \nIt is also possible to automatically open or close the water gate of a seawall in response to a rise in the sea surface level.  Alternatively, the subject of monitoring of a monitoring system that monitors landslides to be caused by rainfall,\nearthquakes, or the like may be the ground surface of a mountainous area, etc.\n [Traffic Monitoring System]\n A second monitoring system is a system that monitors traffic (hereinafter referred to as a \"traffic monitoring system\").  The subject of monitoring of this traffic monitoring system may be, for example, a railroad crossing, a specific railroad,\nan airport runway, a road intersection, a specific road, a parking lot, etc.\n For example, when the subject of monitoring is a railroad crossing, the sensor section 1010 is placed at a position where the inside of the crossing can be monitored.  In this case, in addition to the millimeter wave radar, the sensor section\n1010 may also include an optical sensor such as a camera, which will allow a target (subject of monitoring) to be detected from more perspectives, through a fusion process based on radar information and image information.  The target information which is\nobtained with the sensor section 1010 is sent to the main section 1100 via the telecommunication lines 1300.  The main section 1100 collects other information (e.g., train schedule information) that may be needed in a more sophisticated recognition\nprocess or control, and issues necessary control instructions or the like based thereon.  As used herein, a necessary control instruction may be, for example, an instruction to stop a train when a person, a vehicle, etc. is found inside the crossing when\nit is closed.\n If the subject of monitoring is a runway at an airport, for example, a plurality of sensor sections 1010, 1020, etc., may be placed along the runway so as to set the runway to a predetermined resolution, e.g., a resolution that allows any\nforeign object on the runway that is 5 cm by 5 cm or larger to be detected.  The monitoring system 1500 perpetually monitors the runway, regardless of daytime or nighttime and irrespective of weather.  This function is enabled by the very ability of the\nmillimeter wave radar according to an embodiment of the present disclosure to support UWB.  Moreover, since the present millimeter wave radar device can be embodied with a small size, a high resolution, and a low cost, it provides a realistic solution\nfor covering the entire runway surface from end to end.  In this case, the main section 1100 keeps the plurality of sensor sections 1010, 1020, etc., under integrated management.  If a foreign object is found on the runway, the main section 1100\ntransmits information concerning the position and size of the foreign object to an air-traffic control system (not shown).  Upon receiving this, the air-traffic control system temporarily prohibits takeoff and landing on that runway.  In the meantime,\nthe main section 1100 transmits information concerning the position and size of the foreign object to a separately-provided vehicle, which automatically cleans the runway surface, etc., for example.  Upon receive this, the cleaning vehicle may\nautonomously move to the position where the foreign object exists, and automatically remove the foreign object.  Once removal of the foreign object is completed, the cleaning vehicle transmits information of the completion to the main section 1100. \nThen, the main section 1100 again confirms that the sensor section 1010 or the like which has detected the foreign object now reports that \"no foreign object exists\" and that it is safe now, and informs the air-traffic control system of this.  Upon\nreceiving this, the air-traffic control system may lift the prohibition of takeoff and landing from the runway.\n Furthermore, in the case where the subject of monitoring is a parking lot, for example, it may be possible to automatically recognize which position in the parking lot is currently vacant.  A related technique is described in the specification\nof U.S.  Pat.  No. 6,943,726, the entire disclosure of which is incorporated herein by reference.\n [Security Monitoring System]\n A third monitoring system is a system that monitors a trespasser into a piece of private land or a house (hereinafter referred to as a \"security monitoring system\").  The subject of monitoring of this security monitoring system may be, for\nexample, a specific region within a piece of private land or a house, etc.\n For example, if the subject of monitoring is a piece of private land, the sensor section(s) 1010 may be placed at one position, or two or more positions where the sensor section(s) 1010 is able to monitor it.  In this case, in addition to the\nmillimeter wave radar, the sensor section(s) 1010 may also include an optical sensor such as a camera, which will allow a target (subject of monitoring) to be detected from more perspectives, through a fusion process based on radar information and image\ninformation.  The target information which was obtained by the sensor section 1010(s) is sent to the main section 1100 via the telecommunication lines 1300.  The main section 1100 collects other information (e.g., reference data or the like needed to\naccurately recognize whether the trespasser is a person or an animal such as a dog or a bird) that may be needed in a more sophisticated recognition process or control, and issues necessary control instructions or the like based thereon.  As used herein,\na necessary control instruction may be, for example, an instruction to sound an alarm or activate lighting that is installed in the premises, and also an instruction to directly report to a person in charge of the premises via mobile telecommunication\nlines or the like, etc. The processing section 1101 in the main section 1100 may allow an internalized, sophisticated apparatus of recognition (that adopts deep learning or a like technique) to recognize the detected target.  Alternatively, such a\nsophisticated apparatus of recognition may be provided externally, in which case the sophisticated apparatus of recognition may be connected via the telecommunication lines 1300.\n A related technique is described in the specification of U.S.  Pat.  No. 7,425,983, the entire disclosure of which is incorporated herein by reference.\n Another embodiment of such a security monitoring system may be a human monitoring system to be installed at a boarding gate at an airport, a station wicket, an entrance of a building, or the like.  The subject of monitoring of such a human\nmonitoring system may be, for example, a boarding gate at an airport, a station wicket, an entrance of a building, or the like.\n If the subject of monitoring is a boarding gate at an airport, the sensor section(s) 1010 may be installed in a machine for checking personal belongings at the boarding gate, for example.  In this case, there may be two checking methods as\nfollows.  In a first method, the millimeter wave radar transmits an electromagnetic wave, and receives the electromagnetic wave as it reflects off a passenger (which is the subject of monitoring), thereby checking personal belongings or the like of the\npassenger.  In a second method, a weak millimeter wave which is radiated from the passenger's own body is received by the antenna, thus checking for any foreign object that the passenger may be hiding.  In the latter method, the millimeter wave radar\npreferably has a function of scanning the received millimeter wave.  This scanning function may be implemented by using digital beam forming, or through a mechanical scanning operation.  Note that the processing by the main section 1100 may utilize a\ncommunication process and a recognition process similar to those in the above-described examples.\n [Building Inspection System (Non-Destructive Inspection)]\n A fourth monitoring system is a system that monitors or checks the concrete material of a road, a railroad overpass, a building, etc., or the interior of a road or the ground, etc., (hereinafter referred to as a \"building inspection system\"). \nThe subject of monitoring of this building inspection system may be, for example, the interior of the concrete material of an overpass or a building, etc., or the interior of a road or the ground, etc.\n For example, if the subject of monitoring is the interior of a concrete building, the sensor section 1010 is structured so that the antenna 1011 can make scan motions along the surface of a concrete building.  As used herein, \"scan motions\" may\nbe implemented manually, or a stationary rail for the scan motion may be separately provided, upon which to cause the movement by using driving power from an electric motor or the like.  In the case where the subject of monitoring is a road or the\nground, the antenna 1011 may be installed face-down on a vehicle or the like, and the vehicle may be allowed to travel at a constant velocity, thus creating a \"scan motion\".  The electromagnetic wave to be used by the sensor section 1010 may be a\nmillimeter wave in e.g. the so-called terahertz region, exceeding 100 GHz.  As described earlier, even with an electromagnetic wave over e.g. 100 GHz, an array antenna according to an embodiment of the present disclosure can be adapted to have smaller\nlosses than do conventional patch antennas or the like.  An electromagnetic wave of a higher frequency is able to permeate deeper into the subject of checking, such as concrete, thereby realizing a more accurate non-destructive inspection.  Note that the\nprocessing by the main section 1100 may also utilize a communication process and a recognition process similar to those in the other monitoring systems described above.\n A related technique is described in the specification of U.S.  Pat.  No. 6,661,367, the entire disclosure of which is incorporated herein by reference.\n [Human Monitoring System]\n A fifth monitoring system is a system that watches over a person who is subject to nursing care (hereinafter referred to as a \"human watch system\").  The subject of monitoring of this human watch system may be, for example, a person under\nnursing care or a patient in a hospital, etc.\n For example, if the subject of monitoring is a person under nursing care within a room of a nursing care facility, the sensor section(s) 1010 is placed at one position, or two or more positions inside the room where the sensor section(s) 1010 is\nable to monitor the entirety of the inside of the room.  In this case, in addition to the millimeter wave radar, the sensor section 1010 may also include an optical sensor such as a camera.  In this case, the subject of monitoring can be monitored from\nmore perspectives, through a fusion process based on radar information and image information.  On the other hand, when the subject of monitoring is a person, from the standpoint of privacy protection, monitoring with a camera or the like may not be\nappropriate.  Therefore, sensor selections must be made while taking this aspect into consideration.  Note that target detection by the millimeter wave radar will allow a person, who is the subject of monitoring, to be captured not by his or her image,\nbut by a signal (which is, as it were, a shadow of the person).  Therefore, the millimeter wave radar may be considered as a desirable sensor from the standpoint of privacy protection.\n Information of the person under nursing care which has been obtained by the sensor section(s) 1010 is sent to the main section 1100 via the telecommunication lines 1300.  The main section 1100 collects other information (e.g., reference data or\nthe like needed to accurately recognize target information of the person under nursing care) that may be needed in a more sophisticated recognition process or control, and issues necessary control instructions or the like based thereon.  As used herein,\na necessary control instruction may be, for example, an instruction to directly report a person in charge based on the result of detection, etc. The processing section 1101 in the main section 1100 may allow an internalized, sophisticated apparatus of\nrecognition (that adopts deep learning or a like technique) to recognize the detected target.  Alternatively, such a sophisticated apparatus of recognition may be provided externally, in which case the sophisticated apparatus of recognition may be\nconnected via the telecommunication lines 1300.\n In the case where a person is the subject of monitoring of the millimeter wave radar, at least the two following functions may be added.\n A first function is a function of monitoring the heart rate and/or the respiratory rate.  In the case of a millimeter wave radar, an electromagnetic wave is able to see through the clothes to detect the position and motions of the skin surface\nof a person's body.  First, the processing section 1101 detects a person who is the subject of monitoring and an outer shape thereof.  Next, in the case of detecting a heart rate, for example, a position on the body surface where the heartbeat motions\nare easy to detect may be identified, and the motions there may be chronologically detected.  This allows a heart rate per minute to be detected, for example.  The same is also true when detecting a respiratory rate.  By using this function, the health\nstatus of a person under nursing care can be perpetually checked, thus enabling a higher-quality watch over a person under nursing care.\n A second function is a function of fall detection.  A person under nursing care such as an elderly person may fall from time to time, due to weakened legs and feet.  When a person falls, the velocity or acceleration of a specific site of the\nperson's body, e.g., the head, will reach a certain level or greater.  When the subject of monitoring of the millimeter wave radar is a person, the relative velocity or acceleration of the target of interest can be perpetually detected.  Therefore, by\nidentifying the head as the subject of monitoring, for example, and chronologically detecting its relative velocity or acceleration, a fall can be recognized when a velocity of a certain value or greater is detected.  When recognizing a fall, the\nprocessing section 1101 can issue an instruction or the like corresponding to pertinent nursing care assistance, for example.\n Note that the sensor section(s) 1010 is secured to a fixed position(s) in the above-described monitoring system or the like.  However, the sensor section(s) 1010 can also be installed on a moving entity, e.g., a robot, a vehicle, a flying object\nsuch as a drone.  As used herein, the vehicle or the like may encompass not only an automobile, but also a smaller sized moving entity such as an electric wheelchair, for example.  In this case, this moving entity may include an internal GPS unit which\nallows its own current position to be always confirmed.  In addition, this moving entity may also have a function of further improving the accuracy of its own current position by using map information and the map update information which has been\ndescribed with respect to the aforementioned fifth processing apparatus.\n Furthermore, in any device or system that is similar to the above-described first to third detection devices, first to sixth processing apparatuses, first to fifth monitoring systems, etc., a like construction may be adopted to utilize an array\nantenna or a millimeter wave radar according to an embodiment of the present disclosure.\nApplication Example 3: Communication System\nFirst Example of Communication System\n The waveguide device and antenna device (array antenna) according to the present disclosure can be used for the transmitter and/or receiver with which a communication system (telecommunication system) is constructed.  The waveguide device and\nantenna device according to the present disclosure are composed of layered conductive members, and therefore are able to keep the transmitter and/or receiver size smaller than in the case of using a hollow waveguide.  Moreover, there is no need for\ndielectric, and thus the dielectric loss of electromagnetic waves can be kept smaller than in the case of using a microstrip line.  Therefore, a communication system including a small and highly efficient transmitter and/or receiver can be constructed.\n Such a communication system may be an analog type communication system which transmits or receives an analog signal that is directly modulated.  However, a digital communication system may be adopted in order to construct a more flexible and\nhigher-performance communication system.\n Hereinafter, with reference to FIG. 35, a digital communication system 800A in which a waveguide device and an antenna device according to an embodiment of the present disclosure are used will be described.\n FIG. 35 is a block diagram showing a construction for the digital communication system 800A.  The communication system 800A includes a transmitter 810A and a receiver 820A.  The transmitter 810A includes an analog to digital (A/D) converter 812,\nan encoder 813, a modulator 814, and a transmission antenna 815.  The receiver 820A includes a reception antenna 825, a demodulator 824, a decoder 823, and a digital to analog (D/A) converter 822.  The at least one of the transmission antenna 815 and the\nreception antenna 825 may be implemented by using an array antenna according to an embodiment of the present disclosure.  In this exemplary application, the circuitry including the modulator 814, the encoder 813, the A/D converter 812, and so on, which\nare connected to the transmission antenna 815, is referred to as the transmission circuit.  The circuitry including the demodulator 824, the decoder 823, the D/A converter 822, and so on, which are connected to the reception antenna 825, is referred to\nas the reception circuit.  The transmission circuit and the reception circuit may be collectively referred to as the communication circuit.\n With the analog to digital (A/D) converter 812, the transmitter 810A converts an analog signal which is received from the signal source 811 to a digital signal.  Next, the digital signal is encoded by the encoder 813.  As used herein, \"encoding\"\nmeans altering the digital signal to be transmitted into a format which is suitable for communication.  Examples of such encoding include CDM (Code-Division Multiplexing) and the like.  Moreover, any conversion for effecting TDM (Time-Division\nMultiplexing) or FDM (Frequency Division Multiplexing), or OFDM (Orthogonal Frequency Division Multiplexing) is also an example of encoding.  The encoded signal is converted by the modulator 814 into a radio frequency signal, so as to be transmitted from\nthe transmission antenna 815.\n In the field of communications, a wave representing a signal to be superposed on a carrier wave may be referred to as a \"signal wave\"; however, the term \"signal wave\" as used in the present specification does not carry that definition.  A\n\"signal wave\" as referred to in the present specification is broadly meant to be any electromagnetic wave to propagate in a waveguide, or any electromagnetic wave for transmission/reception via an antenna element.\n The receiver 820A restores the radio frequency signal that has been received by the reception antenna 825 to a low-frequency signal at the demodulator 824, and to a digital signal at the decoder 823.  The decoded digital signal is restored to an\nanalog signal by the digital to analog (D/A) converter 822, and is sent to a data sink (data receiver) 821.  Through the above processes, a sequence of transmission and reception processes is completed.\n When the communicating agent is a digital appliance such as a computer, analog to digital conversion of the transmission signal and digital to analog conversion of the reception signal are not needed in the aforementioned processes.  Thus, the\nanalog to digital converter 812 and the digital to analog converter 822 in FIG. 35 may be omitted.  A system of such construction is also encompassed within a digital communication system.\n In a digital communication system, in order to ensure signal intensity or expand channel capacity, various methods may be adopted.  Many such methods are also effective in a communication system which utilizes radio waves of the millimeter wave\nband or the terahertz band.\n Radio waves in the millimeter wave band or the terahertz band have higher straightness than do radio waves of lower frequencies, and undergoes less diffraction, i.e., bending around into the shadow side of an obstacle.  Therefore, it is not\nuncommon for a receiver to fail to directly receive a radio wave that has been transmitted from a transmitter.  Even in such situations, reflected waves may often be received, but a reflected wave of a radio wave signal is often poorer in quality than is\nthe direct wave, thus making stable reception more difficult.  Furthermore, a plurality of reflected waves may arrive through different paths.  In that case, the reception waves with different path lengths might differ in phase from one another, thus\ncausing multi-path fading.\n As a technique for improving such situations, a so-called antenna diversity technique may be used.  In this technique, at least one of the transmitter and the receiver includes a plurality of antennas.  If the plurality of antennas are parted by\ndistances which differ from one another by at least about the wavelength, the resulting states of the reception waves will be different.\n Accordingly, the antenna that is capable of transmission/reception with the highest quality among all is selectively used, thereby enhancing the reliability of communication.  Alternatively, signals which are obtained from more than one antenna\nmay be merged for an improved signal quality.\n In the communication system 800A shown in FIG. 35, for example, the receiver 820A may include a plurality of reception antennas 825.  In this case, a switcher exists between the plurality of reception antennas 825 and the demodulator 824. \nThrough the switcher, the receiver 820A connects the antenna that provides the highest-quality signal among the plurality of reception antennas 825 to the demodulator 824.  In this case, the transmitter 810A may also include a plurality of transmission\nantennas 815.\nSecond Example of Communication System\n FIG. 36 is a block diagram showing an example of a communication system 800B including a transmitter 810B which is capable of varying the radiation pattern of radio waves.  In this exemplary application, the receiver is identical to the receiver\n820A shown in FIG. 35; for this reason, the receiver is omitted from illustration in FIG. 36.  In addition to the construction of the transmitter 810A, the transmitter 810B also includes an antenna array 815b, which includes a plurality of antenna\nelements 8151.  The antenna array 815b may be an array antenna according to an embodiment of the present disclosure.  The transmitter 810B further includes a plurality of phase shifters (PS) 816 which are respectively connected between the modulator 814\nand the plurality of antenna elements 8151.  In the transmitter 810B, an output of the modulator 814 is sent to the plurality of phase shifters 816, where phase differences are imparted and the resultant signals are led to the plurality of antenna\nelements 8151.  In the case where the plurality of antenna elements 8151 are disposed at equal intervals, if a radio frequency signal whose phase differs by a certain amount with respect to an adjacent antenna element is fed to each antenna element 8151,\na main lobe 817 of the antenna array 815b will be oriented in an azimuth which is inclined from the front, this inclination being in accordance with the phase difference.  This method may be referred to as beam forming.\n The azimuth of the main lobe 817 may be altered by allowing the respective phase shifters 816 to impart varying phase differences.  This method may be referred to as beam steering.  By finding phase differences that are conducive to the best\ntransmission/reception state, the reliability of communication can be enhanced.  Although the example here illustrates a case where the phase difference to be imparted by the phase shifters 816 is constant between any adjacent antenna elements 8151, this\nis not limiting.  Moreover, phase differences may be imparted so that the radio wave will be radiated in an azimuth which allows not only the direct wave but also reflected waves to reach the receiver.\n A method called null steering can also be used in the transmitter 810B.  This is a method where phase differences are adjusted to create a state where the radio wave is radiated in no specific direction.  By performing null steering, it becomes\npossible to restrain radio waves from being radiated toward any other receiver to which transmission of the radio wave is not intended.  This can avoid interference.  Although a very broad frequency band is available to digital communication utilizing\nmillimeter waves or terahertz waves, it is nonetheless preferable to make as efficient a use of the bandwidth as possible.  By using null steering, plural instances of transmission/reception can be performed within the same band, whereby efficiency of\nutility of the bandwidth can be enhanced.  A method which enhances the efficiency of utility of the bandwidth by using techniques such as beam forming, beam steering, and null steering may sometimes be referred to as SDMA (Spatial Division Multiple\nAccess).\nThird Example of Communication System\n In order to increase the channel capacity in a specific frequency band, a method called MIMO (Multiple-Input and Multiple-Output) may be adopted.  Under MIMO, a plurality of transmission antennas and a plurality of reception antennas are used. \nA radio wave is radiated from each of the plurality of transmission antennas.  In one example, respectively different signals may be superposed on the radio waves to be radiated.  Each of the plurality of reception antennas receives all of the\ntransmitted plurality of radio waves.  However, since different reception antennas will receive radio waves that arrive through different paths, differences will occur among the phases of the received radio waves.  By utilizing these differences, it is\npossible to, at the receiver side, separate the plurality of signals which were contained in the plurality of radio waves.\n The waveguide device and antenna device according to the present disclosure can also be used in a communication system which utilizes MIMO.  Hereinafter, an example such a communication system will be described.\n FIG. 37 is a block diagram showing an example of a communication system 800C implementing a MIMO function.  In the communication system 800C, a transmitter 830 includes an encoder 832, a TX-MIMO processor 833, and two transmission antennas 8351\nand 8352.  A receiver 840 includes two reception antennas 8451 and 8452, an RX-MIMO processor 843, and a decoder 842.  Note that the number of transmission antennas and the number of reception antennas may each be greater than two.  Herein, for ease of\nexplanation, an example where there are two antennas of each kind will be illustrated.  In general, the channel capacity of an MIMO communication system will increase in proportion to the number of whichever is the fewer between the transmission antennas\nand the reception antennas.\n Having received a signal from the data signal source 831, the transmitter 830 encodes the signal at the encoder 832 so that the signal is ready for transmission.  The encoded signal is distributed by the TX-MIMO processor 833 between the two\ntransmission antennas 8351 and 8352.\n In a processing method according to one example of the MIMO method, the TX-MIMO processor 833 splits a sequence of encoded signals into two, i.e., as many as there are transmission antennas 8352, and sends them in parallel to the transmission\nantennas 8351 and 8352.  The transmission antennas 8351 and 8352 respectively radiate radio waves containing information of the split signal sequences.  When there are N transmission antennas, the signal sequence is split into N. The radiated radio waves\nare simultaneously received by the two reception antennas 8451 and 8452.  In other words, in the radio waves which are received by each of the reception antennas 8451 and 8452, the two signals which were split at the time of transmission are mixedly\ncontained.  Separation between these mixed signals is achieved by the RX-MIMO processor 843.\n The two mixed signals can be separated by paying attention to the phase differences between the radio waves, for example.  A phase difference between two radio waves of the case where the radio waves which have arrived from the transmission\nantenna 8351 are received by the reception antennas 8451 and 8452 is different from a phase difference between two radio waves of the case where the radio waves which have arrived from the transmission antenna 8352 are received by the reception antennas\n8451 and 8452.  That is, the phase difference between reception antennas differs depending on the path of transmission/reception.  Moreover, unless the spatial relationship between a transmission antenna and a reception antenna is changed, the phase\ndifference therebetween remains unchanged.  Therefore, based on correlation between reception signals received by the two reception antennas, as shifted by a phase difference which is determined by the path of transmission/reception, it is possible to\nextract any signal that is received through that path of transmission/reception.  The RX-MIMO processor 843 may separate the two signal sequences from the reception signal e.g. by this method, thus restoring the signal sequence before the split.  The\nrestored signal sequence still remains encoded, and therefore is sent to the decoder 842 so as to be restored to the original signal there.  The restored signal is sent to the data sink 841.\n Although the MIMO communication system 800C in this example transmits or receives a digital signal, an MIMO communication system which transmits or receives an analog signal can also be realized.  In that case, in addition to the construction of\nFIG. 37, an analog to digital converter and a digital to analog converter as have been described with reference to FIG. 35 are provided.  Note that the information to be used in distinguishing between signals from different transmission antennas is not\nlimited to phase difference information.  Generally speaking, for a different combination of a transmission antenna and a reception antenna, the received radio wave may differ not only in terms of phase, but also in scatter, fading, and other conditions. These are collectively referred to as CSI (Channel State Information).  CSI may be utilized in distinguishing between different paths of transmission/reception in a system utilizing MIMO.\n Note that it is not an essential requirement that the plurality of transmission antennas radiate transmission waves containing respectively independent signals.  So long as separation is possible at the reception antenna side, each transmission\nantenna may radiate a radio wave containing a plurality of signals.  Moreover, beam forming may be performed at the transmission antenna side, while a transmission wave containing a single signal, as a synthetic wave of the radio waves from the\nrespective transmission antennas, may be formed at the reception antenna.  In this case, too, each transmission antenna is adapted so as to radiate a radio wave containing a plurality of signals.\n In this third example, too, as in the first and second examples, various methods such as CDM, FDM, TDM, and OFDM may be used as a method of signal encoding.\n In a communication system, a circuit board that implements an integrated circuit (referred to as a signal processing circuit or a communication circuit) for processing signals may be stacked as a layer on the waveguide device and antenna device\naccording to an embodiment of the present disclosure.  Since the waveguide device and antenna device according to an embodiment of the present disclosure is structured so that plate-like conductive members are layered therein, it is easy to further stack\na circuit board thereupon.  By adopting such an arrangement, a transmitter and a receiver which are smaller in volume than in the case where a hollow waveguide or the like is employed can be realized.\n In the first to third examples of the communication system as described above, each element of a transmitter or a receiver, e.g., an analog to digital converter, a digital to analog converter, an encoder, a decoder, a modulator, a demodulator, a\nTX-MIMO processor, or an RX-MIMO processor, is illustrated as one independent element in FIGS. 35, 36, and 37; however, these do not need to be discrete.  For example, all of these elements may be realized by a single integrated circuit.  Alternatively,\nsome of these elements may be combined so as to be realized by a single integrated circuit.  Either case qualifies as an embodiment of the present invention so long as the functions which have been described in the present disclosure are realized\nthereby.\n As described above, the present disclosure encompasses slot array antennas, radars, radar systems, and communication systems as recited in the following Items.\n [Item 1]\n A slot array antenna comprising:\n a first electrically conductive member having a first electrically conductive surface and a plurality of slots arranged in a first direction along the first electrically conductive surface;\n a second electrically conductive member having a second electrically conductive surface that opposes the first electrically conductive surface;\n a waveguide member being interposed between the first and second electrically conductive members and having an electrically-conductive waveguide face that opposes the first or second electrically conductive surface, the waveguide member\nextending alongside the first or second electrically conductive surface; and\n an artificial magnetic conductor extending around the waveguide member, wherein,\n the waveguide face, the first or second electrically conductive surface opposing the waveguide face, and the artificial magnetic conductor define a waveguide;\n the waveguide member includes\n a stem extending along a direction,\n a first joint portion and a second joint portion branching out from an end of the stem, the first and second joint portions each extending into a region between two adjacent slots among the plurality of slots,\n a first branch connected to an end of the first joint portion, the first branch extending from the end of the first joint portion in the first direction and coupling to one or more of the plurality of slots, and\n a second branch connected to an end of the second joint portion, the second branch extending from the end of the second joint portion in an opposite direction of the first direction and coupling to other one or more of the plurality of slots;\n each of the plurality of slots intersects the waveguide member or splits the waveguide member;\n the end of the stem is not located in the region between the two slots as viewed perpendicularly to the first electrically conductive surface; and\n a length of the first joint portion as measured along the waveguide is not equal to a length of the second joint portion as measured along the waveguide.\n [Item 2]\n A slot array antenna for use in at least one of transmission and reception of an electromagnetic wave of a predetermined band, the slot array antenna comprising:\n a first electrically conductive member having a first electrically conductive surface and a plurality of slots arranged in a first direction along the first electrically conductive surface;\n a second electrically conductive member having a second electrically conductive surface that opposes the first electrically conductive surface;\n a waveguide member being interposed between the first and second electrically conductive members and having an electrically-conductive waveguide face that opposes the first or second electrically conductive surface, the waveguide member\nextending alongside the first or second electrically conductive surface; and\n an artificial magnetic conductor extending around the waveguide member, wherein,\n the waveguide face, the first or second electrically conductive surface opposing the waveguide face, and the artificial magnetic conductor define a waveguide;\n the waveguide member includes\n a stem extending along a direction,\n a first joint portion and a second joint portion branching out from an end of the stem, the first and second joint portions each extending into a region between two adjacent slots among the plurality of slots,\n a first branch connected to an end of the first joint portion, the first branch extending from the end of the first joint portion in the first direction and coupling to one or more of the plurality of slots, and\n a second branch connected to an end of the second joint portion, the second branch extending from the end of the second joint portion in an opposite direction of the first direction and coupling to other one or more of the plurality of slots;\n each of the plurality of slots intersects the waveguide member or splits the waveguide member;\n the end of the stem is not located in the region between the two slots as viewed perpendicularly to the first electrically conductive surface; and\n a difference between a phase variation to occur when an electromagnetic wave of the band propagates from the end of the stem to a closest slot to the first joint portion and a phase variation to occur when the electromagnetic wave propagates\nfrom the end of the stem to a closest slot to the second joint portion is greater than .pi./4 and smaller than (3/4).pi..\n [Item 3]\n The slot array antenna of item 1 or 2, wherein the stem includes a portion extending in parallel to the first branch and the second branch.\n [Item 4]\n The slot array antenna of item 3, wherein,\n the artificial magnetic conductor comprises a plurality of electrically conductive rods; and\n one row of electrically conductive rods is interposed between the first or second branch and the stem, the row of electrically conductive rods being arranged along the first direction.\n [Item 5]\n The slot array antenna of item 3, wherein,\n the artificial magnetic conductor comprises a plurality of electrically conductive rods;\n at least one portion of a side face of the first joint portion and at least one portion of a side face of the second joint portion are opposed to each other; and\n at least one electrically conductive rod is interposed between the at least one portion of the side face of the first joint portion and the at least one portion of the side face of the second joint portion.\n [Item 6]\n The slot array antenna of item 4, wherein,\n at least one portion of a side face of the first joint portion and at least one portion of a side face of the second joint portion are opposed to each other; and\n at least one electrically conductive rod is interposed between the at least one portion of the side face of the first joint portion and the at least one portion of the side face of the second joint portion.\n [Item 7]\n The slot array antenna of item 1, wherein,\n the slot array antenna is used for at least one of transmission and reception of an electromagnetic wave of a predetermined band,\n a difference between a phase variation to occur when an electromagnetic wave of the band propagates from the end of the stem to a closest slot to the first joint portion and a phase variation to occur when the electromagnetic wave propagates\nfrom the end of the stem to a closest slot to the second joint portion is greater than .pi./4 and smaller than (3/4).pi..\n [Item 8]\n The slot array antenna of item 1, wherein,\n the slot array antenna is used for at least one of transmission and reception of an electromagnetic wave of a band having a central wavelength .lamda.o in free space; and\n a difference between a distance from the end of the stem to a closest slot to the first joint portion as measured along the waveguide and a distance from the end of the stem to a closest slot to the second joint portion as measured along the\nwaveguide is greater than .lamda.o/4 and smaller than (3/4).lamda.o.\n [Item 9]\n The slot array antenna of any of items 1 to 8, wherein,\n the plurality of slots comprises four slots;\n two of the four slots are opposed to the first branch; and\n other two of the four slots are opposed to the second branch.\n [Item 10]\n The slot array antenna of item 9, wherein the plurality of slots are disposed in a symmetric manner along the first direction.\n [Item 11]\n The slot array antenna of any of items 1 to 10, wherein, the artificial magnetic conductor comprises a plurality of electrically conductive rods;\n the slot array antenna is used for at least one of transmission and reception of an electromagnetic wave of a predetermined band;\n among electromagnetic waves of the predetermined band, an electromagnetic wave of a highest frequency has a wavelength .lamda.m in free space; and\n a width of the waveguide member, a width of each electrically conductive rod, a width of a space between two adjacent electrically conductive rods, a distance between the first and second electrically conductive surfaces, and a width of a space\nbetween the waveguide member and an electrically conductive rod adjacent to the waveguide member are each less than .lamda.m/2.\n [Item 12]\n The slot array antenna of any of items 1 to 10, further comprising at least another waveguide member which is adjacent to the waveguide member, the other waveguide member or members being interposed between the first and second electrically\nconductive members and having an electrically-conductive waveguide face that opposes the first or second electrically conductive surface, the other waveguide member or members extending alongside the first or second electrically conductive surface,\nwherein,\n the artificial magnetic conductor extends around the other waveguide member or members;\n the first electrically conductive member includes at least another slot row which is adjacent to a slot row consisting of the plurality of slots,\n the slot row and the other slot row or rows flank each other along a second direction which intersects the first direction;\n the other slot row or rows each include a plurality of slots arranged in the first direction,\n the waveguide face of the other waveguide member or members, the first or second electrically conductive surface opposing the waveguide face, and the artificial magnetic conductor define another waveguide;\n the other waveguide member or members each include\n a stem extending along a direction,\n a first joint portion and a second joint portion branching out from an end of the stem, the first and second joint portions each extending into a region between two adjacent slots among the plurality of slots in the other slot row or rows,\n a first branch connected to an end of the first joint portion, the first branch extending from the end of the first joint portion in the first direction and coupling to one or more of the plurality of slots, and\n a second branch connected to an end of the second joint portion, the second branch extending from the end of the second joint portion in an opposite direction of the first direction and coupling to other one or more of the plurality of slots;\n the plurality of slots in the other slot row or rows each intersect the other waveguide member or members or split the other waveguide member or members;\n the end of the stem of the other waveguide member or members is not located in the region between the two slots as viewed perpendicularly to the first electrically conductive surface; and\n a length of the first joint portion of the other waveguide member or members as measured along the other waveguide is not equal to a length of the second joint portion as measured along the other waveguide.\n [Item 13]\n A radar device comprising:\n the slot array antenna of any of items 1 to 12; and\n at least one microwave integrated circuit connected to the slot array antenna.\n [Item 14]\n A radar system comprising:\n the radar device of item 13; and\n a signal processing circuit connected to the microwave integrated circuit of the radar device.\n [Item 15]\n A wireless communication system comprising:\n the slot array antenna of any of items 1 to 12; and\n a communication circuit connected to the slot array antenna.\n A slot array antenna according to the present disclosure is usable in any technological field that makes use of an antenna.  For example, they are available to various applications where transmission/reception of electromagnetic waves of the\ngigahertz band or the terahertz band is performed.  In particular, they are suitably used in onboard radar systems, various types of monitoring systems, indoor positioning systems, wireless communication systems, etc., where downsizing is desired.\n While the present invention has been described with respect to exemplary embodiments thereof, it will be apparent to those skilled in the art that the disclosed invention may be modified in numerous ways and may assume many embodiments other\nthan those specifically described above.  Accordingly, it is intended by the appended claims to cover all modifications of the invention that fall within the true spirit and scope of the invention.\n This application is based on Japanese Patent Applications No. 2017-059458 filed Mar.  24, 2017, the entire contents of which are hereby incorporated by reference.", "application_number": "15923053", "abstract": " A slot array antenna includes: a first conductive member having a first\n     conductive surface and a plurality of slots arranged in a first\n     direction; a second conductive member having a second conductive surface\n     that opposes the first conductive surface; a waveguide member having a\n     waveguide face that extends so as to oppose one of the first and second\n     conductive surfaces; and an artificial magnetic conductor extending\n     around the waveguide member. The waveguide member includes: a stem\n     extending along a direction; first and second joint portions branching\n     out from an end of the stem and extending into a region between two\n     adjacent slots; a first branch extending in the first direction from an\n     end of the first joint portion and coupling to some slots; and a second\n     branch extending from an end of the second joint portion in an opposite\n     direction of the first direction and coupling to other slots.\n", "citations": ["6191704", "6339395", "6403942", "6611610", "6628299", "6661367", "6703967", "6903677", "6943726", "7161561", "7355524", "7358889", "7417580", "7420159", "7425983", "7570198", "7978122", "8068134", "8446312", "8543277", "8593521", "8604968", "8610620", "8614640", "8636393", "8730096", "8730099", "8803638", "8861842", "9286524", "20110187614", "20120092224", "20130033404", "20150264230", "20160140424", "20160264065", "20170084971", "20180026378", "20180351261", "20180375219", "20190006743", "20190013589"], "related": []}, {"id": "20180285659", "patent_code": "10373002", "patent_name": "Method, apparatus, and system for a parametric representation of lane\n     lines", "year": "2019", "inventor_and_country_data": " Inventors: \nKwant; Richard (Oakland, CA), Mittal; Anish (Berkeley, CA)  ", "description": "BACKGROUND\n Autonomous driving has quickly become an area of interest for vehicle manufactures and navigation and mapping service providers.  One particular area of interest is the use of computer vision to enable mapping and sensing of a vehicle's\nenvironment to support autonomous or semi-autonomous operation.  Advances in available computing power has enabled this mapping and sensing to approach or achieve real-time operation through, e.g., machine learning (e.g., neural networks).  As a result,\none application of vision techniques in autonomous driving is localization of the vehicle with respect to known reference marks such as lane markings and/or other visible environmental features.  However, despite the noted advances in available computing\npower, service providers and manufacturers still face significant technical challenges to enable computer vision systems to efficiently recognize features such as lane markings during driving activities, particularly within distributed, multi-node\nsystems, employed in advanced neural networks or other similar machine learning system.\nSOME EXAMPLE EMBODIMENTS\n Therefore, there is a need for an approach for generating a parametric representation of lane lines from captured images (e.g., a video capture stream from an autonomous vehicle) for use in advanced computer vision systems.\n According to one embodiment, a method comprises segmenting, by a computer vision system, an input image into a plurality of grid cells.  The method also comprises processing, by the computer vision system, a portion of the input image in each of\nthe plurality of grid cells to detect one or more lane lines in said each grid cell.  The method further comprises, for said each grid cell in which the one or more lane lines are detected, determining one or more intercepts of the one or more lane lines\nwith one or more edges of said each grid cell, and one or more slopes of the one or more lane lines at the one or more intercepts.  The method further comprises generating, by the computer vision system, a parametric representation of the one or more\nlane lines for said each grid cell.  The parametric representation encodes the one or more intercepts and the one or more slopes into a data structure for said each grid cell.  The method further comprises providing, by the computer vision system, an\noutput parametric representation for the input image, wherein the output parametric representation aggregates the parametric representations of said each grid cell.\n According to another embodiment, an apparatus comprises at least one processor, and at least one memory including computer program code for one or more computer programs, the at least one memory and the computer program code configured to, with\nthe at least one processor, cause, at least in part, the apparatus to segment an input image into a plurality of grid cells.  The apparatus is also caused to process a portion of the input image in each of the plurality of grid cells to detect one or\nmore lane lines in said each grid cell.  The apparatus is also caused to, for said each grid cell in which the one or more lane lines are detected, determine one or more intercepts of the one or more lane lines with one or more edges of said each grid\ncell, and one or more slopes of the one or more lane lines at the one or more intercepts.  The apparatus is further caused to generate a parametric representation of the one or more lane lines for said each grid cell.  The parametric representation\nencodes the one or more intercepts and the one or more slopes into a data structure for said each grid cell.  The apparatus is further caused to provide an output parametric representation for the input image, wherein the output parametric representation\naggregates the parametric representations of said each grid cell.\n According to another embodiment, a non-transitory computer-readable storage medium carries one or more sequences of one or more instructions which, when executed by one or more processors, cause, at least in part, an apparatus to segment an\ninput image into a plurality of grid cells.  The apparatus is also caused to process a portion of the input image in each of the plurality of grid cells to detect one or more lane lines in said each grid cell.  The apparatus is also caused to, for said\neach grid cell in which the one or more lane lines are detected, determine one or more intercepts of the one or more lane lines with one or more edges of said each grid cell, and one or more slopes of the one or more lane lines at the one or more\nintercepts.  The apparatus is further caused to generate a parametric representation of the one or more lane lines for said each grid cell.  The parametric representation encodes the one or more intercepts and the one or more slopes into a data structure\nfor said each grid cell.  The apparatus is further caused to provide an output parametric representation for the input image, wherein the output parametric representation aggregates the parametric representations of said each grid cell.\n According to another embodiment, an apparatus comprises means for segmenting, by a computer vision system, an input image into a plurality of grid cells.  The apparatus also comprises means for processing, by the computer vision system, a\nportion of the input image in each of the plurality of grid cells to detect one or more lane lines in said each grid cell.  The apparatus further comprises means for determining one or more intercepts of the one or more lane lines with one or more edges\nof said each grid cell, and one or more slopes of the one or more lane lines at the one or more intercepts for said each grid cell in which the one or more lane lines are detected.  The apparatus further comprises means for generating, by the computer\nvision system, a parametric representation of the one or more lane lines for said each grid cell.  The parametric representation encodes the one or more intercepts and the one or more slopes into a data structure for said each grid cell.  The apparatus\nfurther comprises means for providing, by the computer vision system, an output parametric representation for the input image, wherein the output parametric representation aggregates the parametric representations of said each grid cell.\n In addition, for various example embodiments of the invention, the following is applicable: a method comprising facilitating a processing of and/or processing (1) data and/or (2) information and/or (3) at least one signal, the (1) data and/or\n(2) information and/or (3) at least one signal based, at least in part, on (or derived at least in part from) any one or any combination of methods (or processes) disclosed in this application as relevant to any embodiment of the invention.\n For various example embodiments of the invention, the following is also applicable: a method comprising facilitating access to at least one interface configured to allow access to at least one service, the at least one service configured to\nperform any one or any combination of network or service provider methods (or processes) disclosed in this application.\n For various example embodiments of the invention, the following is also applicable: a method comprising facilitating creating and/or facilitating modifying (1) at least one device user interface element and/or (2) at least one device user\ninterface functionality, the (1) at least one device user interface element and/or (2) at least one device user interface functionality based, at least in part, on data and/or information resulting from one or any combination of methods or processes\ndisclosed in this application as relevant to any embodiment of the invention, and/or at least one signal resulting from one or any combination of methods (or processes) disclosed in this application as relevant to any embodiment of the invention.\n For various example embodiments of the invention, the following is also applicable: a method comprising creating and/or modifying (1) at least one device user interface element and/or (2) at least one device user interface functionality, the (1)\nat least one device user interface element and/or (2) at least one device user interface functionality based at least in part on data and/or information resulting from one or any combination of methods (or processes) disclosed in this application as\nrelevant to any embodiment of the invention, and/or at least one signal resulting from one or any combination of methods (or processes) disclosed in this application as relevant to any embodiment of the invention.\n In various example embodiments, the methods (or processes) can be accomplished on the service provider side or on the mobile device side or in any shared way between service provider and mobile device with actions being performed on both sides.\n For various example embodiments, the following is applicable: An apparatus comprising means for performing a method of the claims.\n Still other aspects, features, and advantages of the invention are readily apparent from the following detailed description, simply by illustrating a number of particular embodiments and implementations, including the best mode contemplated for\ncarrying out the invention.  The invention is also capable of other and different embodiments, and its several details can be modified in various obvious respects, all without departing from the spirit and scope of the invention.  Accordingly, the\ndrawings and description are to be regarded as illustrative in nature, and not as restrictive. BRIEF DESCRIPTION OF THE DRAWINGS\n The embodiments of the invention are illustrated by way of example, and not by way of limitation, in the figures of the accompanying drawings:\n FIG. 1 is a diagram of a system capable of providing a parametric representation of lane lines, according to one embodiment;\n FIG. 2 is diagram illustrating example vehicle navigation system that can employ localization based on lane lines, according to one embodiment;\n FIG. 3 is a diagram of an input image of lane lines captured by a vehicle camera system, according to one embodiment;\n FIGS. 4A and 4B are diagrams illustrating polyline representations of lane lines, according to one embodiment;\n FIG. 5 is a diagram illustrating a segmentation of an input image into grid cells, according to one embodiment;\n FIG. 6 is a diagram illustrating a parametric representation of a lane line detected in a grid cell unit, according to one embodiment;\n FIG. 7 is a diagram illustrating a parametric representation of multiple lane lines detected in a single grid cell in multiple output channels of the grid cell, according to one embodiment;\n FIG. 8 is a diagram illustrating a parametric representation of lane lines detected in an input image, according to one embodiment;\n FIGS. 9A-9C are diagrams illustrating grid cells grouped based on shared borders that depict a continuous lane line, according to one embodiment;\n FIG. 10 is a diagram illustrating a process for decoding a parametric representation of a lane line into a polyline representation of the lane line, according to one embodiment;\n FIG. 11 is a diagram of a geographic database, according to one embodiment;\n FIG. 12 is a flowchart of a process for generating a parametric representation of lane lines detected in an input image, according to one embodiment;\n FIG. 13 is a flowchart of a process for grouping grid cells based on their respective parametric representations of lane lines, according to one embodiment;\n FIG. 14 is a flowchart for decoding parametric representations of lane lines into polylines, according to one embodiment;\n FIG. 15 is a diagram of hardware that can be used to implement an embodiment of the invention;\n FIG. 16 is a diagram of a chip set that can be used to implement an embodiment of the invention; and\n FIG. 17 is a diagram of a mobile terminal (e.g., handset) that can be used to implement an embodiment of the invention.\nDESCRIPTION OF SOME EMBODIMENTS\n Examples of a method, apparatus, and computer program for providing a parametric representation of travel lane lines are disclosed.  In the following description, for the purposes of explanation, numerous specific details are set forth in order\nto provide a thorough understanding of the embodiments of the invention.  It is apparent, however, to one skilled in the art that the embodiments of the invention may be practiced without these specific details or with an equivalent arrangement.  In\nother instances, well-known structures and devices are shown in block diagram form in order to avoid unnecessarily obscuring the embodiments of the invention.\n FIG. 1 is a diagram of a system capable of providing a parametric representation of lane lines, according to one embodiment.  As discussed above, autonomous driving has quickly become an area of intense interest, with recent advances in machine\nlearning, computer vision and compute power enabling real-time mapping and sensing of a vehicle's environment.  Such an understanding of the environment enables autonomous, semi-autonomous, or highly assisted driving in a vehicle (e.g., a vehicle 101) in\nat least two distinct ways.\n First, real-time sensing of the environment provides information about potential obstacles, the behavior of others on the road, and safe, drivable areas.  An understanding of where other cars are and what they might do is critical for a vehicle\n101 to safely plan a route.  Moreover, vehicles 101 generally must avoid both static (lamp posts, e.g.) and dynamic (cats, deer, e.g.) obstacles, and these obstacles may change or appear in real-time.  More fundamentally, vehicles 101 can use a semantic\nunderstanding of what areas around them are navigable and safe for driving.  Even in a situation where the world is completely mapped in high resolution, exceptions will occur in which a vehicle 101 might need to drive off the road to avoid a collision,\nor where a road's geometry or other map attributes like direction of travel have changed.  In this case, detailed mapping may be unavailable, and the vehicle 101 has to navigate using real-time sensing of road features or obstacles using a computer\nvision system (e.g., a computer vision system 103).\n A second application of vision techniques in autonomous driving is localization of the vehicle 101 with respect to a map of reference landmarks.  Understanding one's location on a map enables planning of a route, both on fine and coarse scales. \nOn a coarse scale, navigation maps allow vehicles 101 to know what roads to use to reach a particular destination.  However, on a finer scale, maps allow vehicles 101 to know what lanes to be in and when to make lane changes.  Knowing this information is\nimportant for planning an efficient and safe route, for in complicated driving situations maneuvers need to be executed in a timely fashion, and sometimes before they are visually obvious.  In addition, localization with respect to a map enables the\nincorporation of other real-time information into route planning.  Such information could include traffic, areas with unsafe driving conditions (ice, fog, potholes, e.g.), and temporary road changes like construction.\n With respect to lane localization and also generally with respect to autonomous driving, high accuracy and real-time localization of vehicles 101 is needed.  Traditionally, most vehicle navigation system has been accomplished this localization\nusing GPS, which generally provides a real-time location with a 95% confidence interval of 7.8 meters.  However, in complicated urban environments, reflection of GPS signals can further increase this error, such that one's location may be off by as much\nas 30 meters.  Given that the width of many lanes is 3-4 meters, this accuracy is not sufficient to properly localize a vehicle 101 (e.g., an autonomous vehicle) so that it can make safe route planning decisions.  Other sensors, such as inertial\nmeasurement units (IMUs) can increase the accuracy of localization by taking into account vehicle movement, but these sensors tend to drift and still do not provide sufficient accuracy for localization.\n In general, a localization accuracy of around 10 cm is needed for safe driving in many areas.  One way to achieve this level of accuracy is to use visual odometry, in which features are detected from imagery.  These features can then be matched\nto a database of features to determine one's location.  For example, FIG. 2 illustrates a vehicle 101 equipped with a navigation or autonomous vehicle control system 201 that employs an embodiment of a computer vision-based localization system as\ndescribed herein.  In this example, a pothole 205 has been detected or otherwise reported in the left lane of the roadway 203.  The navigation or vehicle control system 201 can use lane localization to detect which lane of the roadway 203 the vehicle 101\nis currently driving in (e.g., currently driving in the right lane).  By employing visual odometry, the lane detection can be performed with sufficient accuracy to enable the system 201 to instruct the driver to remain in the detected right lane to avoid\nthe pothole 205 in the left lane.  In autonomous mode, the system 201 can operate the vehicle 101 so that it remains in the right lane until the pothole 205 is passed.\n Traditional feature-based localization that both detect features and localize against them generally rely on low-level features.  However, low-level features typically used in these algorithms (e.g., Scale-Invariant Feature Transform (SIFT) or\nOriented FAST and rotated BRIEF (ORB)) tend to be brittle and not persist in different environmental and lighting conditions.  As a result, they often cannot be used to localize a vehicle on different days in different weather conditions.  Aside from\nreproducibility, the ability to detect and store higher level features of different types can provide better and more accurate localization with higher confidence.\n In response to these issues, the system 100 of FIG. 1 (e.g., including the computer vision system 103) focuses on high level features that have semantic meaning for human beings.  One such feature that is important to autonomous driving is the\ndetection of lane markings and corresponding lane models.  Lane-level information is important for self-driving applications because it defines the grammar of driving.  Without knowledge of lane markings, it can difficult or impossible to determine where\na vehicle 101 should drive, can drive, and what maneuvers are possible.  As a result, the ability to detect lane-lines in real-time constitutes a fundamental part for the design of an autonomous vehicle 101.\n One technique that has shown significant ability to detect lanes is the use of convolutional neural networks.  Neural networks have shown unprecedented ability to recognize objects in images, understand the semantic meaning of images, and\nsegment images according to these semantic categories.  Despite the successful use of neural networks to detect lane markings and create lane models, a fundamental problem remains.  Neural network-based lane detectors must compromise between flexibility\nof representation and semantic understanding of the lane geometry.  On the one hand, a pixel-based segmentation of an image of a road can show every pixel that corresponds to a lane line.  Yet converting this image mask into a lane model requires a\nsignificant amount of additional processing.  On the other hand, a model that outputs parametric representations of lane lanes can impose a large bias onto the potential arrangement of lane lines.  This tradeoff is particularly notable when one considers\nthe fact that the most successful implementations of neural networks are often those for which the network can be trained completely end-to-end.  Such end-to-end networks have shown increased accuracy and speed.\n In addition, the success of localization based on features from an image depends on the precise localization of those features within the image.  From this standpoint, pixel-based approaches present another challenge.  To increase the precision\nof the network output, more pixels must be used.  Thus, an increase in the precision of the localization of features requires an increase in the number of pixels, which in turn increases the number of computational operations and the potentially the\nnumber of weights for the network.  This behavior is undesirable, and in general parametric representations do not suffer from this problem because they can specify the values of parameters with arbitrary precision.\n Accordingly, the system 100 of FIG. 1 introduces a flexible approach to parametrically represent a lane model in such a way that addresses the problems discussed above.  In one embodiment, the embodiments of approach to parametrically\nrepresenting lane lines as lane models can easily be represented by a neural network.  More specifically, this various embodiments described herein enable the encoding of a set of polylines that represent the centerlines of lane markings into a compact\nrepresentation with minimal loss of information.  After creating the parametric representation of a lane line, this representation can be decoded into the original lane models for use during localization and mapping.  The conversion between the encoded\nand decoded representations of the lane models is generally computationally cheap, such that it can be performed in real-time while a vehicle 101 is driving.  In one embodiment, such a representation enables a neural network to describe the semantics of\na lane model without imposing burdensome constraints.\n Although the various embodiments described herein discuss a computer vision system 103 that employs a neural network (e.g., a convolutional neural network) to recognize lane lines or markings in input image data, it is contemplated that any type\nof computer vision system 103 using any other machine learning technique or other image processing technique can use the approaches to parametric representations of lane lines as described herein.  In addition, although the parametric representations are\ndiscussed with respect lane lines or markings, it is also contemplated that the parametric representations can be used to represent any linear or curved edge or feature depicted in captured image data.\n FIG. 3 is a diagram of an example input image 301 depicting lane lines 303a-303c captured by a vehicle camera system, according to one embodiment.  In this example, the image 301 is captured in real-time by a camera system of a vehicle 101 as\nraster images at a predetermined pixel resolution.  In one embodiment, the image 301 can be captured using cameras sensitive to visible light, infra-red, and/or any other wavelength of light.  To support real-time operation, the image 301 can be part of\nan image stream captured at a relatively high frequency (e.g., 20 Hz, 30 Hz, or higher).  Each frame of the image stream can then be processed to provide real-time detection of lane-lines.\n In one embodiment, as shown in FIG. 4A, lane models are typically represented as sets of polylines 401a-401c, in which the centerlines of the respective lanes 303a-303c are represented by piecewise-linear functions with an arbitrary number of\npoints.  In the example of FIG. 4A, the polylines 401a-401c represent each lane 303a-303a a series of line segments (e.g., shown as dotted lines) with articulation points between the line segments indicated by circles.\n Generally, it can difficult for a neural network to output this type of representation (i.e., output polyline representations directly from an image), for several reasons.  First, neural networks are usually a fixed size, and as a result have a\nfixed size input and output.  On the other hand, the polyline representation above can have both a variable number of lines per image and a variable number of points per line.  Representing this variability in a fixed size output can be challenging.  One\npossible solution could be to output a tensor that is large enough to capture even a large number of points and lines.  However, in this case the correspondence between outputs and lines is still not well defined.  For example, output 1 could correspond\nto the left-most lane 303a, output 2 could correspond to the middle lane 303b, and output 3 could correspond to the right lane 303c.  In this scenario, as shown in FIG. 4B, the appearance of a lane 303d far to the left would affect every other lane. \nThis is because to maintain the same left-to-right lane ordering as shown in FIG. 4A, the existing lanes 303a-303c would have to be moved to outputs 2, 3, and 4, to allow output 1 to correspond to newly detected lane 303d.  This dependence of this type\nof parametric representation of one lane line on the existence of other lane lines is not ideal, because if the system 100 makes one mistake in detecting or not detecting a lane line, that mistake could affect all other lane markings as well.  For\nexample, if the left lane 303d in FIG. 4B was not detected in a previous image or frame of a video stream (e.g., FIG. 4A has no left lane 303d), then the outputs of the polyline detection would no longer match from image to image without further\nprocessing to realign the outputs.  Moreover, even if this problem can be addressed, such a representation would still require the output of such a computer vision system to have a global scope because at least one processing node (e.g., a neuron of a\nneural network) would have to process image data that encompasses all features of the entire image to ensure no lane is missed or to realign the outputs.  As a result, the system or neural network would require at least one fully connected layer (e.g., a\nlayer that spans the entire input image).  Such fully connected layers significantly can increase both the size and the complexity of neural networks.\n To address this problem, in one embodiment, the system 100 of FIG. 1 introduces a more natural representation of lane lines for a neural network that is based on a grid of squares overlaid with the input image as shown in the grid 501 of FIG. 5. In the example of FIG. 5, the grid 501 segments the input image (e.g., the image as shown in FIG. 3) into individual grid cells.  In one embodiment, such a grid can be output by a fully convolutional neural network, which has the advantage of being\ncomputationally fast without having an excess of parameters that might lead to overfitting.  For example, with respect to a neural network or other similar parallel processing system, each of the grid cells can be processed by a different neuron or\nprocessing node to more efficiently employ the available neurons or nodes and distribute the computational load for processing the entire input image.  In other words, in one layer of the neural network, the scope of each neuron corresponds to the extent\nof the input image area within each respective grid cell.  Each neuron or node can make is prediction (e.g., detection of a lane line) for each individual grid cell, thereby advantageously avoiding the computational resource burden associated with having\nto have a fully connected layer.\n As a result of this segmentation, the basic unit of representation then becomes each cell of the grid, in which each lane line is parametrically encoded.  It is contemplated that any number of parametric encodings are possible to parametrically\nrepresent the portion of the lane line detected in each cell.  For example, in one embodiment, the system 100 can encode the line as a slope and an intercept, such that the lane is assumed to be locally linear within each grid cell.  Accordingly, in this\nembodiment, a lane line in a grid cell can be parametrically represented with a minimum of two parameters: (1) one parameter indicating an intercept of the lane line with an edge of the grid cell, and (2) a slope of detected lane line at the intercept.\n In another embodiment, the system 100 can use a more sophisticated representation for the parametric encoding.  Specifically, instead of assuming a straight line through a cell, the system 100 can track the behavior of a lane line at each of the\nfour edges of a grid cell.  FIG. 6 is a diagram illustrating an example of this more sophisticated parametric representation of a lane line detected in a grid cell unit, according to one embodiment.  In this example, a detected lane line 601 (dotted\nline) crosses a grid cell 603 at intercept points 605a and 605b along edges 2 and 4 of the grid cell.  The tangents 607a and 607b of the lane line are shown in red at each edge of the cell.  The slopes of the tangents 607a and 607b are then associated\nwith each respective intercept.  It is contemplated that the slopes can also be expressed as angles of the tangents 607a and 607b with respect to a reference line.\n In one embodiment, at each of these edges, the system 100 records three values as the parametric representation of the detected lane lines: (1) an indicator value to say that the lane crossed the edge, (2) the slope or angle of the line, and (3)\nthe intercept of the line at that edge.  As a result, in one embodiment, each lane in each cell will be represented by 12 parameters (e.g., shown as representation 609 in FIG. 6).  For example, the representation 609 of the lane in the above cell would\nbe [0, 1, 0, 1, 0, 0.7, 0, 0.2, 0, 1.4, 0, 0.7] where the first four numbers are indicators that describe whether the lane crosses edges 1, 2, 3, 4, respectively, the next four numbers describe the value of the intercepts at those edges, and the last\nfour numbers describe the angle of the lane in radians.\n In one embodiment, the indicator value is 1 if the detected lane line crosses the edge, and 0 if the detected lane line does not cross the edge.  In one embodiment, during prediction by the system 100 (e.g., neural network), the indicator values\nabove become probabilities that a lane line crosses the given boundary (e.g., 0 corresponding to zero probability of a crossing, 1 corresponding to crossing highest probability of a crossing, and other values spanning the range to indicate different\nprobabilities).  In one embodiment, the intercept value represents a point of intersection on the edge with the edge spanning from 0 to 1 distance units along a reference direction.  In one embodiment, as discussed above, the slope value represents a\nslope of the tangent line at the intercept which can also be expressed as an angle with respect to a reference line.  This angle can be express according to any angular measurement unit (e.g., radians, degrees, etc.) and/or coordinate system.\n The embodiments of the parametric representation of lane lines discussed above have several advantages.  First, the curvature of the line within the cell can be represented because the input and output angles are known.  In other words, a curve\ncorresponding to the detected lane line can be determined from just the parametric representation of the intercepts and slopes of the detected lane lines.  Therefore, a representation of the detected lane line within each cell can be determined from data\nfrom the edges of the cell, thereby reducing the memory and computational results that would be needed if additional data is needed to describe the lane line within each cell.\n As another advantage, although parametric representation of a lane line in each grid cell can be independently determined without reference to data external to the cells, two adjacent cells will both predict the intercept and slope for a shared\nedge.  In one embodiment, the system 100 can use this duplicate information to join cells back together during decoding, and to smooth the representation of the line by averaging.\n In one embodiment, an arbitrary number of lane lines can be output per cell by simply increasing the number of channels.  For example, the representation 609 has 12 parameters.  Therefore, the system 100 can increase the number of output\nchannels from 12 to 12n, where n is the desired number of lane lines per cell.  An example of increasing the number of output channels to accommodate additional lane lines is illustrated in FIG. 7.  In this example, a grid cell 701 delineates a portion\nof an input message that depicts three lane lines 703a-703c.  Using, for instance, the 12-parameter parametric representation of a lane, the system 100 can represent lane line 703a in output channels 1-12 to generate parametric representation 705a,\nrepresent lane line 703b in output channels 13-24 to generate parametric representation 705b, and represent lane line 703c in output channels 25-36 to generate parametric representation 705c.\n As another advantage, the system 100 can further classify lane lines by adding classification outputs to the representation.  For example, returning to FIG. 6, additional attribute outputs can be added to the 12-parameter representation to\ngenerate the expanded parametric representation 611.  For example, in addition to the 12 parameters that define the geometry of the line, an additional two parameters could be added to specify any additional attributes of the lines.  For example, one\nattribute can used to indicate whether a detected lane line is solid (e.g., indicated by 1,0), dashed (e.g., indicated by 0,0), double lines (e.g., indicated by 0,1), etc. With respect to example grid cell 603 of FIG. 6 above, the solid line may be\nrepresented as [1, 0, 0, 1, 0, 1, 0, 0.7, 0, 0.2, 0, 1.4, 0, 0.7], where the first two numbers describe the type of lane.  In this scenario, each additional class adds a total number of parameters equal to the number of cells.  Because the grid is\nusually low resolution (e.g., a low resolution grid of 50.times.38, giving 1900 outputs), the addition of classes or attributes does not significantly affect the size of the output relative to the total size of the network.\n After each grid cell is processed to generate its respective parametric representation of any detected lane lines within the cells, the parametric representations for each cell can be aggregated to form more complete picture of the detected\nlines.  As shown in FIG. 8, an output parametric representation of the entire input image is superimposed on the initial input image.  In this example, line segments are drawn from the intercepts of lane lines at the appropriate angles at each cell edge. An example of one line segment is line segment 803.  The remaining line segments are not labeled, but are depicted as short solid lines at each edge of a cell at which a line crosses.  These series of line segments correspond for instance to respective\nlane lines 805a-805c.\n In one embodiment, this output 801 of aggregated parametric representations from each cell grid can be decoded to the original polylines using a number of methods.  For example, one method is to look for cells with two edges with large indicator\nvalues and to simply join the intercepts from these cells.  As a result, the joined intercepts would represent the junctions of the polyline, and then line segments can be drawn between each junction to generate a polyline to represent the lane model. \nHowever, while conceptually simple, this example approach does not take advantage of the rich information encoded in this representation.\n In another embodiment, the system 100 uses the encoded rich information to create a more optimized polyline lane model.  For example, the system 100 analyzes all shared cell borders of the aggregated output 801.  At shared each border where both\ncells predict a line crossing at the same edge, the system 100 is configured to assume that two cells sharing border at which a lane line crosses the shared border will predict the same values for confidence, slope, and intercept at each of the\nrespective cells.  Accordingly, the system 100 can identify in the parametric representation output results those cells with shared borders at which lane lines cross.  Then if the parametric representation values are within a certain tolerance (e.g.,\nintercepts and/or slopes match within a predetermined tolerance level), the system 100 merges the cells to be part of the same lane line.  In one embodiment, the tolerance level can be increased to promote a looser merging of cells, or decreased to\npromote a tighter or more restrictive merging of cells.\n Once the merging is complete, the system 100 will have groups of cells that each contain parameters for a given lane line.  FIGS. 9A-9C are diagrams illustrating grid cells grouped based on shared borders that depict a continuous lane line,\naccording to one embodiment.  In each of FIGS. 9A-9C, the aggregated output 801 of FIG. 8 is depicted.  This aggregated output 801 includes three separate detected lane lines.  FIG. 9A depicts a first group of cells 901 (e.g., indicated in FIG. 9A by\ncells with thicker borders) that have been group according to the process described above to merge cells corresponding to a first detected lane line.  FIG. 9B depicts a second group of cells 911 (e.g., indicated in FIG. 9B by cells with thicker borders)\nthat are associated with a second detected lane line.  Similarly, FIG. 9C depicts a third group of cells 921 (e.g., indicated in FIG. 9C by cells with thicker borders) that are associated with a third detected lane line.\n In one embodiment, the system 100 then can decode the grouped cells to construct a lane model of the detected lane lines (e.g., a polyline).  FIG. 10 is a diagram illustrating a process for decoding a parametric representation of a lane line\ninto a polyline representation of the lane line, according to one embodiment.  For each set of group cells, the system 100 averages the intercepts and slopes for all shared cell walls (at 1001).  Within each cell, the system 100 computes a curve based on\nthe intercepts and slopes along edges of the cells (at 1003).  For example, in a typical cell with two intercepts, the system 100 can fit a curve that encompasses the intercepts.  The degree of curvature can then be based on the slope values at each of\nthe intercepts.  By way of example, the system 100 can compute the curve as a third order Hermite polynomial, a Bezier curve, and/or any other similar curves.\n At process 1005, the system 100 can use the determined curve for each cell to calculate a large number of points along the curve.  In one embodiment, a \"large number\" refers to a number points greater than a number needed to represent the curve\nusing a polyline.  They system 100 can then simplify the lane line as now represented by the large number points along the curve by removing points that are not needed to define a shape of the curve to a predetermined degree of accuracy or precision (at\n1007).  For example, the system 100 can use an algorithm such as the Ramer-Douglas-Peucker algorithm to simplify the curve.  In one embodiment, at process 1009, the system 100 connects points remaining in the simplified representation to generate the\nfinal result as a set of polylines representing the lane models.\n Returning to FIG. 1, as shown, the system 100 includes a computer vision system 103 configured to perform the functions associated with generating and/or decoding the parametric representations of lane lines detected in an input image according\nto the various embodiments described herein.  In one embodiment, the computer vision system 103 includes a neural network or other machine learning/parallel processing system to automatically detect features such as lane lines in image data to support\nlocalization of, e.g., a vehicle 101 within the sensed environment.  In one embodiment, the neural network of the computer vision system 103 is a traditional convolutional neural network which consists of multiple layers of collections of one or more\nneurons (e.g., processing nodes of the neural network) which are configured to process a portion of an input image.  In one embodiment, the receptive fields of these collections of neurons (e.g., a receptive layer) can be configured to correspond to the\narea of an input image delineated by a respective a grid cell generated as described above.\n In one embodiment, the computer vision system 103 also has connectivity or access to a geographic database 105 which representations of mapped geographic features to facilitate video odometry to increase localization accuracy.  The geographic\ndatabase 105 can also store parametric representations of lane lines and other similar features and/or related data generated or used to encode or decode parametric representations of lane lines according to the various embodiments described herein.\n In one embodiment, the computer vision system 103 has connectivity over a communication network 107 to a services platform 109 that provides one or more services 111a-111n (also collectively referred to as services 111).  By way of example, the\nservices 111 may be third party services and include mapping services, navigation services, travel planning services, notification services, social networking services, content (e.g., audio, video, images, etc.) provisioning services, application\nservices, storage services, contextual information determination services, location based services, information based services (e.g., weather, news, etc.), etc. In one embodiment, the services 111 uses the output of the computer vision system 103 (e.g.,\nparametric representations of lane lines) to localize the vehicle 101 or a user equipment 113 (e.g., a portable navigation device, smartphone, portable computer, tablet, etc.) to provide services 111 such as POI recommendations, advertising intelligence,\netc.\n In one embodiment, the computer vision system 103 may be a platform with multiple interconnected components.  The computer vision system 103 may include multiple servers, intelligent networking devices, computing devices, components and\ncorresponding software for providing parametric representations of lane lines.  In addition, it is noted that the computer vision system 103 may be a separate entity of the system 100, a part of the one or more services 111, a part of the services\nplatform 109, or included within the UE 113 and/or vehicle 101.\n In one embodiment, content providers 115a-115m (collectively referred to as content providers 115) may provide content or data (e.g., including geographic data, parametric representations of mapped features, etc.) to the geographic database 105,\nthe computer vision system 103, the services platform 109, the services 111, the UE 113, the vehicle 101, and/or an application 117 executing on the UE 113.  The content provided may be any type of content, such as map content, textual content, audio\ncontent, video content, image content, etc. In one embodiment, the content providers 115 may provide content that may aid in the detecting and classifying of lane lines and/or other features in image data.  In one embodiment, the content providers 115\nmay also store content associated with the geographic database 105, computer vision system 103, services platform 109, services 111, UE 113, and/or vehicle 101.  In another embodiment, the content providers 115 may manage access to a central repository\nof data, and offer a consistent, standard interface to data, such as a repository of probe data, speed limit for one or more road links, speed information for at least one vehicle, traffic jam threshold for at least one road link, other traffic\ninformation, etc. Any known or still developing methods, techniques or processes for retrieving and/or accessing features for road links from one or more sources may be employed by the computer vision system 103.\n In one embodiment, the UE 113 and/or vehicle 101 may execute a software application 117 to collect, encode, and/or decode lane line detected in image data into the parametric representations according the embodiments described herein.  By way of\nexample, the application 117 may also be any type of application that is executable on the UE 113 and/or vehicle 101, such as autonomous driving applications, mapping applications, location-based service applications, navigation applications, content\nprovisioning services, camera/imaging application, media player applications, social networking applications, calendar applications, and the like.  In one embodiment, the application 117 may act as a client for the computer vision system 103 and perform\none or more functions of the computer vision system 103 alone or in combination with the system 103.\n By way of example, the UE 113 is any type of embedded system, mobile terminal, fixed terminal, or portable terminal including a built-in navigation system, a personal navigation device, mobile handset, station, unit, device, multimedia computer,\nmultimedia tablet, Internet node, communicator, desktop computer, laptop computer, notebook computer, netbook computer, tablet computer, personal communication system (PCS) device, personal digital assistants (PDAs), audio/video player, digital\ncamera/camcorder, positioning device, fitness device, television receiver, radio broadcast receiver, electronic book device, game device, or any combination thereof, including the accessories and peripherals of these devices, or any combination thereof. \nIt is also contemplated that the UE 113 can support any type of interface to the user (such as \"wearable\" circuitry, etc.).  In one embodiment, the UE 113 may be associated with the vehicle 101 or be a component part of the vehicle 101.\n In one embodiment, the UE 113 and/or vehicle 101 are configured with various sensors for generating or collecting environmental image data (e.g., for processing the computer vision system 103), related geographic data, etc. In one embodiment,\nthe sensed data represent sensor data associated with a geographic location or coordinates at which the sensor data was collected.  By way of example, the sensors may include a global positioning sensor for gathering location data (e.g., GPS), a network\ndetection sensor for detecting wireless signals or receivers for different short-range communications (e.g., Bluetooth, Wi-Fi, Li-Fi, near field communication (NFC) etc.), temporal information sensors, a camera/imaging sensor for gathering image data\n(e.g., the camera sensors may automatically capture road sign information, images of road obstructions, etc. for analysis), an audio recorder for gathering audio data, velocity sensors mounted on steering wheels of the vehicles, switch sensors for\ndetermining whether one or more vehicle switches are engaged, and the like.\n Other examples of sensors of the UE 113 and/or vehicle 101 may include light sensors, orientation sensors augmented with height sensors and acceleration sensor (e.g., an accelerometer can measure acceleration and can be used to determine\norientation of the vehicle), tilt sensors to detect the degree of incline or decline of the vehicle along a path of travel, moisture sensors, pressure sensors, etc. In a further example embodiment, sensors about the perimeter of the UE 113 and/or vehicle\n101 may detect the relative distance of the vehicle from a lane or roadway, the presence of other vehicles, pedestrians, traffic lights, potholes and any other objects, or a combination thereof.  In one scenario, the sensors may detect weather data,\ntraffic information, or a combination thereof.  In one embodiment, the UE 113 and/or vehicle 101 may include GPS or other satellite-based receivers to obtain geographic coordinates from satellites 119 for determining current location and time.  Further,\nthe location can be determined by a triangulation system such as A-GPS, Cell of Origin, or other location extrapolation technologies.  In yet another embodiment, the sensors can determine the status of various control elements of the car, such as\nactivation of wipers, use of a brake pedal, use of an acceleration pedal, angle of the steering wheel, activation of hazard lights, activation of head lights, etc.\n In one embodiment, the communication network 107 of system 100 includes one or more networks such as a data network, a wireless network, a telephony network, or any combination thereof.  It is contemplated that the data network may be any local\narea network (LAN), metropolitan area network (MAN), wide area network (WAN), a public data network (e.g., the Internet), short range wireless network, or any other suitable packet-switched network, such as a commercially owned, proprietary\npacket-switched network, e.g., a proprietary cable or fiber-optic network, and the like, or any combination thereof.  In addition, the wireless network may be, for example, a cellular network and may employ various technologies including enhanced data\nrates for global evolution (EDGE), general packet radio service (GPRS), global system for mobile communications (GSM), Internet protocol multimedia subsystem (IMS), universal mobile telecommunications system (UMTS), etc., as well as any other suitable\nwireless medium, e.g., worldwide interoperability for microwave access (WiMAX), Long Term Evolution (LTE) networks, code division multiple access (CDMA), wideband code division multiple access (WCDMA), wireless fidelity (Wi-Fi), wireless LAN (WLAN),\nBluetooth.RTM., Internet Protocol (IP) data casting, satellite, mobile ad-hoc network (MANET), and the like, or any combination thereof.\n By way of example, the geographic database 105, computer vision system 103, services platform 109, services 111, UE 113, vehicle 101, and/or content providers 115 communicate with each other and other components of the system 100 using well\nknown, new or still developing protocols.  In this context, a protocol includes a set of rules defining how the network nodes within the communication network 107 interact with each other based on information sent over the communication links.  The\nprotocols are effective at different layers of operation within each node, from generating and receiving physical signals of various types, to selecting a link for transferring those signals, to the format of information indicated by those signals, to\nidentifying which software application executing on a computer system sends or receives the information.  The conceptually different layers of protocols for exchanging information over a network are described in the Open Systems Interconnection (OSI)\nReference Model.\n Communications between the network nodes are typically effected by exchanging discrete packets of data.  Each packet typically comprises (1) header information associated with a particular protocol, and (2) payload information that follows the\nheader information and contains information that may be processed independently of that particular protocol.  In some protocols, the packet includes (3) trailer information following the payload and indicating the end of the payload information.  The\nheader includes information such as the source of the packet, its destination, the length of the payload, and other properties used by the protocol.  Often, the data in the payload for the particular protocol includes a header and payload for a different\nprotocol associated with a different, higher layer of the OSI Reference Model.  The header for a particular protocol typically indicates a type for the next protocol contained in its payload.  The higher layer protocol is said to be encapsulated in the\nlower layer protocol.  The headers included in a packet traversing multiple heterogeneous networks, such as the Internet, typically include a physical (layer 1) header, a data-link (layer 2) header, an internetwork (layer 3) header and a transport (layer\n4) header, and various application (layer 5, layer 6 and layer 7) headers as defined by the OSI Reference Model.\n FIG. 11 is a diagram of a geographic database, according to one embodiment.  In one embodiment, the geographic database 105 includes geographic data 1101 used for (or configured to be compiled to be used for) mapping and/or navigation-related\nservices, such as for video odometry based on the parametric representation of lanes include, e.g., encoding and/or decoding parametric representations into lane lines.  In one embodiment, geographic features (e.g., two-dimensional or three-dimensional\nfeatures) are represented using polygons (e.g., two-dimensional features) or polygon extrusions (e.g., three-dimensional features).  For example, the edges of the polygons correspond to the boundaries or edges of the respective geographic feature.  In\nthe case of a building, a two-dimensional polygon can be used to represent a footprint of the building, and a three-dimensional polygon extrusion can be used to represent the three-dimensional surfaces of the building.  It is contemplated that although\nvarious embodiments are discussed with respect to two-dimensional polygons, it is contemplated that the embodiments are also applicable to three dimensional polygon extrusions.  Accordingly, the terms polygons and polygon extrusions as used herein can be\nused interchangeably.\n In one embodiment, the following terminology applies to the representation of geographic features in the geographic database 105.\n \"Node\"--A point that terminates a link.\n \"Line segment\"--A straight line connecting two points.\n \"Link\" (or \"edge\")--A contiguous, non-branching string of one or more line segments terminating in a node at each end.\n \"Shape point\"--A point along a link between two nodes (e.g., used to alter a shape of the link without defining new nodes).\n \"Oriented link\"--A link that has a starting node (referred to as the \"reference node\") and an ending node (referred to as the \"non reference node\").\n \"Simple polygon\"--An interior area of an outer boundary formed by a string of oriented links that begins and ends in one node.  In one embodiment, a simple polygon does not cross itself.\n \"Polygon\"--An area bounded by an outer boundary and none or at least one interior boundary (e.g., a hole or island).  In one embodiment, a polygon is constructed from one outer simple polygon and none or at least one inner simple polygon.  A\npolygon is simple if it just consists of one simple polygon, or complex if it has at least one inner simple polygon.\n In one embodiment, the geographic database 105 follows certain conventions.  For example, links do not cross themselves and do not cross each other except at a node.  Also, there are no duplicated shape points, nodes, or links.  Two links that\nconnect each other have a common node.  In the geographic database 105, overlapping geographic features are represented by overlapping polygons.  When polygons overlap, the boundary of one polygon crosses the boundary of the other polygon.  In the\ngeographic database 105, the location at which the boundary of one polygon intersects they boundary of another polygon is represented by a node.  In one embodiment, a node may be used to represent other locations along the boundary of a polygon than a\nlocation at which the boundary of the polygon intersects the boundary of another polygon.  In one embodiment, a shape point is not used to represent a point at which the boundary of a polygon intersects the boundary of another polygon.\n As shown, the geographic database 105 includes node data records 1103, road segment or link data records 1105, POI data records 1107, parametric representation records 1109, other records 1111, and indexes 1113, for example.  More, fewer or\ndifferent data records can be provided.  In one embodiment, additional data records (not shown) can include cartographic (\"carto\") data records, routing data, and maneuver data.  In one embodiment, the indexes 1113 may improve the speed of data retrieval\noperations in the geographic database 105.  In one embodiment, the indexes 1113 may be used to quickly locate data without having to search every row in the geographic database 105 every time it is accessed.  For example, in one embodiment, the indexes\n1113 can be a spatial index of the polygon points associated with stored feature polygons.\n In exemplary embodiments, the road segment data records 1105 are links or segments representing roads, streets, or paths, as can be used in the calculated route or recorded route information for determination of one or more personalized routes. \nThe node data records 1103 are end points corresponding to the respective links or segments of the road segment data records 1105.  The road link data records 1105 and the node data records 1103 represent a road network, such as used by vehicles, cars,\nand/or other entities.  Alternatively, the geographic database 105 can contain path segment and node data records or other data that represent pedestrian paths or areas in addition to or instead of the vehicle road record data, for example.\n The road/link segments and nodes can be associated with attributes, such as geographic coordinates, street names, address ranges, speed limits, turn restrictions at intersections, and other navigation related attributes, as well as POIs, such as\ngasoline stations, hotels, restaurants, museums, stadiums, offices, automobile dealerships, auto repair shops, buildings, stores, parks, etc. The geographic database 105 can include data about the POIs and their respective locations in the POI data\nrecords 1107.  The geographic database 105 can also include data about places, such as cities, towns, or other communities, and other geographic features, such as bodies of water, mountain ranges, etc. Such place or feature data can be part of the POI\ndata records 307 or can be associated with POIs or POI data records 1107 (such as a data point used for displaying or representing a position of a city).\n In one embodiment, the geographic database 105 can also include parametric representations records 1109 for storing parametric representations of the lane lines detected from input image data according to the various embodiments described\nherein.  In one embodiment, the parametric representation records 1109 can be associated with one or more of the node records 1103, road segment records 1105, and/or POI data records 1107 to support localization or video odometry based on the features\nstored therein and the generated parametric representations of lane lines of the records 1109.  In this way, the parametric representation records 1109 can also be associated with the characteristics or metadata of the corresponding record 1103, 1105,\nand/or 1107.\n In one embodiment, the geographic database 105 can be maintained by the content provider 115 in association with the services platform 109 (e.g., a map developer).  The map developer can collect geographic data to generate and enhance the\ngeographic database 105.  There can be different ways used by the map developer to collect data.  These ways can include obtaining data from other sources, such as municipalities or respective geographic authorities.  In addition, the map developer can\nemploy field personnel to travel by vehicle (e.g., vehicle 101 and/or UE 113) along roads throughout the geographic region to observe features and/or record information about them, for example.  Also, remote sensing, such as aerial or satellite\nphotography, can be used.\n The geographic database 105 can be a master geographic database stored in a format that facilitates updating, maintenance, and development.  For example, the master geographic database or data in the master geographic database can be in an\nOracle spatial format or other spatial format, such as for development or production purposes.  The Oracle spatial format or development/production database can be compiled into a delivery format, such as a geographic data files (GDF) format.  The data\nin the production and/or delivery formats can be compiled or further compiled to form geographic database products or databases, which can be used in end user navigation devices or systems.\n For example, geographic data is compiled (such as into a platform specification format (PSF) format) to organize and/or configure the data for performing navigation-related functions and/or services, such as route calculation, route guidance,\nmap display, speed calculation, distance and travel time functions, and other functions, by a navigation device, such as by a vehicle 101 or UE 113, for example.  The navigation-related functions can correspond to vehicle navigation, pedestrian\nnavigation, or other types of navigation.  The compilation to produce the end user databases can be performed by a party or entity separate from the map developer.  For example, a customer of the map developer, such as a navigation device developer or\nother end user device developer, can perform compilation on a received geographic database in a delivery format to produce one or more compiled navigation databases.\n FIG. 12 is a flowchart of a process for generating a parametric representation of lane lines detected in an input image, according to one embodiment.  In one embodiment, the computer vision system 103 may perform one or more portions of the\nprocess 1200 and may be implemented in, for instance, a chip set including a processor and a memory as shown in FIG. 16.  As such, the computer vision system 103 can provide means for accomplishing various parts of the process 1200.  In addition or\nalternatively, the services platform 109 and/or services 111 may perform any combination of the steps of the process 1200 in combination with the computer vision system 103 or as standalone components.  Although the process 1200 is illustrated and\ndescribed as a sequence of steps, it is contemplated that various embodiments of the process 1200 may be performed in any order or combination and need not include all of the illustrated steps.\n In step 1201, the computer vision system 103 segments an input image into a plurality of grid cells.  As previously discussed, the input image can be part of an image capture stream (e.g., from an onboard camera of a vehicle 101) to support\nvideo odometry to more accurately localize the vehicle 101 (e.g., localized to within 10 cm accuracy).  In one embodiment, the grid is comprised of regular shapes (e.g., square, rectangle, etc.), but it is contemplated that grid can also vary in size\nand/or shape from cell to cell.  For example, in areas where higher resolution is needed (e.g., in the center of an image), smaller cells can be used to provide greater resolution.  Similarly, larger cells can be used for the periphery of an image, where\ngreater resolution may not be needed.\n In one embodiment, the resolution or size of the grid can vary with available processing power and/or desired accuracy/preciseness of the resulting lane models.  As previously discussed, in one embodiment, the grid resolution is at a relatively\nlow level (e.g., 50.times.38).\n In step 1203, the computer vision system 103 processes a portion of the input image in each of the plurality of grid cells to detect one or more lane lines in said each grid cell.  In one embodiment, the computer vision system 103 assigns the\nprocessing of said each grid cell or the generating of the parametric representation of said each grid cell to a different processing node of the computer vision system.  For example, in a neural network, the portion of the image data falling within a\ngrid cell represent the receptive field for a given collection of neurons.  These neurons can then use machine learning to automatically detect lane lines within the image.  In one embodiment, the computer vision system 103 comprises a convolutional\nneural network, and the generating of the parametric representation is completed in one forward pass of the convolutional neural network.\n In step 1205, the computer vision system 103 determines one or more intercepts of the one or more lane lines with one or more edges of said each grid cell, and one or more slopes of the one or more lane lines at the one or more intercepts for\nsaid each grid cell in which the one or more lane lines are detected.  In one embodiment, the center line of the detected lanes can be used to determine the edge intercepts and slopes for each grid cell.  It is contemplated that the computer vision\nsystem 103 can use any coordinate system, measurement unit, and/or scale to define the intercepts and slopes of the line at the intercepts.\n In step 1207, the computer vision system 103 generates a parametric representation of the one or more lane lines for said each grid cell, wherein the parametric representation encodes the one or more intercepts and the one or more slopes into a\ndata structure for said each grid cell.  In one embodiment, the parametric representation of the data structure includes: (1) an indicator value parameter for each of the one or more edges to indicate which of the one or more edges of said each grid cell\nthe one or more intercepts cross, (2) a slope parameter to indicate a slope of the one or more lane lines at the one or more intercepts, and/or (3) an intercept parameter to indicate a position along the one or more edges at which the one or more\nintercept occurs.  In one embodiment, the indicator value parameter represents a probability that the one or more lanes is predicted to cross at the one or more intercepts when the computer vision system is operating in a prediction mode.  One example of\nthis data structure or parametric representation is discussed with respect to FIG. 6 above.\n In one embodiment, the computer vision system 103 can optionally determine that there are a plurality of the one or more lane lines detected for said each grid cell.  The computer vision system 103 then generates the parametric representation\nfor each of the plurality of the one or more lane lines, and outputs the parametric representation for said each of the plurality of the one or more lane lines as a different set of output channels.  In one embodiment, the computer vision system 103 can\nbe configured with a maximum number of lane lines that it is to detect in a given grid cell.  This maximum number can then be used to determine the number of output channels to allocate to the parametric representation.  For example, in the example\ndiscussed above, a parametric representation of a line can have 12 parameters in one set to describe a single lane line.  Accordingly, the number of output channels to allocate be the maximum number lane lines to detect multiplied by the number of\nparameters in a set.\n In one embodiment, the parametric representation can be extended with additional classes and/or attributes to describe a detected lane line.  For example, the parametric representation further includes an attribute parameter indicating a lane\nline type.  This lane type can include a description class or attribute of the lane line such as whether the lane line is a solid lane line type or a dashed lane line type.  In one embodiment, the lane type can be determined directly from the input\nimage.  For example, the computer vision system 103 can identify whether a lane consists of a solid line or a dashed line based on recognized visual features in the input image.  In addition or alternatively, the lane type can be inferred or determined\nfrom the geographic database 105.  For example, the computer vision system 103 can query the database to identify the lane type based on the current coordinates of the vehicle 101 (e.g., as determined from GPS, video odometry, and/or any other available\nlocalization technique).\n In step 1209, the computer vision system 103 provides an output parametric representation for the input image, wherein the output parametric representation aggregates the parametric representations of said each grid cell.  In other words, the\ncomputer vision system 103 can aggregate the parametric representation of lane lines in each grid cell into an output parametric representation that encompasses the entire input image.  This collection of the basic representation units at the grid cells\ncan then represent the overall geometry of lane lines or lane models.\n FIG. 13 is a flowchart of a process for grouping grid cells based on their respective parametric representations of lane lines, according to one embodiment.  In one embodiment, the computer vision system 103 may perform one or more portions of\nthe process 1300 and may be implemented in, for instance, a chip set including a processor and a memory as shown in FIG. 16.  As such, the computer vision system 103 can provide means for accomplishing various parts of the process 1300.  In addition or\nalternatively, the services platform 109 and/or services 111 may perform any combination of the steps of the process 1300 in combination with the computer vision system 103 or as standalone components.  Although the process 1300 is illustrated and\ndescribed as a sequence of steps, it is contemplated that various embodiments of the process 1300 may be performed in any order or combination and need not include all of the illustrated steps.\n In one embodiment, the process 1300 is performed following the creation of the parametric representations of at least some or all of the grid cells as described with respect to the process 1200 of FIG. 12.\n In step 1301, the computer vision system 103 determines shared borders of grid cells.  By way of example, a shared border of occurs at an edge of a first grid cell that is immediately adjacent or overlaps a corresponding edge of an adjacent\nsecond grid cell.  Accordingly, in one embodiment, at each shared border of said each grid cell in the output parametric representation, the computer vision system 103 can look for cells with two edges with large indicator values (step 1303) and join the\nintercepts or simply group the cells (step 1307).  For example, a \"large indicator\" value refers to a predicted or detected lane line for which the computer vision system 103 has predicted a probability of crossing the edge that is above a threshold\nprobability.  In other words, when two cells share a common border and each of the two cells has an intercept on the edge at the common border, then the two intercepts or cells can be joined into a common lane line if the indicator value is above the\nprobability threshold.\n In addition or alternatively, the computer vision system 103 combines two of said each grid cells (step 1307) when the one or more intercepts, the one or more slopes, a confidence value associated with the one or more intercepts, or a\ncombination thereof for said two of said each grid cells are within a tolerance value (step 1305).  In this embodiment, instead of relying on just the indicator value to join or group cells as discussed above, the computer vision system 103 can evaluate\nwhether two adjacent intercepts occur at the same position along the shared border within a tolerance level (e.g., a threshold distance) and/or whether the slopes of the two intercepts also match to within a tolerance level (e.g., a threshold degree of\nvariance).  If the intercepts and/or slopes match within the tolerance level, then the intercepts or cells or joined into a group.\n In one embodiment, a group of said each grid cells resulting from the combining represents a given lane line.  In other words, the computationally cheap act of comparing intercepts and/or slopes at shared cell borders can result in building a\nlane model that advantageously does not require a fully connected layer of a neural network.\n In step 1309, the computer vision system 103 processes the one or more lane lines detected in said each grid cell in the group to generate a polyline representing of a lane model of the given lane.\n FIG. 14 is a flowchart for decoding parametric representations of lane lines into polylines, according to one embodiment.  In one embodiment, the computer vision system 103 may perform one or more portions of the process 1400 and may be\nimplemented in, for instance, a chip set including a processor and a memory as shown in FIG. 16.  As such, the computer vision system 103 can provide means for accomplishing various parts of the process 1400.  In addition or alternatively, the services\nplatform 109 and/or services 111 may perform any combination of the steps of the process 1400 in combination with the computer vision system 103 or as standalone components.  Although the process 1400 is illustrated and described as a sequence of steps,\nit is contemplated that various embodiments of the process 1400 may be performed in any order or combination and need not include all of the illustrated steps.\n The process 1400 describes an embodiment of a process for decoding grouped parametric representations of lane lines to generated lane models as described in step 1309 of the process 1300 above.  Accordingly, in one embodiment, the process 1400\nis performed following the process 1200 of FIG. 12 and the process 1300 of FIG. 13.  One advantage of this decoding approach is that once a parametric representation of a lane line is generated for each grid cell, no further image analysis is needed to\nmanipulate or otherwise decode the lane lines.  This, in turn, results in a significant reduction of computational resources needed from create the lane models.\n In step 1401, the processing of the one or more lane lines comprises averaging the one or more slopes, the one or more intercepts, or a combination thereof for said each shared border in the group.  As previously discussed, the different\nintercept and slope results generated by each adjacent cell of shared border represents duplicate information of the detected lane line edge crossing.  As a results, averaging the duplicate information or values can advantageously improve the accuracy of\nthe predicted lane lines.\n In step 1403, the computer vision system 103 determines a curvature of the one or more lane lines in said each grid cell based on an input slope and an output slope of the one or more slopes at the one or more intercepts.  For example, there are\ntypically two intercepts at two different edges or each grid cell in which a lane line is detected.  These two intercepts and their respective slopes can be used to parametrically describe the shape of the lane line shape within the cell without actually\nhave to store any data points about the line other than the intercepts and slope.  In other words, no data from the interior of the grid cell is needed.  Instead, the computer vision system 103 can compute a curve that encompasses the intercepts with the\ncurvature based on the slopes of the intercepts.  This curve can be based on computing, for instance, a Hermite polynomial, Bezier curve, and/or the like.\n Once the curve is determined, the computer vision system 103 can initiate the process or converting the curve representations into a polyline or other vector-based representation of the lane lines.  At step 1405, for instance, the computer\nvision system 103 uses the determined curves to calculate an excess number of points along the curve.  As described above, the excess or large number points is a number that is greater than needed to as junction points of a polyline or vector-based\nrepresentation of the lane line.\n At step 1407, the computer vision system 103 can simplify the point representation of the lane lines by removing any points from the excess points that are not needed to delineate the polyline or vector-based representation to a predetermined\naccuracy and/or precision.  This simplification process can be performed using any algorithm for simplifying curves such as the Ramer-Douglas-Peucker algorithm.  The computer vision system 103 then uses the simplified point representation to generate the\npolyline or vector-based representation of the lane lines by connecting line segments between the remaining points with the remaining points acting as junction points.  An example of the resulting polyline is illustrated in the example of FIG. 4A above.\n The processes described herein for providing a parametric representation of lane lines may be advantageously implemented via software, hardware (e.g., general processor, Digital Signal Processing (DSP) chip, an Application Specific Integrated\nCircuit (ASIC), Field Programmable Gate Arrays (FPGAs), etc.), firmware or a combination thereof.  Such exemplary hardware for performing the described functions is detailed below.\n FIG. 15 illustrates a computer system 1500 upon which an embodiment of the invention may be implemented.  Computer system 1500 is programmed (e.g., via computer program code or instructions) to provide a parametric representation of lane lines\nas described herein and includes a communication mechanism such as a bus 1510 for passing information between other internal and external components of the computer system 1500.  Information (also called data) is represented as a physical expression of a\nmeasurable phenomenon, typically electric voltages, but including, in other embodiments, such phenomena as magnetic, electromagnetic, pressure, chemical, biological, molecular, atomic, sub-atomic and quantum interactions.  For example, north and south\nmagnetic fields, or a zero and non-zero electric voltage, represent two states (0, 1) of a binary digit (bit).  Other phenomena can represent digits of a higher base.  A superposition of multiple simultaneous quantum states before measurement represents\na quantum bit (qubit).  A sequence of one or more digits constitutes digital data that is used to represent a number or code for a character.  In some embodiments, information called analog data is represented by a near continuum of measurable values\nwithin a particular range.\n A bus 1510 includes one or more parallel conductors of information so that information is transferred quickly among devices coupled to the bus 1510.  One or more processors 1502 for processing information are coupled with the bus 1510.\n A processor 1502 performs a set of operations on information as specified by computer program code related to providing a parametric representation of lane lines.  The computer program code is a set of instructions or statements providing\ninstructions for the operation of the processor and/or the computer system to perform specified functions.  The code, for example, may be written in a computer programming language that is compiled into a native instruction set of the processor.  The\ncode may also be written directly using the native instruction set (e.g., machine language).  The set of operations include bringing information in from the bus 1510 and placing information on the bus 1510.  The set of operations also typically include\ncomparing two or more units of information, shifting positions of units of information, and combining two or more units of information, such as by addition or multiplication or logical operations like OR, exclusive OR (XOR), and AND.  Each operation of\nthe set of operations that can be performed by the processor is represented to the processor by information called instructions, such as an operation code of one or more digits.  A sequence of operations to be executed by the processor 1502, such as a\nsequence of operation codes, constitute processor instructions, also called computer system instructions or, simply, computer instructions.  Processors may be implemented as mechanical, electrical, magnetic, optical, chemical or quantum components, among\nothers, alone or in combination.\n Computer system 1500 also includes a memory 1504 coupled to bus 1510.  The memory 1504, such as a random access memory (RAM) or other dynamic storage device, stores information including processor instructions for providing a parametric\nrepresentation of lane lines.  Dynamic memory allows information stored therein to be changed by the computer system 1500.  RAM allows a unit of information stored at a location called a memory address to be stored and retrieved independently of\ninformation at neighboring addresses.  The memory 1504 is also used by the processor 1502 to store temporary values during execution of processor instructions.  The computer system 1500 also includes a read only memory (ROM) 1506 or other static storage\ndevice coupled to the bus 1510 for storing static information, including instructions, that is not changed by the computer system 1500.  Some memory is composed of volatile storage that loses the information stored thereon when power is lost.  Also\ncoupled to bus 1510 is a non-volatile (persistent) storage device 1508, such as a magnetic disk, optical disk or flash card, for storing information, including instructions, that persists even when the computer system 1500 is turned off or otherwise\nloses power.\n Information, including instructions for providing a parametric representation of lane lines, is provided to the bus 1510 for use by the processor from an external input device 1512, such as a keyboard containing alphanumeric keys operated by a\nhuman user, or a sensor.  A sensor detects conditions in its vicinity and transforms those detections into physical expression compatible with the measurable phenomenon used to represent information in computer system 1500.  Other external devices\ncoupled to bus 1510, used primarily for interacting with humans, include a display device 1514, such as a cathode ray tube (CRT) or a liquid crystal display (LCD), or plasma screen or printer for presenting text or images, and a pointing device 1516,\nsuch as a mouse or a trackball or cursor direction keys, or motion sensor, for controlling a position of a small cursor image presented on the display 1514 and issuing commands associated with graphical elements presented on the display 1514.  In some\nembodiments, for example, in embodiments in which the computer system 1500 performs all functions automatically without human input, one or more of external input device 1512, display device 1514 and pointing device 1516 is omitted.\n In the illustrated embodiment, special purpose hardware, such as an application specific integrated circuit (ASIC) 1520, is coupled to bus 1510.  The special purpose hardware is configured to perform operations not performed by processor 1502\nquickly enough for special purposes.  Examples of application specific ICs include graphics accelerator cards for generating images for display 1514, cryptographic boards for encrypting and decrypting messages sent over a network, speech recognition, and\ninterfaces to special external devices, such as robotic arms and medical scanning equipment that repeatedly perform some complex sequence of operations that are more efficiently implemented in hardware.\n Computer system 1500 also includes one or more instances of a communications interface 1570 coupled to bus 1510.  Communication interface 1570 provides a one-way or two-way communication coupling to a variety of external devices that operate\nwith their own processors, such as printers, scanners and external disks.  In general the coupling is with a network link 1578 that is connected to a local network 1580 to which a variety of external devices with their own processors are connected.  For\nexample, communication interface 1570 may be a parallel port or a serial port or a universal serial bus (USB) port on a personal computer.  In some embodiments, communications interface 1570 is an integrated services digital network (ISDN) card or a\ndigital subscriber line (DSL) card or a telephone modem that provides an information communication connection to a corresponding type of telephone line.  In some embodiments, a communication interface 1570 is a cable modem that converts signals on bus\n1510 into signals for a communication connection over a coaxial cable or into optical signals for a communication connection over a fiber optic cable.  As another example, communications interface 1570 may be a local area network (LAN) card to provide a\ndata communication connection to a compatible LAN, such as Ethernet.  Wireless links may also be implemented.  For wireless links, the communications interface 1570 sends or receives or both sends and receives electrical, acoustic or electromagnetic\nsignals, including infrared and optical signals, that carry information streams, such as digital data.  For example, in wireless handheld devices, such as mobile telephones like cell phones, the communications interface 1570 includes a radio band\nelectromagnetic transmitter and receiver called a radio transceiver.  In certain embodiments, the communications interface 1570 enables connection to the communication network 107 for providing a parametric representation of lane lines.\n The term computer-readable medium is used herein to refer to any medium that participates in providing information to processor 1502, including instructions for execution.  Such a medium may take many forms, including, but not limited to,\nnon-volatile media, volatile media and transmission media.  Non-volatile media include, for example, optical or magnetic disks, such as storage device 1508.  Volatile media include, for example, dynamic memory 1504.  Transmission media include, for\nexample, coaxial cables, copper wire, fiber optic cables, and carrier waves that travel through space without wires or cables, such as acoustic waves and electromagnetic waves, including radio, optical and infrared waves.  Signals include man-made\ntransient variations in amplitude, frequency, phase, polarization or other physical properties transmitted through the transmission media.  Common forms of computer-readable media include, for example, a floppy disk, a flexible disk, hard disk, magnetic\ntape, any other magnetic medium, a CD-ROM, CDRW, DVD, any other optical medium, punch cards, paper tape, optical mark sheets, any other physical medium with patterns of holes or other optically recognizable indicia, a RAM, a PROM, an EPROM, a\nFLASH-EPROM, any other memory chip or cartridge, a carrier wave, or any other medium from which a computer can read.\n FIG. 16 illustrates a chip set 1600 upon which an embodiment of the invention may be implemented.  Chip set 1600 is programmed to provide a parametric representation of lane lines as described herein and includes, for instance, the processor and\nmemory components described with respect to FIG. 15 incorporated in one or more physical packages (e.g., chips).  By way of example, a physical package includes an arrangement of one or more materials, components, and/or wires on a structural assembly\n(e.g., a baseboard) to provide one or more characteristics such as physical strength, conservation of size, and/or limitation of electrical interaction.  It is contemplated that in certain embodiments the chip set can be implemented in a single chip.\n In one embodiment, the chip set 1600 includes a communication mechanism such as a bus 1601 for passing information among the components of the chip set 1600.  A processor 1603 has connectivity to the bus 1601 to execute instructions and process\ninformation stored in, for example, a memory 1605.  The processor 1603 may include one or more processing cores with each core configured to perform independently.  A multi-core processor enables multiprocessing within a single physical package. \nExamples of a multi-core processor include two, four, eight, or greater numbers of processing cores.  Alternatively or in addition, the processor 1603 may include one or more microprocessors configured in tandem via the bus 1601 to enable independent\nexecution of instructions, pipelining, and multithreading.  The processor 1603 may also be accompanied with one or more specialized components to perform certain processing functions and tasks such as one or more digital signal processors (DSP) 1607, or\none or more application-specific integrated circuits (ASIC) 1609.  A DSP 1607 typically is configured to process real-world signals (e.g., sound) in real time independently of the processor 1603.  Similarly, an ASIC 1609 can be configured to performed\nspecialized functions not easily performed by a general purposed processor.  Other specialized components to aid in performing the inventive functions described herein include one or more field programmable gate arrays (FPGA) (not shown), one or more\ncontrollers (not shown), or one or more other special-purpose computer chips.\n The processor 1603 and accompanying components have connectivity to the memory 1605 via the bus 1601.  The memory 1605 includes both dynamic memory (e.g., RAM, magnetic disk, writable optical disk, etc.) and static memory (e.g., ROM, CD-ROM,\netc.) for storing executable instructions that when executed perform the inventive steps described herein to provide a parametric representation of lane lines.  The memory 1605 also stores the data associated with or generated by the execution of the\ninventive steps.\n FIG. 17 is a diagram of exemplary components of a mobile station (e.g., handset) capable of operating in the system of FIG. 1, according to one embodiment.  Generally, a radio receiver is often defined in terms of front-end and back-end\ncharacteristics.  The front-end of the receiver encompasses all of the Radio Frequency (RF) circuitry whereas the back-end encompasses all of the base-band processing circuitry.  Pertinent internal components of the telephone include a Main Control Unit\n(MCU) 1703, a Digital Signal Processor (DSP) 1705, and a receiver/transmitter unit including a microphone gain control unit and a speaker gain control unit.  A main display unit 1707 provides a display to the user in support of various applications and\nmobile station functions that offer automatic contact matching.  An audio function circuitry 1709 includes a microphone 1711 and microphone amplifier that amplifies the speech signal output from the microphone 1711.  The amplified speech signal output\nfrom the microphone 1711 is fed to a coder/decoder (CODEC) 1713.\n A radio section 1715 amplifies power and converts frequency in order to communicate with a base station, which is included in a mobile communication system, via antenna 1717.  The power amplifier (PA) 1719 and the transmitter/modulation\ncircuitry are operationally responsive to the MCU 1703, with an output from the PA 1719 coupled to the duplexer 1721 or circulator or antenna switch, as known in the art.  The PA 1719 also couples to a battery interface and power control unit 1720.\n In use, a user of mobile station 1701 speaks into the microphone 1711 and his or her voice along with any detected background noise is converted into an analog voltage.  The analog voltage is then converted into a digital signal through the\nAnalog to Digital Converter (ADC) 1723.  The control unit 1703 routes the digital signal into the DSP 1705 for processing therein, such as speech encoding, channel encoding, encrypting, and interleaving.  In one embodiment, the processed voice signals\nare encoded, by units not separately shown, using a cellular transmission protocol such as global evolution (EDGE), general packet radio service (GPRS), global system for mobile communications (GSM), Internet protocol multimedia subsystem (IMS),\nuniversal mobile telecommunications system (UMTS), etc., as well as any other suitable wireless medium, e.g., microwave access (WiMAX), Long Term Evolution (LTE) networks, code division multiple access (CDMA), wireless fidelity (WiFi), satellite, and the\nlike.\n The encoded signals are then routed to an equalizer 1725 for compensation of any frequency-dependent impairments that occur during transmission though the air such as phase and amplitude distortion.  After equalizing the bit stream, the\nmodulator 1727 combines the signal with a RF signal generated in the RF interface 1729.  The modulator 1727 generates a sine wave by way of frequency or phase modulation.  In order to prepare the signal for transmission, an up-converter 1731 combines the\nsine wave output from the modulator 1727 with another sine wave generated by a synthesizer 1733 to achieve the desired frequency of transmission.  The signal is then sent through a PA 1719 to increase the signal to an appropriate power level.  In\npractical systems, the PA 1719 acts as a variable gain amplifier whose gain is controlled by the DSP 1705 from information received from a network base station.  The signal is then filtered within the duplexer 1721 and optionally sent to an antenna\ncoupler 1735 to match impedances to provide maximum power transfer.  Finally, the signal is transmitted via antenna 1717 to a local base station.  An automatic gain control (AGC) can be supplied to control the gain of the final stages of the receiver. \nThe signals may be forwarded from there to a remote telephone which may be another cellular telephone, other mobile phone or a land-line connected to a Public Switched Telephone Network (PSTN), or other telephony networks.\n Voice signals transmitted to the mobile station 1701 are received via antenna 1717 and immediately amplified by a low noise amplifier (LNA) 1737.  A down-converter 1739 lowers the carrier frequency while the demodulator 1741 strips away the RF\nleaving only a digital bit stream.  The signal then goes through the equalizer 1725 and is processed by the DSP 1705.  A Digital to Analog Converter (DAC) 1743 converts the signal and the resulting output is transmitted to the user through the speaker\n1745, all under control of a Main Control Unit (MCU) 1703--which can be implemented as a Central Processing Unit (CPU) (not shown).\n The MCU 1703 receives various signals including input signals from the keyboard 1747.  The keyboard 1747 and/or the MCU 1703 in combination with other user input components (e.g., the microphone 1711) comprise a user interface circuitry for\nmanaging user input.  The MCU 1703 runs a user interface software to facilitate user control of at least some functions of the mobile station 1701 to provide a parametric representation of lane lines.  The MCU 1703 also delivers a display command and a\nswitch command to the display 1707 and to the speech output switching controller, respectively.  Further, the MCU 1703 exchanges information with the DSP 1705 and can access an optionally incorporated SIM card 1749 and a memory 1751.  In addition, the\nMCU 1703 executes various control functions required of the station.  The DSP 1705 may, depending upon the implementation, perform any of a variety of conventional digital processing functions on the voice signals.  Additionally, DSP 1705 determines the\nbackground noise level of the local environment from the signals detected by microphone 1711 and sets the gain of microphone 1711 to a level selected to compensate for the natural tendency of the user of the mobile station 1701.\n The CODEC 1713 includes the ADC 1723 and DAC 1743.  The memory 1751 stores various data including call incoming tone data and is capable of storing other data including music data received via, e.g., the global Internet.  The software module\ncould reside in RAM memory, flash memory, registers, or any other form of writable computer-readable storage medium known in the art including non-transitory computer-readable storage medium.  For example, the memory device 1751 may be, but not limited\nto, a single memory, CD, DVD, ROM, RAM, EEPROM, optical storage, or any other non-volatile or non-transitory storage medium capable of storing digital data.\n An optionally incorporated SIM card 1749 carries, for instance, important information, such as the cellular phone number, the carrier supplying service, subscription details, and security information.  The SIM card 1749 serves primarily to\nidentify the mobile station 1701 on a radio network.  The card 1749 also contains a memory for storing a personal telephone number registry, text messages, and user specific mobile station settings.\n While the invention has been described in connection with a number of embodiments and implementations, the invention is not so limited but covers various obvious modifications and equivalent arrangements, which fall within the purview of the\nappended claims.  Although features of the invention are expressed in certain combinations among the claims, it is contemplated that these features can be arranged in any combination and order.", "application_number": "15476347", "abstract": " An approach is provided for parametric representation of lane lines. The\n     approach involves segmenting an input image into grid cells. The approach\n     also involves processing a portion of the input image in each grid cell\n     to detect lane lines. The approach further involves, for each grid cell\n     in which lane lines are detected, determining intercepts of the lane\n     lines with edges of the grid cell, and slopes of the lane lines at the\n     intercepts. The approach further involves generating a parametric\n     representation of the lane lines for each grid cell. The parametric\n     representation encodes the intercepts and slopes into a data structure\n     for each grid cell. The approach further involves providing an output\n     parametric representation for the input image that aggregates the\n     parametric representations of each grid cell.\n", "citations": ["9171485", "20060078205", "20090138188", "20120072080", "20160221592", "20180304891"], "related": []}, {"id": "20180288303", "patent_code": "10375289", "patent_name": "System and method for providing autonomous photography and videography", "year": "2019", "inventor_and_country_data": " Inventors: \nWang; Mengqiu (HangZhou, CN), Liu; Lixin (HangZhou, CN), Zhang; Tong (HangZhou, CN), Lu; Jia (HangZhou, CN)  ", "description": "TECHNICAL FIELD\n This invention relates generally to the aerial system field, and more specifically, to a system and method for providing automatic operation of an aerial system to follow an established trajectory and controllably provide photographic and\nvideographic features.\nBACKGROUND OF THE INVENTION\n Currently, there are two general approaches in allowing a user to control a drone to take photos and videos.  First, the drone may be controller using a remote controller (RC) or other mobile device, such as a mobile phone or tablet.  In these\ntypes of systems, a user must control the drone manually through physical interaction with the RC or mobile device.  This approach provides several shortcomings.  First, it requires hours or days or even months of practice for a user to become proficient\nin controlling the drone.  Additionally, not only does the user have to control operation, i.e., flight, of the drone, but the user must also control the camera to capture pictures and/or video.  Thus, the quality of the image or video is limited by not\nonly the skills of controlling the drone but also the controller's photography or videography experience.\n The second approach is an auto-follow feature.  Using the auto-follow feature, the drone or aerial system chooses and locks onto a person and automatically captures pictures and/or video.  Generally, this is the person that is operating the RC\nor mobile device, i.e., the \"owner\", but may also be another person, such as a person wearing or being associated with a tracking device.  This approach also has shortcomings.  In general, the movement or drone instructions are relatively simple, i.e.,\nfollow the user while capturing pictures and/or video.  Using this approach, the resulting pictures and/or video are limited, i.e., always the same or a single view.  For example, this approach usually results in pictures and/or video that consists of\nall front-view, back-view or side-view pictures and/or video of the user.  Furthermore, the distance between the drone and the target person is generally unchanged.  That is, all of the pictures and/or video are either distant-shots, mid-shot or\nclose-shot.  Furthermore, the interaction is also not intuitive.  The user needs to operate the drone on the smart phone and/or carry a device for tracking.  Furthermore, the auto-follow feature is generally focused on, or locked onto, one person.  This\ninteraction is always locked to one person, and thus, does not work well for capturing images and/or video of groups of people, such as, dancers, people at a large gathering, team athletics, e.g., basketball.\n The present invention is aimed at one or more of the problems identified above.\nSUMMARY OF THE INVENTION\n In one aspect of the present invention, an aerial system is provided.  The aerial system includes a body, a lift mechanism coupled to the body, a camera and a processing system.  The camera is controllably mounted to the body by an actuation\nsystem.  The processing system is coupled to the lift mechanism, the camera, and the actuation system and is configured to establish a desired flight trajectory, to detect a target, and to control the flight of the aerial system as a function of the\ndesired flight trajectory relative to the target using the lift mechanism.  The processing system is further configured to control the camera to capture pictures and/or video.\n In another aspect of the present invention, a method for operating an aerial system is provided.  The aerial system includes a body, a lift mechanism, a camera, and a body by an actuation system.  The method including the steps of establishing,\nby the processing system, a desired flight trajectory; detecting a target; and controlling, by the processing system, the flight of the aerial system as a function of the desired flight trajectory relative to the target using the lift mechanism.  The\nmethod also includes the step of controlling, by the processing system, the camera to capture pictures and/or video. BRIEF DESCRIPTION OF THE FIGURES\n FIG. 1 is a schematic representation of an aerial system and a system for controlling the aerial system, according to an embodiment of the present invention.\n FIG. 2 is a picture of an exemplary aerial system, according to an embodiment of the present invention.\n FIG. 3 is a picture of an exemplary optical system, according to an embodiment of the present invention.\n FIG. 4 is a second schematic representation of the aerial system, according to an embodiment of the present invention.\n FIG. 5 is a third schematic representation of the system for controlling the aerial system and the aerial system, according to an embodiment of the present invention.\n FIG. 6 is a schematic representation of an aerial system including an obstacle detection and avoidance system, according to an embodiment of the present invention.\n FIG. 7 is a block diagram of an autonomous photography and/or videography system, according to an embodiment of the present invention.\n FIG. 8 is a flow diagram of a method associated with the autonomous photography and/or videography system of FIG. 7.\nDETAILED DESCRIPTION OF THE INVENTION\n The following description of the embodiments of the invention is not intended to limit the invention to these embodiments, but rather to enable any person skilled in the art to make and use this invention.  With reference to the drawings and in\noperation, system 10 for controlling an aerial system 12, for example a drone, is provided.  The system 10 may include a remote device 14 with a control client 16.  The control client 16 provides a user interface (see below) that allows a user 18 to send\ninstructions to the aerial system 12 to control operation thereof.  As discussed in more depth below, the aerial system 12 includes one or more cameras (see below) for obtaining pictures and/or video which may be sent to the remote device 14 and/or\nstored in memory on the aerial system 12.\n The aerial system 12 may include one or more sensors (see below) for detecting or sensing operations or actions, i.e., expressions, performed by the user 18 to control operation of the aerial system 12 (see below) without direct or physical\ninteraction with the remote device 14.  In controller-free embodiments, the entire control loop from start (release and hover) to finish (grab and go), as well as controlling motion of the aerial system 12 and trigger of events, e.g., taking pictures and\nvideo, are performed solely on board the aerial system 12 without involvement of the remote device 14.  In some such embodiments or systems 10, a remote device 14 may not be provided or included.\n In some embodiments, the remote device 14 includes one or more sensors that detect or sense operation or actions performed by the user 18 to control operation of the aerial system 12 without physical interaction with the remote device 14 under\ncertain conditions, for example, when the aerial system 12 is too far from the user 18.\n In one aspect of the present invention, the aerial system 12 includes a processing system that is configured to establish a desired flight trajectory, to detect a target, and to control the flight of the aerial system as a function of the\ndesired flight trajectory relative to the target using the lift mechanism.  The processing system is further configured to control the camera to capture pictures and/or video.\n Overview of the System 10 and the Aerial System 12\n An exemplary aerial system 12 and control system 10 is shown in FIGS. 1-5.  The control client 16 of the aerial system 12 functions to receive data from the aerial system 12, including video images and/or video, and control visual display on the\nremote device 14.  The control client 16 may also receive operation instructions and facilitate aerial system 12 remote control based on operation instructions.  The control client 16 is preferably configured to execute on a remote device 14, but can\nalternatively be configured to execute on the aerial system 12 or on any other suitable system.  As discussed above, and more fully below, the aerial system 12 may be controlled solely without direct or physical interaction with the remote device 14.\n The control client 16 can be a native application (e.g., a mobile application), a browser application, an operating system application, or be any other suitable construct.\n The remote device 14 executing the control client 16 functions to display the data (e.g., as instructed by the control client 16), receive user inputs, compute the operation instructions based on the user inputs (e.g., as instructed by the\ncontrol client 16), send operation instructions to the aerial system 12, store control client (16) information (e.g., associated aerial system identifiers, security keys, user account information, user account preferences, etc.), or perform any other\nsuitable functionality.  The remote device 14 can be a user device (e.g., smartphone, tablet, laptop, etc.), a networked server system, or be any other suitable remote computing system.  The remote device 14 can include one or more: outputs, inputs,\ncommunication systems, sensors, power sources, processing systems (e.g., CPU, memory, etc.), or any other suitable component.  Outputs can include: displays (e.g., LED display, OLED display, LCD, etc.), audio speakers, lights (e.g., LEDs), tactile\noutputs (e.g., a tixel system, vibratory motors, etc.), or any other suitable output.  Inputs can include: touchscreens (e.g., capacitive, resistive, etc.), a mouse, a keyboard, a motion sensor, a microphone, a biometric input, a camera, or any other\nsuitable input.  Communication systems can include wireless connections, such as radios supporting: long-range systems (e.g., Wi-Fi, cellular, WLAN, WiMAX, microwave, IR, radio frequency, etc.), short-range systems (e.g., BLE, BLE long range, NFC,\nZigBee, RF, audio, optical, etc.), or any other suitable communication system.  Sensors can include: orientation sensors (e.g., accelerometer, gyroscope, etc.), ambient light sensors, temperature sensors, pressure sensors, optical sensors, acoustic\nsensors, or any other suitable sensor.  In one variation, the remote device 14 can include a display (e.g., a touch-sensitive display including a touchscreen overlaying the display), a set of radios (e.g., Wi-Fi, cellular, BLE, etc.), and a set of\norientation sensors.  However, the remote device 14 can include any suitable set of components.\n The aerial system 12 functions to fly within a physical space, capture video, stream the video in near-real time to the remote device 14, and operate based on operation instructions received from the remote device 14.\n The aerial system 12 can additionally process the video (e.g., video frames) prior to streaming the video to the remote device 14 and/or audio received from an onboard audio sensor; generate and automatically operate based on its own operation\ninstructions (e.g., to automatically follow a subject); or perform any other suitable functionality.  The aerial system 12 can additionally function to move the optical sensor's field of view within the physical space.  For example, the aerial system 12\ncan control macro movements (e.g., large FOV changes, on the order of meter adjustments), micro movements (e.g., small FOV changes, on the order of millimeter or centimeter adjustments), or any other suitable movement.\n The aerial system 12 can perform certain functionality based on onboard processing of sensor data from onboard sensors.  This functionality may include, but is not limited to: Take-off and landing; Owner recognition; Facial recognition; Speech\nrecognition; Facial expression and gesture recognition; and, Control, e.g., motion, of the aerial system based on owner, facial, expression and gesture recognition, and speech recognition.\n As shown in FIGS. 2-5, the aerial system 12 (e.g., drone) can include a body 20, a processing system 22, a communication system 24, an optical system 26, and an actuation mechanism 28 mounting the optical system 26 to the body 20.  The aerial\nsystem 12 can additionally or alternatively include lift mechanisms, sensors, power system, or any other suitable component (see below).\n The body 20 of the aerial system 12 functions to mechanically protect and/or retain the aerial system components.  The body 20 can define a lumen, be a platform, or have any suitable configuration.  The body 20 can be enclosed, open (e.g., a\ntruss), or have any suitable construction.  The body 20 can be made of metal, plastic (e.g., polymer), carbon composite, or any other suitable material.  The body 20 can define a longitudinal axis, a lateral axis, a transverse axis, a front end, a back\nend (e.g., opposing the front end along the longitudinal axis), a top, a bottom (e.g., opposing the top along the transverse axis), or any other suitable reference.  In one variation, while in flight, a transverse axis of the body 20 can be substantially\nparallel a gravity vector (e.g., perpendicular a ground plane) and the body's longitudinal and lateral axes can be substantially perpendicular the gravity vector (e.g., parallel the ground plane).  However, the body 20 can be otherwise configured.\n The processing system 22 of the aerial system 12 functions to control aerial system operation.  The processing system 22 can: receive operation instructions from the communication system 24, interpret the operation instructions into machine\ninstructions, and control aerial system components based on the machine instructions (individually or as a set).  The processing system 22 can additionally or alternatively process the images recorded by the camera, stream images to the remote device 14\n(e.g., in real- or near-real time), or perform any other suitable functionality.  The processing system 22 can include one or more: processors 30 (e.g., CPU, GPU, etc.), memory (e.g., Flash, RAM, etc.), or any other suitable processing component.  In one\nvariation, the processing system 22 can additionally include dedicated hardware that automatically processes the images (e.g., de-warps the image, filters the image, crops the image, etc.) prior to transmission to the remote device 14.  The processing\nsystem 22 is preferably connected to the active components of the aerial system 12 and mounted to the body 20, but can alternatively be otherwise related to aerial system components.\n The communication system 24 of the aerial system 12 functions to send and/or receive information from the remote device 14.  The communication system 24 is preferably connected to the processing system 22, such that the communication system 24\nsends and/or receives data form the processing system 22, but can alternatively be connected to any other suitable component.  The aerial system 12 can include one or more communication systems 24 of one or more types.  The communication system 24 can\ninclude wireless connections, such as radios supporting: long-range systems (e.g., Wi-Fi, cellular, WLAN, WiMAX, microwave, IR, radio frequency, etc.), short-range systems (e.g., BLE, BLE long range, NFC, ZigBee, RF, audio, optical, etc.), or any other\nsuitable communication system 24.  The communication system 24 preferably shares at least one system protocol (e.g., BLE, RF, etc.) with the remote device 14, but can alternatively communicate with the remote device 14 via an intermediary communication\nsystem (e.g., a protocol translation system).  However, the communication system 24 can be otherwise configured.\n The optical system 26 of the aerial system 12 functions to record images of the physical space proximal the aerial system 12.  The optical system 26 is preferably mounted to the body 20 via the actuation mechanism 28, but can alternatively be\nstatically mounted to the body 20, removably mounted to the body 20, or otherwise mounted to the body 20.  The optical system 26 is preferably mounted to the front end of the body 20, but can optionally be mounted to the bottom (e.g., proximal the\nfront), top, back end, or any other suitable portion of the body 20.  The optical system 26 is preferably connected to the processing system 30, but can alternatively be connected to the communication system 24 or to any other suitable system.  The\noptical system 26 can additionally include dedicated image processing hardware that automatically processes images recorded by the camera prior to transmission to the processor or other endpoint.  The aerial system 12 can include one or more optical\nsystems 26 of same or different type, mounted to the same or different position.  In one variation, the aerial system 12 includes a first optical system 26, mounted to the front end of the body 20, and a second optical system 26, mounted to the bottom of\nthe body 20.  The first optical system 26 can actuate about a pivotal support, and the second optical system 26 can be substantially statically retained relative to the body 20, with the respective active surface substantially parallel the body bottom. \nThe first optical sensor 36 can be high-definition, while the second optical sensor 36 can be low definition.  However, the optical system 26 can be otherwise configured.\n The optical system 26 can include one or more optical sensors 36 (see FIG. 5).  The one or more optical sensors 36 can include: a single lens camera (e.g., CCD camera, CMOS camera, etc.), a stereo-camera, a hyperspectral camera, a multispectral\ncamera, or any other suitable image sensor.  However, the optical system 26 can be any other suitable optical system 26.  The optical system 26 can define one or more active surfaces that receive light, but can alternatively include any other suitable\ncomponent.  For example, an active surface of a camera can be an active surface of a camera sensor (e.g., CCD sensor, CMOS sensor, etc.), preferably including a regular array of sensor pixels.  The camera sensor or other active surface is preferably\nsubstantially planar and rectangular (e.g., having a first sensor edge, a second sensor edge opposing the first sensor edge, and third and fourth sensor edges each perpendicular to and extending from the first sensor edge to the second sensor edge), but\ncan alternatively have any suitable shape and/or topography.  The optical sensor 36 can produce an image frame.  The image frame preferably corresponds with the shape of the active surface (e.g., rectangular, having a first and second frame edge opposing\neach other, etc.), more preferably defining a regular array of pixel locations, each pixel location corresponding to a sensor pixel of the active surface and/or pixels of the images sampled by the optical sensor 36, but can alternatively have any\nsuitable shape.  The image frame preferably defines aspects of the images sampled by the optical sensor 36 (e.g., image dimensions, resolution, pixel size and/or shape, etc.).  The optical sensor 36 can optionally include a zoom lens, digital zoom,\nfisheye lens, filter, or any other suitable active or passive optical adjustment.  Application of the optical adjustment can be actively controlled by the controller, manually controlled by the user 18 (e.g., wherein the user manually sets the\nadjustment), controlled by the remote device 14, or otherwise controlled.  In one variation, the optical system 26 can include a housing enclosing the remainder of the optical system 26 components, wherein the housing is mounted to the body 20.  However,\nthe optical system 26 can be otherwise configured.\n The actuation mechanism 28 of the aerial system 12 functions to actionably mount the optical system 26 to the body 20.  The actuation mechanism 28 can additionally function to dampen optical sensor vibration (e.g., mechanically stabilize the\nresultant image), accommodate for aerial system 12 roll, or perform any other suitable functionality.  The actuation mechanism 28 can be active (e.g., controlled by the processing system), passive (e.g., controlled by a set of weights, spring elements,\nmagnetic elements, etc.), or otherwise controlled.  The actuation mechanism 28 can rotate the optical system 26 about one or more axes relative to the body 20, translate the optical system 26 along one or more axes relative to the body 20, or otherwise\nactuate the optical system 26.  The optical sensor(s) 36 can be mounted to the support along a first end, along an optical sensor back (e.g., opposing the active surface), through the optical sensor body, or along any other suitable portion of the\noptical sensor 36.\n In one variation, the actuation mechanism 28 can include a motor (not shown) connected to a single pivoted support (e.g., gimbal), wherein the motor pivots the support about the rotational (or gimbal) axis 34 based on instructions received from\nthe controller.  The support is preferably arranged with the rotational axis substantially parallel the lateral axis of the body 20, but can alternatively be arranged with the rotational axis at any other suitable orientation relative to the body 20. \nThe support is preferably arranged within a recessed cavity defined by the body 20, wherein the cavity further encompasses the optical sensor 36 but can alternatively be arranged along the body 20 exterior or arranged at any other suitable portion of the\nbody 20.  The optical sensor 36 is preferably mounted to the support with the active surface substantially parallel the rotational axis (e.g., with the lateral axis, or axis parallel the lateral axis of the body 20, substantially parallel the rotational\naxis), but can alternatively be arranged with the active surface arranged at any suitable angle to the rotational axis.\n The motor is preferably an electric motor, but can alternatively be any other suitable motor.  Examples of electric motors that can be used include: DC motors (e.g., brushed motors), EC motors (e.g., brushless motors), induction motor,\nsynchronous motor, magnetic motor, or any other suitable electric motor.  The motor is preferably mounted to the body 20 (e.g., the body interior), electrically connected to and controlled by the processing system 22, and electrically connected to and\npowered by a power source or system 38.  However, the motor can be otherwise connected.  The actuation mechanism 28 preferably includes a single motor-support set, but can alternatively include multiple motor-support sets, wherein auxiliary motor-support\nsets can be arranged orthogonal (or at any other suitable angle to) the first motor-support set.\n In a second variation, the actuation mechanism 28 can include a set of pivoted supports and weights connected to the optical sensor 36 offset from the optical sensor center of gravity, wherein the actuation mechanism 28 passively stabilizes the\noptical sensor 36.\n A lift mechanism 40 of the aerial system 12 functions to enable aerial system flight.  The lift mechanism 40 preferably includes a set propeller blades 42 driven by a motor (not shown), but can alternatively include any other suitable propulsion\nmechanism.  The lift mechanism 40 is preferably mounted to the body 20 and controlled by the processing system 22, but can alternatively be otherwise mounted to the aerial system 12 and/or controlled.  The aerial system 12 can include multiple lift\nmechanisms 40.  In one example, the aerial system 12 includes four lift mechanisms 40 (e.g., two pairs of lift mechanisms 40), wherein the lift mechanisms 40 are substantially evenly distributed about the perimeter of the aerial system 12 (e.g., wherein\nthe lift mechanisms 40 of each pair oppose each other across the body 20).  However, the lift mechanisms 40 can be otherwise configured.\n Additional sensors 44 of the aerial system 12 function to record signals indicative of aerial system operation, the ambient environment surrounding the aerial system 12 (e.g., the physical space proximal the aerial system 12), or any other\nsuitable parameter.  The sensors 44 are preferably mounted to the body 20 and controlled by the processing system 22, but can alternatively be mounted to any other suitable component and/or otherwise controlled.  The aerial system 12 can include one or\nmore sensors 36, 44.  Examples of sensors that can be used include: orientation sensors (e.g., accelerometer, gyroscope, etc.), ambient light sensors, temperature sensors, pressure sensors, optical sensors, acoustic sensors (e.g., microphones), voltage\nsensors, current sensors, or any other suitable sensor.\n The power supply 38 of the aerial system 12 functions to power the active components of the aerial system 12.  The power supply 38 is preferably mounted to the body 20, and electrically connected to all active components of the aerial system 12\n(e.g., directly or indirectly), but can be otherwise arranged.  The power supply 38 can be a primary battery, secondary battery (e.g., rechargeable battery), fuel cell, energy harvester (e.g., solar, wind, etc.), or be any other suitable power supply. \nExamples of secondary batteries that can be used include: a lithium chemistry (e.g., lithium ion, lithium ion polymer, etc.), nickel chemistry (e.g., NiCad, NiMH, etc.), or batteries with any other suitable chemistry.\n The aerial system(s) 12, and can optionally be used with a remote computing system, or with any other suitable system.  The aerial system 12 functions to fly, and can additionally function to take photographs, deliver loads, and/or relay\nwireless communications.  The aerial system 12 is preferably a rotorcraft (e.g., quadcopter, helicopter, cyclocopter, etc.), but can alternatively be a fixed-wing aircraft, aerostat, or be any other suitable aerial system 12.  The aerial system 12 can\ninclude a lift mechanism 40, a power supply 38, sensors 36, 44, a processing system 22, a communication system 24, a body 20, and/or include any other suitable component.\n The lift mechanism 40 of the aerial system 12 functions to provide lift, and preferably includes a set of rotors driven (individually or collectively) by one or more motors.  Each rotor is preferably configured to rotate about a corresponding\nrotor axis, define a corresponding rotor plane normal to its rotor axis, and sweep out a swept area on its rotor plane.  The motors are preferably configured to provide sufficient power to the rotors to enable aerial system flight, and are more\npreferably operable in two or more modes, at least one of which includes providing sufficient power for flight and at least one of which includes providing less power than required for flight (e.g., providing zero power, providing 10% of a minimum flight\npower, etc.).  The power provided by the motors preferably affects the angular velocities at which the rotors rotate about their rotor axes.  During aerial system flight, the set of rotors are preferably configured to cooperatively or individually\ngenerate (e.g., by rotating about their rotor axes) substantially all (e.g., more than 99%, more than 95%, more than 90%, more than 75%) of the total aerodynamic force generated by the aerial system 12 (possibly excluding a drag force generated by the\nbody 20 such as during flight at high airspeeds).  Alternatively, or additionally, the aerial system 12 can include any other suitable flight components that function to generate forces for aerial system flight, such as jet engines, rocket engines,\nwings, solar sails, and/or any other suitable force-generating components.\n In one variation, the aerial system 12 includes four rotors, each arranged at a corner of the aerial system body.  The four rotors are preferably substantially evenly dispersed about the aerial system body, and each rotor plane is preferably\nsubstantially parallel (e.g., within 10 degrees) a lateral plane of the aerial system body (e.g., encompassing the longitudinal and lateral axes).  The rotors preferably occupy a relatively large portion of the entire aerial system 12 (e.g., 90%, 80%,\n75%, or majority of the aerial system footprint, or any other suitable proportion of the aerial system 12).  For example, the sum of the square of the diameter of each rotor can be greater than a threshold amount (e.g., 10%, 50%, 75%, 90%, 110%, etc.) of\nthe convex hull of the projection of the aerial system 12 onto a primary plane of the system (e.g., the lateral plane).  However, the rotors can be otherwise arranged.\n The power supply 38 of the aerial system 12 functions to power the active components of the aerial system 12 (e.g., lift mechanism's motors, etc.).  The power supply 38 can be mounted to the body 20 and connected to the active components, or be\notherwise arranged.  The power supply 38 can be a rechargeable battery, secondary battery, primary battery, fuel cell, or be any other suitable power supply.\n The sensors 36, 44 of the aerial system 12 function to acquire signals indicative of the aerial system's ambient environment and/or aerial system operation.  The sensors 36, 44 are preferably mounted to the body 20, but can alternatively be\nmounted to any other suitable component.  The sensors 36, 44 are preferably powered by the power supply 38 and controlled by the processor, but can be connected to and interact with any other suitable component.  The sensors 36, 44 can include one or\nmore: cameras (e.g., CCD, CMOS, multispectral, visual range, hyperspectral, stereoscopic, etc.), orientation sensors (e.g., inertial measurement sensors, accelerometer, gyroscope, altimeter, magnetometer, etc.), audio sensors (e.g., transducer,\nmicrophone, etc.), barometers, light sensors, temperature sensors, current sensor (e.g., Hall effect sensor), air flow meter, voltmeters, touch sensors (e.g., resistive, capacitive, etc.), proximity sensors, force sensors (e.g., strain gauge meter, load\ncell), vibration sensors, chemical sensors, sonar sensors, location sensor (e.g., GPS, GNSS, triangulation, etc.), or any other suitable sensor.  In one variation, the aerial system 12 includes a first camera mounted (e.g., statically or rotatably) along\na first end of the aerial system body with a field of view intersecting the lateral plane of the body; a second camera mounted along the bottom of the aerial system body with a field of view substantially parallel the lateral plane; and a set of\norientation sensors, such as an altimeter and accelerometer.  However, the system can include any suitable number of any sensor type.\n The processing system 22 of the aerial system 12 functions to control aerial system operation.  The processing system 22 can perform the method; stabilize the aerial system 12 during flight (e.g., selectively operate the rotors to minimize\naerial system wobble in-flight); receive, interpret, and operate the aerial system 12 based on remote control instructions; or otherwise control aerial system operation.  The processing system 22 is preferably configured to receive and interpret\nmeasurements sampled by the sensors 36, 44, more preferably by combining measurements sampled by disparate sensors (e.g., combining camera and accelerometer data).  The aerial system 12 can include one or more processing systems, wherein different\nprocessors can perform the same functionality (e.g., function as a multi-core system), or be specialized.  The processing system 22 can include one or more: processors (e.g., CPU, GPU, microprocessor, etc.), memory (e.g., Flash, RAM, etc.), or any other\nsuitable component.  The processing system 22 is preferably mounted to the body 20, but can alternatively be mounted to any other suitable component.  The processing system 22 is preferably powered by the power supply 38, but can be otherwise powered. \nThe processing system 22 is preferably connected to and controls the sensors 36, 44, communication system 24, and lift mechanism 40, but can additionally or alternatively be connected to and interact with any other suitable component.\n The communication system 24 of the aerial system 12 functions to communicate with one or more remote computing systems.  The communication system 24 can be a long-range communication module, a short-range communication module, or any other\nsuitable communication module.  The communication system 24 can facilitate wired and/or wireless communication.  Examples of the communication system 24 include an 802.11x, Wi-Fi, Wi-Max, NFC, RFID, Bluetooth, Bluetooth Low Energy, ZigBee, cellular\ntelecommunications (e.g., 2G, 3G, 4G, LTE, etc.), radio (RF), wired connection (e.g., USB), or any other suitable communication system 24 or combination thereof.  The communication system 24 is preferably powered by the power supply 38, but can be\notherwise powered.  The communication system 24 is preferably connected to the processing system 22, but can additionally or alternatively be connected to and interact with any other suitable component.\n The body 20 of the aerial system 12 functions to support the aerial system components.  The body can additionally function to protect the aerial system components.  The body 20 preferably substantially encapsulates the communication system 24,\npower supply 38, and processing system 22, but can be otherwise configured.  The body 20 can include a platform, a housing, or have any other suitable configuration.  In one variation, the body 20 includes a main body housing the communication system 24,\npower supply 38, and processing system 22, and a first and second frame (e.g., cage) extending parallel the rotor rotational plane and arranged along a first and second side of the main body 20.  The frames can function as an intermediary component\nbetween the rotating rotors and a retention mechanism (e.g., retention mechanism such as a user's hand).  The frame can extend along a single side of the body 20 (e.g., along the bottom of the rotors, along the top of the rotors), along a first and\nsecond side of the body 20 (e.g., along the top and bottom of the rotors), encapsulate the rotors (e.g., extend along all sides of the rotors), or be otherwise configured.  The frames can be statically mounted or actuatably mounted to the main body 20.\n The frame can include one or more apertures (e.g., airflow apertures) fluidly connecting one or more of the rotors to an ambient environment, which can function to enable the flow of air and/or other suitable fluids between the ambient\nenvironment and the rotors (e.g., enabling the rotors to generate an aerodynamic force that causes the aerial system 12 to move throughout the ambient environment).  The apertures can be elongated, or can have comparable length and width.  The apertures\ncan be substantially identical, or can differ from each other.  The apertures are preferably small enough to prevent components of a retention mechanism (e.g., fingers of a hand) from passing through the apertures.  The geometrical transparency (e.g.,\nratio of open area to total area) of the frame near the rotors is preferably large enough to enable aerial system flight, more preferably enabling high-performance flight maneuvering.  For example, each aperture can be smaller than a threshold size\n(e.g., smaller than the threshold size in all dimensions, elongated slots narrower than but significantly longer than the threshold size, etc.).  In a specific example, the frame has a geometrical transparency of 80-90%, and the apertures (e.g., circles,\npolygons such as regular hexagons, etc.) each of define a circumscribed circle with a diameter of 12-16 mm.  However, the body can be otherwise configured.\n The body 20 (and/or any other suitable aerial system components) can define a retention region that can be retained by a retention mechanism (e.g., a human hand, an aerial system dock, a claw, etc.).  The retention region preferably surrounds a\nportion of one or more of the rotors, more preferably completely surrounding all of the rotors, thereby preventing any unintentional interaction between the rotors and a retention mechanism or other object near the aerial system 12.  For example, a\nprojection of the retention region onto an aerial system plane (e.g., lateral plane, rotor plane, etc.) can overlap (e.g., partially, completely, a majority of, at least 90% of, etc.) a projection of the swept area of one or more of the rotors (e.g.,\nswept area of a rotor, total swept area of the set of rotors, etc.) onto the same aerial system plane.\n The aerial system 12 can additionally include inputs (e.g., microphones, cameras, etc.), outputs (e.g., displays, speakers, light emitting elements, etc.), or any other suitable component.\n The remote computing system functions to receive auxiliary user inputs, and can additionally function to automatically generate control instructions for and send the control instructions to the aerial system(s) 12.  Each aerial system 12 can be\ncontrolled by one or more remote computing systems.  The remote computing system preferably controls the aerial system 12 through a client (e.g., a native application, browser application, etc.), but can otherwise control the aerial system 12.  The\nremote computing system can be a user device, remote server system, connected appliance, or be any other suitable system.  Examples of the user device include a tablet, smartphone, mobile phone, laptop, watch, wearable device (e.g., glasses), or any\nother suitable user device.  The user device can include power storage (e.g., a battery), processing systems (e.g., CPU, GPU, memory, etc.), user outputs (e.g., display, speaker, vibration mechanism, etc.), user inputs (e.g., a keyboard, touchscreen,\nmicrophone, etc.), a location system (e.g., a GPS system), sensors (e.g., optical sensors, such as light sensors and cameras, orientation sensors, such as accelerometers, gyroscopes, and altimeters, audio sensors, such as microphones, etc.), data\ncommunication system (e.g., a Wi-Fi module, BLE, cellular module, etc.), or any other suitable component.\n The system 10 may be configured for controller-free user drone interaction.  Normally, the aerial system, or drone, 12 requires a separate device, e.g., the remote device 14.  The remote device 14 may be embodied in different types of devices,\nincluding, but not limited to a ground station, remote control, or mobile phone, etc. .  . . In some embodiments, control of the aerial system 12 may be accomplished by the user through user expression without utilization of the remote device 14.  User\nexpression may include, but is not limited to, any action performed by the user that do not include physical interaction with the remote device 14, including thought (through brain wave measurement), facial expression (including eye movement), gesture\nand/or voice.  In such embodiments, user instructions are received directly via the optical sensors 36 and at least some of the other sensors 44 and processed by the onboard processing system 22 to control the aerial system 12.\n In some embodiments, the aerial system 12 may alternatively be controlled via the remote device 14.\n In at least one embodiment, the aerial system 12 may be controlled without physical interaction with the remote device 14, however, a display of the remote device 14 may be used to display images and/or video relayed from the aerial system 12\nwhich may aid the user 18 in controlling the aerial system 12.  In addition, sensors 36, 44 associated with the remote device 14, e.g., camera(s) and/or a microphone (not show) may relay data to the aerial system 12, e.g., when the aerial system 12 is\ntoo far away from the user 18.  The sensor data relayed from the remote device 14 to the aerial system 12 is used in the same manner as the sensor data from the on-board sensors 36, 44 are used to control the aerial system 12 using user expression.\n In this manner, the aerial system 12 may be fully controlled, from start to finish, either (1) without utilization of a remote device 14, or (2) without physical interaction with the remote device 14.  Control of the aerial system 12 based on\nuser instructions received at various on-board sensors 36, 44.  It should be noted that in the following discussion, utilization of on-board sensors 36, 44 may also include utilization of corresponding or similar sensors on the remote device 14.\n In general, the user 18 may utilize certain gestures and/or voice control to control take-off, landing, motion of the aerial system 12 during flight and other features, such as triggering of photo and/or video capturing.  As discussed above, the\naerial system 12 may provide the following features without utilization of, or processing by, a remote device 14: Take-off and landing; Owner recognition; Facial recognition; Speech recognition; Facial expression and gesture recognition; and, Control,\ne.g., motion, of the aerial system based on owner, facial, expression and gesture recognition, and speech recognition.\n As detailed above, the aerial system 12 includes an optical system 26 that includes one or more optical sensor 36, such as a camera.  The at least one on-board camera is configured for live video streaming and computer vision analysis. \nOptionally the aerial system 12 can have at least one depth sensor (or stereo-vision pair) for multi-pixel depth sensing.  Optionally the aerial system 12 can have at least one microphone on board for voice recognition and control.\n In general, in order to provide full control of the aerial system 12, a plurality of user/drone interactions or activities from start to end of an aerial session are provided.  The user/drone interactions, include, but are not limited to\ntake-off and landing, owner recognition gesture recognition, facial expression recognition, and voice control.\n With reference to FIG. 6, in another aspect of the present invention, the aerial system 12 may include an obstacle detection and avoidance system 50.  In one embodiment, the obstacle detection and avoidance system 50 includes the pair of\nultra-wide angle lens cameras 52A 52B.  As will be described more fully below, the pair of cameras 52A, 52B, are equipped coaxially at the center top and center bottom of the fuselage (see below).\n The method and/or system can confer several benefits over conventional systems.  First, the images recorded by the camera are processed on-board, in real- or near-real time.  This allows the robot to navigate using the images recorded by the\ncameras.\n The pair of cameras 52A, 52B are generally mounted or statically fixed to housing of the body 20.  A memory 54 and a vision processor 56 are connected to the pair of cameras 52A, 52B.  The system functions to sample images of a monitored region\nfor real- or near-real time image processing, such as depth analysis.  The system can additionally or alternatively generate 3D video, generate a map of the monitored region, or perform any other suitable functionality.\n The housing functions to retain the pair of cameras 52A, 52B in a predetermined configuration.  The system preferably includes a single housing that retains the pair of cameras 52A, 52B, but can alternatively include multiple housing pieces or\nany other suitable number of housing pieces.\n The pair of cameras 52A, 52B may function to sample signals of the ambient environment surrounding the system 12.  The pair of cameras 52A, 52B are arranged with the respective view cone of each camera overlapping a view cone of the other camera\n(see below).\n Each camera 52A, 52B can be a CCD camera, CMOS camera, or any other suitable type of camera.  The camera can be sensitive in the visible light spectrum, IR spectrum, or any other suitable spectrum.  The camera can be hyperspectral,\nmultispectral, or capture any suitable subset of bands.  The cameras can have a fixed focal length, adjustable focal length, or any other suitable focal length.  However, the camera can have any other suitable set of parameter values.  The cameras of the\nplurality can be identical or different.\n Each camera is preferably associated with a known location relative to a reference point (e.g., on the housing, a camera of the plurality, on the host robot, etc.), but can be associated with an estimated, calculated, or unknown location.  The\npair of cameras 52A, 52B are preferably statically mounted to the housing (e.g., through-holes in the housing), but can alternatively be actuatably mounted to the housing (e.g., by a joint).  The cameras can be mounted to the housing faces, edges,\nvertices, or to any other suitable housing feature.  The cameras can be aligned with, centered along, or otherwise arranged relative to the housing feature.  The camera can be arranged with an active surface perpendicular a housing radius or surface\ntangent, an active surface parallel a housing face, or be otherwise arranged.  Adjacent camera active surfaces can be parallel each other, at a non-zero angle to each other, lie on the same plane, be angled relative to a reference plane, or otherwise\narranged.  Adjacent cameras preferably have a baseline (e.g., inter-camera or axial distance, distance between the respective lenses, etc.) of 6.35 cm, but can be further apart or closer together.\n The cameras 52A, 52B may be connected to the same visual processing system and memory, but can be connected to disparate visual processing systems and/or memories.  The cameras are preferably sampled on the same clock, but can be connected to\ndifferent clocks (e.g., wherein the clocks can be synchronized or otherwise related).  The cameras are preferably controlled by the same processing system, but can be controlled by different processing systems.  The cameras are preferably powered by the\nsame power source (e.g., rechargeable battery, solar panel array, etc.; host robot power source, separate power source, etc.), but can be powered by different power sources or otherwise powered.\n The obstacle detection and avoidance system 50 may also include an emitter 58 that functions to illuminate a physical region monitored by the cameras 52A, 52B.  The system 50 can include one emitter 58 for one or more of the cameras 52A, 52B,\nmultiple emitters 58 for one or more of the cameras 52A, 52B, or any suitable number of emitters 58 in any other suitable configuration.  The emitter(s) 58 can emit modulated light, structured light (e.g., having a known pattern), collimated light,\ndiffuse light, or light having any other suitable property.  The emitted light can include wavelengths in the visible range, UV range, IR range, or in any other suitable range.  The emitter position (e.g., relative to a given camera) is preferably known,\nbut can alternatively be estimated, calculated, or otherwise determined.\n In a second variation, the obstacle detection and avoidance system 50 operates as a non-contact active 3D scanner.  The non-contact system is a time of flight sensor, including a camera and an emitter, wherein the camera records reflections (of\nthe signal emitted by the emitter) off obstacles in the monitored region and determines the distance between the system 50 and the obstacle based on the reflected signal.  The camera and emitter are preferably mounted within a predetermined distance of\neach other (e.g., several mm), but can be otherwise mounted.  The emitted light can be diffuse, structured, modulated, or have any other suitable parameter.  In a second variation, the non-contact system is a triangulation system, also including a camera\nand emitter.  The emitter is preferably mounted beyond a threshold distance of the camera (e.g., beyond several mm of the camera) and directed at a non-parallel angle to the camera active surface (e.g., mounted to a vertex of the housing), but can be\notherwise mounted.  The emitted light can be collimated, modulated, or have any other suitable parameter.  However, the system 50 can define any other suitable non-contact active system.  However, the pair of cameras can form any other suitable optical\nrange finding system.\n The memory 54 of the system 50 functions to store camera measurements.  The memory can additionally function to store settings; maps (e.g., calibration maps, pixel maps); camera positions or indices; emitter positions or indices; or any other\nsuitable set of information.  The system 50 can include one or more pieces of memory.  The memory is preferably nonvolatile (e.g., flash, SSD, eMMC, etc.), but can alternatively be volatile (e.g. RAM).  In one variation, the cameras 52A, 52B write to the\nsame buffer, wherein each camera is assigned a different portion of the buffer.  In a second variation, the cameras 52A, 52B write to different buffers in the same or different memory.  However, the cameras 52A, 52B can write to any other suitable\nmemory.  The memory 54 is preferably accessible by all processing systems of the system (e.g., vision processor, application processor), but can alternatively be accessible by a subset of the processing systems (e.g., a single vision processor, etc.).\n The vision processing system 56 of the system 50 functions to determine the distance of a physical point from the system.  The vision processing system 56 preferably determines the pixel depth of each pixel from a subset of pixels, but can\nadditionally or alternatively determine the object depth or determine any other suitable parameter of a physical point or collection thereof (e.g., object).  The vision processing system 56 preferably processes the sensor stream from the cameras 52A, 52B\n The vision processing system 56 may process each sensor stream at a predetermined frequency (e.g., 30 FPS), but can process the sensor streams at a variable frequency or at any other suitable frequency.  The predetermined frequency can be\nreceived from an application processing system 60, retrieved from storage, automatically determined based on a camera score or classification (e.g., front, side, back, etc.), determined based on the available computing resources (e.g., cores available,\nbattery level remaining, etc.), or otherwise determined.  In one variation, the vision processing system 56 processes multiple sensor streams at the same frequency.  In a second variation, the vision processing system 56 processes multiple sensor streams\nat different frequencies, wherein the frequencies are determined based on the classification assigned to each sensor stream (and/or source camera), wherein the classification is assigned based on the source camera orientation relative to the host robot's\ntravel vector.\n The application processing system 60 of the system 50 functions to determine the time multiplexing parameters for the sensor streams.  The application processing system 60 can additionally or alternatively perform object detection,\nclassification, tracking (e.g., optical flow), or any other suitable process using the sensor streams.  The application processing system 60 can additionally or alternatively generate control instructions based on the sensor streams (e.g., based on the\nvision processor output).  For example, navigation (e.g., using SLAM, RRT, etc.) or visual odometry processes can be performed using the sensor streams, wherein the system and/or host robot is controlled based on the navigation outputs.\n The application processing system 60 can additionally or alternatively receive control commands and operate the system 12 and/or host robot based on the commands.  The application processing system 60 can additionally or alternatively receive\nexternal sensor information and selectively operate the system and/or host robot based on the commands.  The application processing system 60 can additionally or alternatively determine robotic system kinematics (e.g., position, direction, velocity, and\nacceleration) based on sensor measurements (e.g., using sensor fusion).  In one example, the application processing system 60 can use measurements from an accelerometer and gyroscope to determine the traversal vector of the system and/or host robot\n(e.g., system direction of travel).  The application processing system 60 can optionally automatically generate control instructions based on the robotic system kinematics.  For example, the application processing system 60 can determine the location of\nthe system (in a physical volume) based on images from the cameras 52A, 52B, wherein the relative position (from the orientation sensors) and actual position and speed (determined from the images) can be fed into the flight control module.  In this\nexample, images from a downward-facing camera subset can be used to determine system translation (e.g., using optical flow), wherein the system translation can be further fed into the flight control module.  In a specific example, the flight control\nmodule can synthesize these signals to maintain the robot position (e.g., hover a drone).\n The application processing system 60 can include one or more application processors.  The application processor can be a CPU, GPU, microprocessor, or any other suitable processing system.  The application processing system 60 can implemented as\npart of, or separate from, the vision processing system 56, or be different from the vision processing system 56.  The application processing system 60 may be connected to the visual processing system 56 by one or more interface bridges.  The interface\nbridge can be a high-throughput and/or bandwidth connection, and can use a MIPI protocol (e.g., 2-input to 1-output camera aggregator bridges--expands number of cameras that can be connected to a vision processor), a LVDS protocol, a DisplayPort\nprotocol, an HDMI protocol, or any other suitable protocol.  Alternatively, or additionally, the interface bridge can be a low-throughout and/or bandwidth connection, and can use a SPI protocol, UART protocol, I2C protocol, SDIO protocol, or any other\nsuitable protocol.\n The system can optionally include an image signal processing unit (ISP) 62 that functions to pre-process the camera signals (e.g., images) before passing to vision processing system and/or application processing system.  The ISP 62 can process\nthe signals from all cameras, the signals from the camera subset, or signals any other suitable source.  The ISP 62 can auto-white balance, correct field shading, rectify lens distortion (e.g., dewarp), crop, select a pixel subset, apply a Bayer\ntransformation, demosaic, apply noise reduction, sharpen the image, or otherwise process the camera signals.  For example, the ISP 62 can select the pixels associated with an overlapping physical region between two cameras from images of the respective\nstreams (e.g., crop each image to only include pixels associated with the overlapping region shared between the cameras of a stereocamera pair).  The ISP 62 can be a system on a chip with multi-core processor architecture, be an ASIC, have ARM\narchitecture, be part of the vision processing system, be part of the application processing system, or be any other suitable processing system.\n The system can optionally include sensors 64 that function to sample signals indicative of system operation.  The sensor output can be used to determine system kinematics, process the images (e.g., used in image stabilization), or otherwise\nused.  The sensors 64 can be peripheral devices of the vision processing system 56, the application processing system 60, or of any other suitable processing system.  The sensors 64 are preferably statically mounted to the housing but can alternatively\nbe mounted to the host robot or to any other suitable system.  Sensors 64 can include: orientation sensors (e.g., IMU, gyroscope, accelerometer, altimeter, magnetometer), acoustic sensors (e.g., microphones, transducers), optical sensors (e.g., cameras,\nambient light sensors), touch sensors (e.g., force sensors, capacitive touch sensor, resistive touch sensor), location sensors (e.g., GPS system, beacon system, trilateration system), or any other suitable set of sensors.\n The system can optionally include inputs (e.g., a keyboard, touchscreen, microphone, etc.), outputs (e.g., speakers, lights, screen, vibration mechanism, etc.), communication system (e.g., a WiFi module, BLE, cellular module, etc.), power\nstorage (e.g., a battery), or any other suitable component.\n The system is preferably used with a host robot that functions to traverse within a physical space.  The host robot can additionally or alternatively receive remote control instructions and operate according to the remote control instructions. \nThe host robot can additionally generate remote content or perform any other suitable functionality.  The host robot can include one or more: communication modules, motive mechanisms, sensors, content-generation mechanisms, processing systems, reset\nmechanisms, or any other suitable set of components.  The host robot can be a drone, vehicle, robot, security camera, or be any other suitable remote-controllable system.  The motive mechanism can include a drivetrain, rotors, jets, treads, rotary joint,\nor any other suitable motive mechanism.  The application processing system is preferably the host robot processing system, but can alternatively be connected to the host robot processing system or be otherwise related.  In a specific example, the host\nrobot includes an aerial system (e.g., drone) with a WiFi module, a camera, and the application processing system.  The system can be mounted to the top of the host robot (e.g., as determined based on a gravity vector during typical operation), the\nbottom of the host robot, the front of the host robot, centered within the host robot, or otherwise mounted to the host robot.  The system can be integrally formed with the host robot, removably coupled to the host robot, or otherwise attached to the\nhost robot.  One or more systems can be used with one or more host robots.\n In another aspect of the present invention, a (sub) system and method 70 may be utilized to provide autonomous photography and/or videography to the aerial system 12.  The autonomous photography and/or videography system 70 may be implemented,\nat least in part, by the processing system 22, the optical system 26, the actuation system 28 and the lift mechanism 32.\n As will be discussed in more detail below, the autonomous photography and/or videography system 70 is configured to establish a desired flight trajectory, to detect a target, and to control the flight of the aerial system 12 as a function of the\ndesired flight trajectory relative to the target using the lift mechanism.  The autonomous photography and/or videography system 70 is further configured to control the camera to capture pictures and/or video.\n Further, the autonomous photography and/or videography system 70 may be operable to (1) automatically modify the camera angle and flight trajectory with the target in the picture without any interaction between the user and any device; (2)\nautomatically take photos or record videos without any interaction between the user and any device; and (3) automatically select good candidates of photos and/or video clips from raw photo/video material for further user editing or automatic editing\nprocedures.\n With reference to FIG. 7, in one embodiment the autonomous photography and/or videography system 70 includes an auto-detection and tracking module 72, an auto-shooting module 74, an auto-selection module 76 and an auto-editing and sharing module\n76.  As stated above, the modules 72, 74, 76, 78 may be implemented in part by a combination of software implemented vision algorithms and hardware, e.g., the processing system 22.  From a user perspective, the modules 72, 74, 76, 78 may provide a fully\nautonomous experience.  Alternatively, one or more of the modules 72, 74, 76, 78 may be used to provide a (less than fully autonomous) mode that allows the user to more easily take pictures or videos with the aerial system 12.\n After the aerial system 12 has launched, the auto-detection and tracking module 72 initiates a target detection process.  The target detection process will detect a target, such as a person or other item or object (see above).\n After the target has been detected/located, the auto-detection and tracking module 72 modifies the angle of one of the optical sensors or cameras 36 of the optical system 26 using the actuation system 28 and modifies the flight trajectory of the\naerial system 12 based on a selected flight trajectory.\n The optical sensor(s) 36 acts as a vision sensor for the auto-detection and tracking module 72.  The auto-detection and tracking module 72 may utilize a target detection and tracking algorithm to detect and locate the target from the video feed\nof the optical system 26 and a self-positioning fusion algorithm to integrate positioning data from various sensors 44.  By combining the information from the self-positioning sensor fusion algorithm and the target detection and tracking algorithm, the\nrelative position and velocity of the target to the aerial system 12 can be obtained.\n In one embodiment, the target detection and tracking algorithm may include one or more of the following techniques:\n (a) Tracker based techniques: TLD-tracker, KCF-tracker, Struck-tracker, CNN-based-tracker, etc.\n (b) Detector based techniques: face detection algorithms, like Haar+Adaboost, face recognition algorithms, like EigenFace, human body detection algorithms, like HOG+SVM or DPM, CNN-based-object-detection methods, etc.\n Additional sensor(s) may be attached to the target for even more reliable performance.  For example, a GPS sensor and an inertial measurement unit (IMU) may be included in the tracker device attaching to the target.  Then the information of the\nsensors may be transmitted via a wireless method such as Wi-Fi, or Bluetooth to the main aerial system 12.  The synchronized sensor info can be used as additional supplementary observation data for better assisting the vision based target detection and\ntracking algorithms.  The data can be used either in a filter based manner such as dumping the data into a EKF system, or in a supplementary manner such as using it as prior information for providing better tracking accuracy of the vision based tracker.\n In one embodiment, the self-positioning fusion algorithm may include an extended Kalman Filter (EKF) doing sensor filtering and fusion of accelerometer, gyroscope, magnetometer, barometer, optical flow sensor, GPS, proximity sensor, sonar/radar,\nTOF based range finder, etc;\n The same or additional vision sensor(s) providing visual odometry capability.  The vision sensor is preferably having a known and fixed relative pose to the body of the aerial system.  A movable vision sensor may also be provided (as long as its\nrelative pose to the body can be accurately monitored and updated promptly).  Extra inertial sensor measurements are preferred but not required.  If without synchronous readings from inertial measurement unit (IMU), techniques such as visual SLAM, and\nSVO may be applied.  If we do use the additional IMU info, then VIO and VIN can be applied.\n Once the (1) the aerial system self-positioning information by using self-positioning sensor fusion techniques, (2) gimbal angle(s), and (3) 2D target position from the vision sensor, have been established, an online estimation of absolute\nposition and velocity of the target, as well as the position and velocity of the target relative to the aerial system, may be derived.\n Then the system may apply proper control strategies to fly in a designed trajectory while aiming the target in the meantime.  Several different control strategies may be applied:\n (a) The aerial system 12 may simply follow the target from behind, keeping a fixed distance (indefinitely or for a finite amount of time);\n (b) The aerial system 12 may lead the target at the front while aiming the target, keeping a fixed distance (indefinitely or for a finite amount of time);\n (c) The aerial system 12 may orbit around the target at a fixed distance with a constant/varying speed (indefinitely or for a finite amount of time);\n (d) The aerial system 12 may move closer to or further away from certain camera aiming angle, with a constant/varying speed, for a finite amount of time;\n (e) The aerial system 12 may move in a certain direction (in world coordinates or in target coordinate) while the optical system 26 is aimed the target, with a constant/varying speed, for a finite amount of time;\n (f) The aerial system 12 may fix some degrees of freedom and only use some of its DOFs to track and aim the target, for example, it may stay at a certain 3D position in the air, and only track and aim the target by controlling its own yaw angle\nand the axes of its camera gimbal;\n (g) A piece of trajectory and/or a series of control commands may be performed by professional photographers and recorded as a candidate of pre-defined trajectory.  Data such as camera angle, relative distance and velocity of the target to the\naerial system, location of target in the scene, absolute position and velocity of the aerial system, etc. at each time stamp can be saved, then an online trajectory planning algorithm can be applied to generate control commands to replicate the same\ntrajectory;\n (h) A combination of any above control strategies (or other control strategies under same principle) in sequence, either in a pre-defined order, or in a pseudo random order.\n In one embodiment, one or more of these strategies may be presented to the user and selected as a desired flight trajectory.\n After the target has been identified, the auto-shooting module 74 will control the optical system 26 to automatically begin obtaining pictures and/or video, i.e., \"auto-shooting\".  While auto-shooting, the aerial system 12 or drone will fly on a\ndesigned flight trajectory with the camera angle automatically changing to maintain the target within the pictures and/or video.  Auto-shooting may be based on several mechanisms: auto light condition optimization, face movement analysis, expression\nanalysis, behavior analysis, pose analysis, condition analysis, composition analysis, and object analysis.  From video-taking perspective, the aerial system 12 or drone may also automatically move in a wide range, both low and high, close and distant,\nlift and right, front and back and side, to make the video more vivid.  The designated flight trajectory may be dynamically determined based on predetermined parameters and/or changing conditions based on sensor input.  In other words, the drone or\naerial system 12 could traverse a trajectory to simulate or emulate operation of the camera in a manner similar to a professional photographer or videographer.  Alternatively, the user can select one or more trajectories from a set of pre-designed routes\nor pre-defined routes.\n Further, in another aspect of the present invention, the auto-shooting module 74 has one or more modes.  For example, in one embodiment, the auto-shooting module 74 may have one the following modes: Mode 1: Taking a series of snapshots; Mode 2:\nTaking a continuous video; or, Mode 3: Taking a continuous video, at the same time taking a series of snapshots.  The mode may be selected by the user and/or be associated with a selected flight trajectory (see below).\n The auto-selection module 76 selects pictures and/or video (segments) from among the obtained (or captured) pictures and/video based on a set of predetermined parameters.  The selected pictures and/or video may be retained ad/or stored or\nalternatively, marked as being \"selected\".  The set of predetermined parameters may include, but is not limited to: blurriness, exposure, and/or composition.  For example, a blurriness detector may utilize a either a Laplacian of Gaussian filter or a\nvariance of Laplacian filter or other suitable filter.\n One example of vibration detector may utilize an inertial measurement unit or IMU (accelerometer, gyroscope, etc.) data, for a given section of data, pick a moving window time interval, calculate the variance/standard deviation within this\nmoving window, and compare it to a pre-defined threshold.\n A lower frequency vibration filter, i.e. video stability filter, can be realized by checking the 2D trajectory of the main target in the view, or by checking the sensor detected camera angle traces.  A stable video can better keep the target in\nthe view and/or keep a more stable camera angle.\n For pictures, pictures are selected and/or not-selected based on the predetermined parameters.  For videos, video segments may be selected and/or not selected based on the predetermined parameters.  Alternatively, the auto-selection module 76\nmay select sub-segments from a given video segment based on the predetermined parameters and crop (and save) the sub-segment(s) as a function of the predetermined parameters.\n In one aspect of the present invention, the auto-selection module 76 may be implemented.  This module can work on a drone or on a smart phone.  It is capable of automatically selecting photos or a truncated video clip (for example,\n3-second/6-second/10-second video snippet), from a longer raw video material.  Here are some rules for judging a piece of footage/video snippet: blurriness, video-stability, exposure, composition, etc. Technical points are as follows:\n Over/under exposure detector: Calculate the exposure value at regions of interest, and check whether the values are below the lower threshold--underexposure/above the higher threshold--overexposure.\n Composition: For each photo and video clip candidate, a target object detection or retrieve the recorded target object detection result may be performed.  The results may then be analyzed to determine if the photo composition is \"good\" or\n\"acceptable\", in other words, whether the target is at a good location in the photo/video frame.  A straight forward rule can be that if the center of the bounding box of the detected target is not within certain preferred area of the view, then it is\nconsidered as a bad candidate.  More sophisticated methods leveraging deep learning may also be applied to check whether it is a good photo composition, such as: a number of Good or Acceptable photos and a number of Bad or Unacceptable photos are\ncollected and analyzed.  The collected photos are used to train a neural network to learn the rules.  Finally the trained network can be deployed on the device (drone or phone) to help selecting Good or Acceptable photos.\n The auto-editing and sharing module 78 modifies, i.e., edits, the selected pictures and/or selected video (segments or sub-segments) based on a set of predetermined editing parameters.  The parameters may be modified by the user, e.g., using\ntemplates.  In another aspect of the present invention, the auto-editing and sharing module shares the select and/or edited pictures and/or video segments with other users, devices, social networking or media sharing services.  In still another aspect of\nthe present invention, the auto-editing and sharing module 78 allows users to manually edit pictures and/or video.\n With reference to FIG. 8, a method M10 for operating the aerial system 12, according to an embodiment of the present invention is shown.  In a first step S1, a desired flight trajectory is established.  In general, the flight trajectory may be\nselected by the user from a set of predefined flight trajectories (see above).  In a second step S2, a target is detected.  The flight of the drone, relative to the target, is controlled (step S3) and a camera angle of the optical system 26 (step S4) is\nadjusted according to the desired flight trajectory, for example, to keep the target in frame, in a desired position in the frame and/or along a path within the frame.\n In a fifth step S5, pictures and/or video are automatically captured as the drone is controlled over the desired flight trajectory.\n In a sixth step S6, of the flight trajectory has been completed, then the method M10 ends.  Otherwise, control returns to the third and fifth steps.\n Although omitted for conciseness, the preferred embodiments include every combination and permutation of the various system components and the various method processes, wherein the method processes can be performed in any suitable order,\nsequentially or concurrently.\n As a person skilled in the art will recognize from the previous detailed description and from the figures and claims, modifications and changes can be made to the preferred embodiments of the invention without departing from the scope of this\ninvention defined in the following claims.", "application_number": "15637954", "abstract": " An aerial system, including a processing system, an optical system, an\n     actuation system and a lift mechanism, includes an autonomous photography\n     and/or videography system 70, implemented, at least in part, by the\n     processing system 22, the optical system 26, the actuation system 28 and\n     the lift mechanism 32. The autonomous photograph and/or videography\n     system performs the steps of establishing a desired flight trajectory,\n     detecting a target, controlling the flight of the aerial system as a\n     function of the desired flight trajectory relative to the target using\n     the lift mechanism and controlling the camera to capture pictures and/or\n     video.\n", "citations": ["20100265357", "20120287274", "20150109337", "20150350614", "20160063642", "20160304198", "20160378109", "20170244937", "20170293297", "20170339337", "20180024557"], "related": ["62479766"]}, {"id": "20180293885", "patent_code": "10373490", "patent_name": "Real-time traffic information collection", "year": "2019", "inventor_and_country_data": " Inventors: \nLee; Jae Eun (Seoul, KR), Lee; Sung Jin (Seoul, KR)  ", "description": "BACKGROUND\n Field of the Invention\n The present disclosure relates to traffic information collection, and more specifically, to providing accurate real-time traffic information.\n Background\n Drivers obtain traffic conditions through electronic traffic status signs installed on the road, traffic radio stations, or electronic navigation guidance such as the global position system (GPS).  Thus, the real-time traffic information enables\nthe driver to save time and fuel.  Further, the real-time traffic information can be linked with traffic signaling systems at intersections to reduce traffic congestions.  Accordingly, the accurate real-time traffic information can provide not only time\nand fuel savings for individual drivers, but also reduce energy use and air pollution at the regional and national level.  For autonomous vehicles, generating and providing large-scale, real-time traffic information at the regional and/or national level\nis important in establishing the route guidance, in addition to the basic autonomous operations of the vehicle.  Currently, the traffic information collection is done using various different methods including a closed-circuit television (CCTV) method and\na loop detector method.\n For the CCTV method, the CCTV cameras can be installed on designated roads and the videos collected from the CCTVs are transmitted to a central server.  The operators may divide the videos into local zones and visually confirm and input the\nreal-time traffic conditions.  However, the CCTV method requires a lot of manpower and labor costs to provide a city-wide or nationwide coverage of the real-time traffic information.  Further, providing the real-time traffic information can be difficult\nbecause it requires continuous, real-time visual monitoring.\n For the loop detector method, the loop coil sensors can be installed on roads to sense and collect the movement of vehicles on the road and to combine the movement with information from the location sensors installed in public transportation\nvehicles such as buses and taxis.  Although this method may provide more accurate and automatic traffic volume measurement than the CCTV, the cost of installation and maintenance can be prohibitive.  Further, transmitting the collected data from the loop\ndetectors to the central server may require a high-speed network, which can be very expensive.\nSUMMARY\n The present disclosure describes a traffic information collection system which addresses issues with collecting real-time large scale traffic data.\n In one implementation, a system is disclosed.  The system includes: a sensor installed on a section of a road and configured to collect traffic data related to movements of vehicles on the section of the road, wherein the sensor samples the\ntraffic data at a certain interval long enough to substantially reduce the amount of the collected traffic data; a plurality of narrowband network towers configured to relay the collected traffic data received from a plurality of sensors installed on a\ndesignated area encompassing multiple sections; and a central server configured to receive and process the collected traffic data from the plurality of narrowband network towers to generate traffic information sufficient to provide accurate real-time\ntraffic information of the designated area.\n In one implementation, the sensor includes a narrowband Internet-of-Things (IoT) sensor.  In one implementation, the sensor is a radar sensor using radio frequency (RF) electromagnetic waves.  In one implementation, the designated area is an\nentire city.  In one implementation, the plurality of narrowband network towers is connected using a narrowband IoT network.  In one implementation, the certain interval is between 1 and 10 minutes.  In one implementation, the sensor includes an RF unit\nconfigured to transmit and receive electromagnetic waves onto the vehicles on the section of the road; a signal processing unit configured to process signals reflected from the vehicles; and a positioning unit configured to determine and indicate\nposition information of the sensor.  In one implementation, the positioning unit is a global positioning system (GPS) unit.  In one implementation, the system further includes an encryption unit configured to provide data security for the collected\ntraffic data of the sensor.  In one implementation, the system further includes a plurality of communication units configured to enable the sensor to communicate with the plurality of narrowband network towers.  In one implementation, the plurality of\ncommunication units includes a Wifi unit and an Ethernet unit configured to enable the sensor to communicate with a mobile device during initial alignment of the sensor.  In one implementation, the signal processing unit is also configured to calculate,\nfrom the signals reflected from the vehicles, an arithmetic mean of speed of the vehicles.\n In one implementation, a method is disclosed.  The method includes: collecting traffic data related to movements of vehicles on a section of a road using a radar sensor installed on the section of the road, wherein the sensor samples the traffic\ndata at a certain interval long enough to substantially reduce the amount of the collected traffic data; relaying the collected traffic data received from a plurality of radar sensors installed on a designated area encompassing multiple sections using a\nnarrowband network; and receiving and processing the collected traffic data from the narrowband network to generate traffic information sufficient to provide accurate real-time traffic information of the designated area.\n In one implementation, the narrowband network includes a narrowband Internet-of-Things (IoT) network.  In one implementation, the designated area is an entire city.  In one implementation, the certain interval is between 1 and 10 minutes.  In\none implementation, the method further includes transmitting and receiving electromagnetic waves onto the vehicles on the section of the road; and processing signals reflected from the vehicles.  In one implementation, the method further includes\nproviding data security for the collected traffic data of the radar sensor.  In one implementation, the method further includes enabling the radar sensor to communicate with a mobile device during initial alignment of the radar sensor.\n In one implementation, a non-transitory computer-readable storage medium storing a computer program to collect traffic information is disclosed.  The computer program includes executable instructions that cause a computer to: collect traffic\ndata related to movements of vehicles on a section of a road using a radar sensor installed on the section of the road, wherein the sensor samples the traffic data at a certain interval long enough to substantially reduce the amount of the collected\ntraffic data; relay the collected traffic data received from a plurality of radar sensors installed on a designated area encompassing multiple sections using a narrowband network; and receive and process the collected traffic data from the narrowband\nnetwork to generate traffic information sufficient to provide accurate real-time traffic information of the designated area.\n Other features and advantages of the present disclosure should be apparent from the present description which illustrates, by way of example, aspects of the present disclosure. BRIEF DESCRIPTION OF THE DRAWINGS\n The details of the present disclosure, both as to its structure and operation, may be gleaned in part by study of the appended drawings, in which like reference numerals refer to like parts, and in which:\n FIG. 1 is a diagram of a traffic data collection system in accordance with one implementation of the present disclosure;\n FIG. 2 is an interface diagram illustrating interface between the sensor and other elements of the traffic data collection system in accordance with one implementation of the present disclosure;\n FIG. 3 is a block diagram of a system for utilizing the traffic information generated by an entity in accordance with one implementation of the present disclosure; and\n FIG. 4 is a flow diagram of a process for linking a media consumption history to a consumer using a blockchain-based media ledger in accordance with one implementation of the present disclosure.\nDETAILED DESCRIPTION\n As stated above, although the traffic information collection can be done using various different methods including the CCTV method and the loop detector method, the CCTV method requires a lot of manpower and labor costs, while installation,\nmaintenance, and transmission of the collected data for the loop detector method can be prohibitive and may require an expensive high-speed network.\n Certain implementations of the present disclosure provide for collecting traffic information by using multiple radar (or Light Detection and Ranging (Lidar)) sensors that substantially reduce the requirement for peripheral equipment needed for\ninstallation and information collection.  This substantially reduces investment and communication costs.  The present disclosure can also provide the collected traffic information to general users, traffic broadcasting stations, road management-related\nagencies, and/or navigation service providers.  Further, the present disclosure provides for transmitting the collected traffic data to a central server using low-power narrowband networks such as the Narrowband Internet of Things (IoT) network or the\nLow Power Wide Area Network (LPWAN).  However, to use the low-power narrowband network to transmit the collected data, the traffic data needs to be processed so that only the essential parts of the traffic data can be transmitted.\n After reading these descriptions, it will become apparent how to implement the disclosure in various implementations and applications.  However, although various implementations of the present disclosure will be described herein, it is to be\nunderstood that these implementations are presented by way of example only, and not limitation.  As such, this detailed description of various implementations should not be construed to limit the scope or breadth of the present disclosure.\n In one implementation, the collected traffic data is processed over a predetermined period of time and transmitted to the central server over a narrowband network such as the IoT network.  Thus, to be able to do this, the collected traffic data\nis processed over time into a reduced size of essential data.  Since the IoT network itself is an existing wireless network, there is no need for installing separate communication network and network equipment.  Further, since the sensor can be\nprogrammed to collect certain data at the predetermined interval, the sensor does not need a separate computer for collecting data, which substantially reduces the operating cost.\n In one implementation, the sensors (e.g., radar sensors) are placed at certain points (e.g., intersections) on the road to continuously collect and store the movement, the speed, and the size of objects (e.g., vehicles) within certain sections\nof the road.  In one implementation, the traffic data for a particular section is collected over a predetermined period of time and is processed to calculate information necessary to determine the traffic condition such as the arithmetic mean of the\nspeed of the vehicles, the maximum speed, the sizes of the passing vehicles, of the particular section.  The processed traffic data of the particular section is then transmitted over the narrowband network from the sensor to the central server.\n In one implementation, the central server processes the traffic data transmitted from selected sensors within a desired area (e.g., city wide) to create large-scale traffic information of the desired area.  This large-scale traffic information\ncan be provided to the organizations managing the traffic conditions of the desired area, navigation service providers, or general users.  In one implementation, the large-scale traffic information is used as operating schedule information of autonomous\nvehicles.\n In one implementation, given the above-stated requirements of the sensors, the sensors (e.g., radar or Lidar sensors) can be mass produced into about the size of mobile phones and easily installed on existing street light posts, traffic light\nposts, and/or speed camera mounts, to significantly reduce the cost of installing the sensors.  Further, the cost of the sensors can be significantly reduced by removing the need to have controller computers, but having an on-chip processor and a storage\nunit (e.g., small-sized storage) attached to the radar or Lidar transmitters and receivers.  In one implementation, the on-chip processor can adjust the time interval between arithmetic averaging of the vehicle speed with respect to an intersection,\nbottleneck section/lanes of the road, or carpool lanes.  The traffic data collected by the sensor can provide traffic condition of each lane of the road so that accurate traffic data can be analyzed by the central server.  Further, the processing at the\ncentral server (or at the sensor) can also include object identifications to estimate the flow of the object.  For example, a semi-truck with multiple trailers can be identified so that when the semi-truck is in an accident, the central server can\ndetermine why there would be slow down on multiple lanes of the road.\n FIG. 1 is a diagram of a traffic data collection system 100 in accordance with one implementation of the present disclosure.  In the illustrated implementation of FIG. 1, the traffic data collection system 100 includes a plurality of sensors\n110, 112, a plurality of narrowband network towers 130, 132, 134, 136, and a central server 140.\n The plurality of sensors 110, 112 is installed on existing street light posts, traffic light posts, speed camera mounts, and/or newly-built posts on the road.  In one implementation, the sensors are IoT sensors.  In one implementation, the\nsensors are radar sensors.  In another implementation, the sensors are Lidar sensors.\n In one implementation, the sensors 110, 112 installed on the posts or mounts are placed at certain points (e.g., intersections) on the road and are certain distances apart from each other so that different sections 120, 122 of the road can be\ncovered by the sensors 110, 112.  In one implementation, the distance 114 between the sensors 110, 112 is approximately 200 to 500 meters.\n In FIG. 1, each sensor 110, 112 senses objects (e.g., vehicles) inside a section 120, 122 by radiating electromagnetic waves (in the case of a radar sensor).  Using the reflected signal, the distance, the speed, and the angle (with respect to\nthe line from the sensor to the object) can be sensed and determined.  In one implementation, the radar sensor can detect the speed and the direction of the object using the Doppler effect.  Further, using multiple receiving channels of the radar sensor\nand a triangulation method, objects (e.g., vehicles) moving in each lane can be determined.\n In one implementation, all collected data such as velocity, position, and size of the observed objects are stored in the storage unit of the sensor.  However, only the data sufficient to determine the large-scale traffic information such as the\nvelocity of the objects is transmitted to the central server 140 via the plurality of narrowband network towers 130, 132, 134, 136.  In one implementation, the transmission distances 116, 118 from the sensors 110, 112 to the narrowband network towers\n130, 132, 134, 136 are approximately 1 to 10 km, with the maximum distance being approximately 12 km.\n In one implementation, the velocities of the objects are collected at a certain time interval (e.g., every 2 minutes) that is sufficient to determine the large-scale traffic information at the central server 140.  Thus, the time interval can be\nadjusted depending on the factors such as time of day, determination that an accident has occurred, and/or needs of the central server 140.  For example, in the case of a city section where the vehicles must stop at a pedestrian crossing, the cumulative\narithmetic average speed over several minutes rather than the real-time vehicle speed in seconds can provide more accurate information for determining the actual traffic flow of the city section.\n In another example of a busy intersection, the approximate average speed is about 20 to 30 km/h including the signal waiting time.  Thus, in this example, it may be sufficient to collect the vehicle speed for about 5 to 10 minutes and to perform\nthe arithmetic averaging of the collected speeds.  For low-traffic or wide-area roads (highways, national roads, motorway, etc.), most vehicle speeds are around 70 to 100 km/h, which is close to the speed limit.  On these roads, there may not be any\ndifference between the cumulative arithmetic average speed measured over minutes and the real time traffic speed.  In case of an accident, the speed of the lane on which the accident occurred may change rapidly.  However, since the lanes surrounding the\naccident lane may gradually become stagnant, the average vehicle speed data per minute can provide sufficient traffic information.  Further, in some implementations, more accurate traffic information can be determined by analyzing the maximum speed and\nthe number of vehicles crossing the road in a given time.\n In one implementation, the central server 140 performs real-time processing and analysis of the collected data to produce large-scale traffic information.  This large-scale traffic information can be provided to the organizations 144 managing\nthe traffic conditions of the desired area, navigation service providers 146, or general users through the web service 150 or the mobile service 152.  In one implementation, the large-scale traffic information is used as operating schedule information of\nautonomous vehicles 142.\n In one implementation, as stated above, the traffic data collected by the radar sensors 110, 112 deployed on the road is transmitted to the central server 140 through the narrowband public network (e.g., IoT network) including towers 130, 132,\n134, 136.  In the central server 140, the position information of each radar sensor 110, 112 and the transmitted data are stored on the storage unit.  The average speed (calculated by sensors within a particular section) measured at a certain time\ninterval is used to determine the traffic information for the particular section.  The sensor 110, 112 may require a power supply or other alternative power source (e.g., a battery or solar power unit).  In some implementations, the sensors 110, 112 can\nbe operated in a power-saving mode such that the sensors 110, 112 only operate when the movement is detected.  Otherwise, the sensors 110, 112 are kept in a standby mode.  Further, the sensors 110, 112 can be controlled by the central server 140 so that\nboth the sensors and the server operate in different modes depending on the weather, the time of day, the day of the week, and holidays.\n FIG. 2 is an interface diagram 200 illustrating interface between the sensor 210 and other elements of the traffic data collection system 100 in accordance with one implementation of the present disclosure.  The illustrated implementation of\nFIG. 2 shows the sensor 210 interfacing with the central server 230, a mobile device 240 which may be used to individually control and program the sensor 210, and a satellite unit 250 which may be used when land-based networks are down.\n In one implementation, the sensor 210 (e.g., the radar sensor) includes an RF unit 212, a signal processing unit 214, a positioning unit 216, an encryption unit 218, and various communication units 220, 222, 224.  The RF unit 212 is configured\nto transmit and receive electromagnetic waves.  The signal processing unit 214 is configured to process signals reflected from the vehicles.  The positioning unit 216 is configured to determine and indicate the position information of the sensor 210.  In\none implementation, the positioning unit 216 is a global positioning system (GPS) unit.  The encryption unit 218 is configured to provide data security.  In one implementation, the communication units include a Wifi unit 220, an Ethernet unit 222, and an\nIoT network communication unit 224.  The Ethernet unit 222 can be used to perform initial alignment of the position and direction of the sensor 210 during installation.  The IoT network communication unit 224 can be used to transmit the collected traffic\ndata to the central server 230 at a predetermined interval.\n In one implementation, the Wifi unit 220 can be used during the installation of the sensor 210 to calibrate the mapping and collection of data using, for example, the mobile device 240.  Thus, the mobile device 240 can install the map\napplication and during the sensor installation process, the position of the sensor 210 from the positioning unit 216 is obtained, transmitted to the mobile device 240 using the Ethernet or Wifi unit and displayed on the map application.  The collected\ntraffic data can then be displayed on the map application using the position information of the sensor 210.  Accordingly, the initialization of the sensor 210 can be done using the mobile device 240 using the position information of the sensor, the\ncollected traffic data of the section of the road, and the map application.  Further adjustment can be made simple by calculating the curvature and the shape of the road and tracking the position of the stationary object or the path of the moving object\nat the position where the radar is installed.\n In one implementation, the sensor 210 can also be equipped with a 6-axis or 9-axis motion detector (not shown) so that a precise and accurate initialization of the sensor 210 can be performed.  The installed sensor 210 can transmit the collected\ntraffic data to the central server 230 through the IoT network.  The central server 230 can store the traffic data (e.g., average speed, maximum speed, number of traffic vehicles, etc.) along with the position information of the sensors received from the\nsensors.  The data received from the sensors and stored on the storage unit of the central server 230 forms large-scale real-time traffic information.  In one implementation, the processing of the large-scale real-time traffic information can include an\nartificial intelligence method using machine learning or deep learning.  The traffic information can be combined with information about the section being monitored, the weather, the day of the week, the holiday, and/or other similar factors (e.g.,\nconstructions or special events going on in the section, etc.) to accurately predict and estimate the travel time and recommend best route to take.\n In some implementations, traffic lights on the same section of the road (as the section being monitored for traffic) can be communicated to the central server 230 so that the traffic signal information can also be combined with the\nabove-enumerated information to provide accurate traffic information and route guidance.  In other implementations, data from the CCTV cameras can be added to the traffic data of the sensors to provide additional information.\n In one implementation, the signal processing unit 214 is arranged such that at least one observation point in the real-time object movement interval coincides with a specific physical position on the road, and the intensity of the\nelectromagnetic wave reflected by the vehicle passes through the specific physical position.  The mean values for at least one of the vehicle classification information, the number of vehicles, the vehicle speed, the distance between the vehicles, the\nvehicle traveling direction, and/or the lane information can then be used to calculate the arithmetic mean which is transmitted to the central server at a predetermined interval.\n FIG. 3 is a block diagram of a system 300 for utilizing the traffic information generated by an entity 310 in accordance with one implementation of the present disclosure.  In the illustrated implementation of FIG. 3, the entity 310 (similar to\nthe central server 230 in FIG. 2) includes a real-time traffic data facilitator 312 and a `big data` storage unit 314.\n In one implementation, the real-time traffic data facilitator 312 receives the collected traffic data from multiple sensors (e.g., from thousands to millions of sensors) and stores the received data in the `big data` storage unit 314.  The\nreal-time traffic data facilitator 312 can process and combine the stored data into usable traffic information which can be commoditized and/or monetized.  In one implementation, the traffic information generated by the real-time traffic data facilitator\n312 can be sold directly to the individual users 320, the navigation suppliers 322, and/or the public service agencies 324, or supplied to them using an advertising model.  The real-time traffic information can be shared with public facilities 330 in\nreturn for permitting the installation of the sensors in public places.\n In one implementation, the payment for traffic information can be made through an account setup using a subscription step and a login step, which provides access to the storage unit 314.  The rate charged by the entity 310 can be made different\ndepending on the frequency of usage after the subscription step or subsequent data usage in retrieving traffic information data, downloading it to the memory of the user, and further processing the traffic information so that the information can be used\nin the business of the user.  In another implementation, the rate charged by the entity 310 can be substantially reduced or eliminated by accepting advertisement from a specific advertiser so that the rate is paid by the advertiser.\n FIG. 4 is a flow diagram of a process 400 for linking a media consumption history to a consumer using a blockchain-based media ledger in accordance with one implementation of the present disclosure.\n In the illustrated implementation of FIG. 4, the process includes collecting traffic data related to movements of vehicles on a section of a road, at step 410, using a radar sensor installed on the section of the road.  In one implementation,\nthe radar sensor samples the traffic data at a certain interval long enough to substantially reduce the amount of the collected traffic data.  The collected traffic data of a designated area encompassing multiple sections is then relayed, at step 420,\nusing a narrowband network.  A determination is made, at step 430, whether the collected traffic data has been received.  If the collected traffic data has been received from the narrowband network, the traffic data is processed, at step 440, to generate\ntraffic information sufficient to provide accurate real-time traffic information of the designated area.\n In one implementation, the narrowband network includes a narrowband Internet-of-Things (IoT) network.  In one implementation, the designated area is an entire city.  In one implementation, the certain interval is between 1and 10 minutes.  In one\nimplementation, the process 400 further includes transmitting and receiving electromagnetic waves onto the vehicles on the section of the road and processing the signals reflected from the vehicles.  In one implementation, the process 400 further\nincludes providing data security for the collected traffic data of the radar sensor.  In one implementation, the process 400 further includes enabling the radar sensor to communicate with a mobile device during initial alignment of the radar sensor.\n The above description of the disclosed implementations is provided to enable any person skilled in the art to make or use the invention as described in the specification presented above.  Various modifications to these implementations will be\nreadily apparent to those skilled in the art, and the generic principles described herein can be applied to other implementations without departing from the spirit or scope of the disclosure.  Although the above descriptions mention using the traffic\ndata for navigation and/or autonomous driving, other uses for the collected traffic data are contemplated.  For example, the collected traffic data can be used for city planning, determination of suitable locations for airport, etc. Further, although the\nsensor is described as sensing vehicles on the road, other objects or being can be sensed such as humans or animals.  Accordingly, the techniques are not limited to the specific examples described above.  Thus, it is to be understood that the description\nand drawings presented herein represent a presently possible implementation of the disclosure and are therefore representative of the subject matter that is broadly contemplated by the present disclosure.  It is further to be understood that the scope of\nthe present disclosure fully encompasses other implementations that may become obvious to those skilled in the art and that the scope of the present disclosure is accordingly limited by nothing other than the appended claims.", "application_number": "15949894", "abstract": " System, method, and non-transitory computer-readable storage medium,\n     including: a sensor installed on a section of a road and configured to\n     collect traffic data related to movements of vehicles on the section of\n     the road, wherein the sensor samples the traffic data at a certain\n     interval long enough to substantially reduce the amount of the collected\n     traffic data; a plurality of narrowband network towers configured to\n     relay the collected traffic data received from a plurality of sensors\n     installed on a designated area encompassing multiple sections; and a\n     central server configured to receive and process the collected traffic\n     data from the plurality of narrowband network towers to generate traffic\n     information sufficient to provide accurate real-time traffic information\n     of the designated area.\n", "citations": ["9349287", "20110054731", "20120307065", "20140229255", "20140232564", "20150065036", "20170287332", "20180181095"], "related": ["62483843"]}, {"id": "20180366225", "patent_code": "10373315", "patent_name": "Method and system for computer-aided triage", "year": "2019", "inventor_and_country_data": " Inventors: \nMansi; Christopher (Palo Alto, CA), Golan; David (Palo Alto, CA)  ", "description": "TECHNICAL FIELD\n This invention relates generally to the medical diagnostic field, and more specifically to a new and useful system and method for computer-aided triage in the medical diagnostic field.\nBACKGROUND\n In current triaging workflows, especially those in an emergency setting, a patient presents at a first point of care, where an assessment, such as imaging, is performed.  The image data is then sent to a standard radiology workflow, which\ntypically involves: images being uploaded to a radiologist's queue, the radiologist reviewing the images at a workstation, the radiologist generating a report, an emergency department doctor reviewing the radiologist's report, the emergency department\ndoctor determining and contact a specialist, and making a decision of how to treat and/or transfer the patient to a 2.sup.nd point of care.  This workflow is typically very time-consuming, which increases the time it takes to treat and/or transfer a\npatient to a specialist.  In many conditions, especially those involving stroke, time is extremely sensitive, as it is estimated that in the case of stroke, a patient loses about 1.9 million neurons per minute that the stroke is left untreated (Saver et\nal.).  Further, as time passes, the amount and types of treatment options, such as a mechanical thrombectomy, decrease.\n Thus, there is a need in the triaging field to create an improved and useful system and method for decreasing the time it takes to determine and initiate treatment for a patient presenting with a critical condition. BRIEF DESCRIPTION OF\nTHE FIGURES\n FIG. 1 is a schematic of a system for computer-aided triage.\n FIG. 2 is a schematic of a method for computer-aided triage.\n FIG. 3 depicts a variation of a method for computer-aided triage.\n FIG. 4 depicts a variation of determining a patient condition during a method for computer-aided triage.\n FIGS. 5A and 5B depict a variation of an application on a user device.\n FIG. 6 depict a variation of a method for computer-aided triage.\nDESCRIPTION OF THE PREFERRED EMBODIMENTS\n The following description of the preferred embodiments of the invention is not intended to limit the invention to these preferred embodiments, but rather to enable any person skilled in the art to make and use this invention.\n 1.  Overview\n As shown in FIG. 1, a system 100 for computer-aided triage includes a router 110, a remote computing system 120, and a client application 130.  Additionally or alternatively, the system 100 can include any number of computing systems (e.g.,\nlocal, remote), servers (e.g., PACS server), storage, lookup table, memory, or any other suitable components.\n As shown in FIG. 2, method 200 for computer-aided triage includes determining a parameter associated with a data packet S220, determining a treatment option based on the parameter S230, and transmitting information to a device associated with a\nsecond point of care S250.  Additionally or alternatively, the method 200 can include any or all of: receiving a data set at a first point of care S205, transmitting data to a remote computing system S208, preparing a data packet for analysis S210,\npreparing a data packet for transfer S240, aggregating data S260, or any other suitable steps performed in any suitable order.\n 2.  Benefits\n The system and method for computer-aided triage can confer several benefits over current systems and methods.\n In some variations, the system and/or method confer the benefit of reducing the time to match and/or transfer a patient presenting with a condition (e.g., stroke, LVO) to a specialist.  In some examples, for instance, the average time between\ngenerating a computed tomography angiography (CTA) dataset and notifying a specialist is reduced (e.g., from over 50 minutes to less than 8 minutes).\n In some variations, the method provides a parallel process to a traditional workflow (e.g., standard radiology workflow), which can confer the benefit of reducing the time to determine a treatment option while having the outcome of the\ntraditional workflow as a backup in the case that an inconclusive or inaccurate determination (e.g., false negative, false positive, etc.) results from the method.\n In some variations, the method is configured to have a high sensitivity (e.g., 87.8%, approximately 88%, between 81% and 93%, greater than 87%, etc.), which functions to detect a high number of true positive cases and help these patients reach\ntreatment faster.  In the event that this results in a false positive, only a minor disturbance--if any--is caused to a specialist, which affects the specialist's workflow negligibly (e.g., less than 5 minutes), if at all.  Additionally or alternatively,\nthe method can be configured to have a high specificity (e.g., 89.6%, approximately 90%, between 83% and 94%, greater than 89%, etc.), which can reduce a probability of determining a false negative.\n In some variations, the method confers the benefit of reorganizing a queue of patients, wherein patients having a certain condition are detected early and prioritized (e.g., moved to the front of the queue).\n In some variations, the method confers the benefit of determining actionable analytics to optimize a workflow, such as an emergency room triage workflow.\n Additionally or alternatively, the system and method can confer any other benefit.\n 3.  System\n The system 100 for computer-aided triage, as shown in FIG. 1, includes a router 110, remote computing system 120, and a client application 1300.  Additionally or alternatively, the system 100 can include any number of computing systems (e.g.,\nlocal, remote), servers (e.g., PACS server), storage, lookup table, memory, or any other suitable components.\n The system 100 can implement any or all of the method 200 or any other suitable method.\n The system 100 preferably interfaces with one or more points of care (e.g., 1.sup.st point of care, 2.sup.nd point of care, 3.sup.rd point of care, etc.), which are each typically a healthcare facility.  A 1.sup.st point of care herein refers to\nthe healthcare facility to which a patient presents, typically where the patient first presents (e.g., in an emergency setting).  Conventionally, healthcare facilities include spoke facilities, which are often general (e.g., non-specialist, emergency,\netc.) facilities, and hub (e.g., specialist) facilities, which can be equipped or better equipped (e.g., in comparison to spoke facilities) for certain procedures (e.g., mechanical thrombectomy), conditions, or patients.  Patients typically present to a\nspoke facility at a 1.sup.st point of care, but can alternatively present to a hub facility, such as when it is evident what condition their symptoms reflect, when they have a prior history of a serious condition, when the condition has progressed to a\nhigh severity, when a hub facility is closest, randomly, or for any other reason.  A healthcare facility can include any or all of: a hospital, clinic, ambulances, doctor's office, imaging center, laboratory, primary stroke center (PSC), comprehensive\nstroke center (CSC), stroke ready center, interventional ready center, or any other suitable facility involved in patient care and/or diagnostic testing.\n A patient can be presenting with symptoms of a condition, no symptoms (e.g., presenting for routine testing), or for any other suitable system.  In some variations, the patient is presenting with one or more stroke symptoms (e.g., ischemic\nstroke symptoms), such as, but not limited to, weakness, numbness, speech abnormalities, and facial drooping.  Typically, these patients are then treated in accordance with a stroke protocol, which typically involves an imaging protocol at an imaging\nmodality, such as, but not limited to, a non-contrast CT (NCCT) scan of the head, CTA of the head and neck, CT perfusion (CTP) of the head.\n A healthcare worker herein refers to any individual or entity associated with a healthcare facility, such as, but not limited to: a physician, emergency room physician (e.g., orders appropriate lab and imaging tests in accordance with a stroke\nprotocol), radiologist (e.g., on-duty radiologist, healthcare worker reviewing a completed imaging study, healthcare working authoring a final report, etc.), neuroradiologist, specialist (e.g., neurovascular specialist, vascular neurologist,\nneuro-interventional specialist, neuro-endovascular specialist, expert/specialist in a procedure such as mechanical thrombectomy, cardiac specialist, etc.), administrative assistant, healthcare facility employee (e.g., staff employee), emergency\nresponder (e.g., emergency medical technician), or any other suitable individual.\n The image data can include computed tomography (CT) data (e.g., radiographic CT, non-contrast CT, CT perfusion, etc.), preferably CT angiography (CTA) data (e.g., axial data, axial series, etc.) but can additionally or alternatively any other\nsuitable image data.  The image data is preferably generated at an imaging modality (e.g., scanner at the 1.sup.st point of care), such as a CT scanner, magnetic resonance imaging (MRI) scanner, ultrasound system, or any other scanner.  Additionally or\nalternatively, image data can be generated from a camera, user device, accessed from a database or web-based platform, drawn, sketched, or otherwise obtained.\n 3.1 System--Router 110\n The system 100 can include a router 110 (e.g., medical routing system), which functions to receive a data packet (e.g., dataset) including instances (e.g., images, scans, etc.) taken at an imaging modality (e.g., scanner) via a computing system\n(e.g., scanner, workstation, PACS server) associated with a 1.sup.st point of care.  The instances are preferably in the Digital Imaging and Communications in Medicine (DICOM) file format, as well as generated and transferred between computing system in\naccordance with a DICOM protocol, but can additionally or alternatively be in any suitable format.  Additionally or alternatively, the instances can include any suitable medical data (e.g., diagnostic data, patient data, patient history, patient\ndemographic information, etc.), such as, but not limited to, PACS data, Health-Level 7 (HL7) data, electronic health record (EHR) data, or any other suitable data, and to forward the data to a remote computing system.\n The instances preferably include (e.g., are tagged with) and/or associated with a set of metadata, but can additionally or alternatively include multiple sets of metadata, no metadata, extracted (e.g., removed) metadata (e.g., for regulatory\npurposes, HIPAA compliance, etc.), altered (e.g., encrypted, decrypted, etc.) metadata, or any other suitable metadata, tags, identifiers, or other suitable information.\n The router 110 can refer to or include a virtual entity (e.g., virtual machine, virtual server, etc.) and/or a physical entity (e.g., local server).  The router can be local (e.g., at a 1.sup.st healthcare facility, 2.sup.nd healthcare facility,\netc.) and associated with (e.g., connected to) any or all of: on-site server associated with any or all of the imaging modality, the healthcare facility's PACS architecture (e.g., server associated with physician workstations), or any other suitable\nlocal server or DICOM compatible device(s).  Additionally or alternatively, the router can be remote (e.g., locate at a remote facility, remote server, cloud computing system, etc.), and associated with any or all of: a remote server associated with the\nPACS system, a modality, or another DICOM compatible device such as a DICOM router.\n The router 110 preferably operates on (e.g., is integrated into) a system (e.g., computing system, workstation, server, PACS server, imaging modality, scanner, etc.) at a 1.sup.st point of care but additionally or alternatively, at a 2.sup.nd\npoint of care, remote server (e.g., physical, virtual, etc.) associated with one or both of the 1.sup.st point of care and the 2.sup.nd point of care (e.g., PACS server, EHR server, HL7 server), a data storage system (e.g., patient records), or any other\nsuitable system.  In some variations, the system that the router operates on is physical (e.g., physical workstation, imaging modality, scanner, etc.) but can additionally or alternatively include virtual components (e.g., virtual server, virtual\ndatabase, cloud computing system, etc.).\n The router 110 is preferably configured to receive data (e.g., instances, images, study, series, etc.) from an imaging modality, preferably an imaging modality (e.g., CT scanner, MRI scanner, ultrasound machine, etc.) at a first point of care\n(e.g., spoke, hub, etc.) but can additionally or alternatively be at a second point of care (e.g., hub, spoke, etc.), multiple points of care, or any other healthcare facility.  The router can be coupled in any suitable way (e.g., wired connection,\nwireless connection, etc.) to the imaging modality (e.g., directly connected, indirectly connected via a PACS server, etc.).  Additionally or alternatively, the router 100 can be connected to the healthcare facility's PACS architecture, or other server\nor DICOM-compatible device of any point of care or healthcare facility.\n In some variations, the router includes a virtual machine operating on a computing system (e.g., computer, workstation, user device, etc.), imaging modality (e.g., scanner), server (e.g., PACS server, server at 1.sup.st healthcare facility,\nserver at 2.sup.nd healthcare facility, etc.), or other system.  In a specific example, the router is part of a virtual machine server.  In another specific example, the router is part of a local server.\n 3.2 System--Remote Computing System 120\n The system 100 can include a remote computing system 120, which can function to receive and process data packets (e.g., dataset from router), determine a treatment option (e.g., select a 2.sup.nd point of care, select a specialist, etc.),\ninterface with a user device (e.g., mobile device), compress a data packet, extract and/or remove metadata from a data packet (e.g., to comply with a regulatory agency), or perform any other suitable function.\n Preferably, part of the method 200 is performed at the remote computing system (e.g., cloud-based), but additionally or alternatively all of the method 200 can be performed at the remote computing system, the method 200 can be performed at any\nother suitable computing system(s).  In some variations, the remote computing system 120 provides an interface for technical support (e.g., for a client application) and/or analytics.  In some variations, the remote computing system includes storage and\nis configured to store and/or access a lookup table, wherein the lookup table functions to determine a treatment option (e.g., 2.sup.nd point of care), a contact associated with the 2.sup.nd point of care, and/or any other suitable information.\n In some variations, the remote computing system 120 connects multiple healthcare facilities (e.g., through a client application, through a messaging platform, etc.).\n In some variations, the remote computing system 120 functions to receive one or more inputs and/or to monitor a set of client applications (e.g., executing on user devices, executing on workstations, etc.).\n 3.3 System--Application 130\n The system 100 can include one or more applications 130 (e.g., clients, client applications, client application executing on a device, etc.), such as the application shown in FIGS. 5A and 5B, which individually or collectively function to\nprovide one or more outputs (e.g., from the remote computing system) to a contact.  Additionally or alternatively, they can individually or collectively function to receive one or more inputs from a contact, provide one or more outputs to a healthcare\nfacility (e.g., first point of care, second point of care, etc.), establish communication between healthcare facilities, or perform any other suitable function.\n In some variations, one or more features of the application (e.g., appearance, information content, information displayed, user interface, graphical user interface, etc.) are determined based on any or all of: what kind of device the application\nis operating on (e.g., user device vs.  healthcare facility device, mobile device vs.  stationary device), where the device is located (e.g., 1.sup.st point of care, 2.sup.nd point of care, etc.), who is interacting with the application (e.g., user\nidentifier, user security clearance, user permission, etc.), or any other characteristic.  In some variations, for instance, an application executing on a healthcare facility will display a 1.sup.st set of information (e.g., uncompressed images,\nmetadata, etc.) while an application executing on a user device will display a 2.sup.nd set of information (e.g., compressed images, no metadata, etc.).  In some variations, the type of data to display is determined based on any or all of: an application\nidentifier, mobile device identifier, workstation identifier, or any other suitable identifier.\n The outputs of the application can include any or all of: an alert or notification (e.g., push notification, text message, call, email, etc.), an image set, a set of tools for interacting with the image set (e.g., panning, zooming, rotating,\nwindow leveling, scrolling, maximum intensity projection, changing orientation of 3D scan, etc.), a messaging platform (e.g., text, video, etc.), a telecommunication platform, directory of contact information (e.g., 1.sup.st point of care contact info,\n2.sup.nd point of care contact info, etc.), tracking of a workflow or activity (e.g., real-time or near real-time updates of patient status/workflow/etc.), analytics based on or related to the tracking (e.g., predictive analytics such as predicted time\nremaining in radiology workflow or predicted time until stroke reaches a certain severity, average time in a workflow, average time to transition to a second point of care, etc.), or any other suitable output.\n The inputs can include any or all of the outputs described previously, touch inputs (e.g., received at a touch-sensitive surface), audio inputs, optical inputs, or any other suitable input.  The set of inputs preferably includes an input\nindicating receipt of an output by a contact.  This can include an active input from the contact (e.g., contact makes selection at application), a passive input (e.g., read receipt), or any other input.\n In one variation, the system 100 includes a mobile device application 130 and a workstation application 130--both connected to the remote computing system--wherein a shared user identifier (e.g., specialist account, user account, etc.) can be\nused to connect the applications (e.g., retrieve a case, image set, etc.) and determine the information to be displayed at each application (e.g., variations of image datasets).  In one example, the information to be displayed (e.g., compressed images,\nhigh-resolution images, etc.) can be determined based on: the system type (e.g., mobile device, workstation), the application type (e.g., mobile device application, workstation application), the user account (e.g., permissions, etc.), any other suitable\ninformation, or otherwise determined.\n The application can include any suitable algorithms or processes for analysis, and part or all of the method 200 can be performed by a processor associated with the application.\n The application preferably includes both front-end (e.g., application executing on a user device, application executing on a workstation, etc.) and back-end components (e.g., software, processing at a remote computing system, etc.), but can\nadditionally or alternatively include just front-end or back-end components, or any number of components implemented at any suitable system(s).\n 3.4 System--Additional Components\n The system 100 and/or or any component of the system 100 can optionally include or be coupled to any suitable component for operation, such as, but not limited to: a processing module (e.g., processor, microprocessor, etc.), control module\n(e.g., controller, microcontroller), power module (e.g., power source, battery, rechargeable battery, mains power, inductive charger, etc.), sensor system (e.g., optical sensor, camera, microphone, motion sensor, location sensor, etc.), or any other\nsuitable component.\n 3.5 System--Variations\n In one variation, the system includes a router 110, which operates at a computing system at a 1.sup.st point of care and receives image data from an imaging modality.  The router transmits the image data to a remote computing system, wherein a\nseries of algorithms (e.g., machine learning algorithms) are performed at the remote computing system, which determines a hypothesis for whether or not a suspected condition is present based on the image data and/or any associated metadata.  Based on the\ndetermination, a contact is determined from a lookup table (e.g., in storage at the remote computing system), wherein the contact is notified at a user device (e.g., personal device) and sent image data through a client application executing on the user\ndevice.  One or more inputs from the contact at the application can be received at the remote computing system, which can be used to determine a next point of care for the patient.\n 4.  Method\n As shown in FIG. 2, the method 200 includes determining a parameter associated with a data packet S220, determining a treatment option based on the parameter S230, and transmitting information to a device associated with a second point of care\nS250.  Additionally or alternatively, the method 200 can include any or all of: receiving a data set at a first point of care S205, transmitting data to a remote computing system S208, preparing a data packet for analysis S210, preparing a data packet\nfor transfer S240, aggregating data S260, or any other suitable steps performed in any suitable order.\n The method 200 is preferably performed separate from but in parallel with (e.g., contemporaneously with, concurrently with, etc.) a standard radiology workflow (e.g., as shown in FIG. 3), but can additionally or alternatively be implemented\nwithin a standard workflow, be performed at a separate time with respect to a standard workflow, or be performed at any suitable time.\n The method 200 can be partially or fully implemented with the system 100 or with any other suitable system.\n The method 200 functions to improve communication across healthcare facility networks (e.g., stroke networks, spokes and hubs, etc.) and decrease the time required to transfer a patient having a suspected time-sensitive condition (e.g., brain\ncondition, stroke, ischemic stroke, large vessel occlusion (LVO), cardiac event, trauma, etc.) from a first point of care (e.g., spoke, non-specialist facility, stroke center, ambulance, etc.) to a second point of care (e.g., hub, specialist facility,\ncomprehensive stroke center, etc.), wherein the second point of care refers to a healthcare facility equipped to treat the patient.  In some variations, the second point of care is the first point of care, wherein the patient is treated at the healthcare\nfacility to which he or she initially presents.\n The method 200 preferably functions as a parallel workflow tool, wherein the parallel workflow is performed contemporaneously with (e.g., concurrently, during, partially during) a standard radiology workflow (e.g., radiologist queue), but can\nadditionally or alternatively be implemented within a standard workflow (e.g., to automate part of a standard workflow process, decrease the time required to perform a standard workflow process, etc.), be performed during a workflow other than a\nradiology workflow (e.g., during a routine examination workflow), or at any other suitable time.\n The method 200 is preferably performed in response to a patient presenting at a first point of care.  The first point of care can be an emergency setting (e.g., emergency room, ambulance, imaging center, etc.) or any suitable healthcare\nfacility, such as those described previously.  The patient is typically presenting with (or suspected to be presenting with) a time-sensitive condition, such as a neurovascular condition (e.g., stroke, ischemic stroke, occlusion, large vessel occlusion\n(LVO), thrombus, aneurysm, etc.), cardiac event or condition (e.g., cardiovascular condition, heart attack, etc.), trauma (e.g., acute trauma, blood loss, etc.), or any other time-sensitive (e.g., life-threatening) condition.  In other variations, the\nmethod is performed for a patient presenting to a routine healthcare setting (e.g., non-emergency setting, clinic, imaging center, etc.), such as for routine testing, screening, diagnostics, imaging, clinic review, laboratory testing (e.g., blood tests),\nor for any other reason.\n Any or all of the method can be performed using any number of deep learning (e.g., machine learning) modules.  Each module can utilize one or more of: supervised learning (e.g., using logistic regression, using back propagation neural networks,\nusing random forests, decision trees, etc.), unsupervised learning (e.g., using an Apriori algorithm, using K-means clustering), semi-supervised learning, reinforcement learning (e.g., using a Q-learning algorithm, using temporal difference learning),\nand any other suitable learning style.  Each module of the plurality can implement any one or more of: a regression algorithm (e.g., ordinary least squares, logistic regression, stepwise regression, multivariate adaptive regression splines, locally\nestimated scatterplot smoothing, etc.), an instance-based method (e.g., k-nearest neighbor, learning vector quantization, self-organizing map, etc.), a regularization method (e.g., ridge regression, least absolute shrinkage and selection operator,\nelastic net, etc.), a decision tree learning method (e.g., classification and regression tree, iterative dichotomiser 3, C4.5, chi-squared automatic interaction detection, decision stump, random forest, multivariate adaptive regression splines, gradient\nboosting machines, etc.), a Bayesian method (e.g., naive Bayes, averaged one-dependence estimators, Bayesian belief network, etc.), a kernel method (e.g., a support vector machine, a radial basis function, a linear discriminate analysis, etc.), a\nclustering method (e.g., k-means clustering, expectation maximization, etc.), an associated rule learning algorithm (e.g., an Apriori algorithm, an Eclat algorithm, etc.), an artificial neural network model (e.g., a Perceptron method, a back-propagation\nmethod, a Hopfield network method, a self-organizing map method, a learning vector quantization method, etc.), a deep learning algorithm (e.g., a restricted Boltzmann machine, a deep belief network method, a convolution network method, a stacked\nauto-encoder method, etc.), a dimensionality reduction method (e.g., principal component analysis, partial lest squares regression, Sammon mapping, multidimensional scaling, projection pursuit, etc.), an ensemble method (e.g., boosting, boostrapped\naggregation, AdaBoost, stacked generalization, gradient boosting machine method, random forest method, etc.), and any suitable form of machine learning algorithm.  Each module can additionally or alternatively be a: probabilistic module, heuristic\nmodule, deterministic module, or be any other suitable module leveraging any other suitable computation method, machine learning method, or combination thereof.\n Each module can be validated, verified, reinforced, calibrated, or otherwise updated based on newly received, up-to-date measurements; past measurements recorded during the operating session; historic measurements recorded during past operating\nsessions; or be updated based on any other suitable data.  Each module can be run or updated: once; at a predetermined frequency; every time the method is performed; every time an unanticipated measurement value is received; or at any other suitable\nfrequency.  The set of modules can be run or updated concurrently with one or more other modules, serially, at varying frequencies, or at any other suitable time.  Each module can be validated, verified, reinforced, calibrated, or otherwise updated based\non newly received, up-to-date data; past data or be updated based on any other suitable data.  Each module can be run or updated: in response to determination of an actual result differing from an expected result; or at any other suitable frequency.\n 4.1 Method--Receiving Data from a First Point of Care S205\n The method 200 can include receiving data (e.g., data packet) from a first point of care S205, which functions to collect data relevant to assessing a patient condition.\n The data is preferably received at a router 110, wherein the router is in the form of a virtual machine operating on a computing system (e.g., computer, workstation, quality assurance (QA) workstation, reading workstation, PACS server, etc.)\ncoupled to or part of an imaging modality (e.g., CT scanner, MRI scanner, etc.), or any other suitable router.  Additionally or alternatively, data can be received at a remote computing system (e.g., from an imaging modality, from a database, a server\nsuch as a PACS server, an internet search, social media, etc.), or at any other suitable computing system (e.g., server) or storage site (e.g., database).  In some variations, for instance, a subset of the data (e.g., image data) is received at the\nrouter while another subset of the data (e.g., patient information, patient history, etc.) is received at a remote computing system.  In a specific example, the data subset received at the router is eventually transmitted to the remote computing system\nfor analysis.\n The first point of care is often a spoke facility (e.g., non-specialist facility) but can alternatively be a hub facility (e.g., specialist facility), mobile facility or transportation (e.g., ambulance), or any other suitable healthcare\nfacility.\n S205 is preferably performed in response to (e.g., after, in real time with, substantially in real time with, with a predetermined delay, with a delay of less than 10 seconds, with a delay of less than 1 minute, at the prompting of a medical\nprofessional, etc.) the data (e.g., each of a set of instances) being generated at the imaging modality.  Additionally or alternatively, S205 can be performed in response to a set of multiple instances being generated by the imaging modality (e.g., after\na partial series has been generated, after a full series has been generated, after a study has been generated, etc.), in response to a metadata tag being generated (e.g., for an instance, for a series, for a study, etc.), in response to a trigger (e.g.,\nrequest for images), throughout the method (e.g., as a patient's medical records are accessed, as information is entered a server, as information is retrieved from a server, etc.), or at any other suitable time.\n S205 can be performed a single time or multiple times (e.g., sequentially, at different times in the method, once patient condition has progressed, etc.).  In one variations, each instance is received (e.g., at a router, at a remote computing\nsystem, etc.) individually as it is generated.  In a second variation, a set of multiple instances (e.g., multiple images, full series, etc.) are received together (e.g., after a scan has completed, after a particular anatomical component has been\nimaged, etc.).\n The router (e.g., virtual machine, virtual server, application running on the image sampling system or a computing system connected to the image sampling system, etc.) can be continuously `listening` (e.g., operating in a scanning mode,\nreceiving mode, coupled to or include a radio operating in a suitable mode, etc.) for information from the imaging modality, can receive information in response to prompting of a healthcare facility worker, in response to a particular scan type being\ninitiated (e.g., in response to a head CTA scan being initiated), or in response to any other suitable trigger.\n Image data is preferably received at the router (e.g., directly, indirectly, etc.) from the imaging modality (e.g., scanner) at which the data was generated.  Additionally or alternatively, image data or any other data can be received from any\ncomputing system associated with the healthcare facility's PACS server, any DICOM-compatible devices such as a DICOM router, or any other suitable computing system.  The image data is preferably in the DICOM format but can additionally or alternatively\ninclude any other data format.\n In addition to or alternative to image data, the data can include blood data, electronic medical record (EMR) data, unstructured EMR data, health level 7 (HL7) data, HL7 messages, clinical notes, or any other suitable data related to a patient's\nmedical state, condition, or medical history.\n The data preferably includes a set of one or more instances (e.g., images), which can be unorganized, organized (e.g., into a series, into a study, a sequential set of instances based on instance creation time, acquisition time, image position,\ninstance number, unique identification (UID), other acquisition parameters or metadata tags, anatomical feature or location within body, etc.), complete, incomplete, randomly arranged, or otherwise arranged.\n Each instance preferably includes (e.g., is tagged with) a set of metadata associated with the image dataset, such as, but not limited to: one or more patient identifiers (e.g., name, identification number, UID, etc.), patient demographic\ninformation (e.g., age, race, sex, etc.), reason for presentation (e.g. presenting symptoms, medical severity score, etc.), patient history (e.g., prior scans, prior diagnosis, prior medical encounters, etc.), medical record (e.g. history of present\nillness, past medical history, allergies, medications, family history, social history, etc.), scan information, scan time, scan type (e.g., anatomical region being scanned, scanning modality, scanner identifier, etc.), number of images in scan,\nparameters related to scan acquisition (e.g., timestamps, dosage, gurney position, scanning protocol, contrast bolus protocol, etc.), or any other suitable information.\n In some variations, any or all of the data (e.g., image data) is tagged with metadata associated with the standard DICOM protocol.\n In some variations, one or more tags is generated and/or applied to the data after the data is generated at an imaging modality.  In some examples, the tag is an identifier associated with the 1.sup.st point of care (e.g., 1.sup.st point of care\nlocation, imaging modality identifier, etc.), which can be retrieved by a 2.sup.nd point of care in order to locate the patient (e.g., to enable a quick transfer of the patient, to inform a specialist of who to contact or where to reach the patient,\netc.).\n Additionally or alternatively, image data can be received without associated metadata (e.g., metadata identified later in the method, dataset privately tagged later with metadata later in the method, etc.)\n Data can be received (e.g., at the router) through a wired connection (e.g., local area network (LAN) connection), wireless connection, or through any combination of connections and information pathways.\n In some variations, images are generated at an imaging modality (e.g., CT scanner) in response to a standard stroke protocol.\n 4.2 Method--Transmitting Data to a Remote Computing System S208\n The method can include transmitting data to remote computing system (e.g., remote server, PACS server, etc.) S208, which functions to enable remote processing of the data, robust process, or fast processing (e.g., faster than analysis done in\nclinical workflow, faster than done in a standard radiology workflow, processing less than 20 minutes, less than 10 minutes, less than 7 minutes, etc.) of the dataset.\n The data (e.g., image data, image data and metadata, etc.) is preferably transmitted to a remote computing system from a router (e.g., virtual machine operating on a scanner) connected to a computing system (e.g., scanner, workstation, PACS\nserver, etc.) associated with a healthcare facility, further preferably where the patient first presents (e.g., 1.sup.st point of care), but can additionally or alternatively be transmitted from any healthcare facility, computing system, or storage site\n(e.g., database).\n Each instance (e.g., image) of the dataset (e.g., image dataset) is preferably sent individually as it is generated at an imaging modality and/or received at a router, but additionally or alternatively, multiple instances can be sent together\nafter a predetermined set (e.g., series, study, etc.) has been generated, after a predetermined interval of time has passed (e.g., instances sent every 10 seconds), upon the prompting of a medical professional, or at any other suitable time.  Further\nadditionally or alternatively, the order in which instances are sent to a remote computing system can depend one or more properties of those instances (e.g., metadata).  S208 can be performed a single time or multiple times (e.g., after each instance is\ngenerated).\n S208 can include transmitting all of the dataset (e.g., image dataset and metadata), a portion of the dataset (e.g., only image dataset, subset of image dataset and metadata, etc.), or any other information or additional information (e.g.,\nsupplementary information such as supplementary user information).\n The data is preferably transmitted through a secure channel, further preferably through a channel providing error correction (e.g., over TCP/IP stack of 1.sup.st point of care), but can alternatively be sent through any suitable channel.\n S208 can include any number of suitable sub-steps performed prior to or during the transmitting of data to the remote computing system.  These sub-steps can include any or all of: encrypting any or all of the dataset (e.g., patient information)\nprior to transmitting to the remote computing system, removing information (e.g., sensitive information), supplementing the dataset with additional information (e.g., supplementary patient information, supplemental series of a study, etc.), compressing\nany or all of the dataset, or performing any other suitable process.\n 4.3 Method--Preparing a Data Packet for Analysis S210\n The method 200 preferably includes preparing a data packet S210 for analysis, which can function to decrease the time required to analyze the data packet, eliminate irrelevant data packets from further analysis, remove irrelevant data from a\ndata packet (e.g., irrelevant anatomical regions), or perform any other suitable function.\n Preparing a data packet for analysis can include organizing a set of instances (e.g., images, slices, scans, etc.), preferably into a series, but additionally or alternatively into a study, or any other suitable grouping of images.  The\norganization is preferably performed in response to generating a set of instances (e.g., at an imaging modality), but can additionally or alternatively be performed in response to receiving a set of instances at a location (e.g., router, remote computing\nsystem, server such as a PACS server, etc.), at the request of an individual (e.g., healthcare worker), in response to a trigger, or at any other suitable time.  Additionally or alternatively, the set of instances can be performed multiple times\nthroughout the method (e.g., based on the same organization scheme/metadata, based on different organization schemes/metadata, etc.).  The organization can be done at a remote computing system, a healthcare facility computing system, a virtual machine\n(e.g., operating on a healthcare facility computing system), or at any other suitable computing or processing system, physical or virtual, local (e.g., at a healthcare facility) or remote.  The set of images are preferably organized based on a set of\nmetadata (e.g., metadata tags, conventional DICOM metadata tags, etc.), but can additionally or alternatively be organized in any other suitable way (e.g., organized by time of receipt, ranked order of importance, etc.).  In one variation, a set of\nimages are organized into a series based on a set of metadata, wherein the series is formed from images having metadata corresponding to any or all of the following: images taken in an axial series, images each corresponding to a thin slice (e.g., 0.625\nmillimeters (mm) or thinner), no missing slices (e.g., no jump in a slice number between adjacent images), a consistent pixel spacing across the series, and aligned instance numbers and positions.  Additionally or alternatively, any other metadata can be\nused to determine the series.\n In some variations, the method includes excluding a data packet (e.g., set of instances) from further processing if one or more of a set of metadata are not satisfied, such as, but not limited to, the metadata listed above.\n Preparing a data packet can additionally or alternatively include extraction of data, such as one or more materials or features in the set of instances (e.g., series), which can function to reduce the computational cost and time of one or more\nremaining steps of the method (e.g., by removing irrelevant features in one or more instances).  This can include any or all of pixel-based methods, voxel-based methods, comparing non-image data (e.g., blood test results) with one or more predetermined\nthresholds, or any other suitable method(s).  The data is preferably extracted after the data packet has been organized (e.g., into a series), but can additionally or alternatively be performed in absence of the data packet being organized, in response\nto a trigger, in multiple steps and/or at multiple times throughout the method (e.g., extract a first material and then a subset of that material), or at any other point during the method.  The data is preferably extracted at a remote computing system\n(e.g., single computing system), but can additionally or alternatively be performed at any suitable computing system.  Data can be extracted based on any or all of: HU value thresholding, photomasks, dilation, erosion, or any other technique.\n In some variations, such as those involving patients presenting with a stroke symptom or condition (e.g., large vessel occlusion), this can include extracting portions of the image corresponding to soft matter (e.g., brain tissue, cerebral\nfluid, blood, etc.) and/or removing portions of the image correspond to hard matter (e.g., bone, skull, etc.).  This is preferably done by leveraging the fact that soft matter corresponds to a set of low Hounsfield Unit (HU) values, which is\ndifferentiated from any surrounding hard matter (e.g., bone, skull), which corresponds to a set of high HU values.\n In a specific example, a bone mask is determined and defined as a set of voxels having an HU value above a predetermined threshold (e.g., 750 HU, 700 HU, 800 HU, between 600 HU and 900 HU, etc.).  The bone mask is then dilated with a series of\nkernels of increasing size until it completely encloses a set of voxels of low HU values (e.g., less than the predetermined threshold), thereby defining a soft matter mask.  The soft matter mask is dilated to compensate for the dilation of the bone mask. If the process of defining the soft matter mask fails, this can indicate that the skull has undergone a craniotomy, which in some cases can be used in determining a diagnosis, informing a contact or second point of care, or in any other point in the\nmethod.  Once the soft matter mask is dilated, the mask can then be applied to the set of instances (e.g., organized set of instances, series, etc.), and the HU value of voxels outside of the mask is set to zero.\n Preparing a data packet preferably includes evaluating one or more exclusion criteria in the set of instances, which can function to verify that a set of instances is relevant for evaluation in the rest of the method, save time and/or resources\nby eliminating irrelevant sets of instances, route a set of instances corresponding to one or more exclusion criteria to another workflow in a healthcare facility, or perform any other suitable function.  Alternatively, the method can partially or fully\nprocess all sets of instances.  The exclusion criteria are preferably applied after data has been extracted (e.g., to reduce processing time), but can additionally or alternatively be performed prior to or in absence of the extraction of data, multiple\ntimes throughout the method (e.g., different exclusion criteria applied depending on the degree of processing of the set of instances), or at any other suitable time during the method.  Evaluating data for exclusion criteria is preferably performed at\nthe remote computing system, but can additionally or alternatively be performed at any other computing system.\n The exclusion criteria preferably include any or all of: the presence of an artifact in one or more of the set of instances (e.g., metallic artifact, aneurysm clip, etc.), improper timing at which the set of instances were taken at an imaging\nmodality (e.g., premature timing, improper timing of a bolus, etc.), one or more incomplete regions (e.g., features, anatomical features, etc.) in the set of instances (e.g., incomplete skull, incomplete vessel, incomplete soft matter region, etc.), an\nincorrect scan type or body part (e.g., non-head CT scan, non-contrast CT scan, etc.), poor image quality (e.g., blurry images, low contrast, etc.), movement of patient during scan, or any other suitable exclusion criteria.\n In one variation, a set of instances (e.g., images, series, etc.) are evaluated to determine if an artifact is present, wherein the set of instances is excluded from further steps in the method if an artifact is found.  In a specific example,\nthe method includes inspecting the HU values of voxels in a soft matter mask, wherein voxels having a value above a predetermined threshold (e.g., 3000 HU, between 2000 and 4000 HU, etc.) are determined to be a metallic artifact (e.g., aneurysm clip).\n In a second variation, a set of instances are evaluated to determine if bad bolus timing occurred during the generation of the set of instances at an imaging modality.  In a specific example, a soft matter mask is eroded with a wide kernel,\nwhich functions to remove potential high HU voxels due to a partial volume effect caused by bone voxels.  The HU values of the voxels within the eroded mask are inspected and the number of voxels having a value above a predetermined threshold (e.g., 100\nHU, between 10 and 200 HU, etc.) can be counted (e.g., correlated to a volume) and used to determine if the timing of the scan was premature.  If the timing of the scan was premature, the contrast (e.g., contrast agent, dye, etc.) in a contrast CT scan,\nfor instance, should not be visible within the soft matter and typical HU values of the voxels will be below the predetermined threshold (e.g., less than 100 HU).  If the total volume of voxels having a value greater than the predetermined threshold is\nless than a predetermined volume threshold (e.g., 10 cc, 20 cc, 5 cc, etc.), the set of instances (e.g., series) can be rejected based on bad bolus timing (e.g., premature scan).  In some specific examples, this process is selectively applied (e.g., only\nto an anterior part of the soft matter to avoid mistaking of calcifications of the choroid plexus or pineal gland as contrast).\n In a third variation, a set of instances are evaluated to determine if an anatomical feature is incomplete or missing from the set of instances.  In a specific example, the set of instances are evaluated to determine if a complete or nearly\ncomplete (e.g., area or volume above a predetermined threshold) skull is present.  This can include inspecting a total area of cerebral soft matter in a particular slice (e.g., top slice), wherein if the total area exceeds a predetermined threshold\n(e.g., 80 centimeters squared, 90 centimeters squared, between 70 and 100 centimeters squared, etc.), the set of instances is excluded as being incomplete.\n S210 can include one or more registration steps (e.g., image registration steps), wherein any or all of the set of instances (e.g., soft matter extracted from set of instances) are registered to a reference set of instances (e.g., reference\nseries), which can function to align, scale, calibrate, or otherwise adjust the set of instances.  The registration step is preferably performed in response to a data packet being filtered through a set of exclusion criteria, but can additionally or\nalternatively be performed prior to or in absence of the filtering, multiple times throughout the method, or at any other suitable time.\n The registration step can be intensity-based, feature-based, or any other type or registration and can include any or all of point-mapping, feature-mapping, or any other suitable process.  The reference set of instances is preferably determined\nfrom a training set but can alternatively be determined from any suitable dataset, computer-generated, or otherwise determined.  The reference series can be selected for any or all of orientation, size, three-dimensional positioning, clarity, contrast,\nor any other feature.  In one variation, a references series is selected from a training set, wherein the reference series is selected based on a feature parameter (e.g., largest feature size such as largest skull, smallest feature size, etc.) and/or a\ndegree of alignment (e.g., maximal alignment, alignment above a predetermined threshold, etc.).  Additionally or alternatively, any other criteria can be used to determine the reference series, the reference series can be randomly selected, formed from\naggregating multiple sets of instances (e.g., multiple patient series), or determined in any other suitable way.  In some variations, the registration is performed with one or more particular software packages (e.g., SimpleElastix).  In a specific\nexample, the registration is performed through affline registration (e.g., in SimpleElastix) with a set of predetermined (e.g., default) parameters and iterations (e.g., 4096 iterations, between 3000 and 5000 iterations, above 1000 iterations, above 100\niterations, etc.) in each registration step.\n In one variation, a skull-stripped series (e.g., series having soft matter extracted) is registered to a reference series chosen from a training set, wherein the reference series was chosen based on it having a large skull size and a high level\nof alignment among its set of instances.\n In a second variation, a skull-stripped series is registered to a reference series formed from an aggregated set of series.\n Additionally or alternatively, preparing the data packet can include any other suitable steps.\n 4.4 Method--Determining a Parameter Associated with the Data Packet S220\n The method 200 preferably includes determining a parameter associated with the data packet S220 (e.g., as shown in FIG. 4), which functions to assess a patient condition which subsequently informs the rest of the method 200.  Additionally or\nalternatively, S220 can function to reduce the time to transfer a patient to a second point of care, halt progression of the condition, or perform any other suitable function.  S220 is preferably fully performed at a remote computing system (e.g., remote\nserver, cloud-based server, etc.), further preferably a remote computing system having a graphics processing unit (GPU), but can additionally or alternatively be partially performed at any suitable remote computing system, be partially or fully performed\nat a local computing system (e.g., workstation), server (e.g., PACS server), at a processor of a user device, or at any other system.  S220 is preferably partially or fully performed using software including one or more algorithms, further preferably one\nor more multi-step algorithms containing steps that are either trained (e.g., trained through machine learning, trained through deep learning, continuously trained, etc.) or non-trained (e.g., rule-based image processing algorithms or heuristics). \nAdditionally or alternatively, any software can be implemented.\n S220 preferably includes identifying (e.g., locate, isolate, measure, quantify, etc.) an anatomical feature S222 within the data packet, further preferably within a registered series of images but alternatively within any suitable image dataset. This can be performed through any number of computer vision techniques, such as object recognition, object identification, object detection, or any other form of image analysis.  In some variations, the anatomical feature analysis is performed at least\npartially through image segmentation, wherein the segmentation includes any or all of: thresholding, clustering methods, dual clustering methods, compression-based methods, histogram-based methods, region-growing methods, partial differential\nequation-based methods, variational methods, graph partitioning methods, watershed transformations, model based segmentation, multi-scale segmentation, semi-automatic segmentation, trainable segmentation, or any suitable form of segmentation.  The method\ncan additionally or alternatively include any number of segmentation post-processing steps, such as thresholding, connectivity analyses, or any other processing.  The segmentation is preferably performed with a convolutional neural network (CNN), further\npreferably feed-forward deep CNN (e.g., using three-dimensional convolutions, two-dimensional convolutions, etc.), but can additionally or alternatively be performed using any suitable algorithm or process.\n In variations, such as those involving stroke (e.g., ischemic stroke, LVO, etc.), the anatomical feature can be one or more blood vessels (e.g., arteries, large paired arteries, etc.), such as the internal carotid artery (ICA) (e.g., terminal\nICA (t-ICA)), middle cerebral artery (MCA), or any other vessel or other anatomical feature.  Additionally or alternatively, the anatomical feature can be soft matter (e.g., brain tissue), hard matter (e.g., bone), or any other feature.  In other\nvariations, the anatomical feature can be a part of the heart (e.g., vessel, artery, lobe, etc.), a bone (e.g., fractured bone), or any other part of the body.\n S222 is preferably performed after the image data (e.g., series) has been registered to a reference series but can additionally or alternatively be performed prior to or in absence of a registration step, in response to a trigger, multiple times\nthroughout the method, or at any other suitable time.\n In some variations, such as in the case of a patient presenting with a stroke (e.g., ischemic stroke, vessel occlusion, LVO, etc.), a large vessel region (e.g., t-ICA and MCA-M1 segments) is segmented.\n In other variations, an anatomical feature (e.g., thrombus, aneurysm, etc.) within a vessel is identified.  In a specific example, for instance, a clot is segmented.  The segmented clot can be assessed (e.g., using other processes of the method\n200) to determine, for instance, one or more parameters (e.g., size, length, volume, etc.) of the clot and compare the one or more parameters with one or more predetermined thresholds (e.g., anatomical thresholds or parameters).\n In yet other variations, an anatomical feature outside of the brain vasculature, such as a tumor, tissue region (e.g., infarcted tissue), swelled region, or any other suitable feature can be identified.\n The method further preferably includes determining a parameter associated with the anatomical feature S224, which functions to assess (e.g., quantify) the anatomical feature.  S224 is preferably performed using one or more computer vision/image\nprocessing techniques, which can, for instance, include any or all of: centerline extraction, centerline extension, a distance measurement (e.g., between two ends of a feature, between two ends of a centerline, etc.), size measurement (e.g., length,\nwidth, thickness, volume, estimated mass, etc.), direction or orientation measurement, intensity measurement, or any suitable measurement can be performed.\n In some variations of the method, such as those implemented for a suspected vessel occlusion (e.g., LVO), a centerline length is determined through a centerline extension process.  This can be performed through any or all of: binary masks, voxel\nthresholding, one or more trimming steps, a three-dimensional parallel thinning algorithm, or any other process.  In an example, for instance, the centerline extension process includes extending a large vessel centerline based on a set of HU values of\none or more voxels (e.g., end voxels, voxels adjacent centerline ends of a large vessel occlusion, middle voxels, voxels having HU values above a predetermined threshold, voxels having HU values below a predetermined threshold, voxels having HU values\nwithin a predetermined range, etc.) to generate an extended centerline.  A parameter (e.g., centerline length) can then be calculated, for instance, from the extended centerline.\n In one specific example, a centerline length is determined for a vessel segmentation, such as a vessel segmentation (e.g., probabilistic vessel segmentation) described previously.  Determining the centerline length can include any or all of:\nconversion of image data to a mask (e.g., binary mask), thresholding, converting the mask to a centerline (e.g., through a three-dimensional parallel thinning algorithm), growing the centerline (e.g., based on a predetermined set of criteria), fusing of\ncenterline skeletons, preserving one or more conditions or features (e.g., topological, geometrical, etc.), pre-processing, post-processing, or any other suitable process.\n In some variations, an algorithm (e.g., for determining a centerline length) is determined to optimize for speed.  Additionally or alternatively, an algorithm can be selected to optimize for noise sensitivity or any other suitable feature.\n In some variations, the process is repeated until one or more of a set of conditions are met.  These conditions can include, for instance, that a parameter (e.g., distance, length, volume, area, voxel value, pixel value, etc.) is related in a\npredetermined way (e.g., within, above, below, etc.) a decision threshold, that the process has been repeated for a predetermined number of times (e.g., 5 times, 10 times, etc.), or any other suitable criteria.\n In some variations, a trimming step is performed at the end of each iteration to remove irrelevant features.  In a specific example, for instance, a trimming step is performed to remove (e.g., clean) short branches which do not represent large\nvessels.\n The method preferably includes comparing the parameter with a threshold S226, which functions to determine (or alternatively rule out) a suspected condition.  The condition typically refers to a hypothesized patient condition or diagnosis (e.g.,\nLVO, aneurysm, stroke, etc.) but can additionally or alternatively include a severity (e.g., based on a predetermined severity scale), an urgency, or any other characteristic.\n S226 is preferably performed after and in response to S224 but can additionally or alternatively be performed at any suitable time in the method.  The threshold (e.g., threshold value) is preferably determined based on clinical data and/or\nanatomical data, such as a geometrical feature, size (e.g., average size, aggregated size, random size, optimal size, largest size, etc.) of an anatomical feature, intensity of a feature (e.g., contrast-filled vessel), or any other suitable\ncharacteristic.  In some variations, the threshold is determined based on one or more training sets of data, wherein the training sets are used to develop one or more algorithms used in the method.\n S226 can optionally include determining the threshold.  In some variations, the threshold is chosen to be greater than the value (e.g., average value, highest value, upper limit of a standard range, optimal value, etc.) of the corresponding\nanatomical feature, which can function to increase the sensitivity of the determination of a patient condition, increase the number of false positives (e.g., when false positives have a negligible effect on a workflow), affect a specificity (e.g.,\ndecrease) of the determination of a patient condition, or perform any other suitable function.  In one example, for instance, the threshold against which a centerline length of a vessel (e.g., t-ICA plus proximal MCA-M1) is compared is chosen to be\nlarger than the corresponding anatomical length (e.g., average total length of t-ICA and proximal MCA-M1, maximum total length of t-ICA and proximal MCA-M1, etc.).  Alternatively, the threshold can be chosen to be smaller, approximately average, optimal,\nor otherwise comparable to an anatomical feature.\n In some variations of the method, such as those implemented in patients presenting with an LVO, a computed centerline length is determined and compared with a threshold centerline length (e.g., larger than average centerline length).  If the\ncomputed centerline length is less than the threshold, an LVO is suspected.  If the centerline length is greater than the threshold, no LVO is suspected.  This can be used to determine whether or not the patient should be transferred to a specialist, to\ninform a healthcare worker at a first point of care, or for any other suitable purpose.  In a specific example, a total length of a large vessel region (e.g., t-ICA and proximal MCA-M1) was determined to have a particular value (e.g., 50 mm, 53 mm,\nbetween 50 mm and 60 mm, less than 60 mm, less than 70 mm, etc.), and a threshold length was chosen to be larger (e.g., 60 mm, greater than 60 mm, etc.) than that value to optimize for true positives.\n In some variations, S226 can include performing a process during a training step, wherein the process is used to determine on optimal threshold.  In a specific example, for instance, one or more receiver operating characteristic (ROC) analyses\nare performed to investigate the performance of an algorithm for a variety of potential thresholds, thereby determining an optimal threshold (e.g., elbow point).\n In one variation, user-calibrated distance thresholds are used to determine if the distance between the proximal and distal parts of an extracted centerline is indicative of (e.g., within a range of thresholds) an LVO.  In a specific example,\nuser-calibrated intensity thresholds are used to determine if a partial LVO is present.\n The method 200 can further include testing for a set of special cases, which can function to increase the probability that a true positive is detected during the method.  The special cases (special conditions) typically correspond to less common\nanatomical or clinical configurations of the condition (e.g., LVO) but can additionally or alternatively correspond to a degraded image quality of a set of instances, or any other suitable event.  In some variations of patients presenting with an LVO,\nfor instance, an LVO can be present even when the centerline length is above the predetermined threshold.  This can include investigating one or more features of the anatomical feature and its parameters, such as an orientation of an anatomical feature\n(e.g., orientation of vessel centerline), geometrical feature (e.g., width of a vessel), or any other suitable feature indicative of a special case.\n In one example, the method includes checking for a partial occlusion.  In such cases, contrast can still partially fill the vessel, so a centerline extension can succeed and result in a centerline length above the predetermined threshold. \nChecking for a partial occlusion can include comparing the HU value of the centerline voxels to the HU value of a set of immediately adjacent voxels.  If a difference of greater than a predetermined threshold value (e.g., 200 HU) is seen between the\nvoxel groups, an LVO can be detected and/or indicated in future steps.\n In a second example, the method includes checking for a fetal origin posterior cerebral artery (PCA), which corresponds to an LVO occurring immediately after a fetal origin PCA bifurcation, as the centerline extension extends into the PCA\ninstead of into the MCA.  This can be detected by inspecting an orientation of the centerline extension, and if the centerline extends posteriorly to a greater degree than it extends distally, an LVO can be detected and/or indicated in future steps.\n Additionally or alternatively, any other special cases can be examined in any suitable way.\n 4.5 Method--Determining a Treatment Option S230\n The method can include determining a treatment option S230, preferably in the event that a condition is detected (e.g., based on a comparison with a threshold) but can additionally or alternatively determine a treatment option when a condition\nis not detected, when an analysis is inconclusive, or in any suitable scenario.  S230 can function to initiate the transfer of a patient to a 2.sup.nd point of care (e.g., specialist facility), initiate the transfer of a specialist to a 1.sup.st point of\ncare, or initiate treatment of a patient (e.g., mechanical thrombectomy) within the 1.sup.st point of care, or perform any other suitable function.  In some variations, the treatment option is a 2.sup.nd point of care, wherein it is determined (e.g.,\nsuggested, assigned, etc.) that the patient should be treated at the 2.sup.nd point of care.  Additionally or alternatively, the treatment option can be a procedure (e.g., surgical procedure, mechanical thrombectomy, placement of an aneurysm coil,\nplacement of a stent, retrieval of a thrombus, etc.), treatment (e.g., tissue plasminogen activator (TPA), pain killer, blood thinner, etc.), recovery plan (e.g., physical therapy, speech therapy, etc.), or any other suitable treatment.\n The treatment is preferably determined based on a comparison between a parameter determined from the data packet and a threshold, but can additionally or alternatively be determined based on additional data, such as patient information (e.g.,\ndemographic information, patient history, patient treatment preferences, etc.), input from one or more individuals (e.g., power of attorney, attending physician, emergency physician, etc.), or any other suitable information.\n S230 is preferably at least partially performed with software operating at the remote computing system (e.g., remote server) but can additionally or alternatively be performed at a remote computing system separate from a previous remote\ncomputing system, a local computing system (e.g., local server, virtual machine coupled to healthcare facility server, computing system connected to a PACS server), or at any other location.\n S230 is preferably performed after a patient condition has been determined during the method 200.  Additionally or alternatively, S230 can be performed after a patient condition has been determined in an alternative workflow (e.g., at the\n1.sup.st point of care, at a radiologist workstation during a standard radiology workflow, in the case of a false negative, etc.), prior to or absent the determination of a patient condition (e.g., based on an input from a healthcare worker at the remote\ncomputing system, when patient is admitted to 1.sup.st point of care, etc.), multiple times throughout the method (e.g., after a first treatment option fails, after a first specialist is unresponsive, such as after a threshold amount of time, such as 30\nseconds, 1 minute, 2 minutes, etc.), or at any other time during the method.\n S230 preferably determines a treatment option with a lookup table located in a database accessible at remote computing system (e.g., cloud-computing system).  Additionally or alternatively, a lookup table can be stored at a healthcare facility\ncomputing system (e.g., PACS server), in storage at a user device, or at any other location.\n In other variations, the treatment option can be determined by an algorithm (e.g., predictive algorithm, trained algorithm, etc.), an individual (e.g., specialist), a decision support tool, or through any other process or tool.\n The lookup table preferably correlates a 2.sup.nd point-of-care (e.g., healthcare facility, hub, physician, specialist, neuro-interventionist, etc.), further preferably a specialist or contact (e.g., administrative worker, emergency room\nphysician, etc.), with a patient condition (e.g., presence of an LVO, presence of a pathology, severity, etc.), but can additionally or alternatively correlate any treatment option with the patient condition.  The lookup table can further additionally or\nalternatively correlate a treatment option with supplementary information (e.g., patient history, demographic information, heuristic information, etc.).\n The contact (e.g., healthcare provider, neuro-interventional specialist, etc.) is preferably a healthcare worker, but can additionally or alternatively be any individual associated with the treatment of the patient and/or be associated with any\nhealthcare facility (e.g., prior healthcare facility of patient, current healthcare facility, recommended healthcare facility) related to the patient.  The contact is further preferably a specialist (e.g., neuro-interventional specialist, neurosurgeon,\nneurovascular surgeon, general surgeon, cardiac specialist, etc.) but can additionally or alternatively include an administrative worker associated with a specialist, multiple points of contact (e.g., ranked order, group, etc.), or any other suitable\nindividual or group of individuals.  The contact is preferably associated with a hub facility, wherein the hub facility is determined as an option for second point of care, but can additionally or alternatively be associated with a spoke facility (e.g.,\ncurrent facility, future facility option, etc.), an individual with a relation to the patient (e.g., family member, employer, friend, acquaintance, emergency contact, etc.), or any other suitable individual or entity (e.g., employer, insurance company,\netc.).\n The lookup table is preferably determined based on multiple types of information, such as, but not limited to: location information (e.g., location of a 1.sup.st point of care, location of a 2.sup.nd point of care, distance between points of\ncare, etc.), temporal information (e.g., time of transit between points of care, time passed since patient presented at 1.sup.st point of care, etc.), features of condition (e.g., size of occlusion, severity of condition, etc.), patient demographics\n(e.g., age, general health, history, etc.), specialist information (e.g., schedule, on-call times, historic response time, skill level, years of experience, specialty procedures, historic success or procedures, etc.), healthcare facility information\n(e.g., current number of patients, available beds, available machines, etc.), but can additionally or alternatively be determined based on a single type of information or in any other suitable way.  Information can be actual, estimated, predicted, or\notherwise determined or collected.\n A location can be a set of geographic coordinates (e.g., latitude and longitude), a place name (e.g., county, city, landmark, intersection, etc.), a physical street address, distance from a given location, presence within a specified radius from\na given location, a graphical depiction on a map, or any other suitable location expression.  The location can be determined based on GPS coordinates provided by a device, triangulation between mobile phone towers and public masts (e.g., assistive GPS),\nWi-Fi connection location, WHOIS performed on IP address or MAC address, GSM/CDMA cell IDs, location information self-reported by a user, or determined in any other suitable manner.\n In some variations, the method 200 includes transmitting information (e.g., patient condition, data determined from analysis, optimal set of instances, series, data packet, etc.) to the computing system associated with the lookup table.\n 4.6 Method--Preparing a Data Packet for Transfer S240\n The method 200 can include preparing a data packet for transfer, which can function to produce a compressed data packet, partially or fully anonymize a data packet (e.g., to comply with patient privacy guidelines, to comply with Health Insurance\nPortability and Accountability Act (HIPAA) regulations, to comply with General Data Protection Regulation (GDRP) protocols, etc.), minimize the time to transfer a data packet, or perform any other suitable function.  Additionally or alternatively, any or\nall of a data packet previously described can be transferred.\n The data packet is preferably transferred (e.g., once when data packet is generated, after a predetermined delay, etc.) to a contact, further preferably a specialist (e.g., associated with a 2.sup.nd point of care, located at the 1.sup.st point\nof care, etc.), but can additionally or alternatively be sent to another healthcare facility worker (e.g., at 1.sup.st point of care, radiologist, etc.), an individual (e.g., relative, patient, etc.), a healthcare facility computing system (e.g.,\nworkstation), a server or database (e.g., PACS server), or to any other suitable location.\n S240 preferably includes compressing a set of images (e.g., series), but can additionally or alternatively leave the set of images uncompressed, compress a partial set of images (e.g., a subset depicting the condition), or compress any other\npart of a data packet.  Compressing the data packet functions to enable the data packet to be sent to, received at, and viewed on a user device, such as a mobile device.  Compressing the data packet can include any or all of: removing a particular image\nregion (e.g., region corresponding to air, region corresponding to hard matter, region without contrast dye, irrelevant anatomical region, etc.), thresholding of voxel values (e.g., all values below a predetermined threshold are set to a fixed value, all\nvalues above a predetermined threshold are set to a fixed value, all values below -500 HU are set to -500, all voxel values corresponding to a particular region are set to a fixed value, all voxels corresponding to air are set to a predetermined fixed\nvalue, etc.), reducing a size of each image (e.g., scale image size by factor of 0.9, scale image size by factor of 0.7, scale image size by factor of 0.5, scale image size by a factor between 0.1 and 0.9, reduce image size by a factor of 4, etc.), or\nthrough any other compression method.\n In one variation, the reduction in size of a set of images can be determined based on one or more memory constraints of the receiving device (e.g., user device, mobile device, etc.).\n In some variations, such as those involving a patient presenting with a brain condition (e.g., LVO), the images taken at an imaging modality (e.g., CT scanner) are compressed by determining an approximate or exact region in each image\ncorresponding to air (e.g., based on HU value, based on location, based on volume, etc.) and setting the air region (e.g., voxels corresponding to the air region, pixels corresponding to the air region, etc.) to have a fixed value.  Additionally or\nalternatively, any non-critical region (e.g., bone, unaffected region, etc.) or other region can be altered (e.g., set to a fixed value, removed, etc.) during the compression.  In a specific example, for instance, a set of voxels corresponding to air are\nset to all have a common fixed value (e.g., an upper limit value, a lower limit value, a value between 0 and 1, a predetermined value, etc.).\n In some variations, S240 includes identifying an optimal visualization to be transmitted (e.g., from a remote computing system) and received (e.g., at a user device), which functions to prepare an optimal output for a 2.sup.nd point of care\n(e.g., specialist), reduce the time required to review the data packet, bring attention to the most relevant image data, or to effect any other suitable outcome.\n In some variations, this is involves a reverse registration process.  In a specific example, for instance, this is done through maximum intensity projection (MIP), where an optimal range of instances is determined based on which images contain\nthe largest percentage of the segmented anatomical region of interest in a MIP image.\n Additionally or alternatively, S240 can include removing and/or altering (e.g., encrypting) metadata or any unnecessary, private, confidential, or sensitive information from the data packet.  In some variations, patient information (e.g.,\npatient-identifiable information) is removed from the data packet in order to comply with regulatory guidelines.  In other variations, all metadata are extracted and removed from the data packet.\n In some variations, S240 includes storing a dataset (e.g., at a remote server, at a local server, at a PACS server, etc.).  In one example, metadata are extracted from the image data and stored separately from image data in a relational\ndatabase.  In another example, any or all of the data packet are stored (e.g., temporarily, permanently, etc.) to be used in one or more future analytics processes, which can function to improve the method, better match patients with suitable treatment\noptions, or for any other suitable purpose.\n In some variations, S240 includes applying a low bandwidth implementation process, which can function to reduce the time until a specialist receives a first piece of data or data packet (e.g., an incomplete series, incomplete study, single\ninstance, single image, optimal image, image showing occlusion, etc.), reduce the processing required to inform a specialist of a potential patient condition, reduce the amount of data required to be reviewed by a specialist, reduce the amount of data\nbeing transmitted from a remote computing system to a mobile device, or perform any other suitable function.  The low bandwidth implementation process can include any or all of: organizing (e.g., chunking) data (e.g., chunking a series of images based on\nanatomical region), reordering data (e.g., reordering slices in a CT series), transmitting a portion (e.g., single image, single slice, etc.) of a data packet (e.g., series, study, set of images, etc.) to a device (e.g., user device, mobile device,\nhealthcare facility workstation, computer, etc.), sending the rest of the data packet (e.g., only in response to a request, after a predetermined time has passed, once the data packet has been fully processed, etc.), or any other process.  In a specific\nexample, for instance, the image data (e.g., slices) received at a remote computing system from a scanner are chunked, reordered, and a single slice is sent to the device associated with a specialist first (e.g., prior to sending a remaining set of\nslices).\n Additionally or alternatively, S240 can include any other suitable steps performed in any suitable order.\n The method can additionally or alternatively include any other suitable sub-steps for preparing the data packet.\n 4.7 Method--Transmitting Information to a Device Associated with the 2.sup.nd Point of Care S250\n Transmitting information to a device associated with the 2.sup.nd point of care (e.g., specialist, contact, etc.) S250 (e.g., as shown in FIG. 6) functions to initiate a pull from a 1.sup.st point of care to a 2.sup.nd point of care, which can\ndecrease time to care, improve quality of care (e.g., better match between patient condition and specialist), or have any other suitable outcome.  Preferably, the 2.sup.nd point of care is a hub facility (e.g., specialist facility, interventional center,\ncomprehensive stroke center, etc.).  In some variations, the 1.sup.st point of care (e.g., healthcare facility at which patient initially presents) also functions as the 2.sup.nd point of care, such as when a suitable specialist is associated with the\n1.sup.st point of care, the 1.sup.st point of care is a hub (e.g., specialist facility, interventional center, comprehensive stroke center, etc.), it is not advised to transfer the patient (e.g., condition has high severity), or for any other reason.\n S250 is preferably performed after (e.g., in response to) a 2.sup.nd point of care is determined, but can additionally or alternatively be performed after a data packet (e.g., compressed data packet, encrypted data packet, etc.) has been\ndetermined, multiple times throughout the method (e.g., to multiple recipients, with multiple data packets, with updated information, after a predetermined amount of time has passed since a notification has been sent to a first choice specialist, etc.),\nor at any other time during the method 200.\n The device is preferably a user device, further preferably a mobile device.  Examples of the user device include a tablet, smartphone, mobile phone, laptop, watch, wearable device (e.g., glasses), or any other suitable user device.  The user\ndevice can include power storage (e.g., a battery), processing systems (e.g., CPU, GPU, memory, etc.), user outputs (e.g., display, speaker, vibration mechanism, etc.), user inputs (e.g., a keyboard, touchscreen, microphone, etc.), a location system\n(e.g., a GPS system), sensors (e.g., optical sensors, such as light sensors and cameras, orientation sensors, such as accelerometers, gyroscopes, and altimeters, audio sensors, such as microphones, etc.), data communication system (e.g., a WiFi module,\nBLE, cellular module, etc.), or any other suitable component.\n The device is preferably associated (e.g., owned by, belonging to, accessible by, etc.) a specialist or other individual associated with the 2.sup.nd point of care, but can additionally or alternatively be associated with an individual or\ncomputing system at the 1.sup.st point of care, the patient, or any other suitable individual or system.\n In one variation, the device is a personal mobile phone of a specialist.  In another variation, the device is a workstation at a healthcare facility (e.g., first point of care, second point of care, etc.).\n The information preferably includes a data packet, further preferably the data packet prepared in S240.  Additionally or alternatively, the information can include a subset of a data packet, the original data packet, any other image data set, or\nany other suitable data.  The information further preferably includes a notification, wherein the notification prompts the individual to review the data packet at the device (e.g., a message reciting \"urgent: please review!\").  Additionally or\nalternatively, the notification can prompt the individual to review data (e.g., original data packet, uncompressed images, etc.) at a separate device, such as a workstation in a healthcare facility, a PACS server, or any other location.  Further\nadditionally or alternatively, the notification can include any suitable information, such as, but not limited to: instructions (e.g., for treating patient, directions for reaching a healthcare facility), contact information (e.g., for emergency\nphysician at first point of care, administrative assistant, etc.), patient information (e.g., patient history), or any other suitable information.\n The notification preferably includes an SMS text message but can additionally or alternatively include an email message, audio message (e.g., recording sent to mobile phone), push notification, phone call, or any other suitable notification.\n The information is preferably sent to the device through a client application executing on the user device but can additionally or alternatively be sent through a messaging platform, web browser, or other platform.\n In some variations, a notification is sent which prompts the individual to provide an input, wherein the input can indicate that the individual will view, has viewed, or is in the process of viewing the information (e.g., image data), sees the\npresence of a condition (e.g., true positive, serious condition, time-sensitive condition, etc.), does not see the presence of a condition (e.g., false positive, serious condition, time-sensitive condition, etc.), has accepted treatment of the patient\n(e.g., swipes right, swipes up, clicks a check mark, etc.), has denied treatment of the patient (e.g., swipes left, swipes down, clicks an `x`, etc.), wants to communicate with another individual (e.g., healthcare worker at 1.sup.st point of care), such\nas through a messaging platform (e.g., native to the device, enabled by the client application, etc.), or any other input.  In some variations, one or more additional notifications are provided to the individual (e.g., based on the contents of the\ninput), which can be determined by a lookup table, operator, individual, decision engine, or other tool.  In one example, for instance, if the individual indicates that the condition is a true positive, information related to the transfer of the patient\n(e.g., estimated time of arrival, directions to the location of the patient, etc.) can be provided (e.g., in a transfer request, wherein patient transfer to a specified location, such as the 2.sup.nd point of care, can be initiated upon transfer request\nreceipt).  In some variants, the data (e.g., images) are displayed on the user device (e.g., mobile device, workstation) in response to user interaction with the notification (e.g., in response to input receipt).  However, the input can trigger any\nsuitable action or be otherwise used.\n Additionally or alternatively, an input can automatically be received from the client application, such as a read receipt when the individual has opened the data packet, viewed the notification, or interacted with the client application in any\nother suitable way.  In one example, if a read receipt is not received (e.g., at the remote computing system) from the device within a predetermined amount of time (e.g., 10 seconds), a second notification and/or data packet (e.g., compressed set of\nimages) are sent to a second individual (e.g., second choice specialist based on a lookup table).\n In some variations, various outputs can be sent from the client application (e.g., at the user device) to one or more recipients (e.g., to a second user device, client application on a work station, on a computing system, etc.), such as\nrecipients associated with a first point of care (e.g., radiologists, emergency physicians, etc.).  The outputs can be determined based on the inputs received at the client application associated with the individual (e.g., acceptance of case,\nverification of true positive, etc.), based on a lookup table, or otherwise determined.  The outputs preferably do not alter the standard radiology workflow (e.g., are not shared with radiologists; radiologists are not notified), which functions to\nensure that the method 200 is a true parallel process, and that the standard radiology workflow results in an independent assessment of the patient, but can additionally or alternatively cut short a workflow, bring a specialist in on the patient case\nearlier than normal, or affect any other process in a healthcare facility.\n 4.8 Method--Aggregating Data S260\n The method 200 can optionally include any number of sub-steps involving the aggregation of data involved in and/or generated during the method 200, which can function to improve future iterations of the method 200 (e.g., better match patients\nwith a specialist, decrease time to treat a patient, increase sensitivity, increase specificity, etc.).  The aggregated data is preferably used in one or more analytics steps (e.g., to refine a treatment option, make a recommendation for a drug or\nprocedure, etc.), but can additionally or alternatively be used for any other suitable purpose.\n In some variations, for instance the outcomes of the patients examined during the method 200 are recorded and correlated with their corresponding data packets, which can be used to assess the success of the particular treatment options chosen\nand better inform treatment options in future cases.\n 4.9 Method--Variations\n In one variation, the method functions to augment a standard radiology workflow operating in parallel with the method, which can include any or all of: at a remote computing system (e.g., remote from the first point of care), receiving a set of\nimages (e.g., of a brain of the patient), wherein the set of images is concurrently sent to the standard radiology workflow operating in parallel with the method and automatically detecting a condition (e.g., potential large vessel occlusion) from the\nset of images.  Upon condition detection, the method can include any or all of, automatically: determining a second specialist from the standard radiology workflow, wherein the specialist is associated with a second point of care; notifying the second\nspecialist on a mobile device associated with the second specialist before the radiologist notifies the first specialist; and displaying a compressed version of the set of images on the mobile device.\n In a specific example, the method includes, at a remote computing system, receiving a set of Digital Imaging and Communications in Medicine (DICOM) brain images associated with the patient, wherein the set of DICOM brain images is concurrently\nsent to a standard radiology workflow operating in parallel with the method.  In the standard radiology workflow, the radiologist analyzes the set of DICOM brain images and notifies a specialist based on a visual assessment of the set of DICOM brain\nimages at the workstation, wherein the standard radiology workflow takes a first amount of time.  The method can then include detecting a potential cerebral artery occlusion from the set of DICOM brain images, which includes any or all of: identifying a\nlarge vessel region from the set of DICOM brain images; extracting a centerline from the large vessel region; determining a centerline length of the large vessel region based on the centerline; comparing the centerline length with a predetermined\nthreshold; and detecting the potential cerebral artery occlusion when the centerline length is less than the predetermined threshold.  Upon potential cerebral artery occlusion detection, the method can include, automatically: determining the specialist\nfrom the standard radiology workflow, wherein the specialist is associated with a second point of care; notifying the specialist on a mobile device associated with the specialist, wherein the specialist is notified in a second amount of time shorter than\nthe first amount of time, wherein the radiologist is not automatically notified upon potential cerebral artery occlusion detection; displaying a compressed version of the set of DICOM brain images on the mobile device; and displaying a high-resolution\nversion of the set of DICOM brain images on a workstation associated with the specialist.  Additionally or alternatively, the method can include any other suitable process.\n In another variation, the method functions to determine a specialist (e.g., independently of the performance of a radiology workflow, in parallel with a radiologist workflow, bypassing a radiologist workflow, etc.), where the method includes:\nreceiving a data packet comprising a set of images (e.g., CT images of a brain of the patient) sampled at the first point of care, where the data packet is concurrently sent to the standard radiology workflow; determining an anatomical feature (e.g.,\nlarge vessel region) from the set of images; extracting a feature (e.g., large vessel centerline) from the region; determining a parameter (e.g., calculating a centerline length) of the feature; and comparing the parameter (e.g., centerline length) with\na predetermined threshold.  In one example, the method can then include detecting a large vessel occlusion when the centerline length is less than the predetermined threshold.  In response to the detection of a condition (e.g., large vessel occlusion\ndetection), the method can include any or all of: presenting a notification on a mobile device associated with a specialist from the standard radiology workflow, the specialist associated with a second point of care, displaying a compressed version of\nthe set of images on the mobile device in response to interaction with the notification, or any other suitable process.\n In a specific example, the method includes: receiving, at a remote computing system, a data packet from the first point of care, the data packet including a set of computed tomography (CT) images and a set of metadata associated with the set of\nCT images; processing the data packet at the remote computing system, which can include any or all of: organizing the set of CT images into a series based on the metadata, identifying soft matter voxels from the series based on a soft matter mask, the\nsoft matter mask including a predetermined Hounsfield Unit (HU) threshold, registering the soft matter voxels to a set of reference CT images, thereby determining a registered set of voxels, segmenting (e.g., with a feed-forward deep convolutional\nnetwork) a large vessel region in the registered set of voxels, extracting a centerline of the segmented large vessel region, and determining a length of the segmented large vessel region based on the centerline.  With the centerline length, the method\ncan then include: comparing the centerline length with a predetermined threshold, wherein the predetermined threshold is greater than a corresponding anatomical length.  When the centerline length is less than the predetermined threshold, a specialist\ncan be determined based on a lookup table.  Then, a notification and a second data packet comprising a set of compressed images can be transmitted to a user device associated with the specialist.\n Additionally or alternatively, the method can include any other steps performed in any suitable order.\n Although omitted for conciseness, the preferred embodiments include every combination and permutation of the various system components and the various method processes, wherein the method processes can be performed in any suitable order,\nsequentially or concurrently.\n As a person skilled in the art will recognize from the previous detailed description and from the figures and claims, modifications and changes can be made to the preferred embodiments of the invention without departing from the scope of this\ninvention defined in the following claims.", "application_number": "16012458", "abstract": " A system for computer-aided triage can include a router, a remote\n     computing system, and a client application. A method for computer-aided\n     triage can include determining a parameter associated with a data packet,\n     determining a treatment option based on the parameter, and transmitting\n     information to a device associated with a second point of care.\n", "citations": ["6349330", "8374414", "9307918", "20060140473", "20080021502", "20090279752", "20120065987", "20130208966", "20140142982", "20140142983", "20150320365", "20160037057", "20160063191", "20160180042", "20170143428", "20170147765", "20170228516", "20170258433", "20170340260", "20180085001", "20180116620"], "related": ["62535973", "62535970", "62521968"]}, {"id": "20190007350", "patent_code": "10374982", "patent_name": "Response retrieval using communication session vectors", "year": "2019", "inventor_and_country_data": " Inventors: \nKoukoumidis; Emmanouil (Kirkland, WA), Johnson, Jr.; Joseph Edwin (Seattle, WA), Mu; Hailong (Redmond, WA), Schuerman; Matthew W (Kirkland, WA), Wang; Ying (Bellevue, WA)  ", "description": "TECHNICAL FIELD\n Aspects and implementations of the present disclosure relate to data processing and, more specifically, but without limitation, to response retrieval using communication session vectors.\nBACKGROUND\n Personal digital assistants are applications or services that retrieve information or execute tasks on behalf of a user.  Users can communicate with such personal digital assistants using conversational interfaces such as messaging or chat\ninterfaces.\nSUMMARY\n The following presents a shortened summary of various aspects of this disclosure in order to provide a basic understanding of such aspects.  This summary is not an extensive overview of all contemplated aspects, and is intended to neither\nidentify key or critical elements nor delineate the scope of such aspects.  Its purpose is to present some concepts of this disclosure in a compact form as a prelude to the more detailed description that is presented later.\n In one aspect of the present disclosure, systems and methods are disclosed for response retrieval using communication session vectors.  In one implementation, a first communication session is received.  The first communication session includes a\nfirst communication.  The first communication session is encoded as a first vector.  A second vector is identified within a defined proximity of the first vector.  The second vector represents a second communication session that includes a second\ncommunication.  The second communication is provided within the first communication session in response to the first communication. BRIEF DESCRIPTION OF THE DRAWINGS\n Aspects and implementations of the present disclosure will be understood more fully from the detailed description given below and from the accompanying drawings of various aspects and implementations of the disclosure, which, however, should not\nbe taken to limit the disclosure to the specific aspects or implementations, but are for explanation and understanding only.\n FIG. 1 illustrates an example system, in accordance with an example embodiment.\n FIG. 2 is a flow chart illustrating a method, in accordance with an example embodiment, for response retrieval using communication session vectors.\n FIG. 3 illustrates an example scenario described herein, according to an example embodiment.\n FIG. 4 illustrates an example scenario described herein, according to an example embodiment.\n FIGS. 5A and 5B illustrate example scenarios described herein, according to an example embodiment.\n FIG. 6 is a block diagram illustrating components of a machine able to read instructions from a machine-readable medium and perform any of the methodologies discussed herein, according to an example embodiment.\nDETAILED DESCRIPTION\n Aspects and implementations of the present disclosure are directed to response retrieval using communication session vectors.\n It can be appreciated that intelligent personal assistants and related technologies can enable a user to obtain information, execute tasks, and perform other activities.  Users can interact with or control such personal assistants via\nconversational interfaces such as messaging, chat, audio commands etc. Though such conversational interfaces provide a natural and intuitive medium for performing certain tasks, these interfaces are limited when attempting to mimic certain aspects of\nhuman conversation.  For example, existing technologies can identify relevant content that is semantically similar or identical to communications/content provided by a user (e.g., including the same/similar words, entities, etc.).  However, in many\nscenarios such related content may not contain the same (or even similar) words/entities to communications originating from the user (e.g., in scenarios in which different users refer to such entities using different words).\n Accordingly, described herein in various implementations are technologies, including methods, machine readable mediums, and systems, that enable response retrieval using communication session vectors.  In certain implementations, the described\ntechnologies encode a current conversation/communication session (e.g., between a human user and a personal assistant) as a vector.  Similar vector(s) (corresponding to related conversations, such as those occurring between two or more human users) can\nthen be identified within a vector space.  Appropriate/relevant communication(s) can then be identified within such related conversation(s) and such communications may be further utilized within the current session.  In doing so, relevant communications\nthat are contextually appropriate can be identified and provided to the user, even in scenarios in which the respective sessions share few words/entities in common.  Additionally, the described technologies can leverage communications originally\noccurring between human users to enhance subsequent communications provided by a personal assistant.  For example, the personal assistant can incorporate a contextually relevant communication (originally provided by a human user) to provide a more\nnatural and relevant experience.\n It can therefore be appreciated that the described technologies are directed to and address specific technical challenges and longstanding deficiencies in multiple technical areas, including but not limited to communication interfaces, semantic\nrelationships, and personal digital assistants.  As described in detail herein, the disclosed technologies provide specific, technical solutions to the referenced technical challenges and unmet needs in the referenced technical fields and provide\nnumerous advantages and improvements upon conventional approaches.  Additionally, in various implementations one or more of the hardware elements, components, etc., referenced herein operate to enable, improve, and/or enhance the described technologies,\nsuch as in a manner described herein.\n FIG. 1 illustrates an example system 100, in accordance with some implementations.  As shown, the system 100 includes device 110 which can be a laptop computer, a desktop computer, a terminal, a mobile phone, a tablet computer, a smart watch, a\npersonal digital assistant (PDA), a digital music player, a server, and the like.  User 130 can be a human user who interacts with device 110.  For example, user 130 can provide various inputs (e.g., via an input device/interface such as a keyboard,\nmouse, touchscreen, etc.) to device 110.  Device 110 can also display, project, and/or otherwise provide content to user 130 (e.g., via output components such as a screen, speaker, etc.).\n As shown in FIG. 1, device 110 can include personal assistant 116.  Personal assistant 116 can be an application or module that configures/enables the device to interact with, provide content to, and/or otherwise perform operations on behalf of\nuser 130.  For example, personal assistant 116 can receive communications and/or request(s) from user 130 and present/provide responses to such request(s) (e.g., within a conversational or `chat` interface).  In certain implementations, personal\nassistant 116 can also identify content that can be relevant to user 130 (e.g., based on a location of the user or other such context) and present such content to the user.  Personal assistant 116 can also enable user 130 to initiate and/or configure\nother application(s).  For example, personal assistant 116 can initiate an application (e.g., a media player application) that fulfills a request provided by the user.  Personal assistant 116 can also initiate and/or perform various other operations,\nsuch as are described herein.\n It should be noted that while various components (e.g., personal assistant 116) are depicted and/or described as operating on a device 110, this is only for the sake of clarity.  However, in other implementations the referenced components can\nalso be implemented on other devices/machines.  For example, in lieu of executing locally at device 110, aspects of personal assistant 116 can be implemented remotely (e.g., on a server device or within a cloud service or framework).  By way of\nillustration, personal assistant 116 can be configured to execute on a remote device (e.g., server 140, as described below) and provide communications, information, etc., to device 110.\n In certain implementations, device 110 can include other applications, programs, modules, etc. The referenced applications can be stored in memory of device 110 (e.g. memory 630 as depicted in FIG. 6 and described below).  One or more\nprocessor(s) of device 110 (e.g., processors 610 as depicted in FIG. 6 and described below) can execute such application(s).  In doing so, device 110 can be configured to perform various operations, present content to user 130, etc. Examples of such\napplications include but are not limited to: social media/messaging applications, applications that facilitate transactions (e.g., food purchases), etc.\n It should also be noted that while various components (e.g., personal assistant 116) are depicted (e.g., in FIG. 1) as operating on device 110, this is only for the sake of clarity.  However, in other implementations the referenced component can\nalso be implemented on other devices/machines.  For example, in lieu of executing locally at device 110, personal assistant 116 can be implemented remotely (e.g., on a server device or within a cloud service or framework).\n As also shown in FIG. 1, device 110 can connect to and/or otherwise communicate with server 140 via network 120.  Network 120 can include one or more networks such as the Internet, a wide area network (WAN), a local area network (LAN), a virtual\nprivate network (VPN), an intranet, and the like.\n Server 140 can be, for example, a server computer, computing device, storage service (e.g., a `cloud` service), etc., and can include encoding engine 142, communication coordination engine 144, session repository 160, and vector space 180.\n Session repository 160 can include various communication sessions (e.g., communication session 170A, communication session 170B) (collectively, communication sessions 170).  Such a communication session 170 can be a series of communications 150\n(e.g., messages originating from various users/sources) that are provided in relation to one another (e.g., in a sequence, in response to one another, within a defined amount of time/chronological proximity to one another, etc.).  In certain\nimplementations, such a communication session can refer to a group or set of communications provided back-and-forth between various users, applications, etc., e.g., during a conversation conducted via a messaging, chat, social networking, etc.\nservice/application.  Such communications 150 (e.g., communication 150A, communication 150B, etc., as shown in FIG. 1) can be stored as a communication session 170 within session repository 160.  For example, as shown in FIG. 1, communication 150A (e.g.,\na communication from one user) and communication 150B (e.g., a response provided by another user) can be stored together and/or in association with one another within communication session 170A.\n In certain implementations, communication session 170 can also store metadata (e.g., metadata 152A as stored in communication session 170A).  Such metadata can include information or other characteristics of a communication session.  For\nexample, metadata 152 can include or reflect various aspects of the emotion(s) associated with the associated communications.  In certain implementations, such emotion(s) (e.g., happiness, sadness, excitement, etc.) can be identified, for example, using\nvarious sentiment analysis techniques.\n Encoding engine 142 can be an application or module that configures/enables server 140 to perform various operations such as are described herein.  For example, encoding engine 142 can configure or enable server 140 to process/analyze\ncommunication sessions 170, such as those stored in session repository 160.  Encoding engine 142 can further generate various representations or models (e.g., multidimensional vectors) of the referenced communications/communication sessions.  Such\nrepresentations/vectors can be models that reflect the respective communications contained within a communication session (as well as the content of such communications, e.g., entities, topics, etc., referenced within such communications).  For example,\nvarious words, entities, concepts, etc., can be identified within a communication (e.g., using various natural language processing techniques).  Such words, entities, etc., can then be mapped to vectors (e.g., of real numbers) as reflected in a vector\nspace, e.g., using various language modeling and/or feature learning techniques.\n The various vectors generated by encoding engine 142 can be stored within an index such as vector space 180.  For example, as shown in FIG. 1, vectors 190A, 190B, and 190C are generated by encoding engine 142 and stored within vector space 180. \nEach of the referenced vectors can correspond to a difference communication session (e.g., vector 190A corresponds to communication session 170A, vector 190B corresponds to communication session 170B, etc.).  Additionally, the vectors 190 can be stored\nwithin vector space 180 such that vectors that are more related (e.g., based on content contained within their respective communications, metadata/other characteristics, etc.) are positioned relatively closer together within the vector space. \nAccordingly, vectors 190A and 190B can be determined (based on their relatively close proximity within vector space 180) to be related to one another (while vector 190C, which is positioned relatively further away, is relatively unrelated to the other\nvectors).\n Communication coordination engine 144 can be an application or module that configures/enables server 140 to perform various operations such as are described herein.  For example, communication coordination engine 144 can configure or enable\nserver 140 to utilize the referenced vectors 190 (as stored in vector space 180) to identify or generate communications to be provided to a user, e.g., via a personal assistant.  As described herein, communication coordination engine 144 can receive (or\naccess) a current conversation/communication session (e.g., between user 130 and personal assistant 116), and encode the communication session as a vector.  Using vector space 180, other vector(s) can be identified that are similar/related to the current\ncommunication session.  Such similar/relevant vectors can then be used to identify an appropriate/relevant communication to be provided within the current communication session.  In doing so, relevant communications that are contextually appropriate can\nbe identified and provided to the user.\n In various implementations, the described technologies may control, instruct, or otherwise communicate with various services such as service 128A and service 128B (collectively services 128), as shown in FIG. 1.  Such services can be, for\nexample, third-party services that can provide various functionality, e.g., to user 130 (e.g., social media services, food delivery services, etc.) that may enhance or otherwise be relevant to certain operations described herein.\n While many of the examples described herein are illustrated with respect to a single server 140, this is simply for the sake of clarity and brevity.  However, it should be understood that the described technologies can also be implemented (in\nany number of configurations) across multiple servers and/or other computing devices/services.\n Further aspects and features of device 110 and server 140 are described in more detail in conjunction with FIGS. 2-6, below.\n As used herein, the term \"configured\" encompasses its plain and ordinary meaning.  In one example, a machine is configured to carry out a method by having software code for that method stored in a memory that is accessible to the processor(s) of\nthe machine.  The processor(s) access the memory to implement the method.  In another example, the instructions for carrying out the method are hard-wired into the processor(s).  In yet another example, a portion of the instructions are hard-wired, and a\nportion of the instructions are stored as software code in the memory.\n FIG. 2 is a flow chart illustrating a method 200, according to an example embodiment, for response retrieval using communication session vectors.  The method is performed by processing logic that can comprise hardware (circuitry, dedicated\nlogic, etc.), software (such as is run on a computing device such as those described herein), or a combination of both.  In one implementation, the method 200 is performed by one or more elements depicted and/or described in relation to FIG. 1 (including\nbut not limited to server 140, encoding engine 142, and/or communication coordination engine 144), while in some other implementations, the one or more blocks of FIG. 2 can be performed by another machine or machines.\n For simplicity of explanation, methods are depicted and described as a series of acts.  However, acts in accordance with this disclosure can occur in various orders and/or concurrently, and with other acts not presented and described herein. \nFurthermore, not all illustrated acts may be required to implement the methods in accordance with the disclosed subject matter.  In addition, those skilled in the art will understand and appreciate that the methods could alternatively be represented as a\nseries of interrelated states via a state diagram or events.  Additionally, it should be appreciated that the methods disclosed in this specification are capable of being stored on an article of manufacture to facilitate transporting and transferring\nsuch methods to computing devices.  The term article of manufacture, as used herein, is intended to encompass a computer program accessible from any computer-readable device or storage media.\n At operation 210, a first communication session (or a portion/segment thereof) is received.  Such a communication session can include a first communication, e.g., a communication originating from a user (e.g., user 130).  Such a communication\ncan be, for example, a message/transmission (e.g., as provided within a messaging/chat interface or any other such communication framework).  Such a communication session can include or reflect several communications that are provided in relation to one\nanother (e.g., in a sequence, in response to one another, within a defined amount of time/chronological proximity to one another, etc.).\n For example, FIG. 3 depicts an example scenario in which communication session 370B is received.  Such a communication session can be a sequence of communications between a user (`User1`) and a personal assistant (`PA`).  As shown in FIG. 3,\ncommunication session 370B can include communication 350E originating from `User1` (\"Do you like terriers?\").\n By way of further example, FIG. 4 depicts an example scenario in which communication session 470B is received.  Such a communication session can be a sequence of communications between a user (`User1`) and a personal assistant (`PA`).  As shown\nin FIG. 4, communication session 470B can include communication 450C originating from `User1` (\"I am going to Miami\").\n At operation 220, an emotion associated with the first communication session is identified.  For example, various sentiment analysis techniques (e.g., based on the tone of communications originating from a user) can be applied to the\ncommunications of communication session 370B to determine emotion(s) (e.g., happiness) associated with the communication session, various degrees, intensities, etc., associated with such emotion(s), etc. Such identified emotion(s) can be\nstored/associated with the communication session (e.g., as metadata 352A, as shown in FIG. 3).  It should be understood that, in certain implementations, the referenced emotions can include multiple emotions (e.g., a certain degree of happiness, another\ndegree of excitement, etc.).\n At operation 230, the first communication session (e.g., the communication session received at operation 210) is encoded as a vector (or other such representation).  In doing so, a vector such as a multidimensional vector can be generated which\nreflects the various communications contained within the communication session.  Such a vector can be stored within an index such as a vector space.  Additionally, in certain implementations the referenced vector can be encoded using various deep\nlearning techniques.  In doing so, the vector can represent various similarities between concepts reflected in the various communications (e.g., even in scenarios--such as those described/illustrated herein--in which related concepts are associated with\ndifferent words).\n For example, as shown in FIG. 3, vector 190A can be generated which reflects the various aspects of the communications included within communication session 370B (e.g., communications 350E, 350F, 350G).  Additionally, in certain implementations\nsuch a vector 190A can be encoded to further reflect additional aspects/metadata associated with the communication session 370B, such as metadata 352A (corresponding to the emotion associated with the communication session).  The generated vector 190A\ncan be stored within multidimensional vector space 380, as shown.\n By way of further example, as shown in FIG. 4, vector 490A can be generated which reflects the various aspects of the communications included within communication session 470B (e.g., communication 450C).  Additionally, in certain implementations\nsuch a vector 490A can be encoded to further reflect additional aspects/metadata associated with the communication session 470B, such as metadata 452A (corresponding to the emotion associated with the communication session).  The generated vector 490A\ncan be stored within multidimensional vector space 480, as shown.\n At operation 240, a second vector is identified.  In certain implementations, such a vector can be identified within the same vector space/index referenced above.  Additionally, in certain implementations such a second vector can be identified\nwithin a certain proximity of the first vector (e.g., the vector encoded at operation 230).  In certain implementations, such a proximity can be identified/determined using nearest neighbor search and/or other such techniques to compute the referenced\nproximity between vectors.  Such a second vector can represent another communication session (which is made up of other communication(s)).\n For example, as shown in FIG. 3, vector 190B can be identified, e.g., as the vector within vector space 380 that is closest to vector 190A.  As shown in FIG. 3, vector 190B is a representation of communication session 370A which includes various\ncommunications between `User2` and `User3` (e.g., communications 350A, 350B, 350C, 350D and the like).  It should be noted that such users are not participants in communication session 370B (though, in other implementations, the described technologies\ncan also utilize communication sessions having common participants).  In contrast, vector 190C (corresponding to communication session 370C which includes communications 350H and 350I and metadata 352B) is positioned relatively far away from vectors 190A\nand 190B (on account of such a vector 190C being unrelated/irrelevant to the content of communication sessions 370A and 370B).\n By way of further example, as shown in FIG. 4, vector 490B can be identified, e.g., as the vector within vector space 480 that is closest to vector 490A.  As shown in FIG. 4, vector 490B is a representation of communication session 470A which\nincludes various communications between `User2` and `User3` (e.g., communications 450A, 450B, etc.).\n As noted above, in certain implementations metadata (e.g., emotions) can be associated with such communication sessions and further reflected in their corresponding vectors.  Accordingly, identifying other vectors) that are close in proximity to\na first vector (e.g., within a vector space) can further account for such metadata.  For example, as shown in FIG. 3, communication session 370A is also associated with metadata 352A (corresponding to the emotion of happiness), as reflected in the close\nproximity between vector 190B and vector 190A.\n At operation 250, a communication from the communication session corresponding to the second vector (as identified at operation 250) is provided within the first communication session.\n For example, as shown in FIG. 3, having identified communication session 370A as being highly relevant to communication session 370B (based on the proximity of vectors 190A and 190B), communication 350D can be selected (from communication\nsession 370A) and provided within communication session 370B.  In doing so, even though various words, entities, etc., within the respective communication sessions are not necessarily semantically similar (e.g. `collies` in session 370A and `terriers` in\nsession 370B), the respective vector representations can reflect the overall relevance/similarities between the communication sessions.  Accordingly, having identified such relevance/similarity, communication 350D can be identified as contextually\nrelevant/appropriate to the communication sequence of communication session 370B.\n Having identified such relevance, communication 350D can be inserted/provided within communication session 370B (e.g., by the referenced personal assistant).  Such a communication 350D can be inserted based on its position within communication\nsession 370A as the next communication following the portion/segment of the communication session determined to be relevant to communication session 370B (thus suggesting that communication 350D is a contextually relevant/appropriate response to be\nprovided within communication session 370B, as shown.  In doing so, the personal assistant provides a communication to the user that is determined (e.g., based on communication session 370A) to be contextually relevant.  FIG. 5A depicts an example\ngraphical user interface of personal assistant 116 in which communication 350D has been inserted into the communication session in the manner described above.  It should be understood that in certain implementations such a communication can be inserted\nverbatim into the referenced communication session (e.g., as shown with respect to communication 350D).  In other implementations, various modifications or adjustments can be made to the communication prior to/in conjunction with inserting it into the\ncommunication session (e.g., to personalize the communication, etc.).\n By way of further example, as shown in FIG. 4, having identified communication session 470A as being highly relevant to communication session 470B (based on the proximity of vectors 490A and 490B), communication 450B can be selected (from\ncommunication session 470A) and provided within communication session 470B.  In doing so, even though various words, entities, etc., within the respective communication sessions are not necessarily semantically similar (e.g. `traveling to Florida` in\nsession 470A and `going to Miami` in session 470B), the respective vector representations can reflect the overall relevance/similarities between the communication sessions.  Accordingly, having identified such relevance/similarity, communication 450B can\nbe identified as contextually relevant/appropriate to the communication sequence of communication session 470B.  Having identified such relevance, communication 45013 can be inserted/provided within communication session 470B (e.g., by the referenced\npersonal assistant).  In doing so, the personal assistant provides a communication to the user that is determined (e.g., based on communication session 470A) to be contextually relevant.  FIG. 5B depicts an example graphical user interface of personal\nassistant 116 in which communication 450B has been inserted into the communication session in the manner described above.\n It should also be noted that while the technologies described herein are illustrated primarily with respect to response retrieval using communication session vectors, the described technologies can also be implemented in any number of additional\nor alternative settings or contexts and towards any number of additional objectives.  It should be understood that further technical advantages, solutions, and/or improvements (beyond those described and/or referenced herein) can be enabled as a result\nof such implementations.\n Certain implementations are described herein as including logic or a number of components, modules, or mechanisms.  Modules can constitute either software modules (e.g., code embodied on a machine-readable medium) or hardware modules.  A\n\"hardware module\" is a tangible unit capable of performing certain operations and can be configured or arranged in a certain physical manner.  In various example implementations, one or more computer systems (e.g., a standalone computer system, a client\ncomputer system, or a server computer system) or one or more hardware modules of a computer system (e.g., a processor or a group of processors) can be configured by software (e.g., an application or application portion) as a hardware module that operates\nto perform certain operations as described herein.\n In some implementations, a hardware module can be implemented mechanically, electronically, or any suitable combination thereof.  For example, a hardware module can include dedicated circuitry or logic that is permanently configured to perform\ncertain operations.  For example, a hardware module can be a special-purpose processor, such as a Field-Programmable Gate Array (FPGA) or an Application Specific Integrated Circuit (ASIC).  A hardware module can also include programmable logic or\ncircuitry that is temporarily configured by software to perform certain operations.  For example, a hardware module can include software executed by a general-purpose processor or other programmable processor.  Once configured by such software, hardware\nmodules become specific machines (or specific components of a machine) uniquely tailored to perform the configured functions and are no longer general-purpose processors.  It will be appreciated that the decision to implement a hardware module\nmechanically, in dedicated and permanently configured circuitry, or in temporarily configured circuitry (e.g., configured by software) can be driven by cost and time considerations.\n Accordingly, the phrase \"hardware module\" should be understood to encompass a tangible entity, be that an entity that is physically constructed, permanently configured (e.g., hardwired), or temporarily configured (e.g., programmed) to operate in\na certain manner or to perform certain operations described herein.  As used herein, \"hardware-implemented module\" refers to a hardware module.  Considering implementations in which hardware modules are temporarily configured (e.g., programmed), each of\nthe hardware modules need not be configured or instantiated at any one instance in time.  For example, where a hardware module comprises a general-purpose processor configured by software to become a special-purpose processor, the general-purpose\nprocessor can be configured as respectively different special-purpose processors (e.g., comprising different hardware modules) at different times.  Software accordingly configures a particular processor or processors, for example, to constitute a\nparticular hardware module at one instance of time and to constitute a different hardware module at a different instance of time.\n Hardware modules can provide information to, and receive information from, other hardware modules.  Accordingly, the described hardware modules can be regarded as being communicatively coupled.  Where multiple hardware modules exist\ncontemporaneously, communications can be achieved through signal transmission (e.g., over appropriate circuits and buses) between or among two or more of the hardware modules.  In implementations in which multiple hardware modules are configured or\ninstantiated at different times, communications between such hardware modules can be achieved, for example, through the storage and retrieval of information in memory structures to which the multiple hardware modules have access.  For example, one\nhardware module can perform an operation and store the output of that operation in a memory device to which it is communicatively coupled.  A further hardware module can then, at a later time, access the memory device to retrieve and process the stored\noutput.  Hardware modules can also initiate communications with input or output devices, and can operate on a resource (e.g., a collection of information).\n The various operations of example methods described herein can be performed, at least partially, by one or more processors that are temporarily configured (e.g., by software) or permanently configured to perform the relevant operations.  Whether\ntemporarily or permanently configured, such processors can constitute processor-implemented modules that operate to perform one or more operations or functions described herein.  As used herein, \"processor-implemented module\" refers to a hardware module\nimplemented using one or more processors.\n Similarly, the methods described herein can be at least partially processor-implemented, with a particular processor or processors being an example of hardware.  For example, at least some of the operations of a method can be performed by one or\nmore processors or processor-implemented modules.  Moreover, the one or more processors can also operate to support performance of the relevant operations in a \"cloud computing\" environment or as a \"software as a service\" (SaaS).  For example, at least\nsome of the operations can be performed by a group of computers (as examples of machines including processors with these operations being accessible via a network (e.g., the Internet) and via one or more appropriate interfaces (e.g., an API).\n The performance of certain of the operations can be distributed among the processors, not only residing within a single machine, but deployed across a number of machines.  In some example implementations, the processors or processor-implemented\nmodules can be located in a single geographic location (e.g., within a home environment, an office environment, or a server farm).  In other example implementations, the processors or processor-implemented modules can be distributed across a number of\ngeographic locations.\n The modules, methods, applications, and so forth described in conjunction with FIGS. 1-5B are implemented in some implementations in the context of a machine and an associated software architecture.  The sections below describe representative\nsoftware architecture(s) and machine (e.g., hardware) architecture(s) that are suitable for use with the disclosed implementations.\n Software architectures are used in conjunction with hardware architectures to create devices and machines tailored to particular purposes.  For example, a particular hardware architecture coupled with a particular software architecture will\ncreate a mobile device, such as a mobile phone, tablet device, or so forth.  A slightly different hardware and software architecture can yield a smart device for use in the \"internet of things,\" while yet another combination produces a server computer\nfor use within a cloud computing architecture.  Not all combinations of such software and hardware architectures are presented here, as those of skill in the art can readily understand how to implement the inventive subject matter in different contexts\nfrom the disclosure contained herein.\n FIG. 6 is a block diagram illustrating components of a machine 600, according to some example implementations, able to read instructions from a machine-readable medium (e.g., a machine-readable storage medium) and perform any one or more of the\nmethodologies discussed herein.  Specifically, FIG. 6 shows a diagrammatic representation of the machine 600 in the example form of a computer system, within which instructions 616 (e.g., software, a program, an application, an applet, an app, or other\nexecutable code) for causing the machine 600 to perform any one or more of the methodologies discussed herein can be executed.  The instructions 616 transform the general, non-programmed machine into a particular machine programmed to carry out the\ndescribed and illustrated functions in the manner described.  In alternative implementations, the machine 600 operates as a standalone device or can be coupled (e.g., networked) to other machines.  In a networked deployment, the machine 600 can operate\nin the capacity of a server machine or a client machine in a server-client network environment, or as a peer machine in a peer-to-peer (or distributed) network environment.  The machine 600 can comprise, but not be limited to, a server computer, a client\ncomputer, PC, a tablet computer, a laptop computer, a netbook, a set-top box (STB), a personal digital assistant (PDA), an entertainment media system, a cellular telephone, a smart phone, a mobile device, a wearable device (e.g., a smart watch), a smart\nhome device (e.g., a smart appliance), other smart devices, a web appliance, a network router, a network switch, a network bridge, or any machine capable of executing the instructions 616, sequentially or otherwise, that specify actions to be taken by\nthe machine 600.  Further, while only a single machine 600 is illustrated, the term \"machine\" shall also be taken to include a collection of machines 600 that individually or jointly execute the instructions 616 to perform any one or more of the\nmethodologies discussed herein.\n The machine 600 can include processors 610, memory/storage 630, and I/O components 650, which can be configured to communicate with each other such as via a bus 602.  In an example implementation, the processors 610 (e.g., a Central Processing\nUnit (CPU), a Reduced Instruction Set Computing (RISC) processor, a Complex Instruction Set Computing (CISC) processor, a Graphics Processing Unit (GPU), a Digital Signal Processor (DSP), an ASIC, a Radio-Frequency Integrated Circuit (RFIC), another\nprocessor, or any suitable combination thereof) can include, for example, a processor 612 and a processor 614 that can execute the instructions 616.  The term \"processor\" is intended to include multi-core processors that can comprise two or more\nindependent processors (sometimes referred to as \"cores\") that can execute instructions contemporaneously.  Although FIG. 6 shows multiple processors 610, the machine 600 can include a single processor with a single core, a single processor with multiple\ncores (e.g., a multi-core processor), multiple processors with a single core, multiple processors with multiples cores, or any combination thereof.\n The memory/storage 630 can include a memory 632, such as a main memory, or other memory storage, and a storage unit 636, both accessible to the processors 610 such as via the bus 602.  The storage unit 636 and memory 632 store the instructions\n616 embodying any one or more of the methodologies or functions described herein.  The instructions 616 can also reside, completely or partially, within the memory 632, within the storage unit 636, within at least one of the processors 610 (e.g., within\nthe processor's cache memory), or any suitable combination thereof, during execution thereof by the machine 600.  Accordingly, the memory 632, the storage unit 636, and the memory of the processors 610 are examples of machine-readable media.\n As used herein, \"machine-readable medium\" means a device able to store instructions instructions 616) and data temporarily or permanently and can include, but is not limited to, random-access memory (RAM), read-only memory (ROM), buffer memory,\nflash memory, optical media, magnetic media, cache memory, other types of storage (e.g., Erasable Programmable Read-Only Memory (EEPROM)), and/or any suitable combination thereof.  The term \"machine-readable medium\" should be taken to include a single\nmedium or multiple media (e.g., a centralized or distributed database, or associated caches and servers) able to store the instructions 616.  The term \"machine-readable medium\" shall also be taken to include any medium, or combination of multiple media,\nthat is capable of storing instructions (e.g., instructions 616) for execution by a machine (e.g., machine 600), such that the instructions, when executed by one or more processors of the machine (e.g., processors 610), cause the machine to perform any\none or more of the methodologies described herein.  Accordingly, a \"machine-readable medium\" refers to a single storage apparatus or device, as well as \"cloud-based\" storage systems or storage networks that include multiple storage apparatus or devices. \nThe term \"machine-readable medium\" excludes signals per se.\n The I/O components 650 can include a wide variety of components to receive input, provide output, produce output, transmit information, exchange information, capture measurements, and so on.  The specific I/O components 650 that are included in\na particular machine will depend on the type of machine.  For example, portable machines such as mobile phones will likely include a touch input device or other such input mechanisms, while a headless server machine will likely not include such a touch\ninput device.  It will be appreciated that the I/O components 650 can include many other components that are not shown in FIG. 6.  The I/O components 650 are grouped according to functionality merely for simplifying the following discussion and the\ngrouping is in no way limiting.  In various example implementations, the I/O components 650 can include output components 652 and input components 654.  The output components 652 can include visual components (e.g., a display such as a plasma display\npanel (PDP), a light emitting diode (LED) display, a liquid crystal display (LCD), a projector, or a cathode ray tube (CRT)), acoustic components (e.g., speakers), haptic components (e.g., a vibratory motor, resistance mechanisms), other signal\ngenerators, and so forth.  The input components 654 can include alphanumeric input components (e.g., a keyboard, a touch screen configured to receive alphanumeric input, a photo-optical keyboard, or other alphanumeric input components), point based input\ncomponents (e.g., a mouse, a touchpad, a trackball, a joystick, a motion sensor, or another pointing instrument), tactile input components (e.g., a physical button, a touch screen that provides location and/or force of touches or touch gestures, or other\ntactile input components), audio input components (e.g., a microphone), and the like.\n In further example implementations, the I/O components 650 can include biometric components 656, motion components 658, environmental components 660, or position components 662, among a wide array of other components.  For example, the biometric\ncomponents 656 can include components to detect expressions (e.g., hand expressions, facial expressions, vocal expressions, body gestures, or eye tracking), measure biosignals (e.g., blood pressure, heart rate, body temperature, perspiration, or brain\nwaves), identify a person (e.g., voice identification, retinal identification, facial identification, fingerprint identification, or electroencephalogram based identification), and the like.  The motion components 658 can include acceleration sensor\ncomponents (e.g., accelerometer), gravitation sensor components, rotation sensor components (e.g., gyroscope), and so forth.  The environmental components 660 can include, for example, illumination sensor components (e.g., photometer), temperature sensor\ncomponents (e.g., one or more thermometers that detect ambient temperature), humidity sensor components, pressure sensor components (e.g., barometer), acoustic sensor components (e.g., one or more microphones that detect background noise), proximity\nsensor components (e.g., infrared sensors that detect nearby objects), gas sensors (e.g., gas detection sensors to detect concentrations of hazardous gases for safety or to measure pollutants in the atmosphere), or other components that can provide\nindications, measurements, or signals corresponding to a surrounding physical environment.  The position components 662 can include location sensor components (e.g., a Global Position System (GPS) receiver component), altitude sensor components (e.g.,\naltimeters or barometers that detect air pressure from which altitude can be derived), orientation sensor components (e.g., magnetometers), and the like.\n Communication can be implemented using a wide variety of technologies.  The I/O components 650 can include communication components 664 operable to couple the machine 600 to a network 680 or devices 670 via a coupling 682 and a coupling 672,\nrespectively.  For example, the communication components 664 can include a network interface component or other suitable device to interface with the network 680.  In further examples, the communication components 664 can include wired communication\ncomponents, wireless communication components, cellular communication components, Near Field Communication (NFC) components, Bluetooth.RTM.  components (e.g., Bluetooth.RTM.  Low Energy), Wi-Fi.RTM.  components, and other communication components to\nprovide communication via other modalities.  The devices 670 can be another machine or any of a wide variety of peripheral devices (e.g., a peripheral device coupled via a USB).\n Moreover, the communication components 664 can detect identifiers or include components operable to detect identifiers.  For example, the communication components 664 can include Radio Frequency Identification (RFID) tag reader components, NFC\nsmart tag detection components, optical reader components (e.g., an optical sensor to detect one-dimensional bar codes such as Universal Product Code (UPC) bar code, multi-dimensional bar codes such as Quick Response (QR) code, Aztec code, Data Matrix,\nDataglyph, MaxiCode, PDF417, Ultra Code, UCC RSS-2D bar code, and other optical codes), or acoustic detection components (e.g., microphones to identify tagged audio signals).  In addition, a variety of information can be derived via the communication\ncomponents 664, such as location via Internet Protocol (IP) geolocation, location via Wi-Fi.RTM.  signal triangulation, location via detecting an NFC beacon signal that can indicate a particular location, and so forth.\n In various example implementations, one or more portions of the network 680 can be an ad hoc network, an intranet, an extranet, a virtual private network (VPN), a local area network (LAN), a wireless LAN (WLAN), a WAN, a wireless WAN (WWAN), a\nmetropolitan area network (MAN), the Internet, a portion of the Internet, a portion of the Public Switched Telephone Network (PSTN), a plain old telephone service (POTS) network, a cellular telephone network, a wireless network, a Wi-Fi.RTM.  network,\nanother type of network, or a combination of two or more such networks.  For example, the network 680 or a portion of the network 680 can include a wireless or cellular network and the coupling 682 can be a Code Division Multiple Access (CDMA)\nconnection, a Global System for Mobile communications (GSM) connection, or another type of cellular or wireless coupling.  In this example, the coupling 682 can implement any of a variety of types of data transfer technology, such as Single Carrier Radio\nTransmission Technology (1xRTT), Evolution-Data Optimized (EVDO) technology, General Packet Radio Service (GPRS) technology, Enhanced Data rates for GSM Evolution (EDGE) technology, third Generation Partnership Project (3GPP) including 3G, fourth\ngeneration wireless (4G) networks, Universal Mobile Telecommunications System (UMTS), High Speed Packet Access (HSPA), Worldwide Interoperability for Microwave Access (WiMAX), Long Term Evolution (LTE) standard, others defined by various standard-setting\norganizations, other long range protocols, or other data transfer technology.\n The instructions 616 can be transmitted or received over the network 680 using a transmission medium via a network interface device (e.g., a network interface component included in the communication components 664) and utilizing any one of a\nnumber of well-known transfer protocols (e.g., HTTP).  Similarly, the instructions 616 can be transmitted or received using a transmission medium via the coupling 672 (e.g., a peer-to-peer coupling) to the devices 670.  The term \"transmission medium\"\nshall be taken to include any intangible medium that is capable of storing, encoding, or carrying the instructions 616 for execution by the machine 600, and includes digital or analog communications signals or other intangible media to facilitate\ncommunication of such software.\n Throughout this specification, plural instances can implement components, operations, or structures described as a single instance.  Although individual operations of one or more methods are illustrated and described as separate operations, one\nor more of the individual operations can be performed concurrently, and nothing requires that the operations be performed in the order illustrated.  Structures and functionality presented as separate components in example configurations can be\nimplemented as a combined structure or component.  Similarly, structures and functionality presented as a single component can be implemented as separate components.  These and other variations, modifications, additions, and improvements fall within the\nscope of the subject matter herein.\n Although an overview of the inventive subject matter has been described with reference to specific example implementations, various modifications and changes can be made to these implementations without departing from the broader scope of\nimplementations of the present disclosure.  Such implementations of the inventive subject matter can be referred to herein, individually or collectively, by the term \"invention\" merely for convenience and without intending to voluntarily limit the scope\nof this application to any single disclosure or inventive concept if more than one is, in fact, disclosed.\n The implementations illustrated herein are described in sufficient detail to enable those skilled in the art to practice the teachings disclosed.  Other implementations can be used and derived therefrom, such that structural and logical\nsubstitutions and changes can be made without departing from the scope of this disclosure.  The Detailed Description, therefore, is not to be taken in a limiting sense, and the scope of various implementations is defined only by the appended claims,\nalong with the full range of equivalents to which such claims are entitled.\n As used herein, the term \"or\" can be construed in either an inclusive or exclusive sense.  Moreover, plural instances can be provided for resources, operations, or structures described herein as a single instance.  Additionally, boundaries\nbetween various resources, operations, modules, engines, and data stores are somewhat arbitrary, and particular operations are illustrated in a context of specific illustrative configurations.  Other allocations of functionality are envisioned and can\nfall within a scope of various implementations of the present disclosure.  In general, structures and functionality presented as separate resources in the example configurations can be implemented as a combined structure or resource.  Similarly,\nstructures and functionality presented as a single resource can be implemented as separate resources.  These and other variations, modifications, additions, and improvements fall within a scope of implementations of the present disclosure as represented\nby the appended claims.  The specification and drawings are, accordingly, to be regarded in an illustrative rather than a restrictive sense.", "application_number": "15639282", "abstract": " Systems and methods are disclosed for response retrieval using\n     communication session vectors. In one implementation, a first\n     communication session is received. The first communication session\n     includes a first communication. The first communication session is\n     encoded as a first vector. A second vector is identified within a defined\n     proximity of the first vector. The second vector represents a second\n     communication session that includes a second communication. The second\n     communication is provided within the first communication session in\n     response to the first communication.\n", "citations": ["6999914", "7536413", "8873813", "9430463", "9575963", "9715496", "9860200", "20100235908", "20110258544", "20140244744", "20160103996", "20160359771", "20170134313", "20180052908", "20180189628", "20180196796", "20180240014", "20180285774", "20180287968", "20180316630"], "related": []}, {"id": "20190014488", "patent_code": "10375585", "patent_name": "System and method for deep learning and wireless network optimization\n     using deep learning", "year": "2019", "inventor_and_country_data": " Inventors: \nTan; Yongxi (Hillsborough, NJ), Yang; Jin (Bridgewater, NJ), Song; Qitao (Shanghai, CN), Chen; Yunjun (Shenzhen, CN), Ye; Zhangxiang (Shanghai, CN)  ", "description": "CROSS-REFERENCE TO RELATED APPLICATIONS\n This application is cross-referenced to U.S.  application Ser.  No. 15/642,563, entitled \"Optimizing Cellular Networks Using Deep Learning\", filed concurrently with this application on Jul.  6, 2017, which application is hereby incorporated by\nreference herein as if reproduced in its entirety.\nTECHNICAL FIELD\n The present invention relates generally to a wireless communications, and, in particular embodiments, to a system and method for deep learning and wireless network optimization using deep learning.\nBACKGROUND\n Wireless network optimization techniques are generally used to improve wireless network performance, so as to provide wireless services to network users with improved quality of service and quality of user experience, and also satisfy other\nservice requirements.  As the wireless networks increasingly expand and become complex, wireless network optimization is facing challenges to meet higher requirements, such as lower optimization cost, shorter optimization time, higher optimization\naccuracy, etc., and conventional methods appear to be difficult to meet these requirements.\n Artificial intelligence provides techniques that use a machine to mimic human intelligence.  Artificial intelligence techniques aim to solve many problems using machines, such as reasoning, planning, learning, natural language processing,\nperception, moving and manipulating objects, etc. Artificial intelligence techniques have already been used in various applications, such as autonomous vehicles, medical diagnosis, playing games (such as Chess), search engines (such as Google search),\nonline assistants (such as Siri), and image recognition, among many others.  Artificial intelligence techniques have also been put into application in the field of telecommunications, e.g., for improving telecommunications services and products.\nSUMMARY\n Technical advantages are generally achieved, by embodiments of this disclosure which describe a method for deep learning and wireless network optimization using deep learning.\n According to one aspect of the present disclosure, there is provided a method that includes: initializing a neural network with a set of weight values, the neural network being used to determine actions that adjust one or more settings of cells\nassociated with base stations in a wireless network, each base station providing communication services to user equipments (UEs) within one or more cells; and training the neural network by using a deep reinforcement learning (DRL) process, the DRL\nprocessing comprising generating a first plurality of experience tuples for a plurality of cells in the wireless network, each experience tuple comprising a cell identifier that identifies a cell, a first state of the cell, a second state of the cell, an\naction that causes the cell to transit from the first state to the second state, and a reward value for taking the action, wherein a state of a cell comprises a setting of a base station providing a coverage area of the cell, and a reward value is\ncalculated using a cost function based on measurement reports received from UEs in the wireless network, wherein each experience tuple can be a DRL-generated experience tuple in which a respective action is selected by a DRL agent based on the neural\nnetwork according to a DRL technique or an expert-generated experience tuple in which the respective action is provided based on expert experience, and wherein whether an action is selected by the DRL agent based on the neural network or provided based\non the expert experience is determined based on a first criterion; and selecting a second plurality of experience tuples from the first plurality of experience tuples; and updating the set of weight values of the neural network according to reward values\nin the second plurality of experience tuples.\n According to another aspect of the present disclosure, there is provided a method that includes: training a neural network for a plurality of cells in a wireless network using a deep reinforcement learning (DRL) process for adjusting one or more\ncell parameters of cells associated with base stations in the wireless network, each base station providing communication services to user equipments (UEs) within a coverage area of one or more cells, and the neural network being trained to determine\nactions that can be performed on the base stations, wherein the DRL process comprises: generating a plurality of experience tuples for a plurality of cells in the wireless network, comprising: generating a state tensor for each of the plurality of cells,\neach state tensor indicating a state of a respective cell, wherein a state of a cell comprises a setting of a base station associated with the cell, the base station providing a coverage area of the cell; selecting an action for each of the plurality of\ncells, the action moving the respective cell from one state to another state, wherein an action comprises information for adjusting a setting of a base station associated with a cell; applying respective actions selected for the plurality of cells to the\nrespective cells to adjust one or more cell parameters; and generating an experience tuple for each of the plurality of cells based on the respective action applied, the experience tuple comprising a cell identifier identifying the respective cell, a\nfirst state of the respective cell that is indicated by a respective state tensor, a second state of the respective cell, the action applied to the respective cell that moves the respective cell from the first state to the second state, a local reward\ncalculated for applying the action to the respective cell, and a global reward calculated for applying the action to the respective cell, the local reward being calculated based on a local cost function and the global reward being calculated based on a\nglobal cost function; and determining whether or not an action of a first experience tuple in the plurality of experience tuples corresponding to a first cell is acceptable based on the local reward and the global reward of the first experience tuple;\nand updating weights of the neural network based on whether or not the action is acceptable.  The method also includes selecting an action for adjusting a cell parameter of a cell in the wireless network based on the trained neural network; and\ninstructing to adjust the cell parameter of the cell in the wireless network according to the selected action.\n According to yet another aspect of the present disclosure, there is provided an apparatus that includes a non-transitory memory storage comprising instructions and one or more processors in communication with the memory storage.  The one or more\nprocessors execute the instructions to: train a neural network for a plurality of cells in a wireless network using a deep reinforcement learning (DRL) process for adjusting one or more cell parameters of cells associated with base stations in the\nwireless network, each base station providing communication services to user equipments (UEs) within a coverage area of one or more cells, and the neural network being trained to determine actions that can be performed on the base stations, wherein the\nDRL process comprises: generating a plurality of experience tuples for a plurality of cells in the wireless network, comprising: generating a state tensor for each of the plurality of cells, each state tensor indicating a state of the respective cell,\nwherein a state of a cell comprises a setting of a base station associated with the cell, the base station providing a coverage area of the cell; selecting an action for each of the plurality of cells, the action moving the respective cell from one state\nto another state, wherein an action comprises information for adjusting a setting of a base station associated with the cell; applying respective actions selected for the plurality of cells to the respective cells to adjust one or more cell parameters;\nand generating an experience tuple for each of the plurality of cells based on the respective action applied, the experience tuple comprising a cell identifier identifying the respective cell, a first state of the respective cell that is indicated by a\nrespective state tensor, a second state of the respective cell, the action applied to the respective cell that moves the respective cell from the first state to the second state, a local reward calculated for applying the action to the respective cell,\nand a global reward calculated for applying the action to the respective cell, the local reward being calculated based on a local cost function and the global reward being calculated based on a global cost function; and determining whether or not an\naction of a first experience tuple in the plurality of experience tuples corresponding to a first cell is acceptable based on the local reward and the global reward of the first experience tuple; and updating weights of the neural network based on\nwhether or not the action is acceptable.  The one or more processors also execute the instructions to select an action for adjusting a cell parameter of a cell in the wireless network based on the trained neural network; and instruct to adjust the cell\nparameter of the cell in the wireless network according to the selected action. BRIEF DESCRIPTION OF THE DRAWINGS\n For a more complete understanding of the present invention, and the advantages thereof, reference is now made to the following descriptions taken in conjunction with the accompanying drawings, in which:\n FIG. 1 illustrates a diagram of an embodiment wireless communications network;\n FIG. 2 illustrates a diagram of an embodiment cellular network;\n FIG. 3 illustrates a schematic diagram of an embodiment reinforcement learning (RL) system;\n FIG. 4 illustrates a schematic diagram of an embodiment supervised learning (SL) system;\n FIG. 5 illustrates a flowchart of an embodiment method for wireless network optimization using deep reinforcement learning (DRL);\n FIG. 6 illustrates a flowchart of another embodiment DRL method;\n FIG. 7 illustrates a graph showing a gain percentage for validation scenario obtained using a deep Q-network and a supervised deep Q-network.\n FIG. 8 illustrates a graph showing a gain percentage for validation scenario obtained using a double Q-network and a supervised double Q-network.\n FIG. 9 illustrates a graph showing a gain percentage for validation scenario obtained using A3C and a supervised A3C\n FIG. 10 illustrates a flowchart of yet another embodiment DRL method;\n FIG. 11 illustrates a graph showing a ratio of positive gain obtained using DRL;\n FIG. 12 illustrates a flowchart of an embodiment method for adjusting cell parameters using a DRL technique;\n FIG. 13 illustrates a flowchart of another embodiment method for adjusting cell parameters using a DRL technique;\n FIG. 14 illustrates a flowchart of yet another embodiment DRL method;\n FIG. 15 illustrates a flowchart of yet another embodiment method for adjusting cell parameters;\n FIG. 16 illustrates graphs showing one-shot optimization and iterative optimization;\n FIG. 17 illustrates a diagram of an embodiment processing system; and\n FIG. 18 illustrates a diagram of an embodiment transceiver.\nDETAILED DESCRIPTION OF ILLUSTRATIVE EMBODIMENTS\n The making and using of embodiments of this disclosure are discussed in detail below.  It should be appreciated, however, that the concepts disclosed herein can be embodied in a wide variety of specific contexts, and that the specific\nembodiments discussed herein are merely illustrative and do not serve to limit the scope of the claims.  Further, it should be understood that various changes, substitutions and alterations can be made herein without departing from the spirit and scope\nof this disclosure as defined by the appended claims.\n Embodiments of the present disclosure provide a method for adjusting cell parameters of a wireless network using a deep learning technique.  In some embodiments, the method trains a neural network for selecting actions to adjust cell parameters\nof the wireless network using a deep reinforcement learning (DRL) technique, and adjusts one or more cell parameters of a plurality of cells in the wireless network according to actions that are selected using the trained neural network based on a\ncurrent state of the wireless network.  A cell parameter may be a setting of a base station providing a coverage area of a cell or a setting indicating a relationship between base stations (thus, cells).  Examples of a cell parameter may include\ntransmission power, an antenna tilt, an antenna azimuth, transmission power, or a distance between two base stations.\n During training of the neural network, a plurality of experience tuples may be generated for a plurality of cells in the wireless network.  An experience tuple for a cell includes a cell identifier identifying the cell, a first state of the\ncell, a second state of the cell, an action applied to the cell that moves the cell from the first state to the second state, a local reward calculated for applying the action to the cell, and a global reward calculated for applying the action to the\ncell.  The first state or the second state may be indicated by a state tensor.  A state tensor of a cell may include information about the cell that indicates a state of the cell, such as cell parameters, KPIs, and measurement reports.\n In one embodiment of generating an experience tuple of a cell, the method may select an action for the cell at a first state, apply the action to the cell, obtain a second state of the cell after the action is applied, and calculate the local\nreward and the global reward.  The action may be selected using the neural network according a DRL algorithm.  In some embodiments, the action may also be selected based on expert experience, instead of using the neural network.  The expert experience\nmay include expert experience data, historical data, and/or other data that has been obtained and may be used for help selecting an action in training a neural network.  The local reward may be calculated based on a local cost function, and the global\nreward may be calculated based on a global cost function.  The local reward and the global reward may be calculated using information from different cells in the wireless network.\n The method may then update weights of the neural network based on the generated plurality of experience tuples.  In some embodiments, the method may determine whether each action of the plurality of experience tuples is acceptable, and apply a\npositive or negative gradient to the neural network based on whether the action is acceptable.  In one embodiment, an action may be acceptable when a global reward associated with the action satisfies a first criterion and a local reward associated with\nthe action satisfies a second criterion.  In another embodiment, an action may not be acceptable when the global reward associated with the action does not satisfies the first criterion or when the local reward associated with the action does not\nsatisfies the second criterion.\n The embodiments of the present disclosure also provide a method for training a neural network using a DRL technique.  In some embodiments, the method initializes the neural network with a set of weight values that may be randomly selected or\nobtained from an expert neural network, and generates a plurality of experience tuples for updating the weights of the neural network.  Each experience tuple may include a first state of an environment, a second state of the environment, an action that\ncauses the environment to transit from the first state to the second state, and a reward value for taking the action.  In some embodiments, each experience tuple may be a DRL-generated experience tuple in which a respective action is selected by a DRL\nagent using the neural network according to a DRL technique, or an expert-generated experience tuple in which the respective action is provided based on expert experience.  This method may be referred to as a supervised DRL method.  The method may be\nused in training a neural network for adjusting cell parameters of a wireless network.\n FIG. 1 illustrates a network 100 for wirelessly communicating data.  The network 100 comprises a base station no having a coverage area (or a cell) 101, a plurality of mobile devices 120, and a backhaul network 130.  As shown, the base station\nno establishes uplink (dashed line) and/or downlink (dotted line) connections with the mobile devices 120, which serve to carry data from the mobile devices 120 to the base station no and vice-versa.  Data carried over the uplink/downlink connections may\ninclude data communicated between the mobile devices 120, as well as data communicated to/from a remote-end (not shown) by way of the backhaul network 130.  As used herein, the term \"base station\" refers to any component (or collection of components)\nconfigured to provide wireless access to a network, such as an enhanced base station (eNB), a macro-cell, a femtocell, a Wi-Fi access point (AP), or other wirelessly enabled devices.  Base stations may provide wireless access in accordance with one or\nmore wireless communication protocols, e.g., long term evolution (LTE), LTE advanced (LTE-A), High Speed Packet Access (HSPA), Wi-Fi 802.11a/b/g/n/ac, etc. As used herein, the term \"mobile device\" refers to any component (or collection of components)\ncapable of establishing a wireless connection with a base station, such as a user equipment (UE), a mobile station (STA), and other wirelessly enabled devices.  In some embodiments, the network 100 may comprise various other wireless devices, such as\nrelays, low power nodes, etc.\n A cellular network may include a number of base stations forming a number of interconnected cells.  Through an arrangement of the base stations, the cellular network can provide wireless communication coverage over a large geographical area, and\nenable wireless communication devices to communicate with other devices anywhere in the network.  FIG. 2 illustrates a cellular network 200 including a plurality of base stations (BSs) providing coverage of cells 202.  Each cell represents a geographical\narea.  The area covered by the cells 202 may not overlap each other, or there may be some or substantial overlap of cells 202.  FIG. 2 shows each cell has a hexagon shape and each base station is located in the center of a cell.  In another example, each\nbase station may be located in a corner of a hexagon and providing coverage of three hexagon-shaped cells (or three-sectored cells).  The cellular network 200 may have a certain layout or topology, which includes the relative distance between base\nstations and their antenna angular directions relative to each other.  The cellular network shown in FIG. 2 is by way of example only and may vary in further embodiments.\n Optimization techniques may be applied to the cellular network 200 to improve performance of the cellular network 200 with respect to one or more cellular system parameters or a performance goal.  For example, optimization of the cellular\nnetwork 200 may include optimization for mobility robustness, load balancing, coverage and capacity, power savings, interference reduction, or adjustment and optimization of other key performance indicators (KPIs).  The cellular system parameters may\ninclude antenna electronic tilt (eTilt), antenna azimuth, and antenna mechanical tilt (mTilt), and transmission power.\n In the cellular network 200, neighboring cells 202 may affect each other such that a change in settings of one base station associated with one cell may affect performance, such as coverage and capacity, of a neighboring cell.  For example,\nchanging a setting of one base station to improve its cell coverage and capacity may create interference with other neighboring cells and potentially decrease the coverage and capacity of those neighboring cells, as well as the coverage and capacity of\nthe overall network.  Increasing the number of cells in the cellular network may result in an exponential increase in the number of interactions, relationships and potential interference between neighboring cells.  Thus, performance of a cellular network\nmay be affected not only by each individual cell, but also relationship between the cells, and optimization of the cellular network needs to consider these affecting factors.\n Various optimization techniques have been used to optimize wireless network.  For example, a virtual simulation or model of a cellular network can be constructed to enable variation and optimization of network parameters in a virtual\nenvironment.  In another example, network parameters are optimized iteratively by making small step adjustments and gathering real-world feedback on the effects of those adjustments on a real network until an optimal adjustment action is found.  These\ntechniques, however, generally require knowledge of UE locations, accurate engineer parameters, and costly drive tests and site visiting, and are generally time-consuming.  Moreover, even when an optimal solution is found for one cellular network, the\nsolution is generally not applicable to a different cellular network.  Rather, optimization for a new cellular network would require starting an optimization process all over again from scratch.  In addition, these techniques are not able to satisfy a\nso-called one-shot optimization requirement, where one or more network parameters only need to be adjusted once, rather than multiple times, to optimize a network.\n Embodiments of the present disclosure provide methods for optimizing a plurality of cells in a wireless network using deep learning techniques.  The embodiments may be applied to wireless network optimizations with respect to various\noptimization objectives, including, but not limited to, automatic neighbor relation (ANR), automated configuration of physical cell identity (ACPCI), mobility robustness optimization (MRO), mobility load balancing (MLB), coverage and capacity\noptimization (CCO), energy savings, interference reduction, RACH optimization, inter-cell interference coordination (ICIC), calibration of inaccurate engineer parameters, cell labeling, identification of cells or area to be optimized or adjusted, cell\noutage detection (COD), cell outage compensation (COC), and optimization or adjustment of any other KPIs of a wireless network.  Optimizing the plurality of cells may include adjusting various cell or network parameters to meet an optimization goal. \nWhile embodiments of the present disclosure are described with respect to a cellular network, it is understood that the embodiments may also be applied to cellular networks and other types of wireless communications networks, such as a Wi-Fi network,\nbody area sensor network, Internet-of-Things, cognitive radio network, and communication network for smart grid.\n Deep learning is a technique that is developed in the field of artificial intelligence, where a learning agent interacts with an environment by receiving an observation that characterizes a current state of the environment, selecting an action\nto be performed using a deep learning network in response to receiving the observation, and performing an action on the environment.  The deep learning network is configured to receive an observation and to process the observation to generate or\ndetermine a next action to be performed based on the observation.  The deep learning network is trained in a deep learning process to improve its intelligence for effectively selecting an action.  The training of a deep learning network may be referred\nto as a deep learning method or process.  The deep learning network may be neural network, a Q-learning network, dueling network, or any other applicable network.\n FIG. 3 illustrates a schematic diagram of a reinforcement learning (RL) system 300.  The RL system 300 in this example may be a deep reinforcement learning (DRL) system, where a deep neural network is used.  The RL system 300 may also use other\nlearning networks, such as a deep Q-network.  The RL system 300 uses a RL agent 302 that interacts with an environment 304 to take a sequence of actions (A) in order to maximize a cumulative reward (R).  The RL agent 302 may be implemented by an\nalgorithm, or a control unit/module.  The environment 304 may be a wireless network including a plurality of cells, or a simulator simulating a wireless network.  An observation of the environment 304 may be a state of a wireless network, which may\ninclude states of one or more of a plurality of cells in the network.  As shown, the RL agent 302 takes as input a state (S.sub.t) of the environment and a reward (R.sub.t), and selects an action to perform on the environment.  In some embodiments, the\naction may refer to an adjustment of a cell parameter of a cell associated with a base station.  A cell parameter herein refers to a setting of a base station with respect to a particular cell for wireless communications.  Examples of a cell parameter\nmay include antenna transmission power, an antenna tilt, an antenna height, an antenna azimuth, a handover parameter (e.g., hysteresis, time-to-trigger), a vertical or horizontal direction of an element of antenna array of massive multi-input\nmulti-output (MIMO).  A cell parameter may also include settings indicating relationship between base stations, such as a distance between two base stations (e.g., inter-site-distance).  In a case of one base station providing three-sectored cells, the\nbase station may configure a set of parameters for each of the cells for wireless transmissions.  Those of ordinary skill in the art would recognize that the embodiment methods in this disclosure may be applied to adjust various cell parameters.\n Embodiments of this disclosure in the following are described with respect to cells.  One of ordinary skills in the art would understand that a cell is a physical area that is provided by a base station, and thus, actions taken to a cell refers\nto actions taken to a base station that provides a coverage area of the cell.  For example, adjusting a cell parameter of a cell refers to adjusting a setting of a base station that provides coverage of the cell, and selecting an action for adjusting a\ncell or adjusting a cell parameter of a cell refers to selecting an action for adjusting a setting of a base station that provides coverage of the cell.  In another example, applying an action to a cell refers to adjusting a base station setting\naccording to the action.  Description of the embodiments with respect to cells should not be construed to be limiting to the scope and spirit of the present disclosure.\n The selected action, when performed on the environment 304, causes the environment 304 to move or transit from the state (S.sub.t) to a new state (S.sub.t+1).  The reward (R.sub.t) is associated with the state (S.sub.t) and a previous action\nthat moves the environment 304 to the state (S.sub.t).  A reward may be calculated according to a predefined cost function that indicates an objective for adjusting or optimizing the environment 304.  A reward indicates whether an action contributes\npositively or negatively for moving the environment from one state to another state with respect to the cost function.  After the action is applied to the environment, a reward (R.sub.t+1) for taking the action to move the environment to the new state\n(S.sub.t+1) is then calculated.  The new state (S.sub.t+1) and the reward (R.sub.t+1) are then sent to the RL agent 302, based on which the RL agent 302 selects a new action to be applied to the environment.\n The RL system 300 may begin with a first state of the environment.  The RL agent 302 may select a sequence of actions, and apply the actions to the environment 304.  In some embodiments, the RL agent 302 may select some actions randomly or\naccording to a predefined rule or criterion.  In other embodiments, the RL agent 302 may determine an action to take using a learning network, such as a neural network.  By applying the sequence of actions, a plurality of experience tuples (S.sub.t,\nR.sub.t, A.sub.t, S.sub.t+1, R.sub.t+1) is generated.  Each experience tuple represents an experience, and includes a first state (S.sub.t) of the environment, a second state (S.sub.t+1) of the environment, an action (A.sub.t) that causes the environment\nto transit from the first state (S.sub.t) to the second state (S.sub.t+1), and a first reward value (R.sub.t) associated with a previous action that has transited the environment to the first state (S.sub.t), and a second reward value (R.sub.t+1)\nassociated with the action (A.sub.t).  The experience tuples indicate actions that move the environment from state to states, may indicate what sequence of actions that can moves the environment towards an expected state.  In case of a neural network is\nused for action selection, the experience tuples may be used to adjust weight values of neural network, whereby the neural network is trained for selecting actions for the environment 304.  When the RL system 300 is a DRL system, a deep neural network is\nused as a function approximator to select an action, i.e., f(S.sub.t, R.sub.t)-&gt;A.sub.t.\n The RL system 300 does not generate labeled input (S) and output (A) pairs, such as a labeled state (St) and action (A.sub.t) pair, compared with a supervised learning (SL) system.  FIG. 4 illustrates a schematic diagram of a SL system 400,\nwhere a SL algorithm 402 receives an input (X), such as training data, and generates an output (Y).  Correctly labeled input (X) and output (Y) pairs are then generated by a supervisor, and an estimated error is fed back to the SL algorithm 402, which is\nused to adjust the generation of the output.\n In some embodiments, deep learning techniques may be used to solve complicated decision making problems in wireless network optimization.  For example, deep learning networks may be trained to adjust one or more parameters of a wireless network,\nor a plurality of cells in the wireless network, so as to achieve optimization of the wireless network with respect to an optimization goal.\n FIG. 5 illustrates a flowchart of an embodiment method 500 for wireless network optimization using deep reinforcement learning (DRL).  At step 502, the method 500 trains a deep learning network using a DRL technique for adjusting one or more\nparameters of a plurality of cells in a wireless network.  In some embodiments, the method 500 may receive state information of the plurality of cells, select actions using the deep learning network for adjusting one or more of the plurality of cells,\ncalculate reward values for the actions selected and taken, and update weight values of the deep learning network according to the reward values.  The method 500 may train the deep learning network on a simulator.  The method 500 may also train the deep\nlearning network in real time during operations of the wireless network.  At step 504, once the deep learning network has been trained, the method 500 may select an action or actions using the deep learning network for adjusting the plurality of cells\nbased on a current state of the plurality of cells.  At step 506, the method 500 instructs to perform the selected action(s).\n Conventional DRL techniques typically require a large amount of experience tuples (e.g., more than 100 M experience tuples) to produce a satisfactorily trained neural network.  Generating such large amount of experience tuples is generally very\ntime-consuming for wireless communications systems, as a state of a wireless network is determined based on states of cells in the network, which in turn are based on various types of information, such as measurement reports (MRs) and key performance\nindicators (KPIs).  The information takes time to generate and collect.  For example, it has been shown that it may take one day for a wireless network to generate a MR and a KPI to be used for generating a single experience.  Moreover, a good reward\nfrom a random action or an action suggested by a DRL agent according a neural network may be very sparse, and this dramatically slow the training process due to frequent weak reward or stimulation to the neural network.  In addition, because the state or\noptimization space of a wireless communications system is huge, exhaustive exploration is generally impossible.  As such, directly using conventional DRL techniques in the embodiment method 500 to train a neural network may take much longer time and may\nnot produce a satisfactory training result.\n Various expert methods and algorithms in the field of wireless communications have already been developed and used for optimizing wireless networks with various objectives and for solving various performance problems.  Examples of such methods\nand algorithms include methods or algorithms for radio resource management (RRM) or self-organizing networks (SONs).  Thus, enormous expert data or historical data has already been generated regarding various aspects of a wireless network, and extensive\nexperience has been accumulated in operating and managing the wireless network.  The data and experience are continuing to be generated everywhere and every day.  In the following, \"expert experience\" will be used to refer to expert experience data,\nhistorical data, and/or other data that may be used for help selecting an action in training a neural network.  The expert experience may be utilized in training neural networks for wireless network optimization, e.g., by suggesting actions with good\nrewards based on expert experience.  Use of expert experience can help speed up training of the neural network.\n FIG. 6 illustrates a flowchart of an embodiment DRL method 600.  The method 600 may be a computer-implemented method executed with one or more processors.  The DRL method 600 may be used to train a neural network in a deep learning process.  The\nembodiment method 600 can be used to train a neural network using various deep learning algorithms, and can be used in solving various optimization problems of wireless networks.  The method 600 may be referred to as a supervised DRL method for expert\nexperience can be used to guide the selection of actions in training the neural network.  The neural network in this example may be referred as a supervised DRL neural network (SDNN).  The neural network may have architecture of a Visual Geometry Group\n(VGG) network, Residual network (Resnet), Alex Net, Inception Net, Multi-Layer Perceptron (MLP), Long Short Term Memory (LSTM), asynchronous advantage actor-critic (A3C), or any other applicable architecture.  The DRL method 600 may start with\ninitializing the neural network with a set of weight values.  At step 602, the DRL method 600 determines whether the SDNN is initialized with randomly configured or selected weight values, or with weight values of an expert neural network.  The expert\nneural network may be a neural network that has been trained or configured with weights based on a neural network learning technique and/or expert experience.  The expert neural network may be trained using deep learning techniques such as deep\nsupervised learning, deep unsupervised learning, or deep reinforcement learning.  The DRL method 600 may determine to proceed to step 604 to use random weight values to initialize the SDNN, or proceed to step 606, where weight values of the expert neural\nnetwork are used to initialize the SDNN.\n When the SDNN is initialized with the set of weight values, the method 600 may generate a plurality experience tuples for training the SDNN.  At step 608, the method 600 may determine whether expert is used to explore.  That is, the method 600\ndetermines whether experience tuples may be generated by a DRL agent using the SDNN according to a DRL algorithm, or may be generated according to expert experience provided by the expert.  In particular, the method 600 may determine whether one or more\nactions are selected or determined by the DRL agent using the SDNN (an experience tuple generated in this way is referred to as a DRL-generated tuple), or selected or determined by an expert according to expert experience (an experience tuple generated\nin this way is referred to as an expert-generated tuple).  The expert here may refer to an algorithm or another deep learning network, e.g., a learning network that is different than the SDNN trained in this example.  The expert may make use of expert\nexperience, such as historical data or pre-existing state-action information, to select an action.  It would be more likely to select a good action based on a current state of an environment using the expert experience that is known knowledge, such that\nthe selected action results in a positive reward.  Selecting actions using the expert experience helps expedite training of the SDNN.\n The method 600 may determine whether or not the expert experience is used to explore, i.e., to generate experience tuples, according to various criteria, such as probability-based criteria, similarity-based criteria, or threshold-based criteria. For example, the method 600 may probabilistically determine whether an experience tuple is to be generated (or an action is selected) by the DRL agent using the SDNN, or generated according to expert experience.  The method 600 may make such a\ndetermination using, e.g., an e-greedy technique, a simulated annealing technique, or any other applicable probabilistic techniques/approaches.  In another example, the method 600 may check whether similar scenario has been explored before by the expert,\nor a similar action has been tried before by the expert, and then determine whether to select an action by the DRL agent or by the expert.  For example, when a similar scenario has been explored before by the expert, the method 600 may have confidence to\nselect, according to the expert experience, an action that has a good reward.  In yet another example, the method 600 may pre-determine a threshold, such as a percentage or a number of DRL-generated (or expert-generated) experience tuples, and produce a\nDRL-generated (or expert-generated) experience tuple when the threshold is exceeded.  Those of ordinary skill in the art would recognize many variations, modifications and alternatives to define such a criterion.\n The method 600 may add a label to each experience tuple to indicate whether the respective experience tuple is a DRL-generated or expert-generated experience tuple.  For example, a DRL-generated experience tuple may be represented by (S.sub.t,\nR.sub.t, A.sub.t, S.sub.t+1, R.sub.t+1, DRL), where \"DRL\" is a label indicating that this experience tuple is a DRL-generated experience tuple.  An expert-generated experience tuple may be represented by (S.sub.t, R.sub.t, A.sub.t, S.sub.t+1, R.sub.t+1,\nExpert), where \"Expert\" is a label indicating that this experience tuple is an expert-generated experience tuple.  Based on the determination at step 608, the method 600 may go to step 610 generating DRL-generated experience tuples, or go to step 612\ngenerating expert-generated experience tuples.  In this way, a plurality of experience tuples is generated.  The plurality of experience tuples may include only DRL-generated experience tuples, only expert-generated experience tuples, or a combination of\nDRL-generated experience tuples and expert-generated experience tuples.  The generated experience tuples may be saved, e.g., in a database, for later use.\n At step 614, the method 600 determines whether it needs more exploration to generate more experience tuples.  The method 600 may make the determination based on a criterion.  For example, the method 600 may predefine a threshold, such as a\nnumber of experience tuples to be generated, or a time period elapsed for generating experience tuples, and make the determination based on whether the threshold is satisfied.  Those of ordinary skill in the art would recognize many variations,\nmodifications and alternatives to define such a criterion.  When more experience tuples are needed, the method 600 may go back to step 608 to generate more experience tuples.  When no more experience tuples are needed, the method 600 may then produce a\nmini batch of experience tuples, by which weights of the SDNN are updated.  The mini batch includes at least a subset of the generated experience tuples.  The method 600 may randomly select the subset from the generated experience tuples to produce the\nmini batch.  In some embodiments, the mini batch may only include DRL-generated experience tuples, only expert-generated experience tuples, or a combination of DRL-generated experience tuples and expert-generated experience tuples.\n At step 616, the method 600 determines whether DRL-generated or expert-generated experience tuples are selected for the mini batch.  The method 600 may make the determination based on a predefined criterion.  The criterion may be threshold\nbased, probability based, similarity based, based on relationships between the experience tuples, or based on importance sampling.  For example, a threshold, e.g., a number or percentage of DRL-generated or expert-generated experience tuples, may be\npredefined, and the method determines to select a DRL-generated or expert-generated experience tuple based on whether the threshold is satisfied.  In another example, the method boo may probabilistically select a DRL-generated or expert-generated\nexperience tuple.  For example, a value between 0 and 1 is randomly generated, e.g., using a random uniform generator.  If the value is greater than a threshold, an expert-generated experience tuple is retrieved; and if the value is not greater than the\nthreshold, a DRL-generated experience tuple is retrieved.  The determination may also be made based on relationships between the experience tuples.  For example, instead of selecting only one experience tuple independently at one time, a group of\nrelevant experience tuples may be selected together each time.  The determination may also be made based on other criteria.  In one example, an experience tuple may be selected based on its importance to the current training process.  The importance of\nan experience tuple may be defined according to various factors associated with the experience tuple.  For example the more a TD error associated with an experience tuple is, the more importance this experience tuple is).  In another example, experience\ntuples may be selected based on similarity to one or more currently selected experience tuples.  For example, a probability that an experience tuple is selected may be set to be proportional to its similarity to the currently selected experience tuples,\nand experience tuples are selected based on their probabilities and the currently selected experience tuples.  Those of ordinary skill in the art would recognize many variations, modifications and alternatives to define such a criterion.  In some\nembodiments, the method 600 may select the subset of experience tuples one by one from the generated experience tuples to form the mini batch.  For each selection, the method 600 may determine whether a DRL-generated or an expert-generated experience\ntuple will be selected.  When the method 600 determines to use or select a DRL-generated experience tuple, at step 618, the method 600 retrieves a DRL-generated experience tuple, e.g., from a storage or a database of DRL-generated experience tuples. \nWhen the method 600 determines to use or select an expert-generated experience tuple, at step 620, the method 600 retrieves an expert-generated experience tuple from the expert, e.g., from a storage or a database of expert-generated experience tuples.\n At step 622, the method 600 determines whether the mini batch needs more experience tuples based on a criterion.  The criterion may be threshold-based, probability-based, or based on relationship between the experience tuples.  For example, when\nthe number of experience tuples in the mini batch is less than a predefined threshold, the method 600 may need to select more experience tuples.  If the determination is yes, the method 600 goes back to step 616 to continue select more experience tuples\nfor the mini batch.  Otherwise, the method 600 goes to step 624.  As another example, when the value generated by a random number generator is greater than a predefined or a dynamic changing probability (e.g., based on cooling schedule defined in\nsimulated annealing), the method 600 may select more experience tuples.  In yet another example, when an experience tuple is selected, other experience tuples related to this experience tuple may also be selected.\n At step 624, the method 600 calculates a temporal difference (TD) error corresponding to each action of the experience tuples in the mini batch.  The TD error may be calculated using a method that is value-based, policy-based or model-based, and\nmay be calculated using any applicable algorithms existed or unforeseen, such as techniques of deep Q-network, Double Q, Dueling network, A3C, Deep Sarsa, N-step Q, etc. At step 626, the method 600 back-propagates gradients calculated according to the TD\nerrors to update weights of the SDNN.  The techniques for calculating TD error, gradients, updating weights of a neural network are well-known in the pertinent art and will not be described in detail herein.\n At step 628, the method 600 determines whether the SDNN needs more or further training.  The method 600 may make the determination based on whether a criterion is satisfied.  For example, the method may determine whether the criterion is\nsatisfied based on a predefined threshold.  The threshold may be an epoch value, such as a maximum epoch, a gain threshold, or a time threshold.  In an example, when the maximum epoch is exceeded, the method 600 may determine that the criterion is\nsatisfied and no more training is needed.  In this case, the method 600 goes to step 630 and stops the training of the SDNN.  The method 600 may output the trained SDNN.  If the criterion is not satisfied, the method 600 goes to step 608 and repeat the\nsteps 608-628 to perform further training.\n The embodiment method 600 may dramatically speed up the training of deep learning networks in any DRL algorithms by using expert experience to guide the exploration.  Simulation has shown that the number of experience tuples that are needed to\nbe generated for training a neural network has been reduced from 100 M to several millions, and training time can be shortened from 1.2 years to about 5 days.  The embodiment method also dramatically improves the intelligence of a DRL agent by\nsupervising the training of the deep learning network with an expert.  Actions suggested or selected by the expert generally result in strong and good rewards, which in turn speed up the learning process.  Expert experience used for selecting actions\nalso reduces noises or bias caused by weak and sparse rewards resulted from random exploration or exploration by the neural network itself without supervising.\n The embodiment method may be used to improve the performance and speed of any existing or new DRL algorithms, such as Deep Q-Network, Double Q, Dueling Network, A3C, Deep Sarsa, etc. FIGS. 7-9 illustrate performance of a DRL method supervised by\nan expert compared with the DRL method without being supervised by the expert.  FIG. 7 illustrates a graph showing a gain percentage for validation scenario obtained using a deep Q-network and a supervised deep Q-network.  FIG. 8 illustrates a graph\nshowing a gain percentage for validation scenario obtained using a double Q-network and a supervised double Q-network.  FIG. 9 illustrates a graph showing a gain percentage for validation scenario obtained using A3C and a supervised A3C.  It can be seen,\na higher percentage of gain is obtained by using a supervised DRL method.\n FIG. 10 illustrates a flowchart of another embodiment DRL method 1000 for training a deep learning network in a deep learning process.  The deep learning network may be a deep neural network.  In this example, the DRL method 1000 trains the deep\nlearning network for adjusting one or more parameters of a plurality of cells in a wireless network.  In some embodiments, expert experience may be used during the training process.  For example, expert experience may be used as illustrated in the method\n600 of FIG. 6, e.g., to initialize the neural network, and help suggest expert actions so as to supervise training of the deep learning network.\n In some embodiments, the method 1000 may determine a plurality of cells in the wireless network that are to be adjusted for optimization.  The method 1000 may select the plurality of cells according various criteria or rules.  In this example,\ncells in the wireless network are divided into groups, e.g., based on geographical locations or other rules, and cells are selected for optimization based on the groups.  As shown, at step 1002, the method 1000 may first select a subset of groups of\ncells to be optimized in the wireless network.  The method 1000 may also select the entirety of the groups of cells for optimization.  In some embodiments, the method 1000 may select the subset of groups sequentially so that one group is selected at a\ntime.  In other embodiments, the subset of groups may be selected in parallel at the same time.  Selecting of the subset may be performed according to a predefined criterion.  In some embodiments, selection of the subset of groups may be conducted by\nsorting the groups of cells in the wireless network based on a first criterion and selecting the subset from the sorted groups based on a second criterion.  In one embodiment, the groups of cells may be sorted based on a degree of severity of a problem\nassociated with cells in the groups.  The severity of the problem may be determined by constantly monitoring whether various cell measurements exceed a predetermined threshold.  Examples of such measurements include KPIs, key quality indicators, or\nobjective functions.  In another embodiment, the groups of cells may be sorted according to preference levels or weight values associated with the groups.  Then the subset is selected from the sorted groups of cells based on the second criterion.  For\nexample, a threshold number of the groups of cells may be selected from the sorted groups.  In another example, groups whose values based on the above-mentioned first criteria exceed a predetermined threshold are selected to be optimized.  Those of\nordinary skill in the art would recognize many variations, modifications and alternatives for selecting a subset of groups of cells in a wireless network for optimization.\n At step 1004, the method 1000 selects top K cells from each of the subset of groups that have been selected at step 1002.  In this example, the method 1000 performs optimization on the top K cells in each of the subset of groups by adjusting\nparameters of the top K cells.  Step 1004 may be optional, and optimization may be performed on all cells in each selected subset of groups.  Selection of the top K cells from each subset of groups may be performed based on a predefined criterion.  For\nexample, the top K cells of a group may be selected based on a similar criterion to those discussed above for selecting the subset of groups.  Those of ordinary skill in the art would recognize that various methods and/or criteria may be used to select\nthe top K cells.\n When the subset of groups and cells in each subset of groups are determined for optimization, the method 1000 starts training of a neural network for adjusting the cells selected in the subset.  As discussed above, the method 1000 may initialize\nthe neural network with a set of weights, select a sequence of actions to apply to the selected cells in the subset of groups, generate a plurality of experience tuples, and update the neural network.  In some embodiments, the method 1000 may initialize\nthe neural network using steps 602-606 illustrated in method 600 of FIG. 6.\n At step 1006, the method 1000 generates a state tensor for each of the top K cells in each of the subset of groups.  A state tensor of a cell indicates a state of the cell.  A state of a cell may refer to a parameter of the cell, a performance\nstate of the cell, a relationship with other cells, or any combination thereof.  The state tensors are generated to be used as input for deep learning.  A state tensor of a cell may include any information about the cell and relationships of the cell\nwith other cells.  For example, the state tensor may include information, such as cell parameters, KPIs, and measurement reports.  The information may be obtained from one or multiple channels.  Each channel may provide information regarding one or more\nfeatures of the cell.  Such a state tensor may be referred to as a multi-channel state tensor.  Examples of the features include, but is not limited to, cellular network topology (e.g. inter-site distance (ISD), the angular location of cell sites\nrelative to each other, and the height of cell sites), engineer parameters (e.g. azimuth, mTilt, and eTilt), KPIs (e.g. throughput and cell load), and measurement reports from UEs.  A measurement report may include information such as reference signal\nreceived power (RSRP), reference signal received quality (RSRP), signal interference to noise ratio (SINR), channel quality indicator (CQI)), objective functions, cumulative distribution functions of network performance measures, and interference factor\nmatrices.\n A state tensor of a cell may be constructed using the information described above in various structures.  In some embodiments, a state tensor may be constructed as a collection of one or more state planes.  Each of the state planes may indicate\nor include information about the cell that is obtained from one or more channels.  In some embodiments, a state plane may include a two-dimensional array having a horizontal axis and a vertical axis for storing information of the cell.  When constructing\na state plane, in one embodiment, the cell may be first placed at the center of the horizontal axis and vertical axis.  Neighboring cells of the cell are then ranked according to relationships of the neighboring cells with the cell with respect to the\ntype of information represented by the state plane.  The ranking may be based on multiple features and/or types of information obtained from multiple channels.  In some embodiments, neighboring cells having a stronger relationship with the cell may be\nplaced closer to the cell in the center of the horizontal axis and the vertical axis.  By structuring the information in this way, specifically, to cluster neighboring cells having a stronger relationship closer to the cell, the method facilitates the\nneural network's processing of input data, i.e., the state tensors, to select actions.  In some embodiments, a state tensor may be constructed as a 1D, 2D, 3D or higher dimensional state tensor.  For example, a state tensor may be constructed as a single\nchannel matrix.  In another example, a group of 3D state tensors may be constructed into a 4D state tensor.  Those or ordinary of skill in the art would recognize many variations, modifications and alternatives for constructing a state tensor as used in\nthe embodiments.  When the state tensors for the selected cells in the subset of groups are constructed, these state tensors, indicating states of the selected cells, are taken as input into the neural network for selecting next actions for the selected\ncells.\n At step 1008, the method 1000 selects or generates an action by a DRL agent for each of the selected top K cells in each group of the subset of groups.  The action is to be performed on the respective cell and changes the state of the respective\ncell.  The method 1000 may randomly select the action, select the action using the neural network based on a DRL algorithm, or select the action based on a policy probability.  For example, when the probability of an action is higher than that of any\nother actions, the method 1000 may select the action that has the highest probability among the actions.  The method 1000 may also use an expert to select the action.  For example, the method 1000 may determine whether an expert is used to select the\naction as illustrated in method 600 of FIG. 6, and selects an action to be taken according expert experience.\n At step 1010, the method 1000 applies actions generated or selected to the respective cells.  As discussed above, an action may be adjusting a cell parameter of the respective cell, or changing a relationship with a neighboring cell.  The\nactions may be applied to a simulator simulating the wireless network.  The actions may also be performed on the real cells.  At step 1012, the method 1000 calculates a global reward and a local reward for each of the selected cells with an action\nselected and performed.  The global reward and the local reward of a cell are associated with the action applied to the cell.  The global reward and the local reward of a cell may vary with the state of the cell, the action applied and/or states of other\ncells.  The global reward and the local reward of a cell may be calculated using a global cost function and a local cost function of the cell, respectively.  The global cost function and the local cost function may be configured based on one or more\noptimization objectives to be achieved, such a targeted KPI, an interference level, an energy level.  The global reward and the local reward may be calculated based on information of the cell and other cells in the wireless network, such as information\nfrom measurement reports regarding the cell and its neighboring cells.\n The global reward may generally represent contribution of the cell, in terms of the global cost function, to the performance of the wireless network at a relatively global level, such as in the whole wireless network, or in the subset of groups\nselected, in any number of groups of cells in the wireless network, or in the group that the cell belongs to.  Similarly, the local reward may generally represent contribution of the cell, in terms of the local cost function, to the performance of the\nwireless network in a relatively local or cell level, such as in the group that the cell belongs to, in an area including neighboring cells of the cell, or for the cell itself.  The global reward and the local reward may be calculated using information\nfrom different cells in the wireless network.  The information may include measurement reports received by the different cells, KPIs, and other type of information that is applicable for calculating a reward.  In some embodiments, a global reward of a\ncell associated with an action may be calculated using information of all cells in the wireless network, using information of different groups of cells in the wireless network, or using information of any number of cells in the wireless network.  In some\nembodiments, a local reward of a cell associated with an action may be calculated using information of all or some cells in the group that the cell belongs to, using information of neighboring cells of the cell, or only using information of the cell\nitself.  The global rewards and the local rewards for different cells may be calculated using different mechanisms, using information from different number of cells, and using different cost functions and/or cost objectives.\n There may be many different ways to define a cost function in terms of a performance or optimization goal.  For example, a cost function may be defined for coverage and interference as f.sub.cost=((a*N1)+(1-a)*N2)/N. a is a coefficient or a\nweight value, N1 is the number of MRs from UEs whose RSRP of a serving cell is greater than or equal to a RSRP threshold, N2 is the number of MRs received from UEs whose RSRQ of a serving cell is greater than or equal to a RSRQ threshold, and N is the\ntotal number of MRs received.  The portion of (a*N1) is used to reflect a weight of coverage to the cost function, and the portion (1-a)*N2 reflects a weight of interference to the cost function.  An example of the parameters in this cost function may be\na=0.5, RSRP threshold=-105, and RSRQ threshold=-10.  The interference may also indicated by other information from the MRs, such as RSSINR (reference signal signal-to-interference-plus-noise ratio).  In this case, the N2 may be the number of MRs received\nthat include a RSSINR greater than or equal to a RSSINR threshold.  For example, the RSSINR threshold may be equal to -3.  In different wireless network, the cost function may be defined using different parameters.  For example, Ec/Io may be used to\nmeasure interference in UMTS, and RSCP used to measure coverage in UMTS.  The cost function may also be designed to take into consideration of other factors, such as throughput, CQIs, etc. The cost function may be designed in terms of MRs, KPIs, and/or\ncombination thereof.\n In some embodiments, both the global cost function and the local cost function may use the same formula f.sub.cost to calculate the global and local reward, such as f.sub.cost=((a*N1)+(1-a)*N2)/N. However, the global reward and local reward are\ncalculated based on MRs obtained from different cells.  The local reward may use MRs received from cells covering a small area and the global reward may use MRs received from cells covering a large area.  For example, a local reward of a cell is\ncalculated based on MRs received from UEs served by the cell or served by multiple cells in the same group of the cell within a period of time, while the global reward of the cell is calculated based on MRs received from UEs served by multiple groups of\ncells or all cells in the wireless network.\n By performing steps 1008-1012, the method 1000 can generate an experience tuple for each of the selected cells in the selected subset of groups of cells.  Each experience tuple may be represented by Experience (C.sub.k, S.sub.t, S.sub.t+1,\nA.sub.t, R.sub.g,t+1, R.sub.c,t+1).  C.sub.k identifies the cell for which the experience tuple is generated, (A.sub.t) represents an action selected and applied to the cell (C.sub.k), which transits the cell (C.sub.k) from a state (S.sub.t) to another\nstate (S.sub.t+1).  (R.sub.g,t+1) and (R.sub.c,t+1) represent the global reward and the local reward, respectively, of the cell (C.sub.k) corresponding to the action (A.sub.t).  In some embodiments, each experience tuple may be labeled to indicate\nwhether an experience tuple is generated based on an action selected by using a neural network or by an expert, as discussed above with respect to method 600 in FIG. 6.  At step 1014, the method 1000 adds the generated experience tuples to an experience\npool.\n At step 1016, the method 1000 determines whether it needs to generate more experience tuples.  The method 1000 may make the determination based on a criterion similarly to step 614 in FIG. 600.  The method 1000 may go back to step 1002 when it\nneeds to perform more exploration.  In this case, the method 1000 may reselect a subset of groups of cells and/or the cells in each selected group for optimization.  Alternatively, the method 1000 may skip steps 1002 and 1004, and use the same cells that\nhave been selected for more exploration.  When determining that no more exploration is needed, the method 1000 may proceed to step 1018, where a mini batch of experience tuples is generated using the experience pool.  The method moo may select experience\ntuples from the experience pool based on a predefined criterion to produce the mini batch.  For example, a threshold number or percentage of experience tuples may be selected from the pool.  In another example, the method may select the experience tuples\nfrom the experience pool using a probabilistic approach or based on relationships between the experience tuples.  In some embodiments, the mini batch may include one or more expert-generated experience tuples.  In this case, the method 1000 may select\nexperience tuples for the mini batch using steps 616-620 in method 600 of FIG. 6.\n When the mini batch is produced, the method 1000 may determine whether an action in each of the experience tuples of the mini batch is acceptable or rejectable based on the global reward and local reward of the respective experience tuple. \nBased on the determination, gradients, may be positive or negative, are calculated and applied for the actions and weights of the neural network are updated accordingly.\n Steps 1020-1028 will be performed for each action in experience tuples of the mini batch.  At step 1020, the method 1000 determines whether an action is acceptable at a global level based on the global reward associated with the action.  When\nthe action is not acceptable at the global level, the method 1000 proceeds to step 1026, where the action is rejected for this cell's experience.  When the action is acceptable at the global level, the method 1000 proceed to step 1022, where the method\n1000 continues to determine whether the action is acceptable at a cell level based on the local reward associated with the action.  When the action is not acceptable at the cell level, the method 1000 proceeds to step 1026, where the action is rejected\nfor this cell's experience.  When the action is acceptable at the cell level, the method 1000 proceeds to step 1024, where the action is accepted for this cell's experience.  At step 1028, based on the determination whether the action is accepted or\nrejected, the method 1000 calculates and applies a gradient for the action and updates the neural network's weights.  For example, for a cell experience with an action rejected, the method 1000 may apply a negative gradient for this action to discourage\nit; and for a cell experience with an action accepted, the method 1000 may apply a positive gradient for this action to encourage it.  Those of ordinary skill in the art would recognize that the gradients may be calculated and neural networks' weights\nmay be updated using various methods.\n The method 1000 may determine whether an action is acceptable at a global level based on the global reward associated with the action according to various criteria and using various mechanisms.  For example, when the global reward associated\nwith the action exceeds a threshold, the action is determined to be acceptable at the global level; otherwise, when the global reward associated with the action does not exceed the threshold, the action is rejected.  The threshold may be a reward value\npredefined or selected from historical data of the cell.  For example, the threshold may be the best global reward value that has been obtained for the cell corresponding to a plurality of actions performed on the cell in the past, or in a period of\ntime.  Thus, based on this threshold, an action for a cell associated with a global reward that is better than the best ever global reward value that has been achieved for the cell is accepted, and a positive gradient is back-propagated for updating the\nneural network; and any action for the cell having a global reward that is worse than the best ever global reward value is rejected, and a negative gradient is back-propagated.  This mechanism may be understood as multiple DRL agents competing for\nselecting actions for the cells of interest, in a purpose of gaining a better global reward based on the same initial configurations (or states) of the cells.  The initial configurations of the cells may include some initial parameters of the cells\nbefore a cell optimization process or a deep learning process begins for these cells.  Each DRL agent selects an action for applying to the cell, and a DRL agent wins if it selects an action that results in a global reward that is better than the\nhistorical best reward.  Alternatively, a DRL agent with an action resulting in a best global reward among the DRL agents wins.  Competition among the DRL agents helps improve the intelligence of the DRL agents iteratively.\n In some embodiments, a simulated-annealing based mechanism may be used.  In this example, when a global reward associated with an action performed on a cell exceeds a global reference reward by or more than a first global threshold (GT1), i.e.,\n(global reward-global reference reward)&gt;=GT1, the action is accepted at the global level; and when the global reward associated with the action does not exceed the global reference reward by a second global threshold (GT2), i.e., (global reward-global\nreference reward)&lt;GT2, the action is rejected.  In other words, a global upper bound, i.e., (global reference reward+GT1), and a global lower bound, i.e., (global reference reward+GT2) are defined.  An action with a global reward equal to or greater\nthan the global upper bound is accepted, and an action with a global reward less than the global lower bound is rejected.  In other words, two thresholds may be defined, e.g., threshold 1 and threshold 2.  The action is accepted at the global level if\nthe global reward is equal to or greater than the threshold 1, and the action is not accepted at the global level if the global reward is less than the threshold 2.  An action with a global reward between the threshold 1 and threshold 2 may be accepted\nor not, and this may be determined based on another criterion.\n The global reference reward may be a predefined reward value, or a reward value selected based on a predefined rule.  For example, the global reference reward may be the best global reward value that has been obtained for the cell corresponding\nto a plurality of actions performed on the cell in the past.  When the global reward associated with the action does not fall within these two categories, a metropolis criterion may be used to determine whether the action is accepted or rejected at the\nglobal level.  For example, a random uniform probability value between 0 and 1 may be generated, and the action is determined to be accepted or not by comparing the value with a threshold.  For example, when the value is greater than the threshold, the\naction is accepted; and when the value is not greater than the threshold, the action is rejected.\n The method 1000 may also determine whether an action of a cell is acceptable at a cell level based on the local reward associated with the action according to various criteria and using various mechanisms.  For example, when the local reward\nassociated with an action performed on a cell exceeds a threshold, the action is determined to be acceptable at the cell level; otherwise, when the local reward associated with the action does not exceed the threshold, the action is rejected at the cell\nlevel.  The threshold may be a reward value predefined or selected from historical data with respect to the cell.  For example, the threshold may be an initial local reward value of the cell corresponding to an initial state of the cell.  The initial\nlocal reward value of the cell may be calculated based on an initial state of the cell with or without a first action being taken for the cell.  In this case, actions causing degradation of the local rewards compared with the initial reward will be\nrejected and applied with negative gradients, and actions that can achieve better cell level rewards compared with the initial reward will be accepted and applied with positive gradients.  This example may be understood as multiple DRL agents are\ncooperating to select actions for the cell so as to mitigate risks of performance degradation at the cell level.\n In some embodiments, a simulated-annealing based mechanism may be used to determine whether an action is accepted or not at the cell level.  For example, when a local reward associated with an action performed on a cell exceeds a local reference\nreward by or more than a first local threshold (LT1), i.e., (local reward-local reference reward)&gt;=LT1, the action is accepted at the cell level; and when the local reward associated with the action does not exceed the local reference reward by a\nsecond local threshold (LT2), i.e., (local reward-local reference reward)&lt;LT2, the action is rejected.  In other words, a local upper bound, i.e., (local reference reward+LT1), and a local lower bound, i.e., (local reference reward+LT2) are defined. \nAn action with a local reward greater than the local upper bound is accepted, and an action with a local reward less than the local lower bound is rejected.  The local reference reward may be a predefined reward value, or a reward value selected based on\na predefined rule.  For example, the local reference reward may be an initial local reward value of the cell corresponding to an initial action performed on the cell.  When the local reward associated with the action does not fall within these two\ncategories, a metropolis criterion may be used to determine whether the action is accepted or rejected at the cell level.  For example, a random uniform probability value between 0 and 1 may be generated, and the action is determined to be accepted or\nnot by comparing the value with a threshold.  For example, when the value is greater than the threshold, the action is accepted; and when the value is not greater than the threshold, the action is rejected.\n At step 1030, the method 1000 determines whether the neural network needs more or further training.  The determination may be made similarly to step 628 in method 600 of FIG. 6.  When the method 1000 determines that more training is needed, it\ngoes back to step 1002 to continue the training as discussed above.  When no more training is needed, the method 1000 stops the training, and outputs the training result.\n The neural network trained according to method 1000 may then be used to adjust a cell parameter of a cell in the wireless network based on the current state of the cell.  In some embodiments, method 1000 may select an action for adjusting a cell\nparameter of a cell based on the trained neural network and a state of the cell, and instruct to adjust the cell parameter of the cell according to the selected action.\n The embodiment method may be implemented using various DRL techniques with various neural networks such as Deep Q-Network, Double Q, Dueling Network, A3C, Deep Sarsa, convolutional neural network, recurrent neural networks, Deep Boltzmann\nmachines, deep belief networks, etc.\n The trained neural network can help reduce the number of iteration needed to adjust cell parameters of the wireless network according to an optimization objective, and thus reduce the cost associated with site visiting.  The embodiment method\nmay also help achieve one-shot optimization of the wireless network.  In some embodiments, by using the embodiment method, cell or network parameters, such as eTilt, mTilt, azimuth, or transmission power, may only need to be adjusted once, rather than\niteratively, to achieve an optimization objective.  This can greatly reduce the time and cost taken for site visiting.  The embodiment does not need information of UE locations, and thus avoid time-consuming and costly drive tests for obtaining the\nlocation information.  The embodiment also has an advantage of tolerating inaccurate engineer parameters, which avoids time-consuming and costly onsite verification of the engineer parameters.  The embodiment method greatly improves the intelligence and\nperformance of a DRL agent and may be used in solving many wireless network optimization problems.\n FIG. 11 illustrates a graph showing a ratio of positive gain obtained using the embodiment method 1000 and other state-of-art DRL algorithms.  Curve 1102, 1104, 1106, 1108, and 1110 indicates, respectively, the ratio of positive gain obtained by\nusing methods of double Q, deep Q-network, A3C, Dueling network and the embodiment method moo.  As shown, the embodiment method has a higher gain than all the other algorithms.\n FIG. 12 illustrates a flowchart of an embodiment method 1200 for adjusting cell parameters of a plurality of cells in a wireless network using a DRL technique.  Specifically, the method 1200 utilizes the DRL method 600 to train a DRL neural\nnetwork (DRL-NN) for selecting actions to adjust cell parameters of two cells (cell 1 and cell 2) associated with two base stations.  The two cells are selected from 32 cells in the wireless network.  At step 1202, the method 1200 determines whether the\nDRL-NN is initialized with randomly selected weight values, or with weight values of an expert supervised deep neural network (SDNN).  The expert SDNN is a deep neural network that has been trained with expert supervising for wireless network\noptimization, or for adjusting similar cell parameters.  The DRL method 1200 may determine to proceed to step 1204 to use random weight values to initialize the DRL-NN, or proceed to step 1206, where weight values of the expert SDNN are used to\ninitialize the DRL-NN.\n The method 1200 may then generate a plurality experience tuples for training the DRL-NN.  Each experience tuple is represented by (C.sub.k, S.sub.t, R.sub.t, A.sub.t, S.sub.t+1, R.sub.t+1, Label).  C.sub.k (k=1, 2) identifies the cell for which\nthe experience tuple is generated, and the \"Label\" indicates whether the experience is a DRL-generated or expert-generated tuple.  S.sub.t, R.sub.t, S.sub.t+1, A.sub.t, R.sub.t+1 represents, respectively, a first state, a first reward associated with a\nprevious action, a second state, an action that moves the cell from the first state to the second state, and a second reward associated with the action.\n The two cells each have an initial state when the training begins.  The state of a cell may be represented by a state tensor as discussed with respect to FIG. 10.  For example, the state of cell 1 or cell may be indicated by a\n32.times.32.times.10 image-like 3D tensor, including information of the cell of interest (i.e., cell 1 or cell 2) and relationship information with 31 neighbor cells of the cell of interest.  The 3D tensor includes 10 feature planes including information\nsuch as tilt, azimuth, cumulative distribution function (CDF) of RSRP or RSRQ, an interference matrix, ISD, etc. For the convenience of illustration, in this example, each cell state is represented by a state vector (tilt, azimuth, CDF of RSRP).  The\nRSRP is obtained from MRs collected from the 32 cells within a time window, such as within an hour, 12 hours, or a day.\n In this example, an action may be selected to adjust a tilt, an azimuth, a transmission power, or any combination thereof.  The action may be represented by an action vector (tilt, azimuth, power).  Each vector element represents a value for\nadjusting a cell parameter.  An action may indicate a relative change or an absolute value for adjustment.  In this example, an element value in the action vector indicates a target value that the corresponding parameter will be adjusted to.  For\nexample, a selected action (5, 15, N/A) indicates adjusting the tilt to 5.degree., azimuth to 15.degree., and not adjusting the transmission power.  For each of cell 1 and cell 2, at step 1208, the method 1200 determines whether expert is used to select\nan action to adjust cell parameters of corresponding cells.  Based on the determination at step 1208, the method 1200 may go to step 1210 generating DRL-generated experience tuples, or go to step 1212 generating expert-generated experience tuples.  The\nmethod 1200 may select an action (tilt, azimuth, power), apply the selected action to the corresponding cell, update the cell's state, and calculate the reward, thereby generating an experience tuple.  For example, the method 1200 may select an action\n(-5, +15, N/A) for cell 1 using the DRL-NN, and select an action (+5.2, N/A, N/A) for cell 2 according to an expert.  The method 1200 adjusts parameters of cell 1 and cell 2 according to the selected actions, respectively, updates their states, i.e.,\ngenerates updated state vectors, and calculates their corresponding rewards.  In one example, the reward may be calculated using the cost function as discussed with respect to FIG. 10.  In this example, the reward for each cell is calculated by\nf.sub.cost=(0.5*N1+0.5*N2)/N, where N1 is the number of MRs received from UEs in the 32 cells whose RSRP of a serving cell is greater than or equal to a RSRP threshold, N2 is the number of MRs received from the UEs in the 32 cells whose RSRQ of a serving\ncell is greater than or equal to a RSRQ threshold, and N is the total number of MRs received from the UEs in the 32 cells.  The reward indicates whether an action for a cell is on the right track to adjust the setting of a base station providing a\ncoverage area of the cell for improving the performance of the wireless network.\n At step 1214, the method 1200 determines whether it needs to generate more experience tuples.  The method 1200 may select a sequence of actions for cell 1 and cell 2, and generate a plurality of experience tuples for each of the two cells.  The\nmethod 1200 may go back to step 1208 to generate more experience tuples.  When an experience is generated, it will be saved in the experience pool for future retrieving.  When no more experience tuples are needed, the method 1200 may retrieve a mini\nbatch of experience tuples from the experience pool.  At step 1216, the method 1200 determines whether DRL-generated or expert-generated experience tuples are selected for the mini batch.  When determining to use a DRL-generated experience tuple, at step\n1218, the method 1200 retrieves a DRL-generated experience tuple (C.sub.k, S.sub.t, R.sub.t, A.sub.t, S.sub.t+1, R.sub.t+1, DRL) from the experience pool.  When determining to use an expert-generated experience tuple, at step 1220, the method 1200\nretrieves an expert-generated experience tuple (C.sub.k, S.sub.t, R.sub.t, A.sub.t, S.sub.t+1, R.sub.t+1, Expert) from experience pool.  When the method 1200 determines, at step 1222, that the mini batch needs more experience tuples, it goes back to step\n1216 to continue select more experience tuples for the mini batch from experience pool; Otherwise, it goes to step 1224.  At step 1224, the method 1200 calculates a TD error corresponding to each action of the experience tuples in the mini batch using a\nloss function.  For example, the TD error may be calculated for minimizing MSE loss by stochastic gradient descent.  At step 1226, the method 1200 back-propagates gradients calculated according to the TD errors to update weights of the DRL-NN.  At step\n1228, the method 1200 determines whether the DRL-NN needs more training.  The method 1200 may go to step 1230 and stop the training, or go to step 1208 to perform further training.\n FIG. 13 illustrates a flowchart of another embodiment method 1300 for adjusting cell parameters of a plurality of cells in a wireless network using a DRL technique.  The method 1300 trains a DRL-NN for adjusting one or more parameters of a\nplurality of cells, and uses the trained DRL-NN for wireless network optimization.  In this embodiment, 50 cells in the wireless network are divided into 5 groups, with each group having 10 cells.  Training of the DRL-NN starts from step 1302, where the\nmethod 1300 selects 5 groups of cells to be optimized.  At step 1304, the method 1300 selects two cells in each of the 5 groups for network optimization.  As discussed with respect to FIG. 10, the groups and cells may be selected according to various\ncriteria.\n The two cells in each group have an initial state when the training begins.  The state of a cell may be represented by a state tensor as discussed with respect to FIG. 10.  For the convenience of illustration, in this example, each cell state is\nrepresented by a state vector (tilt, azimuth, CDF of RSRP), as described above with respect to FIG. 12.  The RSRP is obtained from MRs collected from a predetermined number of cells within a time window, such as within an hour, 12 hours, or a day.  At\nstep 1306, the method 1300 generates a cell state vector for each of the two cells in each of the five groups.\n At step 1308, the method 1300 selects an action for each of the selected cells.  In this example, an action vector (tilt, azimuth, power) is used to represent an action that may adjust a tilt, an azimuth, a transmission power, or any combination\nthereof.  In this example, an element value in the action vector indicates a relative value that the corresponding parameter will be adjusted.  For example, a selected action (+2, -10, N/A) indicates adjusting the tilt upward by 2.degree., adjusting the\nazimuth left-ward by 10.degree., and not adjusting the transmission power.  For example, the method 1300 may select an action (+5, -20, N/A) for a selected first cell in a selected group, and select an action (-3, N/A, N/A) for a selected second cell in\nthe selected group.\n At step 1310, the method 1300 applies the selected actions to the selected cells in the five groups, i.e., adjusts the corresponding cell parameters according to the action vectors.  The method 1300 may then updates states of the selected cells. The state of each cell will indicate that the corresponding cell parameters have been adjusted according to the selected action.  At step 1312, the method 1300 calculates a global reward and a local reward for each of the selected cells with an action\nselected and performed.  In one example, both the global reward and the local reward of a cell may be calculated using the same cost function.  In this example, a cost function f.sub.cost=(0.5*N1+0.5*N2)/N is used, where N1 is the number of MRs received\nfrom UEs whose serving RSRP is greater than or equal to a RSRP threshold, N2 is the number of MRs received from UEs whose serving RSRQ is greater than or equal to a RSRQ threshold, and N is the total number of MRs received from the UEs.  A local reward\nfor an action applied to a cell is calculated using MRs received from UEs in the cell, and a global reward for the action applied to the cell is calculated using MRs received from UEs in all the cells in the wireless network.  The local reward indicates\nwhether the action selected is on the right track to adjust the setting of a base station providing a coverage of the cell for improving the performance of the cell itself in terms of the cost function, and the global reward indicates whether the action\nselected is on the right track to adjust the setting of the base station for improving the performance of all the cells in the wireless network in terms of the cost function.  The method 1300 thus generates an experience tuple for each of the selected\ncells (C.sub.k, S.sub.t, S.sub.t+1, A.sub.t, R.sub.g,t+1, R.sub.c,t+1).  At step 1314, adds the generated experience tuples to an experience pool.\n At step 1316, the method 1300 determines whether it needs to generate more experience tuples.  The method 1300 may go back to step 1302 when it needs to perform more exploration.  In this case, the method 1300 may select a different number of\ngroups and a different number of cells.  When determining that no more exploration is needed, the method 1300 may proceed to step 1318, where a mini batch of experience tuples is generated using the experience pool.  In one example, the mini batch may\ninclude 128 experience tuples.  For each of the experience tuples, the method determines whether an action in the corresponding experience tuple is acceptable and updates the DRL-NN based on the determination result.\n At step 1320, the method 1300 determines whether an action in an experience tuple for a cell is acceptable at a global level based on the corresponding global reward.  In this example, the method 1300 determines whether the global reward is\ngreater than a best global reward value by 5%.  The best global reward value is a global reward that has been obtained for the cell corresponding to a plurality of actions performed on the cell during training of the DRL-NN.  When the action is not\nacceptable at the global level, the method 1300 proceeds to step 1326, where the action is rejected.  When the action is acceptable at the global level, the method 1300 proceed to step 1322, where the method 1300 continues to determine whether the action\nis acceptable at a cell level based on the local reward associated with the action.  In this example, the method 1300 determines whether the local reward is greater than an initial reward value of the cell by 3%.  The initial local reward value of the\ncell is calculated based on MRs received from UEs in the cell when the cell is in an initial state without taking any action.  When the action is not acceptable at the cell level, the method 1300 proceeds to step 1326, where the action is rejected.  When\nthe action is also acceptable at the cell level, the method 1300 proceeds to step 1324, where the action is accepted.  At step 1328, based on the determination whether the action is accepted or rejected, the method 1300 calculates and applies a gradient\nfor the action and updates the neural network's weights.  For example, a gradient may be back-propagated by a chain rule.  At step 1330, the method 1300 determines whether the neural network needs more or further training.  When the method 1300\ndetermines that more training is needed, it goes back to step 1302 to continue the training as discussed above.  When no more training is needed, the method 1300 stops the training, and outputs the training result.\n The trained DRL-NN may then be used to adjust a cell parameter of a cell in the wireless network.  The method 1300 may select an action for adjusting a cell parameter of a cell based on the trained neural network, and instruct to adjust the cell\nparameter of the cell according to the selected action.  A neural network may include a plurality of output nodes, and each node may stand for an action to be taken.  An action may be selected based on values output for the output nodes.  For example, in\na case where an action is defined as adjusting a tilt value, a neural network may have 5 output nodes standing for tilt relative changes of -2.degree., -1.degree., 0.degree., 1.degree., 2.degree., respectively.  In another example, in a case where an\naction is defined as adjusting a tilt and/or an azimuth, a neural network may have 5.times.3 matrix of nodes standing for joint adjustment of tilt and azimuth, with 5 rows of this 5.times.3 matrix indicating adjusting tilt by -2.degree., -1.degree.,\n0.degree., 1.degree., and 2.degree., and 3 columns indicating adjusting azimuth by -10.degree., 0.degree., 10.degree..  In either case, an action may be selected by selecting a node based on a value output for that node.  For example, when the values\noutput for the nodes are taken as probability, a node with the highest probability may be selected.\n FIG. 14 illustrates a flowchart of another embodiment DRL method 1400.  At step 1402, the method 1400 initializes a neural network with a set of weight values.  The neural network is used to determine actions that adjust one or more settings of\ncells associated with base stations in a wireless network, and each base station provides communication services to UEs within one or more cells.\n At step 1404, the method 1400 trains the neural network by using a DRL process, where a plurality of experience tuples are generated and each experience tuple can be DRL-generated or expert-generated.  The DRL process includes generating a first\nplurality of experience tuples for a plurality of cells in the wireless network.  Each experience tuple includes a cell identifier that identifies a cell, a first state of the cell, a second state of the cell, an action that causes the cell to transit\nfrom the first state to the second state, and a reward value for taking the action.  A state of a cell includes a setting of a base station providing a coverage area of the cell, and a reward value is calculated using a cost function based on measurement\nreports received from UEs in the wireless network.  Each experience tuple can be a DRL-generated experience tuple in which a respective action is selected by a DRL agent based on the neural network according to a DRL technique or an expert-generated\nexperience tuple in which the respective action is provided based on expert experience.  Whether an action is selected by the DRL agent based on the neural network or provided based on the expert experience is determined based on a first criterion.\n The DRL process also includes selecting a second plurality of experience tuples from the first plurality of experience tuples.  The second plurality of experience tuples may be used for updating the set of weight values of the neural network. \nThe DRL process further includes updating the set of weight values of the neural network according to the reward values of the second plurality of experience tuples.\n The first criterion may be a probability based criterion, a similarity based criterion or a threshold based criterion.  In some embodiments, initializing the neural network with the set of weight values may include determining whether the neural\nnetwork is initialized with randomly selected weight values or weight values obtained from an expert neural network.  The expert neural network has been trained using a deep learning technique and is able to provide weight values for initializing the\nneural network.  In some embodiments, generating the first plurality of experience tuples may include determining, based on the first criterion, whether an experience tuple is generated using an action that is selected by the DRL agent based on the\nneural network or whether the experience tuple is provided based on the expert experience.  In some embodiments, selecting the second plurality of experience tuples from the first plurality of experience tuples may include determining whether selecting a\nDRL-generated experience tuple or an expert-generated experience tuple from the first plurality of experience tuples based on a second criterion.  The second criterion may be a threshold based criterion, a probability based criterion, a similarity based\ncriterion, a criterion based on relationship among experience tuples, or a criterion based on importance sampling.\n FIG. 15 illustrates a flowchart of an embodiment method 1500.  The method 1500 may be used to adjust one or more cell parameters of a wireless network for generally optimizing the wireless network according to an objective.  At step 1502, the\nmethod 1500 performs training of a neural network for a wireless network using a deep reinforcement learning (DRL) process for adjusting one or more cell parameters of cells associated with base stations in the wireless network.  Each base station\nprovides communication services to user equipments (UEs) within a coverage area of one or more associated cell, and the neural network is trained to determine actions that can be performed on the base stations of the wireless network.\n In some embodiments, the DRL process includes performing steps 1504, 1514 and 1516.  As shown, at step 1504, the DRL process generates a plurality of experience tuples for the plurality of cells.  In some embodiments, generating the plurality of\nexperience tuples may include steps 1506-1512.  At step 1506, the DRL process generates a state tensor for each of the plurality of cells.  Each state tensor indicates a state of the respective cell.  A state tensor of a cell may include at least a cell\nparameter of the cell and information obtained from a measurement report provided by a UE.  A state of a cell comprises a setting of a base station associated with the cell, the base station providing a coverage area of the cell.  A state tensor may also\ninclude information of inter-site distance (ISD), a height of a base station, an antenna azimuth, an antenna mechanical tilt (mTilt), an antenna electronic tilt (eTilt), a key performance indicator, reference signal received power (RSRP), reference\nsignal received quality (RSRP), signal interference to noise ratio (SINR), channel quality indicator (CQI)), an objective function, a cumulative distribution function of network performance measurements, or an interference factor matrix.  At step 1508,\nthe DRL process selects an action for each of the plurality of cells.  An action moves the respective cell from one state to another state.  An action includes information for adjusting a setting of a base station associated with the cell.  At step 1510,\nthe DRL process applies the respective actions selected for the plurality of cells to the respective cells to adjust one or more cell parameters.  At step 1512, the DRL process generates an experience tuple for each of the plurality of cells based on the\nrespective action applied.  An experience tuple includes a cell identifier identifying the respective cell, a first state of the respective cell that is indicated by a respective state tensor, a second state of the respective cell, the action applied to\nthe respective cell that moves the respective cell from the first state to the second state, a local reward calculated for applying the action to the respective cell, and a global reward calculated for applying the action to the respective cell.  In some\nembodiments, the local reward may be calculated based on a local cost function and the global reward may be calculated based on a global cost function.  The local reward and the global reward may be calculated using information from different cells in\nthe communications network.\n At step 1514 of the DRL process, the DRL process determine whether or not an action of a first experience tuple in the plurality of experience tuples corresponding to a first cell is acceptable based on the local reward and the global reward of\nthe first experience tuple.  At step 1516 of the DRL process, the DRL process updates weights of the neural network based on whether or not the action is acceptable.  The DRL process may further select the plurality of cells from the wireless network\nbased on a predefined criterion.\n When the neural network has been trained, at step 1518, the method 1500 selects an action for adjusting a cell parameter of a cell in the wireless network based on the trained neural network.  The cell parameter of the cell may include an\nengineer parameter of a base station associated with the cell.  At step 1520, the method 1500 instructs to adjust the cell parameter of the cell according to the selected action.\n In some embodiments, selecting the action for each of the plurality of cells may include selecting an action for a cell in the plurality of cells using the neural network based on the state tensor of the cell.  In some other embodiments,\nselecting the action for each of the plurality of cells comprises selecting an action for a cell in the plurality of cells according to expert experience.\n In some embodiments, determining whether or not the action of the first experience tuple in the plurality of experience tuples is acceptable may include determining whether the global reward of the first experience tuple satisfies a first\ncriterion; when the global reward of the first experience tuple satisfies the first criterion, determining whether the local reward of the first experience tuple satisfies a second criterion; and when the global reward of the first experience tuple\nsatisfies the first criterion and the local reward of the first experience tuple satisfies the second criterion, determining that the action is acceptable.  The method 1500 may also include determining that the action is not acceptable when the global\nreward of the first experience tuple does not satisfy the first criterion or the local reward of the first experience tuple does not satisfy the second criterion.\n In some embodiments, the method 1500 may determine that the global reward of the first experience tuple satisfies the first criterion when the global reward exceeds a first threshold.  The method 1500 may also determine that the global reward of\nthe first experience tuple does not satisfy the first criterion when the global reward is less than a second threshold.  The first threshold or the second threshold may be a best global reward value that has been obtained for the first cell.\n In some embodiments, the method 1500 may determine that the local reward of the first experience tuple satisfies the second criterion when the local reward exceeds a third threshold.  The method may further determine that the local reward of the\nfirst experience tuple does not satisfy the second criterion when the local reward does not exceed a fourth threshold.  The third threshold or the fourth threshold may be an initial local reward value obtained for the first cell.  In some embodiments,\nupdating the weights of the neural network based on whether or not the action is acceptable may include applying a positive gradient when the action is acceptable; and applying a negative gradient when the action is not acceptable.\n Embodiments of the present disclosure have many advantages in performing wireless network optimization, such as shorter training time of a neural network, improved intelligence of DRL agents, and less cost for adjusting cells.  Further, the\nembodiments do not require UE location information and accurate engineer parameters.  Moreover, the embodiments help achieve the so-called one-shot optimization, where cell parameters may be adjusted once, instead of iteratively.  This greatly reduces\nthe time and cost for wireless network optimization.  FIG. 16 illustrates graphs showing one-shot optimization and iterative optimization.  FIG. 16 includes 3-dimensional graphs 1610 and 1630, both of which show network performance varying with a pair of\nparameters, i.e., Para 1 and Para 2, of a wireless network.  The x axis represents Para 1, y axis represents Para 2, and z axis represents the network performance.  Point 1620 corresponds to an optimal point where a specific pair of the parameters\nresults in generally optimal network performance.  Taking, as an example, Para 1 and Para 2 as cell parameters of two cells in the wireless network, Para 1 and Para 2 may be understood to indicate states of the two cells, respectively.  Thus, according\nto the graphs, varying the states of the two cells causes the network to have different levels of performance.\n As shown, in this example, point 1612 is a starting point indicating initial states of the two cells, and actions are to be selected for the two cells so that the network performance can be improved from the starting point 1612 to reach the\noptimal point.  Graph 1610 shows that iterative optimization has been performed for the two cells to move from the starting point 1612 to point 1614, to point 1616, to point 1618, and then to reach the optimal point 1620.  While for one-shot\noptimization, as shown in graph 1630, the two cells are moved from the point 1612 directly to the optimal point 1620, without going through an intermediate point.  One-shot optimization will be greatly appreciated when implementable for wireless network\noptimization.\n FIG. 17 illustrates a block diagram of an embodiment processing system 1700 for performing methods described herein, which may be installed in a host device.  As shown, the processing system 1700 includes a processor 1704, a memory 1706, and\ninterfaces 1710-1714, which may (or may not) be arranged as shown in FIG. 17.  The processor 1704 may be any component or collection of components adapted to perform computations and/or other processing related tasks, and the memory 1706 may be any\ncomponent or collection of components adapted to store programming and/or instructions for execution by the processor 1704.  In an embodiment, the memory 1706 includes a non-transitory computer readable medium.  The interfaces 1710, 1712, 1714 may be any\ncomponent or collection of components that allow the processing system 1700 to communicate with other devices/components and/or a user.  For example, one or more of the interfaces 1710, 1712, 1714 may be adapted to communicate data, control, or\nmanagement messages from the processor 1704 to applications installed on the host device and/or a remote device.  As another example, one or more of the interfaces 1710, 1712, 1714 may be adapted to allow a user or user device (e.g., personal computer\n(PC), etc.) to interact/communicate with the processing system 1700.  The processing system 1700 may include additional components not depicted in FIG. 6, such as long term storage (e.g., non-volatile memory, etc.).\n In some embodiments, the processing system 1700 is included in a network device that is accessing, or part otherwise of, a telecommunications network.  In one example, the processing system 1700 is in a network-side device in a wireless or\nwireline telecommunications network, such as a base station, a relay station, a scheduler, a controller, a gateway, a router, an applications server, or any other device in the telecommunications network.  In other embodiments, the processing system 1700\nis in a user-side device accessing a wireless or wireline telecommunications network, such as a mobile station, a user equipment (UE), a personal computer (PC), a tablet, a wearable communications device (e.g., a smartwatch, etc.), or any other device\nadapted to access a telecommunications network.\n In some embodiments, one or more of the interfaces 1710, 1712, 1714 connects the processing system 1700 to a transceiver adapted to transmit and receive signaling over the telecommunications network.  FIG. 18 illustrates a block diagram of a\ntransceiver 1800 adapted to transmit and receive signaling over a telecommunications network.  The transceiver 1800 may be installed in a host device.  As shown, the transceiver 1800 comprises a network-side interface 1802, a coupler 1804, a transmitter\n1806, a receiver 1808, a signal processor 1810, and a device-side interface 1812.  The network-side interface 1802 may include any component or collection of components adapted to transmit or receive signaling over a wireless or wireline\ntelecommunications network.  The coupler 1804 may include any component or collection of components adapted to facilitate bi-directional communication over the network-side interface 1802.  The transmitter 1806 may include any component or collection of\ncomponents (e.g., up-converter, power amplifier, etc.) adapted to convert a baseband signal into a modulated carrier signal suitable for transmission over the network-side interface 1802.  The receiver 1808 may include any component or collection of\ncomponents (e.g., down-converter, low noise amplifier, etc.) adapted to convert a carrier signal received over the network-side interface 1802 into a baseband signal.  The signal processor 1810 may include any component or collection of components\nadapted to convert a baseband signal into a data signal suitable for communication over the device-side interface(s) 1812, or vice-versa.  The device-side interface(s) 1812 may include any component or collection of components adapted to communicate\ndata-signals between the signal processor 1810 and components within the host device (e.g., the processing system 1700, local area network (LAN) ports, etc.).\n The transceiver 1800 may transmit and receive signaling over any type of communications medium.  In some embodiments, the transceiver 1800 transmits and receives signaling over a wireless medium.  For example, the transceiver 1800 may be a\nwireless transceiver adapted to communicate in accordance with a wireless telecommunications protocol, such as a cellular protocol (e.g., long-term evolution (LTE), etc.), a wireless local area network (WLAN) protocol (e.g., Wi-Fi, etc.), or any other\ntype of wireless protocol (e.g., Bluetooth, near field communication (NFC), etc.).  In such embodiments, the network-side interface 1802 comprises one or more antenna/radiating elements.  For example, the network-side interface 1802 may include a single\nantenna, multiple separate antennas, or a multi-antenna array configured for multi-layer communication, e.g., single input multiple output (SIMO), multiple input single output (MISO), multiple input multiple output (MIMO), etc. In other embodiments, the\ntransceiver 1800 transmits and receives signaling over a wireline medium, e.g., twisted-pair cable, coaxial cable, optical fiber, etc. Specific processing systems and/or transceivers may utilize all of the components shown, or only a subset of the\ncomponents, and levels of integration may vary from device to device.\n Additional aspects may be realized from the following examples.\nExample 1\n A method comprising: initializing a neural network with a set of weight values, the neural network being used to determine actions that adjust one or more settings of cells associated with base stations in a wireless network, each base station\nproviding communication services to user equipments (UEs) within one or more cells; and training the neural network by using a deep reinforcement learning (DRL) process, the DRL processing comprising generating a first plurality of experience tuples for\na plurality of cells in the wireless network, each experience tuple comprising a cell identifier that identifies a cell, a first state of the cell, a second state of the cell, an action that causes the cell to transit from the first state to the second\nstate, and a reward value for taking the action, wherein a state of a cell comprises a setting of a base station providing a coverage area of the cell, and a reward value is calculated using a cost function based on measurement reports received from UEs\nin the wireless network, wherein each experience tuple can be a DRL-generated experience tuple in which a respective action is selected by a DRL agent based on the neural network according to a DRL technique or an expert-generated experience tuple in\nwhich the respective action is provided based on expert experience, and wherein whether an action is selected by the DRL agent based on the neural network or provided based on the expert experience is determined based on a first criterion; and selecting\na second plurality of experience tuples from the first plurality of experience tuples; and updating the set of weight values of the neural network according to reward values in the second plurality of experience tuples.\nExample 2\n The method of example 1, the first criterion is a probability based criterion, a similarity based criterion or a threshold based criterion.\nExample 3\n The method of any one of examples 1-2, wherein initializing the neural network with the set of weight values comprises: determining whether the neural network is initialized with randomly selected weight values or weight values obtained from an\nexpert neural network, the expert neural network having been trained using a deep learning technique and being able to provide weight values for initializing the neural network.\nExample 4\n The method of any one of examples 1-3, wherein generating the first plurality of experience tuples further comprises: determining, based on the first criterion, whether an experience tuple is generated using an action that is selected by the DRL\nagent based on the neural network or whether the experience tuple is provided based on the expert experience.\nExample 5\n The method of any one of examples 1-4, wherein selecting the second plurality of experience tuples from the first plurality of experience tuples comprises: determining whether selecting a DRL-generated experience tuple or an expert-generated\nexperience tuple from the first plurality of experience tuples based on a second criterion.\nExample 6\n The method of any one of examples 1-5, wherein the second criterion is a threshold based criterion, a probability based criterion, a similarity based criterion, a criterion based on relationship among experience tuples, or a criterion based on\nimportance sampling.\nExample 7\n A method comprising: training a neural network for a plurality of cells in a wireless network using a deep reinforcement learning (DRL) process for adjusting one or more cell parameters of cells associated with base stations in the wireless\nnetwork, each base station providing communication services to user equipments (UEs) within a coverage area of one or more cells, and the neural network being trained to determine actions that can be performed on the base stations, wherein the DRL\nprocess comprises: generating a plurality of experience tuples for a plurality of cells in the wireless network, comprising: generating a state tensor for each of the plurality of cells, each state tensor indicating a state of a respective cell, wherein\na state of a cell comprises a setting of a base station associated with the cell, the base station providing a coverage area of the cell; selecting an action for each of the plurality of cells, the action moving the respective cell from one state to\nanother state, wherein an action comprises information for adjusting a setting of a base station associated with a cell; applying respective actions selected for the plurality of cells to the respective cells to adjust one or more cell parameters; and\ngenerating an experience tuple for each of the plurality of cells based on the respective action applied, the experience tuple comprising a cell identifier identifying the respective cell, a first state of the respective cell that is indicated by a\nrespective state tensor, a second state of the respective cell, the action applied to the respective cell that moves the respective cell from the first state to the second state, a local reward calculated for applying the action to the respective cell,\nand a global reward calculated for applying the action to the respective cell, the local reward being calculated based on a local cost function and the global reward being calculated based on a global cost function; and determining whether or not an\naction of a first experience tuple in the plurality of experience tuples corresponding to a first cell is acceptable based on the local reward and the global reward of the first experience tuple; and updating weights of the neural network based on\nwhether or not the action is acceptable; selecting an action for adjusting a cell parameter of a cell in the wireless network based on the trained neural network; and instructing to adjust the cell parameter of the cell in the wireless network according\nto the selected action.\nExample 8\n The method of example 7, wherein a state tensor of a cell comprises at least a cell parameter of the cell and information obtained from a measurement report provided by a UE.\nExample 9\n The method of any one of examples 7-8, wherein a state tensor comprises information of inter-site distance (ISD), a height of a base station, an antenna azimuth, an antenna mechanical tilt (mTilt), an antenna electronic tilt (eTilt), a key\nperformance indicator, reference signal received power (RSRP), reference signal received quality (RSRP), signal interference to noise ratio (SINR), channel quality indicator (CQI)), an objective function, a cumulative distribution function of network\nperformance measurements, or an interference factor matrix.\nExample 10\n The method of any one of examples 7-9, wherein the one or more cell parameters comprises an engineer parameter of a base station associated with a cell.\nExample 11\n The method of any one of examples 7-10, wherein the DRL process further comprises selecting the plurality of cells from the wireless network based on a predefined criterion.\nExample 12\n The method of any one of examples 7-11, wherein selecting the action for each of the plurality of cells comprises selecting an action for a cell in the plurality of cells using the neural network based on the state tensor of the cell.\nExample 13\n The method of any one of examples 7-12, wherein selecting the action for each of the plurality of cells comprises selecting an action for a cell in the plurality of cells according to expert experience.\nExample 14\n The method of any one of examples 7-13, wherein determining whether or not the action of the first experience tuple in the plurality of experience tuples is acceptable comprises: determining whether the global reward of the first experience\ntuple satisfies a first criterion; when the global reward of the first experience tuple satisfies the first criterion, determining whether the local reward of the first experience tuple satisfies a second criterion; and when the global reward of the\nfirst experience tuple satisfies the first criterion and the local reward of the first experience tuple satisfies the second criterion, determining that the action is acceptable.\nExample 15\n The method of any one of examples 7-14, further comprising determining that the action is not acceptable when the global reward of the first experience tuple does not satisfy the first criterion or the local reward of the first experience tuple\ndoes not satisfy the second criterion.\nExample 16\n The method of any one of examples 7-15, wherein determining whether the global reward of the first experience tuple satisfies the first criterion comprises: determining that the global reward of the first experience tuple satisfies the first\ncriterion when the global reward exceeds a first threshold.\nExample 17\n The method of any one of examples 7-16, wherein the first threshold is a best global reward value that has been obtained for the first cell.\nExample 18\n The method of any one of examples 7-17, wherein determining whether the global reward of the first experience tuple satisfies the first criterion comprises: determining that the global reward of the first experience tuple does not satisfies the\nfirst criterion when the global reward is less than a second threshold.\nExample 19\n The method of any one of examples 7-18, wherein the second threshold is a best global reward value that has been obtained for the first cell.\nExample 20\n The method of any one of examples 7-19, wherein determining whether the local reward of the first experience tuple satisfies the second criterion comprises: determining that the local reward of the first experience tuple satisfies the second\ncriterion when the local reward exceeds a third threshold.\nExample 21\n The method of any one of examples 7-20, wherein the third threshold is an initial local reward value obtained for the first cell.\nExample 22\n The method of any one of examples 7-21, wherein determining whether the local reward of the first experience tuple satisfies the second criterion comprises: determining that the local reward of the first experience tuple does not satisfy the\nsecond criterion when the local reward does not exceed a fourth threshold.\nExample 23\n The method of any one of examples 7-22, wherein updating the weights of the neural network based on whether or not the action is acceptable comprises: applying a positive gradient when the action is acceptable; and applying a negative gradient\nwhen the action is not acceptable.\nExample 24\n The method of any one of examples 7-23, wherein the local reward and the global reward are calculated using information from different cells in the wireless network.\nExample 25\n The method of any one of examples 7-24, wherein the one or more cell parameters comprise an antenna tilt, an antenna azimuth, or an antenna transmission power.\nExample 26\n An apparatus comprising: a non-transitory memory storage comprising instructions; and one or more processors in communication with the memory storage, wherein the one or more processors execute the instructions to: train a neural network for a\nplurality of cells in a wireless network using a deep reinforcement learning (DRL) process for adjusting one or more cell parameters of cells associated with base stations in the wireless network, each base station providing communication services to\nuser equipments (UEs) within a coverage area of one or more cells, and the neural network being trained to determine actions that can be performed on the base stations, wherein the DRL process comprises: generating a plurality of experience tuples for a\nplurality of cells in the wireless network, comprising: generating a state tensor for each of the plurality of cells, each state tensor indicating a state of the respective cell, wherein a state of a cell comprises a setting of a base station associated\nwith the cell, the base station providing a coverage area of the cell; selecting an action for each of the plurality of cells, the action moving the respective cell from one state to another state, wherein an action comprises information for adjusting a\nsetting of a base station associated with the cell; applying respective actions selected for the plurality of cells to the respective cells to adjust one or more cell parameters; and generating an experience tuple for each of the plurality of cells based\non the respective action applied, the experience tuple comprising a cell identifier identifying the respective cell, a first state of the respective cell that is indicated by a respective state tensor, a second state of the respective cell, the action\napplied to the respective cell that moves the respective cell from the first state to the second state, a local reward calculated for applying the action to the respective cell, and a global reward calculated for applying the action to the respective\ncell, the local reward being calculated based on a local cost function and the global reward being calculated based on a global cost function; and determining whether or not an action of a first experience tuple in the plurality of experience tuples\ncorresponding to a first cell is acceptable based on the local reward and the global reward of the first experience tuple; and updating weights of the neural network based on whether or not the action is acceptable; and select an action for adjusting a\ncell parameter of a cell in the wireless network based on the trained neural network; and instruct to adjust the cell parameter of the cell in the wireless network according to the selected action.\nExample 27\n The apparatus of example 26, wherein a state tensor of a cell comprises at least a cell parameter of the cell and information obtained from a measurement report provided by a UE.\nExample 28\n The apparatus of any one of examples 26-27, wherein a state tensor comprises information of inter-site distance (ISD), a height of a base station, an antenna azimuth, an antenna mechanical tilt (mTilt), an antenna electronic tilt (eTilt), a key\nperformance indicator, reference signal received power (RSRP), reference signal received quality (RSRP), signal interference to noise ratio (SINR), channel quality indicator (CQI)), an objective function, a cumulative distribution function of network\nperformance measurements, or an interference factor matrix.\nExample 29\n The apparatus of any one of examples 26-28, wherein selecting the action for each of the plurality of cells comprises selecting an action for a cell in the plurality of cells using the neural network based on the state tensor of the cell.\nExample 30\n The apparatus of any one of examples 26-29, wherein selecting the action for each of the plurality of cells comprises selecting an action for a cell in the plurality of cells according to expert experience.\nExample 31\n The apparatus of any one of examples 26-30, wherein determining whether or not the action of the first experience tuple in the plurality of experience tuples is acceptable comprises: determining whether the global reward of the first experience\ntuple satisfies a first criterion; when the global reward of the first experience tuple satisfies the first criterion, determining whether the local reward of the first experience tuple satisfies a second criterion; and when the global reward of the\nfirst experience tuple satisfies the first criterion and the local reward of the first experience tuple satisfies the second criterion, determining that the action is acceptable.\nExample 32\n The apparatus of any one of examples 26-31, wherein updating the weights of the neural network based on whether or not the action is acceptable comprises: applying a positive gradient when the action is acceptable; and applying a negative\ngradient when the action is not acceptable.\n Embodiments of the disclosure may be performed as computer-implemented methods.  The methods may be implemented in a form of software.  In one embodiment, the software may be obtained and loaded into a computer or any other machines that can run\nthe software.  Alternatively, the software may be obtained through a physical medium or distribution system, including, for example, from a server owned by the software creator or from a server not owned but used by the software creator.  The software\nmay be stored on a server for distribution over the Internet.  Embodiments of the disclosure may be implemented as instructions stored on a computer-readable storage device or media, which may be read and executed by at least one processor to perform the\nmethods described herein.  A computer-readable storage device may include any non-transitory mechanism for storing information in a form readable by a machine (e.g., a computer).  For example, a computer-readable storage device may include read-only\nmemory (ROM), random-access memory (RAM), magnetic disk storage media, optical storage media, flash-memory devices, solid state storage media, and other storage devices and media.\n It should be appreciated that one or more steps of the embodiment methods provided herein may be performed by corresponding units or modules.  For example, a signal may be transmitted by a transmitting unit or a transmitting module.  A signal\nmay be received by a receiving unit or a receiving module.  A signal may be processed by a processing unit or a processing module.  Other steps may be performed by a training unit/module, a calculating unit/module, a generating unit/module, a selecting\nunit/module, an applying unit/module, a determining unit/module, an updating unit/module, and instructing unit/module, an initializing unit/module, and/or a setting unit/module.  The respective units/modules may be hardware, software, or a combination\nthereof.  For instance, one or more of the units/modules may be an integrated circuit, such as field programmable gate arrays (FPGAs) or application-specific integrated circuits (ASICs).\n While this invention has been described with reference to illustrative embodiments, this description is not intended to be construed in a limiting sense.  Various modifications and combinations of the illustrative embodiments, as well as other\nembodiments of the invention, will be apparent to persons skilled in the art upon reference to the description.  It is therefore intended that the appended claims encompass any such modifications or embodiments.", "application_number": "15643266", "abstract": " A neural network is trained using deep reinforcement learning (DRL)\n     techniques for adjusting cell parameters of a wireless network by\n     generating a plurality of experience tuples, and updating the neural\n     network based on the generated experience tuples. The trained neural\n     network may be used to select actions to adjust the cell parameters. Each\n     experience tuple includes a cell identifier, a first state, a second\n     state, an action applied to the cell that moves the cell from the first\n     state to the second state, a local reward, and a global reward. The\n     neural network is updated based on whether or not each action is\n     acceptable, which is determined based on the global reward and the local\n     reward associated with each action.\n", "citations": ["20130031036", "20130122885", "20180376390"], "related": []}, {"id": "20190087772", "patent_code": "10373116", "patent_name": "Intelligent inventory management and related systems and methods", "year": "2019", "inventor_and_country_data": " Inventors: \nMedina; Marco Octavio Mascorro (Burlingame, CA), Ranatunga; Thavidu (Burlingame, CA), Sinha; Utkarsh (Burlingame, CA), Kaza; Sivapriya (Burlingame, CA), Hoang; Jason (Burlingame, CA), Mahendran; Jagadish (Burlingame, CA), Yang; Christopher (Burlingame, CA), Fan; Zhengqin (Burlingame, CA)  ", "description": "FIELD OF THE DISCLOSURE\n The present disclosure is generally related to inventory management and is more particularly related to intelligent inventory devices, robots, and related systems and methods thereof.\nBACKGROUND\n For most retail stores and many commercial stores, inventory management is a complex, time-consuming, and expensive issue.  Large stores can carry more than 10,000 items on shelves.  These items must be tagged, tracked, displayed, restocked, and\npriced on a regular basis to ensure product availability to customers.\n Inventory stocking is the process of placing items out on shelves or in displays such that they can be purchased by customers.  Restocking is the process of replenishing items that have been purchased, moved, stolen, or damaged.  Stocking and\nrestocking are time-consuming tasks, since they normally entail the detailed review of all products for sale.  Traditionally, store employees travel each aisle, noting the number and location of depleted or missing items.  They gather new inventory from\na backroom storage area, then travel each aisle again, replenishing low stock with new inventory.  Depending on the store, this process can take dozens of employees and many hours to complete.  Often, restocking must be done after a store has closed or\nlate at night.  This can leave shelves understocked for long periods during business hours.  Additionally, the process can require additional employees working an overnight shift to complete restocking before the opening of the store the next day.\n Another drawback of the conventional inventory process is that it can be difficult to collect accurate loss prevention data.  Employees may only realize that items have been stolen when restocking late in the day.  This makes it difficult to\nanalyze when theft occurred or tailor loss prevention policies to specific items and areas.\n While employees are restocking inventory on shelves, they often must concurrently perform quality assurance checks.  Employees ensure that all items are properly located, returning moved and misplaced items to their appropriate areas.  Often,\nthis means traveling the entire store in search of misplaced items and subsequently placing the misplaced items in their correct locations.  Additionally, employees must also ensure that items are displayed neatly, with price tags and labels visible. \nEmployees also frequently need to make sure that any pricing information displayed is correct.  Often, this means checking item prices against periodic or special sales lists and amending incorrect displays.  Furthermore, this method of pricing is not\ndynamic, as it is difficult for retail stores to adjust prices quickly based on supply and demand.\n Additionally, many franchise or branch stores are required to stock and display products in a manner determined by a corporate office.  Such information is usually displayed in the form of a planogram: a diagram that indicates the placement of\nproducts in a shelf and in a store.  Planogram compliance can be inaccurate for a number of reasons, including human error in reading the diagram, differences in store layout, inattention to placement details, and changes in product packaging.  However,\nplanogram compliance is important to ensure consistency between stores and to present products for sale according to a chosen strategic plan.  If stores do not stock and display products accurately, the data upon which corporate offices analyze sales and\ncreate strategic placement plans is likely to be inaccurate.\n Current solutions to these problems utilize inventory management software, point of sale systems, and tracking devices to manage inventory.  However, the implementation of these solutions is largely dependent on data supplied by humans.  This\ndata can be inconvenient to collect, time-consuming to gather, and inaccurate.\n Thus, a heretofore unaddressed need exists in the industry to address the aforementioned deficiencies and inadequacies.\nSUMMARY OF THE DISCLOSURE\n Embodiments of the present disclosure provide multiple intelligent improvements in devices, robots, systems, and methods for performing inventory management within a facility.\n In one embodiment, a robot for performing inventory management within a facility is provided.  The robot includes a locomotion platform, at least one imaging sensor for detecting inventory, a transmitter for sending inventory information to a\ndatabase, a receiver for receiving inventory information from a database, and a robot computer.  The robot computer communicates with the locomotion platform, the at least one imaging sensor, the transmitter, and the receiver, and the robot computer\nincludes a processor and computer-readable memory.  The robot computer is configured to capture inventory images from the at least one imaging sensor, detect inventory by comparing captured inventory images with stored inventory images, determine\ninventory information, determine a confidence level for the inventory information, and communicate at least a portion of the inventory information to the database.\n In another embodiment, the present disclosure can also be viewed as providing methods of inventorying a commercial facility with a robot.  In this regard, one embodiment of such a method, among others, can be broadly summarized by the following\nsteps: providing a robot within a commercial facility, wherein the robot has a locomotion platform, at least one imaging sensor for detecting inventory, a transmitter for sending inventory information to a database, a receiver for receiving inventory\ninformation from a database, and a robot computer in communication with the locomotion platform, the at least one imaging sensor, the transmitter, and the receiver, the robot computer including a processor and computer-readable memory; capturing\ninventory images from the at least one imaging sensor; detecting inventory by comparing captured inventory images with stored inventory images; determining inventory status; determining a confidence level for the inventory information; and communicating\nat least a portion of the inventory information to the database.\n In another embodiment, the present disclosure can also be viewed as providing a system for performing automated inventory management within a commercial facility.  The system includes at least one imaging sensor for detecting inventory, a\ntransmitter for sending inventory information to a database, a receiver for receiving inventory information from a database, and a computer.  The computer communicates with the locomotion platform, the at least one imaging sensor, the transmitter, and\nthe receiver, and the computer includes a processor and computer-readable memory.  The computer is configured to capture inventory images from the at least one imaging sensor, detect inventory by comparing captured inventory images with stored inventory\nimages, determine inventory information, determine a confidence level for the inventory information, and communicate at least a portion of the inventory information to the database.\n Other systems, methods, features, and advantages of the present disclosure will be or become apparent to one with skill in the art upon examination of the following drawings and detailed description.  It is intended that all such additional\nsystems, methods, features, and advantages be included within this description, be within the scope of the present disclosure, and be protected by the accompanying claims. BRIEF DESCRIPTION OF THE DRAWINGS\n Many aspects of the disclosure can be better understood with reference to the following drawings.  The components in the drawings are not necessarily to scale, emphasis instead being placed upon clearly illustrating the principles of the present\ndisclosure.  Moreover, in the drawings, like reference numerals designate corresponding parts throughout the several views.\n FIG. 1 is a schematic illustration of a robotic device for performing inventory management within a commercial facility, in accordance with a first exemplary embodiment of the disclosure.\n FIG. 2 is a plan view of the robot of FIG. 1 travelling through the facility.\n FIG. 3 is a perspective view of exemplary operational processes of the robot of FIG. 1.\n FIG. 4 is a flow chart showing an exemplary process for determining inventory information using the robot of FIG. 1.\n FIG. 5 is a flow chart showing an exemplary process for determining a confidence level for the inventory information using the robot of FIG. 1\n FIG. 6 is a block diagram of exemplary systems operating within and external to the robot.\n FIGS. 7A-B show exemplary embodiments of additional designs of the inventory management device, in accordance with the present disclosure, of which FIG. 7A shows the inventory management device having a suspended camera system.\n FIG. 8 is a flow chart showing a method of inventorying a commercial facility with a robot.\n FIG. 9 shows one exemplary embodiment of a system for performing inventory management within a commercial facility.\n FIG. 10 shows a user-driven embodiment of a system for performing inventory management within a commercial facility.\n FIG. 11 is a flow chart showing a method of inventorying at least a portion of a quantity of inventory items in a commercial facility with an electronic inventory apparatus, in accordance with a second embodiment of the disclosure.\n FIG. 12 is a flow chart showing an exemplary process for determining the inventory characteristics of at least one inventory item, in accordance with the second embodiment of the disclosure.\n FIG. 13 is a flow chart showing an exemplary process for determining a confidence level for the inventory information using the method of FIG. 3 FIG. 14 is a flow chart showing a method of inventorying at least partially non-visible items, in\naccordance with a third embodiment of the disclosure.\n FIG. 15 is a flow chart showing the central data hub in communication with hardware devices of the system for performing inventory management within a commercial facility.\n FIG. 16 is an illustration of the product manager dashboard interface of the system.\n FIGS. 17A-17B are illustrations of the visual location mapping interface of the system.\n FIG. 18 is an illustration of the store manager dashboard interface of the system.\n FIG. 19 is an illustration of the empty shelves interface of the system.\n FIG. 20 is an illustration of the associate performance interface of the system.\n FIG. 21 is an illustration of the customer home screen interface of the system.\n FIG. 22 is an illustration of the list view interface of the system.\n FIG. 23 is an illustration of the item screen interface of the system.\nDETAILED DESCRIPTION\n In the following description, reference is made to the accompanying drawings, which form a part hereof, and in which is shown, by way of illustration, various embodiments of the present disclosure.  It is understood that other embodiments may be\nutilized and changes may be made without departing from the scope of the present disclosure.\n Many aspects of the invention may take the form of computer-executable instructions, including algorithms executed by a programmable computer.  Those skilled in the relevant art will appreciate that the invention can be practiced with other\ncomputer system configurations as well.  Certain aspects of the invention can be embodied in a special-purpose computer or data processor that is specifically programmed, configured or constructed to perform one or more of the computer-executable\nalgorithms described below.  Accordingly, the term \"computer\" as generally used herein refers to any data processor and includes Internet appliances, hand-held devices (including palm-top computers, wearable computers, cellular or mobile phones,\nmulti-processor systems, processor-based or programmable consumer electronics, network computers, minicomputers) and the like.\n Some aspects of the invention may also be practiced in distributed computing environments, where tasks or modules are performed by remote processing devices that are linked through a communications network.  In a distributed computing\nenvironment, program modules or subroutines may be located in both local and remote memory storage devices.  Aspects of the invention described below may be stored or distributed on computer-readable media, including magnetic and optically readable and\nremovable computer disks, fixed magnetic disks, floppy disk drive, optical disk drive, magneto-optical disk drive, magnetic tape, hard-disk drive (HDD), solid state drive (SSD), compact flash or non-volatile memory, as well as distributed electronically\nover networks.  Data structures and transmissions of data particular to aspects of the invention are also encompassed within the scope of the invention.\n FIG. 1 is a schematic illustration of a robotic device for performing inventory management within a commercial facility, in accordance with a first exemplary embodiment of the disclosure.  The robotic device 1, (hereinafter \"robot\") may be any\ntype of robot, telebot, or similar fully or partially robotic device which is capable of autonomously navigating, sensing, or detecting some element and communicating with a remotely located database.  The robot 1 may include a locomotion platform 10, at\nleast one imaging sensor 20 for detecting inventory, a transmitter 30 for sending inventory information to a database, a receiver 32 for receiving inventory information from a database, and a robot computer 40.  The robot computer 40 may communicate with\nthe locomotion platform 10, the at least one imaging sensor 20, the transmitter 30, and the receiver 32.  The robot computer 40 may include a processor 42 and computer-readable memory 44.\n The robot computer 40 is configured to capture inventory images from the at least one imaging sensor 20, detect inventory by comparing captured inventory images with stored inventory images, determine inventory status, determine a confidence\nlevel for the inventory information, and communicate at least a portion of the inventory information to the database.\n The locomotion platform 10 allows the robot 1 to move.  The locomotion platform 10 may have, for example, two or more wheels and casters 12, allowing the robot 1 to move in any direction.  The robot 1 may include any type of drive system to move\nusing the locomotion platform 10, such as electromechanical drive motors.  The robot 1 may have a variety of heights.  In one example, the robot 1 may be tall enough to capture an image of the entire height of an aisle, shelf, or other product display\nsystem.  The height of the robot 1 may be adjustable.\n The robot computer 40 can be any computing device constructed from various hardware and software components utilizing any known operating system.  In one embodiment, the robot computer 40 is a mini computer that uses Ubuntu operating system and\nincludes a single 12V power supply.  The robot computer 40 may have sufficient processing power to run a variety of software, including for example, Robot Operating System (ROS), video processing with OpenCV, and the like.  Any computing components known\nin the art may be used with the robot computer 40.\n The robot 1 may further include a power system 50.  The power system 50 may include a battery 52 and a charging system 54.  The battery 52 may be a rechargeable lead-acid battery, lithium ion battery, or any other type of battery.  The charging\nsystem 54 may include an interface which allows the robot 1 to electrically couple to a docking station (not shown) for charging.  The power system 50 may include power distribution circuitry and components, including regulators, heat dissipation\ndevices, fuses and/or circuit breakers.  Furthermore, the power system 50 may include an emergency cut-off circuit which may automatically, or manually, cut power from the robot 1 under certain circumstances, for example if the battery 52 is too hot, if\nthe battery 52 is below a certain minimum threshold charge, or if the robot 1 moves outside of a predefined area.  Battery life may vary significantly depending on the operation of the robot 1.  In one example, the battery type, size and capacity may\nallow for a full day of use between charges.\n The robot computer 40 may have one or more processor(s) 42 and associated circuitry for the control and imaging of the robot 1.  The processor 42 may be, for example, an Arduino Mega microcontroller, which allows for easy development along with\nserial output for controlling the locomotion platform 10, and may act as a serial (e.g., via USB) device that provides an interface to the robot computer 40.  The processor 42 may be any processor, microprocessor or microcontroller, and may be a PIC\nmicrocontroller, which is generally powerful and allows for high speed USB and Ethernet connections for data transfer.  The processor 42 may include or be associated with some amount of computer-readable memory 44, including RAM, cache memory, hard\ndrives (HDDs), and solid state drives (SSDs).\n The robot may include a transmitter 30 and a receiver 32 for sending and receiving information from the database, human operators, or other computer systems.  The transmitter 30 and receiver 32 may be any type of communication hardware used for\ncommunicating over wireless protocols, for instance, Wi-Fi.RTM., Bluetooth.RTM., NFC.RTM., cellular communications protocols, or any other network arrangement and/or protocol known to those having ordinary skill in the art.  In one example, the robot may\nuse a combination of wireless protocols to communicate.\n The robot 1 may further include a robot location detector 34.  The robot location detector 34 may utilize any of a number of known location detection techniques, including Global Positioning System (GPS), Indoor Positioning System (IPS) and\nInertial Navigation System (INS), to detect the location of the robot 1.  The robot location detector 34 may also function in coordination with any number of maps, floorplans, or similar schematics of a layout of the facility in which the robot 1 is\nutilized.\n The imaging sensor(s) 20 may be located anywhere on the robot beneficial for capturing images of inventory.  In one example, the imaging sensor 20 may be positioned so that it is substantially parallel to items within the field of view while the\nrobot 1 travels along a route.  The robot may be equipped with additional imaging sensors 22.  The robot 1 may also be equipped with environmental or hazard sensors 24 (hereinafter \"other sensors\").  The imaging sensor 20, additional imaging sensors 22,\nand other sensors 24 are discussed in greater detail in FIGS. 3 and 6, below.\nOperational Example\n In one example, a robot embodiment of the disclosure may be employed in a commercial retail facility such as a store.\n FIG. 2 is a plan view of the robot of FIG. 1 travelling through the facility.  The robot 1 may autonomously navigate through the store at any point in the day.  For instance, the robot 1 may first navigate through the store before it opens, to\nestablish a baseline inventory analysis for the day.  It may navigate through the store several times during business hours, concluding after the store closes.\n The robot 1 may be programmed to navigate through specific waypoints 2 in the store.  Alternatively, the robot 1 may determine its own waypoints 2.  The robot 1 may collect sensor data, such as images, at each waypoint 2, and may attempt to\ncollect sensor data in precisely the same location in relation to each waypoint 2 as possible.  The location of waypoints 2 may be determined based on time of day, number of people in the store, aisle size, item density, or other factors.  Generally,\nwaypoints 2 may at least be determined based on the robot's 1 field of view and the image size required to accurately identify inventory on shelves 6.  In one example, waypoints 2 throughout the store may be calculated once and remain constant for a\nperiod of time.  In another example, waypoints 2 may be recalculated periodically, such as each day or each week.  Waypoints 2 may be determined, or changed ad hoc, by human operators temporarily as well.\n The robot 1 may navigate from one waypoint 2 to another.  In one example, the robot 1 may determine a route 4 that allows it to reach all of the waypoints 2 in the shortest amount of time.  In another example, the robot 1 may determine a route 4\nthat allows it to reach all of the waypoints 2 while traveling through substantially all of the aisles 16 in the facility.  In other examples, the route 4 may vary to avoid customers in the facility, to capture images of certain areas more often, or to\nnavigate through areas of high inventory turnover, among other reasons.\n Waypoints 2 may also assist the robot 1 in navigating.  For example, the robot 1 may confirm its location within the facility by comparing expected image data with actual image data at a waypoint 2.  The robot 1 may expect to capture images of a\ncertain type of product at one waypoint 2, and it may compare the captured images to expected images.  If the images are similar enough, the robot 1 may confirm it is at the intended waypoint 2.  Conversely, if the compared images are different enough,\nthe robot 1 may confirm it is at another waypoint 2 or is having trouble navigating.\n Waypoints 2 may also assist the robot 1 in label and barcode detection.  For example, upon reaching a certain waypoint 2, the robot 1 may expect to capture images of certain items within an aisle 16 associated with the waypoint 2.  The robot 1\nmay use this information to detect labels or barcodes more quickly by limiting its search parameters.  In another example, the robot 1 may know that items associated with a certain waypoint 2 are commonly misplaced, and may use this information to detect\nmisplaced items more quickly.\n Waypoint data may be included as part of the image metadata.  For example, time and date of capture, location within the facility, and the robot 1's distance from the product may be included as metadata.  Metadata may be used in inventory\nanalytics, discussed in greater detail below.\n The robot 1 may communicate waypoint data to a database or a human operator for analysis.  For instance, the robot 1 may communicate the waypoints 2 for which it was able to capture images, the percentage of the area around a waypoint 2 that has\nbeen imaged, or any issues in capturing images relative to a waypoint 2.  If image capture issues arise, human operators can use the waypoint 2 data to pinpoint problems, guide the robot 1, or route around a waypoint 2.\n In one implementation, multiple robots 1 may be used in a facility.  The robot may work together to achieve distributed imaging of all inventory in the facility.  For example, two robots 1 may travel down alternating aisles 16 capturing images\nuntil the entire facility has been scanned.  In another example, multiple robots 1 may travel substantially exclusive routes, but may overlap in areas with high inventory turnover or known theft.  The multiple robots 1 may be in communication with one\nanother.\n FIG. 3 shows a perspective view of exemplary operational processes of the robot of FIG. 1: capturing inventory images 7, processing the inventory images 8, and uploading inventory information 9 to a database on the cloud 60.\n Relative to FIGS. 1-3, as the robot 1 travels between waypoints 2, it may perform the process of capturing inventory images 7 using the imaging sensor 20.  Depending on the design, in implementation, the imaging sensor 20 may be a camera or\ncamera system.  For example, the robot 1 may be equipped with a digital camera that captures the visible, infrared, ultraviolet, radio spectrum, or a combination thereof.  In another example, the robot 1 may be equipped with additional imaging sensors 22\nsuch as sonar, LIDAR, radar or other object detection systems.  Multiple systems may work together to detect objects within the store.  For instance, a visible spectrum camera system may be used to capture images of store inventory, while an infrared\ncamera system may detect persons or obstacles in the path of the robot 1.  In another example, the visible spectrum camera system may capture images of store inventory, and a lower-resolution visible spectrum camera system may detect persons or obstacles\nin the path of the robot 1.\n In one example, the robot 1 may use a visible spectrum camera (hereinafter, a \"camera\") to capture images of inventory along a route 4 between waypoints 2.  The camera may be fixed on the robot 1 at a particular height and orientation.  This\nheight and orientation may be determined by aisle size, product size, inventory location, lighting conditions, or other factors.  The camera may also be movably attached to the robot 1 and may move up or down, forwards or backwards, side to side, or\nrotationally as it captures images.  In one example, the camera may be capable of optical telephoto or digital zoom.  The robot 1 may be programmed to adjust the position, angle, and zoom of the camera based on the robot 1's location or expected\ninventory capture.\n The camera may have adjustable exposure settings, such as aperture, shutter speed, ISO, white balance, exposure compensation, gain, capture rate, gamma, and exposure bracketing.  This may allow the camera to operate under a variety of lighting\nconditions, working distances, capture rates, and travel speeds.  In one example, the exposure settings may be adjusted in software by the robot 1.  Exposure settings may be fixed after being initially set, or they may be adjusted from time to time.  In\none example, the robot 1 may adjust the exposure settings before each image capture based on the conditions mentioned above.  In another example, the robot 1 may adjust the exposure settings based on the inventory to be photographed and the detail or\nresolution necessary to accurately detect a label or barcode.  In yet another example, a human operator may intervene to control some or all of the exposure settings for an image or area, particularly if one or more of the images is not being properly\nexposed.\n In one example, the camera may have a software autofocus feature.  The autofocus may operate in conjunction with the label detection software to determine the location of a label within the field of view before the image is captured.  The camera\nmay then focus on that portion of the field when capturing the image.  For example, if the robot 1 is attempting to capture an image of a particular product, it may take one or more photos of the area where it believes the product to be located.  The\nrobot 1 may run label detection software to detect the presence of product labels within the photos.  The robot 1 may then capture another image, adjusting the focus to the areas where labels were detected.  In this way, the labels will be in focus for\nimage processing and analysis.\n The optical components of the camera may be adjusted based on characteristics of the facility or the inventory.  For instance, the camera may utilize different lenses in different facilities, such as a wide angle lens where store aisles are\nnarrow and a large focal length lens where store aisles are wider.  The field of view of the camera may vary depending on the camera and lens configuration used.  The camera may also operate with lens filters, such as polarizers, ultraviolet filters, and\nneutral density filters, as are commonly used in photography.\n The camera may also utilize an onboard flash to provide key or fill lighting when necessary.  The robot 1 may determine when to use flash automatically based on the environmental conditions and image capture requirements.  For instance, the\nrobot may detect the ambient lighting in the part of the facility where the image is being captured and determine if flash would improve the image exposure.  If that part of the facility is bright, such as during midday or when overhead lighting is on,\nthe camera may use little or no flash.  If that part of the facility is dimly lit or dark, such as during the evening or when overhead lighting is off, the camera may use more flash.  If a part of the facility is bright, but a portion of the item to be\nphotographed is in shadow, the camera may apply some flash to bring out detail in the shadowed area.  Individual waypoints 2 may have custom flash settings that may be determined by the time of day, the robot's 1 location within the facility, or the\namount of detail resolution needed from a particular image.  For example, images requiring very high detail resolution may require neutral exposure in much of the image area.  The camera may provide flash for a portion of the image, or for multiple\nportions of the image area.  As another example, a waypoint 2 in the back corner of the facility may always receive flash, but the waypoint 2 immediately next to it may not.  As yet another example, a particular waypoint 2 may always receive flash when\nthe robot 1 captures an image at the end of the day, but it may not receive flash during other times of capture.  The flash may be direct light, diffuse light, or some combination thereof.  The robot 1 may determine when to use direct or diffuse flash\nlighting based on the waypoint, the time of day, the type of item, or other factors.  In addition to compensating for ambient lighting, the robot 1 may provide flash to compensate for other lighting factors such as excess glare, seasonal display\nlighting, directional or diffuse sunlight, or reflective surfaces.  For example, if a seasonal display places items in a location that receives less ambient light than usual, the robot 1 may detect this and add flash when capturing an image.  Or, if\nsunlight unevenly lights an item or area, the robot 1 may add flash to even the exposure.\n As another example, if the item to be photographed has a shiny or reflective surface, such as glass, plastic, or foil, the addition of flash may cause the item to be overexposed in the image.  In such a case, the robot 1 may employ an off-axis\nflash or diffuse lighting element, in combination with filters and other hardware, to properly expose the item.  Alternatively, the robot 1 may capture an image using normal flash and analyze the image for overexposure from shiny or reflective surfaces. \nIf the robot 1 detects overexposure, it may take additional images at different angles and distances in order to minimize glare.  Additionally, the robot 1 may be able to determine the type and number of reflective items based on their reflective\ncharacteristics.  For example, the robot 1 may learn that a certain type of item creates strong reflections in the area where it is located.  The robot may capture images, with and without flash, and analyze the reflections for characteristics of that\nitem.  As another example, the robot 1 may capture an image without flash to identify the type of a reflective item, then capture other images with flash to identify the number of those items.  The computer 40 may analyze the images with flash to easily\ncount the number of reflective surfaces.\n As another example, if the robot 1 is travelling at high velocity, it may require flash to properly expose the image.  At a high travel velocity, the shutter speed of the camera must increase to prevent motion blur on the imaging sensor 20, and\nincreased shutter speed means that the imaging sensor 20 captures less light when making the image.  Therefore, the robot 1 may use flash to brighten the image properly.\n A flash may also be used to provide backlighting for the image.  For example, when the robot 1 attempts to determine whether an item is out of stock, it may analyze the middle ground or background of the image for clues that items are missing. \nIf a portion of the shelf is visible, or if no item details are detected, the computer 40 may determine that the item is out of stock.  Backlighting may help to properly expose for these areas in the image.  Additionally, the robot 1 may employ a shade\non occasion to block or diffuse harsh direct lighting.  For example, where ambient or overhead lighting casts uneven shadows over an item to be photographed, it may be difficult for the computer 40 to process the image.  The robot 1 may place a shade\nbetween the light source and the item in order to even the exposure on the item.\n The robot 1 may capture images at several points throughout the facility.  Depending on the design, in implementation the robot 1 may capture images of one type of item at a time.  In another example, the robot 1 may capture images of multiple\ntypes of items at a time.  The subject matter captured in an image may be determined by the robot 1 based on item characteristics such as item or label size, field of view of the camera, or other factors.  The robot 1 may capture several images in rapid\nsuccession for image stacking or superresolution processing.  In image stacking, multiple images captured from the same location may be overlaid and blended to bring out details in the final image.  Each successive image may have different exposure\nsettings or focus points.  The final, composite image may have a higher dynamic range or a wider depth of focus than any of the individual images, allowing the robot 1 to better detect subject inventory in the image.  In superresolution, multiple images\ncaptured as the image sensor moves or is moved slightly, creating subpixel shifts in the intensity of light captured in each image.  The combined images can be used to resolve details of finer resolution than in any one image.  After capture, the images\nmay be stored in memory onboard the robot 1.  The robot 1 may automatically determine when to capture multiple images and apply image stacking or superresolution techniques during image processing.\n FIG. 4 is a flow chart showing an exemplary process for determining inventory information using the robot of FIG. 1.  It should be noted that any process descriptions or blocks in flow charts should be understood as representing modules,\nsegments, portions of code, or steps that include one or more instructions for implementing specific logical functions in the process, and alternate implementations are included within the scope of the present disclosure in which functions may be\nexecuted out of order from that shown or discussed, including substantially concurrently or in reverse order, depending on the functionality involved, as would be understood by those reasonably skilled in the art of the present disclosure.\n Relative to FIGS. 1-4, once an image has been captured as shown in box 400, it may be processed using initial post-processing, computer vision, neural network, machine learning, or deep learning techniques.  For instance, important features may\nbe extracted from the image using computer vision.  The computer 40 may then apply neural network, machine learning or deep learning techniques to process the extracted features.  The computer 40 can use this processed information to detect the location\nof items, labels, and barcodes.\n Generally, this kind of image processing uses an image data set to train the computer 40 to detect stock items.  The computer 40 may be given images showing the items, the labels, the items in stock, the items out of stock, and similar\nvariations.  The computer 40 may use these images to learn the characteristic qualities of the item or the level of stock.  With each subsequent viewing, the computer 40 learns more about the characteristic qualities.\n Image processing may be based on a number of approaches used alone or in concert.  For instance, as shown in box 410, image processing may begin with initial post-processing techniques such as adjusting exposure, white balance, highlights,\nshadows, image rotation, or cropping.  These techniques may make the image easier for the computer 40 to process further.  As shown in box 420, the computer 40 may extract features from the image using computer vision techniques.  Extracted features may\nfurther prepare the image for processing.\n The computer vision techniques may be particularly useful in performing label extraction of products, barcode detection and extraction of products, determining if an item is out of stock or in stock, and with providing a street view of an indoor\nenvironment.  Relative to label extraction, the robot 1 may utilize color thresholding and contour detection to determine the location of the labels of products containing the label information.  The extracted labels are then used for barcode detection. \nBarcode detection may utilize a gradient magnitude of the image (label) in horizontal and vertical directions, which can be determined using one or more imaging processing operators.  For example, Scharr operators, which result from an optimization\nminimizing a weighted mean squared angular error in the Fourier domain, may be used to detect barcode edges.  The region with high horizontal gradients and low vertical gradients may be identified.  High frequency noise may be smoothed from the gradient\nimage.  Blurred images may be subject to thresholding and morphological operators are applied on the thresholded image.  Using contour detection, the barcode region from a label is extracted, which permits identification of the item information, a price,\na location of the item, and a window size for searching for the item in an image.\n As shown in box 430, a classification approach may be used to assist in label or barcode detection.  The classification approach uses labels and barcodes cropped from raw images to build classifiers that help the software recognize items. \nClassifiers may be managed at different hierarchical levels to detect stock and recognize products.  For instance, classifier levels may include, but are not limited to: data from all stores, data from a single store, data from a single department across\nall stores, data in a single department in a single store, data for a particular product category across all stores, and data for a particular product category in a single store.  After the classifiers are built, they may be improved using captured image\ndata or data from previous results.\n Boxes 432, 434, 436, and 438 show other approaches that may be used, in detecting inventory and determining inventory information.  As shown in box 432, detection approach may be used to identify whether any items are out of stock by\nconsidering, without looking at every item in the image, whether there appear to be any items out of stock.  The detection approach uses the entire image to train a classifier which determines the location of an item and whether any stock is missing.  As\nshown in box 434, a product recognition approach may use data extracted from labels or barcodes to create product categories.  Product categories can assist in building classifiers using neural network, machine learning, or deep learning techniques.  As\nshown in box 436, an educated estimation approach may compare images from previous captures to determine how much stock remains.  As shown in box 438, a heuristic identification process may be used to identify an item by its price label.  The heuristic\nprocess compares previous images captured under similar conditions, such as location in the store or distance from the item, to new images, comparing detected features and other data.\n As shown in box 440, the robot 1 may use optical character recognition algorithms to extract text from labels.  Once a label has been detected, the computer 40 may run optical character recognition algorithms to extract product names, SKU\nnumbers, barcodes, prices, and the like.  The computer 40 may determine a confidence level for any information extracted using these algorithms.  If the computer 40 is not able to extract the optical character recognition information with enough\nconfidence, it may upload a high resolution image to the database 60 for further optical character recognition processing.  The computer 40 may use partial information or information from more than one category to determine the type or number of items.\n It is noted that the processes described herein may be used with multiple images compiled together, so-called \"image stitching\".  Image stitching may be implemented to account for the regions of an image that are close to the borders of the\nimage, in order to increase the usable area of an image.  For instance, if an item or label is located between the opposite borders of two consecutive images, for instance, between the left border of one image and the right border of the next image, the\ncomputer 40 may stitch the images together and extract the inventory information from the combined image.\n As shown in box 450, these processes can be used to determine inventory information.  Using the locations of detected labels or barcodes, the computer 40 may determine where items are located on a shelf or in an aisle.  Additionally, the\ncomputer 40 may use the locations of detected labels or barcodes to determine stock status, i.e., whether an item is in stock, low on stock, or out of stock.  For example, to determine which items are in-stock or out-of-stock, a morphological operator\nmay be applied to the structure background.  Commonly, aisles in retail stores are classified into three different categories: pegs, shelves, and trays.  Considering peg items, for example, if an item is out of stock, the computer 40 may have detected\ncircles in the aisle backing.  Around the label, the circle density within a given area may then be determined.  If the circle density is high, the computer 40 may determine that the item is low stock or out of stock.\n In another example, the robot 1 can determine stock quantity data using additional imaging sensors 22, such as radar, sonar, or LIDAR sensors in combination with computer 40 vision techniques.  This may be useful in helping the computer 40\nidentify items with changing packaging or appearance.  In one example using radar, the robot 1 may emit radio waves using a transmitter as it travels between waypoints.  A radar sensor may detect reflected radio waves, and the computer 40 may process the\ndetected data to determine the number of items in stock or on display.  The radar sensor may be a millimeter-wave sensor (MMW) capable of detecting electromagnetic radiation in the extremely high frequency (EHF) band, between 30-300 GHz.  MMW sensors may\ndetect items at a distance of up to several meters and within a resolution of a few centimeters.  For instance, an integrated transmitter/receiver MMW chip, such as the Texas Instruments.RTM.  IWR1443, which operates between 76-81 GHz, may be used to\ndetect and resolve items just a few centimeters apart on a shelf.  MMW sensors may be used at high travel velocity and without ambient lighting.  In another example, data from the radar sensor may be used in combination with captured images to accurately\ndetermine stock quantity data.  For instance, the robot 1 may transmit and detect radio waves at about the same time the camera is capturing images of a product area.  The computer 40 may process both sets of data together.  The radar data may first be\nprocessed using computer 40 vision techniques to determine an estimated number of items on display.  The captured image data may then be processed to detect and extract labels or barcodes.  This inventory information may be cross-referenced with\ndeterminations made using the radar data to accurately determine stock quantity.  These steps may be performed in any combination, order, or recursion necessary to determine stock quantity and other inventory information.  This may be especially useful\nfor products of small size and shape, for example, screws and other small hardware components.  The computer 40 may also use machine learning, deep learning, and neural network approaches to learn how to process this data together.  In another example,\ndata received from sonar or LIDAR sensors may be processed alone or with captured images in a similar manner.  The robot 1 may have multiple radar, sonar, or LIDAR sensors, or any combination thereof.  The robot 1 may determine when to use one or more of\nthe sensors alone or in combination with the image sensor based on environmental conditions, product or label size, speed of travel, or other factors.  For instance, a robot 1 operating in a low light environment may use two or more sensors to capture\nitem data to increase the information available for processing.  Or a robot 1 traveling at a high velocity may use two or more sensors to capture item data in order to overcome blurry or underexposed captured image data.\n In yet another example, the robot 1 may use multiple sensors in sequence to improved captured data.  For instance, the robot 1 may be equipped with radar and image sensors, with the radar sensor placed ahead of the image sensor in the robot 1's\ndirection of travel.  The radar sensor may capture preliminary data that the computer 40 may process to indicate the quantity of items on display.  The robot 1 may use this information to adjust the camera settings, such as exposure, aperture, or number\nof images taken, to properly capture all of the items on display.  This process may be done with any combination and number of sensors.\n The robot 1 may perform image capture and processing even when it is not connected to the internet.  Images and results may be saved to the robot's memory and uploaded once a connection has been restored.\n FIG. 5 is a flow chart showing an exemplary process for determining a confidence level for the inventory information using the robot of FIG. 1.  After capturing an image, as shown in box 500, and determining inventory information, as shown in\nbox 510, the computer 40 may determine a confidence level for the inventory information, as shown in box 520.  The confidence level may be determined by a number of factors, as shown in boxes 521-525, including captured image quality, the type of items\ndetected, the number of items detected, the stock status of the items, similarity to historic results, respectively.  Other factors known to those of skill in the art may be considered.  In one example, the computer 40 may assign a higher confidence\nlevel for images taken in optimal lighting conditions or clearly showing a label or a barcode.  The computer 40 may assign a lower confidence level for images with low contrast, or where a label or barcode is obscured.  Similarly, the computer 40 may\nassign a higher confidence level for images where the type and number of products can be accurately determined, while assigning a lower confidence level where the computer 40 cannot make a determination.  Further, the computer 40 may assign a higher\nconfidence level where the determined inventory information is similar to historically determined inventory information, but assign a lower confidence level where the determined inventory information varies in a statistically significant way.  In one\nexample, the computer 40 may use some combination of these factors in determining the confidence level.  Some factors may be considered more or less heavily, i.e., given weights, depending on the presence and extent of the factors.\n As shown in box 530, inventory information with a confidence level above a threshold may be communicated to the database 60, as shown in box 550.  This information may automatically be entered.  This threshold may be the same for all items in a\ncommercial facility, or it may differ from item to item.  For example, an extremely high confidence level may be desired for expensive, high margin items, or items prone to theft.  A lower confidence level may be acceptable for less expensive or low\nmargin items, as there may be too great a trade-off between accuracy and the effort required for accuracy.  Threshold confidence levels may be determined by the robot 1, the database 60, facility owners, or other software.  Threshold confidence levels\nmay be changed on occasion, for example, seasonally.\n As shown in box 540, inventory information with a confidence level below a threshold may not be automatically entered by the database.  In box 542, inventory information with a confidence level below a threshold may be sent to a human operator\nfor additional analysis or confirmation of results.  The human operator may use the image to manually determine inventory type, label, amount, or stock status, as shown in box 544.  The human operator may then direct the database to enter the inventory\ninformation, as shown in box 550.  Alternatively, as shown in box 546, the human operator may send the inventory information back to the robot 1 for additional processing and further communication to the inventory database 60.  In one example, inventory\ninformation with a confidence level below a threshold may direct the robot 1 to capture additional images for the subject inventory.  The robot 1 may return to the portion of the facility where the image was taken and take another image.  The subsequent\nimage may be processed and compared with the original image for confirmation of results.  As shown in box 548, inventory information with a confidence level below a threshold may also direct the robot 1 to ask a human employee in the facility to\nphysically verify the results within the facility.  The employee may check the inventory status and report it to the database 60, which may compare the inventory status with the original inventory information and submit that to the database.\n When detecting inventory details such as labels or barcodes, the robot 1 may use high resolution images for feature extraction.  These images may have large file sizes, and a typical retail store environment may not have sufficient internet\nbandwidth to upload the images in or close to real-time.  To circumvent this issue, the computer 40 may perform a portion of the image processing onboard.  After the computer 40 has finished the image processing necessary to detect and identify, it may\nupload portions of images to the cloud for further processing or for validation by a human operator.  For example, the robot 1 may capture an image of an item and run its processing software onboard.  After the item has been identified by its label, the\ncomputer 40 may crop the label from the image.  The label may be uploaded at full resolution, while the original image may be uploaded at a lower resolution for later display or other analysis.  As another example, after an item has been identified by\nits barcode, the computer 40 may crop the barcode from the image.  The barcode may be uploaded at full resolution, while the original image may be uploaded at a lower resolution for later display or other analysis.  As another example, when the computer\n40 is attempting to detect the amount of stock on display, it may analyze the entire high resolution image in an effort to reach a decision above a threshold confidence level.  If it can make a determination above the threshold confidence level, the\ncomputer 40 may upload a low resolution image, or no image at all.  If it cannot make a determination above the threshold confidence level, the computer 40 may upload all or part of the high resolution image.\n FIG. 6 is a block diagram of exemplary systems operating within and external to the robot 600.\n Block 600 shows the robot and all of the hardware and software systems within.  Block 601 shows that the robot 1 may operate in conjunction and communicate with other robots.  Block 602 shows a human operator that may be contacted by the robot\n600 or the database 660.\n Additional hardware systems are shown in blocks 610, 650, 634, 630, and 632.  Block 610 shows the locomotion platform system.  Block 650 shows the power system.  Block 634 shows the location detector system.  Blocks 630 and 632 show the\ntransmitter and receiver systems, respectively.\n Block 640 shows the computer system having a processor 642 and computer-readable memory 644.\n Block 620 shows the sensors system having image sensors 622 and other sensors 624.  The other sensors 624 may be temperature sensors, smoke detectors, carbon monoxide monitors, and the like.  The robot 600 may use these other sensors 624 to\npassively monitor for fire, carbon monoxide gas, or other environmental conditions adverse to inventory and humans.  If adverse conditions are detected, the robot 600 may send an alert to a human operator for further action.  The robot 600 may capture\nimages of the affected areas and send them to the human operator for additional confirmation of the adverse conditions.  The images may be sent to the database and marked to indicate potential problem areas or areas where items may be damaged.\n Block 660 shows the database system.  The database system 660 may have, as an example, training 661, analytics 662, and e-commerce 663 subsystems, among others.\n The training system 661 may help the computer 640 to learn to recognize inventory items, labels, barcodes, and stock status in a number of ways.  Training may be accomplished using images stored on the computer 640, on the database 660, or\nshared from the database to the computer 640.  The computer 640 may initially learn inventory characteristics by applying machine learning techniques to a set of training images designed to teach the computer 640.  The training images may show generic\nlabels, barcodes, or other inventory characteristics, and the computer 640 may learn to recognize the characteristics based on relationships between like images.  The training images may be customized to show inventory characteristics for a particular\ncommercial facility.  This may help the computer 640 learn more efficiently.  Initially, the computer 640 may learn to identify the labels and barcodes of an item by corroborating with a price tag located on the shelf underneath or nearby the item. \nPrice tags on shelves may be relatively static, and therefore, easier to identify with image processing.  The computer 640 may attempt to detect an item's label or barcode, detect the item's price tag, and compare both results to determine a higher\nconfidence level.  After some time, the computer 640 may be sufficiently confident with the results of the label or barcode detection that it does not confirm with the price tag detection.  Additionally, the computer 640 may learn inventory\ncharacteristics from an initial image set processed with mechanical turk, i.e, using human intelligence to process images while the computer 640 discovers relationships between the processed images.  After some time, the computer 640 may recognize\ninventory items, labels, barcodes, and stock status without the human contribution.\n The computer 640 may also learn inventory characteristics from captured images taken during operation of the robot 1.  As the robot 1 captures images and processes them to detect inventory, it may develop or refine the rules it uses to detect\ninventory characteristics.  This may be done in real-time, as the robot 1 is operating, or during idle time when the robot 1 is charging or otherwise not in use.  The robot 1 may use captured images from other robot is in the same facility or other\nfacilities to learn inventory characteristics as well.  The robot 1 may be able to download images from the database to use in learning.  To this end, the database may identify and make available images that are particularly helpful in robot 1 learning.\n The computer 640 may also learn inventory characteristics from captured images that resulted in confidence levels below a threshold.  The computer 640 may use these captured images to understand why the confidence levels fell below the\nthreshold, and develop rules for determining increasing confidence levels under those scenarios.  The computer 640 may also use results obtained when it communicated those images to a human operator.  The computer 640 may develop rules to understand why\nthe human operator made a particular decision.  In one example, the robot 1 may use low confidence level images from other robot is to learn inventory characteristics as well.  The robot 1 may be able to download images from the database to use in\nlearning.  To this end, the database may identify and make available images that are particularly helpful in robot 1 learning.\n Classifiers and other types of rules may be improved using captured image data or using data which produced an incorrect result.\n The analytics system 662 may provide detailed analysis of inventory information for human end users.  By example, several types of analysis are discussed below.\n Inventory information may be used in planogram analytics.  For example, inventory information concerning the location and placement of items in the commercial facility may be compared to the location and placement of items dictated in a\nplanogram.  Users can use this comparison to ensure that items are placed in the proper locations in one facility.  Other users may be able to monitor compliance with planograms across multiple facilities, for example, in franchise locations.  Further,\nimages and related data gathered by the robot 600 may be useful in comparing different planogram layouts.  In one example, a user may wish to compare the effectiveness of two or more different planogram layouts in multiple facilities.  The facilities may\nstock their shelves or displays according to the planograms, the robot 600 may capture, analyze, and communicate inventory information, and the database 660 may enter the inventory information from the facilities.  The user may then use the information\nto compare sales, inventory turnover, product misplacement, or any combination thereof across the facilities.  In another example, a user may wish to determine how multiple facilities unintentionally deviate from the prescribed planogram layout.  The\nuser may use the inventory information in the database 660 to quantify deviation across the facilities.  In another example, the user may wish to correlate unintentional planogram deviation with sales or inventory turnover in a facility.  In still\nanother example, planograms may be software-optimized based on inventory information from the database 660 and sales goals for a facility.  Planogram analysis may not be limited to one item or one area of the facility at a time.  The database 660 may\nallow analysis of overall planogram compliance in one facility, or in multiple facilities.  The database 660 may also allow analysis of planograms containing complementary or related items in different parts of the store.  For example, the database 660\nmay allow a user to analyze how planogram compliance in the wine section and the cheese section is related to sales or inventory turnover.\n Inventory information may be used to analyze stock data, such as the number of empty spots or out-of-stocks detected in an area or over a period of time.  The analytics system 662 may analyze the percentage and number of discrepancies spotted in\nan area over time, the number of units of missing inventory, the number of out-of-stock events over a period of time, and opportunity costs because of products missing from a shelf.  The analytics system 662 may also provide a heat-map of the facility\nshowing where out-of-stocks occur over time.\n The analytics system 662 may provide pricing analytics.  For instance, inventory information can be used to provide real-time or near real-time analysis with competitors.  In one example, software on the database 660 may pull stock and price\ninformation from competitors' websites.  That information can be compared with the inventory information determined by the robot 600 to provide a customer or store employee with accurate pricing and stock data.  In another example, this comparison may\nallow stores to offer dynamic pricing based on the pricing and availability of similar items in a nearby geographic area.  For instance, if the database 660 software determined that demand for a particular item was high, but availability in the area was\nlow, it could communicate a suggested increased price to a user.  Or, if availability was high and demand was low, it might communicate a suggested decrease price to a user.  This dynamic pricing may be used within several stores under the same ownership\nto better price inventory based on demand and availability.\n In another example, pricing analytics can be used to ensure correct pricing within a facility.  The robot 600 may be able to extract price information from the labels using optical character recognition, as discussed above.  The computer 640 may\nthen cross-check the extracted price information with price information contained within the database 660 or stored onboard to verify that the marked price of an item is the same as the intended price.  If there is a discrepancy, the computer 640 may\ncommunicate it to the database 660 for further action by a human operator.\n In still another example, the analytics system 662 may track and analyze a facility's responses to low inventory events, theft, and discrepancies.  For instance, the analytics system 662 may track the number of low inventory events, the number\nof events resolved over time, the nature of resolutions, and the like.  The analytics system 662 may also track employee information, such as which employees responded to events, response time, response effectiveness, and the like.\n The analytics system 662 may also assist in tracking theft.  When the computer 640 determines an area to have decreased inventory, employees may physically go to the area to confirm.  In some cases, employees may simply restock according to a\nplanogram.  In other cases, the employee may be able to determine that one or more items have been stolen, such as when damaged packaging remains, but a product is missing.  In those cases, the employee may be able to note the theft in the database 660. \nThe robot 600 may use this information in combination with machine learning or deep learning techniques to learn to detect theft automatically.  To this end, the database 660 may identify and make available images of known theft for robot 600 learning. \nIn another example, the robot 600 may adjust its route or waypoints to capture images more frequently in areas with known theft.  The analytics system 662 may provide analysis of known theft, including times, locations, and item types, to allow facility\nowners to better protect this inventory.\n The e-commerce system 663 may provide a platform for internet sales of items located within the facility.  The database 660 may maintain inventory and location data for every item in a commercial facility, or across multiple facilities. \nE-commerce customers seeking to purchase items may interact with the database 660, which may search for in-stock items and provide location data to customers.  In another example, once a customer places an order online, store associates may use the\ndatabase 660 information to fill the order accurately and efficiently.  For instance, a store software interface might communicate the customer's order to the associate, along with an optimal route for gathering items to fulfill the order.  In still\nanother example, the database 660 may display to the customer one or more of the captured images containing an item, allowing the customer to see the item, its packaging, and its location within a store.  In still another example, the database 660 may\nactively communicate the stock status, price, regional availability, or other information about an item to a customer who is considering buying the item.  This may allow the customer to decide to purchase when an item becomes available within a certain\ngeographic region, or when the price within a geographic region has reached a desired level.  This may also allow a customer shopping in one physical location to purchase from another physical location.\n The monitoring system 664 may help human operators ensure that the robot 600 is functioning properly.  In one example, the database 660 may enter information useful for troubleshooting or calibrating the robot 600.  For instance, the database\n660 may include information about waypoints where image data was and was not captured.  This may help human operators determine why the robot 600 was unsuccessful.  As another example, the database 660 may include information about the percentage of an\narea that was successfully imaged or analyzed.  This may help human operators determine where waypoints need to be adjusted or where environmental conditions need to be improved.  The robot 600 may send maintenance information, such as bug reports, test\ndata, alerts, and notifications, to the database 660 or to human operators.\n In another example, when multiple robots 600 are deployed within a facility to perform distributed imaging of the inventory, the monitoring system 664 may monitor the robots 600 to ensure proper coverage of the facility.  For instance, the\nmonitoring system 664 may analyze the location and capture time of images as the robots 600 upload them to the database 660.  The monitoring system 664 may compare this information to expected routes and may direct one or more of the robots 600 to change\nroutes.  In another example, the monitoring system 664 may coordinate the deployment and return of robots 600 during peak business hours.  In still another example, if one or more robots 600 sends inventory information with a low confidence level to the\ndatabase 660, the monitoring system 664 may direct another robot to capture another image to improve on or verify the results.\n All of the database systems 661-664 may be accessed using a graphical interface through a software application or a website.  The interface may work with a virtual model of the shelves and aisles in facility, such as a \"master-shelf\" model which\nmaintains a virtual representation of all of the inventory information communicated to the database.  The virtual model may not be a visual representation, but may be primarily a numerical representation of the inventory.  The virtual model may be\nupdated each time the robot 600 communicates new inventory information to the database 660.  Old inventory information may be maintained, averaged, or weighted with the new inventory information to provide a historical representation of the inventory\ninformation over time.  Significant events, such as out of stock determinations, may cause the virtual model to be updated.  The virtual model may use historic inventory information to form a general representation of a facility, i.e., a model of the way\nthe facility is usually stocked and where items are usually located.  The robot 600 may use information from this general representation to detect low stock, stolen, or misplaced items by comparing a recently captured image with the general\nrepresentation.\n FIGS. 7A-B show exemplary embodiments of additional shapes and designs of the inventory management device contemplated within the scope of this disclosure.\n FIG. 7A shows a device 701 designed as a camera system suspended from the ceiling.  The imaging sensor 720 captures images of inventory and communicates at least a portion of inventory information to a database 760.  The camera system may be\nstationary on the ceiling, fixed on one area in particular.  Multiple camera system devices 701 may be deployed throughout a facility to effectively cover substantially all of the facility.  In some cases, camera systems may be deployed in front of\nimportant, high volume, or high value areas of the facility to provide a minimum amount of coverage of those areas.  In one example, the camera system may be mounted to a wall, a shelf, a column, or another sturdy element.  In another example, the camera\nsystem device 701 may rotate in one or more directions to allow the imaging sensor 720 to capture images in other portions of the facility.  For instance, the device 701 may rotate parallel to the ceiling to allow imaging of items within a field of view\nas wide as 360.degree..  The imaging sensor 720 may pivot perpendicular to the ceiling to allow device 701 to image the items at a better angle.  In another example, the device 701 may be robotic, with a locomotion platform resembling one or more tracks\nlocated along the ceiling.  The device may move along the tracks using a rolling system in order to reach waypoints for image capture.  Once the device has reached the waypoints, it may capture an image from above the items.\n FIG. 7B shows a robot 701 designed as an aerial drone, with a locomotion platform 710 of propellers.  The imaging sensor 720 captures images of inventory and communicates at least a portion of inventory information to a database 760.  A drone\nembodiment of the robot 701 may be particularly useful in retail facilities with tall shelves or outdoor product displays.  For instance, a drone embodiment of the robot 701 may be used in a building supply facility that carries large items or has tall\nshelves.  The drone may easily capture images of large inventory or inventory stored at height, since the drone may fly at varying altitudes.  The drone may perform multiple imaging passes this way, assigning waypoints to different heights within the\nfacility.  One aisle may have waypoints for items at ground level, waypoints for items at a second level higher up, and so on.  A drone embodiment of the robot 701 may also be particularly useful in large consumer warehouse facilities, where items are\noften stored at height.  In these facilities, the risk of interaction with humans may be minimized.  Additionally, the drone may also be used to provide order fulfillment while it is capturing images for inventory management.  For example, a drone may be\ndispatched to a part of the facility to retrieve a product for shipment to a customer.  Along the way, the drone may capture images of other items in the facility.  The drone may capture images when taking the product to a fulfillment area.  This may\nexpedite inventory management by combining it with other routine activities.\n FIG. 8 is a flow chart showing a method of inventorying a commercial facility with a robot.  As shown in block 800, a robot is provided within a commercial facility, wherein the robot has a locomotion platform, at least one imaging sensor for\ndetecting inventory, a transmitter for sending inventory information to a database, a receiver for receiving inventory information from a database, and a robot computer in communication with the locomotion platform, the at least one imaging sensor, the\ntransmitter, and the receiver, the robot computer having a processor and computer-readable memory.  As shown in block 810, inventory images are captured from the at least one imaging sensor.  As shown in block 820, inventory is detected by comparing\ncaptured inventory images with stored inventory images.  As shown in block 830, a confidence level is determining for the inventory information.  As shown in block 840, at least a portion of the inventory information is communicated to the database.\n FIG. 9 shows one exemplary embodiment of a system 900 for performing inventory management within a commercial facility.  The system 900 includes at least one imaging sensor 920 for detecting inventory, a transmitter 930 for sending inventory\ninformation to a database, a receiver 932 for receiving inventory information from a database 960, and a computer 901 in communication with the at least one imaging sensor, the transmitter, and the receiver.  The computer has a processor and\ncomputer-readable memory, and is configured to capture inventory images from the at least one imaging sensor 920, detect inventory by comparing captured inventory images with stored inventory images, determine inventory information, determine a\nconfidence level for the inventory information, and communicate at least a portion of the inventory information to the database 960.  In one example, the system 900 may include a tablet or smartphone having the imaging sensor 920, transmitter 930,\nreceiver 932, and computer 901.  Employees may carry the smartphone or tablet during the day and capture images at specific or predetermined intervals.  In another example, the smartphone or tablet may be affixed to a shopping cart and may capture images\nas customers shop through the store.  The smartphone or tablet may communicate inventory information while in use.  The smartphone or tablet may alternatively store inventory information until the employee or the customer has returned the device.\n FIG. 10 shows a user-driven embodiment of a system 1001 for performing inventory management within a commercial facility.  The system 1001 may include a user-drivable sensor platform having a steering mechanism 1040 and wheels or castors 1002. \nThe user 1050 may use the steering mechanism 1040 to push, pull, and direct the motion of the platform.  The platform may include electronic components similar to the examples above, including a battery 1004, wireless communication equipment 1006, and a\ncomputer 1008.  The electronic components 1004, 1006, 1008 may be in electrical communication with each other and the other components included in the system 1001.  The platform may be lined with a combination of 2D and 3D sensors.  For example, shelf\nsensors 1010 may be placed at several points along a vertical column on the platform.  Each sensor 1010 may image portions of a shelf at a given height as the platform is maneuvered through the commercial facility.  Other sensors, such as distance sensor\n1012 may be used to help the system 1001 determine the correct distance between the platform and a shelf being imaged.  Such sensors may, for example, be laser ranging sensors.  Depth sensors 1014, 1016 can perform ranging of the depth of shelves and\nitems on the shelves as they are being imaged.  Handheld sensors 1030 may be used by the user to scan particular items or areas that need further attention or are being imaged incorrectly.  A display 1020 may provide the user 1050 with information about\nthe scanning and imaging being performed, including progress updates, graphical information, alerts, errors, and the like.  The platform may be adjustable in height to accommodate a number of shelving arrangements.  Data that is received from the\nnumerous sensors may be processed onboard, as discussed relative to the examples above.  Processed information may be wirelessly uploaded to a cloud database 1060 for further processing and distribution to other terminals connected to the system.\n In one example, the sensors 1010 may be RFID scanners capable of detecting RFID devices located within items on the shelves.  The computer 1008 may use simultaneous location and mapping (SLAM) techniques in conjunction with the RFID tag data to\nknow in real-time its exact location within the environment.  This information may be leveraged by the system 1001 along with other data such as the signal strength of the RFID tags detected to calculate the exact position of the tag in the environment. \nAs the platform is driven around, it may pick up all of the RFID tags and accurately track its location and movement over time.  In order to increase accuracy, the system may rapidly cycle through all of the antennas, turning them on and off in\nsuccession.  This greatly improves reading accuracy by filtering out signal reflections from other antennas.  In one example, the RFID sensors may be mounted on a supporting pole to allow them to read RFID signals regardless of an RFID tag's vertical\nheight in the aisle.  In another example, the supporting pole may be detachable or retractable to allow the sensors to reach difficult areas.\n Information received by the RFID sensors may be analyzed using machine learning or any combination of the other data analysis techniques discussed herein.  For example, machine learning techniques may allow the robot computer 40 or other\ncomputers within the system to detect patterns, identify shapes corresponding to product configurations, predict item counts and orientations, and the like.  RFID data may be combined with other data to form a multi-layer understanding of the placement\nand location of inventory items, i.e., using visual data to understand inventory arrangements, using RFID data to understand ranging and location.  Machine learning techniques are discussed in greater detail below.  Machine learning processing may be\napplied through any one or more of the computers within the system as discussed below.\n FIG. 11 is a flow chart 1100 showing a method of inventorying at least a portion of a quantity of inventory items in a commercial facility with an electronic inventory apparatus, in accordance with a second embodiment of the disclosure.  The\nmethod may be performed with the apparatus 1 of FIG. 1 and any of the exemplary embodiments of FIG. 2, or it may be performed using other suitable hardware and software components.  It should be noted that any process descriptions or blocks in flow\ncharts should be understood as representing modules, segments, portions of code, or steps that include one or more instructions for implementing specific logical functions in the process, and alternate implementations are included within the scope of the\npresent disclosure in which functions may be executed out of order from that shown or discussed, including substantially concurrently or in reverse order, depending on the functionality involved, as would be understood by those reasonably skilled in the\nart of the present disclosure.\n Step 1110 includes generating, with a signal generator, a detection signal having an initial frequency substantially between 30 GHz and 300 GHz.\n Step 1120 includes transmitting, with at least one antenna in communication with the signal generator, the detection signal to at least one inventory item.\n Step 1130 includes receiving, with the at least one antenna, a reflected signal from the at least one inventory item.  As discussed above, the transmitting and receiving antennas may be the same antenna alternating between transmit and receive\nfunctions, or separate antennas dedicated specifically to a transmit or a receive function.\n Step 1140 includes comparing, with a processor in communication with the at least one antenna, the reflected signal with the detection signal to determine a difference between the reflected and detection signals.  In one example, after\nperforming this step, the reflected and detected signals may be stored on computer-readable memory.  In another example, the difference determination may be stored.\n Step 1150 includes determining, with the processor, at least one of: a location, a count, an orientation, and a type of the at least one inventory item, wherein the determination is based on the difference between the reflected and detection\nsignals.  In one example, after performing this step, the determination may be stored on computer-readable memory.  In another example, Step 1150 may include the steps of learning determination rules from a set of \"training\" reflected millimeter wave\nsignals (hereinafter, \"training signals\") stored on computer-readable memory, and applying at least one determination rule to the received reflected millimeter wave signal to make the determination.  Alternatively, the training signals may be received by\nthe transmitter and used to train the processor.  This is described further in FIG. 12, below.\n Step 1160 includes calculating, with the processor, a confidence level for the determination.  The process of calculating a confidence level is described further in FIG. 13, below.\n Step 1170 includes communicating, with a transmitter in communication with the processor, at least a portion of the determination and the confidence level to a database.  The database may be a cloud database, local network, or other computer\nstorage.\n The method of inventorying a commercial facility may be repeated several times, in full or in part, to inventory the desired portion of the commercial facility.  This may depend on multiple factors, for instance, the field of view and resolution\nof the apparatus or scanning device, the size of the facility, the nature of the items being inventoried, and the like.  A user may initiate scans at several points throughout the facility.  For example, a user using a handheld embodiment as in FIG. 9\nmay scan a single shelf using steps 1110-1160, repeat those steps on the next shelf, and continue repeating steps 1110-1160 until a full aisle has been scanned.  The user may then perform step 1170 to communicate the determination and confidence level to\nthe database.  In another example, the fixed embodiment of FIG. 7A capable of rotating 360.degree.  may have a larger field of view (FOV), and may only perform steps 1110-1160 a few times as it scans an entire aisle.  Or the fixed embodiment may perform\nthe steps many times more in order to receive more granular results.  In another example, the robotic embodiment of FIG. 1 may perform the entire method hundreds of times as it travels along aisles of shelves in the commercial facility, receiving\nconstant feedback about the quality of its calculations.\n The location and frequency of scans may be determined by the user.  For example, the user may adjust settings on the apparatus or scanning device to perform scans in a desired manner.  The user may adjust the frequency of the signal, FOV of the\nsensor, sensitivity, and the like.  The location and frequency of scans may alternatively be determined by the processor.  Based on the system limitations--FOV, angular resolution, processing speed, memory read/write speeds, etc.--the processor may\ninstruct the user to perform more or fewer scans in certain parts of the facility or when detecting certain inventory items.  The processor may use location detection to establish waypoints where scans must be performed.\n In one example, the apparatus or scanning device may perform several scans in rapid succession in order to stack the processed signals or perform superresolution processing.  In stacking, multiple processed signals captured from the same\nlocation may be overlaid and further processed to reduce noise and increase signal contrast.  In superresolution, multiple reflected signals may be received as the receiver moves slightly or with multiple receivers slightly offset from each other,\ncreating small shifts in the temporal or spatial characteristics of the reflected signals.  The combined processed signals may resolve finer details than individual signals alone.  Stacking or superresolution imaging may be chosen by either the user or\nthe processor, depending on the intended inventory items or use conditions.\n FIG. 12 is a flow chart 1201 showing an exemplary process for determining the inventory characteristics of at least one inventory item, in accordance with a second embodiment of the disclosure.  The process will be discussed relative to FIGS.\n1-4.\n Step 1200 includes receiving one or more reflected signals.  The reflected signals may include continuous or discrete chirps initially transmitted by the transmitter.\n Step 1210 includes initial signal processing.  The transmitted and reflected signals are directed to the processor.  In one example, they may be combined by the mixer, which may perform basic signal processing operations such as adding,\nsubtracting, time-shifting, and the like, as discussed above.  These operations may reduce noise or otherwise prepare the signals, alone or combined, for processing.  In another example, the signals may pass through an analog-to-digital converter (ADC)\nto prepare them to be read by the processor.  The processor may perform some initial syncing operations as well.  Any other signal processing techniques commonly known in the art may be performed during this step.\n Step 1220 includes extracting details using computer vision.  Computer vision may include the use of signal thresholding and contour detection to determine where discrete inventory items are located or how they are oriented.  Additional signal\nprocessing operators, such as Scharr operators resulting from an optimization minimizing a weighted mean squared angular error in the Fourier domain, may be used to detect inventory item edges.  Regions with contrasting mean gradients may be identified. \nHigh frequency noise may be smoothed from the gradient signal image plane.  Morphological operators may be applied on the thresholded signal.  This step may focus on processing only portions of the total FOV that are helpful for detecting and\ndifferentiating inventory items.\n Step 1230 includes using a classification approach to assist in inventory characteristic detection.  Generally, this kind of image processing uses a set of signals to train the processor 30 to detect stock items, orientation, count, and\nlocation.  The processor 30 may be given signals indicative of certain items, certain arrangements of items, items in stock, items out of stock, and similar variations.  The processor may use these training signals to learn the characteristic qualities\nof the item or the level of stock.  With each subsequent viewing, the processor learns more about the characteristic qualities.  The training data may be stored in on-board memory, transmitted from the database, or developed from previous scans run by\nthe apparatus.  Some combination of all training data may be used to develop robust detection rules for the processor.\n The processor 30 may initially train by applying machine learning techniques to a set of training signals.  The training signals may show generic signal examples, and the processor may learn to recognize inventory characteristics based on\nrelationships between like signal sets.  The training signals may be customized for certain conditions in a particular facility, which may help the apparatus 1 learn more efficiently.\n The classification approach may use the training signals to build classifiers that help it recognize items.  Classifiers may be managed at different hierarchical levels to detect stock and recognize products.  For instance, classifier levels may\ninclude, but are not limited to: data from all stores, data from a single store, data from a single department across all stores, data in a single department in a single store, data for a particular product category across all stores, and data for a\nparticular product category in a single store.  After the classifiers are built, they may be improved using captured image data or data from previous results.\n Steps 1232, 1234, 1236, and 1238 show other approaches that may be used in detecting inventory items and determining inventory characteristics.  In step 1232, a detection approach may be used to identify whether any items are out of stock by\nconsidering, without considering every single detected inventory item, whether there appear to be any items out of stock.  The detection approach uses the received signals as a whole to train a classifier which determines the location of an item and\nwhether any stock is missing.  In step 1234, a product recognition approach may use unique signal characteristics to create product categories.  For example, if a particular inventory item always returns a reflected signal with a particular amplitude\nmodulation or a unique peak frequency, the processor may use this to create a category for that item.  Product categories can assist in building classifiers using neural network, machine learning, or deep learning techniques.  In step 1236, an educated\nestimation approach may compare processed signals from previous inventory scans to the current inventory scans in order to determine how much stock remains.  In step 1238, a heuristic identification process may be used to identify an item by the details\nextracted from the computer vision process 1220.  The heuristic process compares previous signals captured under similar conditions, such as location in the facility or distance from the item, to the current signals, comparing detected features and other\ndata.\n It is noted that the processes described herein may be used with multiple signal images compiled together, so-called \"image stitching\".  Image stitching may be implemented to account for the regions of an image that are close to the borders of\nthe FOV, in order to increase the usable area of a signal scan.  For instance, if an item is located between the opposite borders of two adjacent scans, for instance, between the left border of one scan and the right border of the next scan, the\nprocessor may stitch the images together and extract the inventory information from the combined signal image.\n As shown in box 1240, these processes can be used to determine inventory characteristics for detected inventory items.  The processor may use one or more of the classification processes, in conjunction with the computer vision and other signal\nprocessing operations, to discover pieces of inventory characteristics or patterns during the scan.  The processor 30 may combine the pieces or patterns, giving some pieces more weight, to determine the desired inventory characteristics.\n For example, using the locations of detected edges and contours, the processor 30 may determine where items are located on a shelf or in an aisle.  Additionally, the processor 30 may use the locations of detected edges and contours to determine\norientation by associating certain edge or contour patterns with certain orientations.\n In another example, the apparatus 1 can determine stock quantity data using additional imaging sensors, such as visual cameras, radar, sonar, or LIDAR sensors in combination with processing already described.  This may be useful in helping the\nprocessor identify items that are difficult to distinguish using millimeter waves, such as items that have substantially similar shapes.  In yet another example, the apparatus may use multiple sensors in sequence to improve captured data.  For instance,\nthe apparatus may be equipped with millimeter wave and image sensors, with one sensor configured to scan first.  Based on the results of the first scan, the other sensor may focus in on particular areas for increased resolving power, noise removal, or\nother analysis.\n FIG. 13 is a flow chart 1301 showing an exemplary process for determining a confidence level for the inventory information using the apparatus of FIG. 1.  The confidence level indicates how accurate the processor 30 believes the determined\ninventory characteristics to be.\n In step 1300, the apparatus receives a reflected signal.  In step 1310, the apparatus 1 determines any desired inventory characteristics.  In step 1320, the apparatus calculates a confidence level for the determination.  The confidence level may\nbe determined by a number of factors, as shown in steps 1321-1325, including captured signal quality, the type of items detected, the number of items detected, the stock status of the items, and similarity to historic results, respectively.  Other\nfactors known to those of skill in the art may be considered.  In one example, the processor may assign a higher confidence level for signals received with optimal noise levels, or signals clearly indicating one or more inventory items.  The processor\nmay assign a lower confidence level for noisy or incomplete signals, or where items are located too close together to distinguish.  Similarly, the processor may assign a higher confidence level for images where the type and number of products can be\naccurately determined, while assigning a lower confidence level where the processor cannot make a determination.  For example, where the detected inventory items have unique characteristics, the confidence level may be higher.  Further, the processor may\nassign a higher confidence level where the determined inventory characteristics are similar to historically determined inventory characteristics, but assign a lower confidence level where the determined inventory characteristics vary in a statistically\nsignificant way.  In one example, the processor may use some combination of these factors in determining the confidence level.  Some factors may be considered more or less heavily, i.e., given weights, depending on the presence and extent of the factors.\n In step 1330, inventory characteristics with a confidence level above a threshold may be communicated to the database, as shown in box 1350.  This information may automatically be entered.  This threshold may be the same for all items in a\ncommercial facility, or it may differ from item to item.  For example, an extremely high confidence level may be desired for expensive, high margin items, or items prone to theft.  A lower confidence level may be acceptable for less expensive or low\nmargin items, as there may be too great a trade-off between accuracy and the effort required for accuracy.  Threshold confidence levels may be determined by the processor, the database, facility owners, or other software.  Threshold confidence levels may\nbe changed on occasion, for example, seasonally.\n In step 1340, inventory characteristics with a confidence level below a threshold may not be automatically entered by the database.  In step 1342, inventory characteristic determinations with a confidence level below a threshold may be passed to\na user for additional analysis or confirmation of results.  The user may use signals from other sensors, if present, to manually verify inventory location, count, orientation, or type, as shown in box 1344.  The user may then direct the database to enter\nthe inventory characteristics, as shown in step 1350.  Alternatively, as shown in step 1346, the user may send the signal back to the apparatus for additional processing and further communication to the inventory database.  In one example, inventory\ncharacteristics with a confidence level below a threshold may require the apparatus to capture additional signal data for the subject inventory items.  The apparatus may perform another scan of the relevant area.  The subsequent signals may be processed\nand compared with the original signals for confirmation of results.  As shown in step 1348, inventory characteristics with a confidence level below a threshold may also require the user to physically verify the results.  The user may check the inventory\nstatus and report it to the database, which may compare the inventory status with the original inventory characteristics and submit that to the database.\n In one example, the apparatus may learn inventory characteristics from scans that resulted in low confidence levels.  The processor may use these scans to understand why the confidence levels fell below a threshold and develop rules that will\nlead to increased confidence levels.  The apparatus may learn inventory characteristics from scans confirmed by a user.  The processor may develop rules to understand why the user made a particular decision.  In another example, the apparatus may learn\ninventory characteristics from scans performed by other apparatuses within its network.  To this end, the database may identify and make available scans that are particularly helpful in apparatus learning.\n In another example, the database may include an analytics system, e-commerce system, or other end-user systems for processing and utilizing the scanned inventory characteristics.\n The analytics system may provide detailed analysis of inventory information for human end users.  Planogram analytics may be used to analyze inventory layout plans for improvements.  Inventory analytics may be used to analyze ordering patterns\nfor effectiveness.  Pricing analytics may be used to compare the prices of available inventory to competitors.  Response analytics can be used to analyze a facility's response to low inventory events, theft, and discrepancies.\n The e-commerce system may provide a platform for internet sales of items located within the facility.  The database may maintain inventory and location data for every item in a commercial facility, or across multiple facilities.  E-commerce\ncustomers seeking to purchase items may interact with the database, which may search for in-stock items and provide location data to customers.\n A theft tracking system may allow a facility to track every item in the facility in real time.  For example, the facility may be constantly scanned by several apparatuses placed to provide full coverage of the facility.  As items are moved\nthroughout the facility, an apparatus may detect the motion and track the item's location.  The apparatus may report this to a theft tracking system on the database, which coordinates the other apparatuses to track the item as well.  At an appropriate\npoint, loss prevention teams may be alerted to investigate items that are moving suspiciously.\n Database systems may be accessed using a graphical interface through a software application or a website.  The interface may work with a virtual model of the shelves and aisles in facility, such as a \"master-shelf\" model which maintains a\nvirtual representation of all of the inventory information communicated to the database.  The virtual model may not be a visual representation, but may be primarily a numerical representation of the inventory.  The virtual model may be updated each time\nthe apparatus communicates new information to the database.\n FIG. 14 is a flow chart 1400 showing a method of inventorying at least partially non-visible items, in accordance with a third embodiment of the disclosure.  The method may be performed with the apparatus of FIG. 1 and any of the additional\nexemplary embodiments or it may be performed using other suitable hardware and software components.\n Step 1410 includes generating a detection signal.  The detection signal has an initial frequency substantially between 30 GHz and 300 GHz and is a millimeter wave signal.\n Step 1420 includes transmitting the detection signal to a plurality of at least partially non-visible inventory items located within an inventory area, wherein the detection signal is reflected off of the plurality of at least partially\nnon-visible inventory items.  The detection signal is transmitted using at least one suitable antenna in communication with the signal generator.\n Non-visible inventory items may be any inventory items described above which are visually obscured or difficult to see.  For instance, non-visible inventory items may be products located in product packaging, shipping packaging, delivery\nvehicles, and the like.  Such inventory items may not be visible to the human eye, as packaging materials generally obscure them.  However, millimeter waves may be able to propagate through packing materials and reflect off of the inventory items. \nNon-visible inventory items may additionally be items obscured by their placement, for example, items located on high or dark shelving.  Millimeter waves may propagate through shelving.  Non-visible inventory items may additionally be items commonly\ngrouped closely together, such as screws and other hardware.  It may be difficult for humans to visually count such items, but millimeter waves may be able to resolve them.  At least partially non-visible inventory items are those items that are\npartially or completely obscured for any of the above reasons.\n An inventory area may be any area, whether within a facility or other location, in which non-visible inventory items are located.  For example, this may be a shipping box, pallet, shelf, aisle, or display.  This may also be a shopping cart,\nshopping bag, storage container, and the like.\n Step 1430 includes receiving the reflected signal from the plurality of at least partially non-visible inventory items.  The reflected signal may be received with any number of suitable antennas.\n Step 1440 includes comparing, with a processor in communication with the at least one antenna, the reflected signal with the detection signal to determine a difference between the reflected and detection signals.\n Step 1450 includes determining, with the processor, at least one of: a location, a count, an orientation, and a type of the plurality of at least partially non-visible inventory items.\n Step 1460 includes calculating, with the processor, a confidence level for the determination.\n Step 1470 includes communicating at least a portion of the determination and the confidence level to a database.  A transmitter in communication with the processor may transmit the determination and the confidence level.\n FIG. 15 is a flow chart 1500 showing the central data hub 60 in communication with hardware devices 1510, 1520, 1530, 1540, 1550, 1560 of the system for performing inventory management within a commercial facility.  Hardware device 1510 may be a\ncomputer system or terminal at a retailer's warehouse.  Exemplary information exchanged between the device 1510 and the hub 60 may include data collection to the hub 60 and refill/order information to the device 1510.  Hardware device 1520 may be a\ncomputer system or terminal at a retailer store.  Information from the hub 60 to the store may include online orders, supplier delivery information, transit information, and the like, while information from the device 1520 may include sales data,\ninventory information, and refill orders.  Device 1530 may be a computer system or terminal for customer executives.  Information between the hub 60 and the device 1530 may include sales predictions from the hub 60 and administrative plans from the\ncustomer executive device 1530.  Device 1540 may be a computer system or terminal at a retail supplier's location.  Information exchanged between the hub 60 and the device 1540 may include refill demand information from the hub and delivery information\nfrom the retailer supplier device 1540.  Device 1550 may be a computer system or terminal for other third party agencies, such as strategic partners, social media groups, and the like.  Information exchanged between the hub 60 and the device 1550 may\ninclude targeting information from the hub 60 and weather and special event information from the third party agency device 1550.  Device 1560 may be a computer system or terminal for retail or individual customers.  Information exchanged between the hub\n60 and the device 1560 may include product and retail location recommendations from the hub 60 and transaction and order data from the customer device 1560.  The devices may communicate with one another through the hub 60, either directly or indirectly. \nOne device may send a request for information that may be passed to one or more other devices connected to the hub 60.  The system of devices and the hub may work together to provide all of the respective users and producers with holistic information\nabout the product supply and sales chains.\n The terminals or devices 1510-1560 act as interfaces between the various users and the hub 60, presenting insights obtained by analyzing the tremendous amounts of data collected across the network and allowing for the collection of important\nuser input and relaying of user actions.  For customer executives and middle management, the device-hub connection presents the overall business insights and allows for the sharing of internal discussions.  For district managers, the device-hub\nconnection shares the key performances indicator information, including district performance with store data, and connects district managers to regional and store managers in nearby areas.  For store managers, the device-hub connection displays key\nperformance indicators for the store, real-time inventory information, options for ordering/stock refill, and provides connections between nearby managers and superiors.  For store associates, the device-hub connection may provide goals for the day,\nweek, or month, tasks to perform, tasks accomplished, real-time inventory data, and tasking/messaging within the store.  For affiliated vendors and third parties, the device-hub connection may provide demand and supply information, historical data\nanalysis, product stocking information for a particular location, supply ordering, and supplier information and communication.\n In one example, the central data hub 60 may be a central data hub that serves as an information processing center.  The information from all input sources over the retail network may be fed through the cloud, categorized, analyzed, processed in\nother ways, and redistributed back to the appropriate entities within the network.  The processing and analysis may be primarily performed by automated computer algorithms, discussed in greater detail below, with occasional manual adjustment where\nneeded.  The central data hub 60 may also aggregate and learn from external data sources, such as social media, publicly-available data, and the like.  In one example, the central data hub 60 may surface data insights that it finds are probabilistically\nmost actionable to users.  It may also distribute and update the more intelligent components of its network with the additional training it has gathered from the stream of aggregated information across the network, such that they can be taken advantage\nof locally by those devices.\n The data hub 60 may collect, process, and communicate information using social media.  Social media may allow the system to provide targeted marketing to users or customer groups.  For instance, a user can choose to receive marketing information\nin real-time or on a fixed schedule, and can set a preference on marketing content, such as discounts or offers on products liked on Facebook.RTM.  and the like.  Social media may allow marketing based on a user's location, particularly when a customer\nis marked as checked into a competitor's store and can be offered a discount.  Marketing may be event based, for instance, if a user posts about an upcoming vacation, birthday, anniversary, loss of family member, and the like.  The data hub 60 may also\nanalyze a user's profile to determine the user's economic condition (employed, wealthy), personal condition (married, social) and other details.  The data hub 60 may also be able to use social media to foresee product demand or supply issues due to\nevents such as holidays, celebrations, natural disasters, and the like.  The hub 60 can direct more or fewer supplies to areas with higher need or diminished capacity, or send alerts to drivers, inventory people, and managers in those areas.\n The graphical and user interface elements of the central data hub 60 are discussed in further detail below.\n FIG. 16 is an illustration of the product manager dashboard 1600 interface of the system.  The product manager dashboard 1600 may be a portion of the interface available for use by customer executives and product managers using a device 1530\nconnected to the hub 60 shown in FIG. 15.  The dashboard 1600 may contain informational boxes corresponding to high level product information across a number of retail or other environments.  Text boxes 1610 may contain textual and numeric data, for\nexample, the number of empty spots on shelves, the value of those empty spots, the number of empty spots where inventory can be restocked from store stock, and the number of price discrepancies, among others.  Graphical boxes 1620 may present information\nin a visual or graphical format, such as charts corresponding to the ratio of empty spots on shelves for stores having inventory vs.  not having inventory.\n The information offered in the dashboard 1600 may help executives and product managers understand their products in a number of areas.  For instance, they may allow managers to understand the percentage of inventory in the store but not on the\nshelf vs.  inventory not in the store; understand the dollar value associated with empty spots on the shelves; understand frequently searched for or requested products in a store; understand the number of price discrepancies happening in the store or\ndepartment; and provide feedback that can be passed to departments with frequent duplicate scans or error messages.\n FIGS. 17A-17B are illustrations of the visual location mapping interface of the system.  FIG. 17A shows a data capturing page 1700 that provides a graphical representation 1710 of data capture attempts by the robots, devices, or systems\ndiscussed in FIGS. 1-10, above.  Each graphic 1710 may indicate a different capture result.  For instance, one graphic may indicate portions of the store that were captured successfully, one graphic 1710 may indicate portions of the store that were not\ncaptured successfully, and one may indicate portions of the store where no attempts to capture were made.  Additional menus 1720 may allow a user to refine the results by department, product, aisle, store, and the like.  Using this dashboard, a product\nmanager can monitor the data capturing status across one or more stores and filter information as needed.\n FIG. 17B shows a product location page 1701 that provides a map of a store 1711 and a visual representation of a product's location within the store.  Additional menus may allow a user to search for particular items, groups of items, price\npoints, and the like.  Searches may be performed by item names, SKUs, ID numbers, departments, and the like.\n FIG. 18 is an illustration of the store manager dashboard interface 1800 of the system.  The store manager dashboard 1800 may provide relevant data and communication capabilities for store managers and product managers/executives managing\ncertain products.  The interface may include text information boxes 1810 containing textual or numeric data.  For instance, the text information boxes may include data on the number of new discrepancies in data capture, SKU values of all empty spots, and\nmissing inventory data.  Graphical information boxes 1820 may provide graphical information about the products, for instance, charts showing the percentage of empty shelves that are known vs.  unknown.  The data may be displayed across regions,\nparticular stores, particular store brands, or some combination.  Store managers may be granted restricted access to some information, for instance, only being allowed to see their own store.  Store managers may additionally be able to inquire further\nabout categories of information displayed and which store associates populated the data.\n FIG. 19 is an illustration of the empty shelves interface 1900 of the system.  The empty shelves interface 1900 may provide the store or product manager with historical data related to a product's empty shelves.  The user may be allowed to\nselect a range of dates to view data.  Graphical data such as charts and graphs may show the number of times and locations where empty shelves where recorded during data capture.  Textual and numeric data may indicate the number of new discrepancies, the\ntotal of missing inventory, and the SKU value of the empty spots.\n FIG. 20 is an illustration of the associate performance interface 2000 of the system.  The associate performance interface 2000 may provide graphical, text, and numeric data to store and product managers showing the actions of their store\nassociates in documenting inventory data throughout a date range.  The range may be selected by the user.  Graphical, text, and numeric information may include a number of new discrepancies that were unknown to the store, the number of missing inventory\nitems, the SKU value of empty spots, the number of discrepancies resolved by the associate, and the type of action taken by the associate when a discrepancy was reported.  Graphs may show trends or analysis of multiple associates, allowing for\nproductivity and work to be compared across a date range.\n FIG. 21 is an illustration of the customer home screen interface 2100 of the system.  The customer home screen interface 2100 may allow a store associate to view and prepare online or in-store orders from customers.  The store associate may\nreceive an order notification showing the items ordered.  An order box 2110 may become available to the associate, showing details such as the item number, item location, and availability, either in-store or in other stores.  The associate may also click\non or interact with availability boxes 2120, which may open additional menus and lists.  For instance, an availability box showing the items available in store may allow the associate to find all of those items.  An availability box showing a list of the\nitems not available in the store may allow the associate to backorder those items.  And an availability box indicating that the order has been fulfilled may allow the associate to communicate to the customer that their order is ready for pickup.\n FIG. 22 is an illustration of the list view interface 2200 of the system.  The list view interface 2200 may show the contents of the availability boxes discussed in FIG. 21 in list form.  For example, a not available list 2210 may show all of\nthe items not available in the store, but may indicate where the items are otherwise available.  An available list may indicate which items are available in the store for fulfillment.  The lists may be organized for ease of use.  For instance, the\navailable list may be organized by the quickest route for fulfillment so that the associate can pick up the items in the optimal amount of time.  Both lists may include buttons for notifying the customer that the items are or are not available.\n FIG. 23 is an illustration of the item screen interface 2300 of the system.  The associate may interact with items on the lists in FIG. 22 to bring up the item screen interface 2300.  The interface 2300 may bring up boxes 2310, 2320 including\ninformation about the product and action items to accomplish.  In the product box 2310, a picture of the product, along with pictures of the shelf, may be displayed to allow the associate to easily find the correct product.  In the action box 2320, the\nassociate may be able to mark whether the item was picked or unavailable, and may decide to notify the customer.  The associate may use both boxes as he or she goes through fulfilling an order in order to keep track of which items have been obtained.\n It should be emphasized that the above-described embodiments of the present disclosure, particularly, any \"preferred\" embodiments, are merely possible examples of implementations, merely set forth for a clear understanding of the principles of\nthe disclosure.  Many variations and modifications may be made to the above-described embodiments of the disclosure without departing substantially from the spirit and principles of the disclosure.\n All such modifications and variations are intended to be included herein within the scope of the present disclosure and protected by the following claims.", "application_number": "16138758", "abstract": " An intelligent system for performing inventory management within a\n     facility includes at least one imaging sensor, a transmitter for sending\n     inventory information to a database, a receiver for receiving information\n     from a database, and a computer in communication with the locomotion\n     platform, at least one imaging sensor, the transmitter, and the receiver.\n     The computer is configured to capture inventory images from the at least\n     one imaging sensor, detect inventory by comparing captured inventory\n     images with stored inventory images, determine inventory information,\n     determine a confidence level for the inventory information, and\n     communicate at least a portion of the inventory information to the\n     database. In one embodiment, the system includes a robot having a\n     locomotion platform, the at least one imaging sensor, the transmitter,\n     the receiver, and the computer.\n", "citations": ["4354252", "4638445", "4890241", "4939728", "5293639", "6292713", "6347261", "7461156", "7609686", "7693757", "7702739", "7801959", "7827459", "8046744", "8050684", "8407306", "8594845", "8619799", "8700722", "8965762", "9191619", "9205886", "9767432", "20020071427", "20020160805", "20020165790", "20040164696", "20060047665", "20060071929", "20060105792", "20060106471", "20060178559", "20060190526", "20060258287", "20070069014", "20070100951", "20070123307", "20070130255", "20070156817", "20070162582", "20070299918", "20080039974", "20080077511", "20080080370", "20080109519", "20080140789", "20080244040", "20080305775", "20090047929", "20090055019", "20090149992", "20090163244", "20090209250", "20090281880", "20100070588", "20100094461", "20100094985", "20100131103", "20100150171", "20100238262", "20100245532", "20100314226", "20110071676", "20110087571", "20110125856", "20110158476", "20110173621", "20110213657", "20110225578", "20110231050", "20110238211", "20110288684", "20110307403", "20120033605", "20120042028", "20120069131", "20120122425", "20120185547", "20120239196", "20120284397", "20120303774", "20120311046", "20120315879", "20130007299", "20130047034", "20130050743", "20130111572", "20130120547", "20130151007", "20130173727", "20130179514", "20130212203", "20130231779", "20140009561", "20140009612", "20140095216", "20140184423", "20140270115", "20140304238", "20140344118", "20150052029", "20150246654", "20160114488", "20160132707", "20160247116", "20160325934", "20170337508", "20180164213", "20180370628"], "related": ["15369812", "14921899", "62068474", "62622000", "62561588"]}, {"id": "20190097865", "patent_code": "10374863", "patent_name": "Apparatus, systems and methods for event recognition based on a wireless\n     signal", "year": "2019", "inventor_and_country_data": " Inventors: \nXu; Qinyi (College Park, MD), Zhang; Feng (Greenbelt, MD), Chen; Chen (Burlingame, CA), Wang; Beibei (Clarksville, MD), Wu; Chenshu (Greenbelt, MD), Zhang; Hangfang (Greenbelt, MD), Wong; Chau-Wai (Raleigh, NC), Claffey; David N. (Somerville, MA), Chen; Chun-I (Brookeville, MD), Lai; Hung-Quoc Duc (Parkville, MD), Wu; Zhung-Han (College Park, MD), Wu; Min (Clarksville, MD), Han; Yi (Ellicott City, MD), Au; Oscar Chi-Lim (San Jose, CA), Liu; K. J. Ray (Potomac, MD)  ", "description": "TECHNICAL FIELD\n The present teaching generally relates to event recognition and monitoring.  More specifically, the present teaching relates to recognizing and classifying events in a venue based on a wireless signal.\nBACKGROUND\n Security systems are becoming increasingly prevalent for both public buildings and private dwellings.  A conventional security system typically uses contact sensors that are monitored and/or controlled by a central panel that is usually mounted\nin the house.  Various sensors can be installed at windows, doors, and other locations to detect intrusion, e.g. one sensor per door or window.  A typical house (e.g. with a size of 1500 sqft) may need at least 6-8 sensors.  This induces a considerable\nhardware cost.  The installation of the conventional security system (especially the wiring) is tedious, time consuming and expensive.  A conventional security system is very difficult, if not impossible, to upgrade, which is needed over time as\ntechnology develops.  Maintenance and repair of a conventional security system are also tedious, expensive, and close to impossible.\n In addition, existing security systems based on object motion detection cannot provide enough accuracy and often lead to false alarms.  Existing approaches include those based on passive infrared (PIR), active infrared (AIR) and Ultrasonic.  PIR\nsensors are the most widely used motion sensor in home security systems, and can detect human motions by sensing the difference between background heat and the heat emitted by moving people.  However, systems based on PIR sensors are prone to false\nalarms due to its sensitivity to environmental changes, like hot/cold air flow and sunlight.  They are easily defeated by blocking the body heat emission by e.g. wearing a heat-insulating full-body suit.  Their range is limited and need a line-of-sight\n(LOS) condition; thus multiple devices are needed.  In AIR based systems, an IR emitter sends a beam of IR which will be received by an IR receiver.  When the beam is interrupted, a motion is detected.  This solution can be easily seen using a regular\ncamera or any IR detection mechanism and also has a limited range and thus needs LOS.  Ultrasonic sensors detect human motion by sending out ultrasonic sound waves into a space and measuring the speed at which they return, and motion can be detected if\nthere is a frequency change.  But this approach can be defeated by wearing an anechoic suit.  Also, ultrasound cannot penetrate solid objects such as furniture or boxes, which causes gaps in detection field.  Slow movements by a burglar may not trigger\nan alarm in this case.\n Therefore, there is a need for apparatus and methods for recognizing events (e.g. events related to object motion and/or security) to solve the above-mentioned problems and to avoid the above-mentioned drawbacks.\nSUMMARY\n The present teaching generally relates to periodic motion (e.g. human breathing) detection.  More specifically, the present teaching relates to periodic motion detection and monitoring based on time-reversal technology in a rich-scattering\nwireless environment, such as an indoor environment or urban metropolitan area, enclosed environment, underground environment, open-air venue with barriers such as parking lot, storage, yard, square, forest, cavern, valley, etc.\n In one embodiment, a method implemented on a machine having a processor, a memory communicatively coupled with the processor and a set of instructions stored in the memory for recognizing an event is disclosed.  The method comprises: for each of\nat least one known event happening in a venue in a respective training time period: transmitting, by an antenna of a first transmitter, a respective training wireless signal to at least one first receiver through a wireless multipath channel impacted by\nthe known event in the venue in the training time period associated with the known event, obtaining, asynchronously by each of the at least one first receiver based on the training wireless signal, at least one time series of training channel information\n(training CI time series) of the wireless multipath channel between the first receiver and the first transmitter in the training time period associated with the known event, and pre-processing the at least one training CI time series; training at least\none classifier for the at least one known event based on the at least one training CI time series; and for a current event happening in the venue in a current time period, transmitting, by an antenna of a second transmitter, a current wireless signal to\nat least one second receiver through the wireless multipath channel impacted by the current event in the venue in the current time period associated with the current event, obtaining, asynchronously by each of the at least one second receiver based on\nthe current wireless signal, at least one time series of current channel information (current CI time series) of the wireless multipath channel between the second receiver and the second transmitter in the current time period associated with the current\nevent, pre-processing the at least one current CI time series, and applying the at least one classifier to: classify at least one of: the at least one current CI time series, a portion of a particular current CI time series, and a combination of the\nportion of the particular current CI time series and a portion of an additional CI time series, and associate the current event with at least one of: a known event, an unknown event and another event.  A training CI time series associated with a first\nreceiver and a current CI time series associated with a second receiver have at least one of: different starting times, different time durations, different stopping times, different counts of items in their respective time series, different sampling\nfrequencies, different sampling periods between two consecutive items in their respective time series, and channel information (CI) with different features.\n In another embodiment, a system having a processor, a memory communicatively coupled with the processor and a set of instructions stored in the memory for recognizing an event is disclosed.  The system comprises a first transmitter and a second\ntransmitter in a venue, at least one first receiver and at least one second receiver in the venue, and an event recognition engine.  The first transmitter is configured for: for each of at least one known event happening in the venue in a respective\ntraining time period, transmitting a respective training wireless signal through a wireless multipath channel impacted by the known event in the venue in the training time period associated with the known event.  Each of the at least one first receiver\nis configured for: for each of the at least one known event happening in the venue, receiving asynchronously the respective training wireless signal through the wireless multipath channel, obtaining, asynchronously based on the respective training\nwireless signal, at least one time series of training channel information (training CI time series) of the wireless multipath channel between the first receiver and the first transmitter in the training time period associated with the known event, and\npre-processing the at least one training CI time series.  The second transmitter is configured for: for a current event happening in the venue in a current time period, transmitting a current wireless signal through the wireless multipath channel\nimpacted by the current event in the venue in the current time period associated with the current event.  Each of the at least one second receiver is configured for: for the current event happening in the venue in the current time period, receiving\nasynchronously the current wireless signal through the wireless multipath channel, obtaining, asynchronously based on the current wireless signal, at least one time series of current channel information (current CI time series) of the wireless multipath\nchannel between the second receiver and the second transmitter in the current time period associated with the current event, and pre-processing the at least one current CI time series.  The event recognition engine is configured for: training at least\none classifier for the at least one known event based on the at least one training CI time series; and applying the at least one classifier to: classify at least one of: the at least one current CI time series, a portion of a particular current CI time\nseries, and a combination of the portion of the particular current CI time series and a portion of an additional CI time series, and associate the current event with at least one of: a known event, an unknown event and another event.  A training CI time\nseries associated with a first receiver and a current CI time series associated with a second receiver have at least one of: different starting times, different time durations, different stopping times, different counts of items in their respective time\nseries, different sampling frequencies, different sampling periods between two consecutive items in their respective time series, and channel information (CI) with different features.\n In yet another embodiment, an event recognition engine of a wireless monitoring system is disclosed.  The system comprises a first transmitter and a second transmitter in a venue, at least one first receiver and at least one second receiver in\nthe venue, and the event recognition engine.  The event recognition engine comprises: a processor, a memory communicatively coupled with the processor, and a set of instructions stored in the memory.  The set of instructions, when executed, causes the\nprocessor to perform: for each of at least one known event happening in a venue, obtaining, from each of at least one first receiver in the venue, at least one time series of training channel information (training CI time series) of a wireless multipath\nchannel, wherein the first receiver extracts the training CI time series from a respective training wireless signal received from a first transmitter in the venue through the wireless multipath channel between the first receiver and the first transmitter\nin a training time period associated with the known event, training at least one classifier for the at least one known event based on the at least one training CI time series, for a current event happening in the venue in a current time period,\nobtaining, from each of at least one second receiver in the venue, at least one time series of current channel information (current CI time series) of the wireless multipath channel impacted by the current event, wherein the second receiver extracts the\ncurrent CI time series from a current wireless signal received from a second transmitter in the venue through the wireless multipath channel between the second receiver and the second transmitter in the current time period associated with the current\nevent, and applying the at least one classifier to: classify at least one of: the at least one current CI time series, a portion of a particular current CI time series, and a combination of the portion of the particular current CI time series and a\nportion of an additional CI time series, and associate the current event with at least one of: a known event, an unknown event and another event.  A training CI time series associated with a first receiver and a current CI time series associated with a\nsecond receiver have at least one of: different starting times, different time durations, different stopping times, different counts of items in their respective time series, different sampling frequencies, different sampling periods between two\nconsecutive items in their respective time series, and channel information (CI) with different features.\n In still another embodiment, a receiver of a wireless monitoring system is disclosed.  The receiver comprises: a wireless circuitry, a processor communicatively coupled with the wireless circuitry, a memory communicatively coupled with the\nprocessor, and a set of instructions stored in the memory.  The wireless circuitry is configured to: for each of at least one known event happening in a venue, receive a respective training wireless signal through a wireless multipath channel, wherein\nthe respective training wireless signal is transmitted by a first transmitter through the wireless multipath channel between the receiver and the first transmitter in a training time period associated with the known event, and for a current event\nhappening in the venue in a current time period, receive a current wireless signal through the wireless multipath channel impacted by the current event, wherein the current wireless signal is transmitted by a second transmitter through the wireless\nmultipath channel between the receiver and the second transmitter in the current time period associated with the current event.  The set of instructions, when executed, causes the processor to: for each of the at least one known event happening in the\nvenue, obtain, asynchronously based on the respective training wireless signal, at least one time series of training channel information (training CI time series) of the wireless multipath channel, and for the current event happening in the venue in the\ncurrent time period, obtain, asynchronously based on the current wireless signal, at least one time series of current channel information (current CI time series) of the wireless multipath channel.  The at least one training CI time series is used by an\nevent recognition engine of the wireless monitoring system to train at least one classifier for the at least one known event.  The at least one classifier is applied to: classify at least one of: the at least one current CI time series, a portion of a\nparticular current CI time series, and a combination of the portion of the particular current CI time series and a portion of an additional CI time series, and associate the current event with at least one of: a known event, an unknown event and another\nevent.\n Other concepts are related to software for implementing the present teaching on recognizing security related events based on wireless channel information in a rich-scattering environment.\n Additional novel features will be set forth in part in the description which follows, and in part will become apparent to those skilled in the art upon examination of the following and the accompanying drawings or may be learned by production or\noperation of the examples.  The novel features of the present teachings may be realized and attained by practice or use of various aspects of the methodologies, instrumentalities and combinations set forth in the detailed examples discussed below.\nBRIEF DESCRIPTION OF DRAWINGS\n The methods, systems, and/or programming described herein are further described in terms of exemplary embodiments.  These exemplary embodiments are described in detail with reference to the drawings.  These embodiments are non-limiting exemplary\nembodiments, in which like reference numerals represent similar structures throughout the several views of the drawings.\n FIG. 1 illustrates an exemplary network environment for event detection and monitoring in a venue, according to one embodiment of the present teaching.\n FIG. 2 illustrates an exemplary diagram of a device in a wireless monitoring system, according to one embodiment of the present teaching.\n FIG. 3 illustrates an exemplary flow of detecting indoor events using time-reversal technology, according to one embodiment of the present teaching.\n FIG. 4 illustrates an exemplary scheme for breathing signal extraction and maximization, according to one embodiment of the present teaching.\n FIG. 5 illustrates an exemplary breathing signal based on real-world measurements, according to one embodiment of the present teaching.\n FIG. 6 demonstrates gains of a disclosed scheme for breathing signal extraction and maximization, according to one embodiment of the present teaching.\n FIG. 7A and FIG. 7B illustrate comparison results between a wake state and a sleep state, according to one embodiment of the present teaching.\n FIG. 8A and FIG. 8B illustrate breathing rate performances of different sleep stages, according to one embodiment of the present teaching.\n FIG. 9 illustrates an exemplary network environment for sleep monitoring, according to one embodiment of the present teaching.\n FIG. 10 illustrates an exemplary algorithm design for sleep monitoring, according to one embodiment of the present teaching.\n FIG. 11 illustrates an exemplary demonstration of a Smart TV use case, according to one embodiment of the present teaching.\n FIG. 12 illustrates an exemplary setting of Smart TV with deployment of devices under the TV, according to one embodiment of the present teaching.\n FIG. 13 illustrates an exemplary experiment setting of Smart TV with area partition in front of the TV, according to one embodiment of the present teaching.\n FIG. 14 illustrates an exemplary setting of Smart TV with deployment of devices inside the TV, according to one embodiment of the present teaching.\n FIG. 15 illustrates an exemplary setting of Smart TV with Origin and Bot installed in a speaker close to the TV, according to one embodiment of the present teaching.\n FIG. 16 illustrates an exemplary setting of Smart TV with Origin and Bot installed on a table close to the TV, according to one embodiment of the present teaching.\n FIG. 17 illustrates an exemplary setting of Smart fan with Origin and Bot installed on the high stand of the fan, according to one embodiment of the present teaching.\n FIG. 18 illustrates an exemplary setting of Smart car with Origin and Bot installed outside of the car, according to one embodiment of the present teaching.\n FIG. 19 illustrates an exemplary interior bird-view of a car for seat occupancy detection and people counting, according to one embodiment of the present teaching.\n FIGS. 20A-20D illustrate changes of channel information according to various seat occupancy situations in a car, according to one embodiment of the present teaching.\nDETAILED DESCRIPTION\n In the following detailed description, numerous specific details are set forth by way of examples in order to provide a thorough understanding of the relevant teachings.  However, it should be apparent to those skilled in the art that the\npresent teachings may be practiced without such details.  In other instances, well known methods, procedures, components, and/or circuitry have been described at a relatively high-level, without detail, in order to avoid unnecessarily obscuring aspects\nof the present teachings.\n The present teaching discloses systems, apparatus, and methods for recognizing events (e.g. events related to object motion and/or security) in a venue based on a time series of channel information (CI) of a wireless multipath channel that is\nimpacted by the object motion.  According to various embodiments, the disclosed system can be integrated into a security system to monitor opening of front door (e.g. by intruders or by dementia patients), opening of window, etc. The disclosed system can\nbe trained to know \"when the door is opened\", \"when the door is closed\", etc.\n In one embodiment, the system uses discrete training to recognize: \"door fully open\", \"door fully close\", and \"door half open\".  In another embodiment, the system is trained for continuous monitoring, and may use dynamic time warping (DTW) to\nmonitor the door movement continuously so that every position of the door (e.g. x % open with x being between 0 and 100) can be detected and recognized.\n In one embodiment, the disclosed system has hardware components (e.g. wireless transmitter/receiver with antenna, analog circuitry, power supply, processor, memory, etc.) and corresponding software components.  According to various embodiments\nof the present teaching, the disclosed system includes a Bot (referred to as a Type 1 device) and an Origin (referred to as a Type 2 device) for transient motion detection and monitoring.  Each device comprises a transceiver, a processor and a memory.\n The disclosed system is easy and simple to install with a low hardware cost.  In one embodiment, the disclosed system only needs two wireless devices (a Type 1 device and a Type 2 device) to cover a 1500-sqft house.  The wireless devices are\nplug-and-play, and can be plugged into wall power sockets in opposite corner of the house.  The disclosed system can be improved over time with machine learning, and can be upgraded via remote software upgrade.  The system can do software diagnosis,\nwhich makes its maintenance and repair an easy job.\n The disclosed system includes features that are significantly more than an abstract idea.  A security system with low cost and high accuracy has been desired for a long time.  The disclosed system solves the long-time problem using physical WiFi\nchips as sensors to monitor the environment with WiFi signals and the associated multipath patterns.  The disclosed system has hardware components (e.g. Type 1 device, Type 2 device each with processor, memory, wireless transceivers, cloud server, etc.). The WiFi signals are electromagnetic (EM) waves in the air which are measurable and with deterministic structure and frequency characteristics.  The system has matching software components (e.g. embedded software in Type 1 device, in Type 2 device, in\nservers, etc.).  The software can interface with low-level WiFi chips and firmware to extract CSI.  Experimental results show that the disclosed system can recognize security related events and motions with very high accuracy.\n The disclosed system can be applied in many cases.  In one example, the Type 1 device (transmitter) may be a small WiFi-enabled device resting on the table.  It may also be a WiFi-enabled television (TV), set-top box (STB), a smart speaker (e.g.\nAmazon echo), a smart refrigerator, a smart microwave oven, a mesh network router, a mesh network satellite, a smart phone, a computer, a tablet, a smart plug, etc. In one example, the Type 2 (receiver) may be a WiFi-enabled device resting on the table. \nIt may also be a WiFi-enabled television (TV), set-top box (STB), a smart speaker (e.g. Amazon echo), a smart refrigerator, a smart microwave oven, a mesh network router, a mesh network satellite, a smart phone, a computer, a tablet, a smart plug, etc.\nThe Type 1 device and Type 2 devices may be in a home security system for an owner of a house to detect any intrusion and any unexpected activity in and around the house.  The Type 1 device and Type 2 devices may be placed in bedrooms, bathrooms,\nbaby-rooms, restrooms to monitor unexpected intrusion or suspicious activities.  The Type 1 device and Type 2 devices may be deployed in an area to detect sudden or unexpected motions indicating any intruder or accident.\n Hardware modules may be constructed to contain the Type 1 transceiver and/or the Type 2 transceiver.  The hardware modules may be sold to/used by variable brands to design, build and sell final commercial products.  Products using the disclosed\nsystem and/or method may be home/office security products, motion monitoring products, WiFi products, mesh products, TV, STB, entertainment system, HiFi, speaker, home appliance, lamps, stoves, oven, microwave oven, table, chair, bed, shelves, tools,\nutensils, torches, vacuum cleaner, smoke detector, sofa, piano, fan, door, window, door/window handle, locks, smoke detectors, car accessories, computing devices, office devices, air conditioner, heater, pipes, connectors, surveillance camera, access\npoint, computing devices, mobile devices, LTE devices, 3G/4G/5G/6G devices, gaming devices, eyeglasses, glass panels, VR goggles, necklace, watch, waist band, belt, wallet, pen, hat, wearables, implantable device, tags, parking tickets, smart phones,\netc.\n In addition, sleep monitoring acts as a critical and challenging task that attracts increasing demands and interests.  The present teaching discloses the model, design, and implementation of SMARS (Sleep Monitoring via Ambient Radio Signals),\nwhich is the first practical Sleep Monitoring system that exploits Ambient Radio Signals to recognize sleep stages and assess sleep quality.  This will enable a future smart home that monitors daily sleep in a ubiquitous, non-invasive and contactless\nmanner, without instrumenting the subject's body or the bed.  Different from previous RF-based approaches, the present teaching devises a statistical model that accounts for all reflecting and scattering multipaths, allowing highly accurate and\ninstantaneous breathing estimation with ever best performance achieved on commodity devices.  On this basis, SMARS then recognizes different sleep stages, including wake, REM, and NREM, which is previously only possible with dedicated hardware.  A\nreal-time system is implemented on commercial WiFi chipsets and deployed in 6 homes with 6 participants, resulting in 32 nights of data in total.  The results demonstrate that SMARS yields a median error of 0.47 bpm and a 95% error of only 2.92 bpm for\nbreathing estimation, and detects breathing robustly even when a person is 10 m away from the link, or behind a wall.  SMARS achieves sleep staging accuracy of 85%, outperforming advanced solutions using contact sensor or radar.  Furthermore, SMARS is\nevaluated upon a recently released dataset that measures 20 patients' overnight sleep, which confirms the performance.  By achieving promising results with merely a single commodity RF link, SMARS will set the stage for a practical in-home sleep\nmonitoring solution.\n Sleep plays a vital role in an individual's health and well-being, both mentally and physically.  It is well recognized that sleep quantity and quality is fundamentally related to health risks like cardiovascular decease, stroke, kidney failure,\ndiabetes, and adverse mental conditions, etc. Unfortunately, in modern society, a number of people suffer from sleep disorders.  As recently reported, 10% of the population suffers from chronic insomnia (which is even higher among elders), and 1/3 of\nAmericans do not get sufficient sleep.  Monitoring sleep emerges as an essential demand to help, manage, diagnose, and treat the growing group of sleep disorders as well as to keep regular tabs on personal health.\n Sleep monitoring, however, is a challenging task that has drawn tremendous efforts for decades.  Generally, it measures sleep time, recognizes different sleep stages, e.g., wake, REM (Rapid Eye Movement) and NREM (Non-REM), and accordingly\nassesses an individual's sleep quality.  Various solutions have been proposed.  The medical gold standard relies on Polysomnography (PSG), which monitors various physiological parameters such as brain activities, respirations, and body movements by a\nnumber of wired sensors attached to the patient.  Albeit accurate and comprehensive, PSG is usually expensive and cumbersome with the invasive sensors that may cause sleep difficulties, limiting itself to clinical usage for confirmed patients.  Other\napproaches including photoplethysmography (PPG) and actigraphy (ACT) require users to wear dedicated sensors during sleep.  Ballistocardiogram (BCG) needs to instrument the mattress with an array of EMFi sensors to measure ballistic force.  Despite of\nthe costs, these approaches provide suitable solutions for those who need special cares but are less-than-ideal for the public.  Recent efforts in mobile computing envision in-home sleep monitoring using smartphones and wearables.  These methods,\nhowever, only provide coarse-grained, less accurate measurements and fail to monitor vital signs like respiratory rate.  In addition, mobiles and wearables are undesirable for especially elders and those with dementia.\n Different from prevailing solutions, the disclosed solution looks forward to a future smart home that monitors daily sleep in a ubiquitous, non-invasive, contactless, and accurate manner, without instrumenting the body or the bed.  One can\nobserve an opportunity towards such a system by two perspectives: 1) Clinical study has shown that physiological activity varies among different sleep stages.  For example, breathing rate becomes irregular and fast since brain oxygen consumption\nincreases during REM sleep, and is more stable and slower during NREM sleep, rendering the feasibility of sleep staging based on breathing monitoring.  2) Recent advances in wireless technology have demonstrated non-contact sensing of body motions in the\nenvironments.  Chest and abdomen motions caused by breathing can alter radio signal propagations and thus modulate the received signals, from which it is then possible to decipher breathing.  One can explore a synergy between the two perspectives,\nresulting in a system to leverage ambient radio signals (e.g., WiFi) to capture a person's breathing and motion during sleep and further monitor the sleep behaviors.\n While early works have investigated the feasibility of RF-based breathing estimation and sleep monitoring, they either rely on specialized hardware like FMCW radar, or only work in controlled environments.  Solutions based on dedicated radios\nare usually expensive and not ubiquitously applicable.  Others using commodity devices typically require the user to lay still on a bed with radios exceedingly close to his/her chest and fail in presence of extraneous motions or in Non-Line-Of-Sight\n(NLOS) scenarios.  In addition, none of them can identify different sleep stages due to their limited accuracy in breathing estimation.  Such limitations prevent them from application for practical in-home sleep monitoring.\n The present teaching disclose the model, design, and implementation of SMARS, the first practical Sleep Monitoring system that exploits commodity Ambient Radio Signals to recognize sleep stages and assess the otherwise elusive sleep quality. \nSMARS works in a non-obtrusive way without any body contact.  All that a user needs to do is to set up one single link between two commodity radios by, e.g., simply placing a receiver if a wireless router is already installed inside the home.  SMARS\nadvances the literature by a novel statistical model that allows highly accurate and instantaneous breathing estimation.  On this basis, SMARS is able to distinguish different sleep stages, which is previously only obtainable by expensive dedicated\nhardware.  Specifically, SMARS excels in three unique aspects to deliver practical sleep monitoring.  First, one can devise a statistical model on motion in Channel State Information (CSI) that leverages all reflection and scattering multipaths indoors. \nExisting works usually assume a geometrical model with a few multipaths and one major path reflected off the human body (e.g., using a 2-ray model developed for outdoor space).  Under real-world indoor environments, however, there could be as many as\nhundreds of multipaths, and signals not only reflect but also scatter off a person's body and other objects in the space.  As a consequence, previous approaches fail in NLOS environments and for minute motions due to the lack of a dominant reflection\npath.  In contrast, the disclosed model investigates the statistical characteristics of motion in CSI without making such unrealistic assumptions, underlying robust detection of arbitrary motions including breathing.\n Second, SMARS achieves accurate respiratory rate estimation instantaneously and robustly.  Most of previous breathing estimation schemes assume constant breathing rate during a relatively large time window to gain sufficient frequency\nresolution, losing fine-grained breathing variations during sleep.  In addition, minute breathing motions can be easily buried in CSI measurement noises, rendering existing philosophies effective only in extraordinary close proximity (typically within\n2-3 m) without any extraneous motions.  To improve time resolution, SMARS exploits the time-domain Auto-Correlation Function (ACF) to estimate breathing rate, which can report real-time breathing rates as frequent as every one second and make it possible\nto capture instantaneous breathing rate changes.  By using ACF, SMARS also circumvents the use of noisy phase and the usually handcrafted CSI denoising procedure.  More importantly, by eliminating the frequency offsets and thus synchronizing breathing\nsignal over different subcarriers, ACF allows us to perform Maximal Ratio Combining (MRC) to combine multiple subcarriers to combat measurement noises and maximize breathing signals in an optimal way.  By doing so, one can push the limit of the breathing\nsignal to noise ratio (SNR) and thus significantly increase the sensing sensitivity for larger coverage as well as weaker breathing.  Specifically, SMARS can reliably detect breathing when a person is 10 m away from the link, or behind a wall, which is\neven better than specialized low-power radars.\n Finally, based on the extracted breathing rates and motion statistics during sleep, one can recognize different sleep stages (including wake, REM and NREM) and comprehensively assess the overall sleep quantity and quality.  Based on in-depth\nunderstanding of the relationship between breathing rates and sleep stages, one can extract distinctive breathing features for classification for sleep staging.  None of existing works using off-the-shelf devices can achieve the same goal of staging\nsleep.\n A real-time system has been implemented on different commercial WiFi chipsets and its performance is evaluated through extensive experiments.  The evaluation includes two parts: 1) One can deploy SMARS in 6 homes with 6 healthy subjects and\ncollect 32 nights of data, 5 out of which have PSG data recorded by commercial devices.  The results show that SMARS achieves great performance, with a median breathing estimation error of 0.47 breath per minute (bpm) and a 95% tile error of 2.92 bmp. \nRegarding sleep staging, SMARS produces a remarkable accuracy of 85.2%, while commercial solutions, e.g., EMFIT based on contact sensors and ResMed using radar, have accuracies of only 69.8% and 83.7% respectively.  2) One can further validate SMARS on a\nrecently released dataset on RF-based respiration monitoring.  The dataset collected 20 patients' overnight sleep for comparative evaluation of four state-of-the-art breathing monitoring systems, using clinically labeled PSG data as ground truths.  All\nthe four systems (based on ZigBee, Sub-RSS radio, UWB radar, and WiFi CSI, respectively) produce significant median errors of about 2-3 bpm and 95% tile errors of around or above 10 bpm.  As comparison, SMARS achieves significant improvements by\ndecreasing the median error to 0.66 bpm and the 95% tile error to 3.79 bpm.  By achieving promising performance, SMARS can deliver clinically meaningful sleep monitoring for daily and regular use in practice and takes an important step towards a future\nsmart home that monitors personal health everyday life.\n In a nutshell, the core contribution here is SMARS, the first system that enables a smart home to stage an inhabitant's sleep using commodity off-the-shelf WiFi devices, by achieving highly accurate and instantaneous breathing estimation in the\nwild.  SMARS also contributes the first statistical model for understanding and capturing motions in CSI, which will renovate various applications in wireless sensing.\n In one embodiment, the present teaching discloses a method, an apparatus, a device, a system, and/or software (method/apparatus/device/system/software) of a wireless monitoring system.  A time series of channel information (CI) of a wireless\nmultipath channel may be obtained using a processor, a memory communicatively coupled with the processor and a set of instructions stored in the memory.  The time series of CI may be extracted from a wireless signal transmitted between a Type 1\nheterogeneous wireless device and a Type 2 heterogeneous wireless device in a venue through the wireless multipath channel.  The wireless multipath channel may be impacted by a motion of an object in the venue.  A characteristics and/or a\nspatial-temporal information of the object and/or of the motion of an object may be monitored based on the time series of CI.  A task may be performed based on the characteristics and/or the spatial-temporal information.  A presentation associated with\nthe task may be generated in a user-interface (UI) on a device of a user.  The time series of CI may be preprocessed.\n The Type 1 device may comprise at least one heterogeneous wireless transmitter.  The Type 2 device may comprise at least one heterogeneous wireless receiver.  The Type 1 device and the Type 2 device may be the same device.  Any device may have a\nprocessor, a memory communicatively coupled with the processor, and a set of instructions stored in the memory to be executed by the processor.\n There may be multiple Type 1 heterogeneous wireless devices interacting with the same Type 2 heterogeneous wireless device, and/or there may be multiple Type 2 heterogeneous wireless devices interacting with the same Type 1 heterogeneous\nwireless device.  The multiple Type 1 devices/Type 2 devices may be synchronized and/or asynchronous, with same/different window width/size and/or time shift.  The multiple Type 1 devices/Type 2 devices may operate independently and/or collaboratively. \nThe multiple Type 1 devices/Type 2 devices may be communicatively coupled to same or different servers (e.g. cloud server, edge server, local server).  Operation of one device may be based on operation, state, internal state, storage, processor, memory\noutput, physical location, computing resources, network of another device.  Difference devices may communicate directly, and/or via another device/server/cloud server.  The devices may be associated with one or more users, with associated settings.  The\nsettings may be chosen once, pre-programmed, and/or changed over time.\n In the case of multiple Type 1 devices interacting with multiple Type 2 devices, any processing (e.g. time domain processing and/or frequency domain processing) may be different for different devices.  The processing may be based on locations,\norientation, direction, roles, user-related characteristics, settings, configurations, available resources, available bandwidth, network connection, hardware, software, processor, co-processor, memory, battery life, available power, antennas, antenna\ntypes, directional/unidirectional characteristics of the antenna, power setting, and/or other parameters/characteristics of the devices.\n The wireless receiver (e.g. Type 2 device) may receive the wireless signal and/or another wireless signal from the wireless transmitter (e.g. Type 1 device).  The wireless receiver may receive another wireless signal from another wireless\ntransmitter (e.g. a second Type 1 device).  The wireless transmitter may transmit the wireless signal and/or another wireless signal to another wireless receiver (e.g. a second Type 2 device).  The wireless transmitter, the wireless receiver, the another\nwireless receiver and/or the another wireless transmitter may be moving with the object and/or another object.  The another object may be tracked.\n The Type 1 device may be capable of wirelessly coupling with at least two Type 2 devices.  The Type 2 device may be capable of wirelessly coupling with at least two Type 1 devices.  The Type 1 device may be caused to switch/establish wireless\ncoupling from the Type 2 device to a second Type 2 heterogeneous wireless device at another location in the venue.  Similarly, the Type 2 device may be caused to switch/establish wireless coupling from the Type 1 device to a second Type 1 heterogeneous\nwireless device at yet another location in the venue.  The switching may be controlled by a server, the processor, the Type 1 device, the Type 2 device, and/or another device.  The radio used before switching may be different from the radio used after\nthe switching.  A second wireless signal may be caused to be transmitted between the Type 1 device and the second Type 2 device (or between the Type 2 device and the second Type 1 device) through the wireless multipath channel.  A second time series of\nCI of the wireless multipath channel extracted from the second wireless signal may be obtained.  The second wireless signal may be the first wireless signal.  The characteristics, the spatial-temporal information and/or another quantity of the motion of\nthe object may be monitored based on the second time series of CI.  The Type 1 device and the Type 2 device may be the same device.\n The wireless signal and/or the another wireless signal may have data embedded.  The wireless receiver, the wireless transmitter, the another wireless receiver and/or the another wireless transmitter may be associated with at least one processor,\nmemory communicatively coupled with respective processor, and/or respective set of instructions stored in the memory which when executed cause the processor to perform any and/or all steps needed to determine the spatial-temporal information, the initial\nspatial-temporal information, the initial time, the direction, the instantaneous location, the instantaneous angle, and/or the speed, of the object.\n The processor, the memory and/or the set of instructions may be associated with the Type 1 heterogeneous wireless transceiver, one of the at least one Type 2 heterogeneous wireless transceiver, the object, a device associated with the object,\nanother device associated with the venue, a cloud server, and/or another server.\n The Type 1 device may transmit the wireless signal (e.g. a series of probe signals) in a broadcasting manner to at least one Type 2 device(s) through the wireless multipath channel in the venue.  The wireless signal is transmitted without the\nType 1 device establishing wireless connection with any of the at least one Type 2 device receiver(s).\n The Type 1 device may transmit to a particular destination media access control (MAC) address common for more than one Type 2 devices.  Each of the more than one Type 2 devices may adjust their MAC address to the particular MAC address.\n The particular destination MAC address (common destination MAC address) may be associated with the venue.  The association may be recorded in an association table of an Association Server.  The venue may be identified by the Type 1 device, a\nType 2 device and/or another device based on the particular destination MAC address, the series of probe signals, and/or the at least one time series of CI extracted from the probe signals.\n For example, a Type 2 device may be moved to a new location in the venue (e.g. from another venue).  And the Type 1 device may be newly set up in the venue such that the Type 1 device and the Type 2 device are not aware of the existence of each\nother, and they may not establish wireless connection at all.  During set up, the Type 1 device may be instructed/guided/caused/controlled to send the series of probe signals to the particular destination MAC address (e.g. using a dummy receiver, using a\nhardware pin setting/connection, using a stored setting, using a local setting, using a remote setting, using a downloaded setting, or using a server).  Upon power up, the Type 2 device may scan for probe signals according to a table of destination MAC\naddresses that may be used at different locations (e.g. different MAC address for different venue such as house, office, enclosure, floor, multi-storey building, store, airport, mall, stadium, hall, station, subway, lot, area, zone, region, district,\ncity, country, continent).  When the Type 2 device detects the probe signals sent to the particular destination MAC address, the Type 2 device can use the table to identify the venue based on the particular destination MAC address.\n A location of a Type 2 device in the venue may be computed based on the particular destination MAC address, the series of probe signals, and/or the at least one time series of CI obtained by the Type 2 device from the probe signals.  The\ncomputing may be performed by the Type 2 device.\n The particular destination MAC address may be changed over time.  It may be changed according to a time table, a rule, a policy, a mode, a condition, a situation and/or a change.  The particular destination MAC address may be selected based on\navailability of the MAC address, a pre-selected list, collision pattern, traffic pattern, data traffic between the Type 1 device and another device, effective bandwidth, random selection, and/or a MAC address switching plan.  The particular destination\nMAC address may be the MAC address of a second wireless device (e.g. a dummy receiver, or a receiver that serves as a dummy receiver).\n The Type 1 device may transmit the probe signals in a channel selected from a set of channels.  At least one CI of the selected channel may be obtained by a respective Type 2 device from the probe signal transmitted in the selected channel.\n The selected channel may be changed over time.  The change may be according to a time table, a rule, a policy, a mode, a condition, a situation, and/or a change.  The selected channel may be selected based on availability of channels, a\npre-selected list, co-channel interference, inter-channel interference, channel traffic patterns, data traffic between the Type 1 device and another device, effective bandwidth associated with channels, a security criterion, random selection, a channel\nswitching plan, a criterion, and/or a consideration.\n The particular destination MAC address and/or an information of the selected channel may be communicated between the Type 1 device and a server through a network.  The particular destination MAC address and/or the information of the selected\nchannel may also be communicated between a Type 2 device and a server through another network.  The Type 2 device may communicate the particular destination MAC address and/or the information of the selected channel to another Type 2 device (e.g. via a\nmesh network, via Bluetooth, via WiFi, etc).  The particular destination MAC address and/or the information of the selected channel may be chosen by a server.  The particular destination MAC address and/or the information of the selected channel may be\nsignaled in an announcement channel by the Type 1 device and/or a server.  Before the particular destination MAC address and/or the information of the selected channel is being communicated, it may be pre-processed.\n Wireless connection between the Type 1 device and another wireless device may be established.  The Type 1 device may send a first wireless signal (e.g. a sounding frame, a probe signal, a request-to-send RTS) to the another wireless device.  The\nanother wireless device may reply by sending a second wireless signal (e.g. a command, or a clear-to-send CTS) to the Type 1 device, triggering the Type 1 device to transmit the wireless signal (e.g. series of probe signals) in the broadcasting manner to\nthe at least one Type 2 device without establishing wireless connection with any of the at least one Type 2 device.  The second wireless signal may be a response or an acknowledge (e.g. ACK) to the first wireless signal.  The second wireless signal may\ncontain a data with information of at least one of: the venue, and the Type 1 device.  The first wireless signal may also be in response to the second wireless signal.\n The another wireless device may be a dummy wireless device with a purpose (e.g. a primary purpose, a secondary purpose) to establish the wireless connection with the Type 1 device, to receive the first wireless signal, and/or to send the second\nwireless signal.  The another wireless device may be physically attached to the Type 1 device.\n In another example, the another wireless device may send a third wireless signal to the Type 1 device triggering the Type 1 device to transmit the wireless signal (e.g. series of probe signals) in the broadcasting manner to the at least one Type\n2 device without establishing wireless connection with any of the at least one Type 2 device.  The Type 1 device may reply to the third wireless signal by transmitting a fourth wireless signal to the another wireless device.\n The another wireless device may not communicate further with the Type 1 device after establishing the connection with the Type 1 device.  The another wireless device may suspend communication with the Type 1 device after establishing the\nconnection.  The communication may be resumed in the future.  The another wireless device may enter an inactive mode, a hibernation mode, a sleep mode, a stand-by mode, a low-power mode, an OFF mode and/or a power-down mode, after establishing the\nwireless connection with the Type 1 device.\n The another wireless device may have the particular destination MAC address.  It may use the wireless connection to trigger the Type 1 device to send the at least one series of probe signals in a broadcasting manner to the particular destination\nMAC address.  Each of the at least one Type 2 device may set its MAC address to the particular destination MAC address so as to receive the at least one series of probe signals from the Type 1 device.  A new Type 2 device in the venue may obtain the\nparticular destination MAC address from a designated source (e.g. a cloud server, an edge server, a remote server, a local server, a web server, an internet server, a webpage, a database) and set its MAC address to the particular destination MAC address\nso as to receive the at least one series of probe signals from the Type 1 device\n Both the Type 1 device and the another wireless device may be controlled and/or coordinated by at least one of: a first processor associated with the Type 1 device, a second processor associated with the another wireless device, a third\nprocessor associated with the designated source and/or a fourth processor associated with another device.  The first processor and the second processor may coordinate with each other.\n A first series of probe signals may be transmitted by a first antenna of the Type 1 device to at least one first Type 2 device through a first wireless multipath channel in a first venue.  A second series of probe signals may be transmitted by a\nsecond antenna of the Type 1 device to at least one second Type 2 device through a second wireless multipath channel in a second venue.  The first series and the second series may/may not be different.  The at least one first Type 2 device may/may not be\ndifferent from the at least one second Type 2 device.  The first series and/or the second series of probe signals may be transmitted in a broadcasting manner.  The first antenna may/may not be different from the second antenna.\n The Type 1 device may send the two separate series of probe signals to two separate groups of Type 2 devices located in two venues.  The first set of Type 2 devices (the at least one first Type 2 device) may receive the first time series of\nprobe signals through a first wireless multipath channel of the first venue.  The second set of Type 2 devices (the at least one second Type 2 device) may receive the second time series of probe signals through a second wireless multipath channel in the\nsecond venue.  If in broadcasting mode, Type 1 device may not establish wireless connection with any of the at least one first and second Type 2 devices.\n The two venues may have different sizes, shape, multipath characteristics.  The first venue and the second venue may overlap partially.  The immediate area around the first antenna and second antenna may be part of the overlapped area.  The\nfirst venue may be a subset of the second venue, or vice versa.  The first wireless multipath channel and the second wireless multipath channel may be different.  For example, the first one may be WiFi while the second may be LTE.  Or, both may be WiFi,\nbut the first one may be 2.4 GHz WiFi and the second may be 5 GHz WiFi.  Or, both may be 2.4 GHz WiFi, but may have different channel numbers, with different SSID names, and WiFi settings (e.g. encryption--the first may use PKA and the second may use\nPKA2).\n Each first Type 2 device may obtain at least one first time series of CI from the first series of probe signals, the CI being of the first wireless multipath channel between the first Type 2 device and the Type 1 device.  Each second Type 2\ndevice may obtain at least one second time series of CI from the second series of probe signals, the CI being of the second wireless multipath channel between the second Type 2 device and the Type 1 device.\n The first antenna may/may not be the second antenna.  The at least one first Type 2 device may/may not be the same as the at least one second Type 2 device.  A particular first Type 2 device may be the same as a particular second Type 2 device. \nThe first and second series of probe signals may/may not be synchronous.  The transmission of the first series of probe signals may not be synchronous to the transmission of the second series of probe signals.  A probe signal may be transmitted with\ndata.  A probe signal may be replaced by a data signal.\n The first series of probe signals may be transmitted at a first pseudo-regular interval (e.g. at a rate of 30 Hz).  The second series of probe signals may be transmitted at a second pseudo-regular interval (e.g. at a rate of 300 Hz).  The first\npseudo-regular interval may/may not be different from the second pseudo-regular interval.  The first and/or second pseudo-regular interval may be changed over time.  The change may be according to at least one of: a time table, a rule, a policy, a mode,\na condition, a situation, and/or a change.  Any pseudo-regular interval may be changed over time.\n The first series of probe signals may be transmitted to a first destination MAC address.  The second series of probe signals may be transmitted to a second destination MAC address.  The two destination MAC addresses may/may not be different. \nAny of the two destination MAC addresses may be changed over time.  The change may be according to at least one of: a time table, a rule, a policy, a mode, a condition, a situation, and/or a change.\n The first series of probe signals may be transmitted in a first channel.  The second series of probe signals may be transmitted in a second channel.  The first channel and the second channel may/may not be different.  The first channel and/or\nthe second channel may be changed over time.  The change may be according to at least one of: a time table, a rule, a policy, a mode, a condition, a situation, and/or a change.\n A first wireless connection may be established between the Type 1 device and another wireless device.  The Type 1 device may establish the first wireless connection with the another wireless device.  The Type 1 device may send a first wireless\nsignal to the another wireless device.  The another wireless device may send a second wireless signal to the Type 1 device.  One of the first wireless signal and the second wireless signal may be in response to the other.  The second wireless signal to\nthe Type 1 device may trigger the Type 1 device to transmit the first series of probe signals in the broadcasting manner.  The second wireless signal may be an acknowledge to the first wireless signal.\n A second wireless connection may be established between the Type 1 device and yet another wireless device.  The Type 1 device may establish the second wireless connection with the yet another wireless device.  The Type 1 device may send a third\nwireless signal to the yet another wireless device.  The yet another wireless device may send a fourth wireless signal to the Type 1 device.  One of the third wireless signal and the fourth wireless signal may be in response to the other.  The fourth\nwireless signal to the Type 1 device may trigger the Type 1 device to transmit the second series of probe signals in the broadcasting manner.  The fourth wireless signal may be an acknowledge to the third wireless signal.  The third wireless signal may\nbe similar to the first wireless signal.  The fourth wireless signal may be similar to the second wireless signal.  The yet another wireless device may be the another wireless device.  The third wireless signal may be sent by the Type 1 device before the\nfirst wireless communication is fully established and/or completed.\n The another wireless device and the yet another wireless device may be the same device, which may perform signal handshake (e.g. the handshake with the first wireless signal and second wireless signal, or the handshake with the third signal and\nthe fourth signal) with all Type 1 devices sequentially to trigger them to transmit in the broadcasting manner.  The signal handshake with the Type 1 devices may be performed in at least one of: a sequential manner, a parallel manner and a mixed manner\n(partially sequential, partially parallel).\n The Type 1 device, the another wireless device and/or the yet another wireless device may be controlled and/or coordinated, physically attached, or may be of/in/of a common device.  At least two of the Type 1 device, the another wireless device\nand/or the yet another wireless device may be controlled by/connected to a common data processor, or may be connected to a common bus interconnect/network/LAN/Bluetooth network/BLE network/wired network/wireless network/mesh network/mobile network/cloud,\nor may share a common memory, or may be associated with a common user/user profile/user account/identity (ID)/household/house/physical address/location/geographic coordinate/IP subnet/SSID/user device/home device/office device/manufacturing device.\n The Type 1 device, the another wireless device and/or the yet another wireless device may each be associated a processor, a memory communicatively coupled with the processor, and a set of instruction stored in the memory to be executed by the\nprocessor.  The Type 1 device, the another wireless device and/or the yet another wireless device may each comprise a processor, a memory communicatively coupled with the processor, and a set of instruction stored in the memory to be executed by the\nprocessor.  When executed, the set of instructions may cause the device to perform an operation.  Their processors may be coordinated.\n At least one series of probe signals may be transmitted asynchronously and contemporaneously by each of more than one Type 1 heterogeneous wireless devices.  Each respective series of probe signals is transmitted by an antenna of respective Type\n1 heterogeneous wireless device in a broadcasting manner to a respective set of Type 2 devices through a wireless multipath channel, without the respective Type 1 device establishing wireless connection with any of the respective set of Type 2 receivers.\n The transmitting may be using a respective processor, a respective memory and a respective set of instructions.  Each of the more than one Type 1 heterogeneous wireless devices may have a heterogeneous integrated circuit (IC).  Each of the\nrespective set of Type 2 device may have a respective heterogeneous IC.\n Each Type 1 device may be the signal source of a set of respective Type 2 devices (i.e. the Type 1 device sends a respective wireless signal (series of probe signals) to the set of respective Type 2 devices.  Each respective Type 2 device\nchooses the Type 1 device from among all Type 1 devices as its signal source.  Each Type 2 device may choose asynchronously (at different time).  At least one time series of CI may be obtained by each respective Type 2 device from the respective series\nof probe signals from the Type 1 device, the CI being of the wireless multipath channel between the Type 2 device and the Type 1 device.\n The respective Type 2 device chooses the Type 1 device from among all Type 1 devices as its signal source based on at least one of: identity (ID) of the Type 1 device and/or the respective Type 2 receiver, information of at least one of: a user,\nan account, access info, a parameter, a characteristics, and a signal strength associated with the Type 1 device and/or the respective Type 2 receiver, a task to be performed by the Type 1 device and/or the respective Type 2 receiver, whether the Type 1\ndevice is the same as a past signal source, a history (of the past signal source, the Type 1 device, another Type 1 device, the respective Type 2 receiver, and another Type 2 receiver), and/or at least one threshold for switching signal source.\n Initially, the Type 1 device may be the signal source of a set of initial respective Type 2 devices (i.e. the Type 1 device sends a respective wireless signal (series of probe signals) to the set of initial respective Type 2 devices) at an\ninitial time.  Each initial respective Type 2 device chooses the Type 1 device from among all Type 1 devices as its signal source.\n The signal source (Type 1 device) of a particular Type 2 device may be changed when at least one of the following occurs: (1) a time interval between two adjacent probe signals (e.g. between the current probe signal and an immediate past probe\nsignal, or between the next probe signal and the current probe signal) received from the current signal source of the particular Type 2 device exceeds a first threshold, (2) a signal strength associated with the current signal source of the Type 2 device\nis below a second threshold at the respective current time, (3) a processed signal strength associated with the current signal source of the Type 2 device is below a second threshold at the respective current time after the signal strength is processed\nwith at least one of: a low pass filter, a band pass filter, a median filter, a moving average filter, a weighted averaging filter, a linear filter and a non-linear filter, and/or (4) the signal strength (or processed signal strength) associated with the\ncurrent signal source of the Type 2 device is below a fourth threshold for a significant percentage of a recent time window (e.g. 70%, 80%, 90%, etc.).  The percentage may exceed a fifth threshold.  Any of first threshold, second threshold, third\nthreshold, fourth threshold and fifth threshold may be time varying.\n Condition (1) may occur when the Type 1 device and the Type 2 device become progressively far away from each other, such that some probe signal from the Type 1 device becomes too weak and is not received by the Type 2 device.  Conditions (2)-(4)\nmay occur when the two devices become far from each other such that the signal strength becomes very weak.\n The signal source of the particular Type 2 device may not change if the other Type 1 devices have signal strength weaker than the current signal source (current selected Type 1 device).  In another example, the signal source of the particular\nType 2 device may also not change if the other Type 1 devices have signal strength weaker than a factor (e.g. 1.1, 1.2, or 1.5, etc.) of signal strength of the current signal source.\n If the signal source is changed, the new signal source may take effect at a near future time (e.g. the respective next time).  The new signal source may be the Type 1 device with strongest signal strength.  The new signal source may also be the\nType 1 device with strongest processed signal strength, wherein the signal strength of each Type 1 device may be processed with at least one of: a low pass filter, a band pass filter, a median filter, a moving average filter, a weighted averaging filter,\na linear filter and a non-linear filter.\n A list of available Type 1 devices may be initialized.  The list may be updated by examining signal strength associated with the respective set of Type 1 devices.  The list of available Type 1 devices may also be updated by examining processed\nsignal strength associated with the respective set of Type 1 devices.\n The respective chosen Type 1 device may/may not be different from the current signal source.\n A Type 2 device may choose between the first series of probe signals and the second series of probe signals based on: respective pseudo-regular intervals, respective destination MAC addresses, respective channels, respective\ncharacteristics/properties/states, a respective task to be performed by the Type 2 device, signal strength of first series and second series, and/or another consideration.\n The yet another wireless device and the another wireless device may be the same device.  They may be in/on/of a common device.  The Type 1 device may first establish the first wireless communication with the another wireless device, and then\nestablish the second wireless communication with the yet another wireless device.  The Type 1 device may start establishing the second wireless communication with the yet another wireless device before the first wireless communication is fully\nestablished.  The Type 1 device may start establishing the second wireless communication with the yet another wireless device while the first wireless communication is being established.\n The another wireless device may set its MAC address to a first destination MAC address and establish the first wireless connection with the Type 1 device using the first destination MAC address.  The yet another wireless device may set its MAC\naddress to a second destination MAC address and establish the second wireless connection with the Type 1 device using the second destination MAC address.\n The transmitting may be using a processor, a memory and a set of instructions.  Each of the Type 1 device and/or at least one Type 2 device may also have a heterogeneous IC.  The heterogeneous IC may comprise the processor, memory and the set of\ninstructions.  The heterogeneous IC in different devices may be the same or different.  Each Type 2 device may extract at least one time series of CI of the wireless multipath channel between the Type 1 device and the Type 2 device from the wireless\nsignal.  Each time series of CI is associated with an antenna of the Type 1 device and an antenna of the Type 2 device.  The heterogeneous IC may generate the wireless signal, transmit or receive the wireless signal, extract the time series of CI from\nthe wireless signal, and make the time series of CI available (e.g. to the processor/memory, other processors/memory).\n Multiple Type 1 devices may transmit wireless signals asynchronously (e.g. to the same Type 2 device, to a number of Type 2 devices).\n Multiple Type 2 devices may receive the wireless signal asynchronously.  They may extract respective time series of CI from the wireless signal asynchronously, with respective heterogeneous starting time.\n The wireless signal may be a series of probe signals.  A probe signal may be transmitted with data.  A probe signal may be replaced by a data signal.\n The series of probe signals may be transmitted at a pseudo-regular interval (e.g. 100 probe signals per second).  The pseudo-regular interval may be changed.  The series of probe signals may be scheduled at a regular interval (e.g. 100 probe\nsignals per second), but each probe signal may experience small time perturbation, perhaps due to timing requirement, timing control, network control, handshaking, message passing, collision avoidance, carrier sensing, congestion, availability of\nresources, and/or another consideration.\n The series of probe signals may be transmitted at the pseudo-regular interval for a period of time.  The pseudo-regular interval, the duration of the period of time and/or the starting time of the period of time, may be changed over time.  For\nexample, it may be 100 probe signals per second (probe/sec in short) for 2 hours and then changed to 1000 probe/sec for 30 minutes, and may change again.\n The change may be according to a time table, a rule, a policy, a mode, a condition and/or a change (e.g. once every hour, or changed whenever a certain event occur).  For example, the pseudo-regular interval may normally be 100 probe/sec, but\nwill be changed to 1000 probe/sec in some demanding situations, and may be changed to 10 probe/sec in some low power and/or standby situation.  The probe signals may be sent in burst also, e.g. at 100 probe/sec for 1 minute and no signal for some time\nbefore and after the burst.  Or, the probe signal may be sent at 100 probe/sec and then momentarily at a burst rate of 1000 probe/sec for 1 minute and then back to 100 probe/sec. In another example, the probe signal may be sent at 1 probe/sec in a low\npower mode, at 10 probe/sec in a standby mode, at 100 probe/sec in a normal mode, and at 1000 probe/sec in an armed mode, etc.\n The change based on at least one task performed by Type 1 device or Type 2 device.  For example, the task of one receiver may need 10 probe/sec while the task of another receiver may need 1000 probe/sec. The pseudo-regular interval may be\nincreased to 1000 probe/sec to satisfy the most demanding receiver.  In one example, the receivers, the tasks associated with the receivers and/or the wireless multipath channel as experienced by the receivers may be divided into at least one class (e.g.\na low priority class, a high priority class, an emergency class, a critical-task class, a regular class, a privileged class, a non-subscription class, a subscription class, a paying class, a non-paying class, etc.) and the pseudo-regular interval may be\nadjusted for the sake of some selected class (e.g. the high priority class).  When the need of that selected class is less demanding, the pseudo-regular interval may be changed (increased or decreased).  When a receiver has critically low power, the\npseudo-regular interval may be reduced to reduce the power consumption of the receiver to respond to the probe signals.\n In one example, the probe signals may be used to transfer power wirelessly to a receiver, and the pseudo-regular interval may be adjusted to control the amount of power being wirelessly transferred to the receiver.\n The pseudo-regular interval may be changed by and/or based on a server, the Type 1 device and/or the Type 2 device.  The server may be communicatively coupled with the Type 1 device and/or the Type 2 device.  The change may be communicated\nbetween the server, the Type 1 device and/or the Type 2 device.  For example, the server may monitor, track, forecast and/or anticipate the needs of the Type 2 device and/or the tasks performed by the receivers, and may decide to change the\npseudo-regular interval accordingly.  The server may send the change request to the Type 1 device (immediately or with some time delay) and the Type 1 device may then make the change.  The server may keep track of a time table and may make scheduled\nchanges to the pseudo-regular interval according to the time table.  The server may detect an emergency situation of a receiver and change the pseudo-regular interval immediately.  The server may detect a slowly happening condition (and/or an evolving\ncondition) associated with a receiver and gradually adjust the pseudo-regular interval accordingly.\n There may be more than one Type 1 devices.  Each Type 1 device may transmit a respective wireless signal to more than one respective Type 2 devices through the wireless multipath channel in the venue.  A CI time series may be obtained, each CI\ntime series associated with a Type 1 device and a Type 2 device.\n The characteristics and/or the spatial-temporal information of the motion of the object may be monitored individually based on a CI time series associated with a particular Type 1 device and a particular Type 2 device.\n The characteristics and/or the spatial-temporal information of the motion of the object may be monitored jointly based on any CI time series associated with the particular Type 1 device and any Type 2 device.\n The characteristics and/or the spatial-temporal information of the motion of the object may also be monitored jointly based on any CI time series associated with the particular Type 2 device and any Type 1 device.\n The characteristics and/or the spatial-temporal information of the motion of the object may be monitored globally based on any CI time series associated with any Type 1 device and any Type 2 device.\n Any joint monitoring (of the same motion or of the same object) may be associated with at least one of: a user, a user account, a profile, a household, a room, a location, and/or a user history, etc.\n A first wireless multipath channel between a Type 1 device and a Type 2 device may be different from a second wireless multipath channel between another Type 1 device and another Type 2 device.  The two wireless multipath channels may be\nassociated with different frequency bands, different bandwidth, different carrier frequency, different modulation, different wireless standards, different coding, different encryption, different payload characteristics, different networks, different\nnetwork parameters, etc.\n The first wireless multipath channel and the second wireless multipath channel may be associated with different kinds of wireless system (e.g. two of the following: WiFi, LTE, LTE-A, 2.5G, 3G, 3.5G, 4G, beyond 4G, 5G, 6G, 7G, a 802.11 system, a\n802.15 system, a 802.16 system, mesh network, Zigbee, WiMax, Bluetooth, BLE, RFID, UWB, microwave system, radar like system, etc.).  For example, the first may be WiFi while the second may be LTE.\n The first wireless multipath channel and the second wireless multipath channel may be associated with similar kinds of wireless system, but in different network.  For example, the first wireless multipath channel may be associated with a WiFi\nnetwork named \"Pizza and Pizza\" in the 2.4 GHz band with a bandwidth of 20 MHz while the second may be associated with a WiFi network called \"StarBud hotspot\" in the 5 GHz band with a bandwidth of 40 MHz.\n The first wireless multipath channel may be associated with a particular wireless system (e.g. WiFi, or LTE, or 3G, or BLE, or mesh network, or adhoc network) while the second wireless multipath channel may be associated another wireless system\nof same kind as the first wireless multipath channel (e.g. both WiFi, or both LTE, or both 3G, etc.) with different network ID, SSID, characteristics, settings, and/or parameters.  The first wireless multipath channel may also be a WiFi channel while the\nsecond wireless multipath channel may be another WiFi channel.  For example, both first and second wireless multipath channels may be in the same WiFi network named \"StarBud hotspot\", using two different WiFi channels in \"StarBud hotspot\".\n In one embodiment, a wireless monitoring system may comprise training at least one classifier of at least one known events in a venue based on training CI time series associated with the at least one events.\n For each of the at least one known event happening in the venue in a respective training time period associated with the known event, a respective training wireless signal (e.g. a respective series of training probe signals) may be transmitted\nby an antenna of a first Type 1 heterogeneous wireless device using a processor, a memory and a set of instructions of the first Type 1 device to at least one first Type 2 heterogeneous wireless device through a wireless multipath channel in the venue in\nthe respective training time period.\n At least one respective time series of training CI (training CI time series) may be obtained asynchronously by each of the at least one first Type 2 device from the (respective) training wireless signal.  The CI may be CI of the wireless\nmultipath channel between the first Type 2 device and the first Type 1 device in the training time period associated with the known event.  The at least one training CI time series may be preprocessed.\n For a current event happening in the venue in a current time period, a current wireless signal (e.g. a series of current probe signals) may be transmitted by an antenna of a second Type 1 heterogeneous wireless device using a processor, a memory\nand a set of instructions of the second Type 1 device to at least one second Type 2 heterogeneous wireless device through the wireless multipath channel impacted by the current event in the venue in the current time period associated with the current\nevent.\n At least one time series of current CI (current CI time series) may be obtained asynchronously by each of the at least one second Type 2 device from the current wireless signal (e.g. the series of current probe signals).  The CI may be CI of the\nwireless multipath channel between the second Type 2 device and the second Type 1 device in the current time period associated with the current event.  The at least one current CI time series may be preprocessed.\n The at least one classifier may be applied to classify at least one current CI time series obtained from the series of current probe signals by the at least one second Type 2 device, to classify at least one portion of a particular current CI\ntime series, and/or to classify a combination of the at least one portion of the particular current CI time series and another portion of another CI time series.  The at least one classifier may also be applied to associate the current event with a known\nevent, an unknown event and/or another event.\n Each wireless signal (e.g. each series of probe signals) may comprise at least two CI each with an associated time stamp.  Each CI may be associated with a respective time stamp.\n A current CI time series associated with a second Type 2 device and another current CI time series associated with another second Type 2 device may have different starting time, different time duration, different stopping time, different count\nof items in the time series, different sampling frequency, different sampling period between two consecutive items in the time series, and/or CI (CI) with different features.\n The first Type 1 device and the second Type 1 device may be the same device.  The first Type 1 device and the second Type 1 device may be at same location in the venue.\n The at least one first Type 2 device and the at least one second Type 2 device may be the same.  The at least one first Type 2 device may be a permutation of the at least one second Type 2 device.  A particular first Type 2 device and a\nparticular second Type 2 device may be the same device.\n The at least one first Type 2 device and/or a subset of the at least one first Type 2 device may be a subset of the at least one second Type 2 device.  The at least one second Type 2 device and/or a subset of the at least one second Type 2\ndevice may be a subset of the at least one first Type 2 device.\n The at least one first Type 2 device and/or a subset of the at least one first Type 2 device may be a permutation of a subset of the at least one second Type 2 device.  The at least one second Type 2 device and/or a subset of the at least one\nsecond Type 2 device may be a permutation of a subset of the at least one first Type 2 device.\n The at least one second Type 2 device and/or a subset of the at least one second Type 2 device may be at same respective location as a subset of the at least one first Type 2 device.  The at least one first Type 2 device and/or a subset of the\nat least one first Type 2 device may be at same respective location as a subset of the at least one second Type 2 device.\n The antenna of the Type 1 device and the antenna of the second Type 1 device may be at same location in the venue.  Antenna(s) of the at least one second Type 2 device and/or antenna(s) of a subset of the at least one second Type 2 device may be\nat same respective location as respective antenna(s) of a subset of the at least one first Type 2 device.  Antenna(s) of the at least one first Type 2 device and/or antenna(s) of a subset of the at least one first Type 2 device may be at same respective\nlocation(s) as respective antenna(s) of a subset of the at least one second Type 2 device.\n A first section of a first time duration of the first CI time series and a second section of a second time duration of the second section of the second CI time series may be aligned.  A map between items of the first section and items of the\nsecond section may be computed.\n The first CI time series may be processed by a first operation.\n The second CI time series may be processed by a second operation.\n The first operation and/or the second operation may comprise at least one of: subsampling, re-sampling, interpolation, filtering, transformation, feature extraction, pre-processing, and/or another operation.\n A first item of the first section may be mapped to a second item of the second section.  The first item of the first section may also be mapped to another item of the second section.  Another item of the first section may also be mapped to the\nsecond item of the second section.  The mapping may be one-to-one, one-to-many, many-to-one, many-to-many.\n At least one function of at least one of: the first item of the first section of the first CI time series, another item of the first CI time series, a time stamp of the first item, a time difference of the first item, a time differential of the\nfirst item, a neighboring time stamp of the first item, another time stamp associated with the first item, the second item of the second section of the second CI time series, another item of the second CI time series, a time stamp of the second item, a\ntime difference of the second item, a time differential of the second item, a neighboring time stamp of the second item, and another time stamp associated with the second item, may satisfy at least one constraint.\n One constraint may be that a difference between the time stamp of the first item and the time stamp of the second item may be upper-bounded by an adaptive upper threshold and lower-bounded by an adaptive lower threshold.\n The first section may be the entire first CI time series.  The second section may be the entire second CI time series.  The first time duration may be equal to the second time duration.\n A section of a time duration of a CI time series may be determined adaptively.  A tentative section of the CI time series may be computed.  A starting time and an ending time of a section (e.g. the tentative section, the section) may be\ndetermined.  The section may be determined by removing a beginning portion and an ending portion of the tentative section.\n A beginning portion of a tentative section may be determined as follows.  Iteratively, items of the tentative section with increasing time stamp may be considered as a current item, one item at a time.\n In each iteration, at least one activity measure may be computed and/or considered.  The at least one activity measure may be associated with at least one of: the current item associated with a current time stamp, past items of the tentative\nsection with time stamps not larger than the current time stamp, and/or future items of the tentative section with time stamps not smaller than the current time stamp.  The current item may be added to the beginning portion of the tentative section if at\nleast one criterion associated with the at least one activity measure is satisfied.\n The at least one criterion associated with the activity measure may comprise at least one of: (a) the activity measure is smaller than an adaptive upper threshold, (b) the activity measure is larger than an adaptive lower threshold, (c) the\nactivity measure is smaller than an adaptive upper threshold consecutively for at least a predetermined amount of consecutive time stamps, (d) the activity measure is larger than an adaptive lower threshold consecutively for at least another\npredetermined amount of consecutive time stamps, (e) the activity measure is smaller than an adaptive upper threshold consecutively for at least a predetermined percentage of the predetermined amount of consecutive time stamps, (f) the activity measure\nis larger than an adaptive lower threshold consecutively for at least another predetermined percentage of the another predetermined amount of consecutive time stamps, (g) another activity measure associated with another time stamp associated with the\ncurrent time stamp is smaller than another adaptive upper threshold and larger than another adaptive lower threshold, (h) at least one activity measure associated with at least one respective time stamp associated with the current time stamp is smaller\nthan respective upper threshold and larger than respective lower threshold, (i) percentage of time stamps with associated activity measure smaller than respective upper threshold and larger than respective lower threshold in a set of time stamps\nassociated with the current time stamp exceeds a threshold, and (j) another criterion.\n An activity measure associated with an item at time T1 may comprise at least one of: (1) a first function of the item at time T1 and an item at time T1-D1, wherein D1 is a pre-determined positive quantity (e.g. a constant time offset), (2) a\nsecond function of the item at time T1 and an item at time T1+D1, (3) a third function of the item at time T1 and an item at time T2, wherein T2 is a pre-determined quantity (e.g. a fixed initial reference time; T2 may be changed over time; T2 may be\nupdated periodically; T2 may be the beginning of a time period and T1 may be a sliding time in the time period), and (4) a fourth function of the item at time T1 and another item.\n At least one of: the first function, the second function, the third function, and/or the fourth function may be a function (e.g. F(X, Y, .  . . )) with at least two arguments: X and Y.\n The two arguments may be scalars.  The function (e.g. F) may be a function of at least one of: X, Y, (X-Y), (Y-X), abs(X-Y), X^a, Y^b, abs(X^a-Y^b), (X-Y)^a, (X/Y), (X+a)/(Y+b), (X^a/Y^b), and ((X/Y)^a-b), wherein a and b are may be some\npredetermined quantities.  For example, the function may simply be abs(X-Y), or (X-Y)^2, (X-Y)^4.  The function may be a robust function.  For example, the function may be (X-Y)^2 when abs (X-Y) is less than a threshold T, and (X-Y)+a when abs(X-Y) is\nlarger than T. Alternatively, the function may be a constant when abs(X-Y) is larger than T. The function may also be bounded by a slowly increasing function when abs(X-y) is larger than T, so that outliers cannot severely affect the result.  Another\nexample of the function may be (abs(X/Y)-a), where a=1.  In this way, if X=Y (i.e. no change or no activity), the function will give a value of 0.  If X is larger than Y, (X/Y) will be larger than 1 (assuming X and Y are positive) and the function will\nbe positive.  And if X is less than Y, (X/Y) will be smaller than 1 and the function will be negative.\n In another example, both arguments X and Y may be n-tuples such that X=(x_1, x_2, .  . . , x_n) and Y=(y_1, y_2, .  . . , y_n).  The function may be a function of at least one of: x_i, y_i (x_i-y_i), (y_i-x_i), abs(x_i-y_i), x_i^a, y_i^b,\nabs(x_i^a-y_i^b), (x_i-y_i)^a, (x_i/y_i), (x_i+a)/(y_i+b), (x_i^a/y_i^b), and ((x_i/y_i)^a-b), wherein i is a component index of the n-tuple X and Y, and 1&lt;=i&lt;=n. E.g. component index of x_1 is i=1, component index of x_2 is i=2.\n The function may comprise a component-by-component summation of another function of at least one of the following: x_i, y_i, (x_i--y_i), (y_i-x_i), abs(x_i-y_i), x_i^a, y_i^b, abs(x_i^a-y_i^b), (x_i-y_i)^a, (x_i/y_i), (x_i+a)/(y^i+b),\n(x_i^a/y_i^b), and ((x_i/y_i)^a-b), wherein i is the component index of the n-tuple X and Y. For example, the function may be in a form of sum {i=1}^n (abs(x_i/y_i)-1)/n, or sum_{i=1}^n w_i*(abs(x_i/y_i)-1), where w_i is some weight for component i.\n The map may be computed using dynamic time warping (DTW).  The DTW may comprise a constraint on at least one of: the map, the items of the first CI time series, the items of the second CI time series, the first time duration, the second time\nduration, the first section, and/or the second section.  Suppose in the map, the i^{th} domain item is mapped to the j^{th} range item.  The constraint may be on admissible combination of i and j (constraint on relationship between i and j).\n Mismatch cost between a first section of a first time duration of a first CI time series and a second section of a second time duration of a second CI time series may be computed.\n The first section and the second section may be aligned such that a map comprising more than one links may be established between first items of the first CI time series and second items of the second CI time series.  With each link, one of the\nfirst items with a first time stamp may be associated with one of the second items with a second time stamp.\n A mismatch cost between the aligned first section and the aligned second section may be computed.  The mismatch cost may comprise a function of: an item-wise cost between a first item and a second item associated by a particular link of the map,\nand a link-wise cost associated with the particular link of the map.\n The aligned first section and the aligned second section may be represented respectively as a first vector and a second vector of same vector length.  The mismatch cost may comprise at least one of: an inner product, an inner-product-like\nquantity, a quantity based on correlation, a quantity based on covariance, a discriminating score, a distance, a Euclidean distance, an absolute distance, an Lk distance (e.g. L1, L2, .  . . ), a weighted distance, a distance-like quantity and/or another\nsimilarity value, between the first vector and the second vector.  The mismatch cost may be normalized by the respective vector length.\n A parameter derived from the mismatch cost between the first section of the first time duration of the first CI time series and the second section of the second time duration of the second CI time series may be modeled with a statistical\ndistribution.  At least one of: a scale parameter, a location parameter and/or another parameter, of the statistical distribution may be estimated.\n The first section of the first time duration of the first CI time series may be a sliding section of the first CI time series.  The second section of the second time duration of the second CI time series may be a sliding section of the second CI\ntime series.\n A first sliding window may be applied to the first CI time series and a corresponding second sliding window may be applied to the second CI time series.  The first sliding window of the first CI time series and the corresponding second sliding\nwindow of the second CI time series may be aligned.\n Mismatch cost between the aligned first sliding window of the first CI time series and the corresponding aligned second sliding window of the second CI time series may be computed.  The current event may be associated with at least one of: the\nknown event, the unknown event and/or the another event, based on the mismatch cost.\n The classifier may be applied to at least one of: each first section of the first time duration of the first CI time series, and/or each second section of the second time duration of the second CI time series, to obtain at least one tentative\nclassification results.  Each tentative classification result may be associated with a respective first section and a respective second section.\n The current event may be associated with at least one of: the known event, the unknown event and/or the another event, based on the mismatch cost.\n The current event may be associated with at least one of: the known event, the unknown event and/or the another event, based on a largest number of tentative classification results in more than one sections of the first CI time series and\ncorresponding more than sections of the second CI time series.  For example, the current event may be associated with a particular known event if the mismatch cost points to the particular known event for N consecutive times (e.g. N=10).  In another\nexample, the current event may be associated with a particular known event if the percentage of mismatch cost within the immediate past N consecutive N pointing to the particular known event exceeds a certain threshold (e.g. &gt;80%).\n In another example, the current event may be associated with a known event that achieves smallest mismatch cost for the most times within a time period.  The current event may be associated with a known event that achieves smallest overall\nmismatch cost, which is a weighted average of at least one mismatch cost associated with the at least one first sections.  The current event may be associated with a particular known event that achieves smallest of another overall cost.\n The current event may be associated with the \"unknown event\" if none of the known events achieve mismatch cost lower than a first threshold T1 in a sufficient percentage of the at least one first section.  The current event may also be\nassociated with the \"unknown event\" if none of the events achieve an overall mismatch cost lower than a second threshold T2.\n The current event may be associated with at least one of: the known event, the unknown event and/or the another event, based on the mismatch cost and additional mismatch cost associated with at least one additional section of the first CI time\nseries and at least one additional section of the second CI time series.\n The known events may comprise at least one of: a door closed event, a door open event, a window closed event, a window open event, a multi-state event, an on-state event, an off-state event, an intermediate state event, a continuous state event,\na discrete state event, a human-present event, a human-absent event, a sign-of-life-present event, and/or a sign-of-life-absent event.\n A projection for each CI may be trained using a dimension reduction method based on the training CI time series.  The dimension reduction method may comprise at least one of: principal component analysis (PCA), PCA with different kernel,\nindependent component analysis (ICA), Fisher linear discriminant, vector quantization, supervised learning, unsupervised learning, self-organizing maps, auto-encoder, neural network, deep neural network, and/or another method.  The projection may be\napplied to at least one of: the training CI time series associated with the at least one event, and/or the current CI time series, for the at least one classifier.\n The at least one classifier of the at least one event may be trained based on the projection and the training CI time series associated with the at least one event.  The at least one current CI time series may be classified based on the\nprojection and the current CI time series.\n The projection may be re-trained using at least one of: the dimension reduction method, and another dimension reduction method, based on at least one of: the projection before the re-training, the training CI time series, at least one current CI\ntime series before retraining the projection, and/or additional training CI time series.\n The another dimension reduction method may comprise at least one of: a simplified dimension reduction method, principal component analysis (PCA), PCA with different kernels, independent component analysis (ICA), Fisher linear discriminant,\nvector quantization, supervised learning, unsupervised learning, self-organizing maps, auto-encoder, neural network, deep neural network, and/or yet another method,\n The at least one classifier of the at least one event may be re-trained based on at least one of: the re-trained projection, the training CI time series associated with the at least one events, and/or at least one current CI time series.\n The at least one current CI time series may be classified based on: the re-trained projection, the re-trained classifier, and/or the current CI time series.\n Each CI may comprise a vector of complex values.  Each complex value may be preprocessed to give the magnitude of the complex value.  Each CI may be preprocessed to give a vector of non-negative real numbers comprising the magnitude of\ncorresponding complex values.\n Each training CI time series may be weighted in the training of the projection.\n The projection may comprise more than one projected components.  The projection may comprise at least one most significant projected component.  The projection may comprise at least one projected component that may be beneficial for the at least\none classifier.\n The channel information (CI) may be associated with/may comprise signal strength, signal amplitude, signal phase, received signal strength indicator (RSSI), channel state information (CSI), a channel impulse response (CIR), a channel frequency\nresponse (CFR).  The CI may be associated with information associated with a frequency band, a frequency signature, a frequency phase, a frequency amplitude, a frequency trend, a frequency characteristics, a frequency-like characteristics, a time domain\nelement, a frequency domain element, a time-frequency domain element, an orthogonal decomposition characteristics, and/or a non-orthogonal decomposition characteristics of the wireless signal through the wireless multipath channel.\n The CI may also be associated with information associated with a time period, a time signature, a time stamp, a time amplitude, a time phase, a time trend, and/or a time characteristics of the wireless signal.  The CI may be associated with\ninformation associated with a time-frequency partition, a time-frequency signature, a time-frequency amplitude, a time-frequency phase, a time-frequency trend, and/or a time-frequency characteristics of the wireless signal.  The CI may be associated with\na decomposition of the wireless signal.  The CI may be associated with information associated with a direction, an angle of arrival (AoA), an angle of a directional antenna, and/or a phase of the wireless signal through the wireless multipath channel. \nThe CI may be associated with attenuation patterns of the wireless signal through the wireless multipath channel.  Each CI may be associated with a Type 1 device and a Type 2 device.  Each CI may be associated with an antenna of the Type 1 device and an\nantenna of the Type 2 device.\n The channel information (CI) may be obtained from a communication hardware that is capable of providing the CI.  The communication hardware may be a WiFi-capable chip/IC (integrated circuit), a chip compliant with a 802.11 or 802.16 or another\nwireless/radio standard, a next generation WiFi-capable chip, a LTE-capable chip, a 5G-capable chip, a 6G/7G/8G-capable chip, a Bluetooth-enabled chip, a BLE (Bluetooth low power)-enabled chip, a UWB chip, another communication chip (e.g. Zigbee, WiMax,\nmesh network), etc. The communication hardware computes the CI and stores the CI in a buffer memory and make the CI available for extraction.  The CI may comprise data and/or at least one matrices related to channel state information (CSI).  The at least\none matrices may be used for channel equalization, and/or beam forming, etc.\n The wireless multipath channel may be associated with a venue.  The attenuation may be due to signal propagation in the venue, signal propagating/reflection/refraction/diffraction through/at/around air (e.g. air of venue), refraction\nmedium/reflection surface such as wall, doors, furniture, obstacles and/or barriers, etc. The attenuation may be due to reflection at surfaces and obstacles (e.g. reflection surface, obstacle) such as floor, ceiling, furniture, fixtures, objects, people,\npets, etc.\n Each CI may be associated with a time stamp.  Each CI may comprise N1 components (e.g. N1 frequency domain components in CFR, N1 time domain components in CIR, or N1 decomposition components).  Each component may be associated with a component\nindex.  Each component may be a real quantity, an imaginary quantity, a complex quantity, a magnitude, a phase, a flag, and/or a set.  Each CI may comprise a vector of complex numbers, a matrix of complex numbers, a set of mixed quantities, and/or a\nmulti-dimensional collection of at least one complex numbers.\n Components of a CI time series associated with a particular component index may form a respective component time series associated with the respective index.  A CI time series may be divided into N1 component time series.  Each respective\ncomponent time series is associated with a respective component index.  The characteristics/spatial-temporal information of the motion of the object may be monitored based on the component time series.\n A component-wise characteristics of a component-feature time series of a CI time series may be computed.  The component-wise characteristics may be a scalar (e.g. energy) or a function with a domain and a range (e.g. an autocorrelation function,\na transform, an inverse transform).  The characteristics/spatial-temporal information of the motion of the object may be monitored based on the component-wise characteristics.\n A total characteristics of the CI time series may be computed based on the component-wise characteristics of each component time series of the CI time series.  The total characteristics may be a weighted average of the component-wise\ncharacteristics.  The characteristics/spatial-temporal information of the motion of the object may be monitored based on the total characteristics.\n The Type 1 device and Type 2 device may support WiFi, WiMax, 3G/beyond 3G, 4G/beyond 4G, LTE, 5G, 6G, 7G, Bluetooth, BLE, Zigbee, a proprietary wireless system, and/or another wireless system.\n A common wireless system and/or a common wireless channel may be shared by the Type 1 transceiver and/or the at least one Type 2 transceiver.  The at least one Type 2 transceiver may transmit respective wireless signal contemporaneously using\nthe common wireless system and/or the common wireless channel.  The Type 1 transceiver may transmit a wireless signal to the at least one Type 2 transceiver using the common wireless system and/or the common wireless channel.\n A Type 1 device may temporarily function as a Type 2 device, and vice versa.\n Each Type 1 device and Type 2 device may have at least one transmitting/receiving antenna.  Each CI may be associated with one of the transmitting antenna of the Type 1 device and one of the receiving antenna of the Type 2 device.\n The at least one time series of CI may correspond to various antenna pairs between the Type 1 device and the Type 2 device.  The Type 1 device may have at least one antenna.  The Type 2 device may also have at least one antenna.  Each time\nseries of CI may be associated with an antenna of the Type 1 device and an antenna of the Type 2 device.  Averaging or weighted averaging over antenna links may be performed.  The averaging or weighted averaging may be over the at least one time series\nof CI.  The averaging may optionally be performed on a subset of the at least one time series of CI corresponding to a subset of the antenna pairs.\n Time stamps of CI of a portion of a time series of CI may be irregular and may be corrected so that corrected timestamps of time-corrected CI may be uniformly spaced in time.  In the case of multiple Type 1 devices and/or multiple Type 2\ndevices, the corrected time stamp may be with respect to the same or different clock.\n An original timestamp associated with each of the CI may be determined.  The original timestamp may not be uniformly spaced in time.  Original timestamps of all CI of the particular portion of the particular time series of CI in the current\nsliding time window may be corrected so that corrected timestamps of time-corrected CI may be uniformly spaced in time.\n The characteristics and/or spatial-temporal information may comprise: a location, a position (e.g. initial position, new position), a position on a map, a height, a horizontal location, a vertical location, a distance, a displacement, a speed,\nan acceleration, a rotational speed, a rotational acceleration, an angle of motion, direction of motion, rotation, a path, deformation, transformation, shrinking, expanding, a gait, a gait cycle, a head motion, a repeated motion, a periodic motion, a\npseudo-periodic motion, an impulsive motion, a sudden motion, a fall-down motion, a transient motion, a behavior, a transient behavior, a period of motion, a frequency of motion, a time trend, a temporal profile, a temporal characteristics, an\noccurrence, a change, a change in frequency, a change in timing, a change of gait cycle, a timing, a starting time, an ending time, a duration, a history of motion, a motion type, a motion classification, a frequency, a frequency spectrum, a frequency\ncharacteristics, a presence, an absence, a proximity, an approaching, a receding, an identity of the object, a composition of the object, a head motion rate, a head motion direction, a mouth-related rate, an eye-related rate, a breathing rate, a heart\nrate, a hand motion rate, a hand motion direction, a leg motion, a body motion, a walking rate, a hand motion rate, a positional characteristics, a characteristics associated with movement of the object, a tool motion, a machine motion, a complex motion,\nand/or a combination of multiple motions, an event and/or another information.  The processor shares computational workload with the Type 1 heterogeneous wireless device and Type 2 heterogeneous wireless device.\n The Type 1 device and/or Type 2 device may be a local device.  The local device may be: a smart phone, a smart device, TV, sound bar, set-top box, access point, router, repeater, remote control, speaker, fan, refrigerator, microwave, oven,\ncoffee machine, hot water pot, utensil, table, chair, light, lamp, door lock, camera, microphone, motion sensor, security device, fire hydrant, garage door, switch, power adapter, computer, dongle, computer peripheral, electronic pad, sofa, tile,\naccessory, smart home device, smart vehicle device, smart office device, smart building device, smart manufacturing device, smart watch, smart glasses, smart clock, smart television, smart oven, smart refrigerator, smart air-conditioner, smart chair,\nsmart table, smart accessory, smart utility, smart appliance, smart machine, smart vehicle, an internet-of-thing (IoT) device, an internet-enabled device, a computer, a portable computer, a tablet, a smart house, a smart office, a smart building, a smart\nparking lot, a smart system, and/or another device.\n Each Type 1 device may be associated with a respective identify (ID).  Each Type 2 device may also be associated with a respective identify (ID).  The ID may comprise: a numeral, a combination of text and numbers, a name, a password, an account,\nan account ID, a web link, a web address, index to some information, and/or another ID.  The ID may be assigned.  The ID may be assigned by hardware (e.g. hardwired, via a dongle and/or other hardware), software and/or firmware.  The ID may be stored\n(e.g. in a database, in memory, in a server, in the cloud, stored locally, stored remotely, stored permanently, stored temporarily) and may be retrieved.  The ID may be associated with at least one record, account, user, household, address, phone number,\nsocial security number, customer number, another ID, time stamp, and/or collection of data.  The ID and/or part of the ID of a Type 1 device may be made available to a Type 2 device.  The ID may be used for registration, initialization, communication,\nidentification, verification, detection, recognition, authentication, access control, cloud access, networking, social networking, logging, recording, cataloging, classification, tagging, association, pairing, transaction, electronic transaction, and/or\nintellectual property control, by the Type 1 device and/or the Type 2 device.\n The object may be a person, passenger, child, older person, baby, sleeping baby, baby in a vehicle, patient, worker, high-value worker, expert, specialist, waiter, customer in a mall, traveler in airport/train station/bus terminal/shipping\nterminals, staff/worker/customer service personnel in a factory/mall/supermarket/office/workplace, serviceman in sewage/air ventilation system/lift well, lifts in lift wells, elevator, inmate, people to be tracked/monitored, animal, a plant, a living\nobject, a pet, dog, cat, smart phone, phone accessory, computer, tablet, portable computer, dongle, computing accessory, networked devices, WiFi devices, IoT devices, smart watch, smart glasses, smart devices, speaker, keys, smart key, wallet, purse,\nhandbag, backpack, goods, cargo, luggage, equipment, motor, machine, air conditioner, fan, air conditioning equipment, light fixture, moveable light, television, camera, audio and/or video equipment, stationary, surveillance equipment, parts, signage,\ntool, cart, ticket, parking ticket, toll ticket, airplane ticket, credit card, plastic card, access card, food packaging, utensil, table, chair, cleaning equipment/tool, vehicle, car, cars in parking facilities, merchandise in a\nwarehouse/store/supermarket/distribution center, boat, bicycle, airplane, drone, remote control car/plane/boat, robot, manufacturing device, assembly line, material/unfinished part/robot/wagon/transports on a factory floor, object to be tracked in\nairport/shopping mart/supermarket, non-object, absence of an object, presence of an object, object with a form, object with a changing form, object with no form, mass of fluid, mass of liquid, mass of gas/smoke, fire, flame, electromagnetic (EM) source,\nEM medium, and/or another object.\n The object itself may be communicatively coupled with some network, such as WiFi, MiFi, 3G/4G/LTE/5G, Bluetooth, BLE, WiMax, Zigbee, mesh network, adhoc network, and/or other network.  The object itself may be bulky with AC power supply, but is\nmoved during installation, cleaning, maintenance, renovation, etc. It may also be installed in a moveable platform such as a lift, a pad, a movable, platform, an elevator, a conveyor belt, a robot, a drone, a forklift, a car, a boat, a vehicle, etc.\n The object may have multiple parts, each part with different movement.  For example, the object may be a person walking forward.  While walking, his left hand and right hand may move in different direction, with different instantaneous speed,\nacceleration, motion, etc.\n The wireless transmitter (e.g. Type 1 device), the wireless receiver (e.g. Type 2 device), another wireless transmitter and/or another wireless receiver may move with the object and/or another object (e.g. in prior movement, current movement\nand/or future movement.  They may be communicatively coupled to one or more nearby device.  They may transmit time series of CI and/or information associated with the time series of CI to the nearby device, and/or each other.  They may be with the nearby\ndevice.\n The wireless transmitter and/or the wireless receiver may be part of a small (e.g. coin-size, cigarette box size, or even smaller), light-weight portable device.  The portable device may be wirelessly coupled with a nearby device.\n The nearby device may be smart phone, iPhone, Android phone, smart device, smart appliance, smart vehicle, smart gadget, smart TV, smart refrigerator, smart speaker, smart watch, smart glasses, smart pad, iPad, computer, wearable computer,\nnotebook computer, gateway.  The nearby device may be connected to a cloud server, a local server and/or other server via internet, wired internet connection and/or wireless internet connection.  The nearby device may be portable.\n The portable device, the nearby device, a local server and/or a cloud server may share the computation and/or storage for a task (e.g. obtain time series of CI, determine characteristics/spatial-temporal information of the object associated with\nthe movement of the object, computation of time series of power information, determining/computing the particular function, searching for local extremum, classification, identifying particular value of time offset, de-noising, processing, simplification,\ncleaning, wireless smart sensing task, extract CI from wireless signal, switching, segmentation, estimate trajectory, process the map, correction, corrective adjustment, adjustment, map-based correction, detecting error, checking for boundary hitting,\nthresholding, etc.) and information (e.g. time series of CI).\n The nearby device may/may not move with the object.  The nearby device may be portable/not portable/moveable/non-moveable.  The nearby device may use battery power, solar power, AC power and/or other power source.  The nearby device may have\nreplaceable/non-replaceable battery, and/or rechargeable/non-rechargeable battery.  The nearby device may be similar to the object.  The nearby device may have identical (and/or similar) hardware and/or software to the object.  The nearby device may be a\nsmart device, a network enabled device, a device with connection to WiFi/3G/4G/5G/6G/Zigbee/Bluetooth/adhoc network/other network, a smart speaker, a smart watch, a smart clock, a smart appliance, a smart machine, a smart equipment, a smart tool, a smart\nvehicle, an internet-of-thing (IoT) device, an internet-enabled device, a computer, a portable computer, a tablet, and another device.\n The nearby device and/or at least one processor associated with the wireless receiver, the wireless transmitter, the another wireless receiver, the another wireless transmitter and/or a cloud server (in the cloud) may determine the initial\nspatial-temporal information of the object.  Two or more of them may determine the initial spatial-temporal info jointly.  Two or more of them may share intermediate information in the determination of the initial spatial-temporal information (e.g.\ninitial position).\n In one example, the wireless transmitter (e.g. Type 1 device, or Tracker Bot) may move with the object.  The wireless transmitter may send the wireless signal to the wireless receiver (e.g. Type 2 device, or Origin Register) or determining the\ninitial spatial-temporal information (e.g. initial position) of the object.  The wireless transmitter may also send the wireless signal and/or another wireless signal to another wireless receiver (e.g. another Type 2 device, or another Origin Register)\nfor the monitoring of the motion (spatial-temporal info) of the object.  The wireless receiver may also receive the wireless signal and/or another wireless signal from the wireless transmitter and/or the another wireless transmitter for monitoring the\nmotion of the object.  The location of the wireless receiver and/or the another wireless receiver may be known.\n In another example, the wireless receiver (e.g. Type 2 device, or Tracker Bot) may move with the object.  The wireless receiver may receive the wireless signal transmitted from the wireless transmitter (e.g. Type 1 device, or Origin Register)\nfor determining the initial spatial-temporal info (e.g. initial position) of the object.  The wireless receiver may also receive the wireless signal and/or another wireless signal from another wireless transmitter (e.g. another Type 1 device, or another\nOrigin Register) for the monitoring of the current motion (e.g. spatial-temporal info) of the object.  The wireless transmitter may also transmit the wireless signal and/or another wireless signal to the wireless receiver and/or the another wireless\nreceiver (e.g. another Type 2 device, or another Tracker Bot) for monitoring the motion of the object.  The location of the wireless transmitter and/or the another wireless transmitter may be known.\n The venue may be a space such as a room, a house, an office, a workplace, a hallway, a walkway, a lift, a lift well, an escalator, an elevator, a sewage system, air ventilations system, a staircase, a gathering area, a duct, an air duct, a pipe,\na tube, an enclosed structure, a semi-enclosed structure, an enclosed area, an area with at least one wall, a plant, a machine, an engine, a structure with wood, a structure with glass, a structure with metal, a structure with walls, a structure with\ndoors, a structure with gaps, a structure with reflection surface, a structure with fluid, a building, a roof top, a store, a factory, an assembly line, a hotel room, a museum, a classroom, a school, a university, a government building, a warehouse, a\ngarage, a mall, an airport, a train station, a bus terminal, a hub, a transportation hub, a shipping terminal, a government facility, a public facility, a school, a university, an entertainment facility, a recreational facility, a hospital, a seniors\nhome, an elderly care facility, a community center, a stadium, a playground, a park, a field, a sports facility, a swimming facility, a track and/or field, a basketball court, a tennis court, a soccer stadium, a baseball stadium, a gymnasium, a hall, a\ngarage, a shopping mart, a mall, a supermarket, a manufacturing facility, a parking facility, a construction site, a mining facility, a transportation facility, a highway, a road, a valley, a forest, a wood, a terrain, a landscape, a den, a patio, a\nland, a path, an amusement park, an urban area, a rural area, a suburban area, a metropolitan area, a garden, a square, a plaza, a music hall, a downtown facility, an over-air facility, a semi-open facility, a closed area, a train platform, a train\nstation, a distribution center, a warehouse, a store, a distribution center, a storage facility, an underground facility, a space (e.g. above ground, outer-space) facility, a floating facility, a cavern, a tunnel facility, an indoor facility, an open-air\nfacility, an outdoor facility with some walls/doors/reflective barriers, an open facility, a semi-open facility, a car, a truck, a bus, a van, a container, a ship/boat, a submersible, a train, a tram, an airplane, a vehicle, a mobile home, a cave, a\ntunnel, a pipe, a channel, a metropolitan area, downtown area with relatively tall buildings, a valley, a well, a duct, a pathway, a gas line, an oil line, a water pipe, a network of interconnecting pathways/alleys/roads/tubes/cavities/caves/pipe-like\nstructure/air space/fluid space, a human body, an animal body, a body cavity, an organ, a bone, a teeth, a soft tissue, a hard tissue, a rigid tissue, a non-rigid tissue, a blood/body fluid vessel, windpipe, air duct, a den, etc. The venue may be an\nindoor space, an outdoor space, The venue may include both the inside and outside of the space.  For example, the venue may include both the inside of a building and the outside of the building.  For example, the venue can be a building that has one\nfloor or multiple floors, and a portion of the building can be underground.  The shape of the building can be, e.g., round, square, rectangular, triangle, or irregular-shaped.  These are merely examples.  The disclosure can be used to detect events in\nother types of venue or spaces.\n The wireless transmitter (e.g. Type 1 device) and/or the wireless receiver (e.g. Type 2 device) may be embedded in a portable device (e.g. a module, or a device with the module) that may move with the object (e.g. in prior movement and/or\ncurrent movement).  The portable device may be communicatively coupled with the object using a wired connection (e.g. through USB, microUSB, Firewire, HDMI, serial port, parallel port, and other connectors) and/or a wireless connection (e.g. Bluetooth,\nBluetooth Low Energy (BLE), WiFi, LTE, ZigBee, etc.).  The portable device may be a lightweight device.  The portable may be powered by battery, rechargeable battery and/or AC power.  The portable device may be very small (e.g. at sub-millimeter scale\nand/or sub-centimeter scale), and/or small (e.g. coin-size, card-size, pocket-size, or larger).  The portable device may be large, sizable, and/or bulky (e.g. heavy machinery to be installed).  The portable device may be a WiFi hotspot, an access point,\na mobile WiFi (MiFi), a dongle with USB/micro USB/Firewlire/other connector, a smartphone, a portable computer, a computer, a tablet, a smart device, an internet-of-thing (IoT) device, a WiFi-enabled device, an LTE-enabled device, a smart watch, a smart\nglass, a smart mirror, a smart antenna, a smart battery, a smart light, a smart pen, a smart ring, a smart door, a smart window, a smart clock, a small battery, a smart wallet, a smart belt, a smart handbag, a smart clothing/garment, a smart ornament, a\nsmart packaging, a smart paper/book/magazine/poster/printed matter/signage/display/lighted system/lighting system, a smart key/tool, a smart bracelet/chain/necklace/wearable/accessory, a smart pad/cushion, a smart tile/block/brick/building material/other\nmaterial, a smart garbage can/waste container, a smart food carriage/storage, a smart ball/racket, a smart chair/sofa/bed, a smart shoe/footwear/carpet/mat/shoe rack, a smart glove/hand wear/ring/hand ware, a smart hat/headwear/makeup/sticker/tattoo, a\nsmart mirror, a smart toy, a smart pill, a smart utensil, a smart bottle/food container, a smart tool, a smart device, an IoT device, a WiFi enabled device, a network enabled device, a 3G/4G/5G/6G enabled device, an embeddable device, an implantable\ndevice, air conditioner, refrigerator, heater, furnace, furniture, oven, cooking device, television/set-top box (STB)/DVD player/audio player/video player/remote control, hi-fi, audio device, speaker, lamp/light, wall, door, window, roof, roof\ntile/shingle/structure/attic structure/device/feature/installation/fixtures, lawn mower/garden tools/yard tools/mechanics tools/garage tools, garbage can/container, 20-ft/40-ft container, storage container, factory/manufacturing/production device, repair\ntools, fluid container, machine, machinery to be installed, a vehicle, a cart, a wagon, warehouse vehicle, a car, a bicycle, a motorcycle, a boat, a vessel, an airplane, a basket/box/bag/bucket/container, a smart plate/cup/bowl/pot/mat/utensils/kitchen\ntools/kitchen devices/kitchen accessories/cabinets/tables/chairs/tiles/lights/water pipes/taps/gas range/oven/dishwashing machine/etc. The portable device may have a battery that may be replaceable, irreplaceable, rechargeable, and/or non-rechargeable. \nThe portable device may be wirelessly charged.  The portable device may be a smart payment card.  The portable device may be a payment card used in parking lots, highways, entertainment parks, or other venues/facilities that need payment.  The portable\ndevice may have an identity (ID) as described above.\n An event may be monitored based on the time series of CI.  The event may be an object related event, such as fall-down of the object (e.g. an person and/or a sick person), a rotation, a hesitation, a pause, an impact (e.g. a person hitting a\nsandbag, a door, a window, a bed, a chair, a table, a desk, a cabinet, a box, another person, an animal, a bird, a fly, a table, a chair, a ball, a bowling ball, a tennis ball, a football, a soccer ball, a baseball, a basketball, a volley ball, etc.), a\ntwo-body action (e.g. a person letting go a balloon, catching a fish, molding a clay, writing a paper, a person typing on a computer, etc.), a car moving in a garage, a person carrying a smart phone and walking around an airport/mall/government\nbuilding/office/etc, an autonomous moveable object/machine moving around (e.g. vacuum cleaner, utility vehicle, a car, drone, self-driving car, etc.).\n The task or the wireless smart sensing task may comprise: object detection, presence detection, object recognition, object verification, tool detection, tool recognition, tool verification, machine detection, machine recognition, machine\nverification, human detection, human recognition, human verification, baby detection, baby recognition, baby verification, human breathing detection, motion detection, motion estimation, motion verification, periodic motion detection, periodic motion\nestimation, periodic motion verification, stationary motion detection, stationary motion estimation, stationary motion verification, cyclo-stationary motion detection, cyclo-stationary motion estimation, cyclo-stationary motion verification, transient\nmotion detection, transient motion estimation, transient motion verification, trend detection, trend estimation, trend verification, breathing detection, breathing estimation, breathing estimation, human biometrics detection, human biometrics estimation,\nhuman biometrics verification, environment informatics detection, environment informatics estimation, environment informatics verification, gait detection, gait estimation, gait verification, gesture detection, gesture estimation, gesture verification,\nmachine learning, supervised learning, unsupervised learning, semi-supervised learning, clustering, feature extraction, featuring training, principal component analysis, eigen-decomposition, frequency decomposition, time decomposition, time-frequency\ndecomposition, functional decomposition, other decomposition, training, discriminative training, supervised training, unsupervised training, semi-supervised training, neural network, sudden motion detection, fall-down detection, danger detection,\nlife-threat detection, regular motion detection, stationary motion detection, cyclo-stationary motion detection, intrusion detection, suspicious motion detection, security, safety monitoring, navigation, guidance, map-based processing, map-based\ncorrection, irregularity detection, locationing, tracking, multiple object tracking, indoor tracking, indoor position, indoor navigation, power transfer, wireless power transfer, object counting, car tracking in parking garage, patient detection, patient\nmonitoring, patient verification, wireless communication, data communication, signal broadcasting, networking, coordination, administration, encryption, protection, cloud computing, other processing and/or other task.  The task may be performed by the\nType 1 device, the Type 2 device, another Type 1 device, another Type 2 device, a nearby device, a local server, an edge server, a cloud server, and/or another device.\n A first part of the task may comprise at least one of: preprocessing, signal conditioning, signal processing, post-processing, denoising, feature extraction, coding, encryption, transformation, mapping, motion detection, motion estimation,\nmotion change detection, motion pattern detection, motion pattern estimation, motion pattern recognition, vital sign detection, vital sign estimation, vital sign recognition, periodic motion detection, periodic motion estimation, breathing rate\ndetection, breathing rate estimation, breathing pattern detection, breathing pattern estimation, breathing pattern recognition, heart beat detection, heart beat estimation, heart pattern detection, heart pattern estimation, heart pattern recognition,\ngesture detection, gesture estimation, gesture recognition, speed detection, speed estimation, object locationing, object tracking, navigation, acceleration estimation, acceleration detection, fall-down detection, change detection, intruder detection,\nbaby detection, baby monitoring, patient monitoring, object recognition, wireless power transfer, and/or wireless charging.\n A second part of the task may comprise at least one of: a smart home task, a smart office task, a smart building task, a smart factory task (e.g. manufacturing using a machine or an assembly line), a smart internet-of-thing (IoT) task, a smart\nsystem task, a smart home operation, a smart office operation, a smart building operation, a smart manufacturing operation (e.g. moving supplies/parts/raw material to a machine/an assembly line), an IoT operation, a smart system operation, turning on a\nlight, turning off the light, controlling the light in at least one of: a room, a region, and/or the venue, playing a sound clip, playing the sound clip in at least one of: the room, the region, and/or the venue, playing the sound clip of at least one\nof: a welcome, a greeting, a farewell, a first message, and/or a second message associated with the first part of the task, turning on an appliance, turning off the appliance, controlling the appliance in at least one of: the room, the region, and/or the\nvenue, turning on an electrical system, turning off the electrical system, controlling the electrical system in at least one of: the room, the region, and/or the venue, turning on a security system, turning off the security system, controlling the\nsecurity system in at least one of: the room, the region, and/or the venue, turning on a mechanical system, turning off a mechanical system, controlling the mechanical system in at least one of: the room, the region, and/or the venue, and/or controlling\nat least one of: an air conditioning system, a heating system, a ventilation system, a lighting system, a heating device, a stove, an entertainment system, a door, a fence, a window, a garage, a computer system, a networked device, a networked system, a\nhome appliance, an office equipment, a lighting device, a robot (e.g. robotic arm), a smart vehicle, a smart machine, an assembly line, a smart device, an internet-of-thing (IoT) device, a smart home device, and/or a smart office device.\n The task may include: detect a user returning home, detect a user leaving home, detect a user moving from one room to another, detect/control/lock/unlock/open/close/partially open a window/door/garage door/blind/curtain/panel/solar panel/sun\nshade, detect a pet, detect/monitor a user doing something (e.g. sleeping on sofa, sleeping in bedroom, running on treadmill, cooking, sitting on sofa, watching TV, eating in kitchen, eating in dining room, going upstairs/downstairs, going outside/coming\nback, in the rest room, etc.), monitor/detect location of a user/pet, do something automatically upon detection, do something for the user automatically upon detecting the user, turn on/off/dim a light, turn on/off music/radio/home entertainment system,\nturn on/off/adjust/control TV/HiFi/set-top-box (STB)/home entertainment system/smart speaker/smart device, turn on/off/adjust air conditioning system, turn on/off/adjust ventilation system, turn on/off/adjust heating system, adjust/control curtains/light\nshades, turn on/off/wake a computer, turn on/off/pre-heat/control coffee machine/hot water pot, turn on/off/control/preheat cooker/oven/microwave oven/another cooking device, check/adjust temperature, check weather forecast, check telephone message box,\ncheck mail, do a system check, control/adjust a system, check/control/arm/disarm security system/baby monitor, check/control refrigerator, give a report (e.g. through a speaker such as Google home, Amazon Echo, on a display/screen, via a\nwebpage/email/messaging system/notification system, etc.).\n For example, when a user arrives home in his car, the task may be to, automatically, detect the user or his car approaching, open the garage door upon detection, turn on the driveway/garage light as the user approaches the garage, turn on air\nconditioner/heater/fan, etc. As the user enters the house, the task may be to, automatically, turn on the entrance light, turn off driveway/garage light, play a greeting message to welcome the user, turn on the music, turn on the radio and tuning to the\nuser's favorite radio news channel, open the curtain/blind, monitor the user's mood, adjust the lighting and sound environment according to the user's mood or the current/imminent event (e.g. do romantic lighting and music because the user is scheduled\nto eat dinner with girlfriend in 1 hour) on the user's daily calendar, warm the food in microwave that the user prepared in the morning, do a diagnostic check of all systems in the house, check weather forecast for tomorrow's work, check news of interest\nto the user, check user's calendar and to-do list and play reminder, check telephone answer system/messaging system/email and give a verbal report using dialog system/speech synthesis, remind (e.g. using audible tool such as speakers/HiFi/speech\nsynthesis/sound/voice/music/song/sound field/background sound field/dialog system, using visual tool such as TV/entertainment system/computer/notebook/smart pad/display/light/color/brightness/patterns/symbols, using haptic tool/virtual reality\ntool/gesture/tool, using a smart device/appliance/material/furniture/fixture, using web tool/server/cloud server/fog server/edge server/home network/mesh network, using messaging tool/notification tool/communication tool/scheduling tool/email, using user\ninterface/GUI, using scent/smell/fragrance/taste, using neural tool/nervous system tool, using a combination, etc.) the user of his mother's birthday and to call her, prepare a report, and give the report (e.g. using a tool for reminding as discussed\nabove).  The task may turn on the air conditioner/heater/ventilation system in advance, or adjust temperature setting of smart thermostat in advance, etc. As the user moves from the entrance to the living room, the task may be to turn on the living room\nlight, open the living room curtain, open the window, turn off the entrance light behind the user, turn on the TV and set-top box, set TV to the user's favorite channel, adjust an appliance according to the user's preference and conditions/states (e.g.\nadjust lighting and choose/play music to build a romantic atmosphere), etc.\n Another example may be: When the user wakes up in the morning, the task may be to detect the user moving around in the bedroom, open the blind/curtain, open the window, turn off the alarm clock, adjust indoor temperature from night-time\ntemperature profile to day-time temperature profile, turn on the bedroom light, turn on the restroom light as the user approaches the restroom, check radio or streaming channel and play morning news, turn on the coffee machine and preheat the water, turn\noff security system, etc. When the user walks from bedroom to kitchen, the task may be to turn on the kitchen and hallway lights, turn off the bedroom and restroom lights, move the music/message/reminder from the bedroom to the kitchen, turn on the\nkitchen TV, change TV to morning news channel, lower the kitchen blind and open the kitchen window to bring in fresh air, unlock backdoor for the user to check the backyard, adjust temperature setting for the kitchen, etc.\n Another example may be: When the user leaves home for work, the task may be to detect the user leaving, play a farewell and/or have-a-good-day message, open/close garage door, turn on/off garage light and driveway light, turn off/dim lights to\nsave energy (just in case the user forgets), close/lock all windows/doors (just in case the user forgets), turn off appliance (especially stove, oven, microwave oven), turn on/arm the home security system to guard the home against any intruder, adjust\nair conditioning/heating/ventilation systems to \"away-from-home\" profile to save energy, send alerts/reports/updates to the user's smart phone, etc.\n A motion may comprise at least one of: a no-motion, a resting motion, a non-moving motion, a deterministic motion, a transient motion, a fall-down motion, a repeating motion, a periodic motion, a pseudo-periodic motion, a periodic motion\nassociated with breathing, a periodic motion associated with heartbeat, a periodic motion associated with a living object, a periodic motion associated with a machine, a periodic motion associated with a man-made object, a periodic motion associated with\nnature, a complex motion with a transient element and a periodic element, a repetitive motion, a non-deterministic motion, a probabilistic motion, a chaotic motion, a random motion, a complex motion with a non-deterministic element and a deterministic\nelement, a stationary random motion, a pseudo-stationary random motion, a cyclo-stationary random motion, a non-stationary random motion, a stationary random motion with a periodic autocorrelation function (ACF), a random motion with a periodic ACF for a\nperiod of time, a random motion that is pseudo-stationary for a period of time, a random motion of which an instantaneous ACF has a pseudo-periodic element for a period of time, a machine motion, a mechanical motion, a vehicle motion, a drone motion, an\nair-related motion, a wind-related motion, a weather-related motion, a water-related motion, a fluid-related motion, an ground-related motion, a change in electro-magnetic characteristics, a sub-surface motion, a seismic motion, a plant motion, an animal\nmotion, a human motion, a normal motion, an abnormal motion, a dangerous motion, a warning motion, a suspicious motion, a rain, a fire, a flood, a tsunami, an explosion, a collision, an imminent collision, a human body motion, a head motion, a facial\nmotion, an eye motion, a mouth motion, a tongue motion, a neck motion, a finger motion, a hand motion, an arm motion, a shoulder motion, a body motion, a chest motion, an abdominal motion, a hip motion, a leg motion, a foot motion, a body joint motion, a\nknee motion, an elbow motion, an upper body motion, a lower body motion, a skin motion, a below-skin motion, a subcutaneous tissue motion, a blood vessel motion, an intravenous motion, an organ motion, a heart motion, a lung motion, a stomach motion, an\nintestine motion, a bowel motion, an eating motion, a breathing motion, a facial expression, an eye expression, a mouth expression, a talking motion, a singing motion, an eating motion, a gesture, a hand gesture, an arm gesture, a keystroke, a typing\nstroke, a user-interface gesture, a man-machine interaction, a gait, a dancing movement, a coordinated movement, and/or a coordinated body movement.\n The heterogeneous IC of the Type 1 device and/or any Type 2 receiver may comprise a low-noise amplifier (LNA), a power amplifier, a transmit-receive switch, a media access controller, a baseband radio, a 2.4 GHz radio, a 3.65 GHz radio, a 4.9\nGHz radio, a 5 GHz radio, a 5.9 GHz radio, a below 6 GHz radio, a below 60 GHz radio and/or another radio.\n The heterogeneous IC may comprise a processor, a memory communicatively coupled with the processor, and a set of instructions stored in the memory to be executed by the processor.  The IC may be an application specific integrated circuit (ASIC),\na field programmable gate array (FPGA), other programmable logic device, discrete logic, and/or a combination.\n The heterogeneous IC may support a broadband network, a wireless network, a mobile network, a mesh network, a cellular network, a wireless local area network (WLAN), a wide area network (WAN), and a metropolitan area network (MAN), a WLAN\nstandard, WiFi, LTE, a 802.11 standard, 802.11a, 802.11b, 802.11g, 802.11n, 802.11ac, 802.11ad, 802.11af, 802.11ah, 802.11ax, 802.11ay, a mesh network standard, a 802.15 standard, a 802.16 standard, a cellular network standard, 3G, 3.5G, 4G, beyond 4G,\n4.5G, 5G, 6G, 7G, 8G, 9G, Bluetooth, Bluetooth Low-Energy (BLE), Zigbee, WiMax, and/or another wireless network protocol.\n The processor may comprise a general purpose processor, a special purpose processor, a microprocessor, a microcontroller, an embedded processor, a digital signal processor, a central processing unit (CPU), a graphical processing unit (GPU), a\nmulti-processor, a multi-core processor, and/or a processor with graphics capability, and/or a combination.\n The memory may be volatile, non-volatile, random access memory (RAM), Read Only Memory (ROM), Electrically Programmable ROM (EPROM), Electrically Erasable Programmable ROM (EEPROM), hard disk, flash memory, CD-ROM, DVD-ROM, a magnetic storage,\nan optical storage, an organic storage, a storage system, a storage network, network storage, cloud storage, or other form of non-transitory storage medium known in the art.\n The set of instructions (machine executable code) corresponding to the method steps may be embodied directly in hardware, in software, in firmware, or in combinations thereof.\n The presentation may be a presentation in an audio-visual way, a graphical way (e.g. using GUI), a textual way, a symbolic way or a mechanical way.\n Computational workload associated with the method is shared among the processor, the Type 1 heterogeneous wireless device, the Type 2 heterogeneous wireless device, a local server, a cloud server, and another processor.\n An operation, a pre-processing, a processing and/or a postprocessing may be applied to data (e.g. time series of CI, autocorrelation).  An operation may be preprocessing, processing and/or postprocessing.  The preprocessing, processing and/or\npostprocessing may be an operation.  An operation may comprise preprocessing, processing, post-processing, computing a function of the operands, filtering, linear filtering, nonlinear filtering, folding, grouping, energy computation, lowpass filtering,\nbandpass filtering, highpass filtering, median filtering, rank filtering, quartile filtering, percentile filtering, mode filtering, finite impulse response (FIR) filtering, infinite impulse response (IIR) filtering, moving average (MA) filtering,\nautoregressive (AR) filtering, autoregressive moving averaging (ARMA) filtering, selective filtering, adaptive filtering, interpolation, decimation, subsampling, upsampling, resampling, time correction, time base correction, phase correction, magnitude\ncorrection, phase cleaning, magnitude cleaning, matched filtering, enhancement, restoration, denoising, smoothing, signal conditioning, enhancement, restoration, spectral analysis, linear transform, nonlinear transform, frequency transform, inverse\nfrequency transform, Fourier transform, wavelet transform, Laplace transform, Hilbert transform, Hadamard transform, trigonometric transform, sine transform, cosine transform, power-of-2 transform, sparse transform, graph-based transform, graph signal\nprocessing, fast transform, a transform combined with zero padding, cyclic padding, padding, zero padding, feature extraction, decomposition, projection, orthogonal projection, non-orthogonal projection, over-complete projection, eigen-decomposition,\nsingular value decomposition (SVD), principle component analysis (PCA), independent component analysis (ICA), grouping, sorting, thresholding, soft thresholding, hard thresholding, clipping, soft clipping, first derivative, second order derivative, high\norder derivative, convolution, multiplication, division, addition, subtraction, integration, maximization, minimization, local maximization, local minimization, optimization of a cost function, neural network, recognition, labeling, training, clustering,\nmachine learning, supervised learning, unsupervised learning, semi-supervised learning, comparison with another time series of CI, similarity score computation, quantization, vector quantization, matching pursuit, compression, encryption, coding,\nstoring, transmitting, normalization, temporal normalization, frequency domain normalization, classification, clustering, labeling, tagging, learning, detection, estimation, learning network, mapping, remapping, expansion, storing, retrieving,\ntransmitting, receiving, representing, merging, combining, splitting, tracking, monitoring, matched filtering, Kalman filtering, particle filter, intrapolation, extrapolation, importance sampling, Monte Carlo sampling, compressive sensing, representing,\nmerging, combining, splitting, scrambling, error protection, forward error correction, doing nothing, time varying processing, conditioning averaging, weighted averaging, arithmetic mean, geometric mean, averaging over selected frequency, averaging over\nantenna links, a logical operation, permutation, combination, sorting, AND, OR, XOR, union, intersection, vector addition, vector subtraction, vector multiplication, vector division, inverse, a norm, a distance, and/or another operation.  The operation\nmay be the preprocessing, processing, and/or post-processing.  Operations may be applied jointly on multiple time series or functions.\n The function (e.g. function of the operands) may comprise: a scalar function, a vector function, a discrete function, a continuous function, a polynomial function, a magnitude, a phase, an exponential function, a logarithmic function, a\ntrigonometric function, a transcendental function, a logical function, a linear function, an algebraic function, a nonlinear function, a piecewise linear function, a real function, a complex function, a vector-valued function, an inverse function, a\nderivative of a function, an integration of a function, a circular function, a function of another function, a one-to-one function, a one-to-many function, a many-to-one function, a many-to-many function, a zero crossing, absolute function, indicator\nfunction, a mean, a mode, a median, a range, a statistics, a variance, a trimmed mean, a percentile, a square, a cube, a root, a power, a sine, a cosine, a tangent, a cotangent, a secant, a cosecant, an elliptical function, a parabolic function, a\nhyperbolic function, a game function, a zeta function, an absolute value, a thresholding, a quantization, a piecewise constant function, a composite function, a function of function, an input time function processed with an operation (e.g. filtering), a\nprobabilistic function, a stochastic function, a random function, an ergodic function, a stationary function, a deterministic function, a transformation, a frequency transform, an inverse frequency transform, a discrete time transform, Laplace transform,\nHilbert transform, sine transform, cosine transform, triangular transform, wavelet transform, integer transform, power-of-2 transform, sparse transform, projection, decomposition, principle component analysis (PCA), independent component analysis (ICA),\nneural network, feature extraction, a moving function, a function of a moving window of neighboring items of a time series, a filtering function, a convolution, a mean function, a variance function, a statistical function, short-time transform, discrete\ntransform, discrete Fourier transform, discrete cosine transform, discrete sine transform, Hadamard transform, eigen-decomposition, eigenvalue, singular value decomposition (SVD), singular value, orthogonal decomposition, matching pursuit, sparse\ntransform, sparse approximation, any decomposition, graph-based processing, graph-based transform, graph signal processing, classification, labeling, learning, machine learning, detection, estimation, feature extraction, learning network, feature\nextraction, denoising, signal enhancement, coding, encryption, mapping, remapping, vector quantization, lowpass filtering, highpass filtering, bandpass filtering, matched filtering, Kalman filtering, preprocessing, postprocessing, particle filter, FIR\nfiltering, IIR filtering, autoregressive (AR) filtering, adaptive filtering, first order derivative, high order derivative, integration, zero crossing, smoothing, median filtering, mode filtering, sampling, random sampling, resampling function,\ndownsampling, upsampling, interpolation, extrapolation, importance sampling, Monte Carlo sampling, compressive sensing, statistics, short term statistics, long term statistics, mean, variance, autocorrelation function, cross correlation, moment\ngenerating function, time averaging, etc.\n Machine learn, training, discriminative training, deep learning, neural network, continuous time processing, distributed computing, distributed storage, acceleration using GPU/DSP/coprocessor/multicore/multiprocessing may be applied to a step\n(or each step) of this disclosure.\n A frequency transform may include Fourier transform, Laplace transform, Hadamard transform, Hilbert transform, sine transform, cosine transform, triangular transform, wavelet transform, integer transform, power-of-2 transform, combined zero\npadding and transform, Fourier transform with zero padding, and/or another transform.  Fast versions and/or approximated versions of the transform may be performed.  The transform may be performed using floating point, and/or fixed point arithmetic.\n An inverse frequency transform may include inverse Fourier transform, inverse Laplace transform, inverse Hadamard transform, inverse Hilbert transform, inverse sine transform, inverse cosine transform, inverse triangular transform, inverse\nwavelet transform, inverse integer transform, inverse power-of-2 transform, combined zero padding and transform, inverse Fourier transform with zero padding, and/or another transform.  Fast versions and/or approximated versions of the transform may be\nperformed.  The transform may be performed using floating point, and/or fixed point arithmetic.\n Sliding time window may have time varying window width.  It may be smaller at the beginning to enable fast acquisition and may increase over time to a steady-state size.  The steady-state size may be related to the frequency, repeated motion,\ntransient motion, and/or spatial-temporal information to be monitored.  Even in steady state, the window size may be adaptively changed based on battery life, power consumption, available computing power, a change in amount of targets, the nature of\nmotion to be monitored, etc.\n The time shift between two sliding time windows at adjacent time instance may be constant/variable/locally adaptive over time.  When shorter time shift is used, the update of any monitoring may be more frequent which may be used for fast\nchanging situations, object motions, and/or objects.  Longer time shift may be used for slower situations, object motions, and/or objects.\n The window width/size and/or time shift may be changed upon a user request/choice.  The time shift may be changed automatically (e.g. as controlled by processor/computer/server/cloud server) and/or adaptively.\n At least one characteristics of a function (e.g. auto-correlation function, auto-covariance function, cross-correlation function, cross-covariance function, power spectral density, a time function, a frequency domain function, a frequency\ntransform) may be determined (e.g. by an object tracking server, the processor, the Type 1 heterogeneous device, the Type 2 heterogeneous device, and/or another device).  The at least one characteristics of the function may include: a local maximum, a\nlocal minimum, a local extremum, a local extremum with positive time offset, a first local extremum with positive time offset, an n^th local extremum with positive time offset, a local extremum with negative time offset, a first local extremum with\nnegative time offset, an n^th local extremum with negative time offset, a constrained (with argument within a constraint) maximum, minimum, constrained maximum, constrained minimum, a constrained extremum, a slope, a derivative, a higher order\nderivative, a maximum slope, a minimum slope, a local maximum slope, a local maximum slope with positive time offset, a local minimum slope, a constrained maximum slope, a constrained minimum slope, a maximum higher order derivative, a minimum higher\norder derivative, a constrained higher order derivative, a zero-crossing, a zero crossing with positive time offset, an n^th zero crossing with positive time offset, a zero crossing with negative time offset, an n^th zero crossing with negative time\noffset, a constrained zero-crossing, a zero-crossing of slope, a zero-crossing of higher order derivative, and/or another characteristics.  At least one argument of the function associated with the at least one characteristics of the function may be\nidentified.  Some quantity (e.g. a spatial-temporal information of the object) may be determined based on the at least one argument of the function.\n A characteristics of a motion of an object in the venue may comprise at least one of: an instantaneous characteristics, a short-term characteristics, repetitive characteristics, a recurring characteristics, a history, an incremental\ncharacteristics, a changing characteristics, a deviational characteristics, a phase, a magnitude, a time characteristics, a frequency characteristics, a time-frequency characteristics, a decomposition characteristics, an orthogonal decomposition\ncharacteristics, a non-orthogonal decomposition characteristics, a deterministic characteristics, a probabilistic characteristics, a stochastic characteristics, an autocorrelation function (ACF), a mean, a variance, a statistics, a duration, a timing, a\ntrend, a periodic characteristics, a long-term characteristics, a historical characteristics, an average characteristics, a current characteristics, a past characteristics, a future characteristics, a predicted characteristics, a location, a distance, a\nheight, a speed, a direction, a velocity, an acceleration, a change of the acceleration, an angle, an angular speed, an angular velocity, an angular acceleration of the object, a change of the angular acceleration, an orientation of the object, an\nangular of a rotation, a deformation of the object, a shape of the object, a change of shape of the object, a change of size of the object, a change of structure of the object, and/or a change of characteristics of the object.\n At least one local maximum and at least one local minimum of the function may be identified.  At least one local signal-to-noise-ratio-like (SNR-like) parameter may be computed for each pair of adjacent local maximum and local minimum.  The\nSNR-like parameter may be a function (e.g. linear, log, exponential function, a monotonic function) of a fraction of a quantity (e.g. power, magnitude, etc.) of the local maximum over the same quantity of the local minimum.  It may also be the function\nof a difference between the quantity of the local maximum and the same quantity of the local minimum.\n Significant local peaks may be identified or selected.  Each significant local peak may be a local maximum with SNR-like parameter greater than a threshold T1 and/or a local maximum with amplitude greater than a threshold T2.\n The at least one local minimum and the at least one local minimum in the frequency domain may be identified/computed using a persistence-based approach.\n A set of selected significant local peaks may be selected from the set of identified significant local peaks based on a selection criterion.  The characteristics/spatial-temporal information of the object may be computed based on the set of\nselected significant local peaks and frequency values associated with the set of selected significant local peaks.\n In one example, the selection criterion may always correspond to select the strongest peaks in a range.  While the strongest peaks may be selected, the unselected peaks may still be significant (rather strong).\n Unselected significant peaks may be stored and/or monitored as \"reserved\" peaks for use in future selection in future sliding time windows.  As an example, there may be a particular peak (at a particular frequency) appearing consistently over\ntime.  Initially, it may be significant but not selected (as other peaks may be stronger).  But in later time, the peak may become stronger and more dominant and may be selected.  When it became \"selected\", it may be back-traced in time and made\n\"selected\" in the earlier time when it was significant but not selected.  In such case, the back-traced peak may replace a previously selected peak in an early time.  The replaced peak may be the relatively weakest, or a peak that appear in isolation in\ntime (i.e. appearing only briefly in time).\n In another example, the selection criterion may not correspond to select the strongest peaks in the range.  Instead, it may consider not only the \"strength\" of the peak, but the \"trace\" of the peak--peaks that may have happened in the past,\nespecially those peaks that have been identified for a long time.\n For example, if a finite state machine (FSM) is used, it may select the peak(s) based on the state of the FSM.  Decision thresholds may be computed adaptively based on the state of the FSM.\n A similarity score may be computed (e.g. by a server, the processor, the Type 1 device, the Type 2 device, a local server, a cloud server, and/or another device) based on a pair of temporally adjacent CI of a time series of CI.  The pair may\ncome from the same sliding window or two different sliding windows.  The similarity score may also be based on a pair of, temporally adjacent or not so adjacent, CI from two different time series of CI.  The similarity score may be or may include: a time\nreversal resonating strength (TRRS), a correlation, a cross-correlation, an auto-correlation, a covariance, a cross-covariance, an auto-covariance, an inner product of two vectors, a distance score, a discrimination score, a metric, a neural network\noutput, a deep learning network output, and/or another score.  The characteristics and/or spatial-temporal information may be determined/computed based on the similarity score.\n Any threshold may be pre-determined, adaptively determined and/or determined by a finite state machine.  The adaptive determination may be based on time, space, location, antenna, path, link, state, battery life, remaining battery life,\navailable power, available computational resources, available network bandwidth, etc.\n A threshold to be applied to a test statistics to differentiate two events (or two conditions, or two situations, or two states), A and B, may be determined.  Data (e.g. CI, channel state information (CSI)) may be collected under A and/or under\nB in a training situation.  The test statistics may be computed based on the data.  Distributions of the test statistics under A may be compared with distributions of the test statistics under B, and the threshold may be chosen according to some\ncriteria.  The criteria may comprise: maximum likelihood (ML), maximum aposterior probability (MAP), discriminative training, minimum type 1 error for a given type 2 error, minimum type 2 error for a given type 1 error, and/or other criteria.  The\nthreshold may be adjusted to achieve different sensitivity to the A, B and/or another event/condition/situation/state.  The threshold adjustment may be automatic, semi-automatic and/or manual.  The threshold adjustment may be applied once, sometimes,\noften, periodically, occasionally, sporadically, and/or on demand.  The threshold adjustment may be adaptive.  The threshold adjustment may depend on the object, an object movement/location/direction/action, an object characteristics/spatial-temporal\ninformation/size/property/trait/habit/behavior, the venue, a feature/fixture/furniture/barrier/material/machine/living thing/thing/object/boundary/surface/medium that is in/at/of the venue, a map, a constraint of the map, the\nevent/state/situation/condition, a time, a timing, a duration, a current state, a past history, a user, and/or a personal preference, etc.\n A stopping criterion of an iterative algorithm may be that change of a current parameter (e.g. offset value) in the updating in an iteration is less than a threshold.  The threshold may be 0.5, 1, 1.5, 2, or another number.  The threshold may be\nadaptive.  It may change as the iteration progresses.  For the offset value, the adaptive threshold may be determined based on the task, particular value of the first time, the current time offset value, the regression window, the regression analysis,\nthe regression function, the regression error, the convexity of the regression function, and/or an iteration number.\n The local extremum may be determined as the corresponding extremum of the regression function in the regression window.  The local extremum may be determined based on a set of time offset values in the regression window and a set of associated\nregression function values.  Each of the set of associated regression function values associated with the set of time offset values may be within a range from the corresponding extremum of the regression function in the regression window.\n The searching for a local extremum may comprise a robust search, a minimization, a maximization, an optimization, a statistical optimization, a dual optimization, a constraint optimization, a convex optimization, a global optimization, a local\noptimization an energy minimization, a linear regression, a quadratic regression, a higher order regression, a linear programming, a nonlinear programming, a stochastic programming, a combinatorial optimization, a constraint programming, a constraint\nsatisfaction, a calculus of variations, an optimal control, a dynamic programming, a mathematical programming, a multi-objective optimization, a multi-modal optimization, a disjunctive programming, a space mapping, an infinite-dimensional optimization, a\nheuristics, a metaheuristics, a convex programming, a semidefinite programming, a conic programming, a cone programming, an integer programming, a quadratic programming, a fractional programming, a numerical analysis, a simplex algorithm, an iterative\nmethod, a gradient descent, a subgradient method, a coordinate descent, a conjugate gradient method, a Newton's algorithm, a sequential quadratic programming, an interior point method, an ellipsoid method, a reduced gradient method, a quasi-Newton\nmethod, a simultaneous perturbation stochastic approximation, an interpolation method, a pattern search method, a line search, a non-differentiable optimization, a genetic algorithm, an evolutionary algorithm, a dynamic relaxation, a hill climbing, a\nparticle swarm optimization, a gravitation search algorithm, a simulated annealing, a memetic algorithm, a differential evolution, a dynamic relaxation, a stochastic tunneling, a Tabu search, a reactive search optimization, a curve fitting, a least\nsquare, a simulation based optimization, a variational calculus, and/or a variant.  The search for a local extremum may be associated with an objective function, a loss function, a cost function, a utility function, a fitness function, an energy\nfunction, and/or an energy function.\n Regression may be performed using a regression function to fit sampled data (e.g. CI, a feature of CI, a component of CI) or another function (e.g. autocorrelation function) in a regression window.  In at least one iteration, a length of the\nregression window and/or a location of the regression window may change.  The regression function may be a linear function, a quadratic function, a cubic function, a polynomial function, and/or another function.\n The regression analysis may minimize an absolute error, a square error, a higher order error (e.g. third order, fourth order, etc.), a robust error (e.g. square error for smaller error magnitude and absolute error for larger error magnitude, or\na first kind of error for smaller error magnitude and a second kind of error for larger error magnitude), another error, a weighted sum of absolute error (e.g. for a wireless transmitter with multiple antennas and a wireless receiver with multiple\nantennas, each pair of transmitter antenna and receiver antenna form a link.  Error associated with different links may have different weights.  One possibility is that some links and/or some components with larger noise may have smaller or bigger\nweight.), a weighted sum of square error, a weighted sum of higher order error, a weighted sum of robust error, a weighted sum of the another error, an absolute cost, a square cost, a higher order cost, a robust cost, another cost, a weighted sum of\nabsolute cost, a weighted sum of square cost, a weighted sum of higher order cost, a weighted sum of robust cost, and/or a weighted sum of another cost.\n The regression error determined may be an absolute error, a square error, a higher order error, a robust error, yet another error, a weighted sum of absolute error, a weighted sum of square error, a weighted sum of higher order error, a weighted\nsum of robust error, and/or a weighted sum of the yet another error.\n The time offset associated with maximum regression error (or minimum regression error) of the regression function with respect to the particular function in the regression window may become the updated current time offset in the iteration.\n A local extremum may be searched based on a quantity comprising a difference of two different errors (e.g. a difference between absolute error and square error).  Each of the two different errors may comprise an absolute error, a square error, a\nhigher order error, a robust error, another error, a weighted sum of absolute error, a weighted sum of square error, a weighted sum of higher order error, a weighted sum of robust error, and/or a weighted sum of the another error.\n The quantity may be compared with an F-distribution, a central F-distribution, another statistical distribution, a threshold, a threshold associated with a probability, a threshold associated with a probability of finding a false peak, a\nthreshold associated with the F-distribution, a threshold associated the central F-distribution, and/or a threshold associated with the another statistical distribution.\n The regression window may be determined based on at least one of: the movement of the object, a quantity associated with the object, the at least one characteristics and/or spatial-temporal information of the object associated with the movement\nof the object, an estimated location of the local extremum, a noise characteristics, an estimated noise characteristics, an F-distribution, a central F-distribution, another statistical distribution, a threshold, a preset threshold, a threshold\nassociated with a probability, a threshold associated with a desired probability, a threshold associated with a probability of finding a false peak, a threshold associated with the F-distribution, a threshold associated the central F-distribution, a\nthreshold associated with the another statistical distribution, a condition that a quantity at the window center is largest within the regression window, a condition that the quantity at the window center is largest within the regression window, a\ncondition that there is only one of the local extremum of the particular function for the particular value of the first time in the regression window, another regression window, and/or another condition.\n The width of the regression window may be determined based on the particular local extremum to be searched.  The local extremum may comprise first local maximum, second local maximum, higher order local maximum, first local maximum with positive\ntime offset value, second local maximum with positive time offset value, higher local maximum with positive time offset value, first local maximum with negative time offset value, second local maximum with negative time offset value, higher local maximum\nwith negative time offset value, first local minimum, second local minimum, higher local minimum, first local minimum with positive time offset value, second local minimum with positive time offset value, higher local minimum with positive time offset\nvalue, first local minimum with negative time offset value, second local minimum with negative time offset value, higher local minimum with negative time offset value, first local extremum, second local extremum, higher local extremum, first local\nextremum with positive time offset value, second local extremum with positive time offset value, higher local extremum with positive time offset value, first local extremum with negative time offset value, second local extremum with negative time offset\nvalue, and/or higher local extremum with negative time offset value.\n A current parameter (e.g. time offset value) may be initialized based on a target value, a target profile, a trend, a past trend, a current trend, a target speed, a speed profile, a target speed profile, a past speed trend, the movement of the\nobject, at least one characteristics and/or spatial-temporal information of the object associated with the movement of object, a positional quantity of the object, an initial speed of the object associated with the movement of the object, a predefined\nvalue, an initial width of the regression window, a time duration, a value based on a carrier frequency of the wireless signal, a bandwidth of the wireless signal, an amount of antennas associated with the wireless multipath channel, a noise\ncharacteristics, and/or an adaptive value.  The current time offset may be at the center, on the left side, on the right side, and/or at another fixed relative location, of the regression window.\n FIG. 1 illustrates an exemplary network environment 100 for event detection and monitoring in a venue, according to one embodiment of the present teaching.  As shown in FIG. 1, the exemplary network environment 100 includes a transmitter 110, an\nantenna 112, a wireless channel 130, an antenna 122, and a receiver 120.  The antenna 112 is electrically coupled to the transmitter 110; the antenna 122 is electrically coupled to the receiver 120.\n In one embodiment, the transmitter 110 is located at a first position in a venue; while the receiver 120 is located at a second position in the venue.  The transmitter 110 is configured for transmitting a wireless signal through the wireless\nchannel 130.  The wireless channel 130 in this example is a wireless multipath channel that is impacted by a motion of an object in the venue.  According to various embodiments, the object may be a human (e.g. a baby 142, or a patient 146) or a pet (e.g.\na puppy 144).  The receiver 120 in this example receives the wireless signal through the wireless multipath channel 130 and obtains at least one time series of channel information (CI) of the wireless multipath channel based on the wireless signal. \nBecause the motion of the object impacts the wireless multipath channel through which the wireless signal is transmitted, the channel information 125 extracted from the wireless signal includes information related to the object motion.\n In various embodiments, the transmitter 110 may be part of a Bot or a Type 1 device placed in a venue, while the receiver 120 may be part of an Origin or a Type 2 device placed in the venue.  In various embodiments, the Bot and/or the Origin may\ninclude (not shown) multiple transmitters, multiple receivers, and/or multiple transceivers.  In one embodiment, the antenna 112 and/or the antenna 122 is replaced with a multi-antenna array that can form a plurality of beams each of which points in a\ndistinct direction.  The transmitter 110 can be configured to wirelessly transmit signals having different types or functions.  Similarly, the receiver 120 is configured to receive wireless signals having different types or functions.  In one embodiment,\nthe transmitter 110 has at least one antenna; and the receiver 120 has at least one antenna.  Each of the at least one time series of CI is associated with one of the at least one antenna of the transmitter 110 and one of the at least one antenna of the\nreceiver 120.\n FIG. 2 illustrates an exemplary diagram of a device 200 in a wireless monitoring system, according to one embodiment of the present teaching.  The device 200 is an example of a device that can be configured to implement the various methods\ndescribed herein.  According to various embodiments, the device may be: a Type 1 device which is a Bot including a transmitter, a Type 2 device which is an Origin including a receiver, an event recognition engine, and/or other components in FIGS. 1 and\n3-18.  As shown in FIG. 2, the device 200 includes a housing 240 containing a processor 202, a memory 204, a transceiver 210 comprising a transmitter 212 and a receiver 214, a synchronization controller 206, a power module 208, and an operation module\n209.\n In this embodiment, the processor 202 controls the general operation of the device 200 and can include one or more processing circuits or modules such as a central processing unit (CPU) and/or any combination of general-purpose microprocessors,\nmicrocontrollers, digital signal processors (DSPs), field programmable gate array (FPGAs), programmable logic devices (PLDs), controllers, state machines, gated logic, discrete hardware components, dedicated hardware finite state machines, or any other\nsuitable circuits, devices and/or structures that can perform calculations or other manipulations of data.\n The memory 204, which can include both read-only memory (ROM) and random access memory (RAM), can provide instructions and data to the processor 202.  A portion of the memory 204 can also include non-volatile random access memory (NVRAM).  The\nprocessor 202 typically performs logical and arithmetic operations based on program instructions stored within the memory 204.  The instructions (a.k.a., software) stored in the memory 204 can be executed by the processor 202 to perform the methods\ndescribed herein.  The processor 202 and the memory 204 together form a processing system that stores and executes software.  As used herein, \"software\" means any type of instructions, whether referred to as software, firmware, middleware, microcode,\netc. which can configure a machine or device to perform one or more desired functions or processes.  Instructions can include code (e.g., in source code format, binary code format, executable code format, or any other suitable format of code).  The\ninstructions, when executed by the one or more processors, cause the processing system to perform the various functions described herein.\n The transceiver 210, which includes the transmitter 212 and receiver 214, allows the device 200 to transmit and receive data to and from a remote device (e.g., an Origin or a Bot).  An antenna 250 is typically attached to the housing 240 and\nelectrically coupled to the transceiver 210.  In various embodiments, the device 200 includes (not shown) multiple transmitters, multiple receivers, and multiple transceivers.  In one embodiment, the antenna 250 is replaced with a multi-antenna array 250\nthat can form a plurality of beams each of which points in a distinct direction.  The transmitter 212 can be configured to wirelessly transmit signals having different types or functions, such signals being generated by the processor 202.  Similarly, the\nreceiver 214 is configured to receive wireless signals having different types or functions, and the processor 202 is configured to process signals of a plurality of different types.\n In one embodiment, the device 200 may be a Bot or an Origin of a wireless monitoring system.  The wireless monitoring system may comprise at least one Bot and at least one Origin.  The synchronization controller 206 in this example may be\nconfigured to control the operations of the device 200 to be synchronized or un-synchronized with another device, e.g. another Origin or another Bot.  In one embodiment, each of the device 200 and other Bots or Origins in the system may transmit or\nreceive the wireless signals individually and asynchronously.\n The operation module 209 in this example may perform one or more operations for event detection and monitoring.  The operation module 209 may comprise one or more sub-modules to implement different methods disclosed herein.  In one embodiment,\nthe device 200 may be an event recognition engine of a wireless monitoring system, where the operation module 209 includes one or more of the components for recognizing and/or classifying an event, e.g. door open, door close, door x % open, or other\nevents related to security.\n The power module 208 can include a power source such as one or more batteries, and a power regulator, to provide regulated power to each of the above-described modules in FIG. 2.  In some embodiments, if the device 200 is coupled to a dedicated\nexternal power source (e.g., a wall electrical outlet), the power module 208 can include a transformer and a power regulator.\n The various modules discussed above are coupled together by a bus system 230.  The bus system 230 can include a data bus and, for example, a power bus, a control signal bus, and/or a status signal bus in addition to the data bus.  It is\nunderstood that the modules of the device 200 can be operatively coupled to one another using any suitable techniques and mediums.\n Although a number of separate modules or components are illustrated in FIG. 2, persons of ordinary skill in the art will understand that one or more of the modules can be combined or commonly implemented.  For example, the processor 202 can\nimplement not only the functionality described above with respect to the processor 202, but also implement the functionality described above with respect to the operation module 209.  Conversely, each of the modules illustrated in FIG. 2 can be\nimplemented using a plurality of separate components or elements.\n In one embodiment, a method of a system having a processor, a memory communicatively coupled with the processor and a set of instructions stored in the memory for recognizing events in a venue is disclosed.  The system comprises a first\ntransmitter, a second transmitter, at least one first receiver, at least one second receiver, and an event recognition engine.  Each of the first transmitter, the second transmitter, the at least one first receiver, the at least one second receiver, and\nthe event recognition engine can be implemented as the device 200.  The method comprises: for each of at least one known event happening in a venue: transmitting, by an antenna of the first transmitter, a respective training wireless signal to the at\nleast one first receiver through a wireless multipath channel in the venue in a training time period associated with the known event, obtaining, asynchronously by each of the at least one first receiver based on the training wireless signal, at least one\ntime series of training channel information (training CI time series) of the wireless multipath channel between the first receiver and the first transmitter in the training time period associated with the known event, and pre-processing the at least one\ntraining CI time series; training, by the event recognition engine, at least one classifier for the at least one known event based on the at least one training CI time series; and for a current event happening in the venue in a current time period,\ntransmitting, by an antenna of the second transmitter, a current wireless signal to the at least one second receiver through the wireless multipath channel impacted by the current event in the venue in the current time period associated with the current\nevent, obtaining, asynchronously by each of the at least one second receiver based on the current wireless signal, at least one time series of current channel information (current CI time series) of the wireless multipath channel between the second\nreceiver and the second transmitter in the current time period associated with the current event, pre-processing the at least one current CI time series, and applying, by the event recognition engine, the at least one classifier to: classify at least one\nof: the at least one current CI time series, a portion of a particular current CI time series, and a combination of the portion of the particular current CI time series and a portion of an additional CI time series, and associate the current event with\nat least one of: a known event, an unknown event and another event.  A training CI time series associated with a first receiver and a current CI time series associated with a second receiver have at least one of: different starting times, different time\ndurations, different stopping times, different counts of items in their respective time series, different sampling frequencies, different sampling periods between two consecutive items in their respective time series, and channel information (CI) with\ndifferent features.\n The venue may be a house to be guarded.  The known events may include: e.g. \"front door open\", \"back door open\", \"all door close\", \"bedroom window open\", etc. The set of devices (the first transmitter, the at least one first receiver) used\nduring training and the set of devices (the second transmitter, the at least one second receiver) used during operation may be the same or different in various situations.  When they are the same, once two devices (a transmitter and a receiver) are\nplugged in or installed, training can be performed and security operation can begin.  In case the devices are moved, another round of training may be performed for security operation.  Pre-processing may include denoising and correction of phase error\nand other errors.  The system may perform monitoring and guard operation using a small section of CI time series (e.g. 0.1 second, or 1 second).  In case a portion of CI contains part of a training motion (e.g. the door moves from \"close\" to 30% open,\nand then does not move), the portion may be combined with a past recording of the door opening from 30% to fully open so that the classification can be applied.  If the CI associated with the current event does not match any of the training CI, the\ncurrent event may be declared as an \"unknown event\".\n In one embodiment, each training CI time series and each current CI time series comprise at least two CI each with an associated time stamp.  Each CI is associated with a respective time stamp.  According to various embodiments, the first\ntransmitter and the second transmitter may be the same device; the first transmitter and the second transmitter may be at a same location in the venue; the at least one first receiver and the at least one second receiver may be the same; the at least one\nfirst receiver may be a permutation of the at least one second receiver; a particular first receiver and a particular second receiver may be the same device; at least one of: the at least one second receiver and a subset of the at least one second\nreceiver, may be a subset of the at least one first receiver; at least one of: the at least one second receiver and a subset of the at least one second receiver, may be a permutation of a subset of the at least one first receiver; at least one of: the at\nleast one first receiver and a subset of the at least one first receiver, may be a subset of the at least one second receiver; at least one of: the at least one first receiver and a subset of the at least one first receiver, may be a permutation of a\nsubset of the at least one second receiver; an antenna of the first transmitter and an antenna of the second transmitter may be at a same location in the venue; at least one of: the at least one second receiver and a subset of the at least one second\nreceiver, may be at a same respective location as a subset of the at least one first receiver; at least one of: the at least one first receiver and a subset of the at least one first receiver, may be at a same respective location as a subset of the at\nleast one second receiver; an antenna of the first transmitter and an antenna of the second transmitter may be at a same location in the venue; at least one of: antennas of the at least one second receiver and antennas of a subset of the at least one\nsecond receiver, may be at same respective locations as respective antennas of a subset of the at least one first receiver; at least one of: antennas of the at least one first receiver and antennas of a subset of the at least one first receiver, may be\nat same respective locations as respective antennas of a subset of the at least one second receiver.\n In one embodiment, the pre-processing comprises at least one of: doing nothing, zero-padding, time-domain processing, frequency domain processing, time-frequency processing, spatially varying processing, temporally varying processing, adaptive\nprocessing, de-noising, smoothing, conditioning, enhancement, restoration, feature extraction, weighted averaging, averaging over antenna links, averaging over selected frequency, averaging over selected components, quantization, vector quantization,\nfiltering, linear filtering, nonlinear filtering, low-pass filtering, bandpass filtering, high-pass filtering, median filtering, ranked filtering, quartile filtering, percentile filtering, mode filtering, linear filtering, nonlinear filtering, finite\nimpulse response (FIR) filtering, infinite impulse response (IIR) filtering, moving average (MA) filtering, auto-regressive (AR) filtering, auto-regressive moving average (ARMA) filtering, thresholding, soft thresholding, hard thresholding, soft\nclipping, local maximization, local minimization, optimization of a cost function, neural network, machine learning, supervised learning, unsupervised learning, semi-supervised learning, transformation, mapping, transform, inverse transform, integer\ntransform, power-of-2 transform, real transform, floating-point transform, fixed-point transform, complex transform, fast transform, Fourier transform, Laplace transform, Hadamard transform, Hilbert transform, sine transform, cosine transform, triangular\ntransform, wavelet transform, transformation, decomposition, selective filtering, adaptive filtering, derivative, first order derivative, second order derivative, higher order derivative, integration, zero crossing, indicator function, absolute\nconversion, convolution, multiplication, division, another transform, another processing, another filter, another function, and another preprocessing.  In one embodiment, the pre-processing comprises at least one of: normalization, temporal\nnormalization, frequency normalization, magnitude correction, phase correction, phase cleaning, cleaning a phase associated with the channel information, normalizing components associated with the channel information, cleaning a phase of frequency\ncomponents of the channel information, normalizing the frequency components, re-sampling, labeling, tagging, training, sorting, grouping, folding, thresholding, matched filtering, spectral analysis, clustering, quantization, vector quantization, time\ncorrection, time base correction, time stamp correction, sampling rate up-conversion/down-conversion, interpolation, intrapolation, extrapolation, subsampling, decimation, compression, expansion, encryption decryption, coding, storing, retrieving,\ntransmitting, receiving, representing, merging, combining, splitting, tracking, monitoring, projection, orthogonal projection, non-orthogonal projection, over-complete projection, decomposition, eigen-decomposition, principal component analysis (PCA),\nsparse approximation, matching pursuit, and another operation etc.\n In one embodiment, the method comprises aligning a first section of a first time duration of a first CI time series (e.g. the training CI time series) and a second section of a second time duration of a second CI time series (e.g. the current CI\ntime series), and computing a map between items of the first section and items of the second section.\n In one embodiment, the first CI time series may be processed by a first operation.  The second CI time series may be processed by a second operation.  At least one of: the first operation and the second operation, to comprise at least one of:\nsubsampling, re-sampling, interpolation, filtering, transformation, feature extraction, pre-processing, and another operation.\n In one embodiment, the method further comprises mapping a first item of the first section to a second item of the second section.  In one embodiment, the method further comprises mapping the first item of the first section also to another item\nof the second section.  In one embodiment, mapping another item of the first section to the second item of the second section.\n In one embodiment, at least one constraint is satisfied by at least one function of at least one of: the first item of the first section of the first CI time series, another item of the first CI time series, a time stamp of the first item, a\ntime difference of the first item, a time differential of the first item, a neighboring time stamp of the first item, another time stamp associated with the first item, the second item of the second section of the second CI time series, another item of\nthe second CI time series, a time stamp of the second item, a time difference of the second item, a time differential of the second item, a neighboring time stamp of the second item, and another time stamp associated with the second item.  In one\nembodiment, one of the at least one constraint is that a difference between the time stamp of the first item and the time stamp of the second item is upper-bounded by an adaptive upper threshold and lower-bounded by an adaptive lower threshold.\n In one embodiment, the first section is the entire first CI time series, and/or the second section is the entire second CI time series.  In one embodiment, the first time duration is not equal to the second time duration.  In one embodiment, the\nmethod further comprises determining a section of a time duration of a CI time series adaptively.\n In one embodiment, the method further comprises determining a starting time and an ending time of a section.  In one embodiment, the method further comprises computing a tentative section of the CI time series, and determining the section by\nremoving a beginning portion and an ending portion of the tentative section.\n In one embodiment, the method further comprises: determining a beginning portion of a tentative section by: considering items of the tentative section with increasing time stamp as a current item iteratively, one item at a time; computing\nrecursively an activity measure associated with at least one of: the current item associated with a current time stamp, past items of the tentative section with time stamps not larger than the current time stamp, and future items of the tentative section\nwith time stamps not smaller than the current time stamp; adding the current item to the beginning portion of the tentative section if a criterion associated with the activity measure is satisfied.\n In one embodiment, the method further comprises: determining an ending portion of a tentative section by: considering items of the tentative section with decreasing time stamp as a current item iteratively, one item at a time; iteratively\ncomputing and determining at least one activity measure associated with at least one of: the current item associated with a current time stamp, past items of the tentative section with time stamps not larger than the current time stamp, and future items\nof the tentative section with time stamps not smaller than the current time stamp; and adding the current item to the ending portion of the tentative section if a criterion associated with the at least one activity measure is satisfied.  In one\nembodiment, the criterion associated with the activity measure comprises at least one of: the activity measure is smaller than an adaptive upper threshold, the activity measure is larger than an adaptive lower threshold, the activity measure is smaller\nthan an adaptive upper threshold consecutively for at least a predetermined amount of consecutive time stamps, the activity measure is larger than an adaptive lower threshold consecutively for at least another predetermined amount of consecutive time\nstamps, the activity measure is smaller than an adaptive upper threshold consecutively for at least a predetermined percentage of the predetermined amount of consecutive time stamps, the activity measure is larger than an adaptive lower threshold\nconsecutively for at least another predetermined percentage of the another predetermined amount of consecutive time stamps, another activity measure associated with another time stamp associated with the current time stamp is smaller than another\nadaptive upper threshold and larger than another adaptive lower threshold, at least one activity measure associated with at least one respective time stamp associated with the current time stamp is smaller than respective upper threshold and larger than\nrespective lower threshold, percentage of time stamps with associated activity measure smaller than respective upper threshold and larger than respective lower threshold in a set of time stamps associated with the current time stamp exceeds a threshold,\nand another criterion.  In one embodiment, the activity measure associated with an item at time T1 to comprise at least one of: a first function of the item at time T1 and an item at time T1-D1, wherein D1 is a pre-determined positive quantity, a second\nfunction of the item at time T1 and an item at time T1+D1, a third function of the item at time T1 and an item at time T2, wherein T2 is a pre-determined quantity, and a fourth function of the item at time T1 and another item.\n In one embodiment, at least one of: the first function, the second function, the third function, and the fourth function, is at least one of: a function F1(x, y, .  . . ) with at least two scalar arguments: x and y, a function F2(X, Y, .  . . )\nwith at least two vector arguments: X and Y, and a function F3(X1, Y1, .  . . ) with at least two arguments: X1 and Y1.  In one embodiment, the function F1 is a function of at least one of the following: x, y, (x-y), (y-x), abs(x-y), x^a1, y^b1,\nabs(x^a1-y^b1), (x-y)^a1, (x/y), (x+a1)/(y+b1), (x^a1/y^b1), and ((x/y)^a1-b1), wherein a1 and b1 are predetermined quantities; both X and Y are n-tuples such that X=(x_1, x_2, .  . . , x_n) and Y=(y_1, y_2, .  . . , y_n); the function F2 is a function\nof at least one of the following: x_i, y_i, (x_i-y_i), (y_i-x_i), abs(x_i-y_i), x_i^a2, y_i^b2, abs(x_i^a2-y_i^b2), (x_i-y_i)^a2, (x_i/y_i), (x_i+a2)/(y_i+b2), (x_i^a2/y_i^b2), and ((x_i/y_i)^a2-b2); i, ranging from 1 to n, is a component index of the\nn-tuples X and Y; both X1 and Y1 are n-tuples comprising N components such that X1=(x1_1, x1_2, .  . . , x1_N) and Y1=(y1_1, y1_2, .  . . , y1_N); the function F3 comprises a component-by-component summation of another function of at least one of the\nfollowing: x1_j, y1_j, (x1_j-y1_j), (y1_j-x1_j), abs(x1_j-y1_j), x1_j^a3, y1_j^b3, abs(x1_j^a3-y1_j^b3), (x1_j-y1_)^a3, (x1_j/y1_j), (x1_j+a3)/(y1_j+b3), (x1_j^a3/y1_j^b3), and ((x1_j/y1_j)^a3-b3); and j, ranging from 1 to N, is a component index of the\nn-tuples X1 and Y1.\n In one embodiment, the method further comprises computing the map using dynamic time warping (DTW).  In one embodiment, the dynamic time warping (DTW) comprises a constraint on at least one of: the map, the items of the first CI time series, the\nitems of the second CI time series, the first time duration, the second time duration, the first section, and the second section.\n In one embodiment, the method further comprises: aligning a first section of a first time duration of a first CI time series and a second section of a second time duration of a second CI time series, computing a map comprising more than one\nlinks between first items of the first section and second items of the second section, each link associating a first item with a first time stamp with a second item with a second time stamp, computing a mismatch cost between the aligned first section and\nthe aligned second section, and applying the at least one classifier based on the mismatch cost.  In one embodiment, the mismatch cost comprises a function of: an item-wise cost between a first item of the first section of the first CI time series and a\nsecond item of the second section of the second CI time series associated with the first item by a link of the map, and a link-wise cost associated with the link of the map.  In one embodiment, the aligned first section and the aligned second section are\nrepresented respectively as a first vector and a second vector, both of same vector length; the mismatch cost to comprise at least one of: an inner product, an inner-product-like quantity, a quantity based on correlation, a quantity based on covariance,\na discriminating score, a distance, a Euclidean distance, an absolute distance, an L_1 distance, an L_2 distance, an L_k distance, a weighted distance, a distance-like quantity and another similarity value, between the first vector and the second vector;\nand the mismatch cost is normalized by the vector length.  In one embodiment, a parameter derived from the mismatch cost between the first section of the first time duration of the first CI time series and the second section of the second time duration\nof the second CI time series is modeled with a statistical distribution; and at least one of: a scale parameter, a location parameter and another parameter, of the statistical distribution is estimated.  In one embodiment, the first section of the first\ntime duration of the first CI time series is a sliding section of the first CI time series, and the second section of the second time duration of the second CI time series is a sliding section of the second CI time series.\n In one embodiment, the method further comprises: applying a first sliding window to the first CI time series and a corresponding second sliding window to the second CI time series, aligning the first sliding window of the first CI time series\nand the corresponding second sliding window of the second CI time series, computing mismatch cost between the aligned first sliding window of the first CI time series and the corresponding aligned second sliding window of the second CI time series, and\nassociating the current event with at least one of: the known event, the unknown event and the another event, based on the mismatch cost.  In one embodiment, the method further comprises: applying the classifier to at least one of: the first section of\nthe first time duration of the first CI time series, and the second section of the second time duration of the second CI time series.  In one embodiment, the method further comprises: applying the at least one classifier to more than one first sections\nof the first CI time series and more than one respective second sections of the second CI time series, obtaining at least one tentative classification results, each tentative classification result being associated with a respective first section and a\nrespective second section, and associating the current event with at least one of: the known event, the unknown event and the another event, based on a largest number of the at least one tentative classification results associated with at least one of:\nthe known event, the unknown event and the another event.  In one embodiment, the method further comprises: associating the current event with at least one of: the known event, the unknown event and the another event, based on the mismatch cost.  In one\nembodiment, the method further comprises: associating the current event with at least one of: the known event, the unknown event and the another event, based on the mismatch cost and additional mismatch cost associated with at least one additional\nsection of the first CI time series and at least one additional section of the second CI time series.\n In one embodiment, the known events comprise at least one of: a door closed event, a door open event, a window closed event, a window open event, a multi-state event, an on-state event, an off-state event, an intermediate state event, a\ncontinuous state event, a discrete state event, a continuous event, a discrete event, a sequential event, a proximity event, a presence event, an absence event, an entrance event, an exit event, a movement event, an approaching event, a receding event, a\nprogressing event, an action event, a factory event, a logistic event, a navigation event, an alignment event, a vehicle event, a parking event, a machine event, a manufacturing event, a robot event, a drone event, a landing event, a take-off event, a\nlaunching event, an accident event, a collision event, an impact event, a striking event, a breaking event, a breakthrough event, an explosive event, an office event, a keyboard event, a computer interface event, a home event, a cleaning event, a hygiene\nevent, a pet event, an insect event, a kitchen event, a cooking event, a meal event, a laundry event, a furniture movement event, a bathroom event, a bedroom event, a living room event, a family room event, a dining room event, a garage event, a foyer\nevent, a staircase event, a basement event, an attic event, a pantry event, a fire event, a water event, a shower event, an air-flow event, a fan event, a heat-related event, a light-related event, a wind event, a sit-down event, a stand-up event, a\nlie-down event, a get-up event, a rolling event, a turning event, a repeated event, an exercise event, a relaxation event, a resting event, a sleeping event, a fall-down event, a staircase event, a party event, a running event, a walking event, a reading\nevent, a musical event, a sound event, an instrument event, a human motion event, a baby event, a child event, an older adult event, a gesture event, a handwriting event, a drawing event, a home event, an office event, a pet event, a sleeping event, a\nhuman-present event, a human-absent event, a sign-of-life-present event, and a sign-of-life-absent event.\n In one embodiment, instead of using the whole CSI (which has many components), the system can train (e.g. by using some training algorithm on some training data) a data reduction scheme (a projection).  For example, one may find a projection to\nreduce a 256-dimension CSI to a 10-dimension feature space.  One training method is principal component analysis (PCA).  In one embodiment, the method further comprises: training a projection for each CI using a dimension reduction method based on the\ntraining CI time series associated with the at least one known event, wherein the dimension reduction method to comprise at least one of: principal component analysis (PCA), PCA with different kernel, independent component analysis (ICA), Fisher linear\ndiscriminant, vector quantization, supervised learning, unsupervised learning, self-organizing maps, auto-encoder, neural network, deep neural network, and another method, applying the projection to all CI, training the at least one classifier of the at\nleast one event based on the projection and the training CI time series associated with the at least one event, and classifying the at least one current CI time series based on the projection and the at least one current CI time series.\n Sometimes the environment is changed over time (e.g. some furniture such as sofa or table is moved) such that the previously trained classifier is no longer appropriate or valid.  Thus retraining is performed.  In other words, the classifier is\nre-trained, adapted, updated, or refreshed.  In one embodiment, the method further comprises: re-training the projection using at least one of: the dimension reduction method, and another dimension reduction method, based on at least one of: the\nprojection before the re-training, the training CI time series, at least one current CI time series obtained as of the re-training of the projection, and additional training CI time series, wherein the another dimension reduction method to comprise at\nleast one of: a simplified dimension reduction method, principal component analysis (PCA), PCA with different kernels, independent component analysis (ICA), Fisher linear discriminant, vector quantization, supervised learning, unsupervised learning,\nself-organizing maps, auto-encoder, neural network, deep neural network, and yet another method; re-training the at least one classifier of the at least one event based on at least one of: the re-trained projection, the training CI time series associated\nwith the at least one events, and at least one current CI time series; and classifying the at least one current CI time series based on the re-trained projection, the re-trained classifier, and the current CI time series.\n The CSI may be preprocessed before projection.  For example, one may retain only the magnitude of each CSI component as the phase tends to be sensitive to noise.  If the CSI has 128 components, the preprocessing would give 128 new components\neach being magnitude of correspond component of the CSI.  In one embodiment, each CI to comprise a vector of complex values; each complex value is preprocessed to give the magnitude of the complex value; each CI is preprocessed to give a vector of\nnon-negative real numbers comprising the magnitude of corresponding complex values.\n During training, some training CI time series may be weighted more.  For example, an important training CI time series may be used more than once in the training, or may have a larger weight in the training cost.  In one embodiment, each\ntraining CI time series is weighted in the training of the projection.  The projection may have N (more than 1) components, e.g. N=5, or 10, or 15, or 20, etc. In one embodiment, the projection comprises more than one projected components.  The\nprojection may have at least one most significant component.  For example, we may use PCA and retain the N components with highest energy.  In one embodiment, the projection comprises at least one projected component that is beneficial for the at least\none classifier.\n In one embodiment, the current section may be 2 seconds long, with 2M current CSI each with corresponding time stamp.  A trained representative section of \"front door open\" may be 3 seconds long, with 3M training CSI (i.e. duration is\ndifferent).  In DTW, one can establish correspondence between time stamps of the 2M current CSI and time stamps of the 3M training CSI.  Then one can compute a mismatch cost between the two sections.  Each mismatch cost is a \"distance\" between a current\nCSI and a training CSI.  To compute the distance, one can compute a CSI-distance between a current CSI and a training CSI, and one can compute the CSI-distance 3M times.  The distance may be normalized by the amount of aligned CSI (3M, which is the\nlargest of the two section length) so that distance associated with different events can be compared.  In one embodiment, the method further comprises: for at least one first section of a first time duration of the current CI time series: for each of the\nat least one known event: determining a respective second section of a respective second time duration of a respective representative training CI time series of the respective event, aligning the first section and the respective second section, and\ncomputing a mismatch cost between the aligned first section and the aligned respective second section; applying the at least one classifier; and obtaining a tentative classification result based on the mismatch costs; and associating the at least one\nsection with at least one of: one of the at least one known event, an unknown event and the another event based on the at least one tentative classification result.\n In one embodiment, a particular classifier associates a first section of the first time duration of the current CI time series to a known event whose respective second section has smallest mismatch cost with the first section.  The classifier\nmay choose the event that gives that smallest (normalized) mismatch cost.  In one embodiment, the method further comprises: computing how many times each known event achieves the smallest mismatch cost; and associating the at least one section with at\nleast one of: a known event that achieves the smallest mismatch cost for the most times, a known event that achieves smallest overall mismatch cost, wherein overall mismatch cost is a weighted average of at least one mismatch cost associated with the at\nleast one first sections, a known event that achieves smallest of another overall cost, and an unknown event.  The associated event is the unknown event in at least one of the following situations: none of the events achieve mismatch cost lower than a\nfirst threshold T1 in a sufficient percentage of the at least one first section; and none of the events achieve an overall mismatch cost lower than a second threshold T2.\n In one embodiment, the trained representative CI time series is trained based on a large number of training CI time series of the event collected during training phase/stage (e.g. the event may be front-door-open).  Distance (mismatch cost)\nbetween CI time series may be defined as follows: DTW is applied to a pair (any pair) of training CI time series (or an \"activity\" or \"significant\" section of it) so that normalized mismatch cost can be computed.  The trained representative may be\nobtained by clustering, discriminative training, or some kind of machine learning.  Important training CI time series may be weighted more than others.  In one embodiment, the trained representative CI time series associated with the known event is\nobtained based on the at least one training CI time series associated with the known event.  In one embodiment, the trained representative CI time series associated with the known event is one of the at least one training CI time series associated with\nthe known event.\n In one embodiment, the aggregate mismatch associated with a training data point (a training CI time series) among a set of training data points (e.g. a set of training CI time series for \"front door open\") is a measure of how good it is to\nrepresent a set of training data points.  The aggregate mismatch can be an average (or weighted average, or other function of) distance between the training data point and each of the other training data point in the set.  A good representative data\npoint is close to the other data points.  Training representative CI time series is one of the training CI time series.  In one embodiment, the trained representative CI time series associated with the known event is a particular one of the at least one\ntraining CI time series associated with the known event such that it has smallest aggregate mismatch among the at least one training CI time series; the aggregate mismatch of a particular training CI time series is a function of at least one mismatch\ncost between the particular training CI time series and each of the rest of the at least one training CI time series aligned with the particular training CI time series; and the function to comprise at least one of: average, weighed average, mean,\ntrimmed mean, median, mode, arithmetic mean, geometric mean, harmonic mean, truncated mean, generalized mean, power mean, f-mean, interquartile mean, and another mean.\n In one embodiment, training representative CI time series is not one of the training CI time series.  It is computed by minimizing aggregate mismatch.  Its duration (length) needs to be determined.  In one embodiment, a particular trained\nrepresentative CI time series associated with a particular known event has a particular time duration; the particular trained representative CI time series of the particular time duration is aligned with each of the at least one training CI time series\nassociated with the known event and respective mismatch cost is computed; the particular trained representative CI time series to minimize an aggregate mismatch with respect to the at least one training CI time series; the aggregate mismatch of a CI time\nseries is a function of at least one mismatch cost between the CI time series and each of at least one training CI time series aligned with the CI time series; and the function to comprise at least one of: average, weighed average, mean, trimmed mean,\nmedian, mode, arithmetic mean, geometric mean, harmonic mean, truncated mean, generalized mean, power mean, f-mean, interquartile mean, and another mean.\n In one embodiment, even the duration/length of the representative CI time series (section) is obtained by training.  Different duration can be checked (trial computed).  Different duration of the section can be examined.  In one embodiment, the\nparticular time duration minimizes the aggregate mismatch among more than one candidate time durations of the particular trained representative CI time series.  In one embodiment, for each of at least one candidate time duration, the optimal trained\nrepresentative CI time series with the candidate time duration is computed; the particular time duration is chosen as the candidate time duration that gives minimal normalized aggregate mismatch; and the normalized aggregated mismatch associated with a\ncandidate time duration is the respective aggregate mismatch normalized by the candidate time duration.\n In another embodiment, the particular time duration is obtained by performing a search among the at least one candidate time durations with the cost function being at least one of: normalized aggregate mismatch, hybrid normalized aggregate\nmismatch, combination normalized aggregate match, a simplified mismatch, and another cost function; the normalized aggregated mismatch associated with a candidate time duration is the respective aggregate mismatch normalized by the candidate time\nduration; and the search to comprise at least one of: brute force exhaustive search, gradient descent, steepest descent, stochastic search, genetic search, predictive search, local search, multi-resolution search, hierarchical search, constrained search,\nunconstrained search, fast search, simplified search, and another search.  In one embodiment, the particular time duration and the particular trained representative CI time series with the particular time duration are computed iteratively; a current time\nduration is initialized as one of the candidate time durations; for the current time duration, a current optimal trained representative CI time series with the current time duration is computed; and for the current optimal trained representative CI time\nseries with the current time duration, the current time duration is changed to give smaller normalized aggregate mismatch.\n The initial duration/length of current section of the current CI time series (section) may be equal to a \"typical\" duration of the event (e.g. \"front-door-open\").  In one embodiment, the current time duration is initialized with a value based on\ndurations associated with the at least one training CI time series associated with the known event.  The initial duration/length of current section of the current CI time series (section) may be computed based on duration of training CI time series of\nthe event (e.g. \"front-door-open\").  In one embodiment, the current time duration is initialized with a value based on durations associated with the at least one training CI time series associated with the known event.\n In one embodiment, each pair of transmitting antenna and receiving antenna corresponds to a current CI time series.  For example, each current CI time series is associated with an antenna of the second transmitter and an antenna of a second Type\n2 heterogeneous wireless device.\n FIG. 3 illustrates an exemplary flow of detecting indoor events using time-reversal technology, according to one embodiment of the present teaching.  In one embodiment, the present teaching discloses methods, apparatus, devices, and software of\na wireless monitoring system comprising training at least one classifier of at least one known events in a venue based on training CI time series associated with the at least one events.  For each of the at least one known event happening in the venue in\na respective training time period associated with the known event, a respective series of training probe signals is transmitted by an antenna of a first wireless device using a processor, a memory and a set of instructions of the first wireless device to\nat least one heterogeneous first target wireless receiver through a wireless multipath channel in the venue in the respective training time period.  At least one respective time series of training channel information (training CI time series) is obtained\nasynchronously by each of the at least one heterogeneous first target wireless receiver from the (respective) series of training probe signals.  The channel information (CI) is CI of the wireless multipath channel between the heterogeneous first target\nwireless receiver and the first wireless device in the training time period associated with the known event.  The at least one training CI time series is preprocessed.  For a current event happening in the venue in a current time period, a series of\ncurrent probe signals is transmitted by an antenna of a second wireless device using a processor, a memory and a set of instructions of the second wireless device to at least one heterogeneous second target wireless receiver through the wireless\nmultipath channel impacted by the current event in the venue in the current time period associated with the current event.  At least one time series of current channel information (current CI time series) is obtained asynchronously by each of the at\nleast one heterogeneous second target wireless receiver from the series of current probe signals.  The channel information (CI) is CI of the wireless multipath channel between the heterogeneous second target wireless receiver and the second wireless\ndevice in the current time period associated with the current event.  The at least one current CI time series is preprocessed.\n The at least one classifier is applied to classify at least one current CI time series obtained from the series of current probe signals by the at least one heterogeneous second target wireless receiver, to classify at least one portion of a\nparticular current CI time series, and/or to classify a combination of the at least one portion of the particular current CI time series and another portion of another CI time series.  The at least one classifier is also applied to associate the current\nevent with a known event, an unknown event and/or another event.  Each series of probe signals may comprise at least two CI each with an associated time stamp.  Each CI may be associated with a respective time stamp.  A current CI time series associated\nwith a heterogeneous second target wireless receiver and another current CI time series associated with another heterogeneous second target wireless receiver may have different starting time, different time duration, different stopping time, different\ncount of items in the time series, different sampling frequency, different sampling period between two consecutive items in the time series, and/or channel information (CI) with different features.\n The first wireless device and the second wireless device may be the same device.  The first wireless device and the second wireless device may be at same location in the venue.  The at least one heterogeneous first target wireless receiver and\nthe at least one heterogeneous second target wireless receiver may be the same.  The at least one heterogeneous first target wireless receiver may be a permutation of the at least one heterogeneous second target wireless receiver.  A particular\nheterogeneous first target wireless receiver and a particular heterogeneous second target wireless receiver may be the same device.  The at least one heterogeneous first target wireless receiver and/or a subset of the at least one heterogeneous first\ntarget wireless receiver may be a subset of the at least one heterogeneous second target wireless receiver.  The at least one heterogeneous second target wireless receiver and/or a subset of the at least one heterogeneous second target wireless receiver\nmay be a subset of the at least one heterogeneous first target wireless receiver.\n The at least one heterogeneous first target wireless receiver and/or a subset of the at least one heterogeneous first target wireless receiver may be a permutation of a subset of the at least one heterogeneous second target wireless receiver. \nThe at least one heterogeneous second target wireless receiver and/or a subset of the at least one heterogeneous second target wireless receiver may be a permutation of a subset of the at least one heterogeneous first target wireless receiver.  The at\nleast one heterogeneous second target wireless receiver and/or a subset of the at least one heterogeneous second target wireless receiver may be at same respective location as a subset of the at least one heterogeneous first target wireless receiver. \nThe at least one heterogeneous first target wireless receiver and/or a subset of the at least one heterogeneous first target wireless receiver may be at same respective location as a subset of the at least one heterogeneous second target wireless\nreceiver.\n The antenna of the first wireless device and the antenna of the second wireless device may be at same location in the venue.  Antenna(s) of the at least one heterogeneous second target wireless receiver and/or antenna(s) of a subset of the at\nleast one heterogeneous second target wireless receiver may be at same respective location as respective antenna(s) of a subset of the at least one heterogeneous first target wireless receiver.  Antenna(s) of the at least one heterogeneous first target\nwireless receiver and/or antenna(s) of a subset of the at least one heterogeneous first target wireless receiver may be at same respective location(s) as respective antenna(s) of a subset of the at least one heterogeneous second target wireless receiver.\n The pre-processing may comprise at least one of: doing nothing, zero-padding, time-domain processing, frequency domain processing, time-frequency processing, spatially varying processing, temporally varying processing, adaptive processing,\nde-noising, smoothing, conditioning, enhancement, restoration, feature extraction, weighted averaging, averaging over antenna links, averaging over selected frequency, averaging over selected components, quantization, vector quantization, filtering,\nlinear filtering, nonlinear filtering, low-pass filtering, bandpass filtering, high-pass filtering, median filtering, ranked filtering, quartile filtering, percentile filtering, mode filtering, linear filtering, nonlinear filtering, finite impulse\nresponse (FIR) filtering, infinite impulse response (IIR) filtering, moving average (MA) filtering, auto-regressive (AR) filtering, auto-regressive moving average (ARMA) filtering, thresholding, soft thresholding, hard thresholding, soft clipping, local\nmaximization, local minimization, optimization of a cost function, neural network, machine learning, supervised learning, unsupervised learning, semi-supervised learning, transformation, mapping, transform, inverse transform, integer transform,\npower-of-2 transform, real transform, floating-point transform, fixed-point transform, complex transform, fast transform, Fourier transform, Laplace transform, Hadamard transform, Hilbert transform, sine transform, cosine transform, triangular transform,\nwavelet transform, transformation, decomposition, selective filtering, adaptive filtering, derivative, first order derivative, second order derivative, higher order derivative, integration, zero crossing, indicator function, absolute conversion,\nconvolution, multiplication, division, another transform, another processing, another filter, another function, and/or another preprocessing.  The pre-processing may also comprise at least one of: normalization, temporal normalization, frequency\nnormalization, magnitude correction, phase correction, phase cleaning, cleaning a phase associated with the channel information, normalizing components associated with the channel information, cleaning a phase of frequency components of the channel\ninformation, normalizing the frequency components, re-sampling, labeling, tagging, training, sorting, grouping, folding, thresholding, matched filtering, spectral analysis, clustering, quantization, vector quantization, time correction, time base\ncorrection, time stamp correction, sampling rate up-conversion/down-conversion, interpolation, intrapolation, extrapolation, subsampling, decimation, compression, expansion, encryption decryption, coding, storing, retrieving, transmitting, receiving,\nrepresenting, merging, combining, splitting, tracking, monitoring, projection, orthogonal projection, non-orthogonal projection, over-complete projection, decomposition, eigen-decomposition, principal component analysis (PCA), sparse approximation,\nmatching pursuit, and/or another operation, etc.\n A first section of a first time duration of the first CI time series and a second section of a second time duration of the second section of the second CI time series may be aligned.  A map between items of the first section and items of the\nsecond section may be computed.  The first CI time series may be processed by a first operation.  The second CI time series may be processed by a second operation.  The first operation and/or the second operation may comprise at least one of:\nsubsampling, re-sampling, interpolation, filtering, transformation, feature extraction, pre-processing, and/or another operation.\n A first item of the first section may be mapped to a second item of the second section.  The first item of the first section may also be mapped to another item of the second section.  Another item of the first section may also be mapped to the\nsecond item of the second section.  At least one function of at least one of: the first item of the first section of the first CI time series, another item of the first CI time series, a time stamp of the first item, a time difference of the first item,\na time differential of the first item, a neighboring time stamp of the first item, another time stamp associated with the first item, the second item of the second section of the second CI time series, another item of the second CI time series, a time\nstamp of the second item, a time difference of the second item, a time differential of the second item, a neighboring time stamp of the second item, and another time stamp associated with the second item, may satisfy at least one constraint.  One\nconstraint may be that a difference between the time stamp of the first item and the time stamp of the second item may be upper-bounded by an adaptive upper threshold and lower-bounded by an adaptive lower threshold.  The first section may be the entire\nfirst CI time series.  The second section may be the entire second CI time series.  The first time duration may be equal to the second time duration.\n A section of a time duration of a CI time series may be determined adaptively.  A tentative section of the CI time series may be computed.  A starting time and an ending time of a section (e.g. the tentative section, the section) may be\ndetermined.  The section may be determined by removing a beginning portion and an ending portion of the tentative section.  A beginning portion of a tentative section may be determined as follows.  Iteratively, items of the tentative section with\nincreasing time stamp may be considered as a current item, one item at a time.  In each iteration, at least one activity measure may be computed and/or considered.  The at least one activity measure may be associated with at least one of: the current\nitem associated with a current time stamp, past items of the tentative section with time stamps not larger than the current time stamp, and/or future items of the tentative section with time stamps not smaller than the current time stamp.  The current\nitem may be added to the beginning portion of the tentative section if at least one criterion associated with the at least one activity measure is satisfied.  The at least one criterion associated with the activity measure may comprise at least one of:\n(a) the activity measure is smaller than an adaptive upper threshold, (b) the activity measure is larger than an adaptive lower threshold, (c) the activity measure is smaller than an adaptive upper threshold consecutively for at least a predetermined\namount of consecutive time stamps, (d) the activity measure is larger than an adaptive lower threshold consecutively for at least another predetermined amount of consecutive time stamps, (e) the activity measure is smaller than an adaptive upper\nthreshold consecutively for at least a predetermined percentage of the predetermined amount of consecutive time stamps, (f) the activity measure is larger than an adaptive lower threshold consecutively for at least another predetermined percentage of the\nanother predetermined amount of consecutive time stamps, (g) another activity measure associated with another time stamp associated with the current time stamp is smaller than another adaptive upper threshold and larger than another adaptive lower\nthreshold, (h) at least one activity measure associated with at least one respective time stamp associated with the current time stamp is smaller than respective upper threshold and larger than respective lower threshold, (i) percentage of time stamps\nwith associated activity measure smaller than respective upper threshold and larger than respective lower threshold in a set of time stamps associated with the current time stamp exceeds a threshold, and (j) another criterion.\n An activity measure associated with an item at time T1 may comprise at least one of: (1) a first function of the item at time T1 and an item at time T1-D1, wherein D1 is a pre-determined positive quantity (e.g. a constant time offset), (2) a\nsecond function of the item at time T1 and an item at time T1+D1, (3) a third function of the item at time T1 and an item at time T2, wherein T2 is a pre-determined quantity (e.g. a fixed initial reference time; T2 may be changed over time; T2 may be\nupdated periodically; T2 may be the beginning of a time period and T1 may be a sliding time in the time period), and (4) a fourth function of the item at time T1 and another item.\n At least one of: the first function, the second function, the third function, and/or the fourth function may be a function (e.g. F(X, Y, .  . . )) with at least two arguments: X and Y. The function (e.g. F) may be a function of at least one of:\nX, Y, (X-Y), (Y-X), abs(X-Y), X^a, Y^b, abs(X^a-Y^b), (X-Y)^a, (X/Y), (X+a)/(Y+b), (X^a/Y^b), and ((X/Y)^a-b), wherein a and b may be some predetermined quantities.  For example, the function may simply be abs(X-Y), or (X-Y)^2, (X-Y)^4.  The function may\nbe a robust function.  For example, the function may be (X-Y)^2 when abs (X-Y) is less than a threshold T, and (X-Y)+a when abs(X-Y) is larger than T. Alternatively, the function may be a constant when abs(X-Y) is larger than T. The function may also be\nbounded by a slowly increasing function when abs(X-y) is larger than T, so that outliers cannot severely affect the result.  Another example of the function may be (abs(X/Y)-a), where a=1.  In this way, if X=Y (i.e. no change or no activity), the\nfunction will give a value of 0.  If X is larger than Y, (X/Y) will be larger than 1 (assuming X and Y are positive) and the function will be positive.  And if X is less than Y, (X/Y) will be smaller than 1 and the function will be negative.  In another\nexample, both X and Y may be n-tuples such that X=(x_1, x_2, .  . . , x_n) and Y=(y_1, y_2, .  . . , y_n).  The function may be a function of at least one of: x_i, y_i, (x_i-y_i), (y_i-x_i), abs(x_i-y_i), x_i^a, y_i^b, abs(x_i^a-y_i^b), (x_i-y_i)^a,\n(x_i/y_i), (x_i+a)/(y_i+b), (x_i^a/y_i^b), and ((x_i/y_i)^a-b), wherein i is a component index of the n-tuple X and Y, and 1&lt;=i&lt;=n. E.g. component index of x_1 is i=1, component index of x_2 is i=2.  The function may comprise a\ncomponent-by-component summation of another function of at least one of the following: x_i, y_i (x_i-y_i), (y_i-x_i), abs(x_i-y_i), x_i^a, y_i^b, abs(x_i^a-y_i^b), (x_i-y_i)^a, (x_i/y_i), (x_i+a)/(y_i+b), (x_i^a/y_i^b), and ((x_i/y_i)^a-b), wherein i is\nthe component index of the n-tuple X and Y. For example, the function may be in a form of sum_{i=1}^n (abs(x_i/y_i)-1)/n, or sum_{i=1}^n w_i*(abs(x^i/y^i)-1), where w_i is some weight for component i.\n The map may be computed using dynamic time warping (DTW).  The DTW may comprise a constraint on at least one of: the map, the items of the first CI time series, the items of the second CI time series, the first time duration, the second time\nduration, the first section, and/or the second section.  Suppose in the map, the i^{th} domain item is mapped to the j^{th} range item.  The constraint may be on admissible combination of i and j (constraint on relationship between i and j).  Pairwise\nmismatch between a first section of a first time duration of a first CI time series and a second section of a second time duration of a second CI time series may be computed.  The first section and the second section may be aligned such that a map\ncomprising more than one links may be established between first items of the first CI time series and second items of the second CI time series.  With each link, one of the first items with a first time stamp may be associated with one of the second\nitems with a second time stamp.\n A mismatch cost between the aligned first section and the aligned second section may be computed.  The mismatch cost may comprise a function of: an item-wise mismatch cost between a first item and a second item associated by a particular link of\nthe map, and a link-wise cost associated with the particular link of the map.  The aligned first section and the aligned second section may be represented respectively as a first vector and a second vector of same vector length.  The mismatch cost may\ncomprise at least one of: an inner product, an inner-product-like quantity, a quantity based on correlation, a quantity based on covariance, a discriminating score, a distance, a Euclidean distance, an absolute distance, an Lk distance (e.g. L1, L2, .  .\n. ), a weighted distance, a distance-like quantity and/or another similarity value, between the first vector and the second vector.  The mismatch cost may be normalized by the respective vector length.  A parameter derived from the pairwise mismatch\nbetween the first section of the first time duration of the first CI time series and the second section of the second time duration of the second CI time series may be modeled with a statistical distribution.  At least one of: a scale parameter, a\nlocation parameter and/or another parameter, of the statistical distribution may be estimated.\n The first section of the first time duration of the first CI time series may be a sliding section of the first CI time series.  The second section of the second time duration of the second CI time series may be a sliding section of the second CI\ntime series.  A first sliding window may be applied to the first CI time series and a corresponding second sliding window may be applied to the second CI time series.  The first sliding window of the first CI time series and the corresponding second\nsliding window of the second CI time series may be aligned.\n Pairwise mismatch between the aligned first sliding window of the first CI time series and the corresponding aligned second sliding window of the second CI time series may be computed.  The current event may be associated with at least one of:\nthe known event, the unknown event and/or the another event, based on the pairwise mismatch.  The classifier may be applied to at least one of: the first section of the first time duration of the first CI time series, and/or the second section of the\nsecond time duration of the second CI time series.\n The current event may be associated with at least one of: the known event, the unknown event and/or the another event, based on a largest number of tentative classification results in more than one sections of the first CI time series and\ncorresponding more than sections of the second CI time series.  For example, the current event may be associated with a particular known event if the pairwise mismatch points to the particular known event for N consecutive times (e.g. N=10).  In another\nexample, the current event may be associated with a particular known event if the percentage of pairwise mismatch within the immediate past N consecutive N pointing to the particular known event exceeds a certain threshold (e.g. &gt;80%).\n The current event may be associated with at least one of: the known event, the unknown event and/or the another event, based on the mismatch cost.  The current event may be associated with at least one of: the known event, the unknown event\nand/or the another event, based on the mismatch cost and additional mismatch cost associated with at least one additional section of the first CI time series and at least one additional section of the second CI time series.\n The known events may comprise at least one of: a door closed event, a door open event, a window closed event, a window open event, a multi-state event, an on-state event, an off-state event, an intermediate state event, a continuous state event,\na discrete state event, a human-present event, a human-absent event, a sign-of-life-present event, and/or a sign-of-life-absent event.\n A projection for each channel information may be trained using a dimension reduction method based on the training CI time series.  The dimension reduction method may comprise at least one of: principal component analysis (PCA), PCA with\ndifferent kernel, independent component analysis (ICA), Fisher linear discriminant, vector quantization, supervised learning, unsupervised learning, self-organizing maps, auto-encoder, neural network, deep neural network, and/or another method.  The\nprojection may be applied to at least one of: the training CI time series associated with the at least one event, and/or the current CI time series, for the at least one classifier.\n The at least one classifier of the at least one event may be trained based on the projection and the training CI time series associated with the at least one event.  The at least one current CI time series may be classified based on the\nprojection and the current CI time series.  The projection may be re-trained using at least one of: the dimension reduction method, and another dimension reduction method, based on at least one of: the projection before the re-training, the training CI\ntime series, at least one current CI time series before retraining the projection, and/or additional training CI time series.\n The another dimension reduction method may comprise at least one of: a simplified dimension reduction method, principal component analysis (PCA), PCA with different kernels, independent component analysis (ICA), Fisher linear discriminant,\nvector quantization, supervised learning, unsupervised learning, self-organizing maps, auto-encoder, neural network, deep neural network, and/or yet another method.\n The at least one classifier of the at least one event may be re-trained based on at least one of: the re-trained projection, the training CI time series associated with the at least one events, and/or at least one current CI time series.  The at\nleast one current CI time series may be classified based on: the re-trained projection, the re-trained classifier, and/or the current CI time series.  Each CI may comprise a vector of complex values.  Each complex value may be preprocessed to give the\nmagnitude of the complex value.  Each CI may be preprocessed to give a vector of non-negative real numbers comprising the magnitude of corresponding complex values.  Each training CI time series may be weighted in the training of the projection.  The\nprojection may comprise more than one projected components.  The projection may comprise at least one most significant projected component.  The projection may comprise at least one projected component that may be beneficial for the at least one\nclassifier.\n A particular classifier may be configured to compute, for a first section of a first time duration of the current CI time series and for each of the at least one known event, pairwise mismatch between the first section of the first time duration\nof the current CI time series and a second section of a second time duration of a trained representative CI time series associated with the known event.  The particular classifier may associate the first section of the first time duration of the current\nCI time series to a known event whose second section has smallest pairwise mismatch with the first section.  The particular classifier may associate the first section of the first time duration of the current CI time series to a known event whose second\nsection has smallest pairwise mismatch with the first section, if a previous first section of another first time duration of the current CI time series has smallest pairwise mismatch with another second section of another second time duration of the\ntrained CI time series associated with the known event.\n The trained representative CI time series associated with the known event may be obtained based on the at least one training CI time series associated with the known event.  The trained representative CI time series associated with the known\nevent may be one of the at least one training CI time series associated with the known event.  The trained representative CI time series associated with the known event may be a particular one of the at least one training CI time series associated with\nthe known event such that it has smallest aggregate mismatch compared with the at least one training CI time series.\n The aggregate mismatch may be a function of at least one pairwise mismatch between the particular one and each of the rest of the at least one training CI time series.  The function may comprise at least one of: average, weighed average, mean,\ntrimmed mean, median, mode, arithmetic mean, geometric mean, harmonic mean, truncated mean, generalized mean, power mean, f-mean, interquartile mean, and/or another mean.\n A particular trained representative CI time series associated with a particular known event may have a particular time duration.  The particular trained representative CI time series of the particular time duration may be aligned with each of\nthe at least one training CI time series associated with the known event.  Respective pairwise mismatch may be computed.  The particular trained representative CI time series may minimize an aggregate mismatch with respect to the at least one training CI\ntime series.  The aggregate mismatch of a CI time series may be a function of at least one pairwise mismatch between the CI time series and each of the at least one training CI time series aligned with the CI time series.  The particular time duration\nmay minimize the aggregate mismatch among more than one candidate time durations of the particular trained representative CI time series.  The function may comprise at least one of: average, weighed average, mean, trimmed mean, median, mode, arithmetic\nmean, geometric mean, harmonic mean, truncated mean, generalized mean, power mean, f-mean, interquartile mean, and/or another mean.\n At least one of: the aggregate mismatch and/or each pairwise mismatch, may be normalized, so that the aggregate mismatch of each of the at least one training CI time series can be compared in search of the one with minimum aggregate mismatch. \nThe particular time duration may minimize the aggregate mismatch among more than one candidate time durations of the particular trained representative CI time series.\n For each of at least one candidate time duration, the optimal trained representative CI time series with the candidate time duration may be computed.  The particular time duration may be chosen as the candidate time duration that gives minimal\nnormalized aggregate mismatch.  The normalized aggregated mismatch associated with a candidate time duration may be the respective aggregate mismatch normalized by the candidate time duration.\n The particular time duration may be obtained by performing a search among the at least one candidate time durations with the cost function being at least one of: normalized aggregate mismatch, hybrid normalized aggregate mismatch, combination\nnormalized aggregate match, a simplified mismatch, and/or another cost function.  The normalized aggregated mismatch associated with a candidate time duration may be the respective aggregate mismatch normalized by the candidate time duration.  The search\nmay comprise at least one of: brute force exhaustive search, gradient descent, steepest descent, stochastic search, genetic search, predictive search, local search, multi-resolution search, hierarchical search, constrained search, unconstrained search,\nfast search, simplified search, and/or another search.\n The particular time duration and the particular trained representative CI time series with the particular time duration may be computed iteratively.  A current time duration may be initialized as one of the candidate time durations.  For the\ncurrent time duration, a current optimal trained representative CI time series with the current time duration may be computed.  For the current optimal trained representative CI time series with the current time duration, the current time duration may be\nchanged to give smaller normalized aggregate mismatch.  The current time duration may be initialized with a value based on durations associated with the at least one training CI time series associated with the known event.  The current time duration may\nbe initialized with a value based on durations associated with the at least one training CI time series associated with the known event.\n At least one of: the first wireless device, and/or the first target wireless receiver, may comprise at least two antennas.  Each pair of first wireless device antenna and first target wireless receiver antenna may form a link between the first\nwireless device and the first target wireless receiver.  At least two links may be formed between the first wireless device and the first target wireless receiver.  Each link may correspond to a current CI time series.\n In one embodiment, the at least one time series of training channel information (CI) of the wireless multipath channel may be obtained from a second wireless signal sent through the wireless multipath channel in a training phase.  The wireless\nmultipath channel may be impacted by a training movement of a second object in the training phase.  The training phase may be a training session, which may be carried out once, occasionally, regularly, and/or on demand.\n At least one time series of first training channel information of the wireless multipath channel associated with a target positive training movement of the second object in the training phase may be obtained.  The positive training movement may\nbe a target movement to be recognized, monitored, measured, studied, processed, detected, estimated, verified, and/or captured.\n At least one time series of second training channel information of the wireless multipath channel associated with a target negative training movement of the second object in the training phase may be obtained.  The negative training movement may\nbe a target movement to be ignored, missed, not monitored, not detected, not estimated, not recognized, not verified, not captured, not measured, and/or not studied.\n At least one first quantity from the at least one time series of first training channel information and/or at least one second quantity from the at least one time series of second training channel information may be computed.  The at least one\nfirst quantity and/or the at least one second quantity may comprise a motion statistics, a location statistics, a map coordinate statistics, a height statistics, a speed statistics, an acceleration statistics, a movement angle statistics, a rotation\nstatistics, a size statistics, a volume statistics, a time trend, a time trend statistics, a time profile statistics, a periodic motion statistics, a frequency statistics, a transient statistics, a breathing statistics, a gait statistics, an action\nstatistics, an event statistics, a suspicious event statistics, a dangerous event statistics, an alarming event statistics, a warning statistics, a belief statistics, a proximity statistics, a collision statistics, a power statistics, a signal\nstatistics, a signal power statistics, a signal strength statistics, a received signal strength indicator (RSSI), a signal amplitude, a signal phase, a signal frequency component, a signal frequency band component, a channel state information (CSI), a\nCSI statistics, a map statistics, a time statistics, a frequency statistics, a time-frequency statistics, a decomposition statistics, a orthogonal decomposition statistics, a non-orthogonal decomposition statistics, a tracking statistics, a breathing\nstatistics, a heartbeat statistics, a biometric statistics, a baby statistics, a patient statistics, a machine statistics, a device statistics, a temperature statistics, a vehicle statistics, a parking lot statistics, a venue statistics, a lift\nstatistics, an elevator statistics, a spatial statistics, a road statistics, a fluid flow statistics, a home statistics, a room statistics, an office statistics, a house statistics, a building statistics, a warehouse statistics, a storage statistics, a\nsystem statistics, a ventilation statistics, a fan statistics, a pipe statistics, a duct statistics, a people statistics, a human statistics, a car statistics, a boat statistics, a truck statistics, an airplane statistics, a drone statistics, a downtown\nstatistics, a crowd statistics, an impulsive event statistics, a cyclo-stationary statistics, an environment statistics, a vibration statistics, a material statistics, a surface statistics, a 3-dimensional statistics, a 2-dimensional statistics, a local\nstatistics, a global statistics, a presence statistics, and/or another statistics.\n The at least one threshold may be determined based on the at least one first quantity and/or the at least one second quantity.  The at least one threshold may be determined such that a first percentage of the first quantity is larger than, equal\nto and/or less than a first threshold (not the at least one threshold).  The at least one threshold may be determined such that a second percentage of the second quantity is larger than, equal to and/or less than a second threshold (not the at least one\nthreshold).\n The first threshold may be greater than, equal to and/or less than the second threshold.  The first threshold may be the second threshold.  The first percentage may be greater than, equal to and/or less than the second percentage.\n The at least one time series of first training channel information of the wireless multipath channel may be associated with a training movement of the second object inside a monitored area in the training phase.  The target positive training\nmovement of the second object may be the training movement of the second object inside the monitored area.\n The at least one time series of second training channel information of the wireless multipath channel may be associated with a training movement of the second object outside the monitored area in the training phase.  The target negative training\nmovement of the second object may be the training movement of the second object outside the monitored area.\n The second object may be the first object.  The second object may be an imitation, a replacement, a backup, and/or a replica of the first object.  The second object may be another object similar to the first object.  The second object may be\nsimilar to the first object in terms of structure, size, shape, functionality, periodicity, deformation characteristics, motion characteristics, speed, acceleration, gait, trend, habit, wireless properties, and other characteristics.\n In one embodiment, the present teaching discloses a method, apparatus, and a system for sleep monitoring.  The disclosed method comprises: obtaining a time series of channel information (CI) of a wireless multipath channel using a processor, a\nmemory communicatively coupled with the processor and a set of instructions stored in the memory, and monitoring the sleep-related motion of the user based on the time series of CI.\n The time series of CI is extracted from a wireless signal transmitted between a Type 1 heterogeneous wireless device and a Type 2 heterogeneous wireless device in a venue through the wireless multipath channel.  The wireless multipath channel is\nimpacted by a sleep-related motion of a user in the venue.\n Monitoring the sleep-related motion comprises monitoring at least one of the following of the user: sleep timings, sleep durations, sleep stages, sleep quality, sleep apnea, sleep problems, sleep disorders, breathing problems, gasping, choking,\nteeth-grinding, pause of sleep, absence of sleep, insomnia, restlessness during sleep, hypersomnia, parasomnia, day-time sleepiness, sleep locations, sleep-while-driving, sleep disruptions, nightmares, night terrors, sleep walking, REM sleep behavior\ndisorder, Circadian rhythm disorder, non-24-hour sleep-wake disorder, periodic limb movement disorder, shift-work sleep disorder, narcolepsy, confusional arousals, sleep paralysis, another sleep-related condition, and/or another sleep-related behavior.\n Sleep timings comprises timings of at least one of: go-to-bed, sleep-onset, wake-up, REM-onset, NREM-onset, onset of sleep stage transitions, sleep disorders, sleep problems, breathing problems, insomnia, hypersomnia, parasomnia, sleep\nhypnogram-related events, sleep disruptions, sleep apnea, snoring during sleep, sleeping-not-on-a-bed, day-time sleep, sleep-walking, sleep-related events, sleep-related condition, and/or, sleep-related behavior, etc.\n Sleep stages comprises at least one of: wake-up, rapid-eye-movement (REM) and/or non-REM (NREM).\n At least one of: a time function of breathing rate, and a time function of motion statistics, of the user may be computed based on the series of CI.  If breathing is not detected at time t, the breathing rate at time t may be computed as zero. \nThe sleep-related motion of the user may be monitored based on at least one of: the time function of breathing rate, and/or the time function of motion statistics, of the user.\n At least one of: a time function of breathing ratio, and a time function of motion ratio, of the user may be computed based on the series of CI.  The breathing ratio at time t may be computed as percentage of time when the time function of\nbreathing rate is non-zero in a first time window comprising the time t. The motion ratio at time t may be computed as percentage of time when the time function of motion statistics is larger than a first threshold within a second time window comprising\nthe time t. The sleep-related motion of the user may be monitored based on at least one of: the time function of breathing ratio, and/or the time function of motion ratio, of the user.\n A sleep stage may be classified as \"awake\" if at least one of: the motion ratio is greater than a second threshold, and/or the breathing ratio is less than a third threshold.  The sleep stage may be classified as \"asleep\" if at least one of: the\nmotion ratio is less than the second threshold, and/or the breathing ratio is greater than the third threshold.  The \"asleep\" stage may comprise at least one of: rapid-eye-movement (REM) stage, and/or non-REM (NREM) stage.\n A breathing rate trend function may be computed by low-pass filtering the time function of breathing rate.  A detrended breathing rate function may be computed by subtracting the breathing rate trend function from the time function of breathing\nrate.  A time function of breathing rate variance may be computed by computing variance of the detrended breathing rate function within a sliding time window.  The sleep-related motion of the user may be monitored based on the time function of breathing\nrate variance.\n An average NREM breathing rate may be computed by identifying a peak of a histogram of the time function of breathing rate in \"asleep\" stage in an overnight period.  (e.g. the whole night, or the whole night subtracting any \"awake\" periods).  A\ntime function of breathing rate deviation may be computed by computing a distance between the average NREM and a percentile of the breathing rate within a sliding time window.  The sleep stage may be classified as at least one of: REM stage and/or NREM\nstage, based on the time function of breathing rate deviation.\n A time function of breathing rate variance may be computed by computing variance of a detrended breathing rate function within a first sliding time window.  A time function of breathing rate deviation may be computed by computing a distance\nbetween an average NREM and a percentile of the breathing rate within a second sliding time window.  The sleep stage may be classified as at least one of: REM stage, and/or NREM stage, based on the time function of breathing rate variance and the time\nfunction of breathing rate deviation.\n A classifier may be trained based on at least one of: breathing rate variance, and breathing rate deviation, using machine learning.  The machine learning may comprise at least one of: supervised learning, unsupervised learning, semi-supervised\nlearning, active learning, reinforcement learning, support vector machine, deep learning, feature learning, clustering, regression, and/or dimensionality reduction.  The sleep stage may be classified as at least one of: REM stage, and/or NREM stage,\nbased on the classifier.\n A quantity related to the sleep-related motion of the user may be computed based on the time series of CI.  The sleep-related motion of the user may be monitored based on the quantity.\n The quantity may comprise at least one of: the time the user goes to bed, the time the user gets out of bed, the sleep onset time, total time it takes the user to fall asleep, the wake up time, sleep disruption time, number of sleep disruption\nperiod, mean disruption duration, variance of disruption duration, total time in bed, total time the user is asleep, time periods of REM, time periods of NREM, time periods of awake, total time of REM, total time of NREM, number of REM periods, number of\nNREM periods, time of toss and turn in bed, duration of tossing and turning, hypnogram, periods of apnea, periods of snore, total duration of apnea, number of apnea periods, average duration of apnea period, periods of breathing problems, sleep quality\nscore, daytime sleep, time periods of daytime sleep, total duration of daytime sleep, number of period of daytime sleep, average duration of period of daytime sleep, and another quantity.\n FIG. 4 summarizes a proposed scheme for breathing signal extraction and maximization.  The left part of FIG. 4 shows the decomposition of the measured ACF of the channel power response when a person breathes normally in the monitored area, and\nthe right part shows the MRC scheme for boosting the SNR of the ACF of the breathing signal.  FIG. 5 depicts an illustrative example based on real-world measurements, where the SNR of the breathing signal is amplified by 2.5 dB compared to the best\nsubcarrier indicated by largest variance and by 3.7 dB compared to directly averaging all subcarriers.  FIG. 6 further demonstrates the gains of the disclosed ACF-based MRC scheme and confirms the observations herein that amplitudes and their variances\nare not effective metrics for subcarrier selection.  As seen, the subcarrier that is the most sensitive to motion (i.e., holding the largest motion statistic) could experience very small amplitude and low variance.\n Sleep Stage Recognition\n SMARS divides the continuous motion and breathing estimates of overnight sleep into 300-second epochs.  For each epoch, SMARS recognizes three different sleep stages, i.e., wake, REM sleep and NREM sleep.  The staging is performed in two steps:\nFirst, SMARS differentiates wake from sleep mainly by body motions; Second, REM and NREM stages are further identified during sleep period.\n Sleep/Wake Detection.  SMARS first implements a sleep-wake detector to identify the sleep and wake states.  The key insight is that, more frequent body movements will be observed when a subject is awake, while mainly breathing motion presents\nwhen he/she is asleep.  Since bodily movements are significantly stronger than breathing motions, and both of them can be easily captured and quantified by the motion statistic defined herein, SMARS utilizes it to distinguish between sleep and wake\nstates.\n Specifically, one can define motion ratio as the percentage of time when the motion statistic, {circumflex over (.rho.)}.sub.b(1/F.sub.s), is larger than a preset threshold.  Thus for the wake state, a higher motion ratio is expected, as shown\nin FIG. 7A.  Similarly, one can also define breathing ratio as the percentage of time when the breathing signal is detected.  Since bodily movements destroy the periodicity of the environmental dynamics, the breathing ratio will be lower when a subject\nis awake, as shown in FIG. 7B.\n Combining the above two features, SMARS identifies an epoch as sleep only when the motion ratio is smaller than the predefined threshold and the breathing ratio is larger than the other threshold.  Both thresholds are empirically determined as\nin FIG. 7A and FIG. 7B.  Since the disclosed model statistically considers all multipaths indoors, the values of both thresholds generalize to different environments and subjects.\n REM/NREM Recognition.  SMARS exploits the following clinical facts and accordingly extracts two distinctive features from breathing rate estimates for REM/NREM stages classification: Breathing rate is usually faster and presents higher\nvariability and irregular patterns for REM stage, while more stable and slower for NREM stage.\n Since NREM stage constitutes the majority (about 75% to 80%) of total sleep for typical healthy adults, the average breathing rate during NREM stage can be estimated by localizing the peak of the histogram of overnight breathing rate estimates,\nas shown in FIG. 8A.  On this basis, one can define breathing rate deviation, the distance between the estimated average NREM breathing rate and the 90% tile of the breathing rate for each epoch, to quantify the deviation of the breathing rate during REM\nstage from that during NREM stage.\n To extract the variability of the breathing rate for each epoch, one can first estimate the trend of breathing rate by applying a low pass filter to the breathing estimates of the whole night, and obtain the detrended breathing rate estimates by\nsubtracting the trend from the original breathing rate estimates.  Then, the breathing rate variability is defined and calculated for each epoch as the variance of the detrended estimates normalized by the length of epoch.\n FIG. 8B visualizes the distribution of the proposed two features under NREM and REM sleep, respectively.  As one can see from FIG. 8B, the majority of the breathing rate variability and breathing rate deviation of NREM sleep are much smaller\nthan those of REM sleep.  Based on these two features, one can train a support vector machine (SVM), a widely used binary classifier, to differentiate between REM and NREM sleep.\n Sleep Quality Assessment\n When one obtains the estimates of wake, REM, and NREM stages of a whole sleep, one can assess the elusive sleep quality for a user by following standard approach used in clinical practice.  In particular, one can calculate the sleep score for\neach night based on the recognized sleep stages as follows.  Let T.sub.N, T.sub.R and T.sub.W denote the durations (measured in hours) of NREM sleep, REM sleep and wake, respectively.  Since there is no standard formula for sleep score calculation, a\nsimple formula for the sleep score is applied in SMARS: S=10*T.sub.N+20*T.sub.R-10*T.sub.W, which means that the more you sleep, the more you have REM sleep, the less you keep awake in the bed, the better your sleep score is.  According to recent\nresearch, REM sleep is crucial for mental recovery, and thus a higher weight has been assigned to REM sleep.\n SMARS envisions a practical sleep monitoring for daily in-home use.  Although it does not make much sense to compare the sleep score among different users, the trend or history of the sleep score for a particular user would reflect the changes\nof his/her sleep quality.  Such results provide clinically meaningful evidences to help diagnose sleep disorders and manage personal health, in an attractive way.  FIG. 9 illustrates an exemplary network environment for sleep monitoring, according to one\nembodiment of the present teaching.  FIG. 10 illustrates an exemplary algorithm design for sleep monitoring, according to one embodiment of the present teaching.\n In the era of Internet of Things (IoT), smart appliances are designed and developed to achieve customer satisfaction and convenience and the market for smart appliances is primed.  For instance, a Smart TV can deliver an innovative TV usage\npattern.  Instead of using the conventional remote controller to control a TV at home, with the help of wireless sensing, the Smart TV can be automatically turned on/off, paused and/or resumed by detecting certain motion patterns in front of the TV and\nby sensing the presence of a human in a certain area, e.g., the living room.  In one embodiment, the present teaching discloses monitoring the presence of a live object and potential motion of the object based on time-reversal technology in a\nrich-scattering environment, e.g. an indoor environment or urban metropolitan area, enclosed environment, underground environment, etc.\n As shown in FIG. 11, hang on the wall of the living room, the Smart TV has one Origin and one Bot, i.e., two WiFi transceivers, embedded inside of it to sense the wireless propagation environment of the living room.  When the user is pacing\naround in front of the TV within a certain distance, e.g. 2 to 3 feet from the TV, the TV will detect it by sensing the environment with the Origin and the Bot and capturing and analyzing the channel state information (CSI), and will finally\nautomatically turn on.  When the user is watching the TV and sitting on the sofa, even it might be far away from the TV, the Origin and the Bot can still capture the perturbation introduced to the CSI by the motion or vital signals of the present user. \nOnly when no one is in the living room, the propagation environment will be quiet and the CSI will be consistent along the time and can be sensed by the Origin and the Bot inside the Smart TV.  In other words, with the help of the Origin and the Bot\ninside the Smart TV, the Smart TV can sense the indoor environment wirelessly and differentiate between different indoor states: (1) when the user is pacing close to the TV within the target area (2 to 3 feet), (2) when the user is sitting on the sofa\nand watching TV (i.e., daily activity inside the living room), and (3) when the room is empty and no one is there.\n In the designed scheme of one embodiment, the Smart TV will turn on and off automatically as a response to the three detected indoor states.  When TV is on (or off) and state (1) is detected, the TV will be turned off (or on) immediately.  When\nthe TV is on and state (2) is detected, the TV will remain as on.  When the TV is on and state (3) is detected for a certain time period, the TV will enter into a countdown mode.  During the count-down period and before the countdown limit is reached, if\nthe state (2) is detected, the TV will remain on and the count-down mode is reset and disabled.  However, if the state (3) persists during the count-down period, the TV will be shut down when the countdown limit is reached.\n Methodology:\n In the Smart TV, the Bot keeps transmitting channel probing signals to the Origin at a given sounding rate 1/Ts where Ts is the channel probing interval in seconds.  Based on each received channel probing signal, the Origin can estimate the\nchannel state information (CSI).  For every 1/Ts CSI, a motion statistic value (metric) is derived as the averaged auto-correlation value between adjacent CSIs for a total of M consecutive CSIs.  Due to the nature of multipath propagation, the CSI will\nbe disturbed when users are inside the room and different activities will result in a different motion pattern in the CSIs.  When the motion is close to the Smart TV (the Origin and the Bot), a larger motion statistic value will be produced than the one\nassociated to when the motion is away from the TV.  When there is no motion inside the room, the corresponding motion statistics will be very small, e.g. around 0.  Hence, by using motion statistics, the aforementioned 3 indoor states can be categorized\nwell.\n Algorithm:\n Different algorithms are disclosed to guarantee the accuracy as well as the robustness of the proposed Smart TV system.\n A) To detect state (1) when the user is pacing in front of the TV, a first-in-first-out buffer B_1 with a fixed length W_1 is used to store the latest motion statistics calculated from incoming CSIs.  The median value X_1 of all elements in\nbuffer B_1 is used as the metric.  X_1 keeps updating and is compared with a predefined threshold R_1.  When X_1&gt;R_1, it is determined that state (1) is detected and the Smart TV will be turned on (or off) if its current state is off (or on).\n B) Meanwhile, to detect state (3) when no one is inside the room, another first-in-first-out buffer B_2 with a fixed length W_2 is used to store the latest motion statistics calculated from incoming CSIs.  The median value X_2 of all elements in\nbuffer B_2 is used as the metric.  X_2 keeps being updated and is compared with a predefined threshold R_2.  When X_2&lt;R_2, it is determined that state (3) is detected and the Smart TV will enter the countdown mode with the countdown limit being T_0. \nDuring the countdown mode and before the limit T_0 is reached, the Origin and the Bot keeps sensing the indoor propagation environment and updating the buffer B_1 and B_2 with latest motion statistics.  Meanwhile, a new first-in first-out motion\nstatistics buffer B_3 is opened with length W_3 and it is aimed to detect if the state (2) or any motion happens during this period.  The median value X_3 of all elements in buffer B_3 is used as the metric and compared with a predefined threshold R_3. \nWhen X_3&gt;R_3, it is determined that state (2) is detected, i.e., there is motion or the user present in the room.  During the countdown period, if the state (2) is detected, the countdown will be terminated and the Smart TV will not be shut down. \nOtherwise, the Smart TV will automatically shut-down when the countdown limit is reached.\n C) The length of each buffer is adjustable.  Typically, W_1 is 5 seconds, W_2 is 15 seconds, W_3 is 5 seconds and T_0 is 30 seconds.  The threshold of R_1, R_2, and R_3 can be adjusted manually or learned through a training process during the\ninitial set-up.\n The Smart TV disclosed herein may display any video on a TV screen.  In one embodiment, the video can be paused and/or resumed automatically as a response to the three detected indoor states: (1) when the user is pacing close to the TV within\nthe target area (2 to 3 feet), (2) when the user is sitting on the sofa and watching TV (i.e., daily activity inside the living room), and (3) when the room is empty and no one is there.  When the TV is on and playing any video on the screen, if the\nstate (1) is detected, the TV will immediately pause and start a countdown period T_1.  During the countdown period of T_1, if the state (1) is detected again, the TV will resume and start to play from the paused scene.  If state (1) is never detected\nwhile the state (2) is detected, the TV will remain paused with its screen being on and showing the paused scene.  However, if the state (3) keeps being detected during the countdown period of T_1, i.e. none of the state (1) or the state (2) has been\ndetected before the countdown limit T_1 is reached, the TV will enter the sleep mode with the display being off and start another countdown period T_2.  During the countdown period of T_2, if the state (1) is detected, the TV will immediately light up\nand start to play from the paused scene.  If the state (2) is detected instead of the state (1), the TV will immediately light up, stay in the pause mode, and show the paused screen.  However, if before the limit T_2 is reached, none of the state (1) or\nthe state (2) has ever been detected, i.e., the TV detects the state (3) all the time, then the TV which is in the sleep mode will turn off automatically, but remember the last scene before it is paused.\n Algorithm:\n Different algorithms are disclosed to guarantee the accuracy as well as the robustness of the proposed Smart TV system.\n A) To detect the state (1) when the user is pacing in front of the TV, a first-in-first-out buffer B_1 with a fixed length W_1 is used to store the latest motion statistics calculated from incoming CSIs.  The median value X_1 of all elements in\nbuffer B_1 is used as the metric.  X_1 keeps updating and is compared with a predefined threshold R_1.  When X_1&gt;R_1, it is determined that state (1) is detected and the Smart TV will be paused or resumed given its current state.  If the current state\nof the Smart TV is being paused, the Smart TV will immediately resume to play from the paused scene.  However, if the Smart TV was playing a movie and it is being paused as a response to the detection of the state (1), a countdown mode of limit T_1 will\nbe started immediately to decide if the Smart TV enter the sleep mode or not.\n B) Meanwhile, to detect the state (3) when no one is inside the room, another first-in-first-out buffer B_2 with a fixed length W_2 is used to store the latest motion statistics calculated from incoming CSIs.  The median value X_2 of all\nelements in buffer B_2 is used as the metric.  X_2 keeps being updated and is compared with a predefined threshold R_2.  When X_2&lt;R_2, it is determined that state (3) is detected.\n C) Moreover, a first-in first-out motion statistics buffer B_3 is opened with length W_3 and it is aimed to detect if the state (2) or any motion happens during this period.  The median value X_3 of all elements in buffer B_3 is used as the\nmetric and compared with a predefined threshold R_3.  When X_3&gt;R_3, it is determined that state (2) is detected, i.e., there is motion or the user present in the room.\n D) During the countdown of T_1 and before the limit T_1 is reached, the Origin and the Bot keeps sensing the indoor propagation environment and updating the buffer B_1, B_2, and B_3 with latest motion statistics.  If the state (1) is detected\nbefore the limit T_1 is reached, the Smart TV will immediately resume and continue to play from the paused scene, and the countdown will be terminated.  If the state (1) is not detected but the state (2) is detected before the limit T_1 is reached, the\ncountdown of T_1 will be reset and started over again while the Smart TV remains in the paused mode.  On the other hand, if none of the state (1) or the state (2) is detected, i.e., the Origin and the Bot in the Smart TV keeps detecting state (3), the TV\nwill enter the sleep mode with the display being off when the countdown limit T_1 is reached.\n E) As soon as the Smart TV enters the sleep mode, a new countdown of limit T_2 starts.  During the countdown of T_2, the Smart TV keeps its display off until any of the following three cases happens.  If the state (1) is detected, the countdown\nof T_2 will be terminated and the Smart TV will light up and resume to play from the paused scene immediately.  If the state (2) is detected, the countdown of T_2 will be terminated while the countdown of T_1 will start over again.  Meanwhile, the Smart\nTV will light up, remain paused with the display showing the paused scene.  If the state (3) keeps being detected, i.e., none of the state (1) or the state (2) is detected, the Smart TV in the sleep mode will be turned off automatically at the time when\nthe countdown limit T_2 is reached.\n C) The length of each buffer is adjustable.  The threshold of R_1, R_2, and R_3 can be adjusted manually or learned through a training process during the initial set-up.\n Experimental Results:\n To validate and demonstrate the idea of Smart TV, experiments are conducted and the set-up is shown in FIG. 12 where the Origin and the Bot are put at the left and right boundary of the TV, right under the TV.  The space in front of the test TV\nis partitioned into 6 zones as shown in FIG. 13 and the Zone 1 which is within 1 meter from the TV is considered as the intended area of the state (1).\n The CSI between the Origin and the Bot can be collected to calculate the motion statistics for cases when one tester is pacing in each zone for 1 minute and for the case when the room is empty for 1 minute.  The distributions of statistics\ncorresponding to different scenarios are plotted as the cumulative distribution function (CDF), where the legend \"CX\" means the scenario of user pacing in Zone X. It is clear that almost 90% of the statistics for an empty room are below 0, while almost\n90% of statistics of pacing in Zone 1 are above 0.6.  For other motion inside the room, most of the motion statistics fall into the range of 0.1 to 0.5.  Hence, with the help of motion statistics, the proposed Smart TV is capable of differentiating\nbetween those 3 indoor states.\n Based on motion statistics and the system output (turn-on detection) along the time for the user pacing at different zones, when the turn-on detection is one, the Smart TV will be turned on (off) automatically.  100% of detection is achieved for\nZone 1, i.e. the intended trigger zone, and 0 false alarm for other zones.\n Based on motion statistics and the system output (activity detection) along the time for both the scenarios of an empty room and someone sitting in Zone 4, when the activity detection is 1, the TV will not be turned off.  When the activity\ndetection is 0, the TV will be turned off after the countdown.  The motion statistics may be sensitive and capable of detecting tiny and distant motion, while maintaining its robustness for an empty room.\n Based on results of 5 turn-on tests and 5 passing-by tests, as the user is intentionally pacing in Zone 1 to activate the state (1), the proposed Smart TV system can always capture and respond to it quickly and accurately.  When the user is only\nwalking through Zone 1 randomly without an intention to activate the state (1), the proposed Smart TV system will never produce false alarm which demonstrates its robustness.\n Potential Use Cases:\n By utilizing the CSI that characterizes the indoor environment to detect and distinguish different indoor environments, the disclosed Smart TV system is intelligent and capable of turning on and off the TV automatically based on the indoor\nstates of when no one is in the room, when the user is within the room performing daily activities and/or when the user is pacing in front of the TV.  The disclosed Smart TV system can detect motion in close proximity, and quickly respond to it. \nMeanwhile, the disclosed system can also be extended to other smart appliances, such as the refrigerator, the electric fireplace, the display screen for advertising and so on.\n FIG. 14 illustrates another exemplary setting of Smart TV where the Origin and/or Bot may be integrated into the TV for presence detection.  FIG. 15 illustrates another exemplary setting of Smart TV where the Origin and/or Bot may be installed\nin a speaker placed in front of the TV for presence detection.  FIG. 16 illustrates another exemplary setting of Smart TV, with TV placed on a table, and Origin/Bot installed on the table.  The table can be computer furniture to house the computer, or\nentertainment center to house the TV.\n FIG. 17 illustrates another scenario of a smart fan.  The fan may be a standing fan with a high stand and the Origin and/or Bot may be installed on the high stand.  When a person is detected, either through motion detection or vital signs (such\nas breathing) detection, the fan can be turned on.\n FIG. 18 illustrates another scenario of a smart car.  The Origin and Bot may be installed on the outside of a car.  When human presence is detected, the car may do something: e.g. activate security; check ID of the person; if the user is\nconfirmed, the car may open the door, trunk, and etc. or start engine (warm up) or start AC to cool the car.\n FIG. 19 illustrates an exemplary interior bird-view of a car for seat occupancy detection and people counting, according to one embodiment of the present teaching.  In an example, one or more Type 1 devices and multiple Type 2 devices (e.g. N=4)\nmay be placed in a confined area (e.g. closed area such as a car, a conference room, a bus, an airplane, or a cinema, semi-open area such as a bus with windows open, or a balcony with 8 outdoor chairs around an outdoor table) with multiple \"seats\" at\nfixed locations (may be a space for standing, sitting, kneeling, lying down, etc.) that may hold a person.  A presence of a person at one or more \"seats\" may be detected based on time series of CI extracted from wireless signals sent from the Type 1\ndevices to the Type 2 devices.\n FIG. 19 shows a particular example of a car with 4 seats, two in front row and two in back row.  Note that each seat has a \"seat bed\" for a person to sit on and a \"seat back\" for a person to lean back on.  The Type 1 device may be placed in the\nfront, on the dash board.  Four Type 2 devices may be deployed, one at each of the 4 seats (e.g. in/on/under the seat bed, or in/on/under the seat bed).  When a seat A (e.g. driver seat, or right seat on front row, or back row left seat, etc.) is\noccupied by the driver or a passenger or a baby in a car-seat, the CI (channel information) associated with the occupied seat A may behave differently (e.g. become smaller or bigger) from the CI associated with an empty seat A. Thus one can detect the\nseat occupancy of each seat by examining the CI.  By performing such test for all the seats, one can count the amount of people in the car.  If a person sits at non-standard locations (e.g. between two seats, in the center of back row, in the center of\nfront row, or a baby in a car seat), CI associated multiple Type 2 devices can be analyzed jointly to determine if there is a person there.  As signature of a baby in car-seat may be different from an adult or a children, adult/children/baby\nclassification may be performed based on the CI.\n A task may be performed based on the seat occupancy described above.  For example, the task may be to arm an air-bag if a seat is occupied, but disarm the airbag if the seat is not occupied.  If a small size person (e.g. a child) instead of a\nregular-size adult is detected, an air-bag designed for adults may not be armed.  The heating/air-condition setting may be adjusted.  The task may be to control windows, lighting, audio system, entertainment system (e.g. video), noise cancelling,\nshock-absorbing system, stabilizing size, car avoidance system, safety features, tire pressure, any other car subsystems, etc. For example, if a passenger is detected at the front right seat, the temperature at that region (front, right) may be\ncontrolled to a preset level.  If the seat is empty, the temperature may be adjusted differently.\n FIGS. 20A-20D illustrate changes of channel information (CI) according to various seat occupancy situations in a car, according to one embodiment of the present teaching.  FIG. 20A shows CI when no seat is occupied.  FIG. 20B shows CI when seat\n1 (e.g. seat with Bot antenna 1 in FIG. 19) is occupied.  FIG. 20C shows CI when seat 3 (e.g. seat with Bot antenna 3 in FIG. 19) is occupied.  FIG. 20D shows CI when both seat 1 and seat 3 are occupied.\n The features described above may be implemented advantageously in one or more computer programs that are executable on a programmable system including at least one programmable processor coupled to receive data and instructions from, and to\ntransmit data and instructions to, a data storage system, at least one input device, and at least one output device.  A computer program is a set of instructions that may be used, directly or indirectly, in a computer to perform a certain activity or\nbring about a certain result.  A computer program may be written in any form of programming language (e.g., C, Java), including compiled or interpreted languages, and it may be deployed in any form, including as a stand-alone program or as a module,\ncomponent, subroutine, a browser-based web application, or other unit suitable for use in a computing environment.\n Suitable processors for the execution of a program of instructions include, e.g., both general and special purpose microprocessors, digital signal processors, and the sole processor or one of multiple processors or cores, of any kind of\ncomputer.  Generally, a processor will receive instructions and data from a read-only memory or a random access memory or both.  The essential elements of a computer are a processor for executing instructions and one or more memories for storing\ninstructions and data.  Generally, a computer will also include, or be operatively coupled to communicate with, one or more mass storage devices for storing data files; such devices include magnetic disks, such as internal hard disks and removable disks;\nmagneto-optical disks; and optical disks.  Storage devices suitable for tangibly embodying computer program instructions and data include all forms of non-volatile memory, including by way of example semiconductor memory devices, such as EPROM, EEPROM,\nand flash memory devices; magnetic disks such as internal hard disks and removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks.  The processor and the memory may be supplemented by, or incorporated in, ASICs (application-specific\nintegrated circuits).\n While the present teaching contains many specific implementation details, these should not be construed as limitations on the scope of the present teaching or of what may be claimed, but rather as descriptions of features specific to particular\nembodiments of the present teaching.  Certain features that are described in this specification in the context of separate embodiments may also be implemented in combination in a single embodiment.  Conversely, various features that are described in the\ncontext of a single embodiment may also be implemented in multiple embodiments separately or in any suitable sub-combination.\n Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations\nbe performed, to achieve desirable results.  In certain circumstances, multitasking and parallel processing may be advantageous.  Moreover, the separation of various system components in the embodiments described above should not be understood as\nrequiring such separation in all embodiments, and it should be understood that the described program components and systems may generally be integrated together in a single software product or packaged into multiple software products.\n Particular embodiments of the subject matter have been described.  Any combination of the features and architectures described above is intended to be within the scope of the following claims.  Other embodiments are also within the scope of the\nfollowing claims.  In some cases, the actions recited in the claims may be performed in a different order and still achieve desirable results.  In addition, the processes depicted in the accompanying figures do not necessarily require the particular\norder shown, or sequential order, to achieve desirable results.  In certain implementations, multitasking and parallel processing may be advantageous.", "application_number": "16203299", "abstract": " Apparatus, systems and methods for recognizing and classifying events in\n     a venue based on a wireless signal are disclosed. In one example, a\n     disclosed system comprises a first transmitter, a second transmitter, at\n     least one first receiver, at least one second receiver, and an event\n     recognition engine, in the venue. The first transmitter transmits a\n     training wireless signal through a wireless multipath channel impacted by\n     a known event in the venue in a training time period associated with the\n     known event. Each first receiver receives asynchronously the training\n     wireless signal, and obtains, asynchronously based on the training\n     wireless signal, at least one time series of training channel information\n     of the wireless multipath channel between the first receiver and the\n     first transmitter. The second transmitter transmits a current wireless\n     signal through the wireless multipath channel impacted by a current event\n     in a current time period associated with the current event. Each second\n     receiver receives asynchronously the current wireless signal, and\n     obtains, asynchronously based on the current wireless signal, at least\n     one time series of current channel information of the wireless multipath\n     channel between the second receiver and the second transmitter. The event\n     recognition engine trains a classifier based on the training channel\n     information; and apples the classifier to: classify the current channel\n     information and associate the current event with at least one of: a known\n     event, an unknown event and another event.\n", "citations": ["9078153", "9119236", "9413580", "9753796", "20120077468", "20140266669", "20150163121", "20150256379", "20160018508", "20170212210", "20180183650", "20180306609", "20180351775", "20180357542", "20180365975", "20190007256", "20190028320"], "related": ["15326112", "2015041037", "14605611", "16203299", "15584052", "15434813", "2017021963", "2017021957", "2017027131", "15384217", "13706342", "13969271", "13969320", "15041677", "15200430", "14262153", "15200429", "14943648", "14202651", "14605611", "14615984", "15004314", "15061059", "2015041037", "14605611", "15268477", "15200429", "14943648", "14202651", "15284496", "2016066015", "16203299", "2017015909", "2016066015", "16203299", "15861422", "15873806", "16101444", "16200608", "16200616", "62148019", "62025795", "62069090", "62331278", "62295970", "62320962", "62307081", "62316850", "62307172", "62334110", "62322575", "62409796", "62593826", "62106395", "62128574", "62219315", "62235958", "62265155", "62411504", "62383235", "62307081", "62316850", "62320965", "62384060", "62678207", "62734224", "62744093", "62753017"]}, {"id": "20190179869", "patent_code": "10372787", "patent_name": "Hardware accelerator pre-configured with coefficients for matrix-transform\n     operations", "year": "2019", "inventor_and_country_data": " Inventors: \nPark; Jong Soo (Mountain View, CA), Rotem; Nadav (Santa Clara, CA), Smelyanskiy; Mikhail (Burlingame, CA), Diril; Abdulkadir Utku (Menlo Park, CA)  ", "description": "BACKGROUND\n Artificial intelligence (AI) can enable computers to perform a variety of complicated tasks, including tasks related to cognitive functions, such as \"learning,\" that are typically associated with humans.  Several approaches to AI are prevalent,\nincluding machine-learning techniques.  In machine-learning systems, a computer may be programmed to parse data, learn from the data, and make predictions from real-world inputs.  One machine-learning model, referred to as an artificial neural network,\nwas inspired by the interconnections of neurons in a biological brain.  Neural networks and other machine-learning systems are widely used to perform a variety of AI-related tasks, including speech recognition and computer vision.\n Unfortunately, neural networks are often extremely computationally intensive.  For example, convolutional neural networks (CNN), which typically apply convolution operations to an input matrix in an effort to emulate the response of a biological\nneuron to visual stimuli, may tax even the most advanced computing systems.  Although researchers have begun using domain-transformation-based algorithms (such as Fast Fourier Transform (FFT)-based algorithms and so-called Winograd minimal filtering\nalgorithms) in an attempt to reduce the number of arithmetic operations required to perform, and thus improve the performance of, the convolution operations required by CNNs, the sheer number of convolution operations typically required by a CNN means\nthat even small gains in neural network efficiency and/or throughput may result in tremendous computational and/or energy savings.  The instant disclosure, therefore, identifies and addresses a need for systems and methods for improving the efficiency\nand/or performance of machine-learning systems and other processing systems in which convolution operations are required or useful.\nSUMMARY\n As will be described in greater detail below, the instant disclosure describes a special-purpose hardware accelerator pre-configured with the coefficients used to perform some or all of the domain-transform operations used in modern convolution\noperations, such as Winograd minimal filtering convolutions.  For example, a special-purpose hardware accelerator may include a cache configured to store an input matrix related to performing a convolution operation and a matrix-multiplication subsystem\npre-configured with matrix-transform coefficients for performing matrix-transform operations.  The matrix-multiplication subsystem may be configured to perform the convolution operation by (1) reading the input matrix from the cache, (2) transforming the\ninput matrix via matrix multiplication, (3) transforming, via matrix multiplication, a parameter matrix that includes convolution parameters for performing the convolution operation, (4) applying the transformed parameter matrix to the transformed input\nmatrix via an element-wise multiplication operation, and then (5) performing an inverse-transformation operation on the results of the element-wise multiplication operation to create an output matrix for the convolution operation.  In some examples, the\nmatrix-multiplication subsystem may use the pre-configured matrix-transform coefficients to transform the input matrix, to transform the parameter matrix, and/or to perform the inverse-transformation operation on the results of the element-wise\nmultiplication operation.\n The pre-configured matrix-transform coefficients may include pre-configured coefficients for transforming the input matrix, pre-configured coefficients for transforming the parameter matrix, and/or pre-configured coefficients for performing the\ninverse-transformation operation on the results of the element-wise multiplication operation.  The pre-configured matrix-transform coefficients may also include a transposed version of the pre-configured coefficients for transforming the input matrix, a\ntransposed version of the pre-configured coefficients for transforming the parameter matrix, and/or a transposed version of the pre-configured coefficients for performing the inverse-transformation operation on the results of the element-wise\nmultiplication operation.  Alternatively, the matrix-multiplication subsystem may directly transpose the pre-configured coefficients using in-place matrix transposition.\n In one example, the matrix-multiplication subsystem may include a dot-product engine configured to perform the matrix-transform operations and/or an element-wise multiplier configured to perform the element-wise multiplication operation.  In\nthis example, the matrix-multiplication subsystem may transform the input matrix on-the-fly when reading the input matrix from the cache to the element-wise multiplier.\n In one embodiment, the parameter matrix may be stored in the cache of the matrix-multiplication subsystem.  In this embodiment, the matrix-multiplication subsystem may transform the parameter matrix on-the-fly when reading the parameter matrix\nfrom the cache to the element-wise multiplier.  In another embodiment, the matrix-multiplication subsystem may perform the inverse-transformation operation on-the-fly when storing the output matrix to the cache.\n In some examples, (1) the input matrix may include the entirety of an input volume for the convolution operation, (2) transforming the input matrix via matrix multiplication may include transforming the entire input volume via matrix\nmultiplication, and (3) the output matrix may include the entirety of an output volume for the convolution operation.  In other examples, (1) the input matrix may include an initial portion of an input volume for the convolution operation, (2)\ntransforming the input matrix via matrix multiplication may include transforming the initial portion of the input volume via matrix multiplication, and (3) creating the output matrix may include creating an initial portion of an output volume for the\nconvolution operation.  In these examples, performing the convolution operation may include (1) receiving at least one additional portion of the input volume from the cache, (2) transforming the additional portion of the input volume via matrix\nmultiplication, (3) applying, via an additional element-wise multiplication operation, the transformed parameter matrix to the additional portion of the input volume that was transformed, and then (4) performing an additional inverse-transformation\noperation on the results of the additional element-wise multiplication operation to create an additional portion of the output volume for the convolution operation.\n In some embodiments, the special-purpose hardware accelerator may be configured to pose each of a plurality of element-wise multiplication operations as a plurality of dot-product operations and then batch the plurality of dot-product operations\ninto a single matrix-multiplication operation for processing by the matrix-multiplication subsystem.\n In one example, the above-described hardware accelerator may be integrated within a larger computing system.  For example, a computing system may include a memory device configured to store an input matrix related to performing a convolution\noperation and a special-purpose hardware accelerator with a matrix-multiplication subsystem that is pre-configured with matrix-transform coefficients for performing matrix-transform operations.  In this example, the matrix-multiplication subsystem may\nperform the convolution operation by (1) reading the input matrix from the memory device, (2) transforming the input matrix via matrix multiplication, (3) transforming, via matrix multiplication, a parameter matrix that includes convolution parameters\nfor performing the convolution operation, (4) applying the transformed parameter matrix to the transformed input matrix via an element-wise multiplication operation, and then (5) performing an inverse-transformation operation on the results of the\nelement-wise multiplication operation to create an output matrix for the convolution operation.  The matrix-multiplication subsystem may use the pre-configured matrix-transform coefficients to transform the input matrix, to transform the parameter\nmatrix, and/or to perform the inverse-transformation operation on the results of the element-wise multiplication operation.\n In some embodiments, the matrix-multiplication subsystem may include a cache.  In these embodiments, reading the input matrix from the memory device may include (1) storing the input matrix from the memory device into the cache and then (2)\nreading the input matrix from the cache.\n In one example, a corresponding computer-implemented method may include reading, from a cache of a special-purpose hardware accelerator, an input matrix related to performing a convolution operation.  In this example, the special-purpose\nhardware accelerator may be pre-configured with matrix-transform coefficients for performing matrix-transform operations.  The method may also include performing, using the special-purpose hardware accelerator, the convolution operation by (1)\ntransforming the input matrix via matrix multiplication, (2) transforming, via matrix multiplication, a parameter matrix that includes convolution parameters for performing the convolution operation, (3) applying the transformed parameter matrix to the\ntransformed input matrix via an element-wise multiplication operation, and then (4) performing an inverse-transformation operation on the results of the element-wise multiplication operation to create an output matrix for the convolution operation.\n Features from any of the above-mentioned embodiments may be used in combination with one another in accordance with the general principles described herein.  These and other embodiments, features, and advantages will be more fully understood\nupon reading the following detailed description in conjunction with the accompanying drawings and claims. BRIEF DESCRIPTION OF THE DRAWINGS\n The accompanying drawings illustrate a number of exemplary embodiments and are a part of the specification.  Together with the following description, these drawings demonstrate and explain various principles of the instant disclosure.\n FIG. 1 is a block diagram of an exemplary special-purpose hardware accelerator capable of using pre-configured coefficients to perform the domain-transform operations used in modern convolution operations.\n FIG. 2 is an additional illustration of the hardware accelerator from FIG. 1.\n FIG. 3 is a flow diagram of an exemplary method for performing convolution operations using a special-purpose hardware accelerator with pre-configured matrix-transform coefficients.\n FIG. 4 is a process diagram illustrating an exemplary input matrix transform operation.\n FIG. 5 is a block diagram of the input matrix transformed in FIG. 4.\n FIG. 6 is a process diagram illustrating an exemplary parameter matrix transform operation.\n FIG. 7 is a process diagram illustrating the performance of an exemplary element-wise multiplication operation on the input and parameter matrices transformed in FIGS. 4 and 6.\n FIG. 8 is a process diagram illustrating an exemplary inverse-transform operation performed on the results of the element-wise multiplication operation performed in FIG. 7.\n FIG. 9 is a block diagram of an exemplary convolutional neural network capable of benefiting from the hardware accelerators of FIGS. 1 and 2.\n FIG. 10 is a block diagram of an exemplary system in which the hardware accelerators of FIGS. 1 and 2 may be implemented.\n FIG. 11 is a block diagram of an exemplary computing system in which the hardware accelerators of FIGS. 1 and 2 may be implemented.\n Throughout the drawings, identical reference characters and descriptions indicate similar, but not necessarily identical, elements.  While the exemplary embodiments described herein are susceptible to various modifications and alternative forms,\nspecific embodiments have been shown by way of example in the drawings and will be described in detail herein.  However, the exemplary embodiments described herein are not intended to be limited to the particular forms disclosed.  Rather, the instant\ndisclosure covers all modifications, equivalents, and alternatives falling within the scope of the appended claims.\nDETAILED DESCRIPTION OF EXEMPLARY EMBODIMENTS\n The instant disclosure is generally directed to a special-purpose hardware accelerator (and computing system incorporating the same) that has been pre-configured or hardcoded with the coefficients used to perform some or all of the\ndomain-transform operations typically used in modern convolution algorithms, such as the Winograd minimal filtering algorithm.  As will be explained in greater detail below, hardware accelerators pre-configured in this manner may provide a number of\nfeatures and advantages over traditional systems.  For example, by using pre-configured or hardcoded coefficients instead of storing the same in what is often valuable and/or expensive memory (both from a price and a performance perspective), the\npre-configured, specialized hardware disclosed herein may, in addition to reducing the overall number of arithmetic operations required to perform a convolution operation, accelerate convolution computation, reduce memory usage, reduce energy consumption\nand heat generation, lower costs, and/or provide a number of other benefits.\n The following will provide, with reference to FIGS. 1 and 2, detailed descriptions of an exemplary special-purpose hardware accelerator pre-configured with the coefficients used to perform the matrix-transform operations used in modern\nconvolution operations.  The description corresponding to FIGS. 3-8 will detail an exemplary method for performing convolution operations using such accelerators.  In addition, a detailed description of an exemplary convolutional neural network capable\nof benefiting from the hardware accelerators described herein will be provided in connection with FIG. 9.  Finally, the description corresponding to FIGS. 10-11 will detail an exemplary server and computing system capable of incorporating the disclosed\nhardware accelerators.\n Turning to the figures, FIGS. 1 and 2 illustrate an exemplary special-purpose hardware accelerator 100.  The term \"special-purpose hardware accelerator\" may, in some examples, refer to various types and forms of processors, logical units, and\nother hardware elements that may be arranged, designed, or otherwise configured to perform one or more tasks more efficiently than general-purpose computing systems (e.g., general-purpose processors and/or memory devices).  For example, some of the\nspecial-purpose hardware accelerators described herein may be configured to perform convolution operations more efficiently and/or more effectively than general-purpose central processing units (CPUs).  Special-purpose hardware accelerators may be\nimplemented in a variety of ways, including via hardwiring and/or using application-specific integrated circuits (ASICs) or field-programmable gate arrays (FPGAs).\n As illustrated in FIG. 1, hardware accelerator 100 may include a cache 110 and a matrix-multiplication subsystem 120.  Cache 110 generally represents any type or form of volatile or non-volatile storage device or medium capable of storing data\nand/or computer-readable instructions, such as the filter weights and/or input/output data used or generated during convolution operations.  Examples of cache 110 include, without limitation, static random-access memory (SRAM), Random Access Memory\n(RAM), Read Only Memory (ROM), flash memory, Hard Disk Drives (HDDs), Solid-State Drives (SSDs), variations or combinations of one or more of the same, or any other suitable storage device.\n In one example, cache 110 may be configured to store an input matrix (such as input matrix 112) related to performing a convolution operation.  In one example, this convolution operation may correspond to a machine-learning task, such as a\ncomputer-vision task that uses convolution operations to identify objects in images, although this convolution operation may also correspond to computing tasks that are unrelated to AI or machine-learning tasks.  The convolution operations described\nherein may be applied to various types of inputs, which may be referred to as \"input volumes\" or \"input matrices.\" Input volumes may correspond to any type or form of data capable of being convolved, including image data, speech data, video data, etc.\nInput volumes may also correspond to one or more inputs and, thus, may represent one-dimensional inputs, two-dimensional inputs, three-dimensional inputs, etc. For example, an input volume may correspond to a two-dimensional image having three channels,\nsuch as red, green, and blue, and may therefore be considered three-dimensional.\n Input matrices (such as input matrix 112) may represent either a subset or the entirety of an input volume.  For example, an input matrix of a 3D input volume may be a smaller matrix (such as a patch or a tile) taken from one or more channels of\nthe 3D input volume.  Specifically, as illustrated in FIG. 5, input matrix 404 may represent a subset or a portion of an input volume 500 that corresponds to a convolution operation (such as pixels from an input image selected based on a filter or\nparameter window).  In other examples, input matrix 112 may represent the entirety of an input volume.\n In some examples, cache 110 may also be configured to store a parameter matrix 113 that includes convolution parameters for performing a convolution operation.  Examples of parameter matrix 113 include, without limitation, filter or weight\nmatrices derived by training a neural network.  As with input matrix 112, parameter matrix 113 may represent a one-dimensional, two-dimensional, or three-dimensional matrix.\n Matrix-multiplication subsystem 120 generally represents any type or form of matrix calculation hardware capable of performing arithmetical operations, such as those performed during convolution operations.  In some examples,\nmatrix-multiplication subsystem 120 may be made up of a number of discrete units, such as one or more general matrix-to-matrix multiplication (GEMM) units, multiply-accumulate (MAC) units, etc. In the example illustrated in FIG. 2, matrix-multiplication\nsubsystem 120 may include at least one dot-product engine 230 and at least one element-wise multiplier 240.\n In some examples, matrix-multiplication subsystem 120 may be pre-configured with matrix-transform coefficients for performing matrix-transform operations.  The term \"matrix-transform operation\" may, in some examples, refer to the operations used\nby some modern convolution algorithms to transform data and/or operations into a more efficient (i.e., less complex, from an arithmetic perspective) domain, such as transforming a time-domain operation into a frequency-domain operation or a so-called\nWinograd-domain operation.  In some examples, such as during Winograd minimal filtering convolutions, these operations may involve transforming (using, e.g., matrix multiplication) an input matrix and/or a parameter matrix (such as a filter map) using a\nmatrix of pre-determined (and often constant or unchanging) coefficients, such as 0, 1, -1, 0.5, etc., referred to herein as \"matrix-transform coefficients.\" A more in-depth discussion of these coefficients, as wells as methods for calculating the same,\nmay be found in \"Fast Algorithms for Convolutional Neural Networks\" by Andrew Lavin and Scott Gray, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp.  4013-4021 (hereafter \"Lavin and Gray\"), the entirety of which is\nhereby incorporated by reference.\n Unfortunately, storing these matrix-transform coefficients to, and reading such coefficients from, memory may consume precious (and often expensive) resources, potentially resulting in decreased performance, increased power usage, and wasted\nmemory space.  In one example, hardware accelerator 100 may avoid these issues by being pre-configured or hardcoded with the matrix-transform coefficients that are used to perform the above-described transformations.  Matrix-multiplication subsystem 120\nmay be pre-configured with matrix-transform coefficients in a variety of ways, including via hardwiring and/or using ASICs or FPGAs.  For example, and as illustrated in FIG. 2, dot-product engine 230 (which may, in some examples, perform the\nmatrix-multiplication operations required to transform input matrix 112 and parameter matrix 113) may be pre-configured with matrix-transform coefficients 232.  In this example, matrix-transform coefficients 232 may be fixed or hardcoded (via, e.g.,\nhardwiring and/or via an ASIC or FPGA design) within the architecture of dot-product engine 230 itself (e.g., one or more of the inputs of dot-product engine 230 may be fixed or hardcoded to the values of matrix-transform coefficients 232), thereby\neliminating the need to store or read such coefficients from cache 110 (or any other memory device within or external to hardware accelerator 100).\n Matrix-transform coefficients 232 may represent some or all of the coefficients used to perform the matrix-transform operations mentioned above.  Matrix-transform coefficients 232 may also represent the coefficients required by a single or a\nplurality of matrix-transform operations.  For example, matrix-transform coefficients 232 may include pre-configured coefficients for transforming an input matrix (e.g., input matrix 112), pre-configured coefficients for transforming a parameter matrix\n(e.g., parameter matrix 113), and/or pre-configured coefficients for performing inverse-transformation operations on the results of an element-wise multiplication operation performed on the transformed input and parameter matrices (described in greater\ndetail below).  In some examples, matrix-transform coefficients 232 may also include transposed versions of one or more such coefficients, such as a transposed version of the pre-configured coefficients for transforming the input matrix, a transposed\nversion of the pre-configured coefficients for transforming the parameter matrix, and/or a transposed version of the pre-configured coefficients for performing the above-mentioned inverse-transformation operations.  In other examples, however,\nmatrix-multiplication subsystem 120 may directly transpose matrix-transform coefficients 232 using in-place matrix transposition, thereby eliminating the need to pre-configure or hardcode matrix-multiplication subsystem 120 with transposed versions of\nthe above-described coefficients.\n By pre-configuring hardware accelerator 100 with the matrix-transform coefficients used to perform the matrix-transform operations required by modern convolution algorithms (such as the Winograd minimal filtering algorithm), hardware accelerator\n100 may be able to perform the convolution operations used by such algorithms in an energy and space-efficient manner.  FIG. 3 is a flow diagram of an exemplary computer-implemented method 300 for using hardware accelerator 100 to perform such\nconvolution operations.  The steps shown in FIG. 3 may be performed by any suitable computer-executable code and/or computing system, including the system(s) illustrated in FIGS. 1, 2, 9, and 12.  In one example, each of the steps shown in FIG. 3 may\nrepresent an algorithm whose structure includes and/or is represented by multiple sub-steps, examples of which will be provided in greater detail below.\n As illustrated in FIG. 3, at step 310 the systems described herein may read an input matrix from a cache of a hardware accelerator that is pre-configured with matrix-transform coefficients for performing matrix-transform operations.  For\nexample, and as illustrated in FIGS. 1 and 2, matrix-multiplication subsystem 120 (which may be pre-configured with matrix-transform coefficients 232) may read input matrix 112 from cache 110.\n At step 320, the systems described herein may transform the input matrix via matrix multiplication.  For example, matrix-multiplication subsystem 120 may transform, using a matrix-multiplication operation, input matrix 112 into a less-complex\n(from an arithmetic perspective) domain (e.g., a domain that facilitates element-wise multiplication operations, as detailed below), as represented by input matrix transform 122 in FIG. 1.\n The systems described herein may perform step 320 in a variety of ways.  In one example, dot-product engine 230 may use the pre-configured matrix-transform coefficients 232 to transform input matrix 112 via a matrix-multiplication operation. \nFor example, and as shown by input transform 400 illustrated in FIG. 4, dot-product engine 230 may multiply an input matrix 404 by a set of matrix-transform coefficients 402.  Dot-product engine 230 may then multiply the results of the prior operation by\na transposed version of matrix-transform coefficients 402 (e.g., transposed matrix-transform coefficients 406), resulting in transformed input matrix 408.  As detailed above, matrix-transform coefficients 402 may be pre-determined and/or pre-selected in\na variety of ways based on a variety of factors (including those outlined in the Lavin and Gray paper previously identified and incorporated herein).\n In some examples, the systems described herein may transform an input matrix on-the-fly when reading the same from memory.  For example, because matrix-multiplication subsystem 120 may be pre-configured or hardcoded with matrix-transform\ncoefficients 402 (and/or transposed versions of the same), matrix-multiplication subsystem 120 may transform input matrix 112 as it reads the same from cache 110 via dot-product engine 230.\n In one example, this on-the-fly transformation may (due to, e.g., the input parameters or size of matrix-multiplication subsystem 120 and/or various discrete units within the same) result in the performance of multiple transformation operations\non a given input volume or channel.  In this example, step 320 may include receiving an initial portion of the input volume (e.g., input matrix 404 in FIG. 5) from cache 110 and then transforming the initial portion of the input volume via matrix\nmultiplication.  Additional or subsequent portions of the input volume may then be read and transformed either in parallel or upon completion of the process flow outlined in FIG. 3.\n In other examples, matrix-multiplication subsystem 120 may transform the entirety of an input volume or channel a single time and then store the results of the same in cache 110.  In these examples, when the input volume is to be retrieved for a\nsubsequent convolution, the transformed input stream may be retrieved instead.\n At step 330, the systems described herein may transform, via matrix multiplication, a parameter matrix that includes convolution parameters for performing a convolution operation.  For example, matrix-multiplication subsystem 120 may transform\nparameter matrix 113 (which may, as detailed above, represent a filter map or weight matrix determined by training a neural network) using a matrix-multiplication operation, as represented by parameter matrix transform 124 in FIG. 1.\n The systems described herein may perform step 330 in a variety of ways.  In one example, dot-product engine 230 may use the pre-configured matrix-transform coefficients 232 to transform parameter matrix 113.  For example, and as shown by\nparameter transform 650 illustrated in FIG. 6, dot-product engine 230 may multiply a parameter matrix 604 by a set of matrix-transform coefficients 602.  Dot-product engine 230 may then multiply the results of the prior operation by a transposed version\nof matrix-transform coefficients 602 (e.g., transposed matrix-transform coefficients 606), resulting in transformed input matrix 608.  As detailed above, matrix-transform coefficients 602 may be pre-determined and/or pre-selected in a variety of ways\nbased on a variety of factors (including those outlined in the Lavin and Gray paper previously identified and incorporated herein).\n As with input matrix 112, in some examples the systems described herein may transform a parameter matrix on-the-fly when reading the same from memory.  For example, because matrix-multiplication subsystem 120 may be pre-configured or hardcoded\nwith matrix-transform coefficients 602 (and/or transposed versions of the same), matrix-multiplication subsystem 120 may transform parameter matrix 113 as it reads the same from cache 110 via dot-product engine 230.\n At step 340, the systems described herein may apply the transformed parameter matrix to the transformed input matrix via element-wise multiplication.  For example, matrix-multiplication subsystem 120 may apply, via an element-wise multiplication\noperation, the transformed parameter matrix from step 330 to the transformed input matrix from step 320, as represented by element-wise multiplication 126 in FIG. 1.  The term \"element-wise multiplication\" may, in some examples, refer to a binary\noperation that takes two matrices of the same dimension as an input (such as the transformed input and parameter matrices from steps 320 and 330) and produces, as an output, another matrix in which each element is the product of corresponding elements\nfrom the original matrices.\n As detailed above, time-domain convolutions (in which an input matrix is convolved with a parameter matrix) traditionally require numerous arithmetic operations, such as dot-product operations iterated across matrix elements.  As the sizes of\nthe input volumes and/or the number of convolution layers within a neural network increase, these convolutions may require significant processing resources.  By transforming input matrix 112 and parameter matrix 113 into a different domain (e.g., the\nfrequency domain or the so-called Winograd domain), however, matrix-multiplication subsystem 120 may reduce the convolution of input matrix 112 and parameter matrix 113 into a simplified element-wise multiplication operation in which corresponding\nelements of matrices are multiplied together, resulting in fewer overall arithmetic operations.  Element-wise multiplication operations may also take advantage of hardware, such as GEMM units or other matrix multiplication units within\nmatrix-multiplication subsystem 120, designed to efficiently perform such operations.\n The systems described herein may perform step 340 in a variety of ways.  For example, and as shown by element-wise multiplication 700 illustrated in FIG. 7, element-wise multiplier 240 may multiply, using element-wise multiplication, transformed\ninput matrix 408 by transformed parameter matrix 508, resulting in transformed output matrix 702.  As detailed above, the domain-transformation operations performed by dot-product engine 230 in steps 320 and 330 may enable element-wise multiplier 240 to\nperform a relatively straightforward element-wise multiplication operation in step 340, which may in turn reduce the total number of arithmetic operations required to convolve input matrix 112 and parameter matrix 113.\n At step 350, the systems described herein may perform an inverse-transformation operation on the results of the element-wise multiplication operation to create an output matrix for the convolution operation.  For example, matrix-multiplication\nsubsystem 120 may perform an inverse-transformation operation on the results of step 340 to create an output matrix 114, as represented by inverse transform 128 in FIG. 1.\n As detailed above, the results of step 340 may represent a domain-transformed convolution of input matrix 112 and parameter matrix 113.  In order to return this domain-transformed convolution to the time-domain, matrix-multiplication subsystem\n120 may perform an inverse-transformation operation on the same, resulting in output matrix 114.  The convolution operations described herein may produce a variety of outputs, which may be referred to as \"output volumes\" or \"output matrices.\" In some\nexamples, the terms \"output volume\" or \"output matrix\" may refer to a transformed input volume or matrix, such as the result of convolving an input matrix with a parameter matrix.  As with input volumes, output volumes may include one-dimensional\noutputs, two-dimensional outputs, three-dimensional outputs, etc. In addition, output matrix 114 may represent either a subset or the entirety of an output volume.\n The systems described herein may perform step 350 in a variety of ways.  In one example, dot-product engine 230 may use the pre-configured matrix-transform coefficients 232 to perform an inverse-transformation operation on the results of step\n340.  For example, and as shown by inverse transform 800 illustrated in FIG. 8, dot-product engine 230 may multiply transformed output matrix 702 (from step 340) by a set of matrix-transform coefficients 802.  Dot-product engine 230 may then multiply the\nresults of the prior operation by a transposed version of matrix-transform coefficients 802 (e.g., transposed matrix-transform coefficients 806), resulting in output matrix 808.  As detailed above, matrix-transform coefficients 802 may be pre-determined\nand/or pre-selected in a variety of ways based on a variety of factors (including those outlined in the Lavin and Gray paper previously identified and incorporated herein).\n Similar to steps 320 and 330, in some examples the systems described herein may, in step 350, perform an inverse transformation operation on transformed output matrix 702 on-the-fly when storing the same to memory.  For example, because\nmatrix-multiplication subsystem 120 may be pre-configured or hardcoded with matrix-transform coefficients 802 (and/or transposed versions of the same), matrix-multiplication subsystem 120 may transform the results of step 340 back into the time domain as\nit stores the same to cache 110 via dot-product engine 230.  Upon completion of step 350, the process flow of method 300 in FIG. 3 may terminate.\n As detailed above, the systems described herein may use pre-configured matrix-transform coefficients to perform one or more of the transform or inverse-transform operations described herein.  In one example, matrix-multiplication subsystem 120\nmay be pre-configured with a single set of matrix-transform coefficients (e.g., one of matrix-transform coefficients 402, 602, or 802).  In other examples, matrix-multiplication subsystem 120 may be pre-configured with multiple or all possible\nmatrix-transform coefficients.  As such, matrix-multiplication subsystem 120 may be pre-configured with matrix-transform coefficients sufficient to transform one or more input matrices, transform one or more parameter matrices, inverse-transform one or\nmore element-wise multiplication operations, and/or any possible variation or combination of the same.\n In some examples, matrix-multiplication subsystem 120 may include a discrete unit dedicated to each transformation (or inverse-transformation) operation described herein.  For example, matrix-multiplication subsystem 120 may include at least one\nGEMM or other matrix-multiplication unit dedicated to performing input-matrix transforms (hardcoded, e.g., with the matrix-transform coefficients used to transform input matrices), at least one GEMM or other matrix-multiplication unit dedicated to\nperforming parameter-matrix transforms (hardcoded, e.g., with the matrix-transform coefficients used to transform parameter matrices), and/or at least one GEMM or other matrix-multiplication unit dedicated to performing inverse-transform operations\n(hardcoded, e.g., with the matrix-transform coefficients used to perform inverse-transform operations).\n In certain implementations, matrix-multiplication subsystem 120 may include a plurality of GEMMs or other matrix-multiplication units, each of which may be pre-configured or hardcoded with a discrete set of matrix-transform coefficients\ncorresponding to a particular volume or channel.  Such a configuration may, in turn, enable matrix-multiplication subsystem 120 to perform a plurality of transformations in parallel, potentially resulting in increased computing performance and/or a\nreduction in memory or power usage.\n In some examples, one or more of the transformation operations described herein may be stored in memory for reuse in subsequent convolution operations.  For example, since a single parameter or filter matrix is often applied multiple times to a\ngiven input volume (e.g., when applying a sliding filter window to an input volume), matrix-multiplication subsystem 120 may store the transformed parameter matrix in cache 110 so that, in future convolution operations involving the parameter matrix,\nmatrix-multiplication subsystem 120 (e.g., element-wise multiplier 240) can read the transformed parameter matrix directly from cache 110 instead of again transforming the parameter matrix.  Matrix-multiplication subsystem 120 may take a similar approach\nwith an input matrix to which multiple parameter matrices are to be applied.  By storing transformed input and parameter matrices in this manner, the systems described herein may amortize the computing cost associated with transforming such matrices\nacross multiple convolution operations, potentially limiting the overhead cost or impact of the same against any single convolution operation.\n Since, as detailed above, the same transformations are often reused across multiple convolution operations, in one example the systems described herein may batch multiple element-wise multiplication operations as a single matrix-multiplication\noperation in order to optimize the utilization of the discrete units within hardware accelerator 100.  For example, when processing multiple input/parameter matrix combinations, hardware accelerator 100 may (1) pose each element-wise multiplication of an\ninput/parameter matrix pair (such as the results of step 340) as a dot-product operation (using, e.g., one or more Basic Linear Algebra Subprograms (BLAS) or sequences, such as BLAS1&gt;BLAS2&gt;BLAS3) and then (2) batch a plurality of such dot-product\noperations together into a larger matrix-multiplication operation for processing by matrix-multiplication subsystem 120.  By processing multiple input matrices simultaneously in this manner, hardware accelerator 100 may more efficiently utilize cache 110\nand/or the various units of matrix-multiplication subsystem 120, potentially resulting in increased computing performance and/or a reduction in memory or power usage.\n As detailed above, the hardware accelerators and computing systems disclosed herein may provide a number of features and advantages over traditional systems.  For example, the pre-configured, specialized hardware discussed above may, in addition\nto reducing the overall number of arithmetic operations required to perform a convolution operation, accelerate convolution computation, reduce memory usage, reduce energy consumption and heat generation, lower costs, and/or provide a number of other\nbenefits.\n The hardware accelerators detailed above may enable computing systems to realize the above-described benefits across a wide variety of computing tasks and workloads, including machine learning.  FIG. 9 is a block diagram of an exemplary neural\nnetwork 900 that may benefit from the hardware accelerators disclosed herein.  As shown in FIG. 9, neural network 900 may include a variety of different types of layers 910.  In convolution layer 912, an input 902 may undergo convolutional\ntransformations, which may be calculated by hardware such as hardware accelerator 100 in FIGS. 1 and 2, server 1006 in FIG. 10, and/or computing system 1110 in FIG. 11.  For example, input 902 may undergo convolutions based on the filters and\nquantization parameters of convolution layer 912 to produce feature maps 904.  In some embodiments, convolution layer 912 may also include a rectification sublayer (e.g., a rectified linear unit, or RELU) with an activation function.\n FIG. 9 also shows that feature maps 904 output by convolution layer 912 may undergo subsampling (e.g., pooling), based on the filters and parameters of subsampling layer 914, to produce feature maps 906, which may be reduced-size feature maps. \nThe convolution and subsampling of layers 912 and 914 may be performed a single time or multiple times before sending an output (e.g., feature maps 906) to a fully connected layer, such as fully connected layer 916.  Fully connected layer 916 may then\nprocess feature maps 906 to identify the most probable inference or classification for input 902 and may provide this classification or inference as output 920.  In some examples, the hardware accelerators described herein may enable all or some of\nlayers 910 within neural network 900 to be computed faster and/or more efficiently, leading to the performance benefits and/or energy savings detailed above.\n FIG. 10 illustrates an exemplary network environment 1000 (such as a social network environment) in which aspects of the present disclosure may be implemented.  As shown, network environment 1000 may include a plurality of computing devices\n1002(1)-(N), a network 1004, and a server 1006.  In one example, server 1006 may host a social network or may be part of a system that hosts a social network.  In this example, server 1006 may include one or more of the hardware accelerators described\nherein, such as hardware accelerator 100.\n Computing devices 1002(1)-(N) may each represent a client device or a user device, such a desktop computer, laptop computer, tablet device, smartphone, or other computing device.  Each of computing devices 1002(1)-(N) may include a physical\nprocessor (e.g., physical processors 1030(1)-(N)), which may represent a single processor or multiple processors, and a memory device (e.g., memory devices 1040(1)-(N)), which may store instructions (e.g., software applications) or data.\n Computing devices 1002(1)-(N) may be communicatively coupled to server 1006 through network 1004.  Network 1004 may be any communication network, such as the Internet, a Wide Area Network (WAN), or a Local Area Network (LAN), and may include\nvarious types of communication protocols and physical connections.\n As noted, server 1006 may host a social network, and in such embodiments, computing devices 1002(1)-(N) may each represent an access point (e.g., an end-user device) for the social network.  In some examples, a social network may refer to any\ntype or form of service that enables users to connect through a network, such as the Internet.  Social networks may enable users to share various types of content, including web pages or links, user-generated content such as photos, videos, posts, and/or\nto make comments or message each other through the social network.\n In some embodiments, server 1006 may access data (e.g., data provided by computing devices 1002(1)-(N)) for analysis.  For example, server 1006 may perform (using, e.g., hardware accelerator 100) various types of AI or machine-learning tasks on\ndata.  For instance, server 1006 may use AI or machine-learning algorithms to rank feeds and search results, to identify spam, pornography, and/or other misleading content, to perform speech recognition (e.g., to automatically caption videos), to\nautomate translation from one language to another, to enable natural language processing, to enable computer vision (e.g., to identify objects in images, to turn panoramic photos into interactive 360 images, etc.), and/or to perform a variety of other\ntasks.  In one example, by incorporating one or more of the hardware accelerators described herein (e.g., hardware accelerator 100), server 1006 may, when performing such tasks, realize the performance benefits and/or energy savings detailed above.\n Embodiments of the instant disclosure may also be applied to various environments in addition to or instead of social networking environments.  For example, the systems and methods disclosed herein may be used in video game development and game\nplay (e.g., in reinforcement-learning techniques), to automate robotics tasks (e.g., grasping, stabilization, navigation, etc.), in medical research (e.g., genomics, cancer research, etc.), for autonomous vehicle navigation, and/or in any other suitable\ncontext.\n In addition to being applied in a variety of technical fields, embodiments of the instant disclosure may also be applied to numerous different types of neural networks.  For example, the systems and methods described herein may be implemented in\nany AI scheme that is designed to provide brain-like functionality via artificial neurons.  In some examples (e.g., recurrent neural networks and/or feed-forward neural networks), these artificial neurons may be non-linear functions of a weighted sum of\ninputs that are arranged in layers, with the outputs of one layer becoming the inputs of a subsequent layer.  In addition, while some off the examples herein are discussed in the context of AI hardware accelerators, aspects of the present disclosure may\nalso be applied to other hardware processing systems in which convolution operations are required or useful.\n As detailed above, the computing devices and systems described and/or illustrated herein broadly represent any type or form of computing device or system capable of executing computer-readable instructions.  FIG. 11 is a block diagram of an\nexemplary computing system 1110 capable of incorporating and/or implementing one or more of the embodiments described and/or illustrated herein.  Computing system 1110 broadly represents any single or multi-processor computing device or system capable of\nexecuting computer-readable instructions.  Examples of computing system 1110 include, without limitation, workstations, laptops, client-side terminals, servers, distributed computing systems, handheld devices, or any other computing system or device.  In\nits most basic configuration, computing system 1110 may include at least one processor 1114, a system memory 1116, and one or more of the hardware accelerators described herein, such as hardware accelerator 100.\n Processor 1114 generally represents any type or form of physical processing unit (e.g., a hardware-implemented central processing unit) capable of processing data or interpreting and executing instructions.  In certain embodiments, processor\n1114 may receive instructions from a software application or module.  These instructions may cause processor 1114 to perform the functions of one or more of the exemplary embodiments described and/or illustrated herein.\n System memory 1116 generally represents any type or form of volatile or non-volatile storage device or medium capable of storing data and/or other computer-readable instructions.  Examples of system memory 1116 include, without limitation,\nRandom Access Memory (RAM), Read Only Memory (ROM), flash memory, or any other suitable memory device.  Although not required, in certain embodiments computing system 1110 may include both a volatile memory unit (such as, for example, system memory 1116)\nand a non-volatile storage device (such as, for example, primary storage device 1132, as described in detail below).\n In some examples, system memory 1116 may store and/or load an operating system 1140 for execution by processor 1114.  In one example, operating system 1140 may include and/or represent software that manages computer hardware and software\nresources and/or provides common services to computer programs and/or applications on computing system 1110.\n In certain embodiments, exemplary computing system 1110 may also include one or more components or elements in addition to processor 1114 and system memory 1116.  For example, as illustrated in FIG. 11, computing system 1110 may include a memory\ncontroller 1118, an Input/Output (I/O) controller 1120, and a communication interface 1122, each of which may be interconnected via a communication infrastructure 1112.  Communication infrastructure 1112 generally represents any type or form of\ninfrastructure capable of facilitating communication between one or more components of a computing device.  Examples of communication infrastructure 1112 include, without limitation, a communication bus (such as an Industry Standard Architecture (ISA),\nPeripheral Component Interconnect (PCI), PCI Express (PCIe), or similar bus) and a network.\n Memory controller 1118 generally represents any type or form of device capable of handling memory or data or controlling communication between one or more components of computing system 1110.  For example, in certain embodiments memory\ncontroller 1118 may control communication between processor 1114, system memory 1116, and I/O controller 1120 via communication infrastructure 1112.\n I/O controller 1120 generally represents any type or form of module capable of coordinating and/or controlling the input and output functions of a computing device.  For example, in certain embodiments I/O controller 1120 may control or\nfacilitate transfer of data between one or more elements of computing system 1110, such as processor 1114, system memory 1116, communication interface 1122, display adapter 1126, input interface 1130, and storage interface 1134.\n As illustrated in FIG. 11, computing system 1110 may also include at least one display device 1124 coupled to I/O controller 1120 via a display adapter 1126.  Display device 1124 generally represents any type or form of device capable of\nvisually displaying information forwarded by display adapter 1126.  Similarly, display adapter 1126 generally represents any type or form of device configured to forward graphics, text, and other data from communication infrastructure 1112 (or from a\nframe buffer, as known in the art) for display on display device 1124.\n As illustrated in FIG. 11, exemplary computing system 1110 may also include at least one input device 1128 coupled to I/O controller 1120 via an input interface 1130.  Input device 1128 generally represents any type or form of input device\ncapable of providing input, either computer or human generated, to exemplary computing system 1110.  Examples of input device 1128 include, without limitation, a keyboard, a pointing device, a speech recognition device, variations or combinations of one\nor more of the same, and/or any other input device.\n Additionally or alternatively, exemplary computing system 1110 may include additional I/O devices.  For example, exemplary computing system 1110 may include I/O device 1136.  In this example, I/O device 1136 may include and/or represent a user\ninterface that facilitates human interaction with computing system 1110.  Examples of I/O device 1136 include, without limitation, a computer mouse, a keyboard, a monitor, a printer, a modem, a camera, a scanner, a microphone, a touchscreen device,\nvariations or combinations of one or more of the same, and/or any other I/O device.\n Communication interface 1122 broadly represents any type or form of communication device or adapter capable of facilitating communication between exemplary computing system 1110 and one or more additional devices.  For example, in certain\nembodiments communication interface 1122 may facilitate communication between computing system 1110 and a private or public network including additional computing systems.  Examples of communication interface 1122 include, without limitation, a wired\nnetwork interface (such as a network interface card), a wireless network interface (such as a wireless network interface card), a modem, and any other suitable interface.  In at least one embodiment, communication interface 1122 may provide a direct\nconnection to a remote server via a direct link to a network, such as the Internet.  Communication interface 1122 may also indirectly provide such a connection through, for example, a local area network (such as an Ethernet network), a personal area\nnetwork, a telephone or cable network, a cellular telephone connection, a satellite data connection, or any other suitable connection.\n In certain embodiments, communication interface 1122 may also represent a host adapter configured to facilitate communication between computing system 1110 and one or more additional network or storage devices via an external bus or\ncommunications channel.  Examples of host adapters include, without limitation, Small Computer System Interface (SCSI) host adapters, Universal Serial Bus (USB) host adapters, Institute of Electrical and Electronics Engineers (IEEE) 1394 host adapters,\nAdvanced Technology Attachment (ATA), Parallel ATA (PATA), Serial ATA (SATA), and External SATA (eSATA) host adapters, Fibre Channel interface adapters, Ethernet adapters, or the like.  Communication interface 1122 may also allow computing system 1110 to\nengage in distributed or remote computing.  For example, communication interface 1122 may receive instructions from a remote device or send instructions to a remote device for execution.\n In some examples, system memory 1116 may store and/or load a network communication program 1138 for execution by processor 1114.  In one example, network communication program 1138 may include and/or represent software that enables computing\nsystem 1110 to establish a network connection 1142 with another computing system (not illustrated in FIG. 11) and/or communicate with the other computing system by way of communication interface 1122.  In this example, network communication program 1138\nmay direct the flow of outgoing traffic that is sent to the other computing system via network connection 1142.  Additionally or alternatively, network communication program 1138 may direct the processing of incoming traffic that is received from the\nother computing system via network connection 1142 in connection with processor 1114.\n Although not illustrated in this way in FIG. 11, network communication program 1138 may alternatively be stored and/or loaded in communication interface 1122.  For example, network communication program 1138 may include and/or represent at least\na portion of software and/or firmware that is executed by a processor and/or ASIC incorporated in communication interface 1122.\n As illustrated in FIG. 11, exemplary computing system 1110 may also include a primary storage device 1132 and a backup storage device 1133 coupled to communication infrastructure 1112 via a storage interface 1134.  Storage devices 1132 and 1133\ngenerally represent any type or form of storage device or medium capable of storing data and/or other computer-readable instructions.  For example, storage devices 1132 and 1133 may be a magnetic disk drive (e.g., a so-called hard drive), a solid state\ndrive, a floppy disk drive, a magnetic tape drive, an optical disk drive, a flash drive, or the like.  Storage interface 1134 generally represents any type or form of interface or device for transferring data between storage devices 1132 and 1133 and\nother components of computing system 1110.\n In certain embodiments, storage devices 1132 and 1133 may be configured to read from and/or write to a removable storage unit configured to store computer software, data, or other computer-readable information.  Examples of suitable removable\nstorage units include, without limitation, a floppy disk, a magnetic tape, an optical disk, a flash memory device, or the like.  Storage devices 1132 and 1133 may also include other similar structures or devices for allowing computer software, data, or\nother computer-readable instructions to be loaded into computing system 1110.  For example, storage devices 1132 and 1133 may be configured to read and write software, data, or other computer-readable information.  Storage devices 1132 and 1133 may also\nbe a part of computing system 1110 or may be a separate device accessed through other interface systems.\n Many other devices or subsystems may be connected to computing system 1110.  Conversely, all of the components and devices illustrated in FIG. 11 need not be present to practice the embodiments described and/or illustrated herein.  The devices\nand subsystems referenced above may also be interconnected in different ways from that shown in FIG. 11.  Computing system 1110 may also employ any number of software, firmware, and/or hardware configurations.\n The process parameters and sequence of the steps described and/or illustrated herein are given by way of example only and can be varied as desired.  For example, while the steps illustrated and/or described herein may be shown or discussed in a\nparticular order, these steps do not necessarily need to be performed in the order illustrated or discussed.  The various exemplary methods described and/or illustrated herein may also omit one or more of the steps described or illustrated herein or\ninclude additional steps in addition to those disclosed.\n The preceding description has been provided to enable others skilled in the art to best utilize various aspects of the exemplary embodiments disclosed herein.  This exemplary description is not intended to be exhaustive or to be limited to any\nprecise form disclosed.  Many modifications and variations are possible without departing from the spirit and scope of the instant disclosure.  The embodiments disclosed herein should be considered in all respects illustrative and not restrictive. \nReference should be made to the appended claims and their equivalents in determining the scope of the instant disclosure.\n Unless otherwise noted, the terms \"connected to\" and \"coupled to\" (and their derivatives), as used in the specification and claims, are to be construed as permitting both direct and indirect (i.e., via other elements or components) connection. \nIn addition, the terms \"a\" or \"an,\" as used in the specification and claims, are to be construed as meaning \"at least one of.\" Finally, for ease of use, the terms \"including\" and \"having\" (and their derivatives), as used in the specification and claims,\nare interchangeable with and have the same meaning as the word \"comprising.\"", "application_number": "15839229", "abstract": " A special-purpose hardware accelerator may include a cache configured to\n     store an input matrix related to performing a convolution operation and a\n     matrix-multiplication subsystem pre-configured with matrix-transform\n     coefficients for performing matrix-transform operations. The\n     matrix-multiplication subsystem may perform the convolution operation by\n     (1) reading the input matrix from the cache, (2) transforming the input\n     matrix via matrix multiplication, (3) transforming, via matrix\n     multiplication, a parameter matrix that includes convolution parameters\n     for performing the convolution operation, (4) applying the transformed\n     parameter matrix to the transformed input matrix via an element-wise\n     multiplication operation, and then (5) performing an\n     inverse-transformation operation on the results of the element-wise\n     multiplication operation to create an output matrix for the convolution\n     operation. Various other systems and methods are also disclosed.\n", "citations": ["20120221617", "20140289445", "20180189237", "20180248562", "20180293691"], "related": []}, {"id": "20190182790", "patent_code": "10375667", "patent_name": "Enhancing indoor positioning using RF multilateration and optical sensing", "year": "2019", "inventor_and_country_data": " Inventors: \nKothari; Shanay Ravin (San Francisco, CA), Snyder; Ian Matthew (San Francisco, CA)  ", "description": "TECHNICAL FIELD\n The present disclosure pertains to wireless cameras, and more specifically to wireless cameras using RF multilateration and machine vision.\nBACKGROUND\n Wireless security cameras are closed-circuit television (CCTV) cameras that transmit a video and audio signal to a wireless receiver through a radio band.  Many wireless security cameras require at least one cable or wire for power--the term\n\"wireless\" is sometimes used to refer only to the transmission process of video and/or audio.  However, some wireless security cameras are battery-powered, making the cameras truly wireless from top to bottom.\n Wireless cameras are proving very popular among modern security consumers due to their low installation costs and flexible mounting options.  For example, there is no need to run expensive video extension cables, and wireless cameras can be\nmounted and/or installed in locations previously unavailable to standard wired cameras.  In addition to the ease of use and convenience of access, wireless security cameras allow users to leverage broadband wireless internet to provide seamless video\nstreaming over the internet.\n Indoor tracking of people and objects is an area of critical importance for a wide variety of industries.  Purely radio frequency (RF) or purely camera based (e.g., machine vision) tracking solutions have performance or corner case limitations\nthat prevent them from becoming robust business intelligence tools.\n For example, all existing methods of RF based indoor positioning have several limitations, ranging from large position inaccuracy (e.g., methods such as RF proximity, Received Signal Strength Indicator (RSSI) trilateration) to complex hardware\narchitectures (e.g., RF triangulation, Time of Arrival (ToA), Time Difference of Arrival (TDoA)) to hefty processing requirements (e.g., RSSI fingerprinting).  RSSI, or the Received Signal Strength Indicator, is a measure of the power level that a RF\ndevice, such as WiFi or 3G client, is receiving from the radio infrastructure at a given location and time.  Other methods, such as RF multi angulation, use complex phased antenna arrays to determine both the RSSI and angle of arrival of an incoming RF\nsignal.  However, multiple radios dedicated to just this on a single device are needed in order to work.  Moreover, RF Time of Arrival methods are cost prohibitive for anything that is shorter range than GPS because the hardware required to detect\nshorter flights is too expensive for commercial deployment.\n Another method of increasing the accuracy of RF based indoor positioning is the use of RSSI fingerprinting to better model the RF surroundings.  Traditionally this is done by placing a fixed beacon at a known distance from the access points and\ncontinuously monitoring the RSSI of its emissions.  These are compared to the fixed Line of Sight approximated values to better model the access points surroundings.  Modelling accuracy tends to increase with the total number of beacons deployed. \nHowever, deploying additional always-on beacons increases cost, and the total number of beacons rises at 1/3 the rate of the deployed access points for the least accurate method.  Accordingly, in certain high deployment scenarios, one might not have the\nspace to accommodate this.\n Meanwhile, camera based indoor tracking solutions using computer vision/machine vision struggle with accuracy, even when using most advanced deep learning algorithms (trained by large datasets) and using very powerful hardware in the cloud.  And\nfor the instances in which all processing needs to be done on the device, there are even more constraints.  There is a need to reduce computer vision processing requirements so that the processing requirements can fit within the camera's processing\nbudget, but still offer people and object tracking benefits to users. BRIEF DESCRIPTION OF THE DRAWINGS\n The above-recited and other advantages and features of the present technology will become apparent by reference to specific implementations illustrated in the appended drawings.  A person of ordinary skill in the art will understand that these\ndrawings only show some examples of the present technology and would not limit the scope of the present technology to these examples.  Furthermore, the skilled artisan will appreciate the principles of the present technology as described and explained\nwith additional specificity and detail through the use of the accompanying drawings in which:\n FIG. 1 illustrates an example embodiment of a system using a camera and wireless access points based approach to locating one or more targets.\n FIG. 2 shows an example implementation of a single camera setup embodiment including bilateration and trilateration zones;\n FIG. 3 shows an example implementation of a multiple camera setup embodiment;\n FIG. 4 is a flow chart illustrating an example embodiment for single device detection;\n FIGS. 5, 6, and 7 illustrates example embodiments of target tracking, including visible and invisible object tracking.\n FIG. 8 is a flow chart illustrating an example embodiment for multiple device detection; and\n FIG. 9 shows an example of a system for implementing certain aspects of the present technology.\nDESCRIPTION OF EXAMPLE EMBODIMENTS\n Various examples of the present technology are discussed in detail below.  While specific implementations are discussed, it should be understood that this is done for illustration purposes only.  A person skilled in the relevant art will\nrecognize that other components and configurations may be used without parting from the spirit and scope of the present technology.\nOverview\n Systems, methods, and devices are disclosed for enhancing target positioning.  A broadcast signal from a device is received by one or more access points, where a first position area of the device is determined from an analysis of the broadcast\nsignal.  A second position area of the target, which is within the first position area, is determined by scanning pixels within the first position area in an image captured by the camera.  Based on the scanned pixels, at least one target comprising a\nportion of the pixels is detected.  The target within the image is classified, and based on the classification and portion of the pixels comprising the target, the second position area of the target within the first position of the image is triangulated.\nExample Embodiments\n The disclosed technology addresses the need in the art for developing an easy to use and easy to deploy camera and wireless access point(s) system without the limitations due to large position inaccuracy, complex hardware architectures, and/or\nhefty processing requirements (e.g., methods like RF proximity, RSSI trilateration, RF triangulation, ToA, TDoA, RSSI fingerprinting, etc.).  Thus, while these processes can be used individually for the detection and location of people and/or objects,\nthese processes are both computationally and financially expensive.  Moreover, these detection techniques produce low detection confidence, especially for wireless cameras.  Accordingly, computer vision processing requirements need to be reduced so that\nthe processing requirements can fit within the camera's processing budget, and yet still offer accurate people and object tracking capabilities.\n The disclosed technology provides a solution to the technological problems outlined above by using a process that combines aspects of RF multilateration with machine vision (e.g., camera sensing).  By using the output of RF multilateration\ntechniques in conjunction with machine vision (such as in optical sensing), more accurate methods, systems, and devices can be made for enhancing target positioning within images captured by a wireless camera.  Moreover, since this process reduces the\nsearch area around the target, processing time and resources are reduced as well.  This provides the benefit of saving on expensive hardware and increasing processing speeds without sacrificing tracking and location accuracy.\n Applying the techniques of RF multilateration with camera machine vision provides a number of benefits.  For example, the technique enables the accuracy of RF multi-angulation based indoor positioning without the hardware complexity and can\nprovide accurate indoor (e.g., 2D) positioning using bilateration (a method that was hitherto impossible to do with just RF).  The technique also enables higher confidence in computer vision object detection by pairing radio broadcast signature\ninformation to what is specifically or currently being looked at, and can confidently locate visually obscured or \"invisible\" objects.  Moreover, the technique allows for tracking and/or detecting additional context to the objects being tracked that can\nbe highly valuable for business intelligence purposes.\n Thus, in the embodiments disclosed, target positioning is enhanced when a broadcast signal from a device is received by at least two wireless access points.  From the at least two wireless access points, a first position area of the device is\ndetermined from an analysis of the broadcast signal according to a multilateration model (or similar model, such as methods from RSSI bilateration, trilateration, etc.).  A position of the target can be refined within the first position area by scanning,\nby a camera using machine vision (e.g., optical sensing), pixels within the first position area only.  Based on those scanned pixels, at least one target is detected as being present in an image.  Each target within the image is classified based on type,\nand, based on the classification and portion of the pixels making up the target, a second, more accurate position area of the target is triangulated within the first position area of the image.\n FIG. 1 illustrates an example embodiment of a system using a camera and wireless access points to locate one or more targets.  System 100 includes multiple wireless access points 120 in communication with device 130 (e.g., a target device to be\nlocated).  While at least two wireless access points 120 for capturing a broadcast signal from device 130 are needed in system 100, device 130 may be one or more devices including, but not limited to, mobile phones, laptops, PDAs, badges, etc.\n System 100 further includes camera 110 in communication with wireless access points 120.  Camera 110 includes at least one receiver for receiving, from wireless access points 110, a broadcast signal from device 130.  System 100 can determine a\nfirst position area corresponding to the location of device 130 by analyzing the broadcast signal according to multilateration model 112.  Multilateration model 112 can include any number of models corresponding to RF tracking, including bilateration,\ntrilateration, and/or any similar models.  In addition, while the embodiments depicted in FIG. 1 show multilateration model 112 being performed at camera 110, other embodiments may perform multilateration model 112 remote from camera 110 (such as in the\ncloud).\n Camera 110 is also trained with algorithms designed to locate both people and objects within an environment.  Camera sensing service 114, for example, analyzes pixels within video frames captured by camera 110 in order to detect people and/or\nobjects (e.g., targets) within its captured field of view (FOV).  This is most commonly done through machine vision that distinguishes objects through optical sensing, although infrared cameras may use analogous infrared sensing to distinguish objects\n(and indeed, any frequency range can be used as long as the target object emits them).\n In some embodiments, system 100 includes broadcast signature database 140, which can be a cloud connected database of radio broadcast signatures.  Broadcast signatures can include, for example, the OUI/MAC address specified by the IEEE and/or\nany other identifying information in the radio transmission format.  In some embodiments, broadcast signature database 140 can be local to camera 110.\n An advantage of system 100 is that most of it requires a one-time initial setup, requiring updates only when a new node is added to the mix (i.e., a wireless access point or camera).\n FIG. 2 shows an example implementation of a single camera setup embodiment including bilateration and trilateration zones.  In the embodiment shown, two wireless access points (210, 220) and camera 230 are shown.  Camera 230 also acts as an\naccess point as well as wireless access points 210, 220 in the shown embodiment, although other embodiments may include a camera without wireless access capabilities and/or may include any number of additional wireless access points.\n Referring back to the embodiment shown, the position of each node of the system (e.g., camera 230 and wireless access points 210, 220) and the scanning radio coverage for each of the wireless access points are known to some accuracy within their\nsurrounding environment.  The scanning radio coverage ideally extends to some radius from the wireless access point (assuming there are no intervening obstructions), and can be known or provided by OEM.\n The angle of camera 230 with respect to the floor of the environment is also known to some accuracy.  This allows accurate mapping of camera 230's pixels to its field of view, FOV (FOV is denoted in FIG. 2 as 232).  In some implementations,\nscanning device positioning (e.g., wireless access points 210, 220 and/or camera 230 with wireless access capabilities) or camera angle with respect to the floor may be automatically determined using sensors inside the hardware of the devices.\n Each wireless access point scans for the broadcast signature of a device within the zone of its radio coverage capabilities.  If the broadcast signature of the device is detected by two or more wireless access points, RSSI multilateration,\ntrilateration, or a similar technique can be used to determine a rough estimate of the device's location.  For example, in FIG. 2, the device is located within trilateration zone 240 if camera 230, wireless access point 210, and wireless access point 220\ndetect the broadcast signature.  Conversely, the device is located within bilateration zone 242 if only camera 230 and wireless access point 210 detect the broadcast signature; is located within bilateration zone 244 if only wireless access point 210 and\nwireless access point 220 detect the broadcast signature; and is located in bilateration zones 246a or 246b if only camera 230 and wireless access point 220 detect the broadcast signature.\n The rough estimate of the device's location can then be improved through computer vision algorithms on camera 230.  Machine vision via the use of computer vision algorithms can be performed in only the zone identified through the RSSI\nmultilateration techniques (or similar) Thus, if RSSI multilateration indicates that the device is within trilateration zone 240, computer vision algorithms would be performed only to the pixels that map to the area within trilateration zone 240.  This\ngives the system the ability to significantly lower the computational requirements for object tracking, and/or increases the performance and accuracy for a given computational resource pool.  During the search, camera 230 will detect within its FOV 232\nany objects or persons of interest (e.g., target object) for location and/or tracking.\n The location of the detected target can be determined from a scan of the given pixel grid captured by camera 230, since the camera tilt angle and the FOV to pixel mapping is known.  Thus, the system can triangulate or narrow down the location of\na target to a more accurate position within trilateration zone 240.  Thus, the location capabilities of the system is limited only by the resolution limit of camera 230 and/or the distance of the detected target while saving computational power,\nresources, and increasing processing speed.\n FIG. 3 illustrates a variation of system 100 in which two cameras with wireless access capabilities are shown.  Camera 310 is positioned such that its FOV 320 overlaps a portion of camera 230's FOV 232.  In this embodiment, the location of a\ndetected target can be triangulated to even finer resolution through comparison between the two cameras.  For example, if camera 230 places the target within bilateration zone 244, whether the target appears in camera 310's FOV 320 can determine whether\nthe location can be refined to zone 244a (within the FOV) or 244b (outside the FOV).  Moreover, the machine vision techniques of camera 310 will have its own resolution associated with its pixel mapping abilities, and thus can narrow down the range of\npossible locations by comparison to a corresponding detection and pixel mapping done by camera 230.  Thus, the accuracy of object and/or person location increases as the number of cameras with machine vision capabilities increase.\n FIG. 4 shows a flow chart illustrating an example embodiment for single device detection, regardless of the number of cameras or wireless access points present in system 100.  The method begins when all system nodes with wireless access point\ncapabilities scan for the RSSI and/or broadcast signature of a device (step 410).  This is mainly done through RF frequencies, although scanning any frequency that the device is broadcasting or emitting at could yield the same effect.  Once the RSSI and\nbroadcast signature of the device on two or more wireless access points are received, a generic RSSI multilateration algorithm is used to obtain an approximate area where the object is located (step 412).\n Assuming that the approximated area is located in the FOV of the camera, a scan of the corresponding pixel grid can be limited to the approximated area (step 414).  Thus, the specific pixels within the approximate area can be scanned for any\nobjects or persons in the images captured by the camera.  The pixels corresponding to portions of the approximate area are known or determined based on the camera tilt angle and the FOV to pixel mapping.  If a target of interest (e.g., a device detected\nthrough RSSI or its broadcast signature, or a person associated with the device) is detected within one or more pixels, a more accurate location of the target can be determined based on pixel mapping the target to a smaller area or position.  The smaller\narea or position is limited only by the resolution of the camera (or combination of cameras), which typically have better resolution than RF multilateration.\n FIG. 5, for example, illustrates this concept.  A camera FOV with multiple objects detected by wireless access points within multilateration zone 520 is shown.  The objects detected by the wireless access points are laptop 510, access badge 512,\nand phone 514 (the cell phone in this example is within the pocket of target 516 but is still detectable through its RSSI/broadcast signature).  The camera then performs a scan of the pixels within multilateration zone 520 and detects, through machine\nvision, target 516 (in area 536), laptop 510 (in area 530), and access badge 512 (in area 532).  Phone 514 is not detected in the pixel scan since it is hidden in target 516's pocket.  In this instance, the system can take an educated guess that target\n516 in area 536 is carrying phone 514, despite not finding visual evidence through machine vision.\n Additionally, some embodiments classify the target within the image, using one or more image classification algorithms (step 416) that match the machine vision detected target to a known object.  The image classification algorithms can be one or\nmore models of devices and people in different positions or orientations.  While the image classification algorithms can be generated, stored, and/or processed locally on the camera, in some embodiments, the classification of the target can be based on a\nmodel stored remotely from the device (e.g., such as in the cloud).  The classification may further narrow down a device's dimensions and size based on a match to a known device model (which, based on the classification of the target and portion of the\npixels that include the target, can help refine or triangulate the device's position).\n In FIG. 5, for example, target 516 may be classified as a person in general (versus a device or object), or may be classified further to a particular individual (e.g., a specific employee) based on facial recognition.  Laptop 510 may be\nclassified as a device, specific type of device (e.g., a laptop), make and/or model (e.g., Apple MacBook Air 13.3-Inch laptop), etc. Access badge 512 can be classified accordingly as well.\n Thus, the method may determine, based on the classification (or lack thereof), that a device is a `known` device or an `unknown` device (step 418).  The device can be a `known` device, and subsequently identified, for example, through one or\nmore matches with the image classification model.  Or, in some embodiments, the device can be known through its broadcast signature, which may include identifying information about the device (e.g., model, manufacturer, dimensions, etc.).\n Depending on the application of this system, a list of `known` devices can be accessed to determine a match.  For example, the list of `known` devices can include, but is not limited to, an employee badge, mobile/laptop (with or without a Meraki\nSystems Manager installed), and/or any device with a detectable and distinguishable broadcast signature.  For example, in embodiments where the list of known devices is part of a cloud backed database, more data than just the Broadcast signature can be\naccessed.  For example, device information can be found in the cloud using its broadcast signature or ID (step 420).\n Device information can be any information associated with the device.  For example, an employee badge and/or mobile laptop can be associated with a known employee.  The system can include a user profile associated with the employee, including\ndistinguishing features detectable by the camera (e.g., facial features based on a photo, hair/eye color, build, height, etc.).  Other information can include device design parameters, such as device dimensions, that can provide some scale to the images\n(e.g., a laptop of known length and width carried by a person can narrow down the person's location depending on how many pixels it extends across).\n Once the device is identified by the camera's machine vision, the accuracy of its approximated location from RSSI/broadcast ID scanning can be improved based on determining the angle of arrival (AoA) from the camera (step 422) and then running a\npixel mapping model to get a triangulated position based on the AoA and multilateration model (step 242).\n In some embodiments, the system can provide security features by quickly narrowing down on any target and the surroundings, and then checking for discrepancies.  For example, a device within the environment can be identified based on a broadcast\nID or application installed on the device that connects with the wireless access points.  The broadcast ID or application installed on the device can be associated with a particular user, such as an employee, that is assigned to the device.  The\napplication can connect with a wireless access point and communicate an ID that identifies the particular user or device assigned to the user.  After the system and/or camera classifies the target in the camera images as a person, machine vision applied\nto that person can determine whether there is a discrepancy in possession of the device.  If the system identifies that the person in the camera images is not the user assigned to the device, the discrepancy in possession can be detected and security\npersonnel (or other appropriate authority) notified.\n Security features that check for discrepancies in device possession can therefore be performed with respect to any device assigned to a user, including ID badges that broadcast to wireless access points.  For example, an unauthorized individual\nwith an employee's ID badge can be flagged down based on a failure to match with the assigned employee's features (e.g., the facial features don't match, the individual is much taller than the employee, etc.).  The system can monitor for unauthorized\nindividuals continuously and around the clock as a way to increase workplace safety.  For example, in FIG. 5, facial recognition can confirm that target 516 is an employee assigned to access badge 512 (or, if there is no facial match, send a notification\nto the system that access badge 512 has been stolen).  In fact, all the devices within target 516's possession can be checked for authorization to be in possession (e.g., laptop 510, which may contain access to sensitive information).\n In some instances, however, the device may be unknown (step 418).  For an `unknown` device, there are at least two separate ways to identify an object: (1) through the broadcast signature and (2) machine vision.  These methods of device\nidentification can be used separately or in conjunction in the disclosed system.\n For an unknown device, the system can look up a vendor associated with the broadcast signature (step 426).  Additionally and/or alternatively, the camera's machine vision can determine or make an educated guess as to the device's type (e.g.,\nmobile phone, laptop, etc.) (step 428).\n For example, if the broadcast signal cannot be used to uniquely identify the device, the classification of the device can proceed by identifying a broadcast signature within the broadcast signal and identifying a type of the device based on\ncomparing the broadcast signature with a number of stored broadcast signatures with associated device information.  The broadcast signatures can, for example, be stored in a database either local to the camera or remote from the camera (such as a cloud\ndatabase).  The broadcast signature may contain an identifier that identifies the vendor of the device.  Based on a determination of the identity of the vendor, the type of the device can be inferred or a request for broadcast signatures associated with\ndevices sold or produced by the vendor can be obtained for comparison to the detected broadcast signal.\n Moreover, the type of the device can be further refined by machine vision applied to multilateration zone 520.  For example, a vendor's OUI can limit the object type down to, say, 1 or 2 types.  The vendor may produce badges only, or phones and\nlaptops only.  In FIG. 6, assuming laptop 510 is an unknown device (e.g., does not broadcast a unique ID recognized by the system), the broadcast signal may include a signature that identifies the vendor.  In this case, for example, the vendor may be\nidentified as Apple.  The type and formatting of the broadcast signature can also reveal the hardware capabilities, further limiting the possible object types.  The system and/or camera can then access a database (either compiled by the system, provided\nby Apple or another party, etc.) to match within some threshold the broadcast signal from laptop 510 that is received by the wireless access points to a number of stored Apple device profiles.  If a match is found, the system can receive, from the\ndatabase or vendor, information related to the device (such as laptop dimensions), which can be used with machine vision to confirm the device's conformation to the received device information (e.g., laptop 510 spans an area of pixels roughly equivalent\nto the dimensions provided in area 610, and therefore laptop 510 is determined to be consistent with a broadcast signature associated with an Apple MacBook Air 13.3-Inch laptop).  As a result, the OUI received by the wireless access points can be used to\ndetermine the vendor of a particular device, and this information in conjunction with applying machine vision techniques to the pixel area scan can give an accurate estimation of the device type (step 430).\n In some embodiments, the camera can track targets over a period of time to identify an unknown device.  In FIG. 5 at some time (say t=0 s), for example, the broadcast signal from laptop 510 can be contaminated with the broadcast signals from\naccess badge 512 and/or phone 514.  Thus, even if laptop 510 is broadcasting a unique signature, that unique signature may be washed out and/or overlap with those of the other devices, producing a signal that doesn't seem to match anything in the\nbroadcast signature database.  However, at some time later (t=75 s) in FIG. 6, target 516 has moved to sit down at a conference table (identified as area 620) within multilateration zone 630.  At this point, target 516 may no longer be located\nproximately with the other devices, and the camera can easily distinguish laptop 510 in area 610.  This can either confirm an initial guess and/or be used to refine RSSI fingerprinting models used in the initial multilateration techniques (see step 432).\n In some embodiments, after the device location is refined based on the angle of arrival of the device with respect to the camera, RSSI triangulation, RSSI fingerprinting, or both can also be refined or updated (step 434).  Both these models of\nlocalization can be used to further increase the baseline accuracy of the initial RSSI multilateration technique (through RSSI triangulation) and help map the immediate room environment (through RSSI fingerprinting).\n For example, a broadcast signature within the broadcast signal from a device in multilateration zone 520 can be received by the surrounding wireless access points.  The type of device can be identified based on machine vision applied to the\ninitial multilateration area.  Whether or not the device is `known` or `unknown`, once the device is identified, its broadcast signature can be noted by the system and/or camera.  The broadcast signature store can then be updated, if needed, based on the\nreceived broadcast signature and identified type of device.  In this way, the updated broadcast signature can dynamically update the broadcast signature databases over time, which increases the accuracy of the multilateration model applied to subsequent\ndevices.  In some embodiments, the broadcast signature database and multilateration model can be updated continuously.\n In some embodiments, the system and/or camera can detect `invisible` targets or objects.  This type of functionality can be performed locally on the camera or can be performed remotely (e.g., using cloud vision) to contextually find an\n`invisible` object's location (step 440).  FIGS. 5 and 7 illustrate how this is done.\n At time t=0 hours, FIG. 5 shows multilateration zone 520 that includes target 516 and phone 514.  While target 516 can be seen by the camera through machine vision, phone 514 is not visible since it's inside target 516's pocket.  Thus, since the\nbroadcast signature of phone 514 identifies the device as a mobile device, but machine vision only captures and identifies a person, laptop, and/or access badge within its FOV, the system and/or camera makes an educated guess or determination that\nmachine vision cannot detect the device associated with the target even though its presence is known (e.g., detectable by multilateration only).\n In this instance, the system and/or camera can track the target associated with the `invisible` device over time until the camera's machine vision picks it up.  Thus, for example, the system and/or camera tracks target 516 from, say, FIG. 6 with\ntarget 516 sitting at conference table at t=1 hour until some later time at FIG. 7, when target 516 has moved to another portion of the room at t=2 hours.  FIG. 7 shows when target 516 takes phone 514 out of their pocket, thus exposing phone 514 to the\ncamera's view and enabling machine vision detection.  Once the device becomes detectable by machine vision, the device is identified--in this case, phone 514 is identified as a mobile device that matches within some threshold the broadcast signature\npreviously detected by the wireless access points.\n In some instances, the device detected and identified by machine vision may not match the detected broadcast signature.  When that happens, some embodiments can update the RSSI multilateration models to increase the accuracy of the system.  For\nexample, in FIG. 7 the system/camera can identify the objects within multilateration zone 720, such as laptop 510 within area 710, target 516 within area 716, access badge 512 within area 712, and phone 514 within area 714.  The wireless access points\ncan detect a broadcast signature that initially classifies `invisible` phone 514 as a certain model (e.g., iPhone 5s).  However, once phone 514 is detected through machine vision, the camera/system may optically identify phone 514 as another model (e.g.,\niPhone X).  Using this information, the multilateration models can update and/or store the detected broadcast signature as a signature corresponding to an iPhone X rather than an iPhone 5s.\n In many instances, however, multiple devices may be within multilateration zone.  FIG. 8 is a flow chart illustrating example embodiments for multiple device detection within the same RSSI multilateration area.  Like the single device detection\nmethodology, multiple devices within the same multilateration can be detected by scanning, via the wireless access points, for the RSSI and/or broadcast signature of a device (step 810).\n Once the RSSI and broadcast signature of the device on two or more wireless access points are received, a generic RSSI multilateration algorithm can be used to obtain an initial approximate area where the object is located (step 812).  Assuming\nthat the approximated area is located in the FOV of the camera, the camera can perform a scan of the corresponding pixel grid using machine vision techniques in that initial area (step 814), and any targets within the image can be classified using one or\nmore image classification algorithms (step 816) that match the detected targets to known objects.  If no objects are within the camera's FOV or captured image, the RSSI scan (step 810) can be repeated until an object is detected.\n The system then determines whether any of the devices detected within the machine vision scan are of the same or different type using the known/unknown method discussed in FIG. 4 (step 818).  If the system can differentiate between the devices,\nthen Angle of Arrival is obtained from the camera (step 820) and the position of each device is triangulated based on running the multilateration models with the Angle of Arrival information (step 822).\n However, the methodology differs when the system cannot completely differentiate the devices.  The reasons for this could be multiple instances of similar devices closely packed together, NLOS devices with LOS devices closely packed together,\netc. In these cases, all unique broadcast ID's can be monitored and/or all of these devices can be assigned a group ID and continuously tracked over a period of time (step 824).  Various methods of separating the various members of a group into\nindividual devices are discussed below, depending on whether there is any device movement (step 826).\n Assuming there is device movement, the system and/or camera determines whether any devices are diverging or moving in a different direction than the group of devices (step 828).  If there is no divergence or the direction of movement is the\nsame, the multiple devices are tracked within the image captured by the camera as a cohesive block and a differencing model is applied to the broadcast signal to extract broadcast signals for each device (step 830).  For example, a particular broadcast\nsignature with a unique portion (say, a strong signal at 30 kHz that is not shared by other devices) can be inferred as a device within the group, and that signature can be subtracted through the differencing model from the group's signature.  The\nremoval of the signature may help in identifying other broadcast signals embedded within the group signature.\n The system/camera can continue to track the group of devices in an effort to further distinguish broadcast ID's from the rest of the group (step 832).  If there is no change in device location with respect to the other devices, the system/camera\ncan assign the group's broadcast signature an averaged RSSI ID (thus including a mixture of all the broadcast IDs) (step 834).\n If there is device movement, such as determining through machine vision that at least one of the devices is moving in a direction that diverges from the group, the moving or diverging device can be visually identified by the camera once it has\nseparated further than the camera's resolution limit.  The identification can then be matched to a broadcast signature, and the broadcast signature associated with the visually identified device can be removed from the group's broadcast signal (step\n836), such as through one or more differencing models.  Accordingly, through these various methodologies, individual devices within a group of indistinguishable devices can be detected, identified, and separated from the group.\n FIG. 9 shows an example of computing system 900 that can be used in combination with the embodiments discussed above.  For example, computing system 900 can represent any of FIGS. 1-3 and/or 5-7, or a combination of such devices.  In computing\nsystem 900 the components of the system are in communication with each other using connection 905.  Connection 905 can be a physical connection via a bus, or a direct connection into processor 910, such as in a chipset architecture.  Connection 905 can\nalso be a virtual connection, networked connection, or logical connection.\n In some embodiments computing system 900 is a distributed system in which the functions described in this disclosure can be distributed within a datacenter, multiple datacenters, a peer network, etc. In some embodiments, one or more of the\ndescribed system components represents many such components each performing some or all of the function for which the component is described.  In some embodiments, the components can be physical or virtual devices.\n Example system 900 includes at least one processing unit (CPU or processor) 910 and connection 905 that couples various system components including system memory 915, such as read only memory (ROM) and random access memory (RAM) to processor\n910.  Computing system 900 can include a cache of high-speed memory connected directly with, in close proximity to, or integrated as part of processor 910.\n Processor 910 can include any general purpose processor and a hardware service or software service, such as services 932, 934, and 936 stored in storage device 930, configured to control processor 910 as well as a special-purpose processor where\nsoftware instructions are incorporated into the actual processor design.  Processor 910 may essentially be a completely self-contained computing system, containing multiple cores or processors, a bus, memory controller, cache, etc. A multi-core processor\nmay be symmetric or asymmetric.\n To enable user interaction, computing system 900 includes an input device 945, which can represent any number of input mechanisms, such as a microphone for speech, a touch-sensitive screen for gesture or graphical input, keyboard, mouse, motion\ninput, speech, etc. Computing system 900 can also include output device 935, which can be one or more of a number of output mechanisms known to those of skill in the art.  In some instances, multimodal systems can enable a user to provide multiple types\nof input/output to communicate with computing system 900.  Computing system 900 can include communications interface 940, which can generally govern and manage the user input and system output.  There is no restriction on operating on any particular\nhardware arrangement and therefore the basic features here may easily be substituted for improved hardware or firmware arrangements as they are developed.\n Storage device 930 can be a non-volatile memory device and can be a hard disk or other types of computer readable media which can store data that are accessible by a computer, such as magnetic cassettes, flash memory cards, solid state memory\ndevices, digital versatile disks, cartridges, random access memories (RAMs), read only memory (ROM), and/or some combination of these devices.\n The storage device 930 can include software services, servers, services, etc., that when the code that defines such software is executed by the processor 910, it causes the system to perform a function.  In some embodiments, a hardware service\nthat performs a particular function can include the software component stored in a computer-readable medium in connection with the necessary hardware components, such as processor 910, connection 905, output device 935, etc., to carry out the function.\n For clarity of explanation, in some instances the present technology may be presented as including individual functional blocks including functional blocks comprising devices, device components, steps or routines in a method embodied in\nsoftware, or combinations of hardware and software.\n Any of the steps, operations, functions, or processes described herein may be performed or implemented by a combination of hardware and software services or services, alone or in combination with other devices.  In some embodiments, a service\ncan be software that resides in memory of a client device and/or one or more servers of a content management system and perform one or more functions when a processor executes the software associated with the service.  In some embodiments, a service is a\nprogram, or a collection of programs that carry out a specific function.  In some embodiments, a service can be considered a server.  The memory can be a non-transitory computer-readable medium.\n In some embodiments the computer-readable storage devices, mediums, and memories can include a cable or wireless signal containing a bit stream and the like.  However, when mentioned, non-transitory computer-readable storage media expressly\nexclude media such as energy, carrier signals, electromagnetic waves, and signals per se.\n Methods according to the above-described examples can be implemented using computer-executable instructions that are stored or otherwise available from computer readable media.  Such instructions can comprise, for example, instructions and data\nwhich cause or otherwise configure a general purpose computer, special purpose computer, or special purpose processing device to perform a certain function or group of functions.  Portions of computer resources used can be accessible over a network.  The\ncomputer executable instructions may be, for example, binaries, intermediate format instructions such as assembly language, firmware, or source code.  Examples of computer-readable media that may be used to store instructions, information used, and/or\ninformation created during methods according to described examples include magnetic or optical disks, solid state memory devices, flash memory, USB devices provided with non-volatile memory, networked storage devices, and so on.\n Devices implementing methods according to these disclosures can comprise hardware, firmware and/or software, and can take any of a variety of form factors.  Typical examples of such form factors include servers, laptops, smart phones, small form\nfactor personal computers, personal digital assistants, and so on.  Functionality described herein also can be embodied in peripherals or add-in cards.  Such functionality can also be implemented on a circuit board among different chips or different\nprocesses executing in a single device, by way of further example.\n The instructions, media for conveying such instructions, computing resources for executing them, and other structures for supporting such computing resources are means for providing the functions described in these disclosures.\n Although a variety of examples and other information was used to explain aspects within the scope of the appended claims, no limitation of the claims should be implied based on particular features or arrangements in such examples, as one of\nordinary skill would be able to use these examples to derive a wide variety of implementations.  Further and although some subject matter may have been described in language specific to examples of structural features and/or method steps, it is to be\nunderstood that the subject matter defined in the appended claims is not necessarily limited to these described features or acts.  For example, such functionality can be distributed differently or performed in components other than those identified\nherein.  Rather, the described features and steps are disclosed as examples of components of systems and methods within the scope of the appended claims.", "application_number": "15834990", "abstract": " Systems and methods are disclosed for enhancing target positioning. A\n     broadcast signal from a device is received by at one or more access\n     points, where a first position area of the device is determined from an\n     analysis of the broadcast signal. A second position area of the target,\n     which is within the first position area, is determined by scanning pixels\n     within the first position area in an image captured by the camera and,\n     based on the scanned pixels, at least one target comprising a portion of\n     the pixels is detected. The target within the image is classified, and\n     based on the classification and portion of the pixels comprising the\n     target, the second position area of the target within the first position\n     of the image is triangulated.\n", "citations": ["4236068", "5642303", "5751223", "6812824", "552603", "7283046", "7573862", "637569", "7975262", "8010079", "8102814", "8260320", "8284748", "8300594", "8325626", "8396485", "8446899", "8458184", "691636", "8549638", "8615254", "8644301", "8650279", "8669902", "8676182", "8682279", "8693367", "8718644", "8768389", "8849283", "8909698", "8958318", "9060352", "9130859", "9173084", "9173158", "744464", "757424", "759639", "9389992", "9426305", "767548", "776634", "9544337", "9609504", "9642167", "9654344", "9713114", "9772927", "9820105", "804450", "9858559", "9860151", "9933224", "9923780", "9967906", "9980220", "9985837", "20030087645", "20030116634", "20040169587", "20040203572", "20050090225", "20050169193", "20050186904", "20060022815", "20060030290", "20060092964", "20060126882", "20060187866", "20070037605", "20070152057", "20070239854", "20080037715", "20080084888", "20080101381", "20080163207", "20080233969", "20090129389", "20090203370", "20090282048", "20090298511", "20090307485", "20100039280", "20100097969", "20110087799", "20110142053", "20110182295", "20110194553", "20110228779", "20120023552", "20120054367", "20120088476", "20120115512", "20120157126", "20120167207", "20120182147", "20120311127", "20120324035", "20130029685", "20130039391", "20130057435", "20130077612", "20130088983", "20130107853", "20130108263", "20130115916", "20130145008", "20130155906", "20130191567", "20130203445", "20130217332", "20130232433", "20130273938", "20130317944", "20130322438", "20130343198", "20130347103", "20140007089", "20140016926", "20140025770", "20140052508", "20140059655", "20140087693", "20140105213", "20140118113", "20140148196", "20140179352", "20140191868", "20140198808", "20140233460", "20140269321", "20140302869", "20140337824", "20140341568", "20150016286", "20150016469", "20150030024", "20150043581", "20150063166", "20150065161", "20150087330", "20150103818", "20150163192", "20150172391", "20150213391", "20150223337", "20150256972", "20150264519", "20150280827", "20150288410", "20150326704", "20150358777", "20150362581", "20160007315", "20160044627", "20160099847", "20160105408", "20160127875", "20160146495", "20160344641", "20170026974", "20170214551", "20180069311", "20180084389", "20180295473"], "related": []}, {"id": "20190188522", "patent_code": "10373017", "patent_name": "Adding new connections using image recognition", "year": "2019", "inventor_and_country_data": " Inventors: \nHendlin; Zachary Garth (San Francisco, CA), Kolli; Samish Chandra (San Jose, CA), Zou; Feng (Santa Clara, CA)  ", "description": "TECHNICAL FIELD\n The present application relates generally to connecting members of a website or mobile application using image recognition.\nBACKGROUND\n A social-networking system, such as LinkedIn, may have its success or usefulness measured at least in part by its ability to generate interest among its members in connecting and engaging with other members on the platform.  The more members\nthat are regularly on the platform, the more valuable the service is for individual members (since more of their network is on the platform), recruiters, or other entities who wish to contact those members.  The amount of interest generated among the\nmembers may depend on many factors, including, for example, the effectiveness of techniques for drawing a user to the system, connecting users to other users of the system, and providing relevant content to the member. BRIEF DESCRIPTION OF THE\nDRAWINGS\n Some embodiments of the present disclosure are illustrated, by way of example and not limitation, in the FIGS. of the accompanying drawings, in which like reference numbers indicate similar elements.\n FIG. 1 illustrates, by way of example, a diagram of an embodiment of a user interface on a first user device, such as a newsfeed on a social networking application.\n FIG. 2 illustrates, by way of example, a flow diagram of an embodiment user interfaces presented to a user in adding a new connection.\n FIG. 3 illustrates, by way of example, a block diagram of an embodiment of a system for connecting with a user of a website.\n FIG. 4 is a block diagram illustrating a client-server system, in accordance with an example embodiment.\n FIG. 5 is a block diagram showing functional components of a professional social network within the networked system, in accordance with an example embodiment.\n FIG. 6 illustrates, by way of example, a diagram of an embodiment of a system for connecting users of a social network.\n FIG. 7 is a flowchart illustrating an example method, according to various embodiments.\n FIG. 8 is a block diagram of an example computer system on which operations, actions and methodologies described herein may be executed, in accordance with an example embodiment.\nDETAILED DESCRIPTION\n The present disclosure describes embodiments of systems methods, systems, apparatuses, and non-transitory machine-readable mediums for connecting users of a website or application using image recognition.\n Current techniques for adding connections between users include: (1) a user typing another user's name in an input box; (2) scanning a quick response (QR) code of another user; (3) other code scanning technologies, such as are implemented on\nFacebook Messenger provided by Facebook, Inc.  of Menlo Park, Calif., USA, or SnapChat provided by Snap, Inc.  of Venice, Calif., USA; and (3) Bluetooth or geolocation.  Typing a person's name is error prone and often does not yield a unique result, as\nmultiple people on the service can have the same name.  Typing a person's name is also time consuming.  Typing a person's name is a primary method used by users to connect on websites.  Often, when typing a person's name, a user has to sift through a\nlist of users that potentially match the name being searched and subsequently verify with photographs or other information that the match is correct.  The QR code or other code scanning technologies require a first user to access a code scanner and a\nsecond user to access their unique code to be scanned, which typically is multiple steps away from the main screen of an application.  The Bluetooth or geolocation technique requires Bluetooth or location tracking to be enabled (depleting battery of the\nmobile device), is often inaccurate, or often confused, as many devices can be within range of the device looking to make a connection particularly in large spaces (e.g. a conference).\n Embodiments allow a member (A) to connect with another member (B) by accessing a camera through a website or application and taking a picture of a newsfeed of the member to be added.  Such embodiments are simpler for a user than navigating to a\nscan code, more accurate than typing a name in a search utility, and require less battery power than geolocation or Bluetooth.\n The newsfeed can uniquely identify a user in a number of ways: (1) an order of individual updates in the newsfeed is known by a website host and is specific to the user; (2) each update has elements, such as like count or social proof from\nshared connections, that make the update differ in how it renders to different users, further reducing an eligible candidate set; and (3) layouts (e.g., feature vectors of the layouts) of the newsfeed across different platforms can be determined and used\nto reduce the candidate users.  The order of the updates can be based on the user's connections, platform (e.g., iOS, Android, web, or the like), time at which the website is accessed, or other factors.  After capturing the user's newsfeed, such as\nthrough a screen shot capture or a camera capture, machine learning, such as deep learning, can be applied to extract image features.  The image features can be compared other newsfeed image features, such as to match to a pre-existing newsfeed pattern. \nThe feature vector can be learned using a machine learning/deep learning technique, such as a convolutional neural network or a recursive neural network.  A set of features that populate the feature vector can be learned such that an information ratio of\nthe features reduces (e.g., optimally reduces) the search space (e.g. set of candidates) with each new feature added to the feature vector.  This results in a small set of features which can quickly be used to reduce the candidate set from billions, for\nexample, of newsfeeds served to a unique user.  The representative feature vector from the captured image can be calibrated to match to corresponding feed update types or modules.  A predefined set of update types can be used as training set.\n FIG. 1 illustrates, by way of example, a diagram of an embodiment of a user interface 100 on a first user device.  The user interface 100 as illustrated includes features including a search control 102, a profile control 104, a social header\n106, a profile image 108 of a user that posted the content related to the social header 106, a name of the user 110 that posted the content, a job title or other characteristics of the user 112, a degree of the connection to the user 114, text of the\npost 116, other content of the post 118, social counts 120, navigation elements 122A, 122B, 122C, 122D, and 122E, and corresponding badge counts 124A, 124B, 124C, 124D, and 124E.  One or more of the features of the user interface 100 can be used to\nreduce a number of candidate users in a candidate set list, such as to help a user connect with another user on the social network.\n A user can search for another user using the search control 102.  More details regarding the operation of the search control are provided in FIGS. 2-3, among others.  The profile control 104, when selected by a user allows a user to access and\nmanage the settings of their experience on the website.  The profile control 104 can include a display of a profile picture of the user logged into the website and for which the newsfeed is being presented.\n The social header 106 can include text indicating why the content of the post is being presented to the user.  For example, a social header can include \"John, Gary, and 3 others like this\", or \"Fred commented on this post by Herbie V.\".  The\nprofile image 108 can be the profile picture chosen by the user that made the post.  The job title or other characteristic of the user 112 can include a status identifier of the user (e.g., celebrity, politician, or the like).\n The degree of the connection to the user 114 can indicate how the user making the post is related to the user associated with the news feed.  For example, \"Herbie V. is friends with Fred and John\".  The content of the update 118 can include a\npicture, video, article, link, or the like.  The social counts 120 can indicate a number of likes, comments, shares, or the like of the post.\n The navigation element 122A-122E, when selected by a user, allows a user to navigate to different areas of the website.  For example, the navigation element 122A-122E can allow a user to navigate to their home page that hosts their news feed, a\nwebpage that displays details regarding a user's network, a webpage that provides a user access to their email through the website, notifications of relevant content on the website, jobs the user may be interested in, or the like.  The badge counts\n124A-124E indicate how many items are deemed to be of particular interest to the user using the navigation element 122A-122E, respectively.  The user interface 100 is merely an example of a user interface of a social network.  The user interface 100 can\ninclude more or fewer features, features in different locations, other features than those illustrated, or the like.\n Some of the features of the user interface 100 provide some degree of uniqueness to each user's newsfeed.  Thus, using an image of a user's newsfeed can be a reliable way to uniquely identify a user for potential connection.  In particular,\nthere are some aspects of the features of the user interface 100 that can be unique to a specific user.  For example, the social header 106 can indicate other users who have liked or commented on the post and are connected to the user.  This can reduce\nthe search space to only those users connected to the user(s) mentioned in the social header.  Social graph data (see social graph database 512 in FIG. 5 and the corresponding description thereof) can be searched to identify or reduce a set of possible\ncandidates.  Consider the degree of the connection to the user 114.  This information, like the social header, can be used to reduce (further reduce) the search space to only those connected to the user(s) mentioned in the degree of the connection to the\nuser 114.\n The social counts 120 often change over time as a post is shown to more people and the social counts increase.  The social counts 120 can be used to identify a time window in which the version of the newsfeed being photographed was presented. \nThe social counts 120 can be used to determine which user's news feeds included the corresponding update with the social counts equal to the values shown.  The social count data, similar to the social graph data (connectivity data between members) can be\nstored in a database 426 or a database in the data layer 505 (see FIGS. 4 and 5).\n Another feature of the newsfeed presented on the user interface 110 that can help uniquely identify a user is the ordering of the updates on the news feed.  In the news feed illustrated, update A precedes update B. For another user, update B can\nbe presented before update A, presented after another update, update C, or not presented at all.  A feed ranking application (e.g., an application of the applications 420, see FIG. 4) can determine an order of the updates and record the order in content\ndelivery database 514, see FIG. 5.  A query of the content delivery database 514 can return users who were presented the update A and then the update B in that order, thus reducing the number of possible users for which the newsfeed was presented.  The\nbadge count 124A-124E can similarly be recorded in the content delivery database 514 and used to filter (further filter) users that match the newsfeed and had badge counts displayed which correspond to the values in the content delivery database 514.\n While FIG. 1 does not show colors, dominant colors of a newsfeed can vary from user to user.  For example, a user can have a profile picture with one or more primarily dominant colors in the picture.  These colors can be extracted through\nanalysis of the profile picture data and compared to colors that are pre-computed for the user associated with the profile picture (e.g., the image 104).  The comparison can help filter (further filter) the users associated with the news feed.\n A decision tree can be applied for each filter (in parallel), such as to filter through possible users and uniquely determine the user associated with the newsfeed.  A decision tree is a structure in which each internal node represents a\ncondition on an attribute.  Each branch of the tree represents the outcome of the condition relative to the attribute.  The leaf nodes represent an outcome or classification of all paths to the leaf node.\n FIG. 2 illustrates, by way of example, a flow diagram of an embodiment user interfaces presented to a user in adding a new connection.  A first user interface 200A is presented to the user.  The user interface 200A as illustrated includes a\nsearch bar control 102.  The user can select the search bar control 102 and begin entering a guess at a name of another user with which they would like to connect on the website.  The user interface 200B is a view after the user has begun typing a name\nin the search control 102.  As the user types, candidate users 204A and 204B and a field of view 206 of a camera of a mobile device on which the user interface 200B can be provided as the user types.  The app that is providing the view of the user\ninterface 200A-200B can access data provided by a camera of the mobile device and render the data in the field of view 206.\n The user of the mobile device can move the mobile device to adjust the field of view 206 to include a view of a newsfeed of the user with which they would like to connect.  The newsfeed can be presented on a display of a device that is capable\nof connecting to the website.  The mobile device can record image data of the newsfeed.  The mobile device can, through the app, extract features from the image, such as can include content of the social header 106, the degree of the connection to the\nuser 114, the social counts 120, an order of the updates in the newsfeed, badge counts 124A-124E, or colors of the profile picture.  In one or more embodiments, the mobile device app can determine the feature vector using the app. In one or more other\nembodiments, the image data can be provided to a server (e.g., application server 418) so that the server can determine the feature vector.\n A feature vector can include data indicating features of the newsfeed that can be used to help uniquely identify the user associated with the newsfeed.  The features can include content of the social header 106, the degree of the connection to\nthe user 114, the social counts 120, an order of the updates in the newsfeed, badge counts 124A-124E, or colors of the profile picture.  The feature vector can be compared to pre-computed feature vectors of the content delivery database 518.  Decision\ntrees, other machine learning, or other technique can be used to filter users that are candidates for connecting.\n The user interface 200C illustrates a state of the mobile device after filtering users based on the feature vector.  A result 208 of the feature vector matching can be provided on the user interface 200C.  The result 208 can include a name of\nthe user, a profile picture of the user, profile data, such as a job title, of the user, and a connect control 210 can be provided.  In response to a user selecting the connect control 210, a connection request message can be provided to the user\nassociated with the result 208.\n A control, such as the connect control, the preview control, camera control, or the like, is a user interface (UI) control element.  A UI control element is a visual element that facilitates interaction with underlying software and/or hardware. \nA UI control can have predefined attributes, operations, and/or values that establish a consistent interaction experience for a user.\n FIG. 3 illustrates, by way of example, a block diagram of an embodiment of a system 300 for connecting with a user of a website.  The system 300 as illustrated includes a first mobile device 320, a second mobile device 324, and application\nserver 418.  The mobile device 320 can present the user interface 200A-200C of FIG. 2 and the mobile device 324 can present the user interface 100 of FIG. 1.  The mobile device 320 includes a camera 322 (in dashed lines because the camera generally faces\naway from the user interface of the mobile device 320 and the camera is not visible on the side of the mobile device 320 in the view of FIG. 3).  The camera 322 has a field of view 326.  Within the field of view 326 is a newsfeed of a user with which the\nuser of the mobile device 320 wishes to connect.  The user of the mobile device 320 can be the user associated with the information provided in the news feed.\n In response to the user tapping the camera preview control 206 (see FIG. 2) or other camera control while the newsfeed is within the field of view 326, the camera 322 can produce image data representative of attributes and dominant colors of the\nnewsfeed.  In some embodiments, the mobile device 320 can provide a feature vector or the image data to the application server 418.  In some other embodiments, the application server 418 can determine a feature vector of the image data provided by the\nmobile device 320.  A feature vector filter 306 of the application server 418 can compare the feature vector to feature vectors of newsfeeds served by the website.  A user associated with the feature vector that most closely matches, such as based on an\nL1 or L2 norm vector distance, the feature of the image data can be provided as a recommended connection, such as discussed with regard to FIG. 2.  The feature vector filter 306 and the application server 418 are discussed in more detail regarding FIGS.\n4 and 5.  These distances can be computed by generating a Histogram of Gradients (HOG) or Scale Invariant Feature Transformation (SIFT) metadata for an image and for it's server-generated comparison set, which can be compared in a multidimensional space\nfor image (non-text) attributes such as the dominant colors or contours in a profile picture or feed update.\n Consider two members: Member 1, whose newsfeed is shown in FIG. 1 and Member 2 who desires to connect with Member 1.  Member 1 can control mobile device 324 and Member 2 can control mobile device 320.  Member 2 takes a photo of member 1's\nnewsfeed with the mobile device 320.  Based on one or more features of the photo, embodiments identify Member 1.  Embodiments can analyze a set of attributes to reduce the search space of potential members using image recognition to map each feed update\nto corresponding update modules that have been served to a member.  This image recognition can be applied either on the client or server.\n If image recognition is applied on client devices, machine learning techniques can be user to extract one or more features of the photo.  The one or more features can be provided in a feature vector to a server.  The image features may include\none or more image gradients, colors, corners, shapes, or orientations.  A machine learning technique can be used to compare the current feature vector against a set of predetermined feature vectors associated with respective newsfeeds that have been\nserved by the social network.  For example, the SIFT algorithm can be used to extract the features and compare a feature vector to other predetermined feature vectors.\n Alternatively, the application clients can send the image (e.g., in a compressed or un-compressed form) to a server.  After receiving the image from the client, the server can analyze the image and apply either machine learning or deep learning\nmethods to determine the feature vector.  For example, a captured author name of a newsfeed can be translated into a string (e.g., using OCR), which can facilitate a subsequent target member to be connected.  As part of the comparison with data the\ncontent database 514, a tolerance allowance is made to allow for approximate matches, such as to account for OCR inaccuracy or variation in image feature extraction.\n FIG. 4 is a block diagram illustrating a client-server system 400, in accordance with an example embodiment.  A networked system 402 provides server-side functionality via a network 404 (e.g., the Internet or Wide Area Network (WAN)) to one or\nmore clients.  FIG. 4 illustrates, for example, a web client 406 (e.g., a browser) and a programmatic client 408 executing on respective client machines 410 and 412.\n An Application Program Interface (API) server 414 and a web server 416 are coupled to, and provide programmatic and web interfaces respectively to, one or more application servers 418.  The application servers 418 host one or more applications\n420.  The application servers 418 are, in turn, shown to be coupled to one or more database servers 424 that facilitate access to one or more databases 426.  While the applications 420 are shown in FIG. 4 to form part of the networked system 402, in\nalternative embodiments, the applications 420 may form part of a service that is separate and distinct from the networked system 402.\n Further, while the system 400 shown in FIG. 4 employs a client-server architecture, the present disclosure is not limited to such an architecture, and could be used in a distributed, or peer-to-peer, architecture system, for example.  The\nvarious applications 420 could also be implemented as standalone software programs, which do not necessarily have networking capabilities.\n The web client 406 accesses the various applications 420 via the web interface supported by the web server 416.  Similarly, the programmatic client 408 accesses the various services and functions provided by the applications 420 via the\nprogrammatic interface provided by the API server 414.\n FIG. 4 also illustrates a third-party application 428, executing on a third-party server machine 430.  The third-party server machine may have programmatic access to the networked system 402 via the programmatic interface provided by the API\nserver 414.  For example, the third-party application 428 may, utilizing information retrieved from the networked system 402, support one or more features or functions on a website hosted by the third party.  The third-party website may, for example,\nprovide one or more functions that are supported by the relevant applications of the networked system 402.  In some embodiments, the networked system 102 may comprise functional components of a professional social network.\n The application servers 418 as illustrated host feature vector filter 306.  The feature vector filter 306 compares a feature vector of a user's news feed to feature vectors of a content delivery database 514 (see FIG. 5).  The feature vector\nfilter 306 can filter users using L1 or L2 norm vector distance, one or more decision trees, Term Frequency Inverse Document Frequency (TF-IDF), or the like.\n The application servers 418 or the mobile device 320 can include an application 420 that receives image data as input and produces a feature vector as output.  An image recognition technique, such as scale invariant feature transform (SIFT),\nHistogram of Gradients (HOG), optical character recognition (OCR), or the like, can be used to extract text, image contours, dominant colors and other features of the image 104 or 108 in the field of view 326 of the camera 322.\n FIG. 5 is a block diagram showing functional components of a professional social network within the networked system 402, in accordance with an example embodiment.  As shown in FIG. 5, the professional social network may include a three-tiered\narchitecture, consisting of a front-end layer 501, an application logic layer 503, and a data layer 505.  In some embodiments, the modules, systems, and/or engines shown in FIG. 5 represent a set of executable software instructions and the corresponding\nhardware (e.g., memory and processing circuitry (e.g., a processor, field programmable gate array (FPGA), and/or components configured to execute instructions and perform operations dictated by the instructions, such as can include a transistor,\nresistor, inductor, capacitor, regulator, power source, multiplexer, amplifier, switch, buffer, diode, or the like) for executing the instructions.  One skilled in the art recognizes that various additional functional modules or engines may be used with\na professional social network, such as that illustrated in FIG. 5, to facilitate additional functionality that is not specifically described herein.  The various functional modules and engines depicted in FIG. 5 may reside on a single server computer, or\nmay be distributed across several server computers in various arrangements.  Moreover, although a professional social network is depicted in FIG. 5 as a three-tiered architecture, embodiments are not limited to such architecture.  Other architectures are\nwithin the scope of the present disclosure.\n As shown in FIG. 5, in some embodiments, the front-end layer 501 comprises a user interface module (e.g., a web server) 502, which receives requests and inputs from various client-computing devices (e.g., client machine 410 or 412, or 3.sup.rd\nparty server 430), and communicates appropriate responses to the requesting client devices.  For example, the user interface module(s) 502 may receive requests in the form of Hypertext Transport Protocol (HTTP) requests, or other web-based, application\nprogramming interface (API) requests.\n In some embodiments, the application logic layer 503 includes various application server modules 504, which, in conjunction with the user interface module(s) 502, generates various user interfaces (e.g., web pages, such as newsfeeds) with data\nretrieved from various data sources in the data layer 505.  In some embodiments, individual application server modules 504 are used to implement the functionality associated with various services and features of the professional social network.  For\ninstance, the ability of an organization to establish a presence in a social graph of the social network service, including the ability to establish a customized web page on behalf of an organization, or to publish messages or status updates on behalf of\nan organization, may be services implemented in independent application server modules 504.  Similarly, a variety of other applications or services that are made available to members of the social network service may be embodied in their own application\nserver modules 504.\n As shown in FIG. 5, the data layer 505 may include several databases, such as a database 510 for storing profile data 516, including both member profile attribute data as well as profile attribute data for various organizations.  In some\nembodiments, when a person initially registers to become a member of the professional social network, the person is prompted to provide some profile attribute data, such as his or her name, age (e.g., birthdate), gender, interests, contact information,\nhome town, address, the names of the member's spouse and/or family members, educational background (e.g., schools, majors, matriculation and/or graduation dates, etc.), employment history, skills, professional organizations, and so on.  This information\nmay be stored, for example, in the database 510.  Similarly, when a representative of an organization initially registers the organization with the professional social network, the representative may be prompted to provide certain information about the\norganization.  This information may be stored, for example, in the database 510, or another database (not shown).  In some embodiments, the profile data 516 may be processed (e.g., in the background or offline) to generate various derived profile data. \nFor example, if a member has provided information about various job titles the member has held with the same or different companies, and for how long, this information can be used to infer or derive a member profile attribute indicating the member's\noverall seniority level, or a seniority level within a company.  In some embodiments, importing or otherwise accessing data from one or more externally hosted data sources may enhance profile data 516 for both members and organizations.  For instance,\nwith companies, financial data may be imported from one or more external data sources, and made part of a company's profile.\n The profile data 516 may also include information regarding settings for members of the professional social network.  These settings may comprise various categories, including, but not limited to, privacy and communications.  Each category may\nhave its own set of settings that a member may control.\n After a user is registered, a member may invite other members, or be invited by other members, to connect via the professional social network.  A \"connection\" may require a bi-lateral agreement by the members, such that both members acknowledge\nthe establishment of the connection.  Similarly, with some embodiments, a member may elect to \"follow\" another member.  In contrast to establishing a connection, the concept of \"following\" another member typically is a unilateral operation, and at least\nwith some embodiments, does not require acknowledgement or approval by the member that is being followed.  When one member follows another, the member who is following may receive status updates or other messages published by the member being followed,\nor relating to various activities undertaken by the member being followed.  Similarly, when a member follows an organization, the member becomes eligible to receive messages or status updates published on behalf of the organization.  For instance,\nmessages or status updates published on behalf of an organization that a member is following will appear in the member's personalized data feed or content stream.  The various associations and relationships that the members establish with other members,\nor with other entities and objects, may be stored and maintained as social graph data within a social graph database 512.\n The professional social network may provide a broad range of other applications and services that allow members the opportunity to share and receive information, often customized to the interests of the member.  For example, with some\nembodiments, the professional social network may include a photo sharing application that allows members to upload and share photos with other members.  With some embodiments, members may be able to self-organize into groups, or interest groups,\norganized around a subject matter or topic of interest.  With some embodiments, the professional social network may host various job listings providing details of job openings with various organizations.\n In some embodiments, the professional social network provides an application programming interface (API) module through which third-party applications can access various services and data provided by the professional social network.  For\nexample, using an API, a third-party application may provide a user interface and logic that enables an authorized representative of an organization to publish messages from a third-party application to a content hosting platform of the professional\nsocial network that facilitates presentation of activity or content streams maintained and presented by the professional social network.  Such third-party applications may be browser-based applications, or may be operating system-specific.  Some\nthird-party applications may reside and execute on one or more mobile devices (e.g., a smartphone, or tablet computing devices) having a mobile operating system, such as the client machine 410 or 412.\n The data in the data layer 505 may be accessed, used, and adjusted by the application 420.  Although the feature vector filter 306 and the application 420 are referred to herein as being used in the context of a professional social network, it\nis contemplated that it may also be employed in the context of any website or online services, including, but not limited to, content sharing sites (e.g., photo- or video-sharing sites, text-sharing sites, financial management sites, retail sites, or the\nlike) and any other apps that try to gain and retain a user's attention.  Although features of the present disclosure are referred to herein as being used or presented in the context of an app, it is contemplated that any user interface view (e.g., a\nuser interface on a mobile device or on desktop software) is within the scope of the present disclosure.\n In one or more embodiments, the data layer 505 further includes a database 514 that includes content delivery data 518 based on newsfeeds provided to users of the social network.  The content delivery data 518 includes data regarding the updates\nprovided to users through their newsfeeds, such as can include a time the newsfeed was presented, updates included in the newsfeed, the order of the updates included in the newsfeeds, social counts of the updates in the newsfeed (e.g., number of likes,\nshares, comments, or the like), status headers of the respective updates, or the like.  A feature vector can be determined for the newsfeed presented, stored in the content delivery database 514, and associated with the newsfeed in the content delivery\ndata 518.  The feature vector can be determined using a same technique that is used to determine the feature vector of the newsfeed captured using the mobile device 320.\n FIG. 6 illustrates, by way of example, a diagram of an embodiment of a system 600 for helping a user connect with another user of a website.  The system 600 as illustrated includes the mobile device 320, the application server 418, and databases\n510, 512, and 514.  The application server 418 as illustrated includes the feature vector filter 306.  The feature vector filter 306 can receive an image or feature vector from the mobile device 320.  The feature vector filter 306 can issue a query to\nthe content delivery database 514 for one or more feature vectors 602 of newsfeeds of users that have been presented, such as within a certain time window specified by the query.  The feature vector filter 306 can determine a subset of users associated\nwith the feature vector or image 604 through a response to one or more queries to the profile data database 510, the social graph data database 512, or the content delivery database 514.\n For example, the feature vector filter 306 can (a) issue a query to the social graph data database 512 to determine users connected to one or more users mentioned in the newsfeed of the image, (b) issue a query to the content delivery database\n514 to determine users that were presented newsfeeds with a specific social counts on an update of their newsfeed, (c) issue a query to the content delivery database 514 to determine users that were presented newsfeeds with updates in a specific order,\n(d) issue a query to the content delivery database 514 to determine one or more feature vectors of users that match one, all, or some number in between of the response(s) from the query issued in (a)-(c), or (e) issue a query to the profile data database\n610 for profile data, such as a profile picture, job data, or the like, of a user determined to match the feature vector or image 604 based on reduction of possible users or comparison of feature vectors 604 and 602.\n Using responses to one or more queries, the feature vector filter 306 can reduce a set of possible users that are associated with the photographed newsfeed.  The feature vector filter 306 can perform queries in a specified order, such as can be\nprogrammed into the feature vector filter 306 or determined by the feature vector filter 306 in accessing a policy that defines the order.\n Consider an example in which the social graph data database 512 indicates one hundred users are connected to two users mentioned on the newsfeed, and of those one hundred users, the content delivery database 514 indicates that two were presented\nupdates in a specified order.  The feature vectors 602 can be retrieved for those two users and one or more colors indicated in those feature vectors 602 can be compared to one or more colors in the feature vector or image to uniquely determine the one\nuser of the two users that are associated with the newsfeed further based on profile picture colors.\n FIG. 7 is a flowchart illustrating an example method 700, according to various embodiments.  The method 700 as illustrated includes, comparing a first feature vector detailing features of an image of a newsfeed of a user of users of a social\nnetwork to a subset of second feature vectors detailing features of newsfeeds presented to the users of the social network, at operation 705; and in response to determining the first feature vector matches a second feature vector of the subset of second\nfeature vectors, providing a name, profile data, and profile picture of a user associated with the newsfeed, at operation 710.\n The method 700 can further include determining the subset of second feature vectors by filtering the second feature vectors to include feature vectors of users currently logged on to the social network.  The method 700 can further include\ndetermining the subset of second feature vectors by filtering the second feature vectors based on a degree of connection between a first user associated with the first feature vector and respective second users associated with respective second feature\nvectors of the second feature vectors.\n The method 700 can further include determining the second feature vectors before presenting associated respective newsfeeds to the users of the social network site.  The method 700 can further include determining the subset of second feature\nvectors by filtering the second feature vectors based on a color in a profile picture in the image of the newsfeed.  The method 700 can further include before comparing the first feature vector to the subset of second feature vectors, determining, based\non image data provided by a mobile device, the first feature vector.  The method 700 can further include, before comparing the first feature vector to the subset of second feature vectors, receiving, from a mobile device, the first feature vector.\n Certain embodiments are described herein as including logic or a number of components, modules, or mechanisms.  Modules may constitute either software modules (e.g., code embodied on a machine-readable medium or in a transmission signal) or\nhardware modules.  A hardware module is a tangible unit capable of performing certain operations and may be configured or arranged in a certain manner.  In example embodiments, one or more computer systems (e.g., a standalone, client or server computer\nsystem) or one or more hardware modules of a computer system (e.g., a processor or a group of processors) may be configured by software (e.g., an application or application portion) as a hardware module that operates to perform certain operations as\ndescribed herein.\n In various embodiments, a hardware module may be implemented mechanically or electronically.  For example, a hardware module may comprise dedicated circuitry or logic that is permanently configured (e.g., as a special-purpose processor, such as\na field programmable gate array (FPGA) or an application-specific integrated circuit (ASIC)) to perform certain operations.  A hardware module may also comprise programmable logic or circuitry (e.g., as encompassed within a general-purpose processor or\nother programmable processor) that is temporarily configured by software to perform certain operations.  The decision to implement a hardware module mechanically, in dedicated and permanently configured circuitry, or in temporarily configured circuitry\n(e.g., configured by software) may be driven by cost and time considerations.\n The term \"hardware module\" is a tangible entity, be that an entity that is physically constructed, permanently configured (e.g., hardwired) or temporarily configured (e.g., programmed) to operate in a certain manner and/or to perform certain\noperations described herein.  Considering embodiments in which hardware modules are temporarily configured (e.g., programmed), each of the hardware modules need not be configured or instantiated at any one instance in time.  For example, where the\nhardware modules comprise a general-purpose processor configured using software, the general-purpose processor may be configured as respective different hardware modules at different times.  Software may accordingly configure a processor, for example, to\nconstitute a particular hardware module at one instance of time and to constitute a different hardware module at a different instance of time.\n Hardware modules can provide information to, and receive information from, other hardware modules.  Accordingly, the described hardware modules may be regarded as being communicatively coupled.  Where multiple of such hardware modules exist\ncontemporaneously, communications may be achieved through signal transmission (e.g., over appropriate circuits and buses) that connect the hardware modules.  In embodiments in which multiple hardware modules are configured or instantiated at different\ntimes, communications between such hardware modules may be achieved, for example, through the storage and retrieval of information in memory structures to which the multiple hardware modules have access.  For example, one hardware module may perform an\noperation, and store the output of that operation in a memory device to which it is communicatively coupled.  Another hardware module may, at a later time, access the memory device to retrieve and process the stored output.  Hardware modules may initiate\ncommunications with input or output devices, and can operate on a resource (e.g., a collection of information).\n The various operations of example methods described herein may be performed, at least partially, by one or more processors that are temporarily configured (e.g., by software) or permanently configured to perform the relevant operations.  Whether\ntemporarily or permanently configured, such processors may constitute processor-implemented modules that operate to perform one or more operations or functions.  The modules referred to herein may, in some example embodiments, comprise\nprocessor-implemented modules.\n Similarly, the methods described herein may be at least partially processor-implemented.  For example, at least some of the operations of a method may be performed by one or more processors or processor-implemented modules.  The performance of\ncertain of the operations may be distributed among the one or more processors, not only residing within a single machine, but deployed across a number of machines.  In some embodiments, the processor or processors may be located in a single location\n(e.g., within a home environment, an office environment or as a server farm), while in other embodiments the processors may be distributed across a number of locations.\n The one or more processors may also operate to support performance of the relevant operations in a \"cloud computing\" environment or as a \"software as a service\" (SaaS).  For example, at least some of the operations may be performed by a group of\ncomputers (as examples of machines including processors), these operations being accessible via a network (e.g., the Internet) and via one or more appropriate interfaces (e.g., application program interfaces (APIs)).\n Example embodiments may be implemented in digital electronic circuitry, or in computer hardware, firmware, software, or in combinations of them.  Example embodiments may be implemented using a computer program product (e.g., a computer program\ntangibly embodied in an information carrier, such as in a machine-readable medium for execution by, or to control the operation of, data processing apparatus, such as a programmable processor, a computer, or multiple computers).\n A computer program can be written in any form of programming language, including compiled or interpreted languages, and it can be deployed in any form, including as a stand-alone program or as a module, subroutine, or other unit suitable for use\nin a computing environment.  A computer program can be deployed to be executed on one computer or on multiple computers at one site or distributed across multiple sites and interconnected by a communication network.\n In example embodiments, operations may be performed by one or more programmable processors executing a computer program to perform functions by operating on input data and generating output.  Method operations can also be performed by, and\napparatus of example embodiments may be implemented as, special purpose logic circuitry (e.g., a FPGA or an ASIC).\n The computing system can include clients and servers.  A client and server is generally remote from each other and typically interact through a communication network.  The relationship of client and server arises through computer programs\nrunning on the respective computers and having a client-server relationship to each other.  In embodiments deploying a programmable computing system, it will be appreciated that that both hardware and software architectures require consideration. \nSpecifically, it will be appreciated that the choice of whether to implement certain functionality in permanently configured hardware (e.g., an ASIC), in temporarily configured hardware (e.g., a combination of software and a programmable processor), or a\ncombination of permanently and temporarily configured hardware may be a design choice.  Below are set out hardware (e.g., machine) and software architectures that may be deployed, in various example embodiments.\n FIG. 8 is a block diagram of an example computer system 800 on which operations, actions and methodologies described herein may be executed, in accordance with an example embodiment.  One or more components of the feature vector filter 306,\nfront end 201, mobile device 320 or 324, application logic layer 203, data layer 205, the system 200, or the like, may include one or more components of the computer system 900.  In alternative embodiments, the machine operates as a standalone device or\nmay be connected (e.g., networked) to other machines.  In a networked deployment, the machine may operate in the capacity of a server or a client machine in server-client network environment, or as a peer machine in a peer-to-peer (or distributed)\nnetwork environment.  The machine may be a personal computer (PC), a tablet PC, a set-top box (STB), a Personal Digital Assistant (PDA), a cellular telephone, a web appliance, a network router, switch or bridge, or any machine capable of executing\ninstructions (sequential or otherwise) that specify actions to be taken by that machine.  Further, while only a single machine is illustrated, the term \"machine\" shall also be taken to include any collection of machines that individually or jointly\nexecute a set (or multiple sets) of instructions to perform any one or more of the methodologies discussed herein.\n Example computer system 900 includes a processor 902 (e.g., a central processing unit (CPU), a graphics processing unit (GPU) or both), a main memory 904, and a static memory 906, which communicate with each other via a bus 908.  Computer system\n700 may further include a video display device 910 (e.g., a liquid crystal display (LCD) or a cathode ray tube (CRT)).  Computer system 900 also includes an alphanumeric input device 912 (e.g., a keyboard), a user interface (UI) navigation device 914\n(e.g., a mouse or touch sensitive display), a disk drive unit 916, a signal generation device 918 (e.g., a speaker) and a network interface device 920.\n Disk drive unit 916 includes a machine-readable medium 922 on which is stored one or more sets of instructions and data structures (e.g., software) 924 embodying or utilized by any one or more of the methodologies or functions described herein. \nInstructions 924 may also reside, completely or at least partially, within main memory 904, within static memory 906, and/or within processor 902 during execution thereof by computer system 900, main memory 904 and processor 902 also constituting\nmachine-readable media.\n While machine-readable medium 922 is shown in an example embodiment to be a single medium, the term \"machine-readable medium\" may include a single medium or multiple media (e.g., a centralized or distributed database, and/or associated caches\nand servers) that store the one or more instructions or data structures.  The term \"machine-readable medium\" includes any tangible medium that may store, encode, or carry instructions for execution by the machine and that cause the machine to perform any\none or more of the methodologies of the present technology, or that may store, encode, or carry data structures utilized by or associated with such instructions.  The term \"machine-readable medium\" shall accordingly be taken to include, but not be\nlimited to, solid-state memories, and optical and magnetic media.  Specific examples of machine-readable media include non-volatile memory, including by way of example semiconductor memory devices (e.g., erasable programmable read-Only Memory (EPROM),\nElectrically Erasable Programmable Read-Only Memory (EEPROM), and flash memory devices; magnetic disks such as internal hard disks and removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks.\n Instructions 924 may further be transmitted or received over a communications network 926 using a transmission medium.  Instructions 924 may be transmitted using network interface device 920 and any one of a number of well-known transfer\nprotocols (e.g., HTTP).  Examples of communication networks include a local area network (\"LAN\"), a wide area network (\"WAN\"), the Internet, mobile telephone networks, Plain Old Telephone (POTS) networks, and wireless data networks (e.g., WiFi and WiMAX\nnetworks).  The term \"transmission medium\" shall be taken to include any intangible medium that may store, encode, or carry instructions for execution by the machine, and includes digital or analog communications signals or other intangible media to\nfacilitate communication of such software.\n Although an embodiment has been described with reference to specific example embodiments, it will be evident that various modifications and changes may be made to these embodiments without departing from the broader spirit and scope of the\npresent disclosure.  Accordingly, the specification and drawings are to be regarded in an illustrative rather than a restrictive sense.  The accompanying drawings that form a part hereof, show by way of illustration, and not of limitation, specific\nembodiments in which the subject matter may be practiced.  The embodiments illustrated are described in sufficient detail to enable those skilled in the art to practice the teachings disclosed herein.  Other embodiments may be utilized and derived\ntherefrom, such that structural and logical substitutions and changes may be made without departing from the scope of this disclosure.  This Detailed Description, therefore, is not to be taken in a limiting sense, and the scope of various embodiments is\ndefined only by the appended claims, along with the full range of equivalents to which such claims are entitled.  Although specific embodiments have been illustrated and described herein, it should be appreciated that any arrangement calculated to\nachieve the same purpose may be substituted for the specific embodiments shown.  This disclosure is intended to cover any and all adaptations or variations of various embodiments.  Combinations of the above embodiments, and other embodiments not\nspecifically described herein, will be apparent to those of skill in the art upon reviewing the above description.", "application_number": "15846268", "abstract": " A method can include comparing a first feature vector detailing features\n     of an image of a newsfeed of a user of users of a social network to a\n     subset of second feature vectors detailing features of newsfeeds\n     presented to the users of the social network; and in response to\n     determining the first feature vector matches a second feature vector of\n     the subset of second feature vectors, providing a name, profile data, and\n     profile picture of a user associated with the newsfeed.\n", "citations": ["20100274815", "20140129627", "20150199443", "20180253193", "20190087865"], "related": []}]